{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcSCLFif59ow"
      },
      "source": [
        "### Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlK7xfpv59ow"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-045.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2WJsTwl59ow"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-048.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJs3X1zz59ow"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-050.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lxnIjkH59ox"
      },
      "source": [
        "Давайте обобщим наш attention слой. \n",
        "\n",
        "* Во-первых, заменим фцию $F_{att}$ на скалярное произведение.\n",
        "\n",
        "* В предыдущем примере мы использовали только один вектор для поиска совпадений (query вектор), который являлся внутренним состоянием декодера. Давайте вместо этого дадим возможность нашей модели использовать сразу нессколько query-векторов. Благодаря этому мы будем получать несколько распределений весов на выходе (по 1 для каждого query-вектора).\n",
        "\n",
        "* Обратим внимание, что мы используем входной вектор Х дважды: при вычислении похожести с query-вектором и при вычислении конечного выходного взвешенного решения. Для большей гибкости давайте разделим их и будем отдельно вычислять key-вектора и отдельно value-вектора. key-вектора мы будем использовать для поиска схожести с value-векторами. А они в свою очередь, будут использоваться для вычисления конечного выходного взевешенного значения. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zfjsz6L59ox"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-056.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlBwWCRr59ox"
      },
      "source": [
        "### Self-Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q_zsJ7D59ox"
      },
      "source": [
        "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
        "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
        "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
        "textual entailment and learning task-independent sentence representations\n",
        "\n",
        "Self-attention, также иногда называемый intra-attention - это механизм , связывающий различные позиции одной последовательности для вычисления представления последовательности. \n",
        "\n",
        "Self-attention успешно используется в различных задачах, включая понимание прочитанного (reading comprehension), абстрактное обобщение, текстовое влечение (entailment) и обучение независимым от задачи представлениям предложений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJYA2Wz059oy"
      },
      "source": [
        "Возникает вопрос, как это сделать наилучшим образом. \n",
        "\n",
        "Допустим, при обработке временных рядов используют некую сглаживающую функцию, например gaussian. \n",
        "\n",
        "Однако при обработке предложений очевидно, что неправильно считать рядом стоящие слова как наиболее близкие\n",
        "друг к другу по контексту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYB9FYoa59oy"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/l08_4-1.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fepb5WPM59oy"
      },
      "source": [
        "Основная идея заключается в том, что мы рассчитываем и query, и input, и key на основе input-векторов. \n",
        "\n",
        "Таким образом мы будем сравнивать каждый наш входной вектор со всеми остальными входными векторами входного множества. \n",
        "\n",
        "Для расчета query-векторов мы добавляем матрицу $W_Q$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNT1qOsq59oy"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-063.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heBX1DEU59oz"
      },
      "source": [
        "Свой self attention инвариантен относительно порядка входных последовательностей. \n",
        "\n",
        "Таким образом мы получили новый тип слоев нейронной сети, который **не** использует информацию о порядке входной последовательности и оперирует только на множестве входных векторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO_ePNO059oz"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-070.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6z8UFvo59oz"
      },
      "source": [
        "Иногда нам хочется чтобы порядок учитывался, например при переводе и генерации описания картинок (caption). \n",
        "\n",
        "Для этого мы добавляем информацию на входы о порядке наших элементов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2OnmQSq59oz"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-072.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REQ_ja_J59o0"
      },
      "source": [
        "### Masked Self-Attention Layer - совместить"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GunDUprh59o0"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-073.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF7bz0_c59o0"
      },
      "source": [
        "#### Multihead Self-Attention Layer\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/l08_7-1.png\" width=\"700\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAC9-akL59o1"
      },
      "source": [
        "Давайте посмотрим на то, к каким словам предложения gave может иметь отношение. В общим случае глагол может иметь связку со многоми частями предложения. Как, например, к подлежащему, так и к причастиям.\n",
        "\n",
        "В идеале, нам бы хотелось обратить внимние функции (attention) на все эти взаимосвязи.\n",
        "\n",
        "**Для этого небходимо ввести несколько механизмов внимания, как это показано на картинке**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2VtCBK959o2"
      },
      "source": [
        " <img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/l08_6-1.png\" width=\"700\"> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIAqIl6R59o2"
      },
      "source": [
        "* Чтобы осуществить задуманное, вместо одного набора query, будем использовать несколько независимых наборов. \n",
        "\n",
        "* Причем каждый набор будет считаться уникальной матрицей. \n",
        "\n",
        "* Аналогично сделаем для keys и values. Количество таких наборов внутри keys, queries, values должно быть **одинаковым**. \n",
        "\n",
        "* Обозначим это число как h - head, далее производим аналогичные манипуляции, при этом введем в параллель h таких функций attention\n",
        "* На последнем шаге мы их соединяем (конкатинируем)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzEfe51759o2"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-075.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viQtPZtX59o3"
      },
      "source": [
        "### Three Ways of Processing Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKYLb7vj59o3"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/l08_1_1.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeIz1iXL59o3"
      },
      "source": [
        "\n",
        "Модель attention позволяет обрабатывать последовательности параллельно в отличие от моделей основанных на RNN, это позволяет данную модель в большей мере использовать возможности использования параллельных вычислений на GPU \n",
        "\n",
        "RNN:\n",
        "**Преимущества** \n",
        "\n",
        "• Возможность обработки входных данных любой длины\n",
        "\n",
        "• Размер модели не увеличивается с размером входного сигнала\n",
        "\n",
        "• Расчет учитывает историческую информацию\n",
        "\n",
        "• Веса распределяются во времени \n",
        "\n",
        "**Недостатки**\n",
        "\n",
        "• Вычисление происходит медленно\n",
        "\n",
        "• Трудность доступа к информации, полученной давным-давно\n",
        "\n",
        "• Не может учитывать какие-либо будущие входные данные для текущего состояния"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EAMsm1Z59o4"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-084.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h_YGPDf59o4"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-085.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFc19_Hb59o4"
      },
      "source": [
        "### The Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPn2WdXC59o4"
      },
      "source": [
        "Раньше мы передавали информацию из блока attention в RNN для ее последующего применения, однако в 2017 году вышла статья под названием Attention is all you need. В ней говорилось о том, что для обработки последовательностей можно ограничиться только блоком внимания. Данная модель получила название Transformer. \n",
        "\n",
        "По сути это базовый блок, который основывается только на self-attention при работе с входными векорами. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EC9uGpV59o4"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-092.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4d2AC7K59o5"
      },
      "source": [
        "Блок устроен следующим образом: входные вектора мы запускаем в блок self-attention и используем residual connection как обходной путь вокруг self-attention, затем мы их складываем. \n",
        "\n",
        "После - применяем слой нормализации, затем - слой feat forward сетей, плюс обходной путь вокруг feat forward. \n",
        "\n",
        "Складываем их и выполняем нормализацию. \n",
        "\n",
        "Это один блок-трансформер. \n",
        "\n",
        "В реальных моделях эти слои стекируют друг с другом и получается большая трансформер-модель. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU3V6_2m59o5"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-094.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZTw34C059o5"
      },
      "source": [
        "Although this achieves parallelization, it is still computationally expensive. The number of operations per layer incurred by RNNs and CNNs is way more unreasonable as compared to the quality of results they offer. The original Transformer paper has put forth a comparison of these parameters for the competent models:\n",
        "\n",
        "Хотя таким образом (свертками) достигается распараллеливание, однако это вычислительно дорого. Количество операций на слой, выполняемых RNN и CNN, гораздо более необоснованно по сравнению с качеством результатов, которые они предлагают.\n",
        "\n",
        "В одной из статей про Трансформеры было предложено сравнение этих параметров для компетентных моделей:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqo6qPKt59o5"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_trans.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3dIUoFn59o5"
      },
      "source": [
        "Here, d (or d_model) is the representation dimension or embedding dimension of a word (usually in the range 128–512), n is the sequence length (usually in the range 40–70), k is the kernel size of the convolution and r is the attention window-size for restricted self-attention. From the table, we can infer the following:\n",
        "\n",
        "* Clearly, the per-layer computational complexity of self-attention is way less than that of others.\n",
        "* With respect to sequential operations, except RNNs, all other approaches offer parallelization, hence their complexity is O(1).\n",
        "* The final metric is maximum path length, which superficially means the complexity for attending long-term dependencies or distant words. Since convolutional models use hierarchical representations, their complexity is nlog(n), while self-attention models attend all the words at the same step, hence their complexity is O(1).\n",
        "\n",
        "\n",
        "The Transformer uses the self-attention mechanism where attention weights are calculated using all the words in the input sequence at once, hence it facilitates parallelization. In addition to that, since the per-layer operations in the Transformer are among words of the same sequence, the complexity does not exceed O(n²d). Hence, the transformer proves to be effective (since it uses attention) and at the same time, a computationally efficient model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lZyd0eG59o5"
      },
      "source": [
        "Здесь d (или *d_model*) - размер выборки или embedding вложения слова (обычно в диапазоне 128-512), n-длина последовательности (обычно в диапазоне 40-70), k-размер ядра свертки, а r-размер окна внимания для ограниченного self-attention. Из таблицы мы можем сделать следующие выводы:\n",
        "\n",
        "* Очевидно, что вычислительная сложность каждого слоя self-attention намного меньше, чем у других.\n",
        "* Что касается последовательных операций, то, за исключением RNN, все другие подходы предлагают параллельность действий, поэтому их сложность составляет O(1).\n",
        "* Конечная метрика - максимальная длина пути, которая поверхностно означает сложность для посещения долгосрочных зависимостей или отдаленных слов. Поскольку сверточные модели используют иерархические представления, их сложность равна nlog(n), в то время как модели самосознания посещают все слова на одном и том же шаге, следовательно, их сложность равна O(1).\n",
        "\n",
        "\n",
        "Трансформер использует механизм само-внимания, где веса внимания вычисляются с использованием всех слов во входной последовательности сразу, следовательно, это облегчает распараллеливание. Кроме того, поскольку операции на каждом слое в трансформерах относятся к словам одной и той же последовательности, сложность не превышает O($n^2d$). Следовательно, трансформер оказывается эффективной (поскольку она использует attention) и в то же время вычислительно качественной моделью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwB7bdWU59o6"
      },
      "source": [
        "статья: https://towardsdatascience.com/transformers-explained-65454c0f3fa7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR0xMl5059o6"
      },
      "source": [
        "### The Transformer: Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RboS0TmY59o6"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-095.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BosmoQ659o6"
      },
      "source": [
        "В 2020 практически все успехи машинного обучения были связаны с применением трансформеров. На сегодняшний день - это самая передовая модель для обработки последовательностей. \n",
        "\n",
        "Стоит отметить, что модели трансформеров имеют огромное кличество параметров, как следствие - требуются большие вычислительные мощности для их обучения. В связи с этим, обучение данных моделей с нуля, преимущественно доступны только развитым государствам и крупным компаниям. \n",
        "\n",
        "Большая часть предприятий может расчитывать только на дообучене этих моделей для своих задач, что все равно показывает хорошие результаты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkrPQkVp59o7"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-104.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj1I0Tw359o7"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h36-uz26f-2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eU8X0gE_PU9"
      },
      "source": [
        "## Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KV0EPcP59oT"
      },
      "source": [
        "\n",
        "\n",
        "Суть этой задачи заключается в том, чтобы нейросеть составила текстовое описание фотографии. Для этого необходимо сначала классифицировать объекты на изображении, а затем передать результат (одну или несколько меток) в языковую рекуррентную модель, которая сможет составить из них осмысленную фразу. При этом мы действуем точно так же, как в случае с обычной языковой моделью: преобразуем метку изображения в вектор, который обрабатывается декодером.\n",
        "\n",
        "Чтобы рекуррентная сеть понимала, где именно начинается предложение, во время обучения на её вход подаётся стартовый опознавательный знак ( token $ <start>$ ). Для построения фразы используется заранее подготовленный словарь, например, из английских слов — и он может быть довольно большим.\n",
        "\n",
        "При переходе на каждое следующее скрытое состояние мы сохраняем как уже сгенерированные слова, так и информацию об изображении. В конце предложения в нейросеть отправляется финальный токен ( $ <end> $). Во время тестирования модель уже самостоятельно определяет, где должно начинаться и заканчиваться описание изображения.\n",
        "\n",
        "Обычно подобные архитектуры создаются с помощью контролируемого обучения (supervized learning) — это означает, что в обучающих датасетах уже присутствуют как изображения, так и описания для них. Наиболее популярным и самым большим набором данных является Microsoft COCO. Помимо image captioning он также применяется для сегментации, поиска ключевых точек и даже построения трёхмерной модели человека на основе его позы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9aGMzgA59oU"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-071.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iupsVOP59oV"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-079.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJH3l3mi59oW"
      },
      "source": [
        "Мы добавили в словарь две специальные команды $ <start> $ и $ <end> $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Ali4xU59oX"
      },
      "source": [
        "Иногда дает очень хорошие описания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSXCH04459oY"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-080.png\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6RAefDNHFoq"
      },
      "source": [
        "# Пример машинного перевода"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAJnLL5m59oE"
      },
      "source": [
        "\n",
        "\n",
        "Подробнее о том, как выглядят нейросети для машинного перевода и, в частности, Google-переводчик, можно прочитать в статье Google преодолевает барьер между человеческим и машинным переводом. А мы вернёмся к распознаванию изображений и поговорим об image captioning.\n",
        "\n",
        "ссылка https://www.reg.ru/blog/google-preodolevaet-barer-mezhdu-chelovecheskim-i-mashinnym-perevodom/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mliHGFlI6jag"
      },
      "source": [
        "# Применение RNN для генерации данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ4AZMyH59n_"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-012.gif\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nj9L4rj59n_"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-013.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrxAnrgC59oA"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-013-1.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeS-8zpj59oB"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-014.gif\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mv05SGQ60Be"
      },
      "source": [
        "# Чулан"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laj7_a0v59n6"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_14.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVgLEpJa59oo"
      },
      "source": [
        "* Rnn обеспечивает большую гибкость в архитектурном проектировании\n",
        "* Vanilla RNN намного проще, однако работают не очень хорошо\n",
        "* LSTM или GRU: аддитивное взаимодействие улучшает градиентный поток\n",
        "* Обратный поток градиентов в RNN делает explode или vanish\n",
        "\n",
        "explode управляется с помощью градиентного отсечения\n",
        "\n",
        "vanish - это контролируемые аддитивные взаимодействия (LSTM)\n",
        "\n",
        "* Лучшие/более простые архитектуры являются актуальной темой текущих исследований\n",
        "* Необходимо лучшее понимание (как теоретическое, так и эмпирическое)"
      ]
    }
  ]
}