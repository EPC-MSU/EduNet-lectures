{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение нейронных сетей в НЕ стационарных временных рядах (Продолжение)\n",
    "----\n",
    "Теперь повторим анализ, сделанный в предыдущем ноутбуке, но для действиетльно нестационарного временного ряда - курса акций.\n",
    "\n",
    "И посмотрим, как поведет себя ARIMA и нейронная сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Когда достаточно Mashine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте скачаем и проанализируем набор данных [AirPassengers](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/09/AirPassengers.csv), который мы будем использовать. Этот пример уже можно считать классическим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "#import yfinance as yf ## Библиотека для получения данных об акциях\n",
    "# fetch data by interval (including intraday if period < 60 days)\n",
    "# valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "# (optional, default is '1d')\n",
    "import yfinance as yf\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Загрузка и сохранение первичных данных\n",
    "tickers = ['SBER.ME']\n",
    "df = yf.download(tickers,start='2010-01-01', end = '2021-12-31', interval = '1wk')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "del df['Open'], df['High'], df['Low'], df['Adj Close'], df['Volume']\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Методы проверки стационарности\n",
    "\n",
    "Следующий шаг - определить, является ли данный ряд стационарным или нет.\n",
    "\n",
    "Вот некоторые общие методы, которые мы можем использовать для выполнения этой проверки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Визуальный тест\n",
    "\n",
    "Рассмотрим график так, как мы это делали в примере выше: мы можем определить ряд, в котором среднее значение, дисперсия и ковариация менялись со временем. Точно так же мы можем построить данные и определить, меняются ли свойства ряда со временем или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "df['Close'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADF (Augmented Dickey Fuller) Test\n",
    "Тест Дики Фуллера - один из самых популярных статистических тестов.\n",
    "\n",
    "Его можно использовать для определения наличия единичного корня в ряду и, следовательно, помочь нам понять, является ли ряд стационарным или нет.\n",
    "\n",
    "Нулевая и альтернативная гипотезы этого теста:\n",
    "- Нулевая гипотеза: ряд имеет единичный корень (значение a = 1)\n",
    "- Альтернативная гипотеза: у ряда нет единичного корня.\n",
    "\n",
    "Если нам не удастся отвергнуть нулевую гипотезу, мы можем сказать, что ряд нестационарен.\n",
    "Это означает, что ряд может быть линейным или разностным стационарным (мы узнаем больше о разностном стационарном в следующем разделе)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for ADF test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def adf_test(timeseries):\n",
    "    #Perform Dickey-Fuller test:\n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "       dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "\n",
    "#apply adf test on the series\n",
    "adf_test(df['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест на стационарность: Когда Test Statistic превышает Critical Value, мы не можем отклонить нулевую гипотезу (что означает, что ряд не является стационарным).\n",
    "\n",
    "В нашем примере выше Test Statistic > Critical Valueя, что означает, что ряд не является стационарным.\n",
    "Это подтверждает наше первоначальное наблюдение, которое мы первоначально видели в визуальном тесте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. KPSS (Kwiatkowski-Phillips-Schmidt-Shin) Test\n",
    "\n",
    "KPSS - еще один тест для проверки стационарности временного ряда (немного менее популярный, чем тест Дики Фуллера).\n",
    "\n",
    "Нулевая и альтернативная гипотезы для теста KPSS противоположны гипотезе теста ADF, что часто вызывает путаницу.\n",
    "\n",
    "Авторы теста KPSS определили нулевую гипотезу, поскольку процесс является стационарным по тренду, к альтернативной гипотезе ряда единичного корня.\n",
    "\n",
    "Давайте посмотрим на результаты теста KPSS:\n",
    "- Нулевая гипотеза: процесс является стационарным.\n",
    "- Альтернативная гипотеза: ряд имеет единичный корень (ряд не является стационарным)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#define function for kpss test\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "#define KPSS\n",
    "def kpss_test(timeseries):\n",
    "    print ('Results of KPSS Test:')\n",
    "    kpsstest = kpss(timeseries, regression='c')\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n",
    "    \n",
    "    for key,value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    print(kpss_output)\n",
    "\n",
    "kpss_test(df['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест на стационарность: если статистика теста больше критического значения, мы отклоняем нулевую гипотезу (ряд не является стационарным), если не удалось отклонить нулевую гипотезу (ряд стационарный).\n",
    "\n",
    "Для данных авиапассажиров значение тестовой статистики превышает критическое значение на всех доверительных интервалах, и, следовательно, мы можем сказать, что ряд не является стационарным.\n",
    "\n",
    "Может случиться так, что оба теста покажут противоречивые результаты.\n",
    "Это потому, что существует более одного типа стационарности:\n",
    "- тест ADF имеет альтернативную гипотезу о линейной или диффиренцируемой стационарности\n",
    "- а тест KPSS определяет стационарность тренда в серии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Создание стационарного временного ряда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дифференцирование\n",
    "В этом методе мы вычисляем разность последовательных членов в серии.\n",
    "\n",
    "Дифференцирование обычно выполняется, чтобы избавиться от переменного среднего.\n",
    "\n",
    "Математически разность можно записать как:\n",
    "y(t)‘= y(t) — y(t-1),\n",
    "где y(t) - значение в момент времени t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close_diff'] = df['Close'] - df['Close'].shift(1)\n",
    "plt.figure(figsize=(16, 4))\n",
    "df['Close_diff'].dropna().plot()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделение сезонности (Seasonal Differencing)\n",
    "При выделении сезонности вместо вычисления разницы между последовательными значениями мы вычисляем разницу между наблюдением и предыдущим наблюдением того же сезона.\n",
    "\n",
    "Например, наблюдение, сделанное в понедельник, будет вычтено из наблюдения, сделанного в предыдущий понедельник.\n",
    "\n",
    "Математически это можно записать так: y(t)‘= log(y(t)) — log(y(t-1)), где y(t) - значение в момент времени t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close_log'] = np.log(df['Close'])\n",
    "df['Close_log_diff'] = df['Close_log'] - df['Close_log'].shift(1)\n",
    "df = df.dropna()\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "df['Close_log_diff'].plot()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mask(df.eq('None')).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, этот график является значительным улучшением по сравнению с предыдущими графиками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Декомпозиция\n",
    "---\n",
    "- Base Level (базовый сигнал)\n",
    "- Trend (тренд)\n",
    "- Seasonality (сезонность)\n",
    "- Error (ошибка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(df['Close'])\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(411)\n",
    "plt.plot(df['Close'], label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако не обязательно, чтобы все временные ряды имели тренд и/или сезонность.\n",
    "\n",
    "В зависимости от характера тренда и сезонности временной ряд может быть смоделирован как аддитивный или мультипликативный. При этом каждое наблюдение в ряду может быть выражено либо как сумма, либо как произведение компонентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "Для анализа цикличных зависимостей, используется функция df.resample (), которая позволяет нам перегруппировать наши временные ряды до желаемых частот.\n",
    "\n",
    "Например, по кварталам, или по годам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(3,figsize=(20,10))\n",
    "\n",
    "ax1.plot(df.resample('M').mean())\n",
    "ax1.set_title('MONTH FREQUENCY');\n",
    "\n",
    "ax2.plot(df.resample('Q').mean())\n",
    "ax2.set_title('QUARTERLY FREQUENCY');\n",
    "\n",
    "ax3.plot(df.resample('A').mean())\n",
    "ax3.set_title('YEAR FREQUENCY');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделаем предикт с помощью ARIMA\n",
    "ARIMA - это авторегрессионная интегрированная модель скользящего среднего, которая определяется тремя параметрами порядка: (p, d, q).\n",
    "\n",
    "И тут мы схитрим :)\n",
    "\n",
    "Используем функцию auto_arima из библиотеки pmdarima для того, чтобы определить наиболее оптимальные параметры для модели ARIMA и плучить наилучшим образом подобранную модель ARIMA. Не факт, что мы смогли бы достигнуть такого результата без auto_arima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the library\n",
    "#!pip install pmdarima\n",
    "clear_output()\n",
    "  \n",
    "# Import the library\n",
    "from pmdarima import auto_arima\n",
    "  \n",
    "# Fit auto_arima function to AirPassengers dataset\n",
    "stepwise_fit = auto_arima(df['Close'], start_p = 1, start_q = 1,\n",
    "                          max_p = 3, max_q = 3, m = 12,\n",
    "                          start_P = 0, seasonal = True,\n",
    "                          d = None, D = 1, trace = True,\n",
    "                          error_action ='ignore',   # we don't want to know if an order does not work\n",
    "                          suppress_warnings = True,  # we don't want convergence warnings\n",
    "                          stepwise = True)           # set to stepwise\n",
    "  \n",
    "# To print the summary\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forecast using ARIMA Model\n",
    "# Fit a SARIMAX(3, 0, 0)x(2, 1, 0, 12) on the training set\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Train the model on the full dataset\n",
    "model = SARIMAX(df['Close'], \n",
    "                order = (0, 1, 1), \n",
    "                seasonal_order =(2, 1, 1, 12))\n",
    "result = model.fit()\n",
    "  \n",
    "# Forecast for the next 3 years\n",
    "forecast = result.predict(start = 0, \n",
    "                          end = (len(df['Close'])-1) + 0, \n",
    "                          typ = 'levels').rename('Forecast')\n",
    "  \n",
    "# Plot the forecast values\n",
    "df['Close'].plot(figsize = (16, 5), legend = True)\n",
    "forecast.plot(legend = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "Таким образом, мы:\n",
    "1. Ппознакомились с вариантами выполнения анализа временного ряда перед применение методов Mashing Learning\n",
    "2. Увидели, что для применения обычных методов ML требуется провести достаточно большую предварительную обработку данных, которые, нейронная сети делает \"автоматически\"\n",
    "3. Наблюдается автокорреляция с ценой (когда модель \"предсказывает\" предыдущее значение. То есть на лицо явное переобучение модели, октрую подобрал pmdarima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. А теперь сделаем предикт с помощью Deep Learning (PyTorch LSTM)\n",
    "----\n",
    "[Time Series Prediction with LSTM Using PyTorchTime Series Prediction with LSTM Using PyTorch](https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/pytorch/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb#scrollTo=NabsV8O5BBd5https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/pytorch/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb#scrollTo=NabsV8O5BBd5)\n",
    "\n",
    "- Dataloading\n",
    "- Скалирование\n",
    "- Формирование ансамблей данных \n",
    "- LSTM Mode\n",
    "- Traning\n",
    "- Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Загрузка и сохранение первичных данных\n",
    "tickers = ['SBER.ME']\n",
    "df = yf.download(tickers,start='2010-01-01', end = '2021-12-31', interval = '1wk')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "del df['Open'], df['High'], df['Low'], df['Adj Close'], df['Volume']\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\n",
    "\n",
    "training_set = df\n",
    "training_set = training_set.values # перевели dataframe в numpy.array\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.plot(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скалирование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler # проверить все\n",
    "\n",
    "xScaler = MinMaxScaler()\n",
    "training_data = xScaler.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формирование ансамблей данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция создания \"ансамблей\" данных\n",
    "def sliding_windows(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# установка длины ансамбля. от нее практически всегда зависит точность предикта и быстродействие\n",
    "seq_length = 8 # сравните 2 и 32\n",
    "x, y = sliding_windows(training_data, seq_length)\n",
    "\n",
    "train_size = int(len(y) * 0.8)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))\n",
    "\n",
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model\n",
    "Ее преимущество при применении в нестационарных временных рядах заключается в способности выявления взаимосвязей данных внутри временных рядов и \"автоматическое\" выделение различных типов сезонности, тенденций изменений дисперсий, ковариации и проч характеристик временного ряда.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    num_classes :\n",
    "    input_size  :\n",
    "    hidden_size :\n",
    "    num_layers  :\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers  = num_layers\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length  = seq_length\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True,\n",
    "                            dropout = 0.4)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        h_out = h_out.view(-1, self.hidden_size)\n",
    "        out = self.fc(h_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 1\n",
    "# Сложность нейронки должна быть адекватна датасету !!!\n",
    "hidden_size = seq_length # а можно попробовать 400 - в чем разница?\n",
    "num_layers = 1\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = lstm(trainX)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, trainY)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.eval()\n",
    "train_predict = lstm(dataX)\n",
    "\n",
    "data_predict = train_predict.data.numpy()\n",
    "dataY_plot = dataY.data.numpy()\n",
    "\n",
    "data_predict = xScaler.inverse_transform(data_predict)\n",
    "dataY_plot = xScaler.inverse_transform(dataY_plot)\n",
    "\n",
    "# Ploitting\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.axvline(x=train_size, c='r', linestyle='--')\n",
    "\n",
    "plt.plot(dataY_plot)\n",
    "plt.plot(data_predict)\n",
    "plt.suptitle('Time-Series Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "---\n",
    "1. Использование LSTM слоев при прогнозировании нестационарных временных рядов - отличное решение, тк нейронка способна самостоятельно выденить динамики изменениях составляющих временного ряда. Правда, в более сложных случаях (трейдинге, например, или чтении и расшивровки волн мозга) приходится прибегать к feature ingineering\n",
    "2. Сложость нейронной сети должна быть адекватна сложности подаваемых в нее данных. С ростом ансамбля и числа нейронов увеличивается заучивание тренировочной выборки и теряется способность к обощению\n",
    "3. Предварительный анализ цикличности в данных (если она есть) помогает понять оптимальный размер ансамбля (тут видно, что цикл в среднем составляет 8 интервалов)\n",
    "4. Также результат может зависеть от типа скалирования, который Вы применяете. Нужно знать принципы работы скаллеров и не стесняться экспериментировать с ними. См. статьи: [Data Preprocessing with Scikit-Learn: Standardization and Scaling](https://towardsdatascience.com/data-preprocessing-with-scikit-learn-standardization-and-scaling-cfb695280412) и [Gradient Descent, the Learning Rate, and the importance of Feature Scaling](https://towardsdatascience.com/gradient-descent-the-learning-rate-and-the-importance-of-feature-scaling-6c0b416596e1)\n",
    "5. При всей выгодности приненении нейронных сетей, необходимо быть осторожным с автокорреляцией (см статью [Avoiding the pitfalls](https://towardsdatascience.com/how-not-to-use-machine-learning-for-time-series-forecasting-avoiding-the-pitfalls-19f9d7adf424))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
