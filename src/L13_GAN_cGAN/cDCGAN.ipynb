{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Исследовать GAN для генерации точек на параболе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1\n",
    "* Посмотреть что будет если подавать в качестве шума uniform распределение\n",
    "* Попробовать избавится от линейности на параболе (любыми известными методами)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2\n",
    "* Сделать генерацию фигуры более сложной формы, например круга\n",
    "* Добится сходимости\n",
    "* Желательно сделать гиф-анимацию/видео какие точки выдаёт в процессе обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3\n",
    "* Предобучить только дискриминатор (сделать его сильным критиком).\n",
    "* Обучать только генератор (если критик достаточно сильный, то генератор не будет учиться)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenModel(nn.Module):\n",
    "    def __init__(self, latent_space):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15,1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g_batch(batch_size, latent_dim):\n",
    "    g_input = torch.randn(size=(batch_size, latent_dim)).cuda()\n",
    "    #g_input = torch.randn(size=(batch_size, latent_dim)).cuda()\n",
    "    labels = torch.ones(size=(batch_size,)).cuda()\n",
    "    return g_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_batch(batch_size, latent_dim, netG):\n",
    "    types_of_points = []\n",
    "    \n",
    "    # Generate true pairs and true labels\n",
    "    x = torch.distributions.Uniform(-1, +1).sample((batch_size,)).cuda()\n",
    "    true_pair = torch.vstack((x, x*x)).T.cuda()\n",
    "    true_labels = torch.ones(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "    types_of_points.append(torch.hstack((true_pair, true_labels)))\n",
    "    \n",
    "    # Generate fake uniform pairs and fake labels\n",
    "    if True:\n",
    "        x_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).cuda()\n",
    "        y_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).cuda()\n",
    "        fake_pair = torch.vstack((x_fake, y_fake)).T\n",
    "        fake_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "        types_of_points.append(torch.hstack((fake_pair, fake_labels)))\n",
    "\n",
    "    # Generate points from generator and set labels as fake\n",
    "    if True:\n",
    "        gan_pair = netG(torch.randn(size=(batch_size, latent_dim)).cuda())\n",
    "        #gan_pair = netG(torch.rand(size=(batch_size, latent_dim)).cuda())\n",
    "        gan_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "        types_of_points.append(torch.hstack((gan_pair, gan_labels)))\n",
    "    \n",
    "    # Stack all types of points\n",
    "    z = torch.vstack(types_of_points)\n",
    "    # Shuffle\n",
    "    z=z[torch.randperm(z.size()[0])]\n",
    "    \n",
    "    # Split back to samples and labels\n",
    "    mixed_pairs = z[:, :2]\n",
    "    mixed_labels = z[:, 2]\n",
    "    return mixed_pairs, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loss(model,test_loader,loss_function):\n",
    "    with torch.no_grad():\n",
    "        loss_test_total = 0\n",
    "        for samples, labels in test_loader:\n",
    "            outputs = model(samples.cuda())\n",
    "            loss = loss_function(outputs, labels.cuda())\n",
    "            loss_test_total += loss.item()\n",
    "        return loss_test_total/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netD_step(netD, batchD, loss_func, optimizer):\n",
    "    samples, labels = batchD\n",
    "    optimizer.zero_grad()\n",
    "    outputs = netD(samples.cuda())\n",
    "    loss = loss_func(outputs.cuda(), labels.unsqueeze(1).detach().cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netG_step(netD, netG, batchG, loss_func, optimizer):\n",
    "    samples, labels = batchG\n",
    "    optimizer.zero_grad()\n",
    "    outputs = netD(netG(samples.cuda()))\n",
    "    loss = loss_func(outputs.cuda(), labels.unsqueeze(1).detach().cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen(netG, epoch=\"Not provided\"):\n",
    "    Gin, _ = get_g_batch(1000, latent_dim)\n",
    "    out = netG(Gin).cpu()\n",
    "    plt.scatter(out.detach().numpy()[:, 0], out.detach().numpy()[:, 1], color=\"blue\", s=1)\n",
    "    plt.title(f'Generator points. End of epoch= {epoch+1}', fontsize=10)\n",
    "    plt.axis([-1,1,-0.5,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 5\n",
    "batch_size = 128\n",
    "batch_per_epoch = 1000\n",
    "epochs = 5\n",
    "\n",
    "netG = GenModel(latent_dim).cuda()\n",
    "netD = DisModel().cuda()\n",
    "loss_func = nn.BCELoss().cuda()\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_per_epoch):\n",
    "            batchG = get_g_batch(batch_size, latent_dim)\n",
    "            batchD = get_mix_batch(batch_size, latent_dim, netG)\n",
    "            \n",
    "            netD.train(True)\n",
    "            netG.train(False)\n",
    "            netD_step(netD, batchD, loss_func, optD)\n",
    "            \n",
    "            netD.train(False)\n",
    "            netG.train(True)\n",
    "            netG_step(netD, netG, batchG, loss_func, optG)\n",
    "            \n",
    "        # clear_output()\n",
    "        plot_gen(netG, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Решение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотреть что будет если подавать в качестве шума uniform распределение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = GenModel(latent_dim).cuda()\n",
    "netD = DisModel().cuda()\n",
    "loss_func = nn.BCELoss().cuda()\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.001)\n",
    "train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Попробовать избавится от линейности на параболе (любыми известными методами)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "class GenModel_2(nn.Module):\n",
    "    def __init__(self, latent_space):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, 50),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(50,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class DisModel_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(50, 15),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(15,1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = GenModel_2(latent_dim).cuda()\n",
    "netD = DisModel_2().cuda()\n",
    "loss_func = nn.BCELoss().cuda()\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.001)\n",
    "train(netD, netG, batch_per_epoch, batch_size, latent_dim, 10, loss_func, optD, optG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Решение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделать генерацию фигуры более сложной формы, например круга**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "def get_mix_batch(batch_size, latent_dim, netG):\n",
    "    types_of_points = []\n",
    "    \n",
    "    # Generate true pairs and true labels\n",
    "    phi = torch.distributions.Uniform(-np.pi, np.pi).sample((batch_size,)).cuda()\n",
    "    R=0.8\n",
    "    true_pair = torch.vstack((R*torch.cos(phi), R*torch.sin(phi))).T.cuda()\n",
    "    true_labels = torch.ones(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "    types_of_points.append(torch.hstack((true_pair, true_labels)))\n",
    "    \n",
    "    # Generate fake uniform pairs and fake labels\n",
    "    if True:\n",
    "        x_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).cuda()\n",
    "        y_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).cuda()\n",
    "        fake_pair = torch.vstack((x_fake, y_fake)).T\n",
    "        fake_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "        types_of_points.append(torch.hstack((fake_pair, fake_labels)))\n",
    "\n",
    "    # Generate points from generator and set labels as fake\n",
    "    if True:\n",
    "        gan_pair = netG(torch.randn(size=(batch_size, latent_dim)).cuda())\n",
    "        #gan_pair = netG(torch.rand(size=(batch_size, latent_dim)).cuda())\n",
    "        gan_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "        types_of_points.append(torch.hstack((gan_pair, gan_labels)))\n",
    "    \n",
    "    # Stack all types of points\n",
    "    z = torch.vstack(types_of_points)\n",
    "    # Shuffle\n",
    "    z=z[torch.randperm(z.size()[0])]\n",
    "    \n",
    "    # Split back to samples and labels\n",
    "    mixed_pairs = z[:, :2]\n",
    "    mixed_labels = z[:, 2]\n",
    "    return mixed_pairs, mixed_labels\n",
    "\n",
    "def plot_gen(netG, epoch=\"Not provided\"):\n",
    "    Gin, _ = get_g_batch(1000, latent_dim)\n",
    "    out = netG(Gin).cpu()\n",
    "    plt.scatter(out.detach().numpy()[:, 0], out.detach().numpy()[:, 1], color=\"blue\", s=1)\n",
    "    plt.title(f'Generator points. End of epoch= {epoch+1}', fontsize=10)\n",
    "    plt.axis([-1,1,-1,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = GenModel_2(latent_dim).cuda()\n",
    "netD = DisModel_2().cuda()\n",
    "loss_func = nn.BCELoss().cuda()\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.002)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.001)\n",
    "train(netD, netG, batch_per_epoch, batch_size, latent_dim, 10, loss_func, optD, optG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Гиф-анимация генерируемых точек в процессе обучения (опционально)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Решение:\n",
    "\n",
    "**Предобучить только дискриминатор (сделать его сильным критиком), а потом обучать только генератор**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "netG = GenModel(latent_dim).cuda()\n",
    "netD = DisModel().cuda()\n",
    "loss_func = nn.BCELoss().cuda()\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "        for _ in range(batch_per_epoch):\n",
    "            batchG = get_g_batch(batch_size, latent_dim)\n",
    "            batchD = get_mix_batch(batch_size, latent_dim, netG)\n",
    "            \n",
    "            netD.train(True)\n",
    "            netG.train(False)\n",
    "            netD_step(netD, batchD, loss_func, optD)\n",
    "            \n",
    "            #netD.train(False)\n",
    "            #netG.train(True)\n",
    "            #netG_step(netD, netG, batchG, loss_func, optG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "  for _ in range(batch_per_epoch):\n",
    "    batchG = get_g_batch(batch_size, latent_dim)\n",
    "    batchD = get_mix_batch(batch_size, latent_dim, netG)\n",
    "            \n",
    "    netD.train(False)\n",
    "    netG.train(True)\n",
    "    netG_step(netD, netG, batchG, loss_func, optG)\n",
    "  plot_gen(netG, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. cGAN на датасете Fashion mnist (или MNIST)\n",
    "\n",
    "Напишите полносвязный GAN с условием. Условием в данном случае будет являтся lablel (номер класса) цифры или вещи.\n",
    "\n",
    "1. Сделайте эмбединг для лэйблов внутри модели\n",
    "2. С помощью torch.cat добавьте этот эмбединг ко входу генератора и дискриминатора\n",
    "3. Используйте такие параметры для генератора:\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(?, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "4. Используйте такую архитектуру для дискриминатора:\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(?, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Code here\n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        \n",
    "        # Code here\n",
    "        z = z.view(z.size(0), 100)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        \n",
    "        out = self.model(x)\n",
    "        return out.view(x.size(0), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        # Code here\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(794, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        \n",
    "        # Code here\n",
    "        x = x.view(x.size(0), 28*28)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([x, c], 1)\n",
    "        out = self.model(x)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код ниже желательно не изменять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5), std=(0.5))\n",
    "])\n",
    "\n",
    "dataset = FashionMNIST(root ='/content/',\n",
    "                       train = True,\n",
    "                       transform = transform, \n",
    "                       target_transform = None, \n",
    "                       download = True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_name(num):\n",
    "    \"\"\"Вспомогательная функция возвращающая название класса по его индексу\n",
    "    num - численный индекс класса\"\"\"\n",
    "    class_names = dataset.class_to_idx #{название класса : индекс класса}\n",
    "\n",
    "    key_list = list(class_names.keys())\n",
    "    val_list = list(class_names.values())\n",
    "    \n",
    "    # print key with val 100\n",
    "    position = val_list.index(num)\n",
    "    print(key_list[position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion):\n",
    "    g_optimizer.zero_grad()\n",
    "    z = torch.randn(batch_size, 100).to(device)\n",
    "    fake_labels = torch.LongTensor(np.random.randint(0, 10, batch_size)).to(device)\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    validity = discriminator(fake_images, fake_labels)\n",
    "    g_loss = criterion(validity, torch.ones(batch_size).to(device))\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss\n",
    "\n",
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images, labels):\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # train with real images\n",
    "    real_validity = discriminator(real_images, labels)\n",
    "    real_loss = criterion(real_validity, torch.ones(batch_size).to(device))\n",
    "    # train with fake images\n",
    "    z = torch.randn(batch_size, 100).to(device)\n",
    "    fake_labels = torch.LongTensor(np.random.randint(0, 10, batch_size)).to(device)\n",
    "    fake_images = generator(z, fake_labels)\n",
    "    fake_validity = discriminator(fake_images, fake_labels)\n",
    "    fake_loss = criterion(fake_validity, torch.zeros(batch_size).to(device))\n",
    "\n",
    "    d_loss = real_loss + fake_loss\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    return d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gen_res(generator):\n",
    "    z = torch.randn(9, 100).to(device)\n",
    "    labels = torch.LongTensor(np.arange(9)).to(device)\n",
    "\n",
    "    sample_images = generator(z, labels).unsqueeze(1).data.cpu()\n",
    "\n",
    "    grid = make_grid(sample_images, nrow=3, normalize=True).permute(1,2,0).numpy()\n",
    "    plt.imshow(grid)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = (0.5, 0.999)\n",
    "device='cuda'\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().cuda()\n",
    "lr = 1e-4\n",
    "\n",
    "criterion = nn.BCELoss().cuda()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "n_critic = 5\n",
    "display_step = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    show_gen_res(generator)\n",
    "    print('Starting epoch {}...'.format(epoch))\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        real_images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        generator.train()\n",
    "        batch_size = real_images.size(0)\n",
    "        d_loss = discriminator_train_step(len(real_images), discriminator,\n",
    "                                          generator, d_optimizer, criterion,\n",
    "                                          real_images, labels)\n",
    "        \n",
    "\n",
    "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
    "\n",
    "    generator.eval()\n",
    "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. cDCGAN (Дополнительное)\n",
    "* Решить предыдущую задачу при помощи развёрток\n",
    "* Основная проблема в том, что тут очень тяжело подобрать правильные коэффициенты.\n",
    "* Эту задачу можно решать абсолютно любым способом (вполть до замены всего кода), единственные условия: сеть должна быть построена по архитектуре cDCGAN и генерировать на датасте Fashion mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже пример неправильно подобранных параметров генератора. (Дискриминатор используется такой же как выше). Даже если учить эту сеть очень долго, результаты получаются очень далёкими от хороших."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.deconv1_1 = nn.ConvTranspose2d(100, d*2, 4, 1, 0)\n",
    "        self.deconv1_1_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv1_2 = nn.ConvTranspose2d(10, d*2, 4, 1, 0)\n",
    "        self.deconv1_2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(d*2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(d)\n",
    "        \n",
    "        self.deconv4 = nn.ConvTranspose2d(d, out_channels=1, kernel_size=2, stride=2, padding=2)\n",
    "        \n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        z = z.view(z.size(0), 100, 1, 1)\n",
    "        c=self.label_emb(labels)\n",
    "        c = c.view(c.shape[0],10,1,1)\n",
    "    \n",
    "        x=self.deconv1_1(z)\n",
    "        x = F.relu(self.deconv1_1_bn(x))\n",
    "        y = F.relu(self.deconv1_2_bn(self.deconv1_2(c)))\n",
    "        x = torch.cat([x, y], 1)\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        \n",
    "        x = F.tanh(self.deconv4(x))\n",
    "        \n",
    "        return x.view(x.size(0), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().cuda()\n",
    "lr = 1e-4\n",
    "\n",
    "criterion = nn.BCELoss().cuda()\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "n_critic = 5\n",
    "display_step = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    show_gen_res(generator)\n",
    "    print('Starting epoch {}...'.format(epoch))\n",
    "    for i, (images, labels) in enumerate(data_loader):\n",
    "        real_images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        generator.train()\n",
    "        batch_size = real_images.size(0)\n",
    "        d_loss = discriminator_train_step(len(real_images), discriminator,\n",
    "                                          generator, d_optimizer, criterion,\n",
    "                                          real_images, labels)\n",
    "        \n",
    "\n",
    "        g_loss = generator_train_step(batch_size, discriminator, generator, g_optimizer, criterion)\n",
    "\n",
    "    generator.eval()\n",
    "    print('g_loss: {}, d_loss: {}'.format(g_loss, d_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
