{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Архитектуры CNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Базовые компоненты свёрточных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Мы рассмотрели базовые компоненты, из которых состоят современные свёрточные нейронные сети, а также техники их обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/basic_components_cnn.png\" >\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/basic_components.png\" width=\"760\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом занятии рассмотрим, какие модели для классификации изображений можно построить на основе этих компонент. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet Large Scale Visual Recognition Challenge\n",
    "\n",
    " ([ILSVRC](http://image-net.org/challenges/LSVRC/2017/))\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/imagenet_large_scale_visual_recognition_challenge.png\"  width=\"800\"></center>\n",
    "<center><em>Лучшие результаты конкурса ILSVRC по годам.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageNet - один из самых известных проектов в области распознавания изображений и машинного зрения. Содержит более 14 миллионов вручную размеченных изображений, принадлежащих 1000 классам.  \n",
    "\n",
    "Для загрузки с [официального сайта](http://image-net.org/download) необходимо запрашивать доступ, но можно загрузить данные с [Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/data). Однако архив занимает порядка 156Gb и не поместится на диск Colab. Поэтому воспользуемся другим репозиторием, в котором находится 1000 изображений из оригинального датасета.\n",
    "\n",
    "P.S. Для загрузки данных, которые стали недоступны на официальных сайтах, можно использовать [Academic Torrents](https://academictorrents.com). В частности для [ImageNet](https://academictorrents.com/browse.php?search=imagenet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# fix random_seed\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# compute in cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Full list of labels \n",
    "#'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L09/imagenet_class_index.json\n",
    "\n",
    "# https://github.com/ajschumacher/imagen.git\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L09/imagen.zip\n",
    "!unzip imagen.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили категории:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)\n",
    "\n",
    "with open('imagenet_class_index.json') as f:\n",
    "      imagenet_labels = json.load(f)\n",
    "\n",
    "pp.pprint(dict(list(imagenet_labels.items())[:10])) # Use Pretty Print to display long dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MicroImageNet(Dataset):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Load labels\n",
    "    self.num2id = {}\n",
    "    with open('imagenet_class_index.json') as f:\n",
    "      imagenet_labels = json.load(f)\n",
    "    w_net = {}\n",
    "    # Because not world net all image codes from imagen exists in imagenet_labels \n",
    "    # we need to filter this image\n",
    "    for key in imagenet_labels.keys():\n",
    "      wn_id = imagenet_labels[key][0]\n",
    "      w_net[wn_id] = {'num': int(key), 'name': imagenet_labels[key][1] }\n",
    "    self.labels = []\n",
    "    self.paths = []\n",
    "\n",
    "    # Load data\n",
    "    images = glob('imagen/*.jpg')\n",
    "    images.sort()\n",
    "    for i, path in enumerate(images):\n",
    "      name = path.split(\"_\")[2] # Class name\n",
    "      id = path.split(\"_\")[0][7:] # WorldNet based ID\n",
    "      if w_net.get(id, None): \n",
    "        self.labels.append([w_net[id]['num'], w_net[id]['name'], id])\n",
    "        self.paths.append(path)\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    im = Image.open(self.paths[idx])\n",
    "    class_num = self.labels[idx][0]\n",
    "    return im, class_num\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.paths)\n",
    "\n",
    "\n",
    "microImgNet = MicroImageNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "\n",
    "def show(img,label_1, num, label_2=\"\"):\n",
    "    ax = plt.subplot(2, 3, num+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(label_1)\n",
    "    ax.set_xlabel(label_2)\n",
    "    plt.axis('off')\n",
    "  \n",
    "for i in range(6):\n",
    "  img, label = microImgNet[i*6]\n",
    "  name = microImgNet.labels[i*6][1]\n",
    "  show(img, name, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети AlexNet(2012 г.)\n",
    "\n",
    "[ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky et al., 2012)](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "создатели: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n",
    "**University of Toronto**\n",
    "\n",
    "Как упоминалось на первой лекции, современный бум нейростевых технологий начался в 2012 году, когда AlexNet с большим отрывом от конкурентов победила в ImageNet.\n",
    "\n",
    "В AlexNet есть все компоненты, которые мы рассматривали ранее. Её архитектура состоит из пяти свёрточных слоёв, между которыми располагаются pooling-слои и слои нормализации, а завершают нейросеть три полносвязных слоя.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/alexnet_architecture.png\"  width=\"800\"></center>\n",
    "\n",
    "<center><em>Архитектура сети AlexNet</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Подробнее про AlexNet](https://neurohive.io/ru/vidy-nejrosetej/alexnet-svjortochnaja-nejronnaja-set-dlja-raspoznavanija-izobrazhenij/)\n",
    "\n",
    "Можно заметить, нейросеть состоит из **двух работающих в параллель нейросетей**, которые обмениваются информацией после 2-го и 5-го сверточного слоя и в полносвязных слоях. Это было необходимо, т.к. для обучения использовались **GPU GTX580** с 3 ГБ видеопамяти. Авторы архитектуры использовали две видеокарты, работающие параллельно. На вход нейронной сети подавалось трёхканальное изображение с пространственными размерами $224 \\times 224$ пикселя, к которому применялось $96$ свёрток с ядром $11 \\times 11 \\times 3$ и сдвигом 4. Веса, соответствующие первым 48 свёрткам хранились на первой видеокарте, а оставшиеся 48 свёрток были связаны со второй видеокартой.\n",
    "\n",
    "\n",
    "- **Пространственные размеры** карты признаков резко сжимаются: 224x224 -> 55x55 -> 27x27 -> 13x13 -> 13x13 -> 13x13. \n",
    "\n",
    "- При этом увеличивается **количество сверток** (фильтров) в каждом слое: 96 (48x2) -> 256 (128x2) -> 384 (192x2) -> 256 (128x2). \n",
    "\n",
    "- На выходе нейросети стоят два **полносвязных слоя**, формирующих ответ (в **ImageNet** 1000 классов). \n",
    "\n",
    "**AlexNet** невозможно напрямую использовать для классификации **CIFAR10**. Если так агрессивно уменьшать изображение размером 32×32 px, то в определенный момент в него просто не поместится следующий фильтр, который нужно применить.Можно сделать **resize** изображения с 32x32 до 224x224, но это не самый рациональный способ использования вычислительных ресурсов. \n",
    "\n",
    "**Структура** некоторых (особенно старых) сетей, заточенных под **ImageNet**, напрямую **зависит от размера изображений**. В более современных сетях есть слой [Adaptive Average Pooling](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html), позволяющий решить эту проблему, но о нем мы расскажем чуть позже. \n",
    "\n",
    "Такая архитектура показала прорывную точность: ошибка упала с **20%** до **15.4%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center><em><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/alexnet_architecture_with_parameters.png\"></em></center>\n",
    "<center><em>Архитектура сети AlexNet c параметрами слоев</em></center>\n",
    "\n",
    "\n",
    "\n",
    "Нестандартные решения, принятые разработчиками **AlexNet**:\n",
    "* Использование **ReLu** в качестве функции активации. Предшественники использовали ограниченные функции, типа сигмоиды или тангенса, которые приводили к затуханию градиента.\n",
    "* Использование авторской **нормализации**. Позже было показано, что их нормализация не давала значительного прироста скорости обучения и качества модели. В современных моделях чаще используется Batch-нормализация. \n",
    "* Использование различных **методов предобработки** входных данных, таких как повороты, увеличение, зашумление и т.д. для увеличения разнообразия входных данных. Это называется аугментация и о ней вы узнаете в других лекциях. \n",
    "* Использование **Dropout** = 0.5 (при обучении отключается случайная половина нейронов).\n",
    "* Использование **SGD Momentum** = 0.9. Для обеспечения сходимости авторы использовали эвристику для **понижения скорости обучения** (learning rate)\n",
    "* **Скорость обучения** стартует со значения $10^{-2}$ и уменьшается в 10 раз, когда качество перестает расти.\n",
    "* **L2 регуляризация** с весом $5 \\cdot 10^{-4}$.\n",
    "* Использование **ансамбля** из 7 обученных из различного начального состояния моделей. Результат работы моделей усредняется. Данное решение увеличило точность с 81.8% до 84.6%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним Pytorch реализацию с оригинальной. В чем отличия? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "alexnet = models.alexnet(weights='AlexNet_Weights.DEFAULT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print('AlexNet architecture')\n",
    "print(summary(alexnet, (3, 224, 224), device='cpu'))\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Torch версии:\n",
    "- отсутствуют нормализации, \n",
    "- отсутсвует разделение на две видеокарты, \n",
    "- немного изменено количество сверток на некоторых слоях,\n",
    "- добавлен слой Adaptive Average Pooling.\n",
    "\n",
    "Количество слоев и размеры сверток те же. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как работает: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def img2tensor(img):\n",
    "  t = F.to_tensor(img)\n",
    "  t = F.normalize(t, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "  return t\n",
    "\n",
    "def catId2names(nums):\n",
    "  titles = []\n",
    "  for num in nums:\n",
    "    titles.append(imagenet_labels[str(num.item())][1])\n",
    "    titles.reverse()\n",
    "  return \", \".join(titles)\n",
    "  \n",
    "for i in range(6, 12):\n",
    "  img, label = microImgNet[i*6]\n",
    "  tensor = img2tensor(img)\n",
    "  out = alexnet(tensor.unsqueeze(0)) # Add batch dimension\n",
    "  labels_num = torch.argsort(out[0]) # Ascending order\n",
    "  weights = out[0][-5:]\n",
    "  predicted = catId2names(labels_num[-5:]) # Top 5\n",
    "  titles = []\n",
    "  name =  microImgNet.labels[i*6][1] \n",
    "  show(img, name, i-6, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Архитектура ZFnet(2013 г.)\n",
    "\n",
    "[Visualizing and Understanding Convolutional Networks (Zeiler et al., 2013)](https://arxiv.org/abs/1311.2901)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/zfnet_architecture.png\"  width=\"1000\"><center>\n",
    "<center><em>Архитектура сети ZFNet </em><center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/zfnet_architecture_2013.png\"  width=\"1000\"></center>\n",
    "<center><em>Архитектура сети ZFNet </em></center>\n",
    "\n",
    "*Тюнингованный AlexNet*\n",
    "\n",
    "Нейросеть **ZFnet**, созданная учеными из Йорского университета, В 2013 году выиграла соревнования, достигнув результата 11.7% — в ней **AlexNet** использовалась в качестве основы, но с изменёнными параметрами и слоями.\n",
    "\n",
    "Отличия от **AlexNet** небольшие:\n",
    "* Немного поменялись пространственные размеры фильтров (было 11, стало 7);\n",
    "* Увеличилось общее количество фильтров;\n",
    "\n",
    "Количество слоев и общая структура сети, когда слои свёртки и пулинга перемежаются друг с другом, а затем идут два полносвязных слоя, сохранились.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети VGGNet(2014 г.)\n",
    "[Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan et al., 2014)](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Karen Simonyan and Andrew Zisserman (Visual Geometry Group — **Oxford**)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/vggnet_architecture.png\"  width=\"400\" rotate=\"180\"></center>\n",
    "<center><em>Сравнение архитектур сетей AlexNet, VGG16 (использует 16 слоев) и VGG19 (использует 19 слоев)</em></center>\n",
    "\n",
    "[Краткое описание VGGNet](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 2014-ом году в Оксфорде была разработана модель **VGGNet**.\n",
    "\n",
    "В архитектуре **VGGNet** появляется идея повторения стандартных блоков и увеличения глубины нейросети повторением таких блоков (**stacking**). На момент создания данной архитектуры идея увеличения глубины нейросети для получения лучших результатов не была очевидна. Стандартный блок **VGGNet** состоит из нескольких слоев свертки (от 2 до 4) и max-pooling слоя.\n",
    "\n",
    "Существует несколько вариантов **VGGNet** архитектуры. Самые известные **VGG11**, **VGG16** и **VGG19**. Цифра ставится по количеству слоев с обучаемыми весами: сверточных и полносвязных слоев. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На изображении выше показаны сети **AlexNet** и две версии **VGG16** и **VGG19** с 16 и 19 обучаемыми слоями соответственно. На соревнованиях победила более глубокая **VGG19**, показавшая процент ошибок при классификации top-5 7.3% (у **AlexNet** 15.4%).\n",
    "\n",
    "Интересные факты о **VGGNet**:\n",
    "* данная архитектура получила 2-е место в задаче классификации и 1-е место в задаче локализации (классификация и обводка объекта ограничивающей рамкой) на **ImageNet**.\n",
    "* авторы **VGGNet** отказались от слоев нормализации.\n",
    "* архитектуры **VGG16** или **VGG19** продолжаются применяется для решения ряда прикладных задач. \n",
    "\n",
    "Особенности архитектуры **VGGNet**:\n",
    "* все свёрточные слои имеют фильтры с рецептивным полем размера **3×3**;\n",
    "* сверточные слои  объединены в блоки, состоящие из некоторого количества свёрток с разным (постепенно увеличивающимся от блока к блоку) количеством фильтров;\n",
    "* между блоками располагаются слои пулинга.\n",
    " \n",
    "Появление \"стандартных\" блоков внутри модели — важное нововведение. Идея базового блока внутри сети будет достаточно широко использоваться дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем на коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "vgg = models.vgg16(weights=None) # Change on True if you want to use VGG to predict something\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять смысл использования сверток **3x3** необходимо познакомиться с понятием рецептивного поля и научиться оценивать количество необходимых вычислительных ресурсов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка вычислительных ресурсов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Размер рецептивного поля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Рецептивное поле** - участок входной карты признаков (входного тензора), который при прохождении одного или нескольких слоев нейронной сети формирует один признак на выходе (одно число выходного тензора). **Рецептивное поле** можно назвать “полем зрения”.\n",
    "\n",
    "Интуитивно кажется, что чем больше “поле зрение”, тем лучше обобщающая способность свертки. Авторы **VGG** решили отказаться от свертки **5x5** и заменить ее двумя свертками **3x3**.\n",
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/receptive_field_size.png\"  width=\"850\"></center>\n",
    "<center><em>Две свёртки 3×3 вместо одной 5×5.</em></center>\n",
    "\n",
    "\n",
    "Рассмотрим **рецептивное поле** свертки **5x5** ($\\color{green}{\\text{зеленый квадрат}}$ на A1). Применение свертки **5x5** к этому квадрату даст 1 признак на выходе ($\\color{red}{\\text{красный квадрат}}$ на A3). Применение к этому же **рецептивному полю** свертки **3x3** ($\\color{orange}{\\text{оранжевые квадраты}}$ на A1) даст на **3x3** признака ($\\color{red}{\\text{красный квадрат}}$ на A2), применение второй свертки **3x3** позволит получить 1 признак на выходе ($\\color{red}{\\text{красный квадрат}}$ A3). \n",
    "\n",
    "**Итого:** одна свертка **5x5** имеет то же **рецептивное поле**, что две свертки **3x3**.\n",
    "\n",
    "При этом применение двух сверток **3x3** дает ряд преимуществ:\n",
    "\n",
    "* меньшее количество параметров. При применении свертки **5x5** необходимо обучать $5⋅5⋅C_{in}⋅C_{out} = 25⋅C_{in}⋅C_{out}$  весов, где $С_{in}$ - глубина входной карты признаков, $C_{out}$ - количество фильтров сверточного слоя. При применении двух сверток **3x3** обучается $2⋅(3⋅3⋅C_{in}⋅C_{out}) = 18⋅C_{in}⋅C_{out}$  весов.\n",
    "* между свертками **3x3** добавляется дополнительный **слой активации**, который позволяет формировать более сложные признаки.\n",
    "\n",
    "**2 маленьких фильтра работают как один большой или даже лучше!**\n",
    "\n",
    "Аналогично 3 свертки **3x3** могут заменить одну свертку **7x7**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/receptive_field_size_example.png\"  width=\"850\"></center>\n",
    "<center><em>Три свёртки 3×3 вместо одной 7×7.</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка количества ресурсов для работы модели\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие ресурсы нужны для работы нейронной сети? \n",
    "* Память для хранения промежуточных представлений.\n",
    "* Память для обучаемых параметров.\n",
    "* Вычислительные ресурсы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/weights_memory.png\"  width=\"650\"></center>\n",
    "<center><em>Пример свертки с фильтрами малого размера.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим один сверточной слой (см. картинку выше).\n",
    "\n",
    "$\\color{orange}{\\text{Оранжевый тензор}}$ - карта признаков, поступившая на вход сверточного слоя. Параметры $H_{in}$, $W_{in}$, $C_{in}$ - длина, ширина и глубина (число каналов изображения или фильтров в предыдущем сверточном слое) входного тензора. Данный тензор не учитывается в затрачиваемых ресурсах, т.к. информация сохранена в предыдущем слое.\n",
    "\n",
    "**Память для хранения промежуточных представлений** определяется **количеством элементов на выходе слоя** ($\\color{blue}{\\text{синий тензор}}$). Они хранятся для вычисления следующих слоев и градиентов. Необходимая память рассчитывается как:\n",
    "\n",
    "<center> $H_{out}⋅W_{out}⋅C_{out}⋅n_{byte}$,</center>\n",
    "\n",
    "где $H_{out}$, $W_{out}$, $C_{out}$ - длина, ширина и глубина (число фильтров в свертке) выходной карты признаков, а $n_{byte}$ - количество байт для хранения одного элемента (4 для float 32).\n",
    "\n",
    "**Память для обучаемых параметров** определяется **весами и смещением** (bias) фильтров свертки ($\\color{green}{\\text{зеленые тензоры}}$). Необходимая память рассчитывается как:\n",
    "\n",
    "<center> $(K_h⋅K_w⋅C_{in}⋅C_{out} + С_{out})⋅n_{byte}$,</center>\n",
    "\n",
    "где $K_h$, $K_w$, $C_{in}$ - длина, ширина и глубина одного фильтра, $C_{out}$ - число фильтров в свертке.\n",
    "\n",
    "Для оценки необходимых **вычислительных ресурсов** посчитаем **количество операций** сложения и умножения при прямом проходе. Каждое число выходного тензора является результатом применения фильтра свертки к некоторому рецептивному полю входного тензора. Количество операций можно оценить, как произведение количества элементов на выходе слоя на размер одного фильтра: \n",
    "\n",
    "<center> $(H_{out}⋅W_{out}⋅C_{out})⋅(K_h⋅K_w⋅C_{in})$.</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценка памяти для хранения параметров слоя:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "conv_sizes = [11, 7, 5, 3]\n",
    "\n",
    "for conv_size in conv_sizes:\n",
    "    conv_layer = nn.Conv2d(3, 64, conv_size, stride=1, padding=1)\n",
    "    print('Convolution size: %ix%i' % (conv_size, conv_size))\n",
    "    for tag, p in conv_layer.named_parameters():\n",
    "        print('Memory reqired for %s: %.2f kb' % \n",
    "              (tag, (np.prod(p.shape) * 4) / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценка количества вычислительных операций (для одной свертки):**\n",
    "\n",
    "**11x11:** (224 * 224 * 64) * (3 * 11 * 11) = 9 633 792 * 11 * 11 =  9 633 792 * 121  = 1 165 688 832\n",
    "\n",
    "**7x7:** (224 * 224 * 64) * (3 * 7 * 7) = 9 633 792 * 7 * 7 = 9 633 792 * 49 = 472 055 808\n",
    "\n",
    "121/49  = ~2.5 раза меньше чем **11x11**\n",
    "\n",
    "**5x5:** (224 * 224 * 64) * (3 * 5 * 5) = 9 633 792 * 5 * 5 = 9 633 792 * 25 = 240 844 800\n",
    "\n",
    "49/25 = ~2 раза меньше чем **7x7**\n",
    "\n",
    "**3x3:** (224 * 224 * 64) * (3 * 3 * 3) = 9 633 792 * 3 * 3 = 9 633 792 * 9 = 86 704 128\n",
    "\n",
    "25/9 = ~2.7 раза меньше чем **5x5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сколько памяти и параметров VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/memory_and_parameters_vgg16.png\"  width=\"800\"></center>\n",
    "<center><em>Архитектура VGG16 с параметрами слоев </em></center>\n",
    "\n",
    "Благодаря такой экономии получилось сделать большую по тем временам сеть (16 слоев).  Тем не менее, несмотря на применённые способы уменьшения вычислительной сложности и снижение числа параметров, сеть все равно получилась огромной (16-слойная версия сети VGG расходует в 25 раз больше дорогой памяти GPU, нежели AlexNet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/compare_parameters_alexnet_and_vgg16.png\"  width=\"900\"></center>\n",
    "<center><em>Сравнение параметров AlexNet и VGG16.</em></center>\n",
    "\n",
    "* Основная часть памяти расходуется на большие свёртки в начальных слоях, где пространственные размеры (ширина и высота) велики;\n",
    "\n",
    "* Больше всего весов в полносвязанных слоях;\n",
    "\n",
    "* Вычислительные ресурсы нужны в первую очередь для сверток.\n",
    "\n",
    "\n",
    "VGG-16 получилась существенно больше по сравнению с и так довольно объемной AlexNet, и тем более по сравнению с современными моделями.\n",
    "\n",
    "В значительной степени с этим связано дальнейшее направление развития моделей. В следующем году ImageNet выиграла сеть под названием GoogLeNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Оценка ресурсов GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы убедиться самостоятельно в размерах сетей, установим библиотеку для мониторинга ресурсов GPU: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil as GPU\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def gpu_usage():\n",
    "    GPUs = GPU.getGPUs()\n",
    "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
    "    if len(GPUs) == 0:\n",
    "        return False\n",
    "    gpu = GPUs[0]\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"GPU RAM Free: {gpu.memoryFree:.0f}MB \\\n",
    "    | Used: {gpu.memoryUsed:.0f}MB \\\n",
    "    | Util {gpu.memoryUtil*100:3.0f}% \\\n",
    "    | Total {gpu.memoryTotal:.0f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько памяти потребуется **VGG19** и какого размера batch можно использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg19 = torchvision.models.vgg19(weights=None, progress=True)\n",
    "vgg19.requires_grad=True\n",
    "vgg19.to(device)\n",
    "\n",
    "!nvidia-smi # Common GPU info\n",
    "\n",
    "vgg19.train()\n",
    "\n",
    "for batch_size in [1, 8, 16, 32, 64]:\n",
    "  input_random = torch.rand(batch_size, 3, 224, 224, device=device)\n",
    "  out = vgg19(input_random)\n",
    "  print(\"Batch size\", batch_size)\n",
    "  gpu_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистка памяти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()\n",
    " !nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_random = None #del input \n",
    "out = None #del out\n",
    "\n",
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети GoogLeNet(2014 г.)\n",
    "\n",
    "[Going Deeper with Convolutions (Szegedy et al., 2014)](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "В отличие от предыдущих нейросетей **GoolLeNet** разработана в коммерческой компании с целью практического применения, поэтому основной упор был сделан на эффективности.\n",
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/inception_module_googlenet.png\"  width=\"400\"></center>\n",
    "<center><em>Inception module </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/googlenet_architecture.png\"  width=\"900\"></center>\n",
    "<center><em>Архитектура GoogLeNet </em></center>\n",
    "\n",
    "**GoogLeNet** (на самом деле архитектура называется **Inception**, а **GoogLeNet** это имя команды в соревновании  [**ILSVRC14**](https://www.image-net.org/challenges/LSVRC/2014/), названной так в честь нейронной сети Яна Лекуна **LeNet 5**) — ещё более глубокая архитектура с 22 слоями. Она содержит менее 7 миллионов параметров — в 9 раз меньше, чем у **AlexNet** и 20 раз меньше **VGG19**. При этом сеть оказалась немного более точной, чем **VGG19**: ошибка снизилась с 7.3% до 6.7%.\n",
    "\n",
    "Рассмотрим, за счёт чего удалось достичь такого огромного выигрыша в ресурсах, так как многие идеи, которые впервые были использованы для **GoogLeNet**, активно применяются до сих пор.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Inception module\n",
    "Составной блок **GoogLeNet** называется **Inception module**. Архитектура **GoogLeNet** состоит из множества таких блоков, следующих друг за другом.\n",
    "\n",
    "Идея **Inception module** состоит в том, чтобы производить параллельные вычисления сверток с различным размером перцептивного поля и Max Pooling, конкатенируя (объединяя, а не складывая) полученные результаты. Это позволяет выделять признаки разного размера и сложности. \n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/naive_inception_module.png\"  width=\"750\"></center>\n",
    "<center><em>Наивная реализация Inception module </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке выше представлена наивная реализация **Inception module**. Данная реализация имеет очень важный недостаток: ** увеличение глубины карты признаков**. За счет конкатенации выходов **сверток** и **Max Pooling** из 256 каналов на входе мы получаем 672 канала на выходе. Количество каналов изображения увеличилось более чем в 2.6 раза. За 9 таких блоков глубина увеличится более чем в 5000 раз! \n",
    "\n",
    "Такое решение плохо совместимо с экономией ресурсов.\n",
    " \n",
    "Стоит также заметить, что слой **Max Pooling** в данной архитектуре имеет шаг 1 и не изменяет пространственные размеры карты признаков. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "#https://pytorch.org/vision/stable/_modules/torchvision/models/googlenet.html#googlenet\n",
    "#https://hackmd.io/@bouteille/Bk-61Fo8U\n",
    "googlenet = torchvision.models.googlenet(init_weights=True)\n",
    "print(googlenet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case Study: GoogLeNet**\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/naive_inception_module_with_dimension_reduction.png\"  width=\"850\"></center>\n",
    "<center><em>Наивные реализации Inception module и Inception module с уменьшением размерности</em></center>\n",
    "\n",
    "Как мы уже показали, при использовании наивной реализации **Inception module** количество фильтров возрастает от слоя к слою. \n",
    "\n",
    "Чтобы этого избежать, введены так называемые **«бутылочные горлышки»** — слои с фильтром **1×1**, уменьшающие глубину изображения. Благодаря им удалось достичь того, чтобы количество каналов на входе и на выходе либо не менялось, либо менялось только в моменты, когда это необходимо.\n",
    "\n",
    "Интересно, что слой свертки **1x1** ставится после слоя **Max Pooling**. Это позволяет более эффективно преобразовывать признаки. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 Convolution\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/1x1_convolution.png\"  width=\"550\"></center>\n",
    "<center><em>Уменьшение глубины изображения с помощью 32 фильтров 1×1.</em></center>\n",
    "\n",
    "Свёртку **1х1** можно сравнить с линейным слоем полносвязанной нейронной сети. Мы берем вектор из карты признаков (столбик 1x1x64 на картинке) и домножаем на матрицу весов (одну для всех векторов, в данном случае матрица будет 64x32, т.к. она составлена из 32 фильтров с размерами 1x1x64), чтобы получить вектор на выходе (столбик 1x1x32 на картинке).\n",
    "\n",
    "Это одновременно формирует более сложные признаки, собирая информацию с различных сверток и **Max Pooling**, и позволяет сократить их количество. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/naive_inception_module_with_dimension_reduction_parameters.png\"  width=\"700\"></center>\n",
    "<center><em> Реализация Inception module с уменьшением размерности </em></center>\n",
    "\n",
    "\n",
    "Количество параметров уменьшается в два с лишним раза по сравнению с наивной реализацией. Сеть получается значительно экономичнее.\n",
    "\n",
    "Использование таких модулей и отсутствие полносвязных слоёв делают GoogleNet очень эффективной и достаточно точной сетью. Но это далеко не все нововведения, которые появились в этой модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Stem network\"\n",
    "\n",
    "В составе **GoogLeNet** есть небольшая подсеть — **Stem Network**. Она состоит из трёх свёрточных слоёв (певый с большим фильтом) с двумя pooling-слоями и располагается в самом начале архитектуры. Цель этой подсети быстро и сильно уменьшить пространственные размеры (сжать изображение перед параллельной обработкой), чтобы минимизировать количество элементов в слоях.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/stem_network.png\"  width=\"1200\"></center>\n",
    "<center><em>Архитектура GoogLeNet. В первых слоях быстро уменьшаются пространственные размеры. </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Average Pooling\n",
    "\n",
    "Отдельного внимания заслуживает завершающая часть сети. В **AlexNet** и **VGGNet** мы привыкли видеть в конце сети вытягивание карты признаков в вектор и два полносвязных слоя. В **GoogLeNet** один из полносвязных слоев заменен на \n",
    "**Global Average Pooling**. Данный слой экономичнее полносвязного. Разберемся почему.\n",
    "\n",
    "[Network In Network, Lin et al., 2013](https://arxiv.org/abs/1312.4400)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/global_average_pooling.png\"  width=\"570\"></center>\n",
    "<center><em>Global Average Pooling</em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про Global Average Pooling](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/)\n",
    "\n",
    "На выходе последнего **Inception module** формируется карта признаков **7x7x1024**. Если вытянуть их в линейный слой получится более 50 тысяч признаков.\n",
    "\n",
    "Идея слоя **Global Average Pooling** (GAP) в том, что все пространственные размеры, какими бы они не были (например, 7х7, как в **GoogLeNet** или 6x6, как на картинке), сворачиваются в 1x1.\n",
    "\n",
    "Мы берем среднее значение независимо по каждому каналу полученной карты признаков. \n",
    "\n",
    "Ранее считалось, что применение **Global Average Pooling** в составе архитектуры сверточной нейронной сети (CNN), то есть осуществление поканального усреднения пространственных измерений тензора, приведёт к полной потере пространственной информации о переданном сети объекте. Тем не менее, последние исследования ([Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs (Islam et al., 2021)](https://arxiv.org/abs/2108.07884)) показывают, что после такого преобразования часть пространственной информации всё же сохраняется. Не смотря на то что семантическая информация (например информация о точных границах объекта) очевидно полностью утрачивается после Global Average Pooling, информация об абсолютная положении объекта на исходном изображении сохраняется и оказывается закодированной порядком следования компонент в оставшимся векторе усреднённых фильтров.\n",
    "\n",
    "Применяя **GAP** мы сократили количество признаков в 49 раз! Кроме того **Global Average Pooling** уменьшает переобучение, т.к. мы избавляемся от влияния менее важных признаков.\n",
    "\n",
    "Итого плюсы **GAP**:\n",
    "\n",
    "* Меньше весов;\n",
    "* Независимость от размера входа;\n",
    "* Регуляризация;\n",
    "* Уменьшение числа параметров;\n",
    "* Положительно влияет на переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def file2tensor(filename):\n",
    "  img = Image.open(filename)\n",
    "  t = torchvision.transforms.functional.to_tensor(img)\n",
    "  t = torchvision.transforms.functional.normalize(t, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "  return t\n",
    "\n",
    "class CNNfromHW(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_module=None):\n",
    "        super().__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, padding=2) # 16xHxW\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 16 x H/2 x W/2 \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1) # 32 x H/2 x W/2\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1)) # Any spatial size -> 32x1x1\n",
    "        self.fc = nn.Linear(32, 10)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape\", x.shape)\n",
    "        x = self.conv1(x) # 16xHxW\n",
    "        x = self.pool(x) # 16 x H/2 x W/2 \n",
    "        x = self.conv2(x) # 32 x H/2 x W/2\n",
    "        x = self.activation(x) # Any spatial size -> 32x1x1\n",
    "        x = self.gap(x)\n",
    "        scores = self.fc(x.flatten(1))\n",
    "        print(\"Output shape\", scores.shape)\n",
    "        return scores\n",
    "\n",
    "print(\"CIFAR10 like\")\n",
    "input_random = torch.rand(1, 3, 32, 32)\n",
    "model_with_gap = CNNfromHW()\n",
    "out = model_with_gap(input_random)\n",
    "\n",
    "\n",
    "print(\"Arbitrary size\")\n",
    "# Different sizes work too!\n",
    "aramdillo_t = file2tensor('imagen/n02454379_10511_armadillo.jpg')\n",
    "out = model_with_gap(aramdillo_t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Затухание градиента\n",
    "\n",
    "**GoogLeNet: дополнительный классификатор**\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/googlenet_additional_classificator.png\"  width=\"700\"></center>\n",
    "<center><em>Архитектура GoogLeNet. Отдельно вынесен блок - дополнительный классификатор.</em></center>\n",
    "\n",
    "Помимо основного классификатора на выходе сети добавлены два дополнительных классификатора, встроенных в промежуточные слои. Они понадобились для того, чтобы улучшить обратное распространение градиента, потому что без батч-нормализации в таких глубоких сетях градиент очень быстро затухал, и обучить сеть такого размера было серьёзной проблемой.\n",
    "\n",
    "Обучение VGG осуществлялось непростым способом: сначала обучали 7 слоев, затем добавляли туда следующие и обучали это вручную. Без использования батч-нормализации вряд ли получится повторить результат.\n",
    "\n",
    "Google подошел более системно, добавив дополнительные выходы, которые способствовали тому, чтобы градиент меньше затухал. Благодаря этому удалось решить серьёзную на тот момент проблему, которая ограничивала возможность обучения глубоких моделей.\n",
    "Статья про батч-нормализацию появилась как раз в 15ом году, видимо уже после выхода этой модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Появление \"глубоких\" моделей (deep models)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/deep_models.png\"  width=\"750\"></center>\n",
    "<center><em>Победители ImageNet Large Scale Visual Recognition Challenge (ILSVRC).</em></center>\n",
    "\n",
    "В 2015 году произошла революция в глубине нейронных сетей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети ResNet(2015 г.)\n",
    "\n",
    "\n",
    "[Deep Residual Learning for Image Recognition (He et al., 2015)](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/resnet_20_and_56_layers.png\"  width=\"550\"></center>\n",
    "<center><em>График ошибки на обучающей и текстовой выборках двух модификаций  \n",
    "VGGNet с 20 и 56 слоями соответственно в ходе процесса обучения. </em></center>\n",
    "\n",
    "Проблема увеличения глубины нейронных сетей была связана не только с отсутствием батч-нормализации. На слайде выше сравнение графиков обучения 56-слойной и 20-слойной сети Microsoft, построенной на принципах VGGNet. Как видно из графиков, у 56-слойной сети и на тренировочном, и на тестовом датасете ошибка больше, чем у 20-слойной. Казалось бы, сеть, состоящая из большего количества слоёв должна работать как минимум не хуже, чем сеть меньшего размера. К сожалению, проблема **затухание градиента** в этом случае не позволяет эффективно обучить более глубокую сеть.\n",
    "\n",
    "В 2015 году соревнования выиграла сеть **ResNet**, архитектура которой предлагала новый подход к решению проблемы обучения глубоких сетей. Она состояла из 152 слоёв и снизила процент ошибок до 3,57%. Это сделало её почти в два раза эффективнее **GoogLeNet**.\n",
    "\n",
    "При обучении нейронной сети методом обратного распространения ошибки модуль градиента постепенно уменьшается, проходя через каждый из слоёв сети. В глубоких сетях \"длина\" такого пути оказывается достаточной, чтобы модуль градиента стал мал и процесс обучения фактически остановился. В архитектуре **ResNet** такая проблема решается отказом от простого последовательного соединения слоёв (stacking) в пользу создания дополнительных связей в вычислительном графе нейронной сети, через которые градиент смог бы распространятся минуя свёрточные слои и таким образом не затухая. \n",
    "\n",
    "Сеть архитектуры **ResNet** состоит из набора так называемых **Residual Block**-ов. В данном блоке тензор входных признаков пропускаются через пару последовательно соединённых свёрточных слоёв, после чего полученный результат сказывается поканально с этим же неизменённым входным тензором. Свёрточные слои в таком блоке аппроксимируют не саму функциональную зависимость между входным и выходным тензором, а разность (анг. residual) между такой искомой зависимостью и тождественным преобразованием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resudial connection\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/resudial_connection.png\"  width=\"700\"></center>\n",
    "<center><em>Сравнение Residual Block с \"обычными\" слоями</em></center>\n",
    "\n",
    "Идея **Residual Block** состоит в уточнении набора признаков на каждом блоке. Вместо того, чтобы перезаписывать признаки, мы добавляем к выходу предыдущего блока уточнение, сформированное на этом блоке.  \n",
    "\n",
    "Бонусы **Residual Block**:\n",
    "- канал суммирования позволяет градиенту легко распространяться без затухания,\n",
    "- не теряется информация о важных признаках, выделенных на предыдущем слое.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура ResNet\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/resnet_architecture.png\"  width=\"730\"></center>\n",
    "<center><em>Архитектура ResNet. Используется стаккинг residual blocks, каждый residual block состоит из двух 3x3 conv слоев, переодически количество фильтров в свертках удваивается, а размер выходных слоев уменьшается в 2 раза (за счет stride = 2). Добавился дополнительный conv слой в начале сети. Отсутствуют FC слои (кроме последнего который предсказывает класс).</em></center>\n",
    "\n",
    "Из таких блоков можно построить очень глубокую сеть (были эксперименты из 1000 слоев). Для решения конкретной задачи победы на **ImageNet** хватило 150 слоев (добавление большего количества блоков уже не давало прироста точности).\n",
    "\n",
    "В **ResNet** используются многие идеи, которые присутствовали в предыдущих моделях: \n",
    "- вначале изображение резко уменьшается,\n",
    "- дальше используются блоки 3х3, как в **VGGNet**, \n",
    "- применяется **Average Pooling** вместо полносвязного слоя. \n",
    "\n",
    "Блоки состоят из конструкций, изображенных выше: \n",
    "- две свёртки 3Х3,\n",
    "- batch normalization после каждой свертки,\n",
    "- функции активации ReLU,\n",
    "- прибавление результата предыдущего слоя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet: bottleneck layer\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/resnet_bottleneck_layer.png\"  width=\"450\"></center>\n",
    "<center><em>В более глубоких сетях (ResNet-50+) для повышения эффективности используется слой \"bottleneck\" (похожим образом, что и в GoogLeNet)</em></center>\n",
    "\n",
    "Стоит отметить, что сеть **ResNet-152** имеет существенно меньше обучаемых параметров, чем **VGG-19** (58 миллионов против 144 миллионов).\n",
    "\n",
    "Это достигнуто за счёт того, что в более глубоких сетях помимо упомянутых Residual Block-ов с двумя свёрточными слоями с ядром 3х3 применялся более эффективный блок - **bottleneck**, состоящий из:\n",
    "- свёртки 1х1 с уменьшением количества фильтров,\n",
    "- свертки 3x3 с маленьким количеством фильтров,\n",
    "- свертки 1x1 с восстанавливаем количества фильтров до начальных значений, чтобы к ним можно было прибавить вход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet реализация в Pytorch\n",
    "\n",
    "Код для базового блока и для **bottleneck** для сетей с количеством слоев до 50.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BasicBlock\n",
    "import inspect\n",
    "import torchvision.models.resnet as resnet\n",
    "\n",
    "code = inspect.getsource(resnet.BasicBlock.forward)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck\n",
    "\n",
    "code = inspect.getsource(resnet.Bottleneck.forward)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так выглядит в коде этот базовый блок для сетей со слоями до 50 слоёв: свёртка, батч-нормализация, активация, свёртка, батч-нормализация. \n",
    "\n",
    "Если некоторый параметр (downsample) задан, то вызывается **downsample**: это свёртка 1x1 с шагом, соответствующим уменьшению размерности, и батч-нормализация.\n",
    "\n",
    "\n",
    "[Модель ResNet на Torchvision](https://pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#resnet18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet = models.resnet18(weights=None)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Обучение ResNet\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/resnet_18_and_34_layers.png\"  width=\"550\"></center>\n",
    "<center><em>График изменения ошибки при обучении двух модификаций ResNet с 20 и 56 слоями соответственно</em></center>\n",
    "\n",
    "При обучении **ResNet** шаг обучения понижали вручную, когда точность выходила на плато.\n",
    "\n",
    "Помимо того, что **ResNet** c огромным отрывом выиграла ImageNet у моделей прошлого года, она стала первой моделью превысившей точность человеческой разметки. Решения на базе этой архитектуры также стали победителями на соревнованиях по детектированию и сегментации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети ResNeXt(2016 г.)\n",
    "\n",
    "[2016 Aggregated Residual Transformations for Deep Neural Networks (Xie et al., 2016)](https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/compare_resnet_and_resnext_blocks.png\"  width=\"550\"></center>\n",
    "<center><em>Сравнение блоков ResNet и ResNeXt. ResNeXt создан тем же коллективом авторов, что и ResNet. Увеличение ширины residual block достигается путем использования параллельных веток \"cardinality\"), которые по духу схожи с Inception module. </em></center>\n",
    "\n",
    "Следующая модель — **ResNeXt**. Эта сеть выиграла **ImageNet** в следующем году. Идея немного напоминает блок **Inception** в **GoogLeNet**: обрабатывать не сразу все каналы, а распараллелить обработку на несколько групп. \n",
    "\n",
    "Подобное было еще в **AlexNet**, но это делалось вынужденно, потому что **AlexNet** обучали на двух видеокартах параллельно (модель не помещалась в память одной видеокарты на тот момент)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnext = models.resnext50_32x4d(weights=None)\n",
    "print(resnext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupped Convolution\n",
    "\n",
    "[Блог-пост про различные типы сверток](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/groupped_convolution.png\"  width=\"600\"></center>\n",
    "<center><em> Groupped Convolution с двумя группами сверток. Групповые свертки позволяют обрабатывать входные данные параллельно. В качестве бонуса - уменьшение числа параметров, и дополнительная регуляризация.</em></center>\n",
    "\n",
    "На картинке выше карта входная карта признаков разбивается на две группы по каналам. Над каждой группой свертки вычисляются независимо, а потом конкатенируются. Это позволяет заменить $D_{out}$ фильтров $H_{in}\\times H_{in}\\times D_{in}$ на $D_{out}$ фильтров $H_{in}\\times H_{in}\\times D_{in}/2$, сократив количество обучаемых весов в 2 раза. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped convolution in Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможность вычисления груповой свертки в PyTorch заложенна в привычную нам функцию свертки `nn.Conv2d`. Количество групп задается параметром `groups`, значение которого по умолчанию равно 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним время, которое требуется на обычную свертку с `groups = 1` и `groups = 64`. При вычислениях на CPU и GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU test\n",
    "from torch import nn\n",
    "import time \n",
    "import torch\n",
    "\n",
    "def time_synchronized():\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    return time.time()\n",
    "\n",
    "input_random = torch.rand(8, 512, 112, 112)\n",
    "start = time_synchronized() \n",
    "normal_conv = nn.Conv2d(512, 1024, 3, groups=1)\n",
    "out = normal_conv(input_random)\n",
    "tm = time_synchronized() - start\n",
    "print(f\"Normal convolution take  {tm} sec.\")\n",
    "\n",
    "start = time_synchronized() \n",
    "groupped_conv = nn.Conv2d(512, 1024, 3, groups=64)\n",
    "out = groupped_conv(input_random)\n",
    "tm = time_synchronized() - start\n",
    "print(f\"Groupped convolution take  {tm} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU test\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "start = time_synchronized() \n",
    "normal_conv = nn.Conv2d(512, 1024, 3, groups=1).to(device)\n",
    "out = normal_conv(input_random.to(device))\n",
    "tm = time_synchronized() - start\n",
    "print(f\"Normal convolution take  {tm} sec.\")\n",
    "\n",
    "start = time_synchronized() \n",
    "groupped_conv = nn.Conv2d(512, 1024, 3, groups=64).to(device)\n",
    "out = groupped_conv(input_random.to(device))\n",
    "tm = time_synchronized() - start\n",
    "print(f\"Groupped convolution take  {tm} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применение групповой свертки со значением 64 дает выигрыш во времени в 21 на CPU и в 41 на GPU. Это связано с уменьшением количества вычислительных операций.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим память:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_random = None\n",
    "out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext, Inception, grouped conv\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/resnext_inception_grouped_convolution.png\"  width=\"850\">\n",
    "\n",
    "[Блог-пост про ResNeXt](https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac)\n",
    "\n",
    "Альтернативой для блока **ResNext** (рисунок слева) является блок **Inception-ResNext** (рисунок посередине), объединивший идеи **ResNext** блока и **Inception** блока. Применение таких блоков вместо **ResNext** позволяет немного улучшить результаты **ResNext**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/feature_extraction.png\"  width=\"650\">\n",
    "\n",
    "В процессе выполнения задачи классификации картинок мы получаем карты признаков, которые отвечают за распознавание разных особенностей и паттернов на изображениях. Эту карту признаков мы используем для решения задачи классификации. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/feature_extraction_add_x.png\"  width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти же признаки можно использовать для решения других задач. Для этого можно заменить финальные слои нейросети:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/feature_extraction_backbone.png\"  width=\"650\">\n",
    "\n",
    "\n",
    "На основании карт признаков можно проводить детектирование объектов, сегментацию, генерировать вектора признаков (embedding) для разных задач: сравнения изображений, распознавания лиц, трекинга и т.д.\n",
    "\n",
    "Часть сети (слева) является достаточно универсальной.\n",
    "\n",
    "Можно обучить веса, обучая сеть классифицировать изображения, а потом использовать предобученный фрагмент для вычисления карты признаков для любой другой задачи.\n",
    "\n",
    "Часть нейросети, вычисляющая карту признаков, называется **backbone** (скелет, основа).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение моделей\n",
    "\n",
    "[Benchmark Analysis of Representative\n",
    "Deep Neural Network Architectures (Bianco et al., 2018)](https://arxiv.org/abs/1810.00736)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/compare_models_accuracy_vs_parameters.png\"  width=\"800\"></center>\n",
    "<center><em>Сравнение моделей по параметрам Точность vs Количество операций (Bianco et al., 2018)</em></center>\n",
    "\n",
    "Картинка из статьи 2018 года (не самая свежая), но она позволяет визуально сравнить модели по трем параметрам :\n",
    "- размер модели (размер кружка),\n",
    "- количество необходимых для обучения вычислительных операций (ось x)\n",
    "- точность на задаче top-1 (ось y).\n",
    "\n",
    "Здесь можно увидеть, что **VGGNet** — огромные по объёму модели, но по нынешним меркам они обладают средней точностью. Они требуют больших вычислительных ресурсов, поэтому сейчас их имеет смысл использовать разве что в учебных целях, а модели на базе **ResNet** (**ResNet-50**, **ResNet-152**) довольно хороши: в плане точности какого-то большого отрыва от них здесь не видно. Но, тем не менее, есть модели, которые работают чуть лучше. Рассмотрим их кратко, чтобы было понимание того, куда двигалась мысль в этой области."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети DenseNet(2016 г.)\n",
    "\n",
    "[Densely Connected Convolutional Networks (Huang et al., 2016)](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/densenet_architecture.png\"  width=\"350\"></center>\n",
    "<center><em>Архитектура DenseNet. Используется несколько \"обходов\" и конкатенация вместо суммы. </em></center>\n",
    "\n",
    "Еще один вариант, что можно сделать со слоями - добавить дополнительные связи в обход сверточных блоков, чтобы градиент проходил ещё лучше.\n",
    "\n",
    "Можно также заменить сумму на конкатенацию. Чтобы не увеличить при этому глубину карт признаков, можно использовать свёртки **1x1**. \n",
    "\n",
    "На этих двух принципах построен **DenseNet**. С точки зрения ресурсов он чуть более требовательный, чем базовый **ResNet** и немного более точный. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "densenet = models.densenet121(weights=None)\n",
    "print(densenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 WideResNet\n",
    "\n",
    "[Wide Residual Networks (Zagoruyko et al., 2016)](https://arxiv.org/abs/1605.07146)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/compare_basic_and_wide_residual_blocks.png\"  width=\"350\"></center>\n",
    "<center><em>Сравнение базового ResBlock и Широкого ResBlock, где используются $F \\times k$ фильтры вместо $F$ фильтров в каждом слое </em></center>\n",
    "\n",
    "Ещё одна идея связана с увеличением количества фильтров: мы можем увеличивать количество фильтров и уменьшить количество слоев. В такого рода моделях первая цифра — это количество слоёв, вторая — коэффициент, с которым мы увеличиваем количество наших фильтров.\n",
    "\n",
    "Авторы утверждают, ширина (количество фильтров) **residuals** блока  значительно более важный фактор, чем глубина нейронной сети. 50 слойный **Wide ResNet** показывает лучшие результаты, чем оригинальный 152-х слойный **ResNet**. С точки зрения вычислительных ресурсов использование ширины вместо глубины позволяет более эффективно параллелить вычисления на GPU (parallelizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура SENet(2017 г.)\n",
    "\n",
    "[Squeeze-and-Excitation Networks (Hu et al., 2017)](https://arxiv.org/abs/1709.01507)\n",
    "\n",
    "[Блог-пост про Squeeze-and-Excitation Networks](https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7)\n",
    "\n",
    "Основным элементом CNN являются фильтры, которые детектируют группы пикселей, обладающих локальной связностью (рецептивное поле). Сверточный фильтр объединяет:\n",
    "- _пространственную информацию_ (_spatial-wise information_) &mdash; информацию о взаимном расположении пикселей друг относительно друга,\n",
    "- _канальную информацию_ (_channel-wise information_)  &mdash; информацию о взаимоотношениях различных каналов внутри одного локального рецептивного поля.\n",
    "\n",
    "Он не отделяет эти два типа информации друг от друга. За счет чередования сверточных слоев и операций субдискретизации (**pooling**) CNN способны получать представления изображений (**image representation**), которые распознают сложные иерархические паттерны.\n",
    "\n",
    "Архитектуры сверточных нейронных сетей, рассмотренные нами до этого, концентрировались на поиске лучшего представления изображения за счет улучшения способов поиска зависимости между признаками в пространстве (**Inception module**, **Residual block** и т.д.), не затрагивая отношения между каналами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SENet (Squeeze-and-Excitation Networks)__ &mdash; архитектура нейронной сети, одержавшая победу в **ILSVRS-2017**. Создатели **SENet** предложили новую архитектуру блока, называемую _Squeeze-and-Excitation (**SE-блок**), целью которой является поиск лучшего представления изображения за счет моделирования взаимодействия между каналами. Идея состоит в том, что не все каналы одинаково важны, поэтому мы можем выборочно выделять из них более информативные и подавлять менее информативные, создав механизм взвешивания каналов (feature recalibration).  **SE-блок** состоит из следующих процессов:\n",
    "\n",
    "1. \"Сжатие\" (**squeeze**) каждого канала до единственного числового значения с использованием global pooling. Эта процедура позволяет получить некое глобальное представление результата обработки исходного изображения, сделанного каждым из сверточных фильтров, (__global information embedding__).\n",
    "\n",
    "2. \"Возбуждение\" (**excitation**) использует информацию, полученную на этапе \"сжатия\", для определения взаимодействий между каналами. Для этого используются два полносвязных слоя, первый из которых вводит \"узкое место\" (**bottleneck**), уменьшающее размерность в соответсвии с параметром сжатия r, а второй восстанавливает размерность до исходной. В результате этой операции получается набор активаций, использующийся для взвешивания соответствующих каналов исходного изображения (__adaptive recalibration__).\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/senet_architecture.png\"  width=\"1000\"></center>\n",
    "<center><em>Архитектура сети SENet. Добавлен модуль для взвешивания признаков, используется GAP + 2 FC слоя. Победитель ILSVRC`17 с использованием ResNeXt-152 в качестве базовой архитектуры.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, **SE-блок** использует механизм, идейно напоминающий **self-attention** для каналов, чьи отношения не ограничены локальным рецептивным полем соответствующих сверточных фильтров.\n",
    "\n",
    "Описанный **SE-блок** может быть интегрирован в современные архитектуры сверточных нейронных сетей, например, входя в состав остаточного блока сети **ResNet** или **Inception** модуля, как изображено на рисунке.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/senet_inception_module.png\"  width=\"1000\"></center>\n",
    "<center><em>Интеграция SE блока в современные архитектуры сверточных нейронных сетей </em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom SE block\n",
    "\n",
    "class SE_Block(nn.Module):\n",
    "    \"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4\"\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(c, c // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c // r, c, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети MobileNet(2017 г.)\n",
    "\n",
    "[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard et al., 2017)](https://arxiv.org/abs/1704.04861)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MobileNet** — архитектура нейронной сети, специально созданная для работы на CPU мобильных устройств. Высокая скорость работы первой версии **MobileNet** достигалась благодаря разделимой по глубине свертке (**Depthwise Separable Convolution**). \n",
    "\n",
    "Также для данной архитектуры предусматривается возможность варьировать размер входного изображения и ширину слоев. \n",
    "\n",
    "Функцией активации у мобилнетов обычно выступает **ReLU6** (ограничение сверху у данной функции активации призвано облегчить дальнейшую 8 битную квантизацию нейронной сети). \n",
    "\n",
    "В 2018 году была предложена архитектура **MobileNetV2**, которая существенно превосходит первую версию благодаря добавлению к архитектуре инвертированных остаточных блоков (**Inverted Residual Block**). \n",
    "\n",
    "А в 2019 году была предложена уже **MobileNetV3**, которая была получена при помощи автоматического поиска архитектуры (**Network Architecture Search**) и дополнительно включала в себя модули **squeeze-and-excitation** и немонотонную функцию активации **swish** (\"жесткая\" версия которой, h-swish, так же призвана облегчить квантизацию).\n",
    "\n",
    "Давайте разберемся, как все эти улучшения работают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depthwise separable convolution\n",
    "\n",
    "Обычная свертка в **CNN** одновременно работает с шириной высотой и глубиной (каналами) карты признаков. Разделимая по глубине свёртка (**Depthwise separable convolution**) разделяет вычисления на два этапа:\n",
    "- для каждого канала изображения используется свой отдельный сверточный фильтр глубины 1.  По сути это - **Grouped Convolution** с количеством групп равным числу каналов. \n",
    "- после вычисляется свертка 1x1 с глубиной равной глубине входной карты признаков. \n",
    "\n",
    "Применение разделимой по глубине свертки позволяет уменьшить количество обучаемых параметров и вычислительных операций при небольшой потере точности.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/depthwise_and_separable_convolution.png\"  width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/depthwise_separable_convolution.png\"  width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted residual block\n",
    "Инвертированный остаточный блок (**Inverted residual block**) напоминает классический **residual block** из архитектуры **ResNet**, однако имеет ряд существенных отличий. Дело в том, что классический **residual block**, из-за резкого уменьшения размерности пространства в комбинации с функцией активации **ReLU** (которая уничтожает всю информацию от отрицательных значений), приводит к потере большого количества информации. Поэтому вместо уменьшения количества слоев в *середине*, оно наоборот увеличивается. Увеличение вычислительной сложности компенсируется использованием разделимой по глубине свертки. А на входе и выходе из блока (где количество слоев уменьшается) отсутствует нелинейность (**ReLU**).\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/The-difference-between-residual-block-and-inverted-residual.png\"  width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты исследователей из Google, показали, что функция активации [**Swish**](https://arxiv.org/pdf/1710.05941v1.pdf) улучшает точность глубоких моделий. Простая замена **ReLU** на **Swish** дала прирост 0.9% точности для **Mobile NASNet-A** (на ImageNet challenge).\n",
    "\n",
    " Посмотрим, что из себя представляет активация **Swish**:\n",
    " $$f(x) = x*\\sigma(\\beta x)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим сигмоиду и некий параметр $ \\beta $, который может быть определен как константа или как обучаемый параметр. \n",
    "\n",
    "* Если $\\beta = 0$, **Swish** становится линейной функцией $f(x) = \\frac{x}{2} $\n",
    "\n",
    "* Если $\\beta\\to \\infty$ сигмоидная составляющая приближается к пороговой функции, поэтому **Swish** становится похожим на функцию **ReLU**. \n",
    "\n",
    "То есть, нелинейность функции **Swish** может контролироваться моделью, если $\\beta$ задан в качестве обучаемого параметра.\n",
    "\n",
    "Главное отличие **Swish** от **ReLU** это ее немонотонность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже на графике представлено распределение значений, которые принимает $ \\beta $ (являясь обучаемым параметром) при обучении **Mobile NASNet-A**. Видно, что значения распределены от 0 до 1.5, также виден резкий пик при значении = 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/swish_b_parameter.png\"  width=\"500\"></center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике, чтобы не увеличивать количество обучаемых параметров, используют **Swish** со значением $\\beta = 1$ . В PyTorch такая реализация вызывается `nn.SiLU()` (**Si**gmoid **L**inear **U**nit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "silu = nn.SiLU()\n",
    "array = np.arange(-5, 5, 0.01)\n",
    "activated = silu(torch.Tensor(array))\n",
    "\n",
    "plt.figure(figsize=(8, 4), dpi=100)\n",
    "plt.plot(array, activated, label='$f(x)=x*\\sigma(x)$')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('SiLU')\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "plt.ylim(bottom=-2) \n",
    "plt.axis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "mobilenet = models.mobilenet_v3_small(weights=None)\n",
    "print(mobilenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Architecture Search\n",
    "\n",
    "[Neural Architecture Search with Reinforcement Learning (Zoph et al., 2016)](https://arxiv.org/abs/1611.01578)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/neural_architecture_search.png\"  width=\"600\"></center>\n",
    "<center><em>Схема автоматического поиска архитектуры нейронных сетей </em></center>\n",
    "\n",
    "Проектировать архитектуры нейронных сетей сложно - можно автоматизировать поиск архитектуры и пойти пить чай!\n",
    "\n",
    "Для создания **MobileNetV3** использовалась система автоматического поиска архитектуры. Вот некоторые ее особенности: \n",
    "- Рекуррентная нейронная сеть (контроллер) выводит архитектуры сетей.\n",
    "- У нас нет размеченных данных. Для того чтобы понять насколько хороша выведенная контроллером архитектура создается и обучается  дочерняя сеть с этой архитектурой. При этом максимизируется итоговая точность обученной дочерней сети (это - обучение без учителя или обучение с подкреплением, подробнее об обучении с подкреплением мы поговорим в других лекциях).\n",
    "- Т.к. дочерняя сеть долго обучается - есть смысл обучать несколько дочерних сетей параллельно. \n",
    "- После обучения партии дочерних сетей, делаем градиентный шаг на сети контроллера (используя градиент политики).\n",
    "- Со временем контроллер учится выдавать хорошие архитектуры!\n",
    "- ОЧЕНЬ ДОРОГО!!! Каждый шаг градиента на контроллере требует обучения дочерних моделей!\n",
    "- Оригинальная статья обучалась на 800 GPU в течение 28 дней!\n",
    "- Последующие работы были сосредоточены на более эффективном поиске архитектур."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети EfficientNet(2019 г.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan et al., 2019)](https://arxiv.org/abs/1905.11946)\n",
    "\n",
    "Идея **EfficientNet**состоит в том, чтобы получить возможность подбирать оптимальную глубину (число слоев), ширину (число каналов в слое) и разрешение (длину и ширину карты признаков) для конкретной задачи. Например, если мы берём входное изображение больше, чем обычно (например, 1024x1024 вместо привычных 256x256), то сети потребуется больше слоёв для увеличения рецептивного поля и больше каналов для захвата более тонких деталей на большом изображении.\n",
    "\n",
    "\n",
    "Масштабирование происходит с помощью составного коэффициента (**compound coefficient**).\n",
    "\n",
    "Например, еслиу у нас есть возможность использовать в $2^N$ больше вычислительных ресурсов, то мы можем просто увеличить глубину сети на $\\alpha^N$ ширину на $\\beta^N$ и размер изображения на $\\gamma^N$ где - $\\alpha$, $\\beta$ и $\\gamma$ постоянные коэффициенты, определяемые grid search на исходной немасштабированной модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая сеть **EfficientNet** основана на инвертированных узких остаточных блоках **MobileNet**, в дополнение к блокам сжатия и возбуждения (**squeeze-and-excitation blocks**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/efficientnet.png\"  width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор Visual Transformers(2020 г.)\n",
    "\n",
    "[Visual Transformers: Token-based Image Representation and Processing for Computer Vision (Wu et al., 2020)](https://arxiv.org/abs/2006.03677)\n",
    "\n",
    "[Реализация](https://github.com/lucidrains/vit-pytorch)\n",
    "\n",
    "[Блог-пост разбор  ViT](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\n",
    "\n",
    "\n",
    "**Vision Transformer** — это модель для классификации изображений, которая использует архитектуру трансформера. Попробуем разобраться, как она работает. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 2020 г стали появляться, работы где модели на базе архитектур трансформер смогли показать результаты лучше, чем у **CNN** моделей.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cited_vit_accuracy.png\"  width=\"650\"></center>\n",
    "\n",
    "\n",
    "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)](https://arxiv.org/abs/2010.11929) \n",
    "\n",
    "BiT - это baseline модель на базе **ResNet**, ViT - **Visual Transformer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Недостатки сверточного слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы практически полностью отказались от использования сверток,  заменив их слоями **self-attention**.  Попробуем понять, почему это сработало."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляя в модель свёрточный слой мы руководствуемся резонным предположением: чем ближе пиксели на изображении, тем больше будет их взаимное влияние.\n",
    "\n",
    " В большинстве случаев это работает:\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cnn_ok.png\"  width=\"700\"></center>\n",
    "\n",
    "\n",
    " - На слое n (красный) активируются нейроны, которые реагируют на морду и на хвост кота.\n",
    "\n",
    " - В карте активаций их выходы оказываются рядом, и в слое n + 1 (синий) они попадают в одну свертку, которая активируется на объектах типа \"кот\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так случается часто, но не всегда:\n",
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cnn_fail.jpg\"  width=\"700\"></center>\n",
    "\n",
    "На этом изображении активации нейронов реагирующих на морду и хвост не попадут в одну свертку на следующем слое. Это может привести к тому, что нейрон, обучившийся реагировать на кошек, не активируется.\n",
    "\n",
    "Причиной этого является допущение ([Inductive bias](https://en.wikipedia.org/wiki/Inductive_bias)) о взаимном влиянии соседних пикселей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-attention** слой лишен этого недостатка. Он обучается оценивать взаимное влияние входов друг на друга. Но как применить его к изображениям?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020) предлагается разбивать картинки на кусочки (patches) размером 16x16 пикселей и подать их на вход модели. \n",
    "\n",
    "Проделаем это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cat.jpeg'\n",
    "!wget -q $URL -O image.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем изображение в тензор, порежем на фрагменты и отобразим их, используя image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('image.jpg')\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "img = transform(img)\n",
    "patches = []\n",
    "sz = 64\n",
    "for r in range(0,img.shape[1] , sz):\n",
    "    for c in range(0,img.shape[2] , sz):\n",
    "        patches.append(img[:, r:r+sz, c:c+sz])\n",
    "\n",
    "patches = torch.stack(patches).type(torch.float)\n",
    "\n",
    "img_grid = utils.make_grid(patches,pad_value=10, normalize=True, nrow=4)\n",
    "plt.imshow(np.transpose(img_grid, (1, 2, 0)))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход модели они поступят в виде вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "img_grid = utils.make_grid(patches, pad_value=10, normalize=True, nrow=256//16);\n",
    "plt.imshow(np.transpose(img_grid, (1, 2, 0)));\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем последовательность из фрагментов изображения передается в модель, где после ряда преобразований попадает на вход слоя **self-attention**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/self_attention.png\"  width=\"900\"></center>\n",
    "\n",
    "Картинки приведены исключительно для наглядности, в действительности слой работает с векторами признаков, которые не визуализируются столь очевидно. Однако компоненты векторов отражают важность того или иного признака с учетом всех остальных входов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате, слой учится предсказывать влияние каждого из входов на каждый."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Соображения относительно размера patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформеры работают с последовательностями за счёт механизма внимания (**self-attention**). И чтобы подать на вход изображение требуется превратить его в последовательность.\n",
    "\n",
    "Сделать это можно разными способами, например, составить последовательность из всех пикселей изображения. Её длина $n =  H*W$ (высота на ширину)\n",
    "\n",
    "\n",
    "[Сложность вычисления](https://www.researchgate.net/figure/Compare-the-computational-complexity-for-self-attention-where-n-is-the-length-of-input_tbl7_347999026) одноголового слоя **self-attention** $O(n^2 d )$  где $n$ — число токенов и $d$ — размерность входа (embedding)  (для любознательных расчеты [тут](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model)).\n",
    "\n",
    "То есть для квадратных изображений $(H==W)$ получим $O(H^3 d )$\n",
    "\n",
    "1. Такой подход будет очень вычислительно сложен.\n",
    "\n",
    "2. Интуитивно понятно, что кодировать каждый пиксел относительно большим embedding-ом не очень осмысленно.\n",
    "\n",
    "\n",
    "*Для тех, кто забыл, напомним что $O()$ — это Big O notation, которая отражает ресурсы, требуемые для вычисления. Так для $O(1)$ — время вычисления будет постоянным, вне зависимости от количества данных, а для $O(N)$ — расти пропорционально количеству данных.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберём на примере: Допустим, мы используем трансформер для предложения длиной в 4 слова — \"Мама мылом мыла раму\" => у нас есть `4 токена`. Закодируем их в *embeddings* с размерностью `256`. Потребуется порядка $4^2*256 = 4096$ операций.\n",
    "\n",
    "А теперь попробуем провернуть то же самое для картинки размерами 256 на 256.\n",
    "Количество токенов \n",
    "\n",
    " $256^3*256  = 256^4 =  4 294 967 296 $. Упс... Кажется, нам так никаких ресурсов не хватит — трансформеры с картинками использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)](https://arxiv.org/abs/2010.11929) предлагается разбивать картинки на кусочки (patches) размером 16x16 пикселей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем сложность для картинки размером 256x256, разбитой на кусочки по 16 px. при том же размере токена (256) $n = 16$.\n",
    "$16^2*256 = 256^2 = 65536 $. И впрямь! ~65000 раз меньше ресурсов требуется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Как устроен  self-attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Self-attention слой в Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не теряем ли мы важной информации, разбивая изображение на фрагменты? На первый взгляд кажется, что модель сможет научиться восстанавливать порядок в котором фрагменты шли в исходном изображении.\n",
    "\n",
    "Всегда ли? \n",
    "\n",
    "Рассмотрим пример изображения, где нет ярко выраженной текстуры:\n",
    "\n",
    "\n",
    " <center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/positional_transformer_explanation.png\"  width=\"500\"></center>\n",
    "\n",
    " На рисунке а) наковальня падает на ребенка, на рисунке б) ребенок прыгает на наковальне.\n",
    " \n",
    "  Суть принципиально отличается, но что будет, если составить из фрагментов любого изображения набор патчей:\n",
    "\n",
    "  <center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/positional_vec_transformer_explanation.png\"  width=\"500\"></center>\n",
    "\n",
    "Восстановить по нему можно будет любой из вариантов!\n",
    "\n",
    "Так как **self-attention** блок никак не кодирует позицию элемента на входе, то важная информация потеряется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы избежать таких потерь, информацию кодирующую позицию фрагмента (patch)  добавляют к входным данным **self-attention** слоя в явном виде.\n",
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/linear_projection_of_flattened_patches.png\"  width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Методы для кодирования позиции](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем грузить наши изображения в **Vi**sual **T**ransformer.\n",
    "\n",
    "**Self-attention** блок мы разобрали, остальные блоки модели нам знакомы:\n",
    "\n",
    "> **MLP** (Multi layer perceptron) - Блок из одного или нескольких линейных слоев\n",
    "\n",
    "> **Norm** - Layer Normalization\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/visual_transformer_architecture.png\"  width=\"1800\"></center>\n",
    "<center><em>Архитектура Visual Transformer </em></center>\n",
    "\n",
    "\n",
    "1.   Изображение режется на фрагменты (patch)\n",
    "2.   Фрагменты (patch) подвергаются линейной проекции с помощью **MLP**\n",
    "3.   С полученными на выходе **MLP** векторами конкатенируются **positional embeddings** (кодирующие информацию о позиции path, как и в обычном трансформере для текста)\n",
    "4. К полученным векторам добавляют еще один **0***, который называют **class embedding**\n",
    "\n",
    " \n",
    "\n",
    "Любопытно, что для предсказания класса используется только выход. Он соответствует дополнительному **class embedding**.  Остальные выходы (а для каждего токена в трансформере есть свой выход) отбрасываются за ненадобностью.\n",
    "\n",
    "В финале этот специальный токен **0***, прогоняют через **MLP** и предсказывают классы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание с помощью ViT\n",
    "\n",
    "\n",
    "Используем пакет [ViT PyTorch](https://pypi.org/project/pytorch-pretrained-vit/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch_pretrained_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В пакете доступны доступно несколько [предобученных моделей](https://github.com/lukemelas/PyTorch-Pretrained-ViT#loading-pretrained-models):\n",
    "\n",
    "B_16, B_32, B_16_imagenet1k, ... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_vit import ViT\n",
    "model = ViT('B_16_imagenet1k', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load image\n",
    "# NOTE: Assumes an image `image.jpg` exists in the current directory\n",
    "img = transforms.Compose([\n",
    "    transforms.Resize((384, 384)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "])(Image.open('image.jpg')).unsqueeze(0)\n",
    "print(img.shape) # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "# Classify\n",
    "with torch.no_grad():\n",
    "    outputs = model(img)\n",
    "print(outputs.shape)  # (1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что нам предсказывает ViT. Для этого подгрузим dict с переводом индексов в человеческие названия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q http://edunet.kea.su/repo/src/L09_CNN_Architectures/data/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('imagenet_class_index.json') as f:\n",
    "      imagenet_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, собственно, переведем индекс в название:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 3 predictions:', [imagenet_labels[str(x.item())][1] for x in outputs.topk(3).indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну что ж, почти (капибар в классах Imagenet 1k, как вы могли догадаться, просто нет)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объем данных и ресурсов\n",
    "\n",
    "Как следует из текста [статьи](https://arxiv.org/abs/2010.11929), **ViT** обученный на **ImageNet**  уступал baseline CNN модели\n",
    "на безе сверточной сети (**ResNet**). И только при увеличении датасетов больше чем **ImageNet** преимущество стало заметным.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cited_vit_accuracy.png\"  width=\"400\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://www.pvsm.ru/obrabotka-izobrazhenij/370026\">Inductive bias and \n",
    "neural networks</a></p> </em></center>\n",
    "\n",
    "\n",
    "\n",
    "Вряд ли в вашем распоряжении окажется датасет сравнимый с [JFT-300M](https://paperswithcode.com/dataset/jft-300m) ( 300 миллионов изображений)\n",
    "и GPU/TPU ресурсы необходимые для обучения с нуля (*it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days*)\n",
    "\n",
    "Поэтому для работы с пользовательскими данными используется техника дообучения ранее обученной модели на пользовательских данных (**fine-tuning**).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeiT: Data-efficient Image Transformers\n",
    "\n",
    "Для практических задач рекомендуем использовать эту реализацию. Авторы предлагают подход, благодаря которому становится возможным обучить модель на стандартном **ImageNet** (ImageNet1k) на одной рабочей станции за 3 дня.\n",
    "\n",
    "*We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.*\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cited_deit_vit.png\"  width=\"700\"></center>\n",
    "\n",
    "[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n",
    "\n",
    "\n",
    "Разбор этого материала уже не входит в наш курс и рекомендуется к самостоятельному изучению.\n",
    "\n",
    "\n",
    "Дополнительно:\n",
    "[Distilling Transformers: (DeiT) Data-efficient Image Transformers](https://towardsdatascience.com/distilling-transformers-deit-data-efficient-image-transformers-61f6cd276a03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Статьи предшествовавшие появлению **ViT**:\n",
    "\n",
    "[Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n",
    "\n",
    "\n",
    "[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование ViT с собственным датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для использования **ViT** с собственными данными рекомендуем не обучать собственную модель с нуля, а использовать уже предобученную.\n",
    "\n",
    "Рассмотрим этот процесс на примере. Есть предобученный на **ImageNet**, **Visual Transformer** например: [deit_tiny_patch16_224](https://github.com/facebookresearch/deit)\n",
    "\n",
    "И мы хотим использовать ее со своим датасетом, который может сильно отличаться от **ImageNet**.\n",
    "\n",
    "Для примера возьмем **CIFAR10**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель. Как указанно на [github](https://github.com/facebookresearch/deit) модель зависит от библиотеки [timm](https://fastai.github.io/timmdocs/), которую нужно установить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загружаем модель с [pytorch-hub](https://pytorch.org/hub/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что модель запускается. \n",
    "Загрузим изображение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qnc https://edunet.kea.su/repo/EduNet-web_dependencies/L09/capybara.jpg -O image.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И подадим его на вход трансформеру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "pil = Image.open('image.jpg')\n",
    "\n",
    "# create the data transform that DeiT expects\n",
    "imagenet_transform = T.Compose([\n",
    "    T.Resize((224, 224)),    \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "\n",
    "out = model(imagenet_transform(pil).unsqueeze(0))\n",
    "print(out.shape)\n",
    "pil.resize((224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать модель с **CIFAR10**, нужно поменять количество выходов слоя, отвечающих за классификацию. Так как в **CIFAR10** десять классов, а в **ImageNet** тысяча.\n",
    "\n",
    "Чтобы понять, как получить доступ к последнему слою, выведем структуру модели:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что последний слой называется head и судя по количеству параметров на выходе (1000), которое совпадает с количеством классов **ImageNet**, именно он отвечает за классификацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим его слоем с 10-ю выходами по количеству классов в CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head = torch.nn.Linear(192, 10, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что модель не сломалась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(imagenet_transform(pil).unsqueeze(0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузим **CIFAR10** и проверим, как дообучится модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cifar10 = CIFAR10(root = \"./\", train=True, download=True, \n",
    "                   transform=imagenet_transform)\n",
    "\n",
    "# We use only part of CIFAR10 to reduce training time\n",
    "trainset, _ = torch.utils.data.random_split(cifar10, [10000, 40000])\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CIFAR10(root = \"./\", train=False, download=True, \n",
    "                  transform=imagenet_transform)\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Проведем стандартный цикл обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, train_loader, optimizer, epochs=1):\n",
    "  model.to(device)\n",
    "  model.train()\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  for epoch in range(epochs):  \n",
    "      for batch in tqdm_notebook(train_loader):\n",
    "          inputs, labels = batch\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(inputs.to(device))\n",
    "          loss = criterion(outputs, labels.to(device))\n",
    "          loss.backward()\n",
    "          optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дообучаем (**fine tune**) только последний слой модели, который мы изменили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.head.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим точность, на всей тестовой подвыборке **CIFAR10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def accuracy(model, testloader):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for batch in testloader:\n",
    "      images, labels = batch\n",
    "      outputs = model(images.to(device))\n",
    "      # the class with the highest energy is what we choose as prediction\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels.to(device)).sum().item()\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy of fine-tuned network : {accuracy(model, test_loader):.2f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дообучив последний слой на одной эпохе с использованием 20% данных, мы получили точность ~0.75\n",
    "\n",
    "Если дообучить все слои на 2-х эпохах, можно получить точность порядка 0.95. \n",
    "\n",
    "Это результат намного лучше чем тот, что мы получали на семинарах.\n",
    "\n",
    "Для этого потребуется порядка 10 мин (на GPU). Сейчас мы этого делать не будем.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И одной из причин того, что обучение идет относительно медленно, является увеличение изображений размером 32x32 до 224x224. \n",
    "\n",
    "Если бы мы использовали изображения **CIFAR10** в их родном размере, мы бы не потеряли никакой информации, но могли бы в разы ускорить обучение.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение размеров входа ViT\n",
    "\n",
    "На первый взгляд, ничего не мешает это сделать: **self-attention** слой работает с произвольным количеством входов.\n",
    "\n",
    "Давайте посмотрим, что будет, если подать на вход модели изображение отличное по размерам от 224x224.\n",
    "\n",
    "Для этого перезагрузим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "  model.head = torch.nn.Linear(192, 10, bias=True)\n",
    "  return model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И уберем из трансформаций Resize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transform = T.Compose([\n",
    "    # T.Resize((224, 224)),    don't remove this line\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "# Change transformation in base dataset\n",
    "cifar10.transform = cifar_transform\n",
    "first_img = trainset[0][0]\n",
    "\n",
    "model.to(torch.device(\"cpu\"))\n",
    "try:\n",
    "  out = model(first_img.unsqueeze(0))\n",
    "except Exception as e:\n",
    "  print(\"Exception:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем ошибку.\n",
    "\n",
    "Ошибка возникает в объекте [PatchEmbed](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/patch_embed.py), который превращает изображение в набор эмбеддингов.\n",
    "\n",
    "У объекта есть свойство `img_size`, попробуем просто поменять его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.patch_embed.img_size = (32, 32)\n",
    "try:\n",
    "  out = model(first_img.unsqueeze(0))\n",
    "except Exception as e:\n",
    "  print(\"Exception:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем новую ошибку.\n",
    "\n",
    "И возникает она в строке  \n",
    "`x = self.pos_drop(x + self.pos_embed)`\n",
    "\n",
    "x - это наши новые эмбеддинги для CIFAR10 картинок\n",
    "\n",
    "Откуда взялось число 5?\n",
    "\n",
    "4 - это закодированные фрагменты (patch) для картинки 32х32 их всего 4 (16x16) + один embedding для предсказываемого класса(class embedding).\n",
    "\n",
    "А 197 это positional encoding - эмбеддинги кодирующие позицию элемента. Они остались от **ImageNet**.\n",
    "\n",
    "Так как в ImageNet картинки размера 224x224 то в каждой помещалось 14x14 = 196 фрагментов и еще embedding для класса, итого 197 позиций.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги для позиций доступны через свойство:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_embed.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам надо изменить количество pos embeddings так что бы оно было равно 5  (количество patch + 1). \n",
    "Возьмем 5 первых:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_embed.data = model.pos_embed.data[:, :5, :]\n",
    "out = model(first_img.unsqueeze(0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заработало!\n",
    "\n",
    "Теперь обучим модель. Так как изображения стали намного меньше, то мы можем увеличить размер batch и использовать весь датасет. Так же будем обучать все слои, а не только последний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10.transform = cifar_transform\n",
    "train_loader = DataLoader(cifar10, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "# Now we train all parameters because model altered\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильно быстрее. \n",
    "Посмотрим на результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.transform = cifar_transform\n",
    "print(f'Accuracy of altered network : {accuracy(model,test_loader):.2f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильно хуже. \n",
    "\n",
    "Это можно объяснить тем, что  маленькие patch  ImageNet(1/196) семантически сильно отличаются от четвертинок картинок из CIFAR10 (1/4).\n",
    "\n",
    "Но есть и другая причина: мы взяли лишь первые 4 pos_embedding а остальные выкинули. Они переобучаться, но двух эпох для этого мало. \n",
    "\n",
    "Зато теперь мы можем использовать модель с изображениями любого размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети MLP-Mixer(2020 г.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы прошли с вами долгий путь: от первых свёрточных сетей типа **AlexNet** до супер новинок типа **Visual Transformer**. Но в 2021 году круг замкнулся и вышла статья [MLP-Mixer: An all-MLP Architecture for Vision (Tolstikhin et al., 2021)](https://arxiv.org/abs/2105.01601), в которой утверждается, что все эти свёртки и трансформеры — это, конечно, хорошо, но результатов, сравнимых с SOTA на распознавании образов, можно достичь и с помощью обычных **Multi-Layer Preceptrons** (они же `nn.Linear`).\n",
    "\n",
    "Конечно же, не обошлось без нюансов и просто настэкать линейные слои не поможет. Давайте разбираться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень хороший видео-разбор можно посмотреть [во влоге Yannic Kilcher](https://www.youtube.com/watch?v=7K4Z8RqjWIk&feature=youtu.be&ab_channel=YannicKilcher)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/mlp_mixer_architecture.png\"  width=\"700\"></center>\n",
    "<center><em>Архитектура сети MLP-Mixer </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, как и в **ViT**, мы будем разбивать исходное изображение на патчи, которые затем будем пропускать через полносвязанный слой (**Fully-connected**), чтобы получить латентное представление каждого патча. На этом сходство заканчивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы будем пропускать наши латентные представления через **N Mixer-Layers**. Предположим, у нас было 16 патчей, и мы решили, что размерность латентного вектора будет 128. Тогда получается, что на вход в Mixer мы подаём матрицу размером (16,128). Затем, мы нашу матрицу транспонируем и получаем (128,16).\n",
    "\n",
    "С простыми линейными слоями мы бы на этом и закончили. Развернули бы двухмерное представление в одномерное, и получили бы вектор размером (128,16). Но в **MLP-Mixer** мы поступим хитрее. Каждую (из 128) строку матрицы мы пропустим через один и тот же **MLP 1** слой (веса разделены между **MLP** слоями). Затем мы снова транспонируем полученные вектора (128,16) -> (16,128), прибавим к ним изначальные латентные представления (вот и пригодилась идея **Skip-Connections**) и повторим операцию, но уже со слоем **MLP 2**.\n",
    "\n",
    "В конце агрегируем полученные вектора с помощью **Global Average Pooling** и навешиваем линейную голову для классификации на столько классов, сколько нам нужно (1000 в случае классического **ImageNet**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внимательный читатель может заметить, что всё это подозрительно напоминает 1x1 convolutions, и будет недалёк от истины. Споры о том, называть ли эти операции свёртками или не называть, не утихают и [по сей день](https://wandb.ai/wandb_fc/pytorch-image-models/reports/Is-MLP-Mixer-a-CNN-in-disguise---Vmlldzo4NDE1MTU).\n",
    "\n",
    "С точки зрения результатов — качество сравнимо с **SOTA** (State Of The Art) моделями, но чуть хуже. Скорость обучения — тоже где-то рядом. Но есть большой бонус — на скорости инференса (работы на конечном устройстве) **MLP-Mixer** значительно выигрывает. Да и в целом, вывод, который можно сделать из их модели: не обязательно городить сложнейший огород, можно использовать простые инструменты, главное заложить правильный *inductive bias* (интуицию работы модели)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\"> Бонус: Практические аспекты работы с размерностью данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частая проблема при написании архитектур — это посчитать промежуточные размеры свёрток. К счастью, эта проблема решается довольно простой функцией. \n",
    "\n",
    "На вход подаём наш размер (например, картинки), и размер, который мы хотим получить на выходе. Указываем количество слоёв и дополнительные параметры (например, нам нужны kernels меньше 4, страйды меньше, чем ядра, и страйды, равные 1, за исключением двух последних слоев)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_chain(input_size, output_size, depth, cond):\n",
    "    '''Written by Francois Fleuret <francois@fleuret.org>'''\n",
    "    if depth == 0:\n",
    "        if input_size == output_size:\n",
    "            return [ [ ] ]\n",
    "        else:\n",
    "            return [ ]\n",
    "    else:\n",
    "        r = [ ]\n",
    "        for kernel_size in range(1, input_size + 1):\n",
    "            for stride in range(1, input_size):\n",
    "                if cond(depth, kernel_size, stride):\n",
    "                    n = (input_size - kernel_size) // stride + 1\n",
    "                    if n >= output_size and (n - 1) * stride + kernel_size == input_size:\n",
    "                        q = conv_chain(n, output_size, depth - 1, cond)\n",
    "                        r += [ [ (kernel_size, stride) ] + u for u in q ]\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "c = conv_chain(\n",
    "    input_size = 64, output_size = 8,\n",
    "    depth = 5,\n",
    "    # We want kernels smaller than 4, strides smaller than the\n",
    "    # kernels, and strides of 1 except in the two last layers\n",
    "    cond = lambda d, k, s: k <= 4 and s <= k and (s == 1 or d <= 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(1, 1, 64, 64)\n",
    "\n",
    "# get all possible variants of models\n",
    "for num, m in enumerate(c):\n",
    "    model = nn.Sequential(*[ nn.Conv2d(1, 1, l[0], l[1]) for l in m ])\n",
    "    \n",
    "    print(\"Variant {}\".format(num))\n",
    "    print(model)\n",
    "    print(x.size(), model(x).size())\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Список использованной литературы\n",
    "\n",
    "<font size=\"5\"> AlexNet\n",
    "\n",
    "[ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky et al., 2012)](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "[Подробнее про AlexNet](https://neurohive.io/ru/vidy-nejrosetej/alexnet-svjortochnaja-nejronnaja-set-dlja-raspoznavanija-izobrazhenij/)\n",
    "\n",
    "<font size=\"5\"> ZFNet\n",
    "\n",
    "[Visualizing and Understanding Convolutional Networks (Zeiler et al., 2013)](https://arxiv.org/abs/1311.2901)\n",
    "\n",
    "<font size=\"5\"> VGG\n",
    "\n",
    "[Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan et al., 2014)](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "[Краткое описание VGGNet](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "\n",
    "<font size=\"5\">GoogLeNet\n",
    "\n",
    "[Going Deeper with Convolutions (Szegedy et al., 2014)](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "<font size=\"5\"> Global Average Pooling\n",
    "\n",
    "[Network In Network, Lin et al., 2013](https://arxiv.org/abs/1312.4400)\n",
    "\n",
    "[Блог-пост про Global Average Pooling](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/)\n",
    "\n",
    "<font size=\"5\"> ResNet\n",
    "\n",
    "[Deep Residual Learning for Image Recognition (He et al., 2015)](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<font size=\"5\"> Fixup Initialization\n",
    "\n",
    "[Fixup Initialization: Residual Learning Without Normalization (Zhang et. al, 2019)](https://arxiv.org/abs/1901.09321)\n",
    "\n",
    "[Блог-пост про Fixup Initialization](https://towardsdatascience.com/understanding-fixup-initialization-6bf08d41b427)\n",
    "\n",
    "<font size=\"5\"> ResNeXt\n",
    "\n",
    "[Aggregated Residual Transformations for Deep Neural Networks (Xie et al., 2016)](https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "[Блог-пост про ResNeXt](https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac)\n",
    "\n",
    "<font size=\"5\"> DensNet\n",
    "\n",
    "[Densely Connected Convolutional Networks (Huang et al., 2016)](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "<font size=\"5\"> WideResNet\n",
    "\n",
    "[Wide Residual Networks (Zagoruyko et al., 2016)](https://arxiv.org/abs/1605.07146)\n",
    "\n",
    "\n",
    "<font size=\"5\"> SENet\n",
    "\n",
    "[Squeeze-and-Excitation Networks (Hu et al., 2017)](https://arxiv.org/abs/1709.01507)\n",
    "\n",
    "[Блог-пост про Squeeze-and-Excitation Networks](https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7)\n",
    "\n",
    "\n",
    "<font size=\"5\"> MobileNet\n",
    "\n",
    "[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard et al., 2017)](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "<font size=\"5\"> EfficientNet\n",
    "\n",
    "[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan et al., 2019)](https://arxiv.org/abs/1905.11946)\n",
    "\n",
    "<font size=\"5\"> Visual Transformer\n",
    "\n",
    "[Visual Transformers: Token-based Image Representation and Processing for Computer Vision (Wu et al., 2020)](https://arxiv.org/abs/2006.03677)\n",
    "\n",
    "[Блог-пост разбор  ViT](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\n",
    "\n",
    "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "<font size=\"5\"> MLP-Mixer\n",
    "\n",
    "[MLP-Mixer: An all-MLP Architecture for Vision (Tolstikhin et al., 2021)](https://arxiv.org/abs/2105.01601)\n",
    "\n",
    "[Разбор во влоге Yannic Kilcher](https://www.youtube.com/watch?v=7K4Z8RqjWIk&feature=youtu.be&ab_channel=YannicKilcher)\n",
    "\n",
    "[Блог-пост про MLP-Mixer](https://wandb.ai/wandb_fc/pytorch-image-models/reports/Is-MLP-Mixer-a-CNN-in-disguise---Vmlldzo4NDE1MTU)\n",
    "\n",
    "<font size=\"5\"> Разное\n",
    "\n",
    "[Блог-пост про различные типы сверток](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)\n",
    "\n",
    "[Benchmark Analysis of Representative\n",
    "Deep Neural Network Architectures (Bianco et al., 2018)](https://arxiv.org/abs/1810.00736)\n",
    "\n",
    "[Neural Architecture Search with Reinforcement Learning (Zoph et al., 2016)](https://arxiv.org/abs/1611.01578)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
