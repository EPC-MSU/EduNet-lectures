{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Архитектуры CNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Базовые компоненты сверточных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Мы рассмотрели базовые компоненты, из которых состоят современные свёрточные нейронные сети, а также техники их обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/basic_components_cnn.png\" >\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/basic_components.png\" width=\"760\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом занятии рассмотрим, какие модели для классификации изображений можно построить на основе этих компонент. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet Large Scale Visual Recognition Challenge\n",
    "\n",
    " ([ILSVRC](http://image-net.org/challenges/LSVRC/2017/))\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/imagenet_large_scale_visual_recognition_challenge.png\"  width=\"800\"></center>\n",
    "<center><em>Лучшие результаты конкнурса ILSVRC по годам.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка с [официального сайта](http://image-net.org/download) недоступна, но можно загрузить данные с [Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/data). Однако архив занимает порядка 156Gb и не поместится на диск Colab. Поэтому воспользуемся другим репозиторием, в котором находится 1000 изображений из оригинального датасета.\n",
    "\n",
    "P.S. Для загрузки данных, которые стали недоступны на официальных сайтах, можно использовать [Academic Torrents](https://academictorrents.com). В частности для [ImageNet](https://academictorrents.com/browse.php?search=imagenet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# fix random_seed\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# compute in cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Full list of labels \n",
    "#'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L09/imagenet_class_index.json\n",
    "\n",
    "# https://github.com/ajschumacher/imagen.git\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L09/imagen.zip\n",
    "!unzip imagen.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили категории:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)\n",
    "\n",
    "with open('imagenet_class_index.json') as f:\n",
    "      imagenet_labels = json.load(f)\n",
    "\n",
    "pp.pprint(dict(list(imagenet_labels.items())[:10])) # Use Pretty Print to display long dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MicroImageNet(Dataset):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Load labels\n",
    "    self.num2id = {}\n",
    "    with open('imagenet_class_index.json') as f:\n",
    "      imagenet_labels = json.load(f)\n",
    "    w_net = {}\n",
    "    # Because not world net all image codes from imagen exists in imagenet_labels \n",
    "    # we need to filter this image\n",
    "    for key in imagenet_labels.keys():\n",
    "      wn_id = imagenet_labels[key][0]\n",
    "      w_net[wn_id] = {'num': int(key), 'name': imagenet_labels[key][1] }\n",
    "    self.labels = []\n",
    "    self.paths = []\n",
    "\n",
    "    # Load data\n",
    "    images = glob('imagen/*.jpg')\n",
    "    images.sort()\n",
    "    for i, path in enumerate(images):\n",
    "      name = path.split(\"_\")[2] # Class name\n",
    "      id = path.split(\"_\")[0][7:] # WorldNet based ID\n",
    "      if w_net.get(id,None): \n",
    "        self.labels.append([w_net[id]['num'], w_net[id]['name'], id])\n",
    "        self.paths.append(path)\n",
    "    \n",
    "  def __getitem__(self,idx):\n",
    "    im = Image.open(self.paths[idx])\n",
    "    class_num = self.labels[idx][0]\n",
    "    return im, class_num\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.paths)\n",
    "\n",
    "\n",
    "microImgNet = MicroImageNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "\n",
    "def show(img,label_1,num,label_2=\"\"):\n",
    "    ax = plt.subplot(2, 3,num+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(label_1)\n",
    "    ax.set_xlabel(label_2)\n",
    "    plt.axis('off')\n",
    "  \n",
    "for i in range(6):\n",
    "  img, label = microImgNet[i*6]\n",
    "  name = microImgNet.labels[i*6][1]\n",
    "  show(img,name,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети AlexNet(2012 г.)\n",
    "\n",
    "[ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky et al., 2012)](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "создатели: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n",
    "**University of Toronto**\n",
    "\n",
    "Как упоминалось на первой лекции, современный бум нейростевых технологий начался в 2012 году, когда AlexNet с большим отрывом от конкурентов победила в ImageNet.\n",
    "\n",
    "В AlexNet есть все компоненты, которые мы рассматривали ранее. Её архитектура состоит из пяти свёрточных слоёв, между которыми располагаются pooling-слои и слои нормализации, а завершают нейросеть три полносвязных слоя.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/alexnet_architecture.png\"  width=\"800\"></center>\n",
    "\n",
    "<center><em>Архитектура сети AlexNet</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Подробнее про AlexNet](https://neurohive.io/ru/vidy-nejrosetej/alexnet-svjortochnaja-nejronnaja-set-dlja-raspoznavanija-izobrazhenij/)\n",
    "\n",
    "На схеме архитектуры все выходные изображения делятся на два одинаковых участка — это связано с тем, что нейросеть **обучалась на двух старых GPU GTX580**, у которых было всего 3 ГБ видеопамяти. Для обработки использовались две видеокарты, чтобы параллельно выполнять операции над двумя частями изображения.\n",
    "Изначально на вход подаётся фотография размером 224×224×3, и **размер свёрточных фильтров первого слоя — 11×11**. Всего применяется 96 фильтров с шагом 4.\n",
    "\n",
    "**Пространственные размеры** изображения сначала довольно **сильно сжимаются**, затем постепенно увеличивается число фильтров. В результате ширина и высота этого уже не изображения, а набора активаций признаков, сильно уменьшаются, после чего оно поступает на **два полносвязных слоя**, где количество весов уже довольно большое.\n",
    "\n",
    "\n",
    "AlexNet не получится использовать для классификации cifar10 (по крайней мере, напрямую), потому что если начать так агрессивно уменьшать изображение размером 32×32 px, то в определенный момент в него просто не поместится следующий фильтр, который нужно применить, и изображение просто исчезнет. В принципе ничто не мешает нам сделать resize изображения до более крупного, но сами понимаете, что будет с качеством такого изображения.\n",
    "\n",
    "Структура некоторых (особенно старых) сетей, заточенных под ImageNet, напрямую зависит от размера изображений: если соотношение сторон позволяет фильтрам поместиться, проблем не возникнет, если же оно меньше и в какой-то момент размер уменьшится до 2×2 или 1×1, то фильтр 3×3 просто не сработает. В современных сетях есть слой, который позволяет решить эту проблему.\n",
    "\n",
    "На тот момент такая архитектура показала прорывную точность (ошибка упала с 20% до 15.4%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center><em><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/alexnet_architecture_with_parameters.png\"></em></center>\n",
    "<center><em>Архитектура сети AlexNet c параметрами слоев</em></center>\n",
    "\n",
    "Также стоит упомянуть о нескольких важных параметрах нейросети, необычных на тот момент:\n",
    "\n",
    "\n",
    "* AlexNet — практически первая архитектура, в которой применяется нелинейность ReLU. Ранее использовались сигмоидальные функции, которые работали медленнее и/или хуже;\n",
    "\n",
    "* Используется собственная нормализация (не столь универсальная, как Batch-нормализация) с отдельными слоями. Как выяснилось позднее, они не дают значительного улучшающего эффекта и поэтому не распространены в современных архитектурах;\n",
    "\n",
    "* На этапе предварительной обработки используется очень большое дополнение данных (аугментация, о ней в следующих лекциях);\n",
    "\n",
    "* Dropout = 0.5 (то есть при регуляризации отсеивается 50% нейронов);\n",
    "\n",
    "* SGD Momentum 0.9 (как показывают сегодняшние эксперименты, это не плохой вариант, но чтобы обучение сходилось, им требовалась с помощью некой эвристики (о ней речь пойдет ниже) периодически изменять Learning rate);\n",
    "\n",
    "* Скорость обучения — 1e−2, снижается в 10 раз вручную, если точность в какой-то момент перестаёт расти;\n",
    "\n",
    "* Затухание весов L2 — 5e−4;\n",
    "\n",
    "* В архитектуре используется ансамбль из 7 CNN — это позволило снизить процент ошибок с 18,2% до 15,4%. \n",
    "Ансамбль моделей — это когда обучается несколько моделей, а результат считается по среднему значению. Здесь используются 7 сетей, результат усредняется. Таким образом, достаточно сильно снижается ошибка.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним Pytorch реализацию с оригинальной. В чем отличия? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print('AlexNet architecture')\n",
    "print(summary(alexnet, (3,224,224),device='cpu'))\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Torch версии отсутствуют нормализации, а в остальном всё так же."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как работает: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def img2tensor(img):\n",
    "  t = F.to_tensor(img)\n",
    "  t = F.normalize(t, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "  return t\n",
    "\n",
    "def catId2names(nums):\n",
    "  titles = []\n",
    "  for num in nums:\n",
    "    titles.append(imagenet_labels[str(num.item())][1])\n",
    "    titles.reverse()\n",
    "  return \", \".join(titles)\n",
    "  \n",
    "for i in range(6,12):\n",
    "  img, label = microImgNet[i*6]\n",
    "  tensor = img2tensor(img)\n",
    "  out = alexnet(tensor.unsqueeze(0)) # Add batch dimension\n",
    "  labels_num = torch.argsort(out[0]) # Ascending order\n",
    "  weights = out[0][-5:]\n",
    "  predicted = catId2names(labels_num[-5:]) # Top 5\n",
    "  titles = []\n",
    "  name =  microImgNet.labels[i*6][1] \n",
    "  show(img,name,i-6,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Архитектура ZFnet(2013 г.)\n",
    "\n",
    "[Visualizing and Understanding Convolutional Networks (Zeiler et al., 2013)](https://arxiv.org/abs/1311.2901)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/zfnet_architecture.png\"  width=\"1000\"><center>\n",
    "<center><em>Архитектура сети ZFNet </em><center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/zfnet_architecture_2013.png\"  width=\"1000\"></center>\n",
    "<center><em>Архитектура сети ZFNet </em></center>\n",
    "\n",
    "*Тюнингованный AlexNet*\n",
    "\n",
    "Нейросеть ZFnet, созданная учеными из Йорского университета, В 2013 году выиграла соревнования, достигнув результата 11.7% — в ней AlexNet использовалась в качестве основы, но с изменёнными параметрами и слоями.\n",
    "\n",
    "Отличия от AlexNet небольшие:\n",
    "* Немного поменялись размеры фильтров (было 11, стало 7);\n",
    "* Увеличилось общее количество фильтров;\n",
    "\n",
    "В целом и количество слоев, и общая структура сети, когда слои свёртки и пулинга перемежаются друг с другом, а затем идут два полносвязных слоя, сохранились.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети VGGNet(2014 г.)\n",
    "[Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan et al., 2014)](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Karen Simonyan and Andrew Zisserman (Visual Geometry Group — **Oxford**)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/vggnet_architecture.png\"  width=\"400\" rotate=\"180\"></center>\n",
    "<center><em>Сравнение архитектур сетей AlexNet, VGG16 (использует 16 слоев) и VGG19 (использует 19 слоев)</em></center>\n",
    "\n",
    "[Краткое описание VGGNet](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "\n",
    "* Появление \"стандартных\" блоков внутри модели;\n",
    "* Свёртки 3x3.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 2014-ом году в Оксфорде была разработана модель VGGNet. Сеть получилась более точной и более глубокой.\n",
    "\n",
    "Авторы VGG подумали: А что, если нам взять AlexNet и увеличить его в глубину путем повторения блоков (*stacking*)? В то время идея *стакинга* была еще не очевидна. Для этого они выделили базовый блок `3x3 Conv -> 3x3 Conv -> Pool` и повторили его $N$ раз (есть разные варианты, такие как VGG11 — 11 повторений блока, VGG16 — 16 повторений и так далее)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На изображении выше изображены сети AlexNet и две версии VGG-16 и VGG-19 с 16 и 19 слоями соответственно. На соревнованиях победила более глубокая VGG19, достигнув более чем в два раза лучшего результата по сравнению с AlexNet.\n",
    "\n",
    "Несколько фактов об архитектуре VGGNet:\n",
    "\n",
    "* Нейросеть заняла 2 место в задаче классификации и 1 место в локализации на соревновании ImageNet (при локализации необходимо не только классифицировать объект, но и обвести его в ограничивающие рамки);\n",
    "\n",
    "* Процедура обучения такая же, как у AlexNet;\n",
    "\n",
    "* Слои нормализации отсутствуют.\n",
    "\n",
    "В прикладных задачах обычно используются (причём до сих пор) архитектуры VGG16 или VGG19 (VGG19 работает лучше, но расходует больше памяти). \n",
    "\n",
    "Особенности архитектуры VGG: все свёрточные слои имеют фильтры с рецептивным полем размера 3×3, они объединены в блоки, состоящие из некоторого количества свёрток с разным (постепенно увеличивающимся) количеством фильтров. Затем идут слои пулинга. Появление \"стандартных\" блоков внутри модели — одно из важных нововведений. Идея базового блока внутри сети будет достаточно широко использоваться дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем на коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "vgg = models.vgg16(pretrained=False) # Change on True if you want to use VGG to predict something\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять смысл использования сверток **3x3** необходимо познакомиться с понятием рецептивного поля и научиться оценивать количество необходимых вычислительных ресурсов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка вычислительных ресурсов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Размер рецептивного поля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Рецептивное поле** - участок входной карты признаков (входного тензора), который при прохождении одного или нескольких слоев нейронной сети формирует один признак на выходе (одно число выходного тензора). **Рецептивное поле** можно назвать “полем зрения”.\n",
    "\n",
    "Интуитивно кажется, что чем больше “поле зрение”, тем лучше обобщающая способность свертки. Авторы **VGG** решили отказаться от свертки **5x5** и заменить ее двумя свертками **3x3**.\n",
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/receptive_field_size.png\"  width=\"850\"></center>\n",
    "<center><em>Две свёртки 3×3 вместо одной 5×5.</em></center>\n",
    "\n",
    "\n",
    "Рассмотрим **рецептивное поле** свертки **5x5** ($\\color{green}{\\text{зеленый квадрат}}$ на A1). Применение свертки **5x5** к этому квадрату даст 1 признак на выходе ($\\color{red}{\\text{красный квадрат}}$ на A3). Применение к этому же **рецептивному полю** свертки **3x3** ($\\color{orange}{\\text{оранжевые квадраты}}$ на A1) даст на **3x3** признака ($\\color{red}{\\text{красный квадрат}}$ на A2), применение второй свертки **3x3** позволит получить 1 признак на выходе ($\\color{red}{\\text{красный квадрат}}$ A3). \n",
    "\n",
    "**Итого:** одна свертка **5x5** имеет то же **рецептивное поле**, что две свертки **3x3**.\n",
    "\n",
    "При этом применение двух сверток **3x3** дает ряд преимуществ:\n",
    "\n",
    "* меньшее количество параметров. При применении свертки **5x5** необходимо обучать $5⋅5⋅C_{in}⋅C_{out} = 25⋅C_{in}⋅C_{out}$  весов, где $С_{in}$ - глубина входной карты признаков, $C_{out}$ - количество фильтров сверточного слоя. При применении двух сверток **3x3** обучается $2⋅(3⋅3⋅C_{in}⋅C_{out}) = 18⋅C_{in}⋅C_{out}$  весов.\n",
    "* между свертками **3x3** добавляется дополнительный **слой активации**, который позволяет формировать более сложные признаки.\n",
    "\n",
    "**2 маленьких фильтра работают как один большой или даже лучше!**\n",
    "\n",
    "Аналогично 3 свертки **3x3** могут заменить одну свертку **7x7**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/receptive_field_size_example.png\"  width=\"850\"></center>\n",
    "<center><em>Три свёртки 3×3 вместо одной 7×7.</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Можно построить картинки с квадратиками самостоятельно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "input_tensor = torch.randn((1, 1, 12, 12))\n",
    "conv_5x5 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 5))\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_conv_5x5_1 = conv_5x5(input_tensor)\n",
    "    out_conv_5x5_2 = conv_5x5(out_conv_5x5_1)\n",
    "\n",
    "max_0 = input_tensor.size(-1)\n",
    "max_1 = out_conv_5x5_1.size(-1)\n",
    "max_2 = out_conv_5x5_2.size(-1)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "ax[0].imshow(input_tensor.squeeze().numpy(), cmap='Greys', \n",
    "             extent=(0, max_0, 0, max_0), alpha=0.5)\n",
    "ax[0].set_xticks(np.arange(0, max_0, 1))\n",
    "ax[0].set_yticks(np.arange(0, max_0, 1))\n",
    "\n",
    "ax[1].imshow(out_conv_5x5_1.squeeze().numpy(), cmap='Greys', \n",
    "             extent=(0, max_1, 0, max_1), alpha=0.5)\n",
    "ax[1].set_xticks(np.arange(0, max_0, 1))\n",
    "ax[1].set_yticks(np.arange(0, max_0, 1))\n",
    "\n",
    "ax[2].imshow(out_conv_5x5_2.squeeze().numpy(), cmap='Greys', \n",
    "             extent=(0, max_2, 0, max_2), alpha=0.5)\n",
    "ax[2].set_xticks(np.arange(0, max_0, 1))\n",
    "ax[2].set_yticks(np.arange(0, max_0, 1))\n",
    "\n",
    "ax[0].add_patch(patches.Rectangle(xy=(6, 6), width=5, height=5, fill=False, \n",
    "                                  edgecolor='blue', linewidth=5)) \n",
    "\n",
    "ax[0].add_patch(patches.Rectangle(xy=(6, 6), width=3, height=3, fill=False, \n",
    "                                  edgecolor='orange', linewidth=3)) \n",
    "\n",
    "ax[0].add_patch(patches.Rectangle(xy=(8, 8), width=3, height=3, fill=False, \n",
    "                                  edgecolor='orange', linewidth=3)) \n",
    "\n",
    "ax[1].add_patch(patches.Rectangle(xy=(3, 3), width=3, height=3, fill=False, \n",
    "                                  edgecolor='red', linewidth=5)) \n",
    "\n",
    "ax[1].add_patch(patches.Rectangle(xy=(3, 3), width=1, height=1, fill=False,\n",
    "                                  edgecolor='orange', linewidth=3)) \n",
    "\n",
    "ax[1].add_patch(patches.Rectangle(xy=(5, 5), width=1, height=1, fill=False,\n",
    "                                  edgecolor='orange', linewidth=3)) \n",
    "\n",
    "ax[2].add_patch(patches.Rectangle(xy=(2, 2), width=1, height=1, fill=False,\n",
    "                                  edgecolor='red', linewidth=3)) \n",
    "\n",
    "ax[0].set_title('A0')\n",
    "ax[1].set_title('A1')\n",
    "ax[2].set_title('A2')\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlim(0, max_0)\n",
    "    a.set_ylim(0, max_0)\n",
    "    a.grid(color='black', linewidth=1, which='major')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка количества ресурсов для работы модели\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие ресурсы нужны для работы нейронной сети? \n",
    "* Память для хранения промежуточных представлений.\n",
    "* Память для обучаемых параметров.\n",
    "* Вычислительные ресурсы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/weights_memory.png\"  width=\"650\"></center>\n",
    "<center><em>Пример свертки с фильтрами малого размера.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим один сверточной слой (см. картинку выше).\n",
    "\n",
    "$\\color{orange}{\\text{Оранжевый тензор}}$ - карта признаков, поступившая на вход сверточного слоя. Параметры $H_{in}$, $W_{in}$, $C_{in}$ - длина, ширина и глубина (число каналов изображения или фильтров в предыдущем сверточном слое) входного тензора. Данный тензор не учитывается в затрачиваемых ресурсах, т.к. информация сохранена в предыдущем слое.\n",
    "\n",
    "**Память для хранения промежуточных представлений** определяется **количеством элементов на выходе слоя** ($\\color{blue}{\\text{синий тензор}}$). Они хранятся для вычисления следующих слоев и градиентов. Необходимая память рассчитывается как:\n",
    "\n",
    "<center> $H_{out}⋅W_{out}⋅C_{out}⋅n_{byte}$,</center>\n",
    "\n",
    "где $H_{out}$, $W_{out}$, $C_{out}$ - длина, ширина и глубина (число фильтров в свертке) выходной карты признаков, а $n_{byte}$ - количество байт для хранения одного элемента (4 для float 32).\n",
    "\n",
    "**Память для обучаемых параметров** определяется **весами и смещением** (bias) фильтров свертки ($\\color{green}{\\text{зеленые тензоры}}$). Необходимая память рассчитывается как:\n",
    "\n",
    "<center> $(K_h⋅K_w⋅C_{in}⋅C_{out} + С_{out})⋅n_{byte}$,</center>\n",
    "\n",
    "где $K_h$, $K_w$, $C_{in}$ - длина, ширина и глубина одного фильтра, $C_{out}$ - число фильтров в свертке.\n",
    "\n",
    "Для оценки необходимых **вычислительных ресурсов** посчитаем **количество операций** сложения и умножения при прямом проходе. Каждое число выходного тензора является результатом применения фильтра свертки к некоторому рецептивному полю входного тензора. Количество операций можно оценить, как произведение количества элементов на выходе слоя на размер одного фильтра: \n",
    "\n",
    "<center> $(H_{out}⋅W_{out}⋅C_{out})⋅(K_h⋅K_w⋅C_{in})$.</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценка памяти для хранения параметров слоя:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_sizes = [11, 7, 5, 3]\n",
    "\n",
    "for conv_size in conv_sizes:\n",
    "    conv_layer = nn.Conv2d(3, 64, conv_size, stride=1, padding=1)\n",
    "    print('Convolution size: %ix%i' % (conv_size, conv_size))\n",
    "    for tag, p in conv_layer.named_parameters():\n",
    "        print('Memory reqired for %s: %.2f kb' % \n",
    "              (tag, (np.prod(p.shape) * 4) / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценка количества вычислительных операций (для одной свертки):**\n",
    "\n",
    "**11x11:** (224 * 224 * 64) * (3 * 11 * 11) = 9 633 792 * 11 * 11 =  9 633 792 * 121  = 1 165 688 832\n",
    "\n",
    "**7x7:** (224 * 224 * 64) * (3 * 7 * 7) = 9 633 792 * 7 * 7 = 9 633 792 * 49 = 472 055 808\n",
    "\n",
    "121/49  = ~2.5 раза меньше чем **11x11**\n",
    "\n",
    "**5x5:** (224 * 224 * 64) * (3 * 5 * 5) = 9 633 792 * 5 * 5 = 9 633 792 * 25 = 240 844 800\n",
    "\n",
    "49/25 = ~2 раза меньше чем **7x7**\n",
    "\n",
    "**3x3:** (224 * 224 * 64) * (3 * 3 * 3) = 9 633 792 * 3 * 3 = 9 633 792 * 9 = 86 704 128\n",
    "\n",
    "25/9 = ~2.7 раза меньше чем **5x5**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сколько памяти и параметров VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/memory_and_parameters_vgg16.png\"  width=\"800\"></center>\n",
    "<center><em>Архитектура VGG16 с параметрами слоев </em></center>\n",
    "\n",
    "Отчасти благодаря такой экономии получилось сделать большую по тем временам сеть (16 слоев), но, несмотря на способы уменьшить вычислительную сложность и количество параметров, сеть все равно получилась огромной (для 16-тислойной сети одно изображение в памяти занимало 96 МБ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/compare_parameters_alexnet_and_vgg16.png\"  width=\"900\"></center>\n",
    "<center><em>Сравнение параметров AlexNet и VGG16.</em></center>\n",
    "\n",
    "* Основная часть памяти расходуется на большие свёртки в начальных слоях, где пространственные размеры (ширина и высота) велики;\n",
    "\n",
    "* Больше всего весов в полносвязанных слоях;\n",
    "\n",
    "* Вычислительные ресурсы нужны в первую очередь для сверток.\n",
    "\n",
    "\n",
    "VGG-16 получилась существенно больше по сравнению с и так довольно объемной AlexNet, и тем более по сравнению с современными моделями.\n",
    "\n",
    "В значительной степени с этим связано дальнейшее направление развития моделей. В следующем году ImageNet выиграла сеть под названием GoogleNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Оценка ресурсов GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы убедиться самостоятельно в размерах сетей, установим библиотеку для мониторинга ресурсов GPU: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil as GPU\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def gpu_usage():\n",
    "    GPUs = GPU.getGPUs()\n",
    "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
    "    if len(GPUs) == 0:\n",
    "        return False\n",
    "    gpu = GPUs[0]\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько памяти потребуется VGG19 и какого размера batch можно использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg19 = torchvision.models.vgg19(pretrained = False, progress = True)\n",
    "vgg19.requires_grad=True\n",
    "vgg19.to(device)\n",
    "\n",
    "!nvidia-smi # Common GPU info\n",
    "\n",
    "vgg19.train()\n",
    "\n",
    "for batch_size in [1,8,16,32,64]:\n",
    "  input_random = torch.rand(batch_size,3,224,224,device = device)\n",
    "  out = vgg19(input_random)\n",
    "  print(\"Batch size\", batch_size)\n",
    "  gpu_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистка памяти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()\n",
    " !nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_random = None #del input \n",
    "out = None #del out\n",
    "\n",
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети GoogleNet(2014 г.)\n",
    "\n",
    "[Going Deeper with Convolutions (Szegedy et al., 2014)](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "В отличие от предшествующих моделей, разработана в коммерческой компании с целью реального применения. \n",
    "\n",
    "Поэтому основной упор был сделан на эффективности.\n",
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/inception_module_googlenet.png\"  width=\"400\"></center>\n",
    "<center><em>Inception module </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/googlenet_architecture.png\"  width=\"900\"></center>\n",
    "<center><em>Архитектура GoogleNet </em></center>\n",
    "\n",
    "GoogleNet (так же часто встречается название Inception) — ещё более глубокая архитектура с 22 слоями. Она содержит всего 5 миллионов параметров — в 12 раз меньше, чем у AlexNet. При этом сеть оказалась немного более точной (ошибка снизилась с 7.3% до 6.7%).\n",
    "\n",
    "Рассмотрим, за счёт чего удалось достичь такого огромного выигрыша в ресурсах, так как многие идеи, которые впервые были использованы для GoogleNet, активно применяются до сих пор.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Inception module\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/naive_inception_module.png\"  width=\"750\"></center>\n",
    "<center><em>Наивная реализация Inception module </em></center>\n",
    "\n",
    "\n",
    "* Очень дорогие вычисления;\n",
    "* Слой пулинга также сохраняет глубину признака, что означает, что общая глубина после конкатенации на каждом слое может только расти.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы разработать с наибольшей вычислительной эффективностью, был придуман так называемый модуль Inception, основная идея которого заключается в том, что он сам по себе является небольшой локальной сетью. Его архитектура состоит из множества таких модулей, следующих друг за другом.\n",
    "\n",
    "Вместо того, чтобы добавлять новые слои фильтров один над другим, инженеры из Google решили делать свёртки параллельно. Они поместили разные варианты сверток в один блок и объединили их результаты между собой (конкатенация, а не сложение).\n",
    "\n",
    "То есть фильтры применяются параллельно. Затем, результаты объединяются, и создаётся выходной сигнал, который переходит на следующий слой.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на архитектуру сети в Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# launching Tensorboard in Colab\n",
    "def reinit_tensorboard(clear_log = True):\n",
    "  # Log files are read from this directory:\n",
    "  logs_base_dir = \"runs\"\n",
    "  if clear_log:\n",
    "    #  clear logs\n",
    "    !rm -rfv {logs_base_dir}/*\n",
    "    os.makedirs(logs_base_dir, exist_ok=True)\n",
    "  # Colab magic\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "#https://pytorch.org/vision/stable/_modules/torchvision/models/googlenet.html#googlenet\n",
    "#https://hackmd.io/@bouteille/Bk-61Fo8U\n",
    "googlenet = torchvision.models.googlenet(init_weights=True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "reinit_tensorboard(clear_log = True)\n",
    "writer = SummaryWriter(comment = \"googlenet\")\n",
    "input_random = torch.rand((1,3,224,224))\n",
    "writer.add_graph(googlenet, input_random)\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case Study: GoogleNet**\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/naive_inception_module_with_dimension_reduction.png\"  width=\"850\"></center>\n",
    "<center><em>Наивные реализации Inception module и Inception module с уменьшением размерности</em></center>\n",
    "\n",
    "При таком подходе \"в лоб\" (когда фильтры применяются параллельно) количество фильтров всё равно растет довольно быстро.\n",
    "\n",
    "Чтобы этого избежать, введены так называемые «узкие места» — слои с фильтром 1×1, уменьшающие глубину изображения. Благодаря им удалось достичь того, чтобы количество каналов на входе и на выходе либо не менялось, либо менялось только в моменты, когда это необходимо.\n",
    "\n",
    "Как видно на изображении выше, для свёрток с неединичным фильтром перед ними добавляется свёртка размером 1×1, при этом max pooling ненастоящий, его шаг 1, он не меняет пространственные размеры, а просто выбирает свёртку с максимальным значением из поля 3×3.\n",
    "Такой слой работает более эффективно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 Convolution\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/1x1_convolution.png\"  width=\"550\"></center>\n",
    "<center><em>Уменьшение глубины изображения с помощью 32 фильтров 1×1.</em></center>\n",
    "\n",
    "Свёртку 1х1 можно сравнить с полносвязанными слоями, когда мы берём столбик из этого тензора и сворачиваем его с некоторым количеством фильтров, получаем на выходе вектор (то есть получаем из одного вектора другой).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/naive_inception_module_with_dimension_reduction_parameters.png\"  width=\"700\">\n",
    "<center><em>Наивная реализация Inception module с уменьшением размерности </em></center>\n",
    "\n",
    "Количество параметров уменьшается в два с лишним раза по сравнению с лобовой реализацией. Сеть получается значительно экономичнее.\n",
    "\n",
    "Использование таких модулей и отсутствие полносвязных слоёв делают GoogleNet очень эффективной и достаточно точной сетью. Но это далеко не все нововведения, которые появились в этой модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Stem network\"\n",
    "\n",
    "В составе GoogleNet есть небольшая подсеть — Stem Network. Она состоит из трёх свёрточных слоёв с двумя pooling-слоями и располагается в самом начале архитектуры.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/stem_network.png\"  width=\"1200\"></center>\n",
    "<center><em>Архитектура GoogleNet. В первых слоях быстро уменьшаются пространственные размеры. </em></center>\n",
    " \n",
    "На входе располагаются свёртки с большим фильтром. Они служат для того, чтобы достаточно быстро и сильно уменьшить пространственные размеры (по сути, сжать изображение перед параллельной обработкой), чтобы минимизировать количество элементов в слоях.\n",
    "\n",
    "Отдельно стоит обратить внимание на завершающую часть сети: Global Average Pooling. Несложно заметить, что больше всего параметров появляется в полносвязных слоях, где каждый элемент связан с каждым. Сверточный слой очень экономичный по сравнению с полносвязным. В предыдущих моделях в конце было два полносвязных слоя, в которых находилась большая часть весов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Average Pooling\n",
    "\n",
    "[Network In Network, Lin et al., 2013](https://arxiv.org/abs/1312.4400)\n",
    "\n",
    "Полносвязанные слои заменены на GAP.\n",
    "\n",
    "* Меньше весов;\n",
    "* Независимость от размера входа;\n",
    "* Регуляризация;\n",
    "* Уменьшение числа параметров;\n",
    "* Положительно влияет на переобучение.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/global_average_pooling.png\"  width=\"570\"></center>\n",
    "<center><em>Global Average Pooling</em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про Global Average Pooling](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/)\n",
    "\n",
    "Идея в том, что все пространственные размеры, какими бы они не были (например, 6х6), сворачиваются в единицу.\n",
    "Мы накладываем фильтр размером 6х6 и берем среднее значение. То есть делаем пулинг с фильтром такого размера. Это происходит независимо по каждому слою.\n",
    "\n",
    "Раньше считалось, что свертывание пространственных измерений трехмерного (пространственно-канального) тензора в сверточной нейронной сети (CNN) в вектор посредством глобального объединения удаляет всю пространственную информацию. Но современные исследования это оспаривают. \n",
    "\n",
    "В частности, в статье [Global Pooling, More than Meets the Eye: Position Information is Encoded Channel-Wise in CNNs (Islam et al., 2021)](https://arxiv.org/abs/2108.07884) авторы демонстрируют, что позиционная информация кодируется на основе упорядочивания размеров каналов, в то время как семантическая информация в основном не кодируется. \n",
    "\n",
    "При этом нам не нужно запоминать кучу параметров, как в полносвязном слое, и таким образом выполняется некоторая регуляризация, которая позволяет, в том числе, бороться с переобучением, ввиду того, что мы избавляемся, от части, менее важных активаций.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def file2tensor(filename):\n",
    "  img = Image.open(filename)\n",
    "  t = torchvision.transforms.functional.to_tensor(img)\n",
    "  t = torchvision.transforms.functional.normalize(t, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "  return t\n",
    "\n",
    "class CNNfromHW(nn.Module):\n",
    "\n",
    "    def __init__(self,conv_module = None):\n",
    "        super().__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(3,16,5,padding=2) # 16xHxW\n",
    "        self.pool = nn.MaxPool2d(2,2) # 16 x H/2 x W/2 \n",
    "        self.conv2 = nn.Conv2d(16,32,3,padding=1) # 32 x H/2 x W/2\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1)) # Any spatial size -> 32x1x1\n",
    "        self.fc = nn.Linear(32,10)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape\",x.shape)\n",
    "        x = self.conv1(x) # 16xHxW\n",
    "        x = self.pool(x) # 16 x H/2 x W/2 \n",
    "        x = self.conv2(x) # 32 x H/2 x W/2\n",
    "        x = self.activation(x) # Any spatial size -> 32x1x1\n",
    "        x = self.gap(x)\n",
    "        scores = self.fc(x.flatten(1))\n",
    "        print(\"Output shape\",scores.shape)\n",
    "        return scores\n",
    "\n",
    "print(\"CIFAR10 like\")\n",
    "input_random = torch.rand(1,3,32,32)\n",
    "model_with_gap = CNNfromHW()\n",
    "out = model_with_gap(input_random)\n",
    "\n",
    "\n",
    "print(\"Arbitrary size\")\n",
    "# Different sizes work too!\n",
    "aramdillo_t = file2tensor('imagen/n02454379_10511_armadillo.jpg')\n",
    "out = model_with_gap(aramdillo_t.unsqueeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Затухание градиента\n",
    "\n",
    "**GoogleNet: дополнительный классификатор**\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/googlenet_additional_classificator.png\"  width=\"700\"></center>\n",
    "<center><em>Архитектура GoogleNet. Отдельно вынесен блок - дополнительный классификатор.</em></center>\n",
    "\n",
    "Помимо основного классификатора на выходе сети добавлены два дополнительных классификатора, встроенных в промежуточные слои. Они понадобились для того, чтобы улучшить обратное распространение градиента, потому что без батч-нормализации в таких глубоких сетях градиент очень быстро затухал, и обучить сеть такого размера было серьёзной проблемой.\n",
    "\n",
    "Обучение VGG осуществлялось непростым способом: сначала обучали 7 слоев, затем добавляли туда следующие и обучали это вручную. Без использования батч-нормализации вряд ли получится повторить результат.\n",
    "\n",
    "Google подошел более системно, добавив дополнительные выходы, которые способствовали тому, чтобы градиент меньше затухал. Благодаря этому удалось решить серьёзную на тот момент проблему, которая ограничивала возможность обучения глубоких моделей.\n",
    "Статья про батч-нормализацию появилась как раз в 15ом году, видимо уже после выхода этой модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Появление \"глубоких\" моделей (deep models)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/deep_models.png\"  width=\"750\"></center>\n",
    "<center><em>Победители ImageNet Large Scale Visual Recognition Challenge (ILSVRC).</em></center>\n",
    "\n",
    "В 15ом году появились существенно более глубокие модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети ResNet(2015 г.)\n",
    "\n",
    "\n",
    "[Deep Residual Learning for Image Recognition (He et al., 2015)](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/resnet_20_and_56_layers.png\"  width=\"550\"></center>\n",
    "<center><em>График обучения и теста двух модификаций ResNet с 20 и 56 слоями соответственно. </em></center>\n",
    "\n",
    "Но одной батч-нормализации было недостаточно, потому что появилась возможность благодаря ее использованию тренировать сети с большим количеством слоев (на слайде выше — пример 56-слойной сети Microsoft). Но, как видно из графиков, у неё ошибка и на тренировочном, и на тестовом датасете больше, чем у 20-тислойной сети.\n",
    "\n",
    "\n",
    "В 2015 году соревнования выиграла сеть ResNet, произведя настоящую революцию глубины нейросетей. Она состояла из 152 слоёв и снизила процент ошибок до 3,57%. Это сделало её почти в два раза эффективнее GoogleNet.\n",
    "\n",
    "Возникло предположение, что сеть, состоящая из большего количества слоёв должна работать как минимум не хуже, чем сеть меньшего размера, потому что в ней есть те же 20 слоев.\n",
    "\n",
    "Что же происходит с нейросетью, когда мы увеличиваем число слоёв? Можно ли, взяв обычную архитектуру вроде VGG, просто накидывать (*stacking*) всё больше и больше слоёв друг на друга и достигать лучшей точности? Нет, нельзя. Скорее всего, более глубокая нейросеть покажет даже худшие результаты как при обучении, так и при тестировании. И переобучение здесь не при чём, поскольку тогда ошибка на train была бы низкой.\n",
    "\n",
    "Можно провести подобную аналогию: если мы видим изображение какое-то короткое время и нам нужно быстро его описать, мы скорее всего заметим какие-то крупные объекты (сцена: либо это помещение, либо лес, улица, возможно, мы отметим фигуры людей на переднем плане или же автомобили, здания). Если же требуется более подробно описать рассмотреть и описать картинку, мы уже к этому же имеющемуся описанию, добавим детали: во что одеты люди, какого цвета машины, погода и так далее.\n",
    "\n",
    "Возможно, руководствуясь подобными соображениями, был придуман остаточный слой (residual).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resudial connection\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/resudial_connection.png\"  width=\"700\"></center>\n",
    "<center><em>Сравнение Residual Block с \"обычными\" слоями</em></center>\n",
    "\n",
    "* сумма, а не конкатенация;\n",
    "* Batch normalization присутствует.\n",
    "\n",
    "\n",
    "Идея Residual Block состоит в том, что мы к имеющемуся уже набору признаков добавляем значения некоторых новых: то есть не перезаписываем то, что было на предыдущем слое, а копируем признаки, сохраняем их и через несколько слоев суммируем (а не конкатенируем) их с результатами свёрток на нескольких слоях, которые здесь присутствуют.\n",
    "\n",
    "Всё это вместе называется остаточным слоем или residual block.\n",
    "\n",
    "Применение этого метода дало потрясающий эффект: с одной стороны, по этому каналу стал хорошо распространяться градиент без дополнительных хаков, с другой стороны, перестали теряться важные свойства, которые удалось выделить на предыдущих блоках.\n",
    "\n",
    "Если сеть из 20-ти слоев способна распознавать ImageNet с точностью более 90%, значит, что в этих слоях уже содержится большая часть информации, а детали не должны её затирать. Такая архитектура показала очень хороший результат.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура ResNet\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/resnet_architecture.png\"  width=\"730\"></center>\n",
    "<center><em>Архитектура ResNet. Используется стаккинг residual blocks, каждый residual block состоит из двух 3x3 conv слоев, переодически количество фильтров в свертках удваиваеется, а размер выходных слоев уменьшается в 2 раза (за счет stride = 2). Добавился дополнительный conv слой в начале сети. Отсутствуют FC слои (кроме последнего который предсказывает класс).</em></center>\n",
    "\n",
    "Из таких блоков удалось построить очень глубокую сеть (были эксперименты из 1000 слоев). Для решения конкретной задачи победы на ImageNet хватило 150 слоев (добавление большего количества блоков уже не давало прироста точности).\n",
    "\n",
    "В ResNet используются многие идеи, которые присутствовали в предыдущих моделях: вначале изображение резко уменьшается, дальше используются блоки 3х3, как в VGG, далее применяется глобальный пулинг вместо полносвязных слоев. Блоки состоят из конструкций, изображенных выше: две свёртки 3Х3 и прибавление результата предыдущего слоя.\n",
    "\n",
    "Также на каждом слое перед активацией используется батч-нормализация.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet: bottleneck layer\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/resnet_bottleneck_layer.png\"  width=\"450\"></center>\n",
    "<center><em>В более глубоких сетях (ResNet-50+) для повышения эффективности используется слой \"bottleneck\" (похожим образом, что и в GoogLeNet)</em></center>\n",
    "\n",
    "Вес изображений уменьшается в два раза. На выходе average pooling. Даже такая огромная 152-х слойная сеть с точки зрения потребления ресурсов получилось более эффективной, чем VGG-19.\n",
    "\n",
    "Это ещё было достигнуто также за счёт того, что в более глубоких сетях вместо двух блоков 3х3 применялся более эффективный блок - *bottleneck*, где сначала происходит свёртка 1х1 и мы уменьшаем количество фильтров. С этим маленьким количеством фильтров делаем свёртку 3х3, а потом восстанавливаем количество фильтров до начальных значений, чтобы можно было прибавить к ним вход, иначе у нас не совпадут размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet реализация в Pytorch\n",
    "\n",
    "Код для базового блока и для bottleneck для сетей с количеством слоев до 50.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BasicBlock\n",
    "import inspect\n",
    "import torchvision.models.resnet as resnet\n",
    "\n",
    "code = inspect.getsource(resnet.BasicBlock.forward)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bottleneck\n",
    "import inspect\n",
    "import torchvision.models.resnet as resnet\n",
    "\n",
    "code = inspect.getsource(resnet.Bottleneck.forward)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так выглядит в коде этот базовый блок для сетей со слоями до 50 слоёв. То есть  свёртка, батч-нормализация, активация, свёртка, батч-нормализация. Если некоторый параметр (downsample) задан, то вызывается несколько слоев для downsampling: то есть это свёртка и ReLU с батч-нормализацией. Реализовано путем простого прибавления значения выходов к входам. \n",
    "\n",
    "\n",
    "[Модель ResNet на Torchvision](https://pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#resnet18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Обучение ResNet\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/resnet_18_and_34_layers.png\"  width=\"550\"></center>\n",
    "<center><em>График обучения и теста двух модификаций ResNet с 20 и 56 слоями соответственно</em></center>\n",
    "\n",
    "Когда точность выходила на плато, шаг обучения понижали вручную. Это давало хороший эффект: ошибка падала.\n",
    "\n",
    "Помимо того, что ResNet c огромным отрывом выиграла ImageNet у моделей прошлого года и, как уже видно по некоторым исследованиям, её результаты лучше, чем у человека, решения на базе этой архитектуры также стали победителями на соревнованиях по детектированию и сегментации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети ResNeXt(2016 г.)\n",
    "\n",
    "[2016 Aggregated Residual Transformations for Deep Neural Networks (Xie et al., 2016)](https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/compare_resnet_and_resnext_blocks.png\"  width=\"550\"></center>\n",
    "<center><em>Сравнение блоков ResNet и ResNeXt. ResNeXt создан тем же коллективом авторов, что и ResNet. Увеличение ширины residual block достигается путем использования параллельных веток \"cardinality\"), которые по духу схожи с Inception module. </em></center>\n",
    "\n",
    "Еще одна идея — это ResNeXt. Эта сеть выиграла ImageNet в следующем году. Здесь налицо заимствование идеи от Inception модуля. Можно обрабатывать не сразу все каналы, а распараллелить обработку и обрабатывать по несколько каналов. \n",
    "\n",
    "Эта идея уже была в AlexNet, но она там присутствовала вынужденно, потому что AlexNet обучали на двух видеокартах параллельно, просто в силу того, что модель не помещалась в память одной видеокарты на тот момент (видеокарты были по 3 Гб) и уже в конце на полносвязных слоях объединяли результаты двух карт, то есть там обработка происходила параллельно вынужденным образом. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnext = models.resnext50_32x4d(pretrained=False)\n",
    "print(resnext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupped Convolution\n",
    "\n",
    "[Блог-пост про различные типы сверток](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/groupped_convolution.png\"  width=\"600\"></center>\n",
    "<center><em>Групповые свертки позволяют обрабатывать входные данные параллельно. В качестве бонуса - уменьшение числа параметров, и дополнительная регуляризация.</em></center>\n",
    "\n",
    "На изображении выше на AlexNet видно, что сеть у них разделена на 2 части. \n",
    "\n",
    "Это демонстрация того, как это работает в обычном слое: можно каким-то образом разделить тензор так, что часть каналов начнет обрабатываться одной свёрткой, а часть другой, а потом их просто конкатенировать. Глубина фильтров, которые потребуются для этой свёртки, будет меньше. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped convolution in Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не нужно дожидаться результатов предыдущих операций, можем считать их параллельно. Этот механизм уже заложен в свёртку, которой пользовались в pytorch. Параметр group = 1, по умолчанию. Это означает, что групп нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним время, которое требуется на обычную свертку с `groups = 1` и `groups = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU test\n",
    "from torch import nn\n",
    "import time \n",
    "import torch\n",
    "\n",
    "def time_synchronized():\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    return time.time()\n",
    "\n",
    "input_random = torch.rand(8,512,112,112)\n",
    "start = time_synchronized() \n",
    "normal_conv = nn.Conv2d(512,1024,3,groups = 1)\n",
    "out = normal_conv(input_random)\n",
    "tm = time_synchronized() -start\n",
    "print(f\"Normal convolution take  {tm} sec.\")\n",
    "\n",
    "start = time_synchronized() \n",
    "groupped_conv = nn.Conv2d(512,1024,3,groups = 64)\n",
    "out = groupped_conv(input_random)\n",
    "tm = time_synchronized() -start\n",
    "print(f\"Groupped convolution take  {tm} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU test\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "start = time_synchronized() \n",
    "normal_conv = nn.Conv2d(512,1024,3,groups = 1).to(device)\n",
    "out = normal_conv(input_random.to(device))\n",
    "tm = time_synchronized() -start\n",
    "print(f\"Normal convolution take  {tm} sec.\")\n",
    "\n",
    "start = time_synchronized() \n",
    "groupped_conv = nn.Conv2d(512,1024,3,groups = 64).to(device)\n",
    "out = groupped_conv(input_random.to(device))\n",
    "tm = time_synchronized() -start\n",
    "print(f\"Groupped convolution take  {tm} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим память:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_random = None\n",
    "out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNext, Inception, grouped conv\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/resnext_inception_grouped_convolution.png\"  width=\"850\">\n",
    "\n",
    "[Блог-пост про ResNeXt](https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac)\n",
    "\n",
    "ResNeXt повторяет строительный блок, который объединяет набор преобразований с одинаковой топологией. По сравнению с ResNet, он раскрывает новое измерение, кардинальность (размер набора преобразований), как существенный фактор в дополнение к измерениям глубины и ширины.\n",
    "\n",
    "То есть эта операция сгруппированных сверток оказалась достаточно эффективной. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/feature_extraction.png\"  width=\"650\">\n",
    "\n",
    "Дело в том, что в процессе выполнения задачи классификации картинок у нас появляются веса, которые отвечают за распознавание разных особенностей, разных паттернов на изображениях. Эти веса в свёрточных слоях представляют большую ценность, потому что мы можем их использовать в данном случае для классификации.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/feature_extraction_add_x.png\"  width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но для решения другой задачи мы можем часть сети просто отрезать и заменить её чем-то еще:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/feature_extraction_backbone.png\"  width=\"650\">\n",
    "\n",
    "\n",
    "Например, мы можем на основании карт активаций, которые мы получили (которые уже намного меньше, чем были начальные изображения) проводить детектирование в этих картах, проводить сегментацию, можем генерировать вектора признаков (embedding) для разных задач: для сравнения изображений, для распознавания лиц, трекинга и многого другого.\n",
    "\n",
    "Часть сети (слева) является достаточно универсальной и получить эти веса можно, обучая сеть распознавать изображения для классификации, а потом использовать ещё  каким-то образом, возможно даже ещё не придуманным в настоящий момент. \n",
    "\n",
    "Эта часть сети называется backbone (скелет, основа), на которые уже строятся какие-то дополнительные алгоритмы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение моделей\n",
    "\n",
    "[Benchmark Analysis of Representative\n",
    "Deep Neural Network Architectures (Bianco et al., 2018)](https://arxiv.org/abs/1810.00736)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/compare_models_accuracy_vs_parameters.png\"  width=\"800\"></center>\n",
    "<center><em>Сравнение моделей по параметрам Точность vs Количество операций (Bianco et al., 2018)</em></center>\n",
    "\n",
    "Картинка из статьи 18 года, то есть не последняя, но тем не менее, на них довольно наглядно собрано много информации: здесь есть информация и о размере модели, и о скорости, и о точности. Здесь можно увидеть, что VGG — огромные по объёму модели, но по нынешним меркам они обладают средней точностью. Они требуют больших вычислительных ресурсов, поэтому сейчас их имеет смысл использовать разве что в учебных целях, а модели на базе ResNet (ResNet-50, ResNet-152) довольно хороши: в плане точности какого-то большого отрыва от них здесь не видно. Но, тем не менее, есть модели, которые работают несколько лучше. Рассмотрим их кратко, чтобы было понимание того, куда двигалась мысль в этой области.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети DenseNet(2016 г.)\n",
    "\n",
    "[Densely Connected Convolutional Networks (Huang et al., 2016)](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/densenet_architecture.png\"  width=\"350\"></center>\n",
    "<center><em>Архитектура DenseNet. Используется несколько \"обходов\" и конкатенация вместо суммы. </em></center>\n",
    "\n",
    "Один из вариантов, что можно сделать — это добавить еще дополнительных связей в обход блоков, чтобы градиент проходил ещё лучше.\n",
    "\n",
    "Можно заменить сумму на конкатенацию. Это тоже работает, но надо понимать, что конкатенация увеличивает количество признаков. Видимо, с этим можно бороться за счет свёрток 1Х1. Это так называемый DenseNet. С точки зрения ресурсов он, как правило, чуть более требовательный, чем базовый ResNet и немного более точный. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "densenet = models.densenet121(pretrained=False)\n",
    "print(densenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 WideResNet\n",
    "\n",
    "[Wide Residual Networks (Zagoruyko et al., 2016)](https://arxiv.org/abs/1605.07146)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/compare_basic_and_wide_residual_blocks.png\"  width=\"350\"></center>\n",
    "<center><em>Сравнение базового ResBlock и Широкого ResBlock, где используются $F \\times k$ фильтры вместо $F$ фильтров в каждом слое </em></center>\n",
    "\n",
    "Ещё одна идея, связанная уже не с распараллеливанием, а с увеличением количества фильтров: мы можем увеличивать количество фильтров и уменьшить количество слоёв. В такого рода моделях первая цифра — это количество слоёв, вторая — коэффициент, с которым мы увеличиваем количество наших фильтров.\n",
    "\n",
    "Авторы утверждают, что сами по себе residuals значительно более важный фактор, чем глубина. Так же стоит отметить, что 50 слойный Wide ResNet показывает лучшие результаты, чем оригинальный 152-х слойный ResNet. С точки зрения вычислительных ресурсов использование ширины вместо глубины - более эффективно (parallelizable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура SENet(2017 г.)\n",
    "\n",
    "[Squeeze-and-Excitation Networks (Hu et al., 2017)](https://arxiv.org/abs/1709.01507)\n",
    "\n",
    "[Блог-пост про Squeeze-and-Excitation Networks](https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7)\n",
    "\n",
    "Основным элементом CNN являются сверточный фильтр, который в каждом из каналов детектирует группы пикселей, обладающих локальной связностью. Таким образом, сверточный фильтр объединяет _пространственную информацию_ (_spatial-wise information_) &mdash; информацию о взаимном расположении пикселей друг относительно друга и _канальную информацию_ (_channel-wise information_)  &mdash; информацию о взаимоотношениях различных каналов внутри одного локального рецептивного поля, не отделяя эти два типа информации друг от друга. За счет чередования сверточных слоев и операций субдискретизации (pooling) CNN способны получать представления изображений (image representation), которые распознают сложные иерархические паттерны.\n",
    "\n",
    "Архитектуры сверточных нейронных сетей, рассмотренные нами до этого, концентрировались на поиске лучшего представления изображения за счет улучшения способов поиска зависимости между признаками в пространстве (Inception module, Residual block и т.д.), не затрагивая отношения между каналами.\n",
    "\n",
    "__SENet (Squeeze-and-Excitation Networks)__ &mdash; архитектура нейронной сети, одержавшая победу в ILSVRS-2017. Создатели SENet предложили новую архитектуру блока, называемую _Squeeze-and-Excitation (SE-блок)_, целью которой является поиск лучшего представления изображения за счет моделирования взаимодействия между каналами. Идея состоит в том, что не все каналы одинаково важны, поэтому мы можем выборочно выделять из них более информативные и подавлять менее информативные, создав механизм взвешивания каналов (feature recalibration).  _SE-блок_ состоит из следующих процессов:\n",
    "\n",
    "1. \"Сжатие\" (_squeeze_) каждого канала до единственного числового значения с использованием global pooling. Эта процедура позволяет получить некое глобальное представление результата обработки исходного изображения, сделанного каждым из сверточных фильтров, (__global information embedding__).\n",
    "\n",
    "2. \"Возбуждение\" (_excitation_) использует информацию, полученную на этапе \"сжатия\", для определения взаимодействий между каналами. Для этого используются два полносвязных слоя, первый из которых вводит \"узкое место\" (bottleneck), уменьшающее размерность в соответсвии с параметром сжатия r, а второй восстанавливает размерность до исходной. В результате этой операции получается набор активаций, использующийся для взвешивания соответствующих каналов исходного изображения (__adaptive recalibration__).\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/senet_architecture.png\"  width=\"1000\"></center>\n",
    "<center><em>Архитектура сети SENet. Добавлен модуль для взвешивания признаков, используется GAP + 2 FC слоя. Победитель ILSVRC`17 с использованием ResNeXt-152 в качестве базовой архитектуры.</em></center>\n",
    "\n",
    "Таким образом, SE-блок использует механизм, идейно напоминающий self-attention для каналов, чьи отношения не ограничены локальным рецептивным полем соответствующих сверточных фильтров.\n",
    "\n",
    "Описанный SE-блок может быть интегрирован в современные архитектуры сверточных нейронных сетей, например, входя в состав остаточного блока сети ResNet или Inception модуля, как изображено на рисунке.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/senet_inception_module.png\"  width=\"1000\"></center>\n",
    "<center><em>Интеграция SE блока в современные архитектуры сверточных нейронных сетей </em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom SE block\n",
    "\n",
    "class SE_Block(nn.Module):\n",
    "    \"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4\"\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(c, c // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c // r, c, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети MobileNet(2017 г.)\n",
    "\n",
    "[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard et al., 2017)](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/mobilenet_architecture.png\"  width=\"380\"></center>\n",
    "<center><em>Сравнение сложности вычисления стандартной сети и MobileNet. Обычные свертки заменены depthwise свёрткой и свёрткой 1х1. Более вычислительно эффективна при небольшой потери точности;\n",
    "Используются в архитектурах mobileNetV2, ShuffleNet итд ...</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobileNet — это сверточная нейронная сеть, специально созданная для работы на CPU на мобильных устройствах с помощью комбинации аппаратно-ориентированного поиска архитектуры сети (*Neural Search*) и последующего улучшения за счёт новых достижений в архитектуре, таких как: новые эффективные версии функций активаций, практичные для мобильных устройств и новый эффективный дизайн сети.\n",
    "\n",
    "Дизайн сети включает в себя использование жесткой активации swish и модулей squeeze-and-excitation в блоках MBConv.\n",
    "\n",
    "Давайте разберемся, что это такое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты исследователей из Google, показали, что функция [Swish](https://arxiv.org/pdf/1710.05941v1.pdf) показывает лучшие результаты точности, в глубоких моделях. Простая замена ReLU на Swish дала прирост 0.9% точности для Mobile NASNet-A(на ImageNet challenge).\n",
    "\n",
    " Посмотрим, что из себя представляет активация Swish:\n",
    " $$f(x) = x*\\sigma(\\beta x)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим сигмоиду и некий параметр $ \\beta $, который может быть определен как константа или как обучаемый параметр. \n",
    "\n",
    "* Если $\\beta = 0$, Swish становится линейной функцией $f(x) = \\frac{x}{2} $\n",
    "\n",
    "* Если $\\beta\\to \\infty$ сигмоидная составляющая приближается к пороговой функции, поэтому Swish становится похожим на функцию ReLU. \n",
    "\n",
    "То есть, нелинейность функции swish может контролироваться моделью, если $\\beta$ задан в качестве обучаемого параметра.\n",
    "\n",
    "Самое главное отличие Swish от ReLU это ее немонотонность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже на графике представлено распределение значений, которые принимает $ \\beta $ (являясь обучаемым параметром) при обучении Mobile NASNet-A. Видно, что значения распределены от 0 до 1.5, также виден резкий пик при значении = 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/swish_b_parameter.png\"  width=\"500\"></center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому, на практике (потому, что это менее трудозатратно) используют Swish со значением $\\beta = 1$. В PyTorch такая реализация вызывается `nn.SiLU()` (**Si**gmoid **L**inear **U**nit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "silu = nn.SiLU()\n",
    "array = np.arange(-5, 5, 0.01)\n",
    "activated = silu(torch.Tensor(array))\n",
    "\n",
    "plt.figure(figsize=(8, 4), dpi=100)\n",
    "plt.plot(array, activated, label='$f(x)=x*\\sigma(x)$')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('SiLU')\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "plt.ylim(bottom=-2) \n",
    "plt.axis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depthwise separable convolution\n",
    "\n",
    "В то время, как стандартная свёртка выполняет поканальные и пространственно-временные вычисления за один шаг, разделимая по глубине свёртка (*Depthwise separable convolution*) разделяет вычисления на два этапа: глубинная свёртка применяет один свёрточный фильтр для каждого входного канала, а точечная свёртка используется для создания линейной комбинации выходных данных глубинной свёртки. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/depthwise_and_separable_convolution.png\"  width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/depthwise_separable_convolution.png\"  width=\"650\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "mobilenet = models.mobilenet_v3_small(pretrained=False)\n",
    "print(mobilenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffled Grouped Convolution\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/shuffled_grouped_convoltion.png\"  width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Architecture Search\n",
    "\n",
    "[Neural Architecture Search with Reinforcement Learning (Zoph et al., 2016)](https://arxiv.org/abs/1611.01578)\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/neural_architecture_search.png\"  width=\"600\"></center>\n",
    "<center><em>Схема автоматического поиска архитектуры нейронных сетей </em></center>\n",
    "\n",
    "Проектировать архитектуры нейронных сетей сложно - давайте автоматизируем это!\n",
    "\n",
    "- Одна сеть (контроллер) выводит архитектуры сетей\n",
    "- Выбираем дочерние сети из контроллера и обучаем их \n",
    "- После обучения партии дочерних сетей, делаем градиентный шаг на сети контроллера (используя градиент политики).\n",
    "- Со временем контроллер учится выдавать хорошие архитектуры!\n",
    "- ОЧЕНЬ ДОРОГО!!! Каждый шаг градиента на контроллере требует обучения партии дочерних моделей!\n",
    "- Оригинальная статья обучалась на 800 GPU в течение 28 дней!\n",
    "- Последующие работы были сосредоточены на более эффективном поиске"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети EfficientNet(2019 г.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan et al., 2019)](https://arxiv.org/abs/1905.11946)\n",
    "\n",
    "Фишка EfficientNet в равномерном масштабировании всех измерений (глубины/ширины/разрешения) с помощью составного коэффициента (*compound coefficient*). \n",
    "\n",
    "Например, если мы хотим использовать в $2^N$ больше вычислительных ресурсов, то мы можем просто увеличить глубину сети на $\\alpha^N$ ширину на $\\beta^N$ и размер изображения на $\\gamma^N$ где - $\\alpha$, $\\beta$ и $\\gamma$ постоянные коэффициенты, определяемые grid search на исходной немасштабированной модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Метод комбинированного масштабирования обоснован следующей интуицией — если мы берём более большое входное изображение, чем обычно (например, 1024x1024 вместо привычных 256x256), то сети потребуется больше слоёв для увеличения рецептивного поля и больше каналов для захвата более тонких деталей на большом изображении.\n",
    "\n",
    "Базовая сеть EfficientNet основана на инвертированных узких остаточных блоках MobileNet, в дополнение к блокам сжатия и возбуждения (*squeeze-and-excitation blocks*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/efficientnet.png\"  width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор Visual Transformers(2020 г.)\n",
    "\n",
    "[Visual Transformers: Token-based Image Representation and Processing for Computer Vision (Wu et al., 2020)](https://arxiv.org/abs/2006.03677)\n",
    "\n",
    "[Реализация](https://github.com/lucidrains/vit-pytorch)\n",
    "\n",
    "[Блог-пост разбор  ViT](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\n",
    "\n",
    "\n",
    "Vision Transformer — это модель для классификации изображений, которая использует архитектуру трансформера. Давайте подробно разберем, как она работает\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 2020 г стали появляться, работы где модели на базе архитектур трансформер смогли показать результаты лучше, чем у CNN моделей.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cited_vit_accuracy.png\"  width=\"650\"></center>\n",
    "\n",
    "\n",
    "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)](https://arxiv.org/abs/2010.11929) \n",
    "\n",
    "BiT - это baseline модель на базе ResNet, ViT - Visual Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Недостатки сверточного слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы практически полностью отказались от использования сверток,  заменив их слоями self-attention.  Давайте попробуем понять, почему это сработало."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляя в модель свёрточный слой мы руководствуемся резонным предположением: чем ближе пиксели на изображении, тем больше будет их взаимное влияние.\n",
    "\n",
    " В большинстве случаев это работает:\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cnn_ok.png\"  width=\"700\"></center>\n",
    "\n",
    "\n",
    " - На слое n (красный) активируются нейроны, которые реагируют на морду и на хвост кота.\n",
    "\n",
    " - В карте активаций их выходы оказываются рядом, и в слое n + 1 (синий) они попадают в одну свертку, которая активируется на объектах типа \"кот\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так случается часто, но не всегда:\n",
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cnn_fail.jpg\"  width=\"700\"></center>\n",
    "\n",
    "На этом изображении активации нейронов реагирующих на морду и хвост не попадут в одну свертку на следующем слое. Это может привести к тому, что нейрон, обучившийся реагировать на кошек, не активируется.\n",
    "\n",
    "Причиной этого является допущение ([Inductive bias](https://en.wikipedia.org/wiki/Inductive_bias)) о взаимном влиянии соседних пикселей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Self-attention слой лишен этого недостатка. Он обучается оценивать взаимное влияние входов друг на друга. Но как применить его к изображениям?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020) предлагается разбивать картинки на кусочки (patches) размером 16x16 пикселей и подать их на вход модели. \n",
    "\n",
    "Проделаем это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/transformers/cat.jpg'\n",
    "!wget -q $URL -O image.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем изображение в тензор, порежем на фрагменты и отобразим их, используя image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open('image.jpg')\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((256,256)),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "img = transform(img)\n",
    "patches = []\n",
    "sz = 64\n",
    "for r in range(0,img.shape[1] , sz):\n",
    "    for c in range(0,img.shape[2] , sz):\n",
    "        patches.append(img[:,r:r+sz,c:c+sz])\n",
    "\n",
    "patches = torch.stack(patches).type(torch.float)\n",
    "\n",
    "img_grid = utils.make_grid(patches,pad_value=10, normalize=True, nrow=4)\n",
    "plt.imshow(np.transpose(img_grid , (1, 2, 0)))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход модели они поступят в виде вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "img_grid = utils.make_grid(patches,pad_value=10, normalize=True, nrow=256//16);\n",
    "plt.imshow(np.transpose(img_grid , (1, 2, 0)));\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем последовательность из фрагментов изображения передается в модель, где после ряда преобразований попадает на вход слоя self-attention:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/self_attention.png\"  width=\"900\"></center>\n",
    "\n",
    "Картинки приведены исключительно для наглядности, в действительности слой работает с векторами признаков, которые не визуализируются столь очевидно. Однако компоненты векторов отражают важность того или иного признака с учетом всех остальных входов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате, слой учится предсказывать влияние каждого из входов на каждый."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Соображения относительно размера patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформеры работают с последовательностями за счёт механизма внимания (*self-attention*). И чтобы подать на вход изображение требуется превратить его в последовательность.\n",
    "\n",
    "Сделать это можно разными способами, например, составить последовательность из всех пикселей изображения. Её длина $n =  H*W$ (высота на ширину)\n",
    "\n",
    "\n",
    "[Сложность вычисления](https://www.researchgate.net/figure/Compare-the-computational-complexity-for-self-attention-where-n-is-the-length-of-input_tbl7_347999026) одноголового слоя self-attention $O(n^2 d )$  где $n$ — число токенов и $d$ — размерность входа (embedding)  (для любознательных расчеты [тут](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model)).\n",
    "\n",
    "То есть для квадратных изображений $(H==W)$ получим $O(H^3 d )$\n",
    "\n",
    "1. Такой подход будет очень вычислительно сложен.\n",
    "\n",
    "2. Интуитивно понятно, что кодировать каждый пиксел относительно большим embedding-ом не очень осмысленно.\n",
    "\n",
    "\n",
    "*Для тех, кто забыл, напомним что $O()$ — это *Big O notation*, которая отражает ресурсы, требуемые для вычисления. Так для $O(1)$ — время вычисления будет постоянным, вне зависимости от количества данных, а для $O(N)$ — расти пропорционально количеству данных. *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберём на примере: Допустим, мы используем трансформер для предложения длиной в 4 слова — \"Мама мылом мыла раму\" => у нас есть `4 токена`. Закодируем их в *embeddings* с размерностью `256`. Потребуется порядка $4^2*256 = 4096$ операций.\n",
    "\n",
    "А теперь попробуем провернуть то же самое для картинки размерами 256 на 256.\n",
    "Количество токенов \n",
    "\n",
    " $256^3*256  = 256^4 =  4 294 967 296 $. Упс... Кажется, нам так никаких ресурсов не хватит — трансформеры с картинками использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)](https://arxiv.org/abs/2010.11929) предлагается разбивать картинки на кусочки (patches) размером 16x16 пикселей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем сложность для картинки размером 256x256, разбитой на кусочки по 16 px. при том же размере токена (256) $n = 16$.\n",
    "$16^2*256 = 256^2 = 65536 $. И впрямь! ~65000 раз меньше ресурсов требуется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Как устроен  self-attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Self-attention слой в Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не теряем ли мы важной информации, разбивая изображение на фрагменты? На первый взгляд кажется, что модель сможет научиться восстанавливать порядок в котором фрагменты шли в исходном изображении.\n",
    "\n",
    "Всегда ли? \n",
    "\n",
    "Рассмотрим пример изображения, где нет ярко выраженной текстуры:\n",
    "\n",
    "\n",
    " <center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/positional_transformer_explanation.png\"  width=\"500\"></center>\n",
    "\n",
    " На рисунке а) наковальня падает на ребенка, на рисунке б) ребенок прыгает на наковальне.\n",
    " \n",
    "  Суть принципиально отличается, но что будет, если составить из фрагментов любого изображения набор патчей:\n",
    "\n",
    "  <center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/positional_vec_transformer_explanation.png\"  width=\"500\"></center>\n",
    "\n",
    "Восстановить по нему можно будет любой из вариантов!\n",
    "\n",
    "Так как self-attention блок никак не кодирует позицию элемента на входе, то важная информация потеряется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы избежать таких потерь, информацию кодирующую позицию фрагмента (patch)  добавляют к входным данным self-attention слоя в явном виде.\n",
    "\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/linear_projection_of_flattened_patches.png\"  width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Методы для кодирования позиции](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем грузить наши изображения в **Vi**sual **T**ransformer.\n",
    "\n",
    "Self-attention блок мы разобрали, остальные блоки модели нам знакомы:\n",
    "\n",
    "> MLP(Multi layer perceptron) - Блок из одного или нескольких линейных слоев\n",
    "\n",
    "> Norm - Layer Normalization\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/visual_transformer_architecture.png\"  width=\"1800\"></center>\n",
    "<center><em>Архитектура Visual Transformer </em></center>\n",
    "\n",
    "\n",
    "\n",
    "1.   Изображение режется на фрагменты (patch)\n",
    "2.   Фрагменты (patch) подвергаются линейной проекции с помощью MLP\n",
    "3.   С полученными на выходе MLP векторами конкатенируются positional embeddings* (как и в обычном трансформере для текста)\n",
    "4. К полученным векторам добавляют еще один **0***, который называют *class embedding*\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "Любопытно, что для предсказания класса используется только выход. Он соответствует дополнительному *class embedding* .  Остальные выходы (а для каждего токена в трансформере есть свой выход) отбрасываются за ненадобностью.\n",
    "\n",
    "  В финале этот специальный токен **0***, прогоняют через MLP и предсказывают классы, остальные выходы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание с помощью ViT\n",
    "\n",
    "\n",
    "Используем пакет [ViT PyTorch](https://pypi.org/project/pytorch-pretrained-vit/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch_pretrained_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В пакете доступны доступно несколько [предобученных моделей](https://github.com/lukemelas/PyTorch-Pretrained-ViT#loading-pretrained-models):\n",
    "\n",
    "B_16, B_32, B_16_imagenet1k, ... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_vit import ViT\n",
    "model = ViT('B_16_imagenet1k', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load image\n",
    "# NOTE: Assumes an image `image.jpg` exists in the current directory\n",
    "img = transforms.Compose([\n",
    "    transforms.Resize((384, 384)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "])(Image.open('image.jpg')).unsqueeze(0)\n",
    "print(img.shape) # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "# Classify\n",
    "with torch.no_grad():\n",
    "    outputs = model(img)\n",
    "print(outputs.shape)  # (1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что нам предсказывает ViT. Для этого подгрузим dict с переводом индексов в человеческие названия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q http://edunet.kea.su/repo/src/L09_CNN_Architectures/data/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('imagenet_class_index.json') as f:\n",
    "      imagenet_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, собственно, переведем индекс в название:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 3 predictions:', [imagenet_labels[str(x.item())][1] for x in outputs.topk(3).indices[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну что ж, почти (капибар в классах Imagenet 1k, как вы могли догадаться, просто нет)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объем данных и ресурсов\n",
    "\n",
    "Как следует из текста [статьи](http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/transformers/cited_vit_accuracy.png), ViT обученный на ImageNet  уступал baseline CNN модели\n",
    "на безе сверточной сети (ResNet). И только при увеличении датасетов больше чем ImageNet преимущество стало заметным.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cited_vit_accuracy.png\"  width=\"400\"></center>\n",
    "\n",
    "\n",
    "\n",
    "Вряд ли в вашем распоряжении окажется датасет сравнимый с [JFT-300M](https://paperswithcode.com/dataset/jft-300m) ( 300 миллионов изображений)\n",
    "и GPU/TPU ресурсы необходимые для обучения с нуля (*it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days*)\n",
    "\n",
    "Поэтому для работы с пользовательскими данными используется техника дообучения ранее обученной модели на пользовательских данных. (fine-tuning)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeiT: Data-efficient Image Transformers\n",
    "\n",
    "Для практических задач рекомендуем использовать эту реализацию. Авторы предлагают подход, благодаря которому становится возможным обучить модель на стандартном ImageNet (ImageNet1k) на одной рабочей станции за 3 дня.\n",
    "\n",
    "*We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.*\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L09/cited_deit_vit.png\"  width=\"700\"></center>\n",
    "\n",
    "[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n",
    "\n",
    "\n",
    "Разбор этого материала уже не входит в наш курс и рекомендуется к самостоятельному изучению.\n",
    "\n",
    "\n",
    "Дополнительно:\n",
    "[Distilling Transformers: (DeiT) Data-efficient Image Transformers](https://towardsdatascience.com/distilling-transformers-deit-data-efficient-image-transformers-61f6cd276a03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Статьи предшествовавшие появлению ViT:\n",
    "\n",
    "[Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n",
    "\n",
    "\n",
    "[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование ViT с собственным датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для использования ViT с собственными данными рекомендуем не обучать собственную модель с нуля, а использовать уже предобученную.\n",
    "\n",
    "Рассмотрим этот процесс на примере. Есть предобученный на ImageNet, Visual Transformer например: [deit_tiny_patch16_224](https://github.com/facebookresearch/deit)\n",
    "\n",
    "И мы хотим использовать ее со своим датасетом, который может сильно отличаться от ImageNet.\n",
    "\n",
    "Для примера возьмем CIFAR10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель. Как указанно на [github](https://github.com/facebookresearch/deit) модель зависит от библиотеки [timm](https://fastai.github.io/timmdocs/), которую нужно установить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загружаем модель с [pytorch-hub](https://pytorch.org/hub/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что модель запускается. \n",
    "Загрузим изображение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qnc https://edunet.kea.su/repo/EduNet-web_dependencies/L09/capybara.jpg -O image.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И подадим его на вход трансформеру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "pil = Image.open('image.jpg')\n",
    "\n",
    "# create the data transform that DeiT expects\n",
    "imagenet_transform = T.Compose([\n",
    "    T.Resize((224,224)),    \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "\n",
    "out = model(imagenet_transform(pil).unsqueeze(0))\n",
    "print(out.shape)\n",
    "pil.resize((224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать модель с CIFAR10, нужно поменять количество выходов слоя, отвечающих за классификацию. Так как в CIFAR10 десять классов, а в ImageNet тысяча.\n",
    "\n",
    "Чтобы понять, как получить доступ к последнему слою, выведем структуру модели:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что последний слой называется head и судя по количеству параметров на выходе (1000), которое совпадает с количеством классов ImageNet, именно он отвечает за классификацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим его слоем с 10-ю выходами по количеству классов в CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head = torch.nn.Linear(192, 10, bias = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что модель не сломалась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(imagenet_transform(pil).unsqueeze(0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузим CIFAR10 и проверим, как дообучится модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cifar10 = CIFAR10(root = \"./\", train=True, download=True, \n",
    "                   transform=imagenet_transform)\n",
    "\n",
    "# We use only part of CIFAR10 to reduce training time\n",
    "trainset, _ = torch.utils.data.random_split(cifar10, [10000,40000])\n",
    "trainloader = DataLoader(trainset, batch_size=128,shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CIFAR10(root = \"./\", train=False, download=True, \n",
    "                  transform=imagenet_transform)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Проведем стандартный цикл обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model,trainloader,optimizer,epochs = 1):\n",
    "  model.to(device)\n",
    "  model.train()\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  for epoch in tqdm_notebook(range(epochs)):  \n",
    "      for data in tqdm_notebook(trainloader):\n",
    "          inputs, labels = data\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(inputs.to(device))\n",
    "          loss = criterion(outputs, labels.to(device))\n",
    "          loss.backward()\n",
    "          optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дообучаем (fine tune) только последний слой модели, который мы изменили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.head.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model,trainloader,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим точность, на всей тестовой подвыборке CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def accuracy(model, testloader):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for data in testloader:\n",
    "      images, labels = data\n",
    "      outputs = model(images.to(device))\n",
    "      # the class with the highest energy is what we choose as prediction\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels.to(device)).sum().item()\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy of fine-tuned network : {accuracy(model,testloader):.2f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дообучив последний слой на одной эпохе с использованием 20% данных, мы получили точность ~0.75\n",
    "\n",
    "Если дообучить все слои на 2-х эпохах, можно получить точность порядка 0.95. \n",
    "\n",
    "Это результат намного лучше тем тот, что мы получали на семинарах.\n",
    "\n",
    "Для этого потребуется порядка 10 мин (на GPU). Сейчас мы этого делать не будем.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И одной из причин того, что обучение идет относительно медленно, является увеличение изображений размером 32x32 до 224x224. \n",
    "\n",
    "Если бы мы использовали изображения CIFAR10 в их родном размере, мы бы не потеряли никакой информации, но могли бы в разы ускорить обучение.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение размеров входа ViT\n",
    "\n",
    "На первый взгляд, ничего не мешает это сделать: self-attention слой работает с произвольным количеством входов.\n",
    "\n",
    "Давайте посмотрим, что будет, если подать на вход модели изображение отличное по размерам от 224x224.\n",
    "\n",
    "Для этого перезагрузим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "  model.head = torch.nn.Linear(192, 10, bias = True)\n",
    "  return model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И уберем из трансформаций Resize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transform = T.Compose([\n",
    "    # T.Resize((224,224)),    don't remove this line\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "# Change transformation in base dataset\n",
    "cifar10.transform = cifar_transform\n",
    "first_img = trainset[0][0]\n",
    "\n",
    "model.to(torch.device(\"cpu\"))\n",
    "try:\n",
    "  out = model(first_img.unsqueeze(0))\n",
    "except Exception as e:\n",
    "  print(\"Exception:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем ошибку.\n",
    "\n",
    "Ошибка возникает в объекте PatchEmbed, который судя по комментариям:\n",
    "\n",
    "`2D Image to Patch Embedding ` (/timm/models/layers/patch_embed.py) \n",
    "\n",
    "превращает изображение в набор эмбеддингов.\n",
    "\n",
    "У объекта есть свойство `img_size`, попробуем просто поменять его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.patch_embed.img_size = (32,32)\n",
    "try:\n",
    "  out = model(first_img.unsqueeze(0))\n",
    "except Exception as e:\n",
    "  print(\"Exception:\",e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем новую ошибку.\n",
    "\n",
    "И возникает она в строке  \n",
    "`x = self.pos_drop(x + self.pos_embed)`\n",
    "\n",
    "x - это наши новые эмбеддинги для CIFAR10 картинок\n",
    "\n",
    "Откуда взялось число 5?\n",
    "\n",
    "4 - это закодированные фрагменты (patch) для картинки 32х32 их всего 4 (16x16) + один embedding для предсказываемого класса(class embedding).\n",
    "\n",
    "А 197 это positional encoding - эмбеддинги кодирующие позицию элемента. Они остались от ImageNet.\n",
    "\n",
    "Так как в ImageNet картинки размера 224x224 то в каждой помещалось 14x14 = 196 фрагментов и еще embedding для класса, итого 197 позиций.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги для позиций доступны через свойство:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_embed.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам надо изменить количество pos embeddings так что бы оно было равно 5  (количество patch + 1). \n",
    "Возьмем 5 первых:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_embed.data = model.pos_embed.data[:,:5,:]\n",
    "out = model(first_img.unsqueeze(0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заработало!\n",
    "\n",
    "Теперь обучим модель. Так как изображения стали намного меньше, то мы можем увеличить размер batch и использовать весь датасет. Так же будем будем обучать все слои, а не только последний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10.transform = cifar_transform\n",
    "trainloader = DataLoader(cifar10, batch_size=512,shuffle=True, num_workers=2)\n",
    "\n",
    "# Now we train all parameters because model altered\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model,trainloader,optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильно быстрее. \n",
    "Посмотрим на результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.transform = cifar_transform\n",
    "print(f'Accuracy of altered network : {accuracy(model,testloader):.2f} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильно хуже. \n",
    "\n",
    "Это можно объяснить тем, что  маленькие patch  ImageNet(1/196) семантически сильно отличаются от четвертинок картинок из CIFAR10 (1/4).\n",
    "\n",
    "Но есть и другая причина: мы взяли лишь первые 4 pos_embedding а остальные выкинули. Они переобучаться, но двух эпох для этого мало. \n",
    "\n",
    "Зато теперь мы можем использовать модель с изображениями любого размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Статьти предшевствовавшие появлению ViT:\n",
    "\n",
    "[Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n",
    "\n",
    "\n",
    "[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор сети MLP-Mixer(2020 г.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы прошли с вами долгий путь: от первых свёрточных сетей типа `AlexNet` до супер новинок типа `Visual Transformer`. Но в 2021 году круг замкнулся и вышла статья [MLP-Mixer: An all-MLP Architecture for Vision (Tolstikhin et al., 2021)](https://arxiv.org/abs/2105.01601), в которой утверждается, что все эти свёртки и трансформеры — это, конечно, хорошо, но результатов, сравнимых с SOTA на распознавании образов, можно достичь и с помощью обычных Multi-Layer Preceptrons (они же `nn.Linear`).\n",
    "\n",
    "Конечно же, не обошлось без нюансов и просто настэкать линейные слои не поможет. Давайте разбираться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень хороший видео-разбор можно посмотреть [во влоге Yannic Kilcher](https://www.youtube.com/watch?v=7K4Z8RqjWIk&feature=youtu.be&ab_channel=YannicKilcher)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/img_license/mlp_mixer_architecture.png\"  width=\"700\"></center>\n",
    "<center><em>Архитектура сети MLP-Mixer </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, как и в ViT, мы будем разбивать исходное изображение на патчи, которые затем будем пропускать через полносвязанный слой (Fully-connected), чтобы получить латентное представление каждого патча. На этом сходство заканчивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы будем пропускать наши латентные представления через N Mixer-Layers. Предположим, у нас было 16 патчей, и мы решили, что размерность латентного вектора будет 128. Тогда получается, что на вход в Mixer мы подаём матрицу размером (16,128). Затем, мы нашу матрицу транспонируем и получаем (128,16).\n",
    "\n",
    "С простыми линейными слоями мы бы на этом и закончили. Развернули бы двухмерное представление в одномерное, и получили бы вектор размером (128*16). Но в MLP-Mixer мы поступим хитрее. Каждую (из 128) строку матрицы мы пропустим через один и тот же MLP 1 слой (веса расширены между MLP слоями). Затем мы снова транспонируем полученные вектора (128,16) -> (16,128), прибавим к ним изначальные латентные представления (вот и пригодилась идея Skip-Connections) и повторим операцию, но уже со слоем MLP 2.\n",
    "\n",
    "В конце агрегируем полученные вектора с помощью Global Average Pooling и навешиваем линейную голову для классификации на столько классов, сколько нам нужно (1000 в случае классического ImageNet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внимательный читатель может заметить, что всё это подозрительно напоминает 1x1 convolutions, и будет недалёк от истины. Споры о том, называть ли эти операции свёртками или не называть, не утихают и [по сей день](https://wandb.ai/wandb_fc/pytorch-image-models/reports/Is-MLP-Mixer-a-CNN-in-disguise---Vmlldzo4NDE1MTU).\n",
    "\n",
    "С точки зрения результатов — качество сравнимо с SOTA моделями, но чуть хуже. Скорость обучения — тоже где-то рядом. Но есть большой бонус — на скорости инференса MLP-Mixer значительно выигрывает. Да и в целом, вывод, который можно сделать из их модели: не обязательно городить сложнейший огород, можно использовать простые инструменты, главное заложить правильный *inductive bias*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\"> Бонус: Практические аспекты работы с размерностью данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частая проблема при написании архитектур — это посчитать промежуточные размеры свёрток. К счастью, эта проблема решается довольно простой функцией. \n",
    "\n",
    "На вход подаём наш размер (например, картинки), и размер, который мы хотим получить на выходе. Указываем количество слоёв и дополнительные параметры (например, нам нужны kernels меньше 4, страйды меньше, чем ядра, и страйды, равные 1, за исключением двух последних слоев)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_chain(input_size, output_size, depth, cond):\n",
    "    '''Written by Francois Fleuret <francois@fleuret.org>'''\n",
    "    if depth == 0:\n",
    "        if input_size == output_size:\n",
    "            return [ [ ] ]\n",
    "        else:\n",
    "            return [ ]\n",
    "    else:\n",
    "        r = [ ]\n",
    "        for kernel_size in range(1, input_size + 1):\n",
    "            for stride in range(1, input_size):\n",
    "                if cond(depth, kernel_size, stride):\n",
    "                    n = (input_size - kernel_size) // stride + 1\n",
    "                    if n >= output_size and (n - 1) * stride + kernel_size == input_size:\n",
    "                        q = conv_chain(n, output_size, depth - 1, cond)\n",
    "                        r += [ [ (kernel_size, stride) ] + u for u in q ]\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "c = conv_chain(\n",
    "    input_size = 64, output_size = 8,\n",
    "    depth = 5,\n",
    "    # We want kernels smaller than 4, strides smaller than the\n",
    "    # kernels, and strides of 1 except in the two last layers\n",
    "    cond = lambda d, k, s: k <= 4 and s <= k and (s == 1 or d <= 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(1, 1, 64, 64)\n",
    "\n",
    "# get all possible variants of models\n",
    "for num, m in enumerate(c):\n",
    "    model = nn.Sequential(*[ nn.Conv2d(1, 1, l[0], l[1]) for l in m ])\n",
    "    \n",
    "    print(\"Variant {}\".format(num))\n",
    "    print(model)\n",
    "    print(x.size(), model(x).size())\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Список использованной литературы\n",
    "\n",
    "<font size=\"5\"> AlexNet\n",
    "\n",
    "[ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky et al., 2012)](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "[Подробнее про AlexNet](https://neurohive.io/ru/vidy-nejrosetej/alexnet-svjortochnaja-nejronnaja-set-dlja-raspoznavanija-izobrazhenij/)\n",
    "\n",
    "<font size=\"5\"> ZFNet\n",
    "\n",
    "[Visualizing and Understanding Convolutional Networks (Zeiler et al., 2013)](https://arxiv.org/abs/1311.2901)\n",
    "\n",
    "<font size=\"5\"> VGG\n",
    "\n",
    "[Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan et al., 2014)](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "[Краткое описание VGGNet](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "\n",
    "<font size=\"5\">GoogLeNet\n",
    "\n",
    "[Going Deeper with Convolutions (Szegedy et al., 2014)](https://arxiv.org/abs/1409.4842)\n",
    "\n",
    "<font size=\"5\"> Global Average Pooling\n",
    "\n",
    "[Network In Network, Lin et al., 2013](https://arxiv.org/abs/1312.4400)\n",
    "\n",
    "[Блог-пост про Global Average Pooling](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/)\n",
    "\n",
    "<font size=\"5\"> ResNet\n",
    "\n",
    "[Deep Residual Learning for Image Recognition (He et al., 2015)](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<font size=\"5\"> Fixup Initialization\n",
    "\n",
    "[Fixup Initialization: Residual Learning Without Normalization (Zhang et. al, 2019)](https://arxiv.org/abs/1901.09321)\n",
    "\n",
    "[Блог-пост про Fixup Initialization](https://towardsdatascience.com/understanding-fixup-initialization-6bf08d41b427)\n",
    "\n",
    "<font size=\"5\"> ResNeXt\n",
    "\n",
    "[Aggregated Residual Transformations for Deep Neural Networks (Xie et al., 2016)](https://arxiv.org/abs/1611.05431)\n",
    "\n",
    "[Блог-пост про ResNeXt](https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac)\n",
    "\n",
    "<font size=\"5\"> DensNet\n",
    "\n",
    "[Densely Connected Convolutional Networks (Huang et al., 2016)](https://arxiv.org/abs/1608.06993)\n",
    "\n",
    "<font size=\"5\"> WideResNet\n",
    "\n",
    "[Wide Residual Networks (Zagoruyko et al., 2016)](https://arxiv.org/abs/1605.07146)\n",
    "\n",
    "\n",
    "<font size=\"5\"> SENet\n",
    "\n",
    "[Squeeze-and-Excitation Networks (Hu et al., 2017)](https://arxiv.org/abs/1709.01507)\n",
    "\n",
    "[Блог-пост про Squeeze-and-Excitation Networks](https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7)\n",
    "\n",
    "\n",
    "<font size=\"5\"> MobileNet\n",
    "\n",
    "[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard et al., 2017)](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "<font size=\"5\"> EfficientNet\n",
    "\n",
    "[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan et al., 2019)](https://arxiv.org/abs/1905.11946)\n",
    "\n",
    "<font size=\"5\"> Visual Transformer\n",
    "\n",
    "[Visual Transformers: Token-based Image Representation and Processing for Computer Vision (Wu et al., 2020)](https://arxiv.org/abs/2006.03677)\n",
    "\n",
    "[Блог-пост разбор  ViT](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\n",
    "\n",
    "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "<font size=\"5\"> MLP-Mixer\n",
    "\n",
    "[MLP-Mixer: An all-MLP Architecture for Vision (Tolstikhin et al., 2021)](https://arxiv.org/abs/2105.01601)\n",
    "\n",
    "[Разбор во влоге Yannic Kilcher](https://www.youtube.com/watch?v=7K4Z8RqjWIk&feature=youtu.be&ab_channel=YannicKilcher)\n",
    "\n",
    "[Блог-пост про MLP-Mixer](https://wandb.ai/wandb_fc/pytorch-image-models/reports/Is-MLP-Mixer-a-CNN-in-disguise---Vmlldzo4NDE1MTU)\n",
    "\n",
    "<font size=\"5\"> Разное\n",
    "\n",
    "[Блог-пост про различные типы сверток](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)\n",
    "\n",
    "[Benchmark Analysis of Representative\n",
    "Deep Neural Network Architectures (Bianco et al., 2018)](https://arxiv.org/abs/1810.00736)\n",
    "\n",
    "[Neural Architecture Search with Reinforcement Learning (Zoph et al., 2016)](https://arxiv.org/abs/1611.01578)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
