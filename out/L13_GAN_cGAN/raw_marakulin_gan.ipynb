{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация точек на параболе и вне неё\n",
    "Нужна выборка для классификации. Для этого используем синтетическую выборку - точки на параболе. Нарисуем параболу и сгенерируем точки на ней."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line makes matplotlib compatible with Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def calculate(x):\n",
    "    return x * x\n",
    "\n",
    "x = np.asarray(list(range(-10, 11, 2)))/20\n",
    "y = calculate(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть функция, а теперь нам нужно сделать случайные точки, лежащие на функции. Это будет наша обучающая выборка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import rand\n",
    " \n",
    "# generate randoms sample from x^2\n",
    "def generate_real_samples(n=100):\n",
    "    # generate random inputs in [-0.5, 0.5]\n",
    "    x = rand(n) - 0.5\n",
    "    y = calculate(x)\n",
    "    # stack arrays\n",
    "    x = x.reshape(n, 1)\n",
    "    y = y.reshape(n, 1)\n",
    "    return np.hstack((x, y))\n",
    " \n",
    "data = generate_real_samples()\n",
    "plt.scatter(data[:, 0], data[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако нам нужны и неправильные точки, которые не лежат на параболе. Это будет вторым классом объектов для нашего классификатора. Сгенерируем случайные точки в прямоугольнике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n fake samples\n",
    "def generate_fake_samples(n=100):\n",
    "    # generate inputs in [-1, 1]\n",
    "    x = -1 + rand(n) * 2\n",
    "    y = -1 + rand(n) * 2\n",
    "    # stack arrays\n",
    "    x = x.reshape(n, 1)\n",
    "    y = y.reshape(n, 1)\n",
    "    return np.hstack((x, y))\n",
    "\n",
    "fake = generate_fake_samples()\n",
    "plt.scatter(data[:, 0], data[:, 1], color='blue')\n",
    "plt.scatter(fake[:, 0], fake[:, 1], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор\n",
    "Определим модель классификатора. Параметры понятны из аргументов функций. Не будем вдаваться в подробности, но модель достаточно простая - всего 101 параметр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discriminator model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    " \n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# define the discriminator model\n",
    "model = define_discriminator()\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем слитую вместе выборку, указав классы (реальный объект, подделка). Теперь можно обучить модель и посмотреть как быстро она обучилась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets merge real objects and fakes\n",
    "print(\"Shape of real data is {} and shape of fakes is {}\".format(data.shape, fake.shape))\n",
    "x_train = np.vstack((data, fake))\n",
    "print(\"Resulting features shape (x_train) is {}\".format(x_train.shape))\n",
    "# Generate answers for our objects\n",
    "data_y = np.ones(len(data))\n",
    "fake_y = np.zeros(len(fake))\n",
    "# Merge answers the same way as features\n",
    "print(\"Answers input shape is {} and {} for real ones and fakes\".format(data_y.shape, fake_y.shape))\n",
    "print(\"Will merge them as well, but using other stacking as they have other shape structure\")\n",
    "y_train = np.hstack((data_y, fake_y))\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 200, verbose = 0)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel(\"binary_crossentropy\")\n",
    "plt.plot(history.epoch, history.history[\"accuracy\"])\n",
    "plt.xlim([0,max(history.epoch)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор\n",
    "Теперь у нас есть выборка, есть обученный классификатор и мы можем сделать генератор. Для этого выберем латентное пространство в виде гауссовского распределения с центром в нуле и дисперсией 0. Для примера нарисуем такое распределение для 2D случая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "latent_example_2d = generate_latent_points(2, 100)\n",
    "plt.scatter(latent_example_2d[:, 0], latent_example_2d[:, 1], color = \"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь определим сеть генератора, которая тоже будет параметризоваться количеством измерений. Выберем пятимерное латентное пространство. Слои примерно такие, как были в классификаторе. Выходов генератора по умолчанию 2, так как нам нужны 2 числа: x и y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    return model\n",
    "\n",
    "generator_model = define_generator(5)\n",
    "# summarize the model\n",
    "generator_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию генератора нашего GAN, задача которого будет подавать точки латентного пространства в сеть генератора и выводить x и y, которые в итоге должны научиться попадать на параболу. Мы делаем это по аналогии с функцией выше, где эти точки равномерно распределялись по прямоугольнику, что позволило обучить классификатор. Обучать генератор независимо мы не планируем, поэтому мы его не компилируем. Скомпилируем его вместе с классификатором в единую сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n)\n",
    "    # predict outputs\n",
    "    return generator.predict(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь возьмём необученную сеть генератора и сгенерируем и нарисуем пары x и y. Удобно сразу сделать функцию визуализации, чтобы там присутствовала и модельная кривая. Если сеть необучена, то мы ожидаем, что результат будет случайным облаком точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(generator, latent_dim, n=100, savename = None):\n",
    "    # prepare fake examples\n",
    "    x_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "    # build model dependence\n",
    "    x = np.asarray(list(range(-10, 11, 2)))/20\n",
    "    y = calculate(x)\n",
    "    plt.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "    x_min = min(x) - 0.5 * (max(x) - min(x))\n",
    "    x_max = max(x) + 0.5 * (max(x) - min(x))\n",
    "    y_min = min(y) - 0.5 * (max(y) - min(y))\n",
    "    y_max = max(y) + 0.5 * (max(y) - min(y))\n",
    "    plt.ylim([y_min, y_max])\n",
    "    plt.xlim([x_min, x_max])\n",
    "    plt.plot(x, y)\n",
    "    if savename is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(savename, format = \"png\")\n",
    "\n",
    "summarize_performance(generator_model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN\n",
    "По сути всё сделано, только сеть генератора вообще не тренирована, поэтому она выдала какой-то хаос из точек. Пришло время тренировать генератор. Для этого нужно соединить генератор и классификатор в единую цепь с латентным пространством на входе и бинарной классификацией на выходе. Только перед обучением запомним веса модели, чтобы можно было бы их восстановить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "gan_model = define_gan(generator_model, model)\n",
    "# summarize gan model\n",
    "gan_model.summary()\n",
    "gan_model.save_weights(\"output/gan_initial.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно выдавать вектора латентного пространства на вход и требовать, чтобы на выходе был класс точек параболы.  Если не получилось, то сеть будет обучаться, чтобы получилось, но обучаться будет только генераторная часть, ведь классификатор зафиксирован в настройках! Именно поэтому выборка включает только один класс. Сделаем для этого функцию обучения и запустим её. Загрузим веса из файла, хотя они и так загружены, но зато этот блок можно перезапускать и каждый раз будет обучаться с нуля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the composite model\n",
    "def train_gan(gan_model, latent_dim, n_epochs=10000, n_batch=128):\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        gan_model.train_on_batch(x_gan, y_gan)\n",
    "\n",
    "gan_model.load_weights(\"output/gan_initial.h5\")\n",
    "train_gan(gan_model, 5, n_epochs = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем результат. Для этого сгенерируем точки на параболе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_performance(generator_model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точки сгрудились вместе где-то в районе параболы. Чем дальше мы будем обучать генератор, тем больше он будет пытаться выдавать константу. Исправим это тем, что будем дообучать дискриминатор с обучением генератора в соревновательном режиме. Для этого определим соответствующую функцию обучения, которая будет принимать дополнительно модель генератора и дискриминатора, ведь дискриминатор мы собираемся доучивать, а генератор нужен, чтобы генерировать ошибочный класс для обучения дискриминатора. Алгоритм похож на алгоритм train_gan, но в цикле еще нужно сделать обучающую выборку из точек на на параболе и точек, генерируемых генератором (все функции уже написаны выше). На этой выборке нужно переобучить дискриминатор, а затем так же, как и раньше, обучить GAN. Это будет одна эпоха.\n",
    "В конце надо не забыть загрузить изначальные веса, чтобы сбросить сети в начальное необученное состояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128):\n",
    "    # type your code here ---------------------\n",
    "        \n",
    "gan_model.load_weights(\"output/gan_initial.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим обучение и посмотрим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(generator_model, model, gan_model, 5, n_epochs = 20000)\n",
    "summarize_performance(generator_model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось! Если не получилось, то запусти обучение еще раз и сеть дообучится. Наконец посмотрим процесс обучения по шагам. Для этого сделаем функцию вывода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "gan_model.load_weights(\"output/gan_initial.h5\")\n",
    "images = []\n",
    "epochs_total = 20000\n",
    "frames = 100\n",
    "for i in range (1, frames):\n",
    "    train(generator_model, model, gan_model, 5, n_epochs = int(epochs_total / frames))\n",
    "    summarize_performance(generator_model, 5, savename = \"output/tmp.png\")\n",
    "    plt.cla()\n",
    "    image = Image.open(\"output/tmp.png\")\n",
    "    ar = np.asarray(image)\n",
    "    images.append(ar)\n",
    "kargs = { 'duration': 0.1 }\n",
    "imageio.mimsave('output/gan_learning.gif', images, None, **kargs)\n",
    "summarize_performance(generator_model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выходные файлы с GIF анимацией находятся в папке output. Далее можно поэкспериментировать с настроечными параметрами обоих сетей, размерностью латентного пространства, методом обучения и т.д."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural1",
   "language": "python",
   "name": "neural1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}