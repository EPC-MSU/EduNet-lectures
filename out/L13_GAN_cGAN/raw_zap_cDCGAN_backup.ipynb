{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPfZ5YjPNHHa"
   },
   "outputs": [],
   "source": [
    "# Author: Alexander Iovlev. Some other sources may be used.\n",
    "# Edited by Sergey Zapunidi\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, Flatten, Dense, \\\n",
    "                                    Reshape, Conv2DTranspose, Dropout, \\\n",
    "                                    Embedding, Concatenate, Input\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr1LSWocO9nU"
   },
   "outputs": [],
   "source": [
    "opt=Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO!\n",
    "Все activation(LeakyReLU(alpha=0.2)) должны быть переделаны на отдельный слой активации: model.add(LeakyReLU(alpha = 0.2)). Иначе save в конце (gan_model.save('gan.h5') и подобные) работать не будут. Смотри https://github.com/keras-team/keras/issues/3816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_K4b82PtMixM"
   },
   "outputs": [],
   "source": [
    "def create_discriminator(in_shape=(28,28,1), n_classes=10):\n",
    "    # input layer\n",
    "    in_label = Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    # scale up to image dimensions\n",
    "    n_nodes = in_shape[0] * in_shape[1]\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    # image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "    merge = Concatenate()([in_image, li])\n",
    "    # downsample\n",
    "    fe = Conv2D(filters=128,\n",
    "                kernel_size=(3,3),\n",
    "                strides=(2,2),\n",
    "                padding='same',\n",
    "                activation=LeakyReLU(alpha=0.2)\n",
    "                )(merge)\n",
    "    fe = Conv2D(filters=128,\n",
    "                kernel_size=(3,3),\n",
    "                strides=(2,2),\n",
    "                padding='same',\n",
    "                activation=LeakyReLU(alpha=0.2)\n",
    "                )(fe)\n",
    "    # Flatten feature maps\n",
    "    fe = Flatten()(fe)  \n",
    "    # Regularization via dropout\n",
    "    fe = Dropout(0.4)(fe)\n",
    "    # Output\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    # Creating model\n",
    "    model = Model([in_image, in_label], out_layer)\n",
    "    model. compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHK9qwTgNkQW"
   },
   "outputs": [],
   "source": [
    "def create_generator(latent_dim=100, n_classes=10):\n",
    "    # label input\n",
    "    in_label = Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    li = Embedding(n_classes, 50)(in_label)\n",
    "    # linear multiplication\n",
    "    n_nodes = 7*7\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((7, 7, 1))(li)\n",
    "    # image generator input\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128*7*7\n",
    "    gen = Dense(n_nodes, activation=LeakyReLU(alpha=0.2))(in_lat)\n",
    "    gen = Reshape((7, 7, 128))(gen)\n",
    "    # merge image gen and label input\n",
    "    merge = Concatenate()([gen, li])\n",
    "    # upsampling to 14x14\n",
    "    gen = Conv2DTranspose(filters=128,\n",
    "                          kernel_size=(3,3),\n",
    "                          strides=(2,2),\n",
    "                          padding='same',\n",
    "                          activation=LeakyReLU(alpha=0.2)\n",
    "                          )(merge)\n",
    "    # updampling to 28x28\n",
    "    gen = Conv2DTranspose(filters=128,\n",
    "                          kernel_size=(3,3),\n",
    "                          strides=(2,2),\n",
    "                          padding='same',\n",
    "                          activation=LeakyReLU(alpha=0.2)\n",
    "                          )(gen)\n",
    "    # output\n",
    "    out_layer = Conv2D(filters=1,\n",
    "                       kernel_size=(7,7),\n",
    "                       activation='tanh',\n",
    "                       padding='same'\n",
    "                       )(gen)\n",
    "    # define model\n",
    "    model = Model([in_lat, in_label], out_layer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vNB3m0EoTQJL"
   },
   "outputs": [],
   "source": [
    "def create_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # get noise and label inputs from generator\n",
    "    gen_noise, gen_label = generator.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = generator.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = discriminator([gen_output, gen_label])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkvG-n_4RKMk"
   },
   "outputs": [],
   "source": [
    "def load_real_data(dataset):\n",
    "    # Loading data\n",
    "    (X_train, y_train), (X_test, y_test) = dataset.load_data()\n",
    "    # We don't need separate test data so we can add it to our training data\n",
    "    X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_test), axis=0)\n",
    "    # Conv2D awaits 3-dimensional input data (color channel) so, we must add 1 axis\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    # Rescale our data from [0, 255] to [-1,1]\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_train = (X_train - 127.5) / 127.5\n",
    "    return [X_train, y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_4NLWJoV9xx"
   },
   "outputs": [],
   "source": [
    "def gen_real_batch(dataset, n_samples):\n",
    "    X_train, y_train = dataset\n",
    "    # Generate n random indexes\n",
    "    ix = np.random.randint(0, X_train.shape[0], n_samples)\n",
    "    # Select n random images from our imageset\n",
    "    X = X_train[ix]\n",
    "    labels = y_train[ix]\n",
    "    y = np.ones(n_samples)\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oaQ-oYay4Y5N"
   },
   "outputs": [],
   "source": [
    "# Create n random latent-space vectors\n",
    "def gen_latent_vecs(n_samples, latent_dim=100, n_classes=10):\n",
    "    latent_vecs = np.random.normal(0, 1, [n_samples, latent_dim])\n",
    "    labels = np.random.randint(0, n_classes, n_samples)\n",
    "    return [latent_vecs, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OioOMAvbXvR9"
   },
   "outputs": [],
   "source": [
    "def gen_fake_batch(generator, n_samples, latent_dim=100, n_classes=10):\n",
    "    [latent_vecs, labels] = gen_latent_vecs(n_samples, latent_dim, n_classes)\n",
    "    X = generator.predict([latent_vecs, labels])\n",
    "    y = np.zeros(n_samples)\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpvZ_21JbPbU"
   },
   "outputs": [],
   "source": [
    "def gen_train_batch(generator, dataset, batch_size, latent_dim=100, n_classes=10):\n",
    "    half_batch_size = batch_size // 2\n",
    "    # Get a half batch of real samples\n",
    "    [X_real, real_labels], y_real = gen_real_batch(dataset, half_batch_size)\n",
    "    # Get a half batch of fake samples\n",
    "    [X_fake, fake_labels], y_fake = gen_fake_batch(generator, half_batch_size, \\\n",
    "                                                   latent_dim, n_classes)\n",
    "    # Make a mixed batch\n",
    "    X = np.concatenate((X_real, X_fake), axis=0)\n",
    "    y = np.concatenate((y_real, y_fake), axis=0)\n",
    "    labels = np.concatenate((real_labels, fake_labels), axis=0)\n",
    "    # Shuffle real and fake images\n",
    "    shuffle = np.random.randint(0, batch_size, batch_size)\n",
    "    X = X[shuffle]\n",
    "    y = y[shuffle]\n",
    "    labels = labels[shuffle]\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVTGH0jv6T8a"
   },
   "outputs": [],
   "source": [
    "def print_generated_images(generator, h, w, latent_dim=100):\n",
    "    n = h*w\n",
    "    X, _ = gen_latent_vecs(n, latent_dim)\n",
    "    labels = np.asarray([x for _ in range(h) for x in range(w)])\n",
    "    y_p = generator.predict([X, labels])\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(h, w, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"label: {}\".format(labels[i]))\n",
    "        plt.imshow(y_p[i, :, :, 0], cmap='gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_UKRC_iY7p3"
   },
   "outputs": [],
   "source": [
    "def train(generator, discriminator, gan, dataset, latent_dim, n_epoches=100, \\\n",
    "          batch_size=128, n_classes=10):\n",
    "    X_train, y_train = dataset\n",
    "    n_batches = X_train.shape[0] // batch_size\n",
    "    half_batch_size = batch_size // 2\n",
    "\n",
    "    for i in range(n_epoches):\n",
    "        for j in range(n_batches):\n",
    "            # Train discriminator\n",
    "            [X_d, labels_d], y_d = gen_train_batch(generator, dataset, \\\n",
    "                                                   batch_size, latent_dim)\n",
    "            d_loss, d_acc = discriminator.train_on_batch([X_d, labels_d], y_d)\n",
    "            # Train generator\n",
    "            [X_gan, labels] = gen_latent_vecs(batch_size, latent_dim)\n",
    "            y_gan = np.ones(batch_size)\n",
    "            g_loss = gan.train_on_batch([X_gan, labels], y_gan)\n",
    "\n",
    "            print('\\r>%d, %d/%d, d_l=%.3f, d_a=%.3f g=%.3f' % (i+1, j+1, n_batches, d_loss, d_acc, g_loss), end='')\n",
    "        print()\n",
    "        if i == 0 or i == 1 or (i+1) % 20 == 0:\n",
    "            print_generated_images(generator, 4, 10, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WKABGZMzafRg",
    "outputId": "c230f9e7-ea5b-459c-a890-5273e81c0374"
   },
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "discriminator = create_discriminator()\n",
    "generator = create_generator(latent_dim)\n",
    "gan_model = create_gan(generator, discriminator)\n",
    "dataset = load_real_data(mnist)\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim)\n",
    "# Save the generator model\n",
    "gan_model.save('output/gan.h5')\n",
    "generator.save('output/generator.h5')\n",
    "discriminator.save('output/discriminator.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnist_cDCGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neural1",
   "language": "python",
   "name": "neural1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
