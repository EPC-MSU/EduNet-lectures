{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN - Генеративно-состязательные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOC\n",
    "* Введение\n",
    "    * Задача генерации\n",
    "    * Latent space\n",
    "    * Наивный подход\n",
    "\n",
    "\n",
    "* GAN - Генеративно-состязательная сеть (англ. Generative adversarial network, сокращённо GAN)\n",
    "    * Дискриминатор\n",
    "    * GAN - принципы работы\n",
    "    * Пример генерации точек на параболе\n",
    "\n",
    "\n",
    "* DCGAN (Deep Convolutional Generative Adversarial Nets)\n",
    "    * DCGAN - GAN для генерации изображений\n",
    "    * deconv - обратные свёртки\n",
    "    * Пример генерации изборажений\n",
    "\n",
    "\n",
    "* cGAN - Условные порождающие состязательные сети (Conditional Generative Adversarial Nets)\n",
    "    * Принципы работы cGAN\n",
    "    * Модификации cGAN\n",
    "\n",
    "\n",
    "* GAN Zoo\n",
    "    * ProGAN - метод обучения сетей до высокого разрешения\n",
    "    * Domain Transfer Network - перенос стиля с помомщью GAN\n",
    "    * SRGAN и StackGANs - сети для увеличения разрешения\n",
    "    * pix2pix - генерация изображения по его рисунку или из сегментации\n",
    "    * Семантическая генерация - изображение по входному вектору из целевых свойств\n",
    "    * ControlGAN и Text to Image - генерация изображения по текстовому описанию\n",
    "\n",
    "* Заключение\n",
    "\n",
    "* Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда мы изучаем окружающий мир, то у нас нет ответов что представляет из себя окружающая действительность, однако, со временем, мы учимся распознавать образы, речь, движения, благодаря тому, что мы просто смотрим и слушаем! Если детям сказать \"Это собака\" - это ничтожно маленькая информация. \n",
    "\n",
    "Положим, если давать человеку бит в секунду, и, учитывая что человек живёт в среднем $10^9$ секунд, а так же то, что в мозге человека примерно $10^{14}$ нейронных связей, значит человек использует мозг на одну стотысячную. Откуда ему получить столько информации чтобы стать человеком? Изучать мир самому! Анализируя все входные источники своего тела: глаза, уши и так далее.\n",
    "\n",
    "Классы, к которым мы привыкли в курсе нейронных сетей, в изображении, которое воспринимают наши глаза отсутствуют. То есть преимущественно мы учимся без учителя. Однако мы способны к творчеству, мы можем писать картины, музыку. Придумывать и прогнозировать. Мы можем генерировать что-то новое.\n",
    "\n",
    "Как подойти к такой задаче с помощью нейронных сетей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поставим задачу генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы имеем неразмеченные данные, и хотим генерировать новые данные, которые будут удовлетворять следующим условиям:\n",
    "* Новые данные должны быть похожи на исходные\n",
    "* При этом не повторять их в точности (или повторять, при случайном стечении обстоятельств)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эволюция в генерации изображений лиц:\n",
    "\n",
    "**https://thispersondoesnotexist.com**\n",
    "\n",
    "<img src=\"face_evol.jpg\" alt=\"alttext\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку мы обусловились о случайности, то нам эту случайность нужно реализовать. Как это сделать с нейронной сетью? В сетях, которые мы проходили при подаче экземпляра и фиксированных весах всегда получается один и тот же результат. Случайно изменять веса сети? Есть способ лучше - давать в качестве входа случайных шум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![random_example.png](random_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему именно вектор? Почему не одно случайное число. Ответ прост: изменяя одно случайное число, мы будем перемещаться на очень небольшое расстояние и генерация будет однообразной. Поскольку мы физически не можем генерировать числа в бесконечных пределах, мы вынуждены использовать фиксированный диапазон.\n",
    "\n",
    "В одномерном случае, взяв диапазон [0,1] у нас всего одна размерность нашего перемещения.\n",
    "Если взять двухмерный, или трёхмерный случай, то у нас уже есть две и три **степени свободы**.\n",
    "\n",
    "То есть случайный шум большой размерности даёт нам больше объёма для генерации. Это называется **input latent space** - входное латентное простанство.\n",
    "\n",
    "Note: из-за неустоявшейся терминологии, случайное распределение на входе генератора называется латентным пространством, так же как и скрытое пространство в автоэнкодерах. Поэтому в этой лекции будем называть его **входным** латентным пространством. Так же встречается вариант: predefined latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Какую размерность и форму выбрать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длина вектора (размерность латентного пространства) выбирается больше, чем количество разных\n",
    "независимых свойств объекта, которые мы хотим получить. Если длина 0, то случайности\n",
    "нет и генератор будет всегда производить один и тот же объект. Если длина 1, то будет\n",
    "шкала, вдоль которой будут расположены, например, генерируемые изображения. Для генератора\n",
    "лиц, это будет, в лучшем случае, шкала от молодой женщины блондинки к пожилому мужчине\n",
    "брюнету. Лучший способ выбрать длину вектора - это посмотреть похожую задачу в публикациях,\n",
    "взять подобную размерность, и начать экспериментировать с размерностью оттуда.\n",
    "\n",
    "У большого латентного пространства есть минусы: увеличивая размерность латентного пространства мы можем расширить его настолько, что при обучении модели точек в этом латентром пространстве будет настолько мало, что в основном пространство будет состоять из пустот. Тогда модель будет крайне некачественно \"понимать\" что ей нужно генерировать в точке, которой не было на обучающей выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичный вопрос встаёт и по поводу формы распределения входного латентного пространства. Как мы знаем из лекции про обучение сети, инициализация весов и нормализация имеют существенный вклад в работу модели. Поэтому, принято использовать многомерное нормальное распределение для input latent space, потому что оно лучше взаимодействует с весами модели и увеличивает сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"MultivariateNormal.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный подход\n",
    "(как делать на практике не нужно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым тривиальным решением кажется подача случайного шума на вход и реальные точки параболы будем ожидать на выходе. Проверим как это будет работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x*x\n",
    "    return np.hstack((x.reshape(-1,1), y.reshape(-1,1))) # Создадим num корректных точек (x,y) на параболе\n",
    "\n",
    "pairs = gen_pair(100)\n",
    "plt.scatter(pairs[:,0], pairs[:,1])\n",
    "plt.title(\"Случайные точки на параболе, которые используем в качестве датасета.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим наши точки на параболе со случайным входным шумом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 10\n",
    "batch_size = 128\n",
    "ls = 3 # latent space\n",
    "\n",
    "noice = np.random.normal(size=(n_batches*batch_size, ls))\n",
    "noice = torch.tensor(noice, dtype=torch.float)\n",
    "print(f\"noice.shape: {noice.shape}\")\n",
    "\n",
    "xy_pair = gen_pair(num = n_batches*batch_size)\n",
    "xy_pair = torch.tensor(xy_pair, dtype=torch.float)\n",
    "print(f\"xy_pair.shape: {xy_pair.shape}\")\n",
    "\n",
    "dataset = TensorDataset(noice, xy_pair) # model inputs, model outputs\n",
    "\n",
    "len_tr = int(len(dataset)*0.8)\n",
    "len_tst = len(dataset) - len_tr\n",
    "trainset, testset = random_split(dataset,[len_tr,len_tst])\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loss(model,test_loader,loss_function):\n",
    "    with torch.no_grad():\n",
    "        loss_test_total = 0\n",
    "        for samples, labels in test_loader:\n",
    "            outputs = model(samples.cuda())\n",
    "            loss = loss_function(outputs, labels.cuda())\n",
    "            loss_test_total += loss.item()\n",
    "        return loss_test_total/len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простую модель, которая будет ожидать шум на вход, и генерировать точки на выходе. (Обратите внимание, что функция активации на последнем слое отсутствует, поскольку мы не ограничиваем наш генератор в каком-то диапазоне.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenModel(nn.Module):\n",
    "    def __init__(self, latent_space):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "model = GenModel(latent_space = ls)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.L1Loss().cuda()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_epoch = 0\n",
    "    for samples, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples.cuda())\n",
    "        loss = loss_function(outputs.cuda(), labels.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "        \n",
    "    loss_test = get_test_loss(model,test_loader,loss_function)\n",
    "    if epoch%100==0:\n",
    "        print(f\"Epoch={epoch} train_loss={loss_epoch/len(train_loader):.4} test_loss={loss_test:.4}\")\n",
    "        # test_image(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим результаты генерации на шуме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(model):\n",
    "    model.cpu()\n",
    "    noice = torch.tensor(np.random.normal(size=(1000, ls)), dtype=torch.float)\n",
    "    xy_pair_gen = model(noice)\n",
    "\n",
    "    xy_pair_gen = xy_pair_gen.detach().numpy()\n",
    "    plt.scatter(xy_pair_gen[:,0], xy_pair_gen[:,1])\n",
    "    plt.axis([-1, 1, 0, 1])\n",
    "    plt.show()\n",
    "    model.cuda()\n",
    "test_image(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Причина неудачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что модель обучается плохо. Давайте разберёмся почему. В одном сэмпле данных мы подаём конкретный шум и конкретную точку, в следующем сэмпле так же. Поскольку в качестве лосс-функции была выбрана L1Loss, то генерация модели по батчу будет устремлять градиент в усреднённую позицию, которая не будет соответствовать позиции точки на параболе. (Хотя в целом, это не нереально)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нужно придумать как сделать так, чтобы точки \"растолкать\" из области, где парабола отсутствует.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить проблему сильного разброса точек (оттолкнуть их от пустот) можно наказывать нейронную сеть не напрямую лосс функцией, а сетью, которая будет говорить нам что точка лежит на параболе или не лежит. Однако, проверять это условие анализируя пару x, y на принадлежность к параболе плохая идея. Потому что точки получаются из случайного шума и ожидать что они попадут точно в параболу мы не имеем права. Поскольку попадание должно быть выше чем точность типа данных (32-bit floating point для torch.float). Мы рискуем не получить в процессе обучения ни одной точки, попавшей в параболу. Нам нужно дейстовать мягче.\n",
    "\n",
    "Поэтому, определим сеть-классификатор точек (лежит/не лежит), которую назовём **дискриминатор или критик**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15,1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы имеем: генератор, выдающий точки то ли попадая в параболу, то ли не попадая в неё. А так же дискриминатор, который будет учиться их различать. Поэтому мы будем подавать на генератор синтетические правильные точки, считая их реальными, и точки, которые выдаёт генератор, считая их подделкой.\n",
    "\n",
    "Таким образом, генератор будет учиться подражать реальным данным, а дискриминатор будет учиться отличать реальные точки, от подделок. \n",
    "\n",
    "Мы пришли к идее генеративно-состязательных нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generative adversarial network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G - генератор\n",
    "\n",
    "D - дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-15.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математически - это игра двух игроков:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-16.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чёрным $P_x$ обозначено реальное распределение сэмплов, зелёным $P_g$ - результаты генерации генератора, а синим - разделение между реальными и сгенерированными образцами. В идеальном случае справа дискриминатор перестаёт различать реальные объекты и сгенерированные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-17.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теоретическая гарантия сходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-18.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-19.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Математическое описание процесса обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-20.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1701.07875.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе совместного конкурентного обучения, если система достаточно сбалансирована, достигается минимаксное состояние равновесия, в котором обе сети значительно улучшили своё качество, и теперь сгенерированные изображения могут быть использованы практически как настоящие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества GAN**\n",
    "* Теоретические гарантии сходимости\n",
    "* Можно обучать обычным SGD\n",
    "* Решает в явном виде задачу generative modeling\n",
    "* Но неявным образом (нейросети)\n",
    "\n",
    "\n",
    "**Недостатки GAN**\n",
    "* Нестабильное обучение\n",
    "* Очень долгая сходимость\n",
    "* Mode-collapsing\n",
    "* Generator/Discriminator starvation\n",
    "* Поиск оптимальных параметров - pure luck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Практический пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим две функции возвращающие батчи:\n",
    "\n",
    "Первая функция будет создавать случайный вектор на вход генератора и метками классов будут единицы, потому что мы хотим чтобы генератор давал такие точки, чтобы классификатор их распознавал как единицы (верные). Поэтому лосс функция будет подгонять веса генератора к единицам.\n",
    "\n",
    "Вторая функция будет давать батч на вход дискриминатора, смешивая:\n",
    "* Истиные точки на параболе (чтобы обучать дискриминатор)\n",
    "* Точки не на параболе (чтобы дискриминатор знал где \"не парабола\")\n",
    "* Выход генератора как неверный (чтобы генератор не коллапсировал в одну верную точку)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во второй функции можно закомментить разные типы точек (не забыв убрать их из z = torch.vstack) и посмотреть на результаты. Например, если убрать gan_pair, то генератор будет коллапсировать в одну точку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g_batch(batch_size, latent_dim):\n",
    "    g_input = torch.randn(size=(batch_size, latent_dim)).cuda()\n",
    "    labels = torch.ones(size=(batch_size,)).cuda()\n",
    "    return g_input, labels\n",
    "\n",
    "def get_real_mix_batch(batch_size, latent_dim, netG):\n",
    "    # Generate true pairs and true labels\n",
    "    x = torch.distributions.Uniform(-1, +1).sample((batch_size,)).cuda()\n",
    "    true_pair = torch.vstack((x, x*x)).T.cuda()\n",
    "    true_labels = torch.ones(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "    \n",
    "    # Generate fake uniform pairs and fake labels\n",
    "    x_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).cuda()\n",
    "    y_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).cuda()\n",
    "    fake_pair = torch.vstack((x_fake, y_fake)).T\n",
    "    fake_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "\n",
    "    # Generate points from generator and set labels as fake\n",
    "    gan_pair = netG(torch.randn(size=(batch_size, latent_dim)).cuda())\n",
    "    gan_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).cuda()\n",
    "    \n",
    "    # Stack all types of points\n",
    "    z = torch.vstack((torch.hstack((true_pair, true_labels)),\n",
    "                      torch.hstack((fake_pair, fake_labels)),\n",
    "                      torch.hstack((gan_pair, gan_labels))\n",
    "                     ))\n",
    "    # Shuffle\n",
    "    z=z[torch.randperm(z.size()[0])]\n",
    "    \n",
    "    # Split back to samples and labels\n",
    "    mixed_pairs = z[:, :2]\n",
    "    mixed_labels = z[:, 2]\n",
    "    return mixed_pairs, mixed_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства обернём в функции шаги backpropagation дискриминатора и генератора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netD_step(netD, batchD, loss_func, optimizer):\n",
    "    samples, labels = batchD\n",
    "    optimizer.zero_grad()\n",
    "    outputs = netD(samples.cuda())\n",
    "    loss = loss_func(outputs.cuda(), labels.unsqueeze(1).detach().cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netG_step(netD, netG, batchG, loss_func, optimizer):\n",
    "    samples, labels = batchG\n",
    "    optimizer.zero_grad()\n",
    "    outputs = netD(netG(samples.cuda()))\n",
    "    loss = loss_func(outputs.cuda(), labels.unsqueeze(1).detach().cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем каждую эпоху отображать что уже умеет генерировать генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen(netG, epoch=\"Not provided\"):\n",
    "    Gin, _ = get_g_batch(1000, latent_dim)\n",
    "    out = netG(Gin).cpu()\n",
    "    plt.scatter(out.detach().numpy()[:, 0], out.detach().numpy()[:, 1], color=\"blue\", s=1)\n",
    "    plt.title(f'Generator points. End of epoch= {epoch+1}', fontsize=10)\n",
    "    plt.axis([-1,1,-0.5,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку сети обучаются по очереди, то создадим два оптимайзера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 5\n",
    "batch_size = 128\n",
    "batch_per_epoch = 2000\n",
    "epochs = 5\n",
    "\n",
    "netG = GenModel(latent_dim).cuda()\n",
    "netD = DisModel().cuda()\n",
    "loss_func = nn.BCELoss().cuda()\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В цикле обучения будет создаваться батч данных (размера batch_size) для генератора, и батч из смеси точек (размера 3\\*batch_size) для дискриминатора. С балансом данных подаваемых на обе сети можно эксперементировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_per_epoch):\n",
    "            batchG = get_g_batch(batch_size, latent_dim)\n",
    "            batchD = get_real_mix_batch(batch_size, latent_dim, netG)\n",
    "            \n",
    "            netD.train(True)\n",
    "            netG.train(False)\n",
    "            netD_step(netD, batchD, loss_func, optD)\n",
    "            \n",
    "            netD.train(False)\n",
    "            netG.train(True)\n",
    "            netG_step(netD, netG, batchG, loss_func, optG)\n",
    "            \n",
    "        # clear_output()\n",
    "        plot_gen(netG, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть отлично обучилась при подаче в дискриминатор всех типов точек. Можно отключить некоторые типы и посмотреть что получится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN - Генерация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью GAN можно, разумеется, генерировать не только точки на параболе. Можно генерировать, например, изображения. Но поднимаются закономерные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как из шума на входе сети получить изображение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым простым ответом будет: взять шум, пропустить его через полносвязные слои и сделать rashape до нужного разрешения. В целом, это будет работать.\n",
    "\n",
    "\n",
    "Однако **DCGAN - Deep Convolutional GAN** использует сверточные и сверточно-транспонированные (convolutional and convolutional-transpose) слои в дискриминаторе и генераторе соответственно. Впервые метод DCGAN был описан Рэдфордом и др. в статье \"Обучение неконтролируемому представлению с помощью Глубоких Сверточных генеративных состязательных сетей\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"dcgan_arcit.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже видна разница в генерации при помощи исключительно полносвязных слоёв и при помощи обратных свёрток. Очевидно, результат DCGAN лучше чем GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"gan_dcgan.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Архитектура DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход порождающей сети поступает шум, который по факту представляет из себя вектор произвольной размерности. Отсюда следует, что нет никакой необходимости расчленять его на признаки с помощью сверточных слоев в отличие от дискриминатора, а требуется, наоборот, преобразовать в результирующее изображение. Для выполнения этой задачи будут использоваться так называемые разверточные (deconvolutional) слои, которые по факту являются обратными к сверточным. Принцип их действия заключается в том, что они дополняют вход нулями до матрицы необходимого размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"dcgan.PNG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как работают обратные свёртки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратные свёртки/Развёртки (deconvolutional, convolutional-transpose). Входное разрешение дополняется нулями (подобно паддингу) до такого разрешения, чтобы выходное разрешение было больше входного. Вы можете возразить: но ведь сильно увеличить изображение не получится! Для этого слоёв ставится несколько, чтобы каждый новый слой лучше восстанавливал то, что в него подают. Итеративно увеличивая разрешения, ядра свёрток учатся генерировать изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"tconv_1.JPG\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"tconv_1_padding.JPG\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((1, 3, 10, 10)) # Like one 3-channal image with 10x10 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convT = nn.ConvTranspose2d(3, 8, kernel_size=3)\n",
    "y = convT(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape # One 8-chanells image with 12x12 size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"convtransponce_doc.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример обученного DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "\n",
    "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_images = 16\n",
    "noise, _ = model.buildNoiseData(num_images)\n",
    "with torch.no_grad():\n",
    "    generated_images = model.test(noise)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16*3, 2*3))\n",
    "ax.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy(), interpolation='nearest', aspect='equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cGAN - GAN с условием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cGAN расшифровывается как Conditional Generative Adversarial Net - это GAN с условием. Условие может быть любым, например, генерация конкретной цифры. В этом случае нам нужен уже размеченный датасет, для того чтобы обучить дискриминатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"cGAN_switcher.JPG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уловием в данном случае будет label, который на рисунке обозначен **Y**. Label добавляется к случайному шуму, тем самым мы говорим генератору генерировать случайное изображение нужного класса. Так же он подаётся в дискриминатор в качестве входа, чтобы дискриминатор знал какое изображение классифицировать как реальное, а какое как вымышленное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение в данном случае будет аналогичным обучению GAN, мы будем обучать сети, чередуя реальные данные и сгенерированные, добавив label."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"cGANS_results.JPG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модификации cGAN\n",
    "\n",
    "Метки классов можно подавать не только способом, описанным выше. Можно вместо подачи их в дискриминатор сделать так, чтобы он их предсказывал - **Semi-Supervized GAN**.\n",
    "\n",
    "Или же не подавать label в дискриминатор, но ждать от него классификации в соответствии с классом, который мы хотим получить от генератора - это **InfoGAN**\n",
    "\n",
    "Ещё одна модификация cGAN - это **AC-GAN** (auxiliary classifier) в которой единственное различие заключается в том, что дискриминатор дожен помимо распознавания реальных и фейковых изображений ещё и классифицировать их. Он имеет эффект стабилизации процесса обучения и позволяет генерировать большие высококачественные изображения, изучая представление в скрытом пространстве, которое не зависит от метки класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"gans_zoo.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Zoo\n",
    "\n",
    "GAN моделей настолько много, что нет смысла рассказывать о всех. Так или иначе, сейчас многие новые нейросети используют принципы GAN для обучения. Будь-то распознавание или сегментация.\n",
    "\n",
    "https://paperswithcode.com/methods/category/generative-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProGAN\n",
    "\n",
    "https://arxiv.org/pdf/1710.10196.pdf\n",
    "\n",
    "\"PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION\" - метод, призванный ускорить обучение сети генерации изображений с помощью добавления слоёв во время генерации. Сначала сеть учится генерировать изображения низкого разрешения, потом добавляется слой, который создаёт более высокое разрешение из фич предыдушего слоя. Эта идея позволяет плавно обучать сеть и добиваться лучшей сходимости.\n",
    "\n",
    "<img src =\"ProGAN.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-23.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain transfer network\n",
    "\n",
    "https://arxiv.org/abs/1611.02200\n",
    "\n",
    "Domain transfer network (DTN)- сеть, совмещающая в себе автоэнкодер, и gan для задачи переноса стиля.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-27.png\" width=\"600\">\n",
    "\n",
    "#### Результаты\n",
    "\n",
    "From SVHN to MNIST\n",
    "\n",
    "<img src =\"domain1.png\" width=\"600\">\n",
    "\n",
    "From Photos to Emoji \n",
    "\n",
    "<img src =\"domain1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRGAN и StackGANs\n",
    "\n",
    "https://arxiv.org/pdf/1609.04802v5.pdf\n",
    "\n",
    "SRGAN (Super resolution GAN), StackGANs - сети для повышения разрешения изображений, отличающаяся от других сетей повышенным качеством восстановления мелких деталей и текстур изображения.\n",
    "\n",
    "<img src =\"SRGAN.jpg\" width=\"700\">\n",
    "\n",
    "Сравнение с другими методами повышения разрешения:\n",
    "\n",
    "<img src =\"SRGAN_res.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pix2Pix\n",
    "\n",
    "https://arxiv.org/abs/1611.07004v3\n",
    "\n",
    "Pix2Pix - сети, переводящая пиксельные рисунки в реалистичные изображения. Она тоже использует принципы GAN для работы.\n",
    "\n",
    "<img src =\"pix2pix.png\" width=\"500\">\n",
    "\n",
    "Попробовать сеть можно на сайте:\n",
    "\n",
    "**https://affinelayer.com/pixsrv/**\n",
    "\n",
    "**https://affinelayer.com/pix2pix/**\n",
    "\n",
    "Ещё примеры:\n",
    "\n",
    "<img src =\"pix2pix2.jpg\" width=\"500\">\n",
    "<img src =\"pix2pix3.jpg\" width=\"500\">\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-38.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семантическая генерация\n",
    "\n",
    "Помимо шума, в модель можно подавать описание того что мы хотим получить. Это описание может быть разным, например текст или вектор заданных свойств.\n",
    "\n",
    "Например:\n",
    "* Класс объекта\n",
    "* Углы и повороты\n",
    "* Заданные параметры трансформаций\n",
    "* Сегментация\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-06.png\" width=\"700\">\n",
    "\n",
    "Результат:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-07.png\" width=\"300\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to image\n",
    "\n",
    "https://arxiv.org/abs/2001.06658\n",
    "\n",
    "### ControlGAN\n",
    "\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641270\n",
    "\n",
    "https://github.com/mrlibw/ControlGAN\n",
    "\n",
    "Описание может быть представлено в виде естественного текста. **ControlGAN** - Pytorch реализация  для управляемого преобразования текста в изображение. Цель состоит в том, чтобы генерировать изображения из текста, а также позволить пользователю манипулировать синтетическими изображениями, используя описания на естественном языке, в одной структуре.\n",
    "\n",
    "<img src =\"controlGAN.jpg\" width=\"500\">\n",
    "\n",
    "Результаты:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-31.png\" width=\"500\">\n",
    "\n",
    "\n",
    "Поэкспериментировать :\n",
    "\n",
    "**https://deepai.org/machine-learning-model/text2img**\n",
    "\n",
    "Одна из возможных архитектур с использованием RNN на входе:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-32.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принципы генеративно-состязательных сетей вышли далеко за пределы генерации из шума. Сейчас с помощью GAN создаются сложнейшие state-of-art сети для самых разнообразных задач. В лекции были рассмотрены самые главные модели: GAN - для генерации из шума, DCGAN - для генерации изображений, с помомщью развёрток и cGAN - сети с генерацией по условию. \n",
    "\n",
    "Совмещая различные сети и подходы можно сконструировать множество самых разнообразных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cDCGAN**\n",
    "\n",
    "https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StackGAN**\n",
    "\n",
    "https://arxiv.org/pdf/1612.03242v2.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1710.10916.pdf\n",
    "\n",
    "https://medium.com/@rangerscience/lets-read-science-stackgan-text-to-photo-realistic-image-synthesis-4562b2b14059\n",
    "\n",
    "https://www.rulit.me/data/programs/resources/pdf/Generative-Adversarial-Networks-with-Python_RuLit_Me_610886.pdf\n",
    "\n",
    "Видео:\n",
    "\n",
    "https://www.youtube.com/watch?v=PXWIaLE7_NU\n",
    "\n",
    "https://www.youtube.com/watch?v=crI5K4RCZws\n",
    "\n",
    "https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ControlGAN**\n",
    "\n",
    "https://github.com/mrlibw/ControlGAN\n",
    "\n",
    "https://github.com/taki0112/ControlGAN-Tensorflow\n",
    "\n",
    "https://arxiv.org/pdf/1708.00598.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1909.07083.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1910.05774.pdf\n",
    "\n",
    "https://meta-guide.com/data/data-processing/text-to-image-systems/natural-language-text-to-image-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AC-GAN**\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/\n",
    "\n",
    "https://towardsdatascience.com/understanding-acgans-with-code-pytorch-2de35e05d3e4\n",
    "\n",
    "https://arxiv.org/pdf/1909.05370.pdf\n",
    "\n",
    "https://openaccess.thecvf.com/content/WACV2021/papers/Kavalerov_A_Multi-Class_Hinge_Loss_for_Conditional_GANs_WACV_2021_paper.pdf\n",
    "\n",
    "https://github.com/lukedeo/keras-acgan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
