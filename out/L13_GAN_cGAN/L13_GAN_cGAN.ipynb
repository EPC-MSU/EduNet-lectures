{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN - Генеративно-состязательные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOC\n",
    "* Введение\n",
    "    * Задача генерации\n",
    "    * Latent space\n",
    "    * Наивный подход\n",
    "\n",
    "\n",
    "* GAN - Генеративно-состязательная сеть (англ. Generative adversarial network, сокращённо GAN)\n",
    "    * Дискриминатор\n",
    "    * GAN - принципы работы\n",
    "    * Пример генерации точек на параболе\n",
    "\n",
    "\n",
    "* DCGAN (Deep Convolutional Generative Adversarial Nets)\n",
    "    * DCGAN - GAN для генерации изображений\n",
    "    * deconv - обратные свёртки\n",
    "    * Пример генерации изборажений\n",
    "\n",
    "\n",
    "* cGAN - Условные порождающие состязательные сети (Conditional Generative Adversarial Nets)\n",
    "    * Принципы работы cGAN\n",
    "    * Модификации cGAN\n",
    "\n",
    "\n",
    "* GAN Zoo\n",
    "    * ProGAN - метод обучения сетей до высокого разрешения\n",
    "    * Domain Transfer Network - перенос стиля с помомщью GAN\n",
    "    * SRGAN и StackGANs - сети для увеличения разрешения\n",
    "    * pix2pix - генерация изображения по его рисунку или из сегментации\n",
    "    * Семантическая генерация - изображение по входному вектору из целевых свойств\n",
    "    * ControlGAN и Text to Image - генерация изображения по текстовому описанию\n",
    "\n",
    "* Заключение\n",
    "\n",
    "* Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда мы изучаем окружающий мир, то у нас нет ответов что представляет из себя окружающая действительность, однако, со временем, мы учимся распознавать образы, речь, движения, благодаря тому, что мы просто смотрим и слушаем!\n",
    "\n",
    "Положим, если давать человеку бит в секунду, и, учитывая что человек живёт в среднем $10^9$ секунд, а так же то, что в мозге человека примерно $10^{14}$ нейронных связей, значит человек использует мозг на одну стотысячную. Откуда ему получить столько информации чтобы стать человеком? Изучать мир самому! Анализируя все входные источники своего тела: глаза, уши и так далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В курсе нейронных сетей, мы привыкли что изображения размечены, однако в жизни такие метки, отсутствуют. То есть преимущественно мы учимся без учителя. Однако мы способны к творчеству, мы можем писать картины, музыку. Придумывать и прогнозировать. Мы можем генерировать что-то новое.\n",
    "\n",
    "Как подойти к такой задаче с помощью нейронных сетей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поставим задачу генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы имеем неразмеченные данные, и хотим генерировать новые данные, которые будут удовлетворять следующим условиям:\n",
    "* Новые данные должны быть похожи на исходные.\n",
    "* Но не повторять их в точности (или повторять, при случайном стечении обстоятельств).\n",
    "* Чтобы результат генерации был различным при разных запусках, нам нужен элемент случайности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эволюция в генерации изображений лиц:\n",
    "\n",
    "[Множество примеров различных генераторов](https://thisxdoesnotexist.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_0.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку мы обусловились о случайности, то нам эту случайность нужно реализовать. Если вспомнить сети, которые мы проходили, то всегда если мы подавали один и тот же вход, то получали один и тот же выход. Как реализовать случайность в нейронной сети? Случайно изменять веса сети? Есть способ лучше - давать в качестве входа случайных шум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему именно вектор? Почему не одно случайное число. Ответ таков: изменяя одно случайное число, мы будем перемещаться на очень небольшое расстояние и генерация будет однообразной. Поскольку мы физически не можем генерировать числа в бесконечных пределах, мы вынуждены использовать фиксированный диапазон.\n",
    "\n",
    "В одномерном случае, взяв диапазон [0,1] у нас всего одна размерность нашего перемещения.\n",
    "Если взять двухмерный, или трёхмерный случай, то у нас уже есть две и три **степени свободы**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть случайный шум большой размерности даёт нам больше объёма для генерации. Это называется **input latent space** - входное латентное простанство.\n",
    "\n",
    "Note: из-за неустоявшейся терминологии, случайное распределение на входе генератора называется латентным пространством, так же как и скрытое пространство в автоэнкодерах. Поэтому в этой лекции будем называть его **входным** латентным пространством. Так же встречается вариант: predefined latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Какую размерность и форму выбрать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длина вектора (размерность латентного пространства) выбирается больше, чем количество разных независимых свойств объекта, которые мы хотим получить. Если длина 0, то случайности нет и генератор будет всегда производить один и тот же объект. Если длина 1, то будет шкала, вдоль которой будут расположены, например, генерируемые изображения. Для генератора лиц, это будет, в лучшем случае, шкала от молодой женщины блондинки к пожилому мужчине брюнету. Лучший способ выбрать длину вектора - это посмотреть похожую задачу в публикациях, взять подобную размерность, и начать экспериментировать с размерностью оттуда."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У большого латентного пространства есть минусы: увеличивая размерность латентного пространства мы можем расширить его настолько, что при обучении модели точек в этом латентром пространстве будет настолько мало, что в основном пространство будет состоять из пустот. Тогда модель будет крайне некачественно \"понимать\" что ей нужно генерировать в точке, которой не было на обучающей выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичный вопрос встаёт и по поводу формы распределения входного латентного пространства. Как мы знаем из лекции про обучение сети, инициализация весов и нормализация имеют существенный вклад в работу модели. Поэтому, принято использовать многомерное нормальное распределение для input latent space, потому что оно лучше взаимодействует с весами модели и увеличивает сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный подход\n",
    "(как делать на практике НЕ нужно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем создать генератор точек на параболе. Самым тривиальным решением кажется подача случайного шума на вход сети и будем ожидать на выходе точки параболы. Проверим как это будет работать."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = \"cuda\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x*x\n",
    "    return np.hstack((x.reshape(-1,1), y.reshape(-1,1))) # Создадим num корректных точек (x,y) на параболе\n",
    "\n",
    "pairs = gen_pair(100)\n",
    "plt.scatter(pairs[:,0], pairs[:,1])\n",
    "plt.title(\"Случайные точки на параболе,\\nкоторые будем используем в качестве датасета.\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём размерность входного латентного пространства $ls = 3$ и объединим шум с точками в датасеты."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_batches = 10\n",
    "batch_size = 128\n",
    "ls = 3 # latent space\n",
    "\n",
    "noise = np.random.normal(size=(n_batches*batch_size, ls))\n",
    "noise = torch.tensor(noise, dtype=torch.float)\n",
    "print(f\"NN Input: noise.shape: {noise.shape}\")\n",
    "\n",
    "xy_pair = gen_pair(num = n_batches*batch_size)\n",
    "xy_pair = torch.tensor(xy_pair, dtype=torch.float)\n",
    "print(f\"NN Output: xy_pair.shape: {xy_pair.shape}\")\n",
    "\n",
    "dataset = TensorDataset(noise, xy_pair) # model inputs, model outputs\n",
    "\n",
    "len_tr = int(len(dataset)*0.8)\n",
    "len_tst = len(dataset) - len_tr\n",
    "trainset, testset = random_split(dataset,[len_tr,len_tst])\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_test_loss(model,test_loader,loss_function):\n",
    "    with torch.no_grad():\n",
    "        loss_test_total = 0\n",
    "        for samples, labels in test_loader:\n",
    "            outputs = model(samples.to(device))\n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss_test_total += loss.item()\n",
    "        return loss_test_total/len(test_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простую модель, которая будет ожидать шум на вход, и генерировать точки на выходе. (Обратите внимание, что функция активации на последнем слое отсутствует, поскольку мы не ограничиваем наш генератор в каком-то диапазоне.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class GenModel(nn.Module):\n",
    "    def __init__(self, latent_space):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "epochs = 300\n",
    "model = GenModel(latent_space = ls)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.L1Loss().to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_epoch = 0\n",
    "    for samples, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples.to(device))\n",
    "        loss = loss_function(outputs.to(device), labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "        \n",
    "    loss_test = get_test_loss(model,test_loader,loss_function)\n",
    "    if epoch%100==0:\n",
    "        print(f\"Epoch={epoch} train_loss={loss_epoch/len(train_loader):.4} test_loss={loss_test:.4}\")\n",
    "        # test_image(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим результаты генерации на шуме"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def test_image(model, ls = 3):\n",
    "    model.cpu()\n",
    "    noice = torch.tensor(np.random.normal(size=(1000, ls)), dtype=torch.float)\n",
    "    xy_pair_gen = model(noice)\n",
    "\n",
    "    xy_pair_gen = xy_pair_gen.detach().numpy()\n",
    "    plt.scatter(xy_pair_gen[:,0], xy_pair_gen[:,1])\n",
    "    plt.axis([-1, 1, 0, 1])\n",
    "    plt.show()\n",
    "    model.to(device)\n",
    "test_image(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Причина неудачи**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, модель обучается плохо. Нужно придумать как сделать так, чтобы точки \"растолкать\" из области, где парабола отсутствует. (Хотя в целом, обучать таким способом реально)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить проблему сильного разброса точек (оттолкнуть их от пустот) можно наказывать нейронную сеть не напрямую лосс функцией, а сетью, которая будет говорить нам что точка лежит на параболе или не лежит. Однако, проверять это условие анализируя пару x, y на принадлежность к параболе плохая идея. Потому что точки получаются из случайного шума и ожидать что они попадут точно в параболу мы не имеем права. Поскольку попадание должно быть выше чем точность типа данных (32-bit floating point для torch.float). Мы рискуем не получить в процессе обучения ни одной точки, попавшей в параболу. Нам нужно дейстовать мягче.\n",
    "\n",
    "Поэтому, определим сеть-классификатор точек (лежит/не лежит), которую назовём **дискриминатор или критик**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15,1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы имеем: генератор, выдающий точки то ли попадая в параболу, то ли не попадая в неё. А так же дискриминатор, который будет учиться их различать. Поэтому мы будем подавать на генератор синтетические правильные точки, считая их реальными, и точки, которые выдаёт генератор, считая их подделкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, генератор будет учиться подражать реальным данным, а дискриминатор будет учиться отличать реальные точки, от подделок. \n",
    "\n",
    "Мы пришли к идее генеративно-состязательных нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generative adversarial network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2014 Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (**Cited by 33430!!!**)\n",
    "\n",
    "[Видео разбор оригинальной статьи](https://youtu.be/eyxmSmjmNS0)\n",
    "\n",
    "[Видео лекции самого Иана](https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks)\n",
    "\n",
    "Генеративно-состязательную сеть описал Иан Гудфеллоу из компании Google в 2014 году. Сейчас он возглавляет подразделение машинного обучения в Apple. Принцип состязательности в сети GAN нередко описывается через метафоры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор - фальшивомонетчик!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**НЕ УВЕРЕН КУДА ПОЙДЕТ. ЗАМЕТКИ:**\n",
    "\n",
    "Еще со времен AlexNet мы знаем, что если мы что-то и умеем делать с нейросетями - так это классификаторы. В случае дискриминатора в классическом GAN - мы выполняем просетйшую из задач классификации - бинарную классификацию (либо *real*, либо *fake*). А вот задача генерации каким-то прямым образом на тот момент решена не была.\n",
    "\n",
    "Что если мы будем использовать всю мощь классификатора, для того, что бы получить генератор?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим что у нас есть некий фальшивомонетчик и некий банкир. Назовем нашего фальшивомонетчика $G$ (или generator), а банкира — $D$ (или discriminator). У нас есть какое-то количество оригинальных денег $X$ для банкира, и пусть на выходе у него будет число диапазоном от нуля до единицы, чтобы оно выражало уверенность банкира в том, что выданные ему на рассмотрение деньги настоящие. Еще — поскольку фальшивомонетчик у нас нейронная сеть, ей нужны какие-то входные данные, назовем их $z$ - это наш случайный шум, который модель будет стараться превратить в деньги."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, очевидно, цель фальшивомонетчика — это максимизировать $D(G(z))$, то есть сделать так, чтобы банкир был уверен, что подделки — настоящие.\n",
    "\n",
    "Цель банкира посложнее — ему нужно одновременно положительно опознавать оригиналы, и отрицательно — подделки. Запишем это как максимизацию $D(x)(1-D(G(z)))$. Умножение можно превратить в сложение, если взять логарифм, поэтому получаем:\n",
    "\n",
    "Для банкира: максимизировать $log(D(X))+log(1-D(G(z)))$\n",
    "\n",
    "Для фальшивомонетчика: максимизировать $log(D(G(z)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математически - это игра двух игроков:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@markdown Запусти меня, что бы посмотреть на уравнение\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "mark1 {\n",
    "  background: linear-gradient(-100deg, hsla(48,92%,75%,.3), hsla(48,92%,75%,.7) 95%, hsla(48,92%,75%,.1));\n",
    "  border-radius: 1em 0;\n",
    "  padding: .5rem;\n",
    "}\n",
    "\n",
    "mark2 {\n",
    "  background: linear-gradient(-100deg, hsla(10,92%,75%,.3), hsla(10,92%,75%,.7) 95%, hsla(10,92%,75%,.1));\n",
    "  border-radius: 1em 0;\n",
    "  padding: .5rem;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<script type=\"text/javascript\" async\n",
    "  src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "</script>\n",
    "\n",
    "<script>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}})</script>\n",
    "\n",
    "$\\min\\limits_{\\Theta_g}  \\max\\limits_{\\Theta_d} [\\mathbb{E}_{x _\\sim p(data)} \n",
    "log$ <mark1>$(D_{\\Theta_d}(x)$</mark1>+$\\mathbb{E}_{z _\\sim p(z)} \n",
    "log(1-$<mark2>$D_{\\Theta_d}(G_{\\Theta_g}(z))$</mark2>)]\n",
    "\n",
    "<br>\n",
    "<p><mark1>Вероятность дискриминатора для real data x</mark1></p>\n",
    "<br>\n",
    "<p><mark2>Вероятность дискриминатора для сгенерированной fake data G(z)</mark2></p>\n",
    "\n",
    "\"\"\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы знаем, что дискриминатор максимизирует эту функцию относительно своих параметров $\\theta_d$, где при фиксированном генераторе ${G}_{\\theta_{g}}$ он выполняет двоичную классификацию: он присваивает вероятность 1 точкам данных из обучающего набора $E_{x∼p_{data}}$ и присваивает вероятность 0 сгенерированным выборкам $E_{z∼p(z)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку используется подход из теории игр, наша лосс-функция представлена как минимаксная функция. Дискриминатор пытается максимизировать целевую функцию, поэтому мы можем выполнить градиентный подъем целевой функции. Генератор пытается минимизировать целевую функцию, поэтому мы можем выполнить градиентный спуск для целевой функции. Посредством чередования градиентного подъема и спуска сеть можно обучить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный подъем на дискриминаторе:\n",
    "\n",
    "\n",
    "$\\max\\limits_{\\theta_d} [\\mathbb{E}_{x _\\tilde{}p(data)} log(D_{\\theta_d}(x)+\\mathbb{E}_{z _\\tilde{}p(z)} log(1-D_{\\theta_d}(G_{\\theta_g}(z)))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный подъем на генераторе:\n",
    "\n",
    "\n",
    "$\\min\\limits_{\\theta_g} \\mathbb{E}_{z _\\tilde{}p(z)} log(1-D_{\\theta_d}(G_{\\theta_d}(z)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако вместо того, чтобы минимизировать вероятность того, что дискриминатор будет правильным, мы максимизируем вероятность того, что дискриминатор ошибается. Поэтому мы выполняем градиентный подъем на генераторе в соответствии с этой целевой функцией.\n",
    "\n",
    "$\\max\\limits_{\\theta_g} \\mathbb{E}_{z _\\tilde{}p(z)} log(D_{\\theta_d}(G_{\\theta_d}(z)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе совместного конкурентного обучения, если система достаточно сбалансирована, достигается минимаксное состояние равновесия, в котором обе сети значительно улучшили своё качество, и теперь сгенерированные изображения могут быть использованы практически как настоящие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**НЕ УВЕРЕН КУДА ПОЙДЕТ. ЗАМЕТКИ:**\n",
    "\n",
    "Если хорошенько подумать - то можно придти к выводу, что loss функция в GAN это не какая-то функция заданная людьми, а еще одна нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее о математике GAN-ов можно почитать в статье [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества GAN**\n",
    "* Теоретические гарантии сходимости\n",
    "* Можно обучать обычным SGD/Adam\n",
    "* Решает в явном виде задачу generative modeling\n",
    "* Но неявным образом (нейросети)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Недостатки GAN**\n",
    "* Нестабильное обучение\n",
    "* Очень долгая сходимость\n",
    "* Mode-collapsing\n",
    "* Generator/Discriminator starvation\n",
    "* Поиск оптимальных параметров - **pure luck**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Практический пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим две функции возвращающие батчи:\n",
    "\n",
    "Первая функция будет создавать случайный вектор на вход генератора и метками классов будут единицы, потому что мы хотим чтобы генератор давал такие точки, чтобы классификатор их распознавал как единицы (верные). Поэтому лосс функция будет подгонять веса генератора к единицам."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_g_batch(batch_size, latent_dim):\n",
    "    g_input = torch.randn(size=(batch_size, latent_dim)).to(device)\n",
    "    labels = torch.ones(size=(batch_size,)).to(device)\n",
    "    return g_input, labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая функция будет давать батч на вход дискриминатора, смешивая:\n",
    "* Истиные точки на параболе (чтобы обучать дискриминатор)\n",
    "* Точки не на параболе (чтобы дискриминатор знал где \"не парабола\")\n",
    "* Выход генератора как неверный (чтобы генератор не коллапсировал в одну верную точку)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_mix_batch(batch_size, latent_dim, netG):\n",
    "    types_of_points = []\n",
    "    \n",
    "    # Generate true pairs and true labels\n",
    "    x = torch.distributions.Uniform(-1, +1).sample((batch_size,)).to(device)\n",
    "    true_pair = torch.vstack((x, x*x)).T.to(device)\n",
    "    true_labels = torch.ones(size=(batch_size,)).unsqueeze(1).to(device)\n",
    "    types_of_points.append(torch.hstack((true_pair, true_labels)))\n",
    "    \n",
    "    # Generate fake uniform pairs and fake labels\n",
    "    if True:\n",
    "        x_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).to(device)\n",
    "        y_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).to(device)\n",
    "        fake_pair = torch.vstack((x_fake, y_fake)).T\n",
    "        fake_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).to(device)\n",
    "        types_of_points.append(torch.hstack((fake_pair, fake_labels)))\n",
    "\n",
    "    # Generate points from generator and set labels as fake\n",
    "    if True:\n",
    "        gan_pair = netG(torch.randn(size=(batch_size, latent_dim)).to(device))\n",
    "        gan_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).to(device)\n",
    "        types_of_points.append(torch.hstack((gan_pair, gan_labels)))\n",
    "    \n",
    "    # Stack all types of points\n",
    "    z = torch.vstack(types_of_points)\n",
    "    # Shuffle\n",
    "    z=z[torch.randperm(z.size()[0])]\n",
    "    \n",
    "    # Split back to samples and labels\n",
    "    mixed_pairs = z[:, :2]\n",
    "    mixed_labels = z[:, 2]\n",
    "    return mixed_pairs, mixed_labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства обернём в функции шаги backpropagation дискриминатора и генератора"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def netD_step(netD, batchD, loss_func, optimizer):\n",
    "    samples, labels = batchD\n",
    "    optimizer.zero_grad()\n",
    "    outputs = netD(samples.to(device))\n",
    "    loss = loss_func(outputs.to(device), labels.unsqueeze(1).detach().to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def netG_step(netD, netG, batchG, loss_func, optimizer):\n",
    "    samples, labels = batchG\n",
    "    optimizer.zero_grad()\n",
    "    outputs = netD(netG(samples.to(device)))\n",
    "    loss = loss_func(outputs.to(device), labels.unsqueeze(1).detach().to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем каждую эпоху отображать что уже умеет генерировать генератор"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_gen(netG, epoch=\"Not provided\"):\n",
    "    Gin, _ = get_g_batch(1000, latent_dim)\n",
    "    out = netG(Gin).cpu()\n",
    "    plt.scatter(out.detach().numpy()[:, 0], out.detach().numpy()[:, 1], color=\"blue\", s=1)\n",
    "    plt.title(f'Generator points. End of epoch= {epoch+1}', fontsize=10)\n",
    "    plt.axis([-1,1,-0.5,1])\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку сети обучаются по очереди, то создадим два оптимайзера."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "latent_dim = 5\n",
    "batch_size = 128\n",
    "batch_per_epoch = 1000\n",
    "epochs = 5\n",
    "\n",
    "netG = GenModel(latent_dim).to(device)\n",
    "netD = DisModel().to(device)\n",
    "loss_func = nn.BCELoss().to(device)\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.001)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В цикле обучения будет создаваться батч данных (размера batch_size) для генератора, и батч из смеси точек (размера 3\\*batch_size) для дискриминатора. С балансом данных подаваемых на обе сети можно эксперементировать."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_per_epoch):\n",
    "            batchG = get_g_batch(batch_size, latent_dim)\n",
    "            batchD = get_mix_batch(batch_size, latent_dim, netG)\n",
    "            \n",
    "            netD.train(True)\n",
    "            netG.train(False)\n",
    "            netD_step(netD, batchD, loss_func, optD)\n",
    "            \n",
    "            netD.train(False)\n",
    "            netG.train(True)\n",
    "            netG_step(netD, netG, batchG, loss_func, optG)\n",
    "            \n",
    "        # clear_output()\n",
    "        plot_gen(netG, epoch)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть отлично обучилась при подаче в дискриминатор всех типов точек. Можно отключить некоторые типы и посмотреть что получится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN - Генерация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью GAN можно, разумеется, генерировать не только точки на параболе. Можно генерировать, например, изображения. Но поднимаются закономерные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как из шума на входе сети получить изображение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым простым ответом будет: взять шум, пропустить его через полносвязные слои и сделать rashape до нужного разрешения. В целом, это будет работать.\n",
    "\n",
    "\n",
    "Однако **DCGAN - Deep Convolutional GAN** использует сверточные и сверточно-транспонированные (convolutional and convolutional-transpose) слои в дискриминаторе и генераторе соответственно. Впервые метод DCGAN был описан Рэдфордом и др. в статье \"Обучение неконтролируемому представлению с помощью Глубоких Сверточных генеративных состязательных сетей\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_4.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже видна разница в генерации при помощи исключительно полносвязных слоёв и при помощи обратных свёрток. Очевидно, результат DCGAN лучше чем GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gan_dcgan.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход порождающей сети поступает шум, который по факту представляет из себя вектор произвольной размерности. Отсюда следует, что нет никакой необходимости расчленять его на признаки с помощью сверточных слоев в отличие от дискриминатора, а требуется, наоборот, преобразовать в результирующее изображение. Для выполнения этой задачи будут использоваться так называемые разверточные (deconvolutional) слои, которые являются обратными к сверточным. Принцип их действия заключается в том, что они дополняют вход нулями до матрицы необходимого размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/dcgan.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution-Transpose Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратные свёртки/Развёртки (convolutional-transpose). \n",
    "\n",
    "Входное разрешение дополняется нулями (подобно паддингу) до такого разрешения, чтобы выходное разрешение было больше входного. Вы можете возразить: но ведь сильно увеличить изображение не получится! Для этого слоёв ставится несколько, чтобы каждый новый слой лучше восстанавливал то, что в него подают. Итеративно увеличивая разрешения, ядра свёрток учатся генерировать изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из изображения 2х2 в изображение 4х4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/deconv.gif\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из изображения 3х3 в изображение 5х5 с использованием паддинга:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/deconv_padding.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://github.com/vdumoulin/conv_arithmetic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/convtransponce_doc.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x = torch.rand((1, 3, 10, 10))*255 # Like one 3-channal image with 10x10 size\n",
    "x.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "convT = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "y = convT(x)\n",
    "y.shape  # One 8-chanells image with 12x12 size"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученное изображение не похоже на входное - потому что были применены свёрточные ядра со случайными коэффициентами."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow(x[0].permute(1,2,0).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow(y[0].permute(1,2,0).detach().numpy().astype(np.uint8))\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обученного DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\n",
    "clear_output()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_images = 16\n",
    "noise, _ = model.buildNoiseData(num_images)\n",
    "with torch.no_grad():\n",
    "    generated_images = model.test(noise)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16*3, 2*3))\n",
    "ax.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy(), interpolation='nearest', aspect='equal')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие способы повышения разрешения  - Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо обратных свёрток, существует другие методы повышения разрешения из низкой размерности.\n",
    "\n",
    "Самый простой способ - выполнить повышение разрешения с помощью интерполяции. В PyTorch это осуществляется слоем [Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x = torch.rand((1, 3, 10, 10)) # Like one 3-channal image with 10x10 size\n",
    "print(\"Input shape:\", x.shape)\n",
    "    \n",
    "upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "y = upsample(x)\n",
    "\n",
    "print(\"Output shape\", y.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow((x[0].permute(1,2,0)*256).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow((y[0].permute(1,2,0).detach().numpy()*256).astype(np.uint8))\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практический пример DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим DCGAN на датасете Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_epochs=2  #number of epochs of training\n",
    "batch_size=64 #size of the batches\n",
    "lr=0.0002     #adam: learning rate\n",
    "b1=0.5 #adam: decay of first order momentum of gradient\n",
    "b2=0.999 #adam: decay of first order momentum of gradient\n",
    "n_cpu=8 # number of cpu threads to use during batch generation\n",
    "latent_dim=100 #dimensionality of the latent space\n",
    "img_size=32 #size of each image dimension\n",
    "channels=1 #number of image channels\n",
    "sample_interval=100 #interval between image sampling"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, как преобразуется шум в генераторе:\n",
    "* Сначала с помощью полносвязного слоя он преобразуется в первичные фичи\n",
    "* Потом с помощью функции view, ресэмплится в картинку низкого разрешения\n",
    "* Потом, проходя через conv_blocks поочерёдно применяются Upsample и ОБЫЧНЫЕ свёртки"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "adversarial_loss.to(device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def show_gen_img(model, latent_dim=100):\n",
    "    z = Tensor(np.random.normal(0, 1, (9, latent_dim)))\n",
    "    sample_images = generator(z)\n",
    "    sample_images = sample_images.cpu().detach()\n",
    "\n",
    "    grid = make_grid(sample_images, nrow=3, ncols=3, normalize=True).permute(1,2,0).numpy()\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "clear_output()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Tensor(imgs.shape[0], 1).fill_(1.0)\n",
    "        fake = Tensor(imgs.shape[0], 1).fill_(0.0)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(Tensor)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n",
    "            show_gen_img(generator)\n",
    "\n",
    "            # save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы картинки обрели приличный вид, хватает 2 эпох. Чтобы стали выглядеть хорошо - 5 эпох."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cGAN - GAN с условием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cGAN расшифровывается как Conditional Generative Adversarial Net - это GAN с условием. Условие может быть любым, например, генерация конкретной цифры. В этом случае нам нужен уже размеченный датасет, для того чтобы обучить дискриминатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_5.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уловием в данном случае будет label, который на рисунке обозначен **Y**. Label добавляется к случайному шуму, тем самым мы говорим генератору генерировать случайное изображение нужного класса. Так же он подаётся в дискриминатор в качестве входа, чтобы дискриминатор знал какое изображение классифицировать как реальное, а какое как вымышленное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение в данном случае будет аналогичным обучению GAN, мы будем обучать сети, чередуя реальные данные и сгенерированные, добавив label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/cGANS_results.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как закодировать метки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку подавать в сеть числа от 0 до 9 (в случае MNSIT) нет смысла, то нужно придумать как подавать их в нейронную сеть. На помощь приходят Embeddings. Мы можем представить каждую метку в виде вектора с десятью элементами.\n",
    "\n",
    "[Документация nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "samples, labels = next(iter(dataloader))\n",
    "\n",
    "\n",
    "label_emb = nn.Embedding(10, 10)\n",
    "\n",
    "e = label_emb(labels)\n",
    "\n",
    "print(f\"Label: {labels[0]}\")\n",
    "print(f\"Embedding for this label: {e[0]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После чего, эмбединги меток обычно склеиваются с входами сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Почему нельзя подать просто число?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы будем подавать просто число, например 0.1 для единицы, и 0.5 для пяти, то вход у нас будет непрерывным, что довольно нелогчино, тогда при небольшом изменении входа, мы будем генерировать другую цифру. А так же потому что сети будет сложнее выучить небольшие расхождения в этом небольшом интервале. В случае с веторным представлением мы избегаем этих проблем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модификации cGAN\n",
    "\n",
    "Метки классов можно подавать не только способом, описанным выше. Можно вместо подачи их в дискриминатор сделать так, чтобы он их предсказывал - **Semi-Supervized GAN**.\n",
    "\n",
    "Или же не подавать label в дискриминатор, но ждать от него классификации в соответствии с классом, который мы хотим получить от генератора - это **InfoGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна модификация cGAN - это **AC-GAN** (auxiliary classifier) в которой единственное различие заключается в том, что дискриминатор дожен помимо распознавания реальных и фейковых изображений ещё и классифицировать их. Он имеет эффект стабилизации процесса обучения и позволяет генерировать большие высококачественные изображения, изучая представление в скрытом пространстве, которое не зависит от метки класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gans_zoo.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein GAN\n",
    "\n",
    "Если поиграться с моделями, про которые мы говорили до этого можно заметить что две проблемы встречаются особенно часто:\n",
    "\n",
    "**Mode collapse** - модель выдает одно и то же изображение независимо от того, какие входные данные ей подаются.\n",
    "\n",
    "**Mode dropping** - часть данных в распределении не поддаются моделированию.\n",
    "\n",
    "\n",
    "В 2017 году была опубликована работа [Wasserstein-GAN (WGAN)](https://arxiv.org/abs/1701.07875), которая эти проблмы в той или иной степени решила + внесла очень интересный теоретический и эмпирический вклад в обучение GANов, прежде всего в их стабилизацию и в то, что обучать их стало проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserstein GAN архитектурно может быть любым, его фишка в новом лоссе - Wasserstein Metric. По другому этот лосс еще называют Earth-Mover distance. Давайте разберемся почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earth-Mover (EM) distance/ Wasserstein Metric"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!sudo apt update\n",
    "!sudo apt install libcairo2-dev ffmpeg texlive texlive-latex-extra texlive-fonts-extra texlive-latex-recommended texlive-science tipa libpango1.0-dev\n",
    "!pip install manim\n",
    "!pip install IPython --upgrade\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте выполним простое упражнение по перемещению коробок с землей. У нас есть 6 коробок, и мы хотим переместить их слева в места, отмеченные серым квадратом справа. Для коробки №1 мы переместим ее из положения 1 в положение 7. Стоимость перемещения равна весу коробки, умноженному на расстояние. Для простоты мы зададим вес равным 1. Поэтому стоимость перемещения коробки №1 равна 6 (7-1).\n",
    "\n",
    "Посмотрим в движении:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from manim import *"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%manim MovingAround\n",
    "\n",
    "class MovingAround(Scene):\n",
    "    def construct(self):\n",
    "        # self.camera.background_color = \"#ece6e2\"\n",
    "        boxes = []\n",
    "        boxes_dest = []\n",
    "        boxes_ghost = []\n",
    "\n",
    "        for i in range(6):\n",
    "            box = Square().scale(0.5)\n",
    "            box_ghost = Square(color=GREY, fill_opacity=0.25).scale(0.5)\n",
    "\n",
    "            box = box.set_x(-5)\n",
    "            boxes.append(box)\n",
    "            boxes_dest.append(box.copy())\n",
    "            boxes_ghost.append(box_ghost)\n",
    "\n",
    "        numberplane = NumberPlane().shift(0.5*DOWN, 0.5*RIGHT)\n",
    "\n",
    "        boxes[1].set_y(1)\n",
    "        boxes[2].set_y(2)\n",
    "        boxes[3].set_x(-4)\n",
    "        boxes[4].set_x(-3)\n",
    "        boxes[5].set_x(-3).set_y(1)\n",
    "\n",
    "        boxes_ghost[0] = boxes_ghost[0].set_x(1)\n",
    "        boxes_ghost[1] = boxes_ghost[1].set_x(2)\n",
    "        boxes_ghost[2] = boxes_ghost[2].set_x(3)\n",
    "        boxes_ghost[3] = boxes_ghost[3].set_x(4)\n",
    "        boxes_ghost[4] = boxes_ghost[4].set_x(3).set_y(1)\n",
    "        boxes_ghost[5] = boxes_ghost[5].set_x(4).set_y(1)\n",
    "\n",
    "        boxes_dest[0] = boxes_dest[0].move_to(boxes_ghost[0])\n",
    "\n",
    "        texts = []\n",
    "        texts_dest = []\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            text = Text(str(i+1)).move_to(box)\n",
    "            texts.append(text)\n",
    "            texts_dest.append(text.copy())\n",
    "\n",
    "        texts_dest[0] = texts_dest[0].set_x(1)\n",
    "\n",
    "        group = VGroup(boxes[0], texts[0])\n",
    "        group_dest = group.copy().set_x(1)\n",
    "\n",
    "        d0 = Dot(color=BLUE).set_x(-5).set_y(-1)\n",
    "        arrow_0 = Arrow(d0.get_center(), [1, -1, 0], buff=0)\n",
    "\n",
    "        arrow_0_text = Text('+6').next_to(arrow_0, DOWN)\n",
    "\n",
    "        self.add(*boxes, *texts, numberplane, *boxes_ghost)\n",
    "        self.play(Transform(group, group_dest))\n",
    "        self.wait()\n",
    "        self.add(d0)\n",
    "        self.play(Transform(d0, arrow_0))\n",
    "        self.add(arrow_0_text)\n",
    "        self.wait()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем переместить коробки разными путями. Например вот так:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%manim MovingAround\n",
    "\n",
    "class MovingAround(Scene):\n",
    "    def construct(self):\n",
    "        # self.camera.background_color = \"#ece6e2\"\n",
    "        boxes = []\n",
    "        boxes_dest = []\n",
    "        boxes_ghost = []\n",
    "\n",
    "        for i in range(6):\n",
    "            box = Square().scale(0.5)\n",
    "            box_ghost = Square(color=GREY, fill_opacity=0.25).scale(0.5)\n",
    "\n",
    "            box = box.set_x(-5)\n",
    "            boxes.append(box)\n",
    "            boxes_dest.append(box.copy())\n",
    "            boxes_ghost.append(box_ghost)\n",
    "\n",
    "        numberplane = NumberPlane().shift(0.5*DOWN, 0.5*RIGHT)\n",
    "\n",
    "        boxes[1].set_y(1)\n",
    "        boxes[2].set_y(2)\n",
    "        boxes[3].set_x(-4)\n",
    "        boxes[4].set_x(-3)\n",
    "        boxes[5].set_x(-3).set_y(1)\n",
    "\n",
    "        boxes_ghost[0] = boxes_ghost[0].set_x(1)\n",
    "        boxes_ghost[1] = boxes_ghost[1].set_x(2)\n",
    "        boxes_ghost[2] = boxes_ghost[2].set_x(3)\n",
    "        boxes_ghost[3] = boxes_ghost[3].set_x(4)\n",
    "        boxes_ghost[4] = boxes_ghost[4].set_x(3).set_y(1)\n",
    "        boxes_ghost[5] = boxes_ghost[5].set_x(4).set_y(1)\n",
    "\n",
    "        boxes_dest[0] = boxes_dest[0].move_to(boxes_ghost[0])\n",
    "\n",
    "        texts = []\n",
    "        texts_dest = []\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            text = Text(str(i+1)).move_to(box)\n",
    "            texts.append(text)\n",
    "            texts_dest.append(text.copy())\n",
    "\n",
    "        texts_dest[0] = texts_dest[0].set_x(1)\n",
    "\n",
    "        groups = []\n",
    "        groups_dest = []\n",
    "        \n",
    "        self.add(*boxes, *texts, numberplane, *boxes_ghost)\n",
    "        self.wait()\n",
    "        for i in range(6):\n",
    "            group = VGroup(boxes[i], texts[i])\n",
    "            group_dest = group.move_to(boxes_ghost[i])\n",
    "            self.play(Transform(group, group_dest))\n",
    "        self.wait()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А можем и по другому:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%manim MovingAround\n",
    "\n",
    "class MovingAround(Scene):\n",
    "    def construct(self):\n",
    "        # self.camera.background_color = \"#ece6e2\"\n",
    "        boxes = []\n",
    "        boxes_dest = []\n",
    "        boxes_ghost = []\n",
    "\n",
    "        for i in range(6):\n",
    "            box = Square().scale(0.5)\n",
    "            box_ghost = Square(color=GREY, fill_opacity=0.25).scale(0.5)\n",
    "\n",
    "            box = box.set_x(-5)\n",
    "            boxes.append(box)\n",
    "            boxes_dest.append(box.copy())\n",
    "            boxes_ghost.append(box_ghost)\n",
    "\n",
    "        numberplane = NumberPlane().shift(0.5*DOWN, 0.5*RIGHT)\n",
    "\n",
    "        boxes[1].set_y(1)\n",
    "        boxes[2].set_y(2)\n",
    "        boxes[3].set_x(-4)\n",
    "        boxes[4].set_x(-3)\n",
    "        boxes[5].set_x(-3).set_y(1)\n",
    "\n",
    "        boxes_ghost[0] = boxes_ghost[0].set_x(1)\n",
    "        boxes_ghost[1] = boxes_ghost[1].set_x(2)\n",
    "        boxes_ghost[2] = boxes_ghost[2].set_x(3)\n",
    "        boxes_ghost[3] = boxes_ghost[3].set_x(4)\n",
    "        boxes_ghost[4] = boxes_ghost[4].set_x(3).set_y(1)\n",
    "        boxes_ghost[5] = boxes_ghost[5].set_x(4).set_y(1)\n",
    "\n",
    "        boxes_dest[0] = boxes_dest[0].move_to(boxes_ghost[0])\n",
    "\n",
    "        texts = []\n",
    "        texts_dest = []\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            text = Text(str(i+1)).move_to(box)\n",
    "            texts.append(text)\n",
    "            texts_dest.append(text.copy())\n",
    "\n",
    "        texts_dest[0] = texts_dest[0].set_x(1)\n",
    "\n",
    "        groups = []\n",
    "        groups_dest = []\n",
    "        \n",
    "        self.add(*boxes, *texts, numberplane, *boxes_ghost)\n",
    "        self.wait()\n",
    "\n",
    "        moves = np.arange(6)\n",
    "        np.random.shuffle(moves)\n",
    "        \n",
    "        for i in moves:\n",
    "            group = VGroup(boxes[i], texts[i])\n",
    "            group_dest = group.move_to(boxes_ghost[i])\n",
    "            self.play(Transform(group, group_dest))\n",
    "        self.wait()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что не все перемещения будут иметь одинаковую стоимость. Какие-то будут дешевле, какие-то дороже. Расстояние Вассерштейна (или расстояние EM) - это стоимость самого дешевого \"транспортного плана\".\n",
    "\n",
    "В применении к нейронным сетям, наша груда коробок слева - это одно распределение, а груда коробок с права - другое. И с помощью WGAN мы пытаемся из первого распреденеия сделать второе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wasserstein loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@markdown Wasserstein loss определен как:\n",
    "#@markdown\n",
    "#@markdown *(Запусти ячейку, что бы посмотреть на уравнение)*\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "mark1 {\n",
    "  background: linear-gradient(-100deg, hsla(48,92%,75%,.3), hsla(48,92%,75%,.7) 95%, hsla(48,92%,75%,.1));\n",
    "  border-radius: 1em 0;\n",
    "  padding: .5rem;\n",
    "}\n",
    "\n",
    "mark2 {\n",
    "  background: linear-gradient(-100deg, hsla(10,92%,75%,.3), hsla(10,92%,75%,.7) 95%, hsla(10,92%,75%,.1));\n",
    "  border-radius: 1em 0;\n",
    "  padding: .5rem;\n",
    "}\n",
    "\n",
    "mark3 {\n",
    "  background: linear-gradient(-100deg, hsla(90,92%,75%,.3), hsla(90,92%,75%,.7) 95%, hsla(90,92%,75%,.1));\n",
    "  border-radius: 1em 0;\n",
    "  padding: .5rem;\n",
    "}\n",
    "\n",
    "mark4 {\n",
    "  background: linear-gradient(-100deg, hsla(200,92%,75%,.3), hsla(200,92%,75%,.7) 95%, hsla(200,92%,75%,.1));\n",
    "  border-radius: 1em 0;\n",
    "  padding: .5rem;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\n",
    "<script type=\"text/javascript\" async\n",
    "  src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "</script>\n",
    "\n",
    "<script>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}})</script>\n",
    "\n",
    "$W(P_r,P_\\Theta) = $<mark3>$sup$</mark3><mark4>$_{||f||_L \\leq 1}$</mark4><mark1>$(\\mathbb{E}_{x \\sim P_r}[f(x)]$</mark1>$-$<mark2>$\\mathbb{E}_{x \\sim P_\\Theta}[f(x)])$</mark2>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<p><mark3>Supremum</mark3> <mark4>Lipschitz continuity</mark4> <mark1>Распределение real data</mark1> <mark2>Распределение fake data</mark2></p>\n",
    "\"\"\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Немного математических объясненний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cупре́мумом*** (лат. supremum — самый высокий) подмножества ${\\displaystyle X}$ частично упорядоченного множества (или класса) ${\\displaystyle M}$, называется наименьший элемент ${\\displaystyle M}$, который равен или больше всех элементов множества ${\\displaystyle X}$. Другими словами, супремум — это наименьшая из всех верхних граней."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lipschitz continuity**:\n",
    "\n",
    "Функция $f$ в новой форме метрики Вассерштейна должна удовлетворять  $‖f‖_L \\leq K$, то есть быть K-липшицевой непрерывной.\n",
    "\n",
    "Действительная функция $f:\\mathbb{R}→\\mathbb{R}$ называется K-липшицевой непрерывной, если существует действительная константа $K \\geq 0$ такая, что для всех $x_1$, $x_2$∈$\\mathbb{R}$\n",
    "\n",
    "$|f(x_1)-f(x_2)| \\leq K|x_1-x_2|$\n",
    "\n",
    "где $K$ известна как константа Липшица для функции $f(.)$. Функции, которые везде непрерывно дифференцируемы, являются непрерывными по Липшицу, поскольку производная, оцениваемая как $|f(x_1)-f(x_2)||x_1-x_2|$, имеет границы. Однако функция, непрерывная по Липшицу, может быть не везде дифференцируемой, например, $f(x)=|x|$.\n",
    "\n",
    "Еще подробннее можно почитать в [этой статье](https://vincentherrmann.github.io/blog/wasserstein/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Назад к Wasserstein GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы показали, что для того что бы GAN сходился, нужно каким-то образом обеспечить, чтобы дискриминатор подчинялся тому самому требованию K-липшица.\n",
    "\n",
    "В оригинальной статье (то есть на практике) это было сделано с помощью (1) *weight clipping technique*.\n",
    "\n",
    "К сожалению с этим подходом не все впорядке, так как возникает проблема того, что мы применяем слишком чрезмерную регуляризацию (это вредит качеству генерируемых изображений)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем определить *weight clipping* как:\n",
    "\n",
    "$y_i = \\text{min}(\\text{max}(x_i, \\text{min value}), \\text{max value})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посчитать с помощью `torch`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = torch.randn(size=(128,))\n",
    "\n",
    "min_val = -0.5\n",
    "max_val = 0.5\n",
    "\n",
    "x_clipped = torch.clamp(x, min_val, max_val)\n",
    "\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "plt.plot(x, label='Input', color='black', alpha=0.5)\n",
    "plt.plot(x_clipped, label='Clipped', color='red')\n",
    "plt.axhline(min_val, ls='--', color='red', alpha=0.5, label='Clipping bounds')\n",
    "plt.axhline(max_val, ls='--', color='red', alpha=0.5, label='Clipping bounds')\n",
    "plt.legend()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В более поздней литературе за этим последовал метод (2) *gradient penalty*.\n",
    "\n",
    "Его проблема в том, что *gradient penalty* дорого вычислять и штрафуются только определенные части функции. Подробнее можно почитать в статье [Improved Training of Wasserstein GANs (Wasserstein with Gradient Penalty)](https://arxiv.org/abs/1704.00028)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И затем (3) *spectral normalisation* (регуляризирует всю функцию и  вычислять дешево). Подробнее в статье [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спектральная нормализация стабилизирует обучение дискриминатора (критика) в GAN путем изменения масштаба тензора весов с помощью спектральной нормы.\n",
    "\n",
    "*Spectral Norm* в `torch` можно посчитать так:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.nn.utils.parametrizations import spectral_norm\n",
    "\n",
    "snm = spectral_norm(nn.Linear(20, 40))\n",
    "print(snm)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProGAN -> StyleGAN -> StyleGAN2 -> AliasFreeGAN\n",
    "\n",
    "[2017 Progressive Growing of GANs for Improved Quality, Stability, and Variation (ProGAN)](https://arxiv.org/abs/1710.10196)\n",
    "\n",
    "[2018 A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN)](https://arxiv.org/abs/1812.04948)\n",
    "\n",
    "[2019 Analyzing and Improving the Image Quality of StyleGAN (StyleGAN2)](https://arxiv.org/abs/1912.04958)\n",
    "\n",
    "[2021 Alias-Free Generative Adversarial Networks (Alias-Free GAN)](https://arxiv.org/abs/2106.12423)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На момент написания этого блокнота летом 2021 года, самым совершенным GANом для генерации изображений считается *Alias Free GAN*. Для того что бы понять как он работает, давайте проследим за эволюцией работ от коллеектива авторов из NVIDIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProGAN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StyleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**КАРТИНКА ОБЫЧНЫЙ GAN И STYLE GAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**КАРТИНКУ НА СЕРВЕР**\n",
    "<img src =\"https://miro.medium.com/max/1384/0*nzmrf7VMLsTWt8SX\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже разобрали как работает генератор (берем шум, пропускаем его через некую обученную сеть-генератор и получаем изображение на выходе). В 2018 году, команде исследователей из NVIDIA пришла в голову мысль как этот процесс можно было бы улучшить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StyleGAN 2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alias-Free GAN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тонкости обучения GANов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья - детальный разбор](https://beckham.nz/2021/06/28/training-gans.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частые/простые ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Убедитесь что сгенерированые сэмплы в том же диапазоне, что и реальные данные.** Например, реальные данные `[-1,1]`, при этом генерируются данные `[0,1]`. Это не хорошо, так как это подсказка для дискриминатора. \n",
    "* **Убедитесь что сгенерированные сэмплы того же размера, что и реальные данные.** Например размер картинок в MNIST `(28,28)`, а генератор выдает `(32,32)`. В таком случае нужно либо изменить архитектуру генератора, что бы получать на выходе размер `(28,28)`, либо сделать ресайз реальных данных до `(32,32)`.\n",
    "* **Старайтесь не использовать `BatchNorm`**. Проблема `BN` в том, что во время обучения его внутренняя статистика считается по минибатчу, а во время инференса она вычисляется как *moving average*, что в свою очередь может повлечь непредсказуемые результаты. Если архитектура GAN предполагает нормализацию - то лучше использовать **`InstanceNorm`**.\n",
    "* **Визуализируйте свои лоссы в процессе обучения**. Для этого существует множество прекрасных библиотек (например `tensorboard`. Следить за бегущими по экрану цифрами от двух соревнующихся между собой лоссов - бессмысленно.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуйте дать преимущество дискриминатору"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ваша версия GAN работает не так хорошо как вам хотелось бы, попробуйте дать своему дискриминатору преимущество, обучив его на относительно большее число итераций, чем генератор. Другими словами, чем лучше дискриминатор различает настоящие и фальшивые данные, тем лучше сигнал, который генератор может извлечь из него. Обратите внимание, что, эта логика не имела смысла во времена \"до WGAN\", поскольку слишком хорошая работа дискриминатора вредила обучению.\n",
    "\n",
    "Например:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def train_on_batch(x, iter_, n_gen=5):\n",
    "    # Generator\n",
    "    ...\n",
    "    ...\n",
    "    if iter_ % n_gen == 0:\n",
    "        g_loss.backward()\n",
    "        opt_g.step()\n",
    "        \n",
    "    # Disc\n",
    "    ...\n",
    "    ...\n",
    "    d_loss.backward()\n",
    "    d_loss.step()\n",
    "```\n",
    "\n",
    "Где `iter_` - текущая итерация шага градиента, а `n_gen` определяет интервал между обновлениями генератора. В данном случае, поскольку он равен 5, мы можем считать, что это означает, что дискриминатор обновляется в 5 раз чаще, чем генератор.\n",
    "\n",
    "Естественно работает не всегда и не везде. Но попробовать стоит\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Используйте ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что почти во всех статьях по GAN используется ADAM. Сложно сказать почему так получается, но он работает и работает очень хорошо. Если качество вашего GAN оставляет желать лучшего - скорее всего оптимизатор тут не причем. Ищите ошибку где-то еще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `epsilon` ADAM по умолчанию в PyTorch равен `1e-8`, что может вызвать проблемы после длительного периода обучения, например, лоссы потери периодически взрываются или увеличиваются. Подробнее об этом на [StackOverflow](https://stackoverflow.com/questions/42327543/adam-optimizer-goes-haywire-after-200k-batches-training-loss-grows) и в комментарии на [Reddit](https://www.reddit.com/r/reinforcementlearning/comments/j9rflf/intuitive_explanation_for_adams_epsilon_parameter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Zoo\n",
    "\n",
    "GAN моделей настолько много, что нет смысла рассказывать о всех. Так или иначе, сейчас многие новые нейросети используют принципы GAN для обучения. Будь-то распознавание или сегментация.\n",
    "\n",
    "[Самые современные генеративные модели](https://paperswithcode.com/methods/category/generative-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProGAN\n",
    "\n",
    "[Progressive growing of GANs for improved quality, stability and variation](https://arxiv.org/abs/1710.10196) - метод, призванный ускорить обучение сети генерации изображений с помощью добавления слоёв во время генерации. Сначала сеть учится генерировать изображения низкого разрешения, потом добавляется слой, который создаёт более высокое разрешение из фич предыдушего слоя. Эта идея позволяет плавно обучать сеть и добиваться лучшей сходимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/ProGAN.gif\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://discourse.world/h/2019/04/12/Pictures-from-rough-sketches:how-exactly-the-NVIDIA-GauGAN-neural-network-works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-23.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/pdf/2102.12593.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain transfer network\n",
    "\n",
    "[Unsupervised Cross-Domain Image Generation](https://arxiv.org/abs/1611.02200)\n",
    "\n",
    "Domain transfer network (DTN) - сеть, совмещающая в себе автоэнкодер, и gan для задачи переноса стиля.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-27.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1611.02200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результаты\n",
    "\n",
    "From SVHN to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/domain1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1611.02200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Photos to Emoji \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/domain2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1611.02200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRGAN и StackGANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)\n",
    "\n",
    "SRGAN (Super resolution GAN), StackGANs - сети для повышения разрешения изображений, отличающаяся от других сетей повышенным качеством восстановления мелких деталей и текстур изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/SRGAN.JPG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1609.04802)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение с другими методами повышения разрешения:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/SRGAN_res.JPG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1609.04802)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pix2Pix\n",
    "\n",
    "[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "Pix2Pix - сети, переводящая пиксельные рисунки в реалистичные изображения. Она тоже использует принципы GAN для работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/pix2pix.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1611.07004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробовать сеть можно на [сайте 1](https://affinelayer.com/pixsrv/) и на [сайте 2](https://affinelayer.com/pix2pix/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё примеры:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/pix2pix2.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка]([ссылка](https://arxiv.org/abs/1611.07004))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семантическая генерация\n",
    "\n",
    "Помимо шума, в модель можно подавать описание того что мы хотим получить. Это описание может быть разным, например текст или вектор заданных свойств.\n",
    "\n",
    "Например:\n",
    "* Класс объекта\n",
    "* Углы и повороты\n",
    "* Заданные параметры трансформаций\n",
    "* Сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-06.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка]([ссылка](https://arxiv.org/pdf/1411.5928.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-07.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка]([ссылка]https://arxiv.org/pdf/1411.5928.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to image\n",
    "\n",
    "[Text-to-Image Generation with Attention Based Recurrent Neural Networks](https://arxiv.org/abs/2001.06658)\n",
    "\n",
    "Описание может быть представлено в виде естественного текста.  Цель состоит в том, чтобы генерировать изображения из текста, а также позволить пользователю манипулировать синтетическими изображениями, используя описания на естественном языке, в одной структуре\n",
    "\n",
    "Одна из возможных архитектур сетей text-to-image с использованием RNN-сети на входе:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-32.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/2001.06658)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ControlGAN** - Pytorch реализация  для управляемого преобразования текста в изображение. На входе используется эмбеддинг слов. Который подаётся во множество генераторов и дискриминаторов:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cтатья Controllable Generative Adversarial Network](https://arxiv.org/abs/1708.00598)\n",
    "\n",
    "[Код ControlGAN](https://github.com/mrlibw/ControlGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/controlGAN.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1708.00598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-31.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1708.00598)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментировать:\n",
    "\n",
    "[Colab где можно вводить собственный текст](https://colab.research.google.com/drive/1_3sCa9QvUI0OqWqx2rQvsSh3C9xfPssZ?usp=sharing)\n",
    "\n",
    "[Open AI DALL-E интерактив](https://openai.com/blog/dall-e/)\n",
    "\n",
    "[OpenSource версия DALL-E](https://colab.research.google.com/drive/1b8va5g852hq3p7yro7xWY3Cc-bd2CRdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача переноса стиля\n",
    "    \n",
    "[PapersWithCode Image Stylization](https://paperswithcode.com/task/image-stylization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Постановка задачи\n",
    "\n",
    "Перенос стиля (style transfer)  — одно из наиболее креативных приложений сверточных нейронных сетей. Взяв контент с одного изображения и стиль от второго, нейронная сеть объединяет их в одно художественное произведение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/style_transfer-2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/style_transfer-3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для целей переноса стиля [можно использовать GAN](https://towardsdatascience.com/style-transfer-with-gans-on-hd-images-88e8efcf3716) (но так делают редко):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_6.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть состоит из одного генератора (G) и сиамского дискриминатора (D): G принимает изображение-контент (А) на входе и выводит некое итоговое изображение G(A). Дискриминатор  принимает эти изображения и изображение-стиль (B) в качестве входных данных и выводит скрытый вектор.\n",
    "\n",
    "Siamese Discriminator преследует 2 цели: дать информацию генератору, как создавать более реалистичные изображения, и поддерживать в этих поддельных изображениях корреляцию (то есть «контент») с исходными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принципы генеративно-состязательных сетей вышли далеко за пределы генерации из шума. Сейчас с помощью GAN создаются сложнейшие state-of-art сети для самых разнообразных задач. В лекции были рассмотрены самые главные модели: GAN - для генерации из шума, DCGAN - для генерации изображений, с помощью развёрток и cGAN - сети с генерацией по условию. \n",
    "\n",
    "Совмещая различные сети и подходы можно сконструировать множество самых разнообразных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Книга по генеративным сетям](https://habr.com/ru/company/piter/blog/504956/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAN**\n",
    "\n",
    "[Generative adversarial networks](https://deepgenerativemodels.github.io/notes/gan/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cDCGAN**\n",
    "\n",
    "[github znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN](https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StackGAN**\n",
    "\n",
    "[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/pdf/1612.03242v2.pdf)\n",
    "\n",
    "[StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/pdf/1710.10916.pdf)\n",
    "\n",
    "[Let’s Read Science! “StackGAN: Text to Photo-Realistic Image Synthesis”](https://medium.com/@rangerscience/lets-read-science-stackgan-text-to-photo-realistic-image-synthesis-4562b2b14059)\n",
    "\n",
    "[Deep Learning Generative Models for Image Synthesis and Image Translation](https://www.rulit.me/data/programs/resources/pdf/Generative-Adversarial-Networks-with-Python_RuLit_Me_610886.pdf)\n",
    "\n",
    "Видео:\n",
    "\n",
    "[youtube [StackGAN++] Realistic Image Synthesis with Stacked Generative Adversarial Networks | AISC](https://www.youtube.com/watch?v=PXWIaLE7_NU)\n",
    "\n",
    "[youtube Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://www.youtube.com/watch?v=crI5K4RCZws)\n",
    "\n",
    "[github zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks](https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ControlGAN**\n",
    "\n",
    "[github mrlibw/ControlGAN](https://github.com/mrlibw/ControlGAN)\n",
    "\n",
    "[github taki0112/ControlGAN-Tensorflow](https://github.com/taki0112/ControlGAN-Tensorflow)\n",
    "\n",
    "[Controllable Generative Adversarial Network](https://arxiv.org/pdf/1708.00598.pdf)\n",
    "\n",
    "[Controllable Text-to-Image Generation](https://arxiv.org/pdf/1909.07083.pdf)\n",
    "\n",
    "[Image Generation and Recognition (Emotions)](https://arxiv.org/pdf/1910.05774.pdf)\n",
    "\n",
    "[Natural Language & Text-to-Image 2019](https://meta-guide.com/data/data-processing/text-to-image-systems/natural-language-text-to-image-2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AC-GAN**\n",
    "\n",
    "[DCGAN TUTORIAL](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "\n",
    "[How to Develop an Auxiliary Classifier GAN (AC-GAN) From Scratch with Keras](https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/)\n",
    "\n",
    "[Understanding ACGANs with code[PyTorch]](https://towardsdatascience.com/understanding-acgans-with-code-pytorch-2de35e05d3e4)\n",
    "\n",
    "[An Auxiliary Classifier Generative Adversarial Framework for Relation Extraction](https://arxiv.org/pdf/1909.05370.pdf)\n",
    "\n",
    "[A Multi-Class Hinge Loss for Conditional GANs](https://openaccess.thecvf.com/content/WACV2021/papers/Kavalerov_A_Multi-Class_Hinge_Loss_for_Conditional_GANs_WACV_2021_paper.pdf)\n",
    "\n",
    "[github lukedeo/keras-acgan](https://github.com/lukedeo/keras-acgan)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {}
}
