{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN - Генеративно-состязательные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOC\n",
    "* Введение\n",
    "    * Задача генерации\n",
    "    * Latent space\n",
    "    * Наивный подход\n",
    "\n",
    "\n",
    "* GAN - Генеративно-состязательная сеть (англ. Generative adversarial network, сокращённо GAN)\n",
    "    * Дискриминатор\n",
    "    * GAN - принципы работы\n",
    "    * Пример генерации точек на параболе\n",
    "\n",
    "\n",
    "* DCGAN (Deep Convolutional Generative Adversarial Nets)\n",
    "    * DCGAN - GAN для генерации изображений\n",
    "    * deconv - обратные свёртки\n",
    "    * Пример генерации изборажений\n",
    "\n",
    "\n",
    "* cGAN - Условные порождающие состязательные сети (Conditional Generative Adversarial Nets)\n",
    "    * Принципы работы cGAN\n",
    "    * Модификации cGAN\n",
    "\n",
    "\n",
    "* GAN Zoo\n",
    "    * ProGAN - метод обучения сетей до высокого разрешения\n",
    "    * Domain Transfer Network - перенос стиля с помомщью GAN\n",
    "    * SRGAN и StackGANs - сети для увеличения разрешения\n",
    "    * pix2pix - генерация изображения по его рисунку или из сегментации\n",
    "    * Семантическая генерация - изображение по входному вектору из целевых свойств\n",
    "    * ControlGAN и Text to Image - генерация изображения по текстовому описанию\n",
    "\n",
    "* Заключение\n",
    "\n",
    "* Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда мы изучаем окружающий мир, то у нас нет ответов что представляет из себя окружающая действительность, однако, со временем, мы учимся распознавать образы, речь, движения, благодаря тому, что мы просто смотрим и слушаем!\n",
    "\n",
    "Положим, если давать человеку бит в секунду, и, учитывая что человек живёт в среднем $10^9$ секунд, а так же то, что в мозге человека примерно $10^{14}$ нейронных связей, значит человек использует мозг на одну стотысячную. Откуда ему получить столько информации чтобы стать человеком? Изучать мир самому! Анализируя все входные источники своего тела: глаза, уши и так далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В курсе нейронных сетей, мы привыкли что изображения размечены, однако в жизни такие метки, отсутствуют. То есть преимущественно мы учимся без учителя. Однако мы способны к творчеству, мы можем писать картины, музыку. Придумывать и прогнозировать. Мы можем генерировать что-то новое.\n",
    "\n",
    "Как подойти к такой задаче с помощью нейронных сетей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поставим задачу генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы имеем неразмеченные данные, и хотим генерировать новые данные, которые будут удовлетворять следующим условиям:\n",
    "* Новые данные должны быть похожи на исходные.\n",
    "* Но не повторять их в точности (или повторять, при случайном стечении обстоятельств).\n",
    "* Чтобы результат генерации был различным при разных запусках, нам нужен элемент случайности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эволюция в генерации изображений лиц:\n",
    "\n",
    "[Множество примеров различных генераторов](https://thisxdoesnotexist.com)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_0.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gan_gen.gif\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку мы обусловились о случайности, то нам эту случайность нужно реализовать. Если вспомнить сети, которые мы проходили, то всегда если мы подавали один и тот же вход, то получали один и тот же выход. Как реализовать случайность в нейронной сети? Случайно изменять веса сети? Есть способ лучше - давать в качестве входа случайных шум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему именно вектор? Почему не одно случайное число. Ответ таков: изменяя одно случайное число, мы будем перемещаться на очень небольшое расстояние и генерация будет однообразной. Поскольку мы физически не можем генерировать числа в бесконечных пределах, мы вынуждены использовать фиксированный диапазон.\n",
    "\n",
    "В одномерном случае, взяв диапазон [0,1] у нас всего одна размерность нашего перемещения.\n",
    "Если взять двухмерный, или трёхмерный случай, то у нас уже есть две и три **степени свободы**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть случайный шум большой размерности даёт нам больше объёма для генерации. Это называется **input latent space** - входное латентное простанство.\n",
    "\n",
    "Note: из-за неустоявшейся терминологии, случайное распределение на входе генератора называется латентным пространством, так же как и скрытое пространство в автоэнкодерах. Поэтому в этой лекции будем называть его **входным** латентным пространством. Так же встречается вариант: predefined latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Какую размерность и форму выбрать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длина вектора (размерность латентного пространства) выбирается больше, чем количество разных независимых свойств объекта, которые мы хотим получить. Если длина 0, то случайности нет и генератор будет всегда производить один и тот же объект. Если длина 1, то будет шкала, вдоль которой будут расположены, например, генерируемые изображения. Для генератора лиц, это будет, в лучшем случае, шкала от молодой женщины блондинки к пожилому мужчине брюнету. Лучший способ выбрать длину вектора - это посмотреть похожую задачу в публикациях, взять подобную размерность, и начать экспериментировать с размерностью оттуда."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У большого латентного пространства есть минусы: увеличивая размерность латентного пространства мы можем расширить его настолько, что при обучении модели точек в этом латентром пространстве будет настолько мало, что в основном пространство будет состоять из пустот. Тогда модель будет крайне некачественно \"понимать\" что ей нужно генерировать в точке, которой не было на обучающей выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичный вопрос встаёт и по поводу формы распределения входного латентного пространства. Как мы знаем из лекции про обучение сети, инициализация весов и нормализация имеют существенный вклад в работу модели. Поэтому, принято использовать многомерное нормальное распределение для input latent space, потому что оно лучше взаимодействует с весами модели и увеличивает сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный подход\n",
    "(как делать на практике НЕ нужно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем создать генератор точек на параболе. Самым тривиальным решением кажется подача случайного шума на вход сети и будем ожидать на выходе точки параболы. Проверим как это будет работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import Dataset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x*x\n",
    "    return np.hstack((x.reshape(-1,1), y.reshape(-1,1))) # Создадим num корректных точек (x,y) на параболе\n",
    "\n",
    "pairs = gen_pair(100)\n",
    "plt.scatter(pairs[:,0], pairs[:,1])\n",
    "plt.title(\"Случайные точки на параболе,\\nкоторые будем используем в качестве датасета.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём размерность входного латентного пространства $ls = 3$ и объединим шум с точками в датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 10\n",
    "batch_size = 128\n",
    "ls = 3 # latent space\n",
    "\n",
    "noise = np.random.normal(size=(n_batches*batch_size, ls))\n",
    "noise = torch.tensor(noise, dtype=torch.float)\n",
    "print(f\"NN Input: noise.shape: {noise.shape}\")\n",
    "\n",
    "xy_pair = gen_pair(num = n_batches*batch_size)\n",
    "xy_pair = torch.tensor(xy_pair, dtype=torch.float)\n",
    "print(f\"NN Output: xy_pair.shape: {xy_pair.shape}\")\n",
    "\n",
    "dataset = TensorDataset(noise, xy_pair) # model inputs, model outputs\n",
    "\n",
    "len_tr = int(len(dataset)*0.8)\n",
    "len_tst = len(dataset) - len_tr\n",
    "trainset, testset = random_split(dataset,[len_tr,len_tst])\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_loss(model,test_loader,loss_function):\n",
    "    with torch.no_grad():\n",
    "        loss_test_total = 0\n",
    "        for samples, labels in test_loader:\n",
    "            outputs = model(samples.to(device))\n",
    "            loss = loss_function(outputs, labels.to(device))\n",
    "            loss_test_total += loss.item()\n",
    "        return loss_test_total/len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простую модель, которая будет ожидать шум на вход, и генерировать точки на выходе. (Обратите внимание, что функция активации на последнем слое отсутствует, поскольку мы не ограничиваем наш генератор в каком-то диапазоне.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenModel(nn.Module):\n",
    "    def __init__(self, latent_space):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "model = GenModel(latent_space = ls)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.L1Loss().to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_epoch = 0\n",
    "    for samples, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples.to(device))\n",
    "        loss = loss_function(outputs.to(device), labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "        \n",
    "    loss_test = get_test_loss(model,test_loader,loss_function)\n",
    "    if epoch%100==0:\n",
    "        print(f\"Epoch={epoch} train_loss={loss_epoch/len(train_loader):.4} test_loss={loss_test:.4}\")\n",
    "        # test_image(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим результаты генерации на шуме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(model, ls = 3):\n",
    "    model.cpu()\n",
    "    noice = torch.tensor(np.random.normal(size=(1000, ls)), dtype=torch.float)\n",
    "    xy_pair_gen = model(noice)\n",
    "\n",
    "    xy_pair_gen = xy_pair_gen.detach().numpy()\n",
    "    plt.scatter(xy_pair_gen[:,0], xy_pair_gen[:,1])\n",
    "    plt.axis([-1, 1, 0, 1])\n",
    "    plt.show()\n",
    "    model.to(device)\n",
    "test_image(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Причина неудачи**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, модель обучается плохо. Нужно придумать как сделать так, чтобы точки \"растолкать\" из области, где парабола отсутствует. (Хотя в целом, обучать таким способом реально)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы решить проблему сильного разброса точек (оттолкнуть их от пустот) можно наказывать нейронную сеть не напрямую лосс функцией, а сетью, которая будет говорить нам что точка лежит на параболе или не лежит. Однако, проверять это условие анализируя пару x, y на принадлежность к параболе плохая идея. Потому что точки получаются из случайного шума и ожидать что они попадут точно в параболу мы не имеем права. Поскольку попадание должно быть выше чем точность типа данных (32-bit floating point для torch.float). Мы рискуем не получить в процессе обучения ни одной точки, попавшей в параболу. Нам нужно дейстовать мягче.\n",
    "\n",
    "Поэтому, определим сеть-классификатор точек (лежит/не лежит), которую назовём **дискриминатор или критик**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15,1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы имеем: генератор, выдающий точки то ли попадая в параболу, то ли не попадая в неё. А так же дискриминатор, который будет учиться их различать. Поэтому мы будем подавать на генератор синтетические правильные точки, считая их реальными, и точки, которые выдаёт генератор, считая их подделкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, генератор будет учиться подражать реальным данным, а дискриминатор будет учиться отличать реальные точки, от подделок. \n",
    "\n",
    "Мы пришли к идее генеративно-состязательных нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generative adversarial network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генеративно-состязательную сеть описал Ян Гудфеллоу из компании Google в 2014 году. Сейчас он возглавляет подразделение машинного обучения в Apple. Принцип состязательности в сети GAN нередко описывается через метафоры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор - фальшивомонетчик!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Назовем нашего фальшивомонетчика $G$ (или generator), а банкира — $D$ (или discriminator). У нас есть какое-то количество оригинальных денег $X$ для банкира, и пусть на выходе у него будет число диапазоном от нуля до единицы, чтобы оно выражало уверенность банкира в том, что выданные ему на рассмотрение деньги настоящие. Еще — поскольку фальшивомонетчик у нас нейронная сеть, ей нужны какие-то входные данные, назовем их $z$ - это наш случайный шум, который модель будет стараться превратить в деньги."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, очевидно, цель фальшивомонетчика — это максимизировать $D(G(z))$, то есть сделать так, чтобы банкир был уверен, что подделки — настоящие.\n",
    "\n",
    "Цель банкира посложнее — ему нужно одновременно положительно опознавать оригиналы, и отрицательно — подделки. Запишем это как максимизацию $D(x)(1-D(G(z)))$. Умножение можно превратить в сложение, если взять логарифм, поэтому получаем:\n",
    "\n",
    "Для банкира: максимизировать $log(D(X))+log(1-D(G(z)))$\n",
    "\n",
    "Для фальшивомонетчика: максимизировать $log(D(G(z)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математически - это игра двух игроков:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gan_loss1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы знаем, что дискриминатор максимизирует эту функцию относительно своих параметров $\\theta_d$, где при фиксированном генераторе ${G}_{\\theta_{g}}$ он выполняет двоичную классификацию: он присваивает вероятность 1 точкам данных из обучающего набора $E_{x∼p_{data}}$ и присваивает вероятность 0 сгенерированным выборкам $E_{z∼p(z)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку используется подход из теории игр, наша лосс-функция представлена как минимаксная функция. Дискриминатор пытается максимизировать целевую функцию, поэтому мы можем выполнить градиентный подъем целевой функции. Генератор пытается минимизировать целевую функцию, поэтому мы можем выполнить градиентный спуск для целевой функции. Посредством чередования градиентного подъема и спуска сеть можно обучить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный подъем на дискриминаторе:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gan_loss2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный подъем на генераторе:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gan_loss3.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако вместо того, чтобы минимизировать вероятность того, что дискриминатор будет правильным, мы максимизируем вероятность того, что дискриминатор ошибается. Поэтому мы выполняем градиентный подъем на генераторе в соответствии с этой целевой функцией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gan_loss4.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе совместного конкурентного обучения, если система достаточно сбалансирована, достигается минимаксное состояние равновесия, в котором обе сети значительно улучшили своё качество, и теперь сгенерированные изображения могут быть использованы практически как настоящие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее о математике GAN-ов можно почитать в статье [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества GAN**\n",
    "* Теоретические гарантии сходимости\n",
    "* Можно обучать обычным SGD/Adam\n",
    "* Решает в явном виде задачу generative modeling\n",
    "* Но неявным образом (нейросети)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Недостатки GAN**\n",
    "* Нестабильное обучение\n",
    "* Очень долгая сходимость\n",
    "* Mode-collapsing\n",
    "* Generator/Discriminator starvation\n",
    "* Поиск оптимальных параметров - **pure luck**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Практический пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим две функции возвращающие батчи:\n",
    "\n",
    "Первая функция будет создавать случайный вектор на вход генератора и метками классов будут единицы, потому что мы хотим чтобы генератор давал такие точки, чтобы классификатор их распознавал как единицы (верные). Поэтому лосс функция будет подгонять веса генератора к единицам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g_batch(batch_size, latent_dim):\n",
    "    g_input = torch.randn(size=(batch_size, latent_dim)).to(device)\n",
    "    labels = torch.ones(size=(batch_size,)).to(device)\n",
    "    return g_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая функция будет давать батч на вход дискриминатора, смешивая:\n",
    "* Истиные точки на параболе (чтобы обучать дискриминатор)\n",
    "* Точки не на параболе (чтобы дискриминатор знал где \"не парабола\")\n",
    "* Выход генератора как неверный (чтобы генератор не коллапсировал в одну верную точку)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_batch(batch_size, latent_dim, netG):\n",
    "    types_of_points = []\n",
    "    \n",
    "    # Generate true pairs and true labels\n",
    "    x = torch.distributions.Uniform(-1, +1).sample((batch_size,)).to(device)\n",
    "    true_pair = torch.vstack((x, x*x)).T.to(device)\n",
    "    true_labels = torch.ones(size=(batch_size,)).unsqueeze(1).to(device)\n",
    "    types_of_points.append(torch.hstack((true_pair, true_labels)))\n",
    "    \n",
    "    # Generate fake uniform pairs and fake labels\n",
    "    if True:\n",
    "        x_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).to(device)\n",
    "        y_fake = torch.distributions.Uniform(-1, +1).sample((batch_size,)).to(device)\n",
    "        fake_pair = torch.vstack((x_fake, y_fake)).T\n",
    "        fake_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).to(device)\n",
    "        types_of_points.append(torch.hstack((fake_pair, fake_labels)))\n",
    "\n",
    "    # Generate points from generator and set labels as fake\n",
    "    if True:\n",
    "        gan_pair = netG(torch.randn(size=(batch_size, latent_dim)).to(device))\n",
    "        gan_labels = torch.zeros(size=(batch_size,)).unsqueeze(1).to(device)\n",
    "        types_of_points.append(torch.hstack((gan_pair, gan_labels)))\n",
    "    \n",
    "    # Stack all types of points\n",
    "    z = torch.vstack(types_of_points)\n",
    "    # Shuffle\n",
    "    z=z[torch.randperm(z.size()[0])]\n",
    "    \n",
    "    # Split back to samples and labels\n",
    "    mixed_pairs = z[:, :2]\n",
    "    mixed_labels = z[:, 2]\n",
    "    return mixed_pairs, mixed_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства обернём в функции шаги backpropagation дискриминатора и генератора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netD_step(netD, batchD, loss_func, optimizer):\n",
    "    samples, labels = batchD\n",
    "    optimizer.zero_grad()\n",
    "    outputs = netD(samples.to(device))\n",
    "    loss = loss_func(outputs.to(device), labels.unsqueeze(1).detach().to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netG_step(netD, netG, batchG, loss_func, optimizer):\n",
    "    samples, labels = batchG\n",
    "    optimizer.zero_grad()\n",
    "    outputs = netD(netG(samples.to(device)))\n",
    "    loss = loss_func(outputs.to(device), labels.unsqueeze(1).detach().to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем каждую эпоху отображать что уже умеет генерировать генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gen(netG, epoch=\"Not provided\"):\n",
    "    Gin, _ = get_g_batch(1000, latent_dim)\n",
    "    out = netG(Gin).cpu()\n",
    "    plt.scatter(out.detach().numpy()[:, 0], out.detach().numpy()[:, 1], color=\"blue\", s=1)\n",
    "    plt.title(f'Generator points. End of epoch= {epoch+1}', fontsize=10)\n",
    "    plt.axis([-1,1,-0.5,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку сети обучаются по очереди, то создадим два оптимайзера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 5\n",
    "batch_size = 128\n",
    "batch_per_epoch = 1000\n",
    "epochs = 5\n",
    "\n",
    "netG = GenModel(latent_dim).to(device)\n",
    "netD = DisModel().to(device)\n",
    "loss_func = nn.BCELoss().to(device)\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.001)\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В цикле обучения будет создаваться батч данных (размера batch_size) для генератора, и батч из смеси точек (размера 3\\*batch_size) для дискриминатора. С балансом данных подаваемых на обе сети можно эксперементировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(batch_per_epoch):\n",
    "            batchG = get_g_batch(batch_size, latent_dim)\n",
    "            batchD = get_mix_batch(batch_size, latent_dim, netG)\n",
    "            \n",
    "            netD.train(True)\n",
    "            netG.train(False)\n",
    "            netD_step(netD, batchD, loss_func, optD)\n",
    "            \n",
    "            netD.train(False)\n",
    "            netG.train(True)\n",
    "            netG_step(netD, netG, batchG, loss_func, optG)\n",
    "            \n",
    "        # clear_output()\n",
    "        plot_gen(netG, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(netD, netG, batch_per_epoch, batch_size, latent_dim, epochs, loss_func, optD, optG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть отлично обучилась при подаче в дискриминатор всех типов точек. Можно отключить некоторые типы и посмотреть что получится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN - Генерация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью GAN можно, разумеется, генерировать не только точки на параболе. Можно генерировать, например, изображения. Но поднимаются закономерные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как из шума на входе сети получить изображение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым простым ответом будет: взять шум, пропустить его через полносвязные слои и сделать rashape до нужного разрешения. В целом, это будет работать.\n",
    "\n",
    "\n",
    "Однако **DCGAN - Deep Convolutional GAN** использует сверточные и сверточно-транспонированные (convolutional and convolutional-transpose) слои в дискриминаторе и генераторе соответственно. Впервые метод DCGAN был описан Рэдфордом и др. в статье \"Обучение неконтролируемому представлению с помощью Глубоких Сверточных генеративных состязательных сетей\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_4.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже видна разница в генерации при помощи исключительно полносвязных слоёв и при помощи обратных свёрток. Очевидно, результат DCGAN лучше чем GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gan_dcgan.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход порождающей сети поступает шум, который по факту представляет из себя вектор произвольной размерности. Отсюда следует, что нет никакой необходимости расчленять его на признаки с помощью сверточных слоев в отличие от дискриминатора, а требуется, наоборот, преобразовать в результирующее изображение. Для выполнения этой задачи будут использоваться так называемые разверточные (deconvolutional) слои, которые являются обратными к сверточным. Принцип их действия заключается в том, что они дополняют вход нулями до матрицы необходимого размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/dcgan.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сonvolutional-Transpose Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратные свёртки/Развёртки (convolutional-transpose). \n",
    "\n",
    "Входное разрешение дополняется нулями (подобно паддингу) до такого разрешения, чтобы выходное разрешение было больше входного. Вы можете возразить: но ведь сильно увеличить изображение не получится! Для этого слоёв ставится несколько, чтобы каждый новый слой лучше восстанавливал то, что в него подают. Итеративно увеличивая разрешения, ядра свёрток учатся генерировать изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из изображения 2х2 в изображение 4х4:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/deconv.gif\" width=\"300\">\n",
    "\n",
    "Из изображения 3х3 в изображение 5х5 с использованием паддинга:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/deconv_padding.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html#torch.nn.ConvTranspose2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/convtransponce_doc.JPG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10))*255 # Like one 3-channal image with 10x10 size\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convT = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "y = convT(x)\n",
    "y.shape  # One 8-chanells image with 12x12 size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученное изображение не похоже на входное - потому что были применены свёрточные ядра со случайными коэффициентами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow(x[0].permute(1,2,0).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow(y[0].permute(1,2,0).detach().numpy().astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обученного DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 16\n",
    "noise, _ = model.buildNoiseData(num_images)\n",
    "with torch.no_grad():\n",
    "    generated_images = model.test(noise)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16*3, 2*3))\n",
    "ax.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy(), interpolation='nearest', aspect='equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие способы повышения разрешения  - Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо обратных свёрток, существует другие методы повышения разрешения из низкой размерности.\n",
    "\n",
    "Самый простой способ - выполнить повышение разрешения с помощью интерполяции. В PyTorch это осуществляется слоем [Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10)) # Like one 3-channal image with 10x10 size\n",
    "print(\"Input shape:\", x.shape)\n",
    "    \n",
    "upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "y = upsample(x)\n",
    "\n",
    "print(\"Output shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow((x[0].permute(1,2,0)*256).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow((y[0].permute(1,2,0).detach().numpy()*256).astype(np.uint8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практический пример DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим DCGAN на датасете Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=2  #number of epochs of training\n",
    "batch_size=64 #size of the batches\n",
    "lr=0.0002     #adam: learning rate\n",
    "b1=0.5 #adam: decay of first order momentum of gradient\n",
    "b2=0.999 #adam: decay of first order momentum of gradient\n",
    "n_cpu=8 # number of cpu threads to use during batch generation\n",
    "latent_dim=100 #dimensionality of the latent space\n",
    "img_size=32 #size of each image dimension\n",
    "channels=1 #number of image channels\n",
    "sample_interval=100 #interval between image sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, как преобразуется шум в генераторе:\n",
    "* Сначала с помощью полносвязного слоя он преобразуется в первичные фичи\n",
    "* Потом с помощью функции view, ресэмплится в картинку низкого разрешения\n",
    "* Потом, проходя через conv_blocks поочерёдно применяются Upsample и ОБЫЧНЫЕ свёртки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "adversarial_loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gen_img(model, latent_dim=100):\n",
    "    z = Tensor(np.random.normal(0, 1, (9, latent_dim)))\n",
    "    sample_images = generator(z)\n",
    "    sample_images = sample_images.cpu().detach()\n",
    "\n",
    "    grid = make_grid(sample_images, nrow=3, ncols=3, normalize=True).permute(1,2,0).numpy()\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Tensor(imgs.shape[0], 1).fill_(1.0)\n",
    "        fake = Tensor(imgs.shape[0], 1).fill_(0.0)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(Tensor)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n",
    "            show_gen_img(generator)\n",
    "\n",
    "            # save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы картинки обрели приличный вид, хватает 2 эпох. Чтобы стали выглядеть хорошо - 5 эпох."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cGAN - GAN с условием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cGAN расшифровывается как Conditional Generative Adversarial Net - это GAN с условием. Условие может быть любым, например, генерация конкретной цифры. В этом случае нам нужен уже размеченный датасет, для того чтобы обучить дискриминатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_5.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уловием в данном случае будет label, который на рисунке обозначен **Y**. Label добавляется к случайному шуму, тем самым мы говорим генератору генерировать случайное изображение нужного класса. Так же он подаётся в дискриминатор в качестве входа, чтобы дискриминатор знал какое изображение классифицировать как реальное, а какое как вымышленное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение в данном случае будет аналогичным обучению GAN, мы будем обучать сети, чередуя реальные данные и сгенерированные, добавив label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/cGANS_results.JPG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как закодировать метки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку подавать в сеть числа от 0 до 9 (в случае MNSIT) нет смысла, то нужно придумать как подавать их в нейронную сеть. На помощь приходят Embeddings. Мы можем представить каждую метку в виде вектора с десятью элементами.\n",
    "\n",
    "[Документация nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = next(iter(dataloader))\n",
    "\n",
    "\n",
    "label_emb = nn.Embedding(10, 10)\n",
    "\n",
    "e = label_emb(labels)\n",
    "\n",
    "print(f\"Label: {labels[0]}\")\n",
    "print(f\"Embedding for this label: {e[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После чего, эмбединги меток обычно склеиваются с входами сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Почему нельзя подать просто число?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы будем подавать просто число, например 0.1 для единицы, и 0.5 для пяти, то вход у нас будет непрерывным, что довольно нелогчино, тогда при небольшом изменении входа, мы будем генерировать другую цифру. А так же потому что сети будет сложнее выучить небольшие расхождения в этом небольшом интервале. В случае с веторным представлением мы избегаем этих проблем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модификации cGAN\n",
    "\n",
    "Метки классов можно подавать не только способом, описанным выше. Можно вместо подачи их в дискриминатор сделать так, чтобы он их предсказывал - **Semi-Supervized GAN**.\n",
    "\n",
    "Или же не подавать label в дискриминатор, но ждать от него классификации в соответствии с классом, который мы хотим получить от генератора - это **InfoGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна модификация cGAN - это **AC-GAN** (auxiliary classifier) в которой единственное различие заключается в том, что дискриминатор дожен помимо распознавания реальных и фейковых изображений ещё и классифицировать их. Он имеет эффект стабилизации процесса обучения и позволяет генерировать большие высококачественные изображения, изучая представление в скрытом пространстве, которое не зависит от метки класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/gans_zoo.JPG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Zoo\n",
    "\n",
    "GAN моделей настолько много, что нет смысла рассказывать о всех. Так или иначе, сейчас многие новые нейросети используют принципы GAN для обучения. Будь-то распознавание или сегментация.\n",
    "\n",
    "[Самые современные генеративные модели](https://paperswithcode.com/methods/category/generative-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProGAN\n",
    "\n",
    "[Progressive growing of GANs for improved quality, stability and variation](https://arxiv.org/abs/1710.10196) - метод, призванный ускорить обучение сети генерации изображений с помощью добавления слоёв во время генерации. Сначала сеть учится генерировать изображения низкого разрешения, потом добавляется слой, который создаёт более высокое разрешение из фич предыдушего слоя. Эта идея позволяет плавно обучать сеть и добиваться лучшей сходимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/ProGAN.gif\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-23.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain transfer network\n",
    "\n",
    "[Unsupervised Cross-Domain Image Generation](https://arxiv.org/abs/1611.02200)\n",
    "\n",
    "Domain transfer network (DTN) - сеть, совмещающая в себе автоэнкодер, и gan для задачи переноса стиля.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-27.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Результаты\n",
    "\n",
    "From SVHN to MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/domain1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Photos to Emoji \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/domain2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRGAN и StackGANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802)\n",
    "\n",
    "SRGAN (Super resolution GAN), StackGANs - сети для повышения разрешения изображений, отличающаяся от других сетей повышенным качеством восстановления мелких деталей и текстур изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/SRGAN.JPG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение с другими методами повышения разрешения:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/SRGAN_res.JPG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pix2Pix\n",
    "\n",
    "[Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "Pix2Pix - сети, переводящая пиксельные рисунки в реалистичные изображения. Она тоже использует принципы GAN для работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/pix2pix.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробовать сеть можно на [сайте 1](https://affinelayer.com/pixsrv/) и на [сайте 2](https://affinelayer.com/pix2pix/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё примеры:\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/pix2pix2.JPG\" width=\"500\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/pix2pix3.JPG\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семантическая генерация\n",
    "\n",
    "Помимо шума, в модель можно подавать описание того что мы хотим получить. Это описание может быть разным, например текст или вектор заданных свойств.\n",
    "\n",
    "Например:\n",
    "* Класс объекта\n",
    "* Углы и повороты\n",
    "* Заданные параметры трансформаций\n",
    "* Сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-06.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-07.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to image\n",
    "\n",
    "[Text-to-Image Generation with Attention Based Recurrent Neural Networks](https://arxiv.org/abs/2001.06658)\n",
    "\n",
    "Описание может быть представлено в виде естественного текста.  Цель состоит в том, чтобы генерировать изображения из текста, а также позволить пользователю манипулировать синтетическими изображениями, используя описания на естественном языке, в одной структуре\n",
    "\n",
    "Одна из возможных архитектур сетей text-to-image с использованием RNN-сети на входе:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-32.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ControlGAN** - Pytorch реализация  для управляемого преобразования текста в изображение. На входе используется эмбеддинг слов. Который подаётся во множество генераторов и дискриминаторов:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cтатья Controllable Generative Adversarial Network](https://arxiv.org/abs/1708.00598)\n",
    "\n",
    "[Код ControlGAN](https://github.com/mrlibw/ControlGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/controlGAN.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/11-31.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментировать:\n",
    "\n",
    "[Colab где можно вводить собственный текст](https://colab.research.google.com/drive/1_3sCa9QvUI0OqWqx2rQvsSh3C9xfPssZ?usp=sharing)\n",
    "\n",
    "[Open AI DALL-E интерактив](https://openai.com/blog/dall-e/)\n",
    "\n",
    "[OpenSource версия DALL-E](https://colab.research.google.com/drive/1b8va5g852hq3p7yro7xWY3Cc-bd2CRdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача переноса стиля\n",
    "    \n",
    "[PapersWithCode Image Stylization](https://paperswithcode.com/task/image-stylization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Постановка задачи\n",
    "\n",
    "Перенос стиля (style transfer)  — одно из наиболее креативных приложений сверточных нейронных сетей. Взяв контент с одного изображения и стиль от второго, нейронная сеть объединяет их в одно художественное произведение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img/style_transfer-2.PNG\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для целей переноса стиля [можно использовать GAN](https://towardsdatascience.com/style-transfer-with-gans-on-hd-images-88e8efcf3716) (но так делают редко):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L13_GAN_cGAN/img_licence/l13_6.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть состоит из одного генератора (G) и сиамского дискриминатора (D): G принимает изображение-контент (А) на входе и выводит некое итоговое изображение G(A). Дискриминатор  принимает эти изображения и изображение-стиль (B) в качестве входных данных и выводит скрытый вектор.\n",
    "\n",
    "Siamese Discriminator преследует 2 цели: дать информацию генератору, как создавать более реалистичные изображения, и поддерживать в этих поддельных изображениях корреляцию (то есть «контент») с исходными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принципы генеративно-состязательных сетей вышли далеко за пределы генерации из шума. Сейчас с помощью GAN создаются сложнейшие state-of-art сети для самых разнообразных задач. В лекции были рассмотрены самые главные модели: GAN - для генерации из шума, DCGAN - для генерации изображений, с помощью развёрток и cGAN - сети с генерацией по условию. \n",
    "\n",
    "Совмещая различные сети и подходы можно сконструировать множество самых разнообразных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAN**\n",
    "\n",
    "[Generative adversarial networks](https://deepgenerativemodels.github.io/notes/gan/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cDCGAN**\n",
    "\n",
    "[github znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN](https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StackGAN**\n",
    "\n",
    "[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/pdf/1612.03242v2.pdf)\n",
    "\n",
    "[StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/pdf/1710.10916.pdf)\n",
    "\n",
    "[Let’s Read Science! “StackGAN: Text to Photo-Realistic Image Synthesis”](https://medium.com/@rangerscience/lets-read-science-stackgan-text-to-photo-realistic-image-synthesis-4562b2b14059)\n",
    "\n",
    "[Deep Learning Generative Models for Image Synthesis and Image Translation](https://www.rulit.me/data/programs/resources/pdf/Generative-Adversarial-Networks-with-Python_RuLit_Me_610886.pdf)\n",
    "\n",
    "Видео:\n",
    "\n",
    "[youtube [StackGAN++] Realistic Image Synthesis with Stacked Generative Adversarial Networks | AISC](https://www.youtube.com/watch?v=PXWIaLE7_NU)\n",
    "\n",
    "[youtube Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://www.youtube.com/watch?v=crI5K4RCZws)\n",
    "\n",
    "[github zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks](https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ControlGAN**\n",
    "\n",
    "[github mrlibw/ControlGAN](https://github.com/mrlibw/ControlGAN)\n",
    "\n",
    "[github taki0112/ControlGAN-Tensorflow](https://github.com/taki0112/ControlGAN-Tensorflow)\n",
    "\n",
    "[Controllable Generative Adversarial Network](https://arxiv.org/pdf/1708.00598.pdf)\n",
    "\n",
    "[Controllable Text-to-Image Generation](https://arxiv.org/pdf/1909.07083.pdf)\n",
    "\n",
    "[Image Generation and Recognition (Emotions)](https://arxiv.org/pdf/1910.05774.pdf)\n",
    "\n",
    "[Natural Language & Text-to-Image 2019](https://meta-guide.com/data/data-processing/text-to-image-systems/natural-language-text-to-image-2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AC-GAN**\n",
    "\n",
    "[DCGAN TUTORIAL](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "\n",
    "[How to Develop an Auxiliary Classifier GAN (AC-GAN) From Scratch with Keras](https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/)\n",
    "\n",
    "[Understanding ACGANs with code[PyTorch]](https://towardsdatascience.com/understanding-acgans-with-code-pytorch-2de35e05d3e4)\n",
    "\n",
    "[An Auxiliary Classifier Generative Adversarial Framework for Relation Extraction](https://arxiv.org/pdf/1909.05370.pdf)\n",
    "\n",
    "[A Multi-Class Hinge Loss for Conditional GANs](https://openaccess.thecvf.com/content/WACV2021/papers/Kavalerov_A_Multi-Class_Hinge_Loss_for_Conditional_GANs_WACV_2021_paper.pdf)\n",
    "\n",
    "[github lukedeo/keras-acgan](https://github.com/lukedeo/keras-acgan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
