{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Автоэнкодеры</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоэнкодер (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.* — **Yann LeCun**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Сегодня мы разберем подход, который относится к задаче обучения без учителя (**unsupervised learning**), когда объекты известны, но им не сопоставлены метки, которые мы должны тем или иным способом предсказывать.\n",
    "\n",
    "Разницу между двумя задачами можно понять при помощи картинки, представленной ниже. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/uns_sup_diff.png\" alt=\"alttext\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае **supervised learning** для каждого объекта нам известна метка, что вот это — изображение яблок, это — изображение груш и т.д. Далее модель учится по изображению определять фрукт. \n",
    "\n",
    "В случае же **unsupervised learning** модель просматривает все изображения фруктов, не зная, где какой фрукт находится, и далее формирует представление, которое **неявно** делит фрукты по похожести. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем вообще изучать такой тип задачи? \n",
    "\n",
    "1. Иногда у нас слишком мало размеченных объектов, чтобы учить на них какую-либо задачу классификации и т. д. При этом у нас огромное количество неразмеченных данных. Мы можем **надеяться**, что если мы как-то обработаем наши данные, то они сами разделятся каким-то образом, согласующимся с метками. \n",
    "\n",
    "2. Человеческий мозг в основном учится в unsupervised манере. Возможно, для того чтобы решать задачи, которые легко по сравнению с компьютером решает человек, нам стоит и компьютер учить похожим образом.\n",
    "\n",
    "3. Часто обучение без учителя дает результаты, которые в дальнейшем позволяют быстро адаптироваться к новым задачам обучения с учителем и переключаться между ними. Причем делать это **эффективнее**, чем transfer learning. (*Stop learning tasks, start learning skills* — Satinder Singh)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из областей, тесно связанной с Unsupervised learning, является задача выучивания представления данных (**representation learning**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выученное представление используется либо как шаг предобучения нейросети перед обучением ее уже на целевую задачу, либо как источник признаков для других, не обязательно нейросевых, алгоритмов. \n",
    "\n",
    "Задачу получения полезного представления данных можно решать при помощи supervised learning, однако, как правило, это куда менее эффективно. Почему?\n",
    "\n",
    "ImageNet содержит 1.28 миллионов изображений. Допустим даже мы передаем их в сетку, приводя все к разрешение 128x128. В этом случае они будует занимать $\\approx$ 500Гбит. \n",
    "\n",
    "А сколько будут занимать метки изображений? У нас 1000 возможных классов. Допустим, мы используем one-hot encoding в виде битового вектора. Получается $\\approx$ 12.8Мбит.\n",
    "\n",
    "Нейронная сеть для ImageNet может легко содержать 30М весов.\n",
    "\n",
    "Есть ли смысл нейросетке учить информации больше, чем закодировано в **предсказываемой величине**? — Нет. Потому, как правило, представление данных, полученное из полностью supervised нейросети, будет крайне бедным и заточенным на конкретную задачу. Могут возникнуть проблемы даже с применением этого представления для похожей задачи. \n",
    "\n",
    "А вот если мы воспользуемся unsupervised подходом, то потенциально можем принудить нейросетку пытаться выработать эффективное представление всех 500Гбит.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Снижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одной областью, тесно связанной с предыдущими двумя, является задача снижения размерности — когда мы хотим данные из пространства высокой размерности преобразовать в пространство более низкой, с сохранением одного или нескольких свойств, например:\n",
    "\n",
    "* данные реконструируются обратно почти без ошибки;\n",
    "* расстояние между объектами сохраняется.\n",
    "\n",
    "Зачем это нужно? По многим причинам.\n",
    "\n",
    "Многие алгоритмы показывают себя плохо на простанствах большой размерности в принципе ([проклятье размерности](https://en.wikipedia.org/wiki/Curse_of_dimensionality)). \n",
    "\n",
    "Некоторые — просто будут значительно дольше работать, при этом качество их работы не изменится от уменьшения размерности. \n",
    "\n",
    "Понижение размерности позволяет использовать память более эффективно и подавать модели на обучение за один раз больше объектов. \n",
    "\n",
    "Также понижение размерности помогает избавится от шума, как мы обсудим дальше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоэнкодер — архитектура нейросети, которая сначала с помощью нейросети-энкодера сжимает изображение в вектор небольшой размерности (он называется скрытым представлением), а затем восстанавливает этот вектор в исходную картинку с помощью нейросети-декодера. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Практика показывает, что скрытое представление картинки позволяет делать очень интересные и красивые вещи — например, очищать изображение от шума, проводить гладкую интерполяцию между 2 написанными от руки цифрами, генерировать новую рукописную цифру со стилем от имеющейся. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откуда берутся эти свойства? Они являются следствием сжатия информации. Одна из форм сжатия — это классификация, которую мы уже делали. Если это цифры, то вместо изображения можно сохранить только какая это цифра. Это предельное сжатие информации, но при попытке перевести цифру в картинку мы уже не имеем достаточно информации, чтобы картинка получалось разной. Если не так сильно ограничивать информацию в точке максимального сжатия, то кроме класса цифры сохранится еще что-то и изображение удастся восстановить с большим количеством сохранённых деталей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/better_enc_dec.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сжатие информации и потери"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоэнкодер может быть без потерь и с потерями (lossless и lossy). В какой-то степени это альтернативно методам сжатия архиваторов и кодирования контента (zip, mp3, jpeg, flac, ...). Можно ли сделать сжатие на нейронных сетях с помощью автоэнкодеров? Да, это будет работать. Размер сети будет большим, но сжатие может превзойти другие алгоритмы. Практический пример [проект Google Lyra](https://ai.googleblog.com/2021/02/lyra-new-very-low-bitrate-codec-for.html), в котором подобный подход был применен для компрессии звука и проект [NVIDIA Maxine](https://developer.nvidia.com/maxine), где в свою очередь сжимают видео.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/lossy_encoding.png\" alt=\"alttext\" width=\"650\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему это может работь? Дело в том, что нейронная сеть может сформулировать набор правил, по которому на основе латентного представления приближенно или точно кодировать (за счет кодировщика), а затем восстанавливать исходный объект (при помощи декодировщика).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/ae_principle.png\" alt=\"alttext\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А почему мы уверены, что такой набор правил будет существовать и мы вообще имеем право понижать размерность пространства?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold assumption "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы предполагаем, что наши данные на самом деле лежат в пространстве меньшем, чем пространство исходных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/14_2.png\" alt=\"alttext\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве случаев это действительно так. Например, лица людей даже на фотографиях 300x300, очевидно, лежат в пространстве меньшей размерности, нежели 90000. Ведь не каждая матрица 300 на 300, заполненная какими-то значениями от 0 до 1, даст нам изображение человека"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/14_1.png\" alt=\"alttext\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод главных компонент (Principal Component Analysis) - это метод отображения векторов свойств объектов (помним, что у нас объект всегда описывается вектором свойств, длина вектора — это количество свойств) в вектора производных свойств (**компонент**), меньшей длины с помощью линейной комбинации, чтобы обратной операцией можно было восстановить значения векторов свойств как можно ближе к исходным. То есть PCA тоже выполняет сжатие информации, он тоже работает для группы объектов (а нейронная сеть автоэнкодера учится под определённую группу объектов). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно PCA работает для центрированных переменных. Каждая следующая компонента проводится перпендикулярно предыдущим и так, чтобы объяснить наибольшую часть разницы, не объясненной предыдущими компонентами между объектами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/14_3.png\" alt=\"alttext\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графически можно представить PCA, как поиск подпространства, проекция точек, на которое минимально меняет координаты в исходном пространстве. Например, для объектов на плоскости PCA можно сделать в одномерное пространство — на прямую.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/pca_decomposition.png\" alt=\"alttext\" width=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямая определяется только вектором нормали, то есть линия проекции проходит через 0. \n",
    "\n",
    "\n",
    "Возвращаясь к примеру с лицами — долгое время для распознавания лиц размерности 128\\*128 использовалось представление, полученное при помощи PCA. Для хорошего качества восстановления хватает около 100 компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналогия AE и PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия PCA и AE в том, что PCA выполняет линейную комбинацию над компонентами исходного вектора свойств объекта, а AE — как правило, нелинейную. PCA вычисляется однозначно, а AE обучается без гарантии нахождения наилучшего положения. PCA гарантирует ортогональный базис для разложения сжатых свойств, а AE — нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA будет частным случаем AE, если в нём сделать только один плотный слой (Dense) с количеством нейронов, равным требуемому числу компонент, сделать линейную функцию активации, сделать потери по среднему квадрату ошибки (mean squared error — MSE). Кроме этого, необходимо будет нормировать признаки перед подачей их на вход AE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/pca_autoencoder.png\" alt=\"alttext\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда PCA позволяет рассчитать веса для нейронов такого автоэнкодера. При этом гарантировав (в отличии от градиентного спуска) наилучшее решение задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очищение изображения от шумов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересное применение автоэнкодеров — очищение входной картинки от шумов. Такое принципиально возможно из-за того, что размерность латентного пространства очень мала по сравнению с размерностью входного пространства(в нашем случае — 32 и 784 соответственно) — в нём попросту нет места случайному шуму, но зато есть место для общих закономерностей из входного пространства."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы подаём на обучении автоэнкодера такой незашумлённый датасет, что в нём на самом деле есть некое простраство свойств, которое его описывает. На выходе энкодера в изображении останутся именно эти свойства. Шум является внешним свойством и не сможет закодироваться.\n",
    "\n",
    "Иными словами — за счет кодировщика и декодировщика автоэнкодер выучивается «проектировать» объекты на латентное пространство и восстанавливать их из него. Если шум небольшой, то автоэнкодер спроецирует объект в нужное место в латентном пространстве и обратно восстановит его уже без шума.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/L14_decoder_noise.png\" alt=\"alttext\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом важно понимать, что если шум поместит наш объект так, что автоэнкодеру придется выбирать между разными вариантами проекции, могут возникнуть артефакты. \n",
    "\n",
    "В случае, приведенном на рисунке, зашумленному x соответствуют две группы объектов из реального датасета. Если мы, к примеру, оптимизируем MSE, то автоэнкодеру «экономнее» всего будет восстанавливать нечто между двумя группами. При этом этого «нечто» в природе не существует или оно очень маловероятно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/bad_bimodal.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление шума к исходной выборке\n",
    "\n",
    "Также в случае отсутствия шума в изначальной выборке, ее малом размере и т.д. можно добавлять шум к самим исходным данным, получая из объекта $x$ объект $\\tilde{x}$, и требуя от энкодера восстановить на основе зашумленного объекта исходный. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/denoising_autoencoder.png\" alt=\"alttext\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот подход может работать и является примером *аугментации данных*. Он может дополнительно заставить автоэнкодер выучивать полезные признаки, т.е. его можно использовать, даже если целью не является получение автоэнкодера, избавляющего данные от шума.\n",
    "\n",
    "С ним, однако, надо быть очень аккуратным:\n",
    "\n",
    "1. Шум, который вы добавляете, не должен сильно менять исходный объект. Если это происходит, то либо аввтоэнкодер легко будет находить места, где был добавлен шум и при этом делать ему это будет легче, чем учить сжатое представление данных. Либо автоэнкодер выучит о ваших данных что-то такое, чего там и в помине быть не может. К примеру, если добавить к признакам, которые всегда целые, нормальный шум, ничего хорошего не выйдет. \n",
    "\n",
    "\n",
    "2. Шум должен соответствовать «естественному шуму». Если реальный шум в данных отличается от того, на котором учился автоэнкодер, есть вероятность, что он не будет очищать данные от исходного шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA для избавления от шума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим PCA, как простеший автоэнкодер, для очищения от шумов изображений базы MNIST. Нам потребуется база MNIST, numpy, библиотека отрисовки matplotlib и сам PCA, который есть в пакете sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision\n",
    "from IPython.display import clear_output \n",
    "\n",
    "root = \"./data\"\n",
    "\n",
    "train_set = dset.MNIST(\n",
    "    root=root, train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "test_set = dset.MNIST(\n",
    "    root=root, train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и приведём размерность к двумерной, чтобы это был набор векторов свойств."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = train_set.data.numpy()\n",
    "Y_train = train_set.targets.numpy()\n",
    "X_test = test_set.data.numpy()\n",
    "Y_test = test_set.targets.numpy()\n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0 # normalize data from 0 to 1\n",
    "xt_shape = X_train.shape\n",
    "print(\"Initial shape \", xt_shape)\n",
    "xt_flat = X_train.reshape(-1 , xt_shape[1] * xt_shape[2]) #  reshape to vector, 28*28 = 784\n",
    "print(\"Reshaped to \", xt_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но графически"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "ax[0].imshow(X_train[0])\n",
    "ax[1].imshow(xt_flat[0].reshape(1, -1), aspect=\"auto\")\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[1].set_title(\"Flattened image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь заведём класс PCA и настроим его, чтобы сохранял 90% исходной картинки. Обучим его и посмотрим, сколько ему потребовалось свойств для описания каждой картинки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.90)\n",
    "xt_encoded = pca.fit_transform(xt_flat)\n",
    "print(\"Encoded features \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер (он же декодер, ведь это просто обратная матрица от энкодера PCA) обучен. Теперь можно проверить, как он закодирует и раскодирует тестовую выборку. Для этого проведём такие же преобразования размерности для неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_shape = X_test.shape\n",
    "xtest_flat = X_test.reshape(-1, xtest_shape[1] * xtest_shape[2])\n",
    "xtest_encoded = pca.transform(xtest_flat)\n",
    "xtest_decoded = pca.inverse_transform(xtest_encoded).reshape(xtest_shape)\n",
    "print(\"Decoded xtest_decoded shape is \", xtest_decoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно определить функцию для отрисовки изображений MNIST. Она будет выводить несколько изображений в ряд, поэтому будет принимать трёхмерный массив. Шкала не должна быть автоподстраиваемой, так как после обработки изображения выйдут за диапазон (0,1), в котором заданы исходные изображения. Мы зафиксируем шкалу в диапазоне (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs(imgs, title):\n",
    "    fig = plt.figure(figsize=(16, 3))\n",
    "    columns = imgs.shape[0]\n",
    "    rows = 1\n",
    "    for i in range(columns):\n",
    "        fig.add_subplot(rows, columns, i + 1)\n",
    "        plt.imshow(imgs[i], cmap=\"gray_r\", clim=(0, 1))\n",
    "    fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем исходное и закодированное изображение для некоторых изображений, которые мы элегантно случайным образом выберем из всей тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(X_test.shape[0], 6) \n",
    "samples_orig = X_test[sample_indices]\n",
    "samples_decoded = xtest_decoded[sample_indices]\n",
    "plot_imgs(samples_orig, \"Original X_test\")\n",
    "plot_imgs(samples_decoded, \"PCA decoded X_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что `pca.n_components_` (87 для 90% PCA) достаточно для описания картинок MNIST вместо 784 исходных пикселей. Но при этом нужно хранить матрицу кодирования-декодирования, а также изображения получаются немного зашумлёнными. Мы получили способ сжатия с потерями для рукописных цифр, где изображение центрировано и отмасштабировано по рамке из 28х28 пикселей (подробней смотрите правила базы MNIST). \n",
    "\n",
    "Степень сжатия у нас условно 87/784 ~= 0.11. То есть сжатие в 9 раз. «Условно», так как сжатое изображение хранится во float, а исходное в uint8, который требует в 4 раза меньше байт. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим, как наш автоэнкодер без нейросетей справится с очисткой от зашумления. Для этого сделаем функцию добавления шумов к MNIST выборке и посмотрим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "def mnist_add_noise(noise_factor, dataset):\n",
    "    return dataset + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=dataset.shape\n",
    "    )\n",
    "\n",
    "\n",
    "X_test_noisy = mnist_add_noise(0.3, X_test)\n",
    "samples_noisy = X_test_noisy[sample_indices]\n",
    "plot_imgs(samples_noisy, \"X_test with added noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно провести ту же операцию PCA энкодера и декодера, что выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCArecode(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1] * dataset.shape[2])\n",
    "    return pca.inverse_transform(pca.transform(dataset_flat)).reshape(dataset.shape)\n",
    "\n",
    "X_filtered = PCArecode(X_test_noisy)\n",
    "samples_filtered = X_filtered[sample_indices]\n",
    "plot_imgs(samples_filtered, \"Noise filtered X_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты сравнения напишем функцию, которая будет строить зашумленные и восстановленные образцы друг под другом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(*args, invert_colors=True, digit_size=28, name=None):\n",
    "    args = [x.squeeze() for x in args]\n",
    "    n = min([x.shape[0] for x in args])\n",
    "    figure = np.zeros((digit_size * len(args), digit_size * n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(len(args)):\n",
    "            figure[\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "            ] = args[j][i].squeeze()\n",
    "\n",
    "    if invert_colors:\n",
    "        figure = 1 - figure\n",
    "\n",
    "    plt.figure(figsize=(2 * n, 2 * len(args)))\n",
    "\n",
    "    plt.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "\n",
    "    plt.grid(False)\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if name is not None:\n",
    "        plt.savefig(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(samples_noisy, samples_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, шумы стали значительно меньше, но и артефакты вокруг цифры усилились. Это неудивительно, ведь мы сжимали информацию линейным образом на целых 87 компоненты. Это не позволяет ни глубокого обучения, ни достаточного сжатия. Повышение уровня сжатия приведёт к еще большему количеству артефактов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Латентное представление цифр после PCA \n",
    "\n",
    "Посмотрим теперь на то, как делятся наши картинки в латентном представлении. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_latent(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1] * dataset.shape[2])\n",
    "    return pca.transform(dataset_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_manifold(latent_r, labels=None, alpha=0.9,title=None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    if labels is None:\n",
    "        plt.scatter(latent_r[:, 0], latent_r[:, 1], cmap=\"tab10\", alpha=alpha)\n",
    "        if title:\n",
    "          plt.title(title)\n",
    "    else:\n",
    "        plt.scatter(latent_r[:, 0], latent_r[:, 1], c=labels, cmap=\"tab10\", alpha=alpha)\n",
    "        plt.colorbar()\n",
    "        if title:\n",
    "          plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_r = pca_latent(X_test)\n",
    "plot_manifold(latent_r, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что латентное представление слабо разделяет картинки по тому, какие цифры на них изображены. Только 1 расположены более-менее обособленно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, вспомним, что в автоэнкодере одна сеть переводит пространство свойств в пространство меньшей размерности, а другая сеть восстанавливает исходное изображение. Вместо вычисления коэффициентов сети мы будем её обучать. Для обучения нужно определить функцию потерь. Обычно используют среднеквадратичное расстояние (MSE). То есть мы требуем, чтобы значения пикселей исходного изображения и восстановленного отличались несильно. В нашем примере, мы будем использовать  Binary Cross Etropy, он обеспечивает лучшую сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/encoder_loss.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем использовать любую сеть для энкодера и декодера: на плотных слоя или на свёрточных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно задать архитектуру модели. Мы будем использовать последовательную модель и свёрточную  архитектуру. В конце кодировщика должен быть вектор длины `latent_size`. И декодировщик должен принимать этот вектор и восстанавливать до целого изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim # latent space size\n",
    "        hidden_dims = [32, 64, 128, 256, 512] # num of filters in layers\n",
    "        modules = []\n",
    "        in_channels = 1 # initial value of channels\n",
    "        for h_dim in hidden_dims[:-1]: # conv layers\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(                    \n",
    "                        in_channels=in_channels, # num of input channels \n",
    "                        out_channels=h_dim, # num of output channels \n",
    "                        kernel_size=3, \n",
    "                        stride=2, # convolution kernel step\n",
    "                        padding=1, # save shape \n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),  \n",
    "                    nn.LeakyReLU(), \n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim # changing number of input channels for next iteration\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=1), # changing the kernel size, because  size of the array (2*2)\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "        )\n",
    "        modules.append(nn.Flatten()) # to vector, size 512 * 2*2 = 2048\n",
    "        modules.append(nn.Linear(512 * 2*2, latent_dim)) \n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dims = [512, 256, 128, 64, 32] # num of filters in layers\n",
    "        self.linear = nn.Linear(in_features=latent_dim, out_features = 512) \n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1): # define ConvTransopse layers\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels = hidden_dims[i],\n",
    "                        out_channels = hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels = hidden_dims[-1],\n",
    "                    out_channels = hidden_dims[-1],\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(in_channels = hidden_dims[-1], out_channels=1, kernel_size=7, padding=1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x) # from latents space to Linear \n",
    "        x = x.view(-1, 512, 1, 1) # reshape\n",
    "        x = self.decoder(x) # reconstruction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем основную функцию для обучения нейросети. single_pass_handler и loss_handler будут меняться в зависимости от сетки, которую мы обучаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "Function to train model, parameters: \n",
    "  enc - encoder\n",
    "  dec - decoder\n",
    "  loader - loader of data\n",
    "  optimizer - optimizer\n",
    "  single_pass_handler - return reconstructed image, use for loss \n",
    "  loss_handler - loss function \n",
    "  epoch - num of epochs\n",
    "  log_interval - output interval\n",
    "'''\n",
    "\n",
    "def train(\n",
    "    enc,\n",
    "    dec,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    single_pass_handler,\n",
    "    loss_handler,\n",
    "    epoch,\n",
    "    log_interval=500,\n",
    "):\n",
    "\n",
    "    for batch_idx, (data, lab) in enumerate(loader): \n",
    "        batch_size = data.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        lab = lab.to(device)\n",
    "\n",
    "        latent, output = single_pass_handler(encoder, decoder, data, lab) # reconstructed image drom decoder \n",
    "\n",
    "        loss = loss_handler(data, output, latent) # compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(loader.dataset),\n",
    "                    100.0 * batch_idx / len(loader),\n",
    "                ).ljust(40), \n",
    "                \"Loss: {:.6f}\".format(loss.item())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "def ae_pass_handler(encoder, decoder, data, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    recon = decoder(latent)\n",
    "    return latent, recon\n",
    "\n",
    "\n",
    "def ae_loss_handler(data, recon, *args, **kwargs):\n",
    "    return (F.binary_cross_entropy(recon, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим загрузчики наших данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим нашу нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from itertools import chain\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2 # size of latent space\n",
    "learning_rate = 1e-4 \n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на архитектуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "print('Архитектура Encoder')\n",
    "print(summary(encoder,(1,28,28)))\n",
    "print('Архитектура Decoder')\n",
    "print(summary(decoder,(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим в течение 5 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, чтобы удобно прогонять датасет через обученную нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function return transforms results to numpy for visualization \n",
    "\n",
    "encoder - encoder\n",
    "decoder - decoder\n",
    "loader - loader of data\n",
    "single_pass_handler - return latent and reconstruction transform\n",
    "return_real - return original images, True/False, default = True\n",
    "return_recon - return transformed image from decoder, True/False, default = True\n",
    "return_latent - return latent representation from encoder, True/False, default = True\n",
    "return_labels - return labels, True/False, default = True\n",
    "'''\n",
    "def run_eval(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    loader,\n",
    "    single_pass_handler,\n",
    "    return_real=True,\n",
    "    return_recon=True,\n",
    "    return_latent=True,\n",
    "    return_labels=True,\n",
    "):\n",
    "\n",
    "    if return_real:\n",
    "        real = []\n",
    "    if return_recon:\n",
    "        reconstr = []\n",
    "    if return_latent:\n",
    "        latent = []\n",
    "    if return_labels:\n",
    "        labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(loader):\n",
    "            if return_labels:\n",
    "                labels.append(label.numpy())\n",
    "            if return_real:\n",
    "                real.append(data.numpy())\n",
    "         \n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            rep, rec = single_pass_handler(encoder, decoder, data, label)\n",
    "            \n",
    "            if return_latent:\n",
    "                latent.append(rep.cpu().numpy())\n",
    "            if return_recon:\n",
    "                reconstr.append(rec.cpu().numpy())\n",
    "\n",
    "    result = {}\n",
    "    if return_real:\n",
    "        real = np.concatenate(real)\n",
    "        result[\"real\"] = real.squeeze()\n",
    "    if return_latent:\n",
    "        latent = np.concatenate(latent)\n",
    "        result[\"latent\"] = latent\n",
    "    if return_recon:\n",
    "        reconstr = np.concatenate(reconstr)\n",
    "        result[\"reconstr\"] = reconstr.squeeze()\n",
    "    if return_labels:\n",
    "        labels = np.concatenate(labels)\n",
    "        result[\"labels\"] = labels\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала оценим то, как ведет себя наш автоэнкодер работает в целом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(run_res['real'][0:9], run_res['reconstr'][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, какое латетное представление он выучил. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res['latent'], run_res['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь обучим автоэнкодер с латентным слоем размера 24. И посмотрим, как он будет бороться с шумом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем dataloader, который добавляет в наш датасет шум автоматически"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise():\n",
    "    def __init__(self, mean=0.0, std=1.0):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(mean={0}, std={1})\".format(\n",
    "            self.mean, self.std\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим MNIST с добавленным шумом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "test_noise_set = dset.MNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(), AddGaussianNoise(0.0, 0.30)]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_noised_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_noise_set, list(range(64))),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(run_res['real'][0:9], run_res['reconstr'][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сжатия мы оценили визуально выше. Если обратить внимание, то исходные картинки даже почистились от мелких шумов и странностей изображения и больше стали похожи на непрерывные линии. Размерность латентного пространства `latent_size` значительно меньше исходного количества свойств (784), поэтому мы получили неплохое сжатие изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженный автоэнкодер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы говорили о том, что построение эффективного автоэнкодера возможно только в случае, когда размер латентного слоя сильно меньше входных слоев. \n",
    "\n",
    "Что если мы сделаем автоэнкодер с размером латентного пространства больше входной размерности? \n",
    "\n",
    "При такой прямой реализации у нас ничего не получится, энкодер будет просто перемещать входные данные полностью в латентное пространство, без изменений. Наш автоэнкодер выучит, что ему нужно входные данные просто скопировать в латентное пространство, а потом перекопировать на вывод, т.е он ничего не выучит.\n",
    "\n",
    "Однако, сделать латентный слой «узким местом» сети можно и иначе. Идея такая: мы добавим loss на латентный слой, который будет заключатся в следующем, что для каждого объекта использовалось ограниченное количество нейронов. \n",
    "\n",
    "Это позволяет модели самой выбирать размер латентного представления и использовать для разных групп объектов латентные представления разного размера, что может быть полезно для части задач и позволять получить более полезное внутреннее представление. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/sparse_autoencoder.png\" alt=\"alttext\" style=\"height: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала нам нужно задать понятие активированности нейрона. По логике это должна быть величина от 0 до 1.\n",
    "\n",
    "0 соответствует полное отсутствие активации, 1 соответствует полная активация нейрона. Нам не подходит решение — применить сигмоиду к активациям, так как они могут быть и отрицательны, а сигмоида на отрицательных активациях стремится к нулю. А нам нужно, чтобы 0 соответствовало изначально нулевое значение. \n",
    "\n",
    "Потому можем сделать следующее:\n",
    "\n",
    " 1. Берем латентный слой автоэнкодера и считаем абсолютные значения активаций. \n",
    " 2. Применяем к этим значениям сигмоиду. Теперь 0 соответствует 0.5. Полученные значения гарантированно не меньше 0.5\n",
    " 3. Вычтем из этих значений 0.5 и домножим результат на 2. Теперь они распределены так, как нам нужно. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_01_activation(latent):\n",
    "    activations = (torch.sigmoid(latent.abs()) - 0.5) * 2\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим то, как для нашего автоэнкодера должен подсчитываться лосс. Он будет состоять из двух частей, первая отвечает за реконструкцию(восстановление) изображения, а вторая штрафует за включение нейронов больше некоторого порога, который мы регулируем с помощью beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_ae_loss_handler(data, recon, latent, beta=0.1, *args, **kwargs):\n",
    "    activations = to_01_activation(latent)\n",
    "    return (\n",
    "        F.binary_cross_entropy(recon, data)\n",
    "        + F.l1_loss(activations, torch.zeros_like(activations)) * beta \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим разреженный автоэнкодер, с размером латентного пространства больше, чем размерность входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 30 * 30\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=partial(sparse_ae_loss_handler, beta=0.01), # regulize beta parameter \n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_loader, ae_pass_handler)\n",
    "plot_digits(run_res['real'][0:9], run_res['reconstr'][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что разреженный автоэнкодер несмотря на большой размер латентного слоя все равно эффективно убирает шум из изображений.\n",
    "\n",
    "Посмотрим, как активируются нейроны латентного слоя для каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(nrows=2, ncols=5, figsize=(16, 5))\n",
    "activations = to_01_activation(torch.from_numpy(run_res[\"latent\"])).numpy()\n",
    "\n",
    "up_lim = activations.max()\n",
    "for label in range(0, 10):\n",
    "\n",
    "    figure = activations[run_res[\"labels\"] == label].mean(axis=0)\n",
    "    figure = figure.reshape(30, 30)\n",
    "    ax = axs[label % 2, label % 5]\n",
    "    ax.imshow(figure, cmap=\"Greys\", clim=(0, 0.5))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что нейронов активируются в среднем крайне мало, между классами активирующиеся нейроны отличаются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "def plot_percent_hist(ax, data, bins):\n",
    "    counts, _ = np.histogram(data, bins=bins)\n",
    "    widths = bins[1:] - bins[:-1]\n",
    "    x = bins[:-1] + widths / 2\n",
    "    ax.bar(x, counts / len(data), width=widths * 0.8)\n",
    "    ax.xaxis.set_ticks(bins)\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        mpl.ticker.FuncFormatter(\n",
    "            lambda y, position: \"{}%\".format(int(np.round(100 * y)))\n",
    "        )\n",
    "    )\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "def plot_activations_histogram(activations, height=1, n_bins=10):\n",
    "    activation_means = activations.mean(axis=0)\n",
    "\n",
    "    mean = activation_means.mean()\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(figsize=(10, 3), nrows=1, ncols=2, sharey=True)\n",
    "    plot_percent_hist(ax1, activations.ravel(), bins)\n",
    "    ax1.plot(\n",
    "        [mean, mean], [0, height], \"k--\", label=\"Overall Mean = {:.2f}\".format(mean)\n",
    "    )\n",
    "    ax1.legend(loc=\"upper center\", fontsize=14)\n",
    "    ax1.set_xlabel(\"Activation\")\n",
    "    ax1.set_ylabel(\"% Activations\")\n",
    "    ax1.axis([0, 1, 0, height])\n",
    "    plot_percent_hist(ax2, activation_means, bins)\n",
    "    ax2.plot([mean, mean], [0, height], \"k--\")\n",
    "    ax2.set_xlabel(\"Neuron Mean Activation\")\n",
    "    ax2.set_ylabel(\"% Neurons\")\n",
    "    ax2.axis([0, 1, 0, height])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations_histogram(activations, height=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что у нас наблюдается в целом. \n",
    "В целом, видно, что средний размер активации — в районе 0.10. Причем часто активация равна 0. \n",
    "Для нейронов наблюдается похожая картина — в среднем активация нейрона по всему тестовому датасету расположена в районе 0.10 и нейроны очень редко отходят от этой области. \n",
    "\n",
    "Вообще говоря, нам нужная именно правая картина. Левая — лишь побочный результат применения нами L1-лосса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход с L1-лосс очень просто реализуется, но в то же время не совсем очевидно, как с помощью него задать условие вида: пусть в среднем на латентном слое активируется 1% нейронов. Понятно, что это косвенно задается величиной коэффициента $\\beta$, но хотелось бы задавать это в явном виде. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дивергенция Кульбака-Лейблера\n",
    "\n",
    "Для данной цели используется дивергенция Кульбака-Лейблера, которая считается по формуле: \n",
    "\n",
    "$$KL(P||Q) = \\int_X p(x)\\log \\dfrac {p(x)} {q(x)} dx$$\n",
    "\n",
    "В теории информации $p$ считается целевым распределением, а $q$ — тем, с которым мы его сравниваем. \n",
    "Важно понимать, что при этом $KL$ не является мерой расстояния, а именно в общем случае $KL(P||Q) != KL(Q||P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Оказывается](https://math.stackexchange.com/questions/90537/what-is-the-motivation-of-the-kullback-leibler-divergence), в подобных задачах она как правило обеспечивают бОльшую сходимость к требуемому распределению, нежели та же L1-регуляризация.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/kl_plot.png\" alt=\"alttext\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы хотим, чтобы на каждом слое для данного объекта активировалось в среднем p% нейронов.\n",
    "\n",
    "В нашем случае для каждой активации i нейрона латентного слоя $a_i^{latent}$ мы можем решить, активирован нейрон или нет. Мы можем посчитать для каждого объекта, какая доля нейронов активировалась в его случае. \n",
    "\n",
    "Или же мы можем усреднить активации нейронов, если активации распределены на отрезке от 0 до 1 (например, мы можем преобразовать активации, как это сделали выше). Получим величину $\\hat{p}$\n",
    "\n",
    "Фактически мы сравниваем два бернулиевских распределения — то, которое хотим мы, с параметром p, и то, которое мы наблюдаем — с оценным параметром $\\hat{p}$. \n",
    "\n",
    "$$KL(P||Q) =  p(x) \\log \\dfrac {p(x)} {\\hat{p}(x)} + (1 - p(x)) \\log \\dfrac {(1 - p(x))} {1 - \\hat{p}(x)} $$\n",
    "\n",
    "Далее лишь остается просуммировать данный лосс по батчу.\n",
    "\n",
    "Можно делать и иначе — требовать, чтобы каждый нейрон в среднем активировался в p% случаев. В этом случае на первом шаге мы усредняем активации не по всему слою, а по батчам. А вот подсчитанный лосс усреднеяем по всем нейронам слоя. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем KL-лосс лучше L1 и L2 лоссов? \n",
    "Он позволяет нам приближаться к решению более плавно и четко регулировать долю активирующихся нейронов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "p = 0.1\n",
    "q = np.linspace(0.001, 0.999, 500)\n",
    "kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))\n",
    "mse = (p - q) ** 2\n",
    "mae = np.abs(p - q)\n",
    "plt.plot([p, p], [0, 0.3], \"k:\")\n",
    "plt.text(0.05, 0.32, \"Target\\nsparsity\", fontsize=14)\n",
    "plt.plot(q, kl_div, \"b-\", label=\"KL divergence\")\n",
    "plt.plot(q, mae, \"g--\", label=r\"MAE ($\\ell_1$)\")\n",
    "plt.plot(q, mse, \"r--\", linewidth=1, label=r\"MSE ($\\ell_2$)\")\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.xlabel(\"Actual sparsity\")\n",
    "plt.ylabel(\"Cost\", rotation=0)\n",
    "plt.axis([0, 1, 0, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодер как генератор и его ограничения. Плавная интерполяция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас уже была система с латентным пространством и возможностью строить по нему объекты — GAN. Значит в случае автоэнкодеров тоже можно подавать случайный вектор на декодер и получать новые объекты. До этого мы же только восстанавливали исходную картинку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/encoder_as_generator.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какое значение вектора выбрать? Мы же никак не управляли латентным пространством. Непонятно какие числа подставлять. Поэтому мы можем выбрать промежуточные значения между двумя представлением двух исходных изображений в латентном пространстве и получить плавную интерполяцию между изображениями. Постепенно свойства одного изображения будут исчезать, а появляться свойства другого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим сначала обычный автоэнкодер "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем несколько изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space1 = encoder(imgs[labels==7][0:1].to(device))\n",
    "latent_space2 = encoder(imgs[labels==6][0:1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_steps = 10\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1.repeat(interp_steps, 1),\n",
    "    latent_space2.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(nrows=1, ncols=interp_steps, figsize=(16, 4))\n",
    "for label in range(0, interp_steps):\n",
    "    figure = iterp_imgs[label].cpu().detach().numpy()\n",
    "    figure = figure.reshape(28, 28)\n",
    "    ax = axs[label]\n",
    "    ax.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы увидеть более плавные изменения, можем сделать видео. Для этого можно использовать уже известный нам OpenCV. Он умеет делать видеофайлы из массивов чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "interp_steps = 200\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1.repeat(interp_steps, 1),\n",
    "    latent_space2.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "\n",
    "resize_coeff = 10\n",
    "imgs = np.squeeze(iterp_imgs.cpu().detach().numpy())\n",
    "size = (imgs.shape[1] * resize_coeff, imgs.shape[2] * resize_coeff)\n",
    "\n",
    "\n",
    "imgs = [\n",
    "    Image.fromarray(np.uint8(img * 255)).resize(size).convert(\"RGB\") for img in imgs\n",
    "]\n",
    "imgs[0].save(\n",
    "    \"ae_img.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    optimize=False,\n",
    "    duration=40,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as iImage\n",
    "\n",
    "iImage(open(\"ae_img.gif\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так себе интерполяция вышла. Старое изображение затухает, а новое изображение появляется. Хочется, чтобы в промежуточных кадрах не было каких-то непонятных очертаний, а изображение было чем-то промежуточным по смыслу между стартовым и конечным изображением. Причина неудачи в том, что в латентном пространстве действительно возникли зоны, которые умеют декодироваться в хорошие изображения. Но никто не сказал, что между этими зонами должно быть что-то адекватное (что мы видели из представления)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/bad_latent_for_ae.png\" alt=\"alttext\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим это графически. Пусть наш очень умный, содержащий очень много коэффициентов, энкодер и декодер смог разложить все входные объекты на одной оси (размерность латентного пространства — 1). По сути он каждому входному изображению присвоил номер и по номеру может это изображение вспомнить. То есть автоэнкодер очень переобученный. Тогда если мы возьмём промежуточный номер (пытаемся интерполировать), то какое изображение мы собираемся получить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/bad_latent_for_ae2.png\" alt=\"alttext\" width=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы хотим, чтобы декодированные промежуточные латентные состояния имели черты близких к ним объектов, то надо притянуть латентные координаты похожих объектов. Например вот так:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/good_latent_for_ae.png\" alt=\"alttext\" width=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариационные автоэнкодеры (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мотивация:\n",
    "\n",
    "Хотим вместо представления слева получить представление справа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/vae_motivation.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "При этом зоны пересечения должны действительно содержать переходные картины \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/vae_ideal.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение с помощью регуляризации\n",
    "\n",
    "Можем попробовать заставить наши объекты «лежать» рядом — будем штрафовать латентные представления, которые далеко уходят от начала координат. \n",
    "\n",
    "Можем использовать как L1, так и L2 регуляризацию, так и их комбинацию — elastic loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, это приведет просто к масштабированию распределения (по сути — мы уже увидели результат этого для случая разреженного аутоэнкодера, у которого эта регуляризация есть). Нам надо одновременно получить связное латентное представление — чтобы у нас не возникало зон в латентном представлении, которым не соответствует ничего, и при этом представление, в котором цифры будут отделены друг от друга. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/reconstruction_loss_only.png\" alt=\"alttext\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Intuitively Understanding Variational Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы имеем ситуацию, как на картинке слева. Переход-интерполяция между объектами проходит через зону отсутствующих в обучении объектов, декодирование которых даст несуществующие в реальности объекты. Нам не удастся погенерировать новые картинки, преобразовывая случайную точку из латентного пространства в случайную картинку. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация вариационного автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом постановка задачи с автоэнкодером говорит нам, что есть некое пространство меньшей размерности $Z$, которое и обуславливает процесс генерации объектов из $X$. Все остальные различия — следствия случайности — один и тот же человек может по-разному нарисовать цифру 5. \n",
    "\n",
    "Будем искать латентное пространство Z, которое удовлетворяет следующему условию:\n",
    "\n",
    "$$p(x) = \\int p(x, z)dz $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, пусть объекты из Z легко генерировать. \n",
    "\n",
    "По формуле совместной вероятности:\n",
    "\n",
    "$$p(x, z) = p(x|z)p(z) $$\n",
    "\n",
    "Ну, осталось только подобрать такие параметры, чтобы все работало)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, сделать это в таком виде не получится. Пространство $X$ может быть высокоразмерным. \n",
    "\n",
    "Но — мы можем существенно сузить область поиска, ведь каждому $x$ из пространства $X$ соответствует лишь небольшая возможная область в $Z$.\n",
    "\n",
    "Для этого будем также учить отображение из пространства $X$ в пространство $Z$, т.е, пытаться выучить $p(z|x)$. Назовем функцию, которой будем его приближать $q(z|x)$. \n",
    "\n",
    "Что же в случае автоэнкодера выполняет роль p(x|z) и q(z|x) ?\n",
    "Очевидно — кодировщик и декодировщик соответственно. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/vae_as_two_functions.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Чтобы все получилось, нужно сделать с кодировщиком две вещи. Заметьте, что декодировщик мы оставим без изменений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первая модификация\n",
    "\n",
    "Пусть наш кодировщик генерирует на основе объекта X вектор средних и вектор стандартных отклонений.\n",
    "\n",
    "Этих двух векторов хватает нам для того, чтобы задать многомерное нормальное распределение с независимыми компонентами (чтобы матрица ковариаций была диагональной), соответствующее данному объекту.\n",
    "\n",
    "Чтобы получить латентное представление объекта, отличающегося от X только в силу случайности нам достаточно сгенерировать вектор из нормального распределения с такими параметрами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/vae_structure.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы можем требовать, чтобы из полученного латентного представления декодировшик восстанавливал объект, похожий на исходный. \n",
    "\n",
    "Здесь, однако, сразу возникает возникает проблема с тем, что граф вычислений, соответствующий предыдущей структуре не может пропускать градиент — как пропустить градиент через генератор случайного нормального числа? Если считать из определения, то даже малейшему изменению параметра могут соответствовать бесконечные изменения генерируемого числа (нормальное распределение определено на бесконечности). В общем, проблема. \n",
    "\n",
    "Но мы можем вспомнить замечательное свойство одномерного нормального распределения:\n",
    "\n",
    "$$N(\\mu,\\sigma^2) = N(0,1) * \\sigma + \\mu$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняется это и для многомерного случая. Потому сделаем следующее — будем генерировать значение из нормального распределения с средними 0 и дисперсиями 1, а затем домножать это на вектор стандартных отклонений и прибавлять вектор средних. Получится вот такое преобразование, которое называется **reparametrization trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/reparametrization_trick.png\" alt=\"alttext\" width=\"850\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличии от левого случая, в правом мы спокойно можем пропустить градиент через детерминистичные ноды. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но если просто применить такой принцип, то он снова имеет проблему предыдущего детерминистического подхода, так как вероятностное распределение сможет свернуться в дельта-функцию — зачем нейросети мучаться с объектами, немного отличающимися от тех, что есть в обучающей выборке и пытаться нормально их восстанавливать, если можно просто начать генерировать стандартные отклонения, близкие к нулю и тем самым получить $\\delta$-функцию, которая будет нашему объекту всегда сопоставлять одну точку в латентном представлении. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/Dirac_function_approximation.gif\" alt=\"alttext\" width=\"170\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вторая модификация\n",
    "\n",
    "Поэтому нам надо ввести регуляризацию, требующую от каждого распределения быть близким к нормальному распределению вокруг нуля координат латентного пространства с дисперсией 1 (наше $P(z)$). \n",
    "\n",
    "В принципе, нам подойдёт любая адекватная мера расстояния между двумя распределениями. Мы уже использовали KL-дивергенцию, будем использовать ее и здесь "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Только KL-дивергенция\n",
    "\n",
    "Что произойдет, если мы будем использовать такой лосс при обучении модели?\n",
    "\n",
    "$$ Loss =  KL(Q(z|x)||P(z)) $$\n",
    "\n",
    "Если подставить на место p(z) нормальное распределение, то получим следующее выражение:\n",
    "\n",
    "\n",
    "$$KL(N(\\mu, \\sigma) || N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}$$\n",
    "\n",
    "$$KL(N(\\mu, \\sigma) || N(0, 1)) = -\\log {\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2} = -\\frac {1} {2}(2 \\log {\\sigma} - \\sigma^2 - \\mu^2 + 1) = -\\frac {1} {2}(\\log {\\sigma^2} - \\sigma^2 - \\mu^2 + 1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/pure_kl_loss.png\" alt=\"alttext\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Видим, что мы забываем про декодировщик — он может выдавать все, что угодно. Потому логично ожидать, что обучится только кодировщик и обучится он отражать наши точки в нормальное распределение со средним 0 и дисперсией 1. И все, большего ему в жизни и не надо. Можем проверить это. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class VAEEncoder(Encoder):\n",
    "    def __init__(self, latent_dim):\n",
    "        if latent_dim % 2 != 0: # check for the parity of the latent space\n",
    "            raise Exception(\"Latent size for VAEEncoder must be even\")\n",
    "        super().__init__(latent_dim)\n",
    "\n",
    "\n",
    "def vae_split(latent):\n",
    "    size = latent.shape[1] // 2 # divide the latent representation into mu and log_var\n",
    "    mu = latent[:, :size] \n",
    "    log_var = latent[:, size:]  \n",
    "    return mu, log_var\n",
    "\n",
    "\n",
    "def vae_reparametrize(mu, log_var): \n",
    "    sigma = torch.exp(0.5 * log_var) \n",
    "    z = torch.randn(mu.shape[0], mu.shape[1]).to(device) \n",
    "    return z * sigma + mu \n",
    "\n",
    "\n",
    "def vae_pass_handler(encoder, decoder, data, *args, **kwargs): \n",
    "    latent = encoder(data) \n",
    "    mu, log_var = vae_split(latent) \n",
    "    sample = vae_reparametrize(mu, log_var) \n",
    "    recon = decoder(sample)\n",
    "    return latent, recon\n",
    "\n",
    "\n",
    "def kld_loss(mu, log_var): \n",
    "    var = log_var.exp()\n",
    "    kl_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - var, dim=1), dim=0)\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "def kl_loss_handler(data, recon, latent, kld_weight=0.1, *args, **kwargs):\n",
    "    mu, log_var = vae_split(latent)\n",
    "    kl_loss = kld_loss(mu, log_var)\n",
    "    return kld_weight * kl_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 3):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=kl_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae_split(run_res['latent'])\n",
    "var = np.exp(log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все генерируемые средние почти неотличимы от нуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mu.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все генерируемые дисперсии почти неотличимы от 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получили практически не разделимые объекты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette(\"Paired\", n_colors=10)\n",
    "plot_manifold(mu, run_res[\"labels\"], title='Manifold mu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совмещаем ошибку восстановления и KL-дивергению\n",
    "\n",
    "\n",
    "Поэтому мы должны сохранить исходный лосс — декодировщик штрафуется за то, что не может нормально реконструировать объект. \n",
    "\n",
    "Формально это записывается следующим образом: \n",
    "\n",
    "$$ vae\\_loss = E_{z \\sim Q(z|x)}[logP(x|z)] + KL[Q(z|x)||P(z)]$$\n",
    "\n",
    "А в итоге:\n",
    "\n",
    "$$ vae\\_loss = ||x - \\tilde{x}||^2 - \\frac 1 2 (1 + \\log\\sigma^2 - \\mu^2 - \\sigma^2)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Вторая компонента осталась без изменений, а первая — это красиво записанное требование корректно восстанавливать объекты из обучающей выборки и чтобы при этом объекты, полученные их небольшим изменением за счет случайности также восстанавливались в объекты, близкие к объектам из тренировочной выборки. И удовлетворять этой компоненте лосса мы можем за счет того же лосса, который использовали в обычном автоэнкодере. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учет обеих компонент позволяет нам получить то, что мы хотели — непрерывное простанство, где нет «дыр» в представлении, и при этом близкие по смыслу объекты расположены рядом, а далекие — далеко. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/kl_repr_loss.png\" alt=\"alttext\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем наш новый loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_handler(data, recons, latent, kld_weight=0.005, *args, **kwargs):\n",
    "    mu, log_var = vae_split(latent)\n",
    "    kl_loss = kld_loss(mu, log_var)\n",
    "    return kld_weight * kl_loss + F.binary_cross_entropy(recons, data) # add bce loss(reconstruction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим наш VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae_split(run_res['latent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette('Paired', n_colors=10)\n",
    "plot_manifold(mu, run_res['labels'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что цифры разделились в пространстве, но при этом жмутся друг к другу. При этом, что интересно, 4 и 9 почти неотличимы. Это можно объяснить тем, что двух компонент недостаточно, чтобы разделить настолько похожие цифры (по сути, все отличие в заполненности области между двумя рожками 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как теперь получится интерполировать между 1 и 9. Для большей красоты и сравнимости с обычным автоэнкодером картин возьмем latent space такого же размера, как у него (24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "latent_space1_mu, _ = vae_split(encoder(imgs[labels == 7][0:1].to(device)))\n",
    "latent_space2_mu, _ = vae_split(encoder(imgs[labels == 6][0:1].to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_steps = 10\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1_mu.repeat(interp_steps, 1),\n",
    "    latent_space2_mu.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "_, axs = plt.subplots(nrows=1, ncols=interp_steps, figsize=(16, 4))\n",
    "for label in range(0, interp_steps):\n",
    "    figure = iterp_imgs[label].cpu().detach().numpy()\n",
    "    figure = figure.reshape(28, 28)\n",
    "    ax = axs[label]\n",
    "    ax.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим плавную интерполяцию. Посмотрим на примере с видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "interp_steps = 200\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1_mu.repeat(interp_steps, 1),\n",
    "    latent_space2_mu.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "\n",
    "\n",
    "resize_coeff = 10\n",
    "imgs = np.squeeze(iterp_imgs.cpu().detach().numpy())\n",
    "size = (imgs.shape[1] * resize_coeff, imgs.shape[2] * resize_coeff)\n",
    "\n",
    "\n",
    "imgs = [\n",
    "    Image.fromarray(np.uint8(img * 255)).resize(size).convert(\"RGB\") for img in imgs\n",
    "]\n",
    "imgs[0].save(\n",
    "    \"vae_img.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    optimize=False,\n",
    "    duration=40,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as iImage\n",
    "iImage(open('vae_img.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все переходы понятны и в процессе не возникает невозможных цифр"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы используем размерность латентного пространства 2, то это позволит нам получать распределение классов цифр на плоскости, типа такого:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/vae_sampling.png\" alt=\"alttext\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Это не просто интерполяция по двум направлениям. Тут именно все 10 цифр должны так занять место на плоскости, чтобы плавно перетекать друг в друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, наконец, что вариационный автоэнкодер работает как автоэнкодер, и может, к примеру, убирать шум"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_loader, vae_pass_handler)\n",
    "plot_digits(run_res['real'][0:9], run_res['reconstr'][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что автоэнкодер работает, пусть не лучше, чем обычный, возможно, даже хуже. Можно добиться улучшения его работы, поставив меньший вес KL-лоссу и увеличив латентное пространство. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично посмотрим, как он восстанавливает изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)\n",
    "plot_digits(run_res['real'][0:9], run_res['reconstr'][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает, но изображения получаются \"размытыми\". Это следствие сэмплирования из нормального распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторная арифметика\n",
    "\n",
    "В принципе, можно даже в латентном пространстве брать разницу черт написания двух одинаковых цифр, прибавлять к другой цифре, получая в результате цифру, написанную немного по-другому. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Такое можно делать и для других примеров — добавлять людям на изображении очки. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/vector_arithm3.png\" alt=\"alttext\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "или получать нечто среднее между двумя объектами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/vector_arithm2.png\" alt=\"alttext\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее: \n",
    "\n",
    "У нас есть 1, написанная без наклона и 1, написанная с наклоном. \n",
    "И у нас есть 9 без наклона.\n",
    "\n",
    "Вычитаем из латентного кода 1 с наклоном латентный код единицы без наклона и прибавляем к 9. Если все пройдет хорошо — получим девятку с наклоном. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/vector_arithm.png\" alt=\"alttext\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем это сделать сами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "real9_f = imgs[labels == 9][0:1]\n",
    "real9_s = imgs[labels == 9][1:2]\n",
    "real1 = imgs[labels == 1][0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "Image.fromarray(np.uint8(np.squeeze(real9_f.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(real9_s.numpy()) * 255) ).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(real1.numpy()) * 255) ).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_9f, _ = vae_split(encoder(real9_f.to(device)))\n",
    "latent_9s, _ = vae_split(encoder(real9_s.to(device)))\n",
    "latent_1, _ = vae_split(encoder(real1.to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = latent_1 + latent_9f - latent_9s\n",
    "gen = decoder(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(gen.cpu().detach().numpy()) * 255) ).resize(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось перенести на 1 часть стиля 9. Получилось плохо. Обычно для получения возможности использовать векторную арифметику, используют специальные лоссы и архитектуры. Простой VAE не гарантирует того, что \"фокус удастся\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Почему KL(Q||P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дивергенция Кульбака-Лейблера:\n",
    "$$KL(P||Q) =  p(x) \\log \\dfrac {p(x)} {\\hat{p}(x)} + (1 - p(x)) \\log \\dfrac {(1 - p(x))} {1 - \\hat{p}(x)} $$\n",
    "\n",
    "Внимательный слушатель может заметить, что почему-то здесь мы используем не KL(P||Q), как в прошлый раз, а KL(Q||P). Одним из объяснений этого является то, что если Q — распределение, которым мы аппроксимируем реальное, то может минимизация KL(P||Q) и KL(Q||P) ведет к разному результирующему Q. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/KL_inclusive_exclusive.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оказывается, чаще всего нам предпочтительнее получать первое поведение, так как лучше не генерировать какую-то часть объектов, чем генерировать объекты, настоящая вероятность которых близка к 0 (обратите внимание на область второго графика, где реальное распределение P жмется к 0, а Q при этом наоборот, достигает максимума)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблемы  «ванильного» VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из проблем VAE, с которой можно столкнуться, состоит в том, что две компоненты лосса конфликтуют друг с другом. Если будет доминировать KL-loss, то мы получим представление, из которого наши объекты очень плохо восстанавливаются — они раскиданы по представлению, как угодно. \n",
    "\n",
    "Если же наоборот, будем доминировать reconstruction loss, то мы получим ситуацию, в которой объекты восстанавливаются нормально, но никакой непрерывностью и не пахло. \n",
    "\n",
    "Проблема возникает и с самой KL-дивергенцией, у которой есть ряд существенных недостатков. Есть другие способы оценки близости двух распределений, которые порой дают лучшие результаты. К ним относится дивергенция Йенсена — Шеннона, которую мы вскользь затронем далее и метрика Васерштейна (используется в Wasserstein autoencoders), изучение которой выходит за рамки курса. \n",
    "\n",
    "Кроме того, в случае, когда декодировщик содержит значительно больше параметров, нежели кодировщик, может возникать ситуация, при которой сгенерированное латентное представление игнорируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодеры с условием(CAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мотивация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как, используя обычный VAE, сгенерировать картинку с заданной меткой? \n",
    "\n",
    "На самом деле, задача нетривиальна. Как вариант, мы можем понять, в какую область латентного пространства VAE отображает все 0 и затем сэмплировать уже из этой области. \n",
    "\n",
    "Хорошо, а если мы хотим нарисовать 1 тем же почерком, которым нарисована данная нам тройка? В этом случае классический VAE вообще не получится использовать. \n",
    "\n",
    "На самом деле, есть еще одна проблема. Что, если распределение объектов действительно сильно зависит от какой-то дополнительной информации, например, того, какую цифру хотел изобразить человек? Тогда KL-loss будет пытаться «скрестить ежа с ужом» и в результате мы получим очень странное представление и опять же, на границах могут получаться несуществующие в реальности мутанты (если внимательно посмотрите на предыдущую картинку — так и получается). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Несвязные компоненты и автокодировщик с условиями\n",
    "\n",
    "Продемонстрировать мы это можем на модельной задаче. Сгенерируем два несвязных набора точек в двумерном пространстве, каждый из которых представляет собой некий паттерн с добавленным шумом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# create dataset\n",
    "x1 = np.linspace(-2.2, 2.2, 2000)\n",
    "fx = np.sin(x1)\n",
    "dots1 = np.vstack([x1, fx]).T\n",
    "\n",
    "t = np.linspace(0, 2 * np.pi, num=2000)\n",
    "dots2 = 0.5 * np.array([np.sin(t), np.cos(t)]).T + np.array([1.5, -0.5])[None, :]\n",
    "\n",
    "dots = np.vstack([dots1, dots2])\n",
    "noise = 0.06 * np.random.randn(*dots.shape)\n",
    "\n",
    "labels = np.array([0] * x1.shape[0] + [1] * t.shape[0])\n",
    "noised = dots + noise\n",
    "\n",
    "\n",
    "# Визуализация\n",
    "colors = [\"b\"] * x1.shape[0] + [\"g\"] * t.shape[0]\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простой автоэнкодер на полносвязных слоях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлых примерах мы этим пренебрегали, но строго говоря автоэнкодер тоже может переобучаться. Потому сделаем разбиение на обучение и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(noised, test_size=0.25, random_state=42)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сильно не мучаться, поставим просто scheduler, который автоматически уменьшает learning rate нашей сети, если она переобучается или просто не улучшает качество на валидационном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encdec = SimpleEncoderDecoder() \n",
    "optimizer = optim.Adam(encdec.parameters()) \n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=50) # to optimize learning rate \n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5000)): \n",
    "    optimizer.zero_grad()\n",
    "    X_restored = encdec(X_train)\n",
    "    loss = criterion(X_train, X_restored)\n",
    "    loss.backward()\n",
    "    if optimizer.param_groups[0][\"lr\"] < 10e-7: # if learning step becomes too small\n",
    "        print(epoch) \n",
    "        break \n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_restored = encdec(X_test)\n",
    "        val_loss = criterion(X_test, X_restored)\n",
    "    scheduler.step(val_loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_restored = encdec(X_test) \n",
    "    dots_restored = X_restored.numpy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что выучил автоэнкодер. Видим, что он показывает связь там, где она явно отсутствует. Требование получить одно и то же представление для точек из двух паттернов мешают автоэнкодеру нормально выучить эти паттерны — они получаются смазанными или даже неверными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.scatter(dots_restored[:, 0], dots_restored[:, 1], color=\"grey\", linewidth=4)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что наш автоэнкодер восстанавливает часть объектов в область, где ничего нет. Потому что у него нет воможности понять, что это две несвязные компоненты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что будет, если мы будем передавать в кодировщик и в декодировщик метку объекта? \n",
    "Тогда окажется, что наш автокодировщик работает в разы лучше:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConditionalEncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y.view(-1, 1)], dim=1) # combine the labels with X, change the dimension of the labels\n",
    "        z = self.encoder(x)\n",
    "        x = torch.cat([z, y.view(-1, 1)], dim=1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    noised, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "Y_train = torch.from_numpy(Y_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "Y_test = torch.from_numpy(Y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "encdec = SimpleConditionalEncoderDecoder()\n",
    "optimizer = optim.Adam(encdec.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=50)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5000)):\n",
    "    optimizer.zero_grad()\n",
    "    X_restored = encdec(X_train, Y_train)\n",
    "    loss = criterion(X_train, X_restored)\n",
    "    loss.backward()\n",
    "    if optimizer.param_groups[0][\"lr\"] < 10e-7:\n",
    "        print(epoch)\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_restored = encdec(X_test, Y_test)\n",
    "        val_loss = criterion(X_test, X_restored)\n",
    "    scheduler.step(val_loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_restored = encdec(X_test, Y_test)\n",
    "    dots_restored = X_restored.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.scatter(dots_restored[:, 0], dots_restored[:, 1], color=\"grey\", linewidth=4)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ситуация стала лучше. То, что мы применили, называется условными автоэнкодерами (Conditional AE). Конкретно — вместе с признаковым описанием объекта мы также передаем метки, которые указывают на то, что он относится к каким-то важным группам объектов, для которых, возможно, сети нужно учить отличное от других представление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Условные вариационные автоэнкодеры (CVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация вариационного автоэнкодера с условиями, CVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычные автокодировщики с условиями применяются редко — они по-прежнему не гарантируют нам связность представления в пределах одной метки. \n",
    "\n",
    "Однако, добавление меток в вариационный автокодировщик часто помогает решать уже описанные задачи на хорошем уровне. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/cvae.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как подмешивать метку к изображению, чтобы передать это полностью конволюционной нейронной сети — не очевидно. Да обычно и не надо нам это. Часто достаточно передавать метку только декодеру. Энкодер имеет в распоряжении изначальный объект — при желании может предсказать его метку сам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем код для CVAE. По сути надо поменять только декодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dims = [512, 256, 128, 64, 32]\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=latent_dim + 10, out_features=hidden_dims[0] # add +10(num of labels) to latent space\n",
    "        )\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        hidden_dims[i],\n",
    "                        hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    hidden_dims[-1],\n",
    "                    hidden_dims[-1],\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(hidden_dims[-1], out_channels=1, kernel_size=7, padding=1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x, lab):\n",
    "        x = torch.cat([x, lab], dim=1)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, 512, 1, 1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_pass_handler(encoder, decoder, data, label, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    mu, log_var = vae_split(latent)\n",
    "    sample = vae_reparametrize(mu, log_var)\n",
    "    label = torch.nn.functional.one_hot(label, num_classes=10) # labels to ohe \n",
    "    recon = decoder(sample, label)\n",
    "    return latent, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-2\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = CDecoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=cvae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, cvae_pass_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавив передачу метки в декодер, мы позволили автоэнкодеру отображать все цифры в \"одно место\". За счет этого ему легче стало учиться, лосс стал чуть ниже. \n",
    "\n",
    "При этом если нарисовать латентное представление для всех наших цифр разом, получится комок, сосредоточенный в области нормального распределения. Это не значит, что оно плохое. Просто наша картина не учитывает, что нейросеть различает цифры теперь по меткам, а не по латентному представлению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res['latent'], run_res['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у каждой цифры \"свое\" нормальное распределение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res['latent'][run_res['labels'] == 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res['latent'][run_res['labels'] == 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация заданных цифр из латентного распределения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядят наше латентное представление, скажем, для четверок, которых мы до этого почти не видели (сливались с 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20\n",
    "space1 = torch.linspace(-2, 2, steps)\n",
    "space2 = torch.linspace(-2, 2, steps)\n",
    "grid = torch.cartesian_prod(space1, space2)\n",
    "label = torch.full((grid.shape[0],), 4)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)\n",
    "with torch.no_grad():\n",
    "\n",
    "    imgs = decoder(grid.to(device), label.to(device))\n",
    "    imgs = imgs.cpu().numpy().squeeze()\n",
    "\n",
    "plot_digits(*[imgs[x : x + steps] for x in range(0, steps * steps, steps)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, как у нас четверки плавно расположены по стилю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20\n",
    "space1 = torch.linspace(-2, 2, steps)\n",
    "space2 = torch.linspace(-2, 2, steps)\n",
    "grid = torch.cartesian_prod(space1, space2)\n",
    "label = torch.full((grid.shape[0],), 9)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)\n",
    "with torch.no_grad():\n",
    "    imgs = decoder(grid.to(device), label.to(device))\n",
    "    imgs = imgs.cpu().numpy().squeeze()\n",
    "\n",
    "plot_digits(*[imgs[x : x + steps] for x in range(0, steps * steps, steps)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При желании, можно посмотреть на процесс того, как нейросеть все это учит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведено то, как хорошо выполняют задачу генерации заданной цифры нейросеть по мере обучения и то, как выглядит латентное представление объектов, относящихся к данной цифре. \n",
    "\n",
    "Для 3:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/gen_cvae_3.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/lat_cvae_3.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для 7:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/gen_cvae_7.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/lat_cvae_7.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация заданных цифр из латентного распределения\n",
    "\n",
    "Также успешно такая нейросеть справится в задаче, где мы используем латентное представление одной цифры  для того, чтобы сгенерировать цифру с таким же стилем написания. \n",
    "\n",
    "Чтобы сделать это, достаточно просто получить латентное представление для 7, а затем передать его в декодер с меткой 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/style_transfer2.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты переноса стилей для нескольких разных 7 представлены ниже.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/style_transfer.png\" alt=\"alttext\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем то же самое для наших двоек и пятерок. \n",
    "Выберем двойку и сгенерим несколько 5 с ее стилем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "real = imgs[labels == 2][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "size = (256, 256)\n",
    "Image.fromarray(np.uint8(np.squeeze(real.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "mu, log_var = vae_split(encoder(real.to(device)))\n",
    "sigma = torch.exp(0.5 * log_var)\n",
    "z = torch.randn(sample_size, mu.shape[1]).to(device)\n",
    "latent = z * sigma + mu\n",
    "\n",
    "\n",
    "label = torch.full((sample_size,), 5)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = decoder(latent.to(device), label.to(device))\n",
    "    imgs = np.squeeze(imgs.cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Состязательные автокодировщики (AAE = AE + GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе курса вы уже познакомились с генеративными состязательными сетями. Возникает искушение как-то использовать принципы, лежащие в их основе, для VAE. Действительно, так делают. Такие нейросети называются **adversarial autoencoders**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать мы будем дискриминатор. К какой части нейросети мы можем его «приделать»? На самом деле — к любой. Но самое распространенное — а давайте уберем наши мучения с KL-дивергенцией. Пусть теперь дискриминатор будет отличать латентное представление, которое мы сгенерировали от стандартного нормального распределения. Если может отличить хорошо — то штрафуем энкодер за это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/aae.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внезапно, это избавляет нас от необходимости даже делать какие-то дополнительные манипуляции с кодировщиком — нам не нужно теперь, чтобы он выдавал средние и дисперсии, а потом мы использовали их для генерации объектов из реального распределения. Нам достаточно того, что дискриминатор не может отличить латентное представление, которое мы получаем от нормального распределения. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, оказывается, что AAE может генерировать более «качественные» объекты, нежели ванильный VAE. Можно теоретически показать, что это является следствием того, что он минимизирует не KL-divergence, а более эффективную дивергенцию Йенсена-Шеннона. Однако подробный разбор этого выходит за рамки курса. \n",
    "\n",
    "Кроме того, для AAE легче добиться того, чтобы ваше латентное представление выглядело специфичным образом. Просто генерируем выборку из нужного распределения и говорим дискриминатору пытаться отличать сгенерированную выборку от объектов из латентного представления. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img/different_latent_spaces.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "И все это без введения дополнительной лосс-функции! Фактически — GAN и есть наша loss-функция. Мы используем одну нейросеть в качестве лосс-функции для обучения другой нейросети. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим дискриминатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.discriminator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какое бы латетное распределение нам выбрать? Давайте выберем равномерное распределение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform(shape):  # U[-2.5, 2.5]\n",
    "    return torch.rand(*shape) * 5 - 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "manifold = generate_uniform( (10000, 2) )\n",
    "plot_manifold(manifold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем части нашей нейросети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "discriminator = Discriminator(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# optimizer for AE, how well it restore the image\n",
    "ae_optimizer = optim.Adam(\n",
    "    chain(\n",
    "        encoder.parameters(),\n",
    "        decoder.parameters(),\n",
    "    ),\n",
    "    weight_decay=weight_decay,\n",
    "    lr=learning_rate,\n",
    ")\n",
    "# optimizer for generator, how well generates real images\n",
    "gen_optimizer = optim.Adam(\n",
    "    encoder.parameters(), weight_decay=weight_decay, lr=learning_rate\n",
    ")\n",
    "# optimizer for discriminator\n",
    "dis_optimizer = optim.Adam(\n",
    "    discriminator.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы увеличиваем размер батча для лучшей сходимости. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 512\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После каждого шага будем визуализировать латентное представление. \n",
    "Обучение нейросети идет не очень быстро — у нас считаются сразу три лосса, по которым делается три обновления сети за шаг. \n",
    "Кроме того, мы сделали большой размер батча реальных примеров равномерного распределения для дискриминатора, чтобы ему легче было изучить латентное пространство, что также улучшает сходимость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# simple uniform\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for ind, batch in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        # testing AE\n",
    "        ae_optimizer.zero_grad()\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        latent = encoder(imgs)\n",
    "        restored = decoder(latent)\n",
    "        ae_loss = F.binary_cross_entropy(restored, imgs)\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        # train encoder(generator) \n",
    "        latent = encoder(imgs) # generating a latent distribution from encoder\n",
    "        gen_optimizer.zero_grad()\n",
    "        neg_pred = discriminator(latent) # discriminator predicts ones for this latent space\n",
    "        fake_labs = torch.ones(latent.shape[0]).view(-1, 1).to(device) # generate ones\n",
    "        gen_loss = F.binary_cross_entropy(neg_pred, fake_labs) * 0.01 # compute loss\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # train discriminator \n",
    "        dis_optimizer.zero_grad()\n",
    "        negative = latent.detach() # use detach(fake and real values are equivalent)\n",
    "        positive = generate_uniform(\n",
    "            (\n",
    "                latent.shape[0]* 20,  # huge sample to sample all latent space and prevent racing between discriminator and generator\n",
    "                latent.shape[1],\n",
    "            )\n",
    "        ).to(device)\n",
    "        neg_pred = discriminator(negative) \n",
    "        pos_pred = discriminator(positive) \n",
    "\n",
    "        dis_labs = (\n",
    "            torch.cat([torch.zeros(negative.shape[0]), torch.ones(positive.shape[0])]) \n",
    "            .view(-1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        pred_lab = torch.cat([neg_pred, pos_pred]) \n",
    "        dis_loss = F.binary_cross_entropy(pred_lab, dis_labs) \n",
    "\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res = run_eval(encoder, decoder, test_loader, ae_pass_handler)\n",
    "        plot_manifold(res[\"latent\"], res[\"labels\"])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = run_eval(encoder, decoder, test_loader, ae_pass_handler)\n",
    "    plot_manifold(res[\"latent\"], res[\"labels\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что дискриминатор уделяет бОльшее внимание форме распределения, нежели его точным границам. \n",
    "Это отчасти обусловлено слоями BatchNorm в нашей сети. Если поучить нейросеть большее число эпох, то латентное представление \"засунется\" в требуемые границы.\n",
    "\n",
    "Построим картинки из этого латентного представления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin = res['latent'].min(axis=0)\n",
    "xmax, ymax = res['latent'].max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 50\n",
    "space1 = torch.linspace(xmin, xmax, steps)\n",
    "space2 = torch.linspace(ymin, ymax, steps)\n",
    "grid = torch.cartesian_prod(space1, space2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    imgs = decoder(grid.to(device))\n",
    "    imgs = imgs.cpu().numpy().squeeze()\n",
    "\n",
    "plot_digits(*[imgs[x : x + steps] for x in range(0, steps * steps, steps)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первый взгляд, получилась хорошая интерполяция. Как минимум — не хуже той, что нам давал обычный VAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь же покажем, что в принципе мы можем выбрать любую форму латентного представления, которая нам будет удобна. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# creating a distribution generator\n",
    "def generator_uniform(size, xmin, xmax, ymin, ymax):\n",
    "    x = torch.FloatTensor(size, 1).uniform_(xmin, xmax)\n",
    "    y = torch.FloatTensor(size, 1).uniform_(ymin, ymax)\n",
    "    return torch.cat([x, y], dim=1)\n",
    "\n",
    "# оcombining distributions into an object\n",
    "def generate_on_grid(size_for_bound, grid, generator):\n",
    "    samples = []\n",
    "    for boundaries in grid:\n",
    "        s = generator(size_for_bound, *boundaries)\n",
    "        samples.append(s)\n",
    "    sample = torch.cat(samples, dim=0)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничто не мешает нам требовать вот такого латентного представления (кроме того, что выбранное здесь оказывается не непрерывным, потому идеальное отображение мы получим вряд ли)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# for uniforms\n",
    "grids = [\n",
    "    (x1, x2, x3, x4)\n",
    "    for (x1, x2), (x3, x4) in product(\n",
    "        [[-1.5, -0.7], [-0.5, 0.3], [0.5, 1.3]], [[-1.5, -0.7], [-0.5, 0.3], [0.5, 1.3]]\n",
    "    )\n",
    "]\n",
    "grids.append((-0.5, 0.3, -2.3, -1.70))\n",
    "\n",
    "\n",
    "plot_manifold(generate_on_grid(516, grids, generator_uniform).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "discriminator = Discriminator(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "ae_optimizer = optim.Adam(\n",
    "    chain(\n",
    "        encoder.parameters(),\n",
    "        decoder.parameters(),\n",
    "    ),\n",
    "    weight_decay=weight_decay,\n",
    "    lr=learning_rate,\n",
    ")\n",
    "gen_optimizer = optim.Adam(\n",
    "    encoder.parameters(), weight_decay=weight_decay, lr=learning_rate\n",
    ")\n",
    "dis_optimizer = optim.Adam(\n",
    "    discriminator.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for ind, batch in tqdm(enumerate(train_loader)):\n",
    "\n",
    "        # train AE\n",
    "        ae_optimizer.zero_grad()\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        latent = encoder(imgs)\n",
    "        labels = torch.nn.functional.one_hot(labels, num_classes=10).to(device)\n",
    "        restored = decoder(latent)\n",
    "        ae_loss = F.binary_cross_entropy(restored, imgs)\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "         # train encoder(generator)\n",
    "        latent = encoder(imgs)\n",
    "        gen_optimizer.zero_grad()\n",
    "        neg_pred = discriminator(latent)\n",
    "        fake_labs = torch.ones(latent.shape[0]).view(-1, 1).to(device)\n",
    "        gen_loss = F.binary_cross_entropy(neg_pred, fake_labs) * 0.01\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # train dicriminator\n",
    "        dis_optimizer.zero_grad()\n",
    "        negative = latent.detach()\n",
    "        positive = generate_on_grid(128, grids, generator_uniform).to(device) \n",
    "        neg_pred = discriminator(negative)\n",
    "        pos_pred = discriminator(positive)\n",
    "\n",
    "        dis_labs = (\n",
    "            torch.cat([torch.zeros(negative.shape[0]), torch.ones(positive.shape[0])])\n",
    "            .view(-1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        pred_lab = torch.cat([neg_pred, pos_pred])\n",
    "        dis_loss = F.binary_cross_entropy(pred_lab, dis_labs)\n",
    "\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res = run_eval(encoder, decoder, test_loader, ae_pass_handler)\n",
    "        plot_manifold(res[\"latent\"], res[\"labels\"])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()\n",
    "with torch.no_grad():\n",
    "    res = run_eval(encoder, decoder, test_loader, ae_pass_handler)\n",
    "    plot_manifold(res[\"latent\"], res[\"labels\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе, получаем что-то похожее на то, что мы хотели. Правда, результат зависит от запуска, фазы луны и тд. GAN капризны.\n",
    "\n",
    "Сможете попробовать подобное в домашнем задании.\n",
    "\n",
    "Балансируя вес лосса генератора, можно это представление делать более/менее похожим. Единственное — параметры надо менять аккуратно. GAN капризны, и AAE не исключение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение (disentangling) стиля и метки "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В CVAE мы полагались на то, что если мы передаем нашим нейросетям метку объекта, то на латентном слое эта метка убирается.\n",
    "Что ж, это существенное допущение. Если этого не произойдет, то в нашей сгенерированной 7 будет немного 5 и так далее.\n",
    "\n",
    "А давайте просто сделаем так, чтобы метка отделялась в латентном слое в явном виде. И будем за неверное предсказание метки нейросеть штрафовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/disentangle_aae.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нам это дает? — теперь мы можем использовать стиль объекта отдельно и для простых ситуаций быть уверены, что в стиль объекта не замешалась метка. \n",
    "Для более сложных ситуаций нейросеть может нас и обмануть — мы никак не прописали явно, что в z не должно быть информации о метке. Ну что же, мы можем добавить это требование. А как? Добавим еще нейросеть, которая будет пытаться предсказать метку на основе только вектора z. \n",
    "\n",
    "Если эта нейросеть предсказывает хорошо, то энкодер не избавился от информации о метке и мы его за это штрафуем. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/disentagle_aae2.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semisupervised AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще мы можем кормить в нашу нейросетку неразмеченные объекты! Для них нейросетка сама будет пытаться определить label. Просто не будем штрафовать нейросеть за неверное предсказание метки (мы же ее и не знаем). А кроме них будем кормить и размеченные. Получаем semisupervised learning. \n",
    "\n",
    "\n",
    "Чтобы на неразмеченных объектах нейросеть не генерировала мусор, можем потребовать от нее, чтобы метка, которую она генерит, приходила из категориального распределения. Как это сделать? Да опять же, добавим дискриминатор, который будет отличать получившееся распределение меток от реального ([категориального](https://en.wikipedia.org/wiki/Categorical_distribution)), задающегося априорно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L14_Encoders/img_licence/semi_aae.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> Полезные материалы \n",
    "\n",
    "<font size=\"5\"> Про unsupervised learning при помощи нейросетей\n",
    "\n",
    "Главы из учебника Гудфеллоу по теме:\n",
    "1. [Representation learning](https://www.deeplearningbook.org/contents/representation.html)\n",
    "2. [Генеративные модели](https://www.deeplearningbook.org/contents/generative_models.html)\n",
    "\n",
    "Про все увеличивающуюся роль unsupervised learning: \n",
    "[Unsupervised Deep Learning - Google DeepMind & Facebook Artificial Intelligence NeurIPS 2018](https://www.youtube.com/watch?v=rjZCjosEFpI)\n",
    "\n",
    "[Лекция по генеративным моделям](https://www.youtube.com/watch?v=5WoItGTWV54)\n",
    "\n",
    "Про проклятье размерности: \n",
    "1. [В целом](https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2)\n",
    "2. [Для классификации](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)\n",
    "3. [Немного другой взгляд](https://towardsdatascience.com/the-curse-of-dimensionality-f07c66128fe1)\n",
    "\n",
    "<font size=\"5\"> Автоэнкодеры\n",
    "\n",
    "[Главы из учебника Гудфеллоу по теме](https://www.deeplearningbook.org/contents/autoencoders.html)\n",
    "\n",
    "\n",
    "\n",
    "[Более подробно про PCA и ссылка на его применение для MNIST](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n",
    "\n",
    "[Способы понижения размерности, PCA и разные типы автокодировщиков, лекция Техносферы](https://www.youtube.com/watch?v=W5JLSKcuaQo)\n",
    "\n",
    "[Eigenfaces](https://ieeexplore.ieee.org/document/139758)\n",
    "\n",
    "[Конспект от Andrew Ng по разреженным автокодировщикам](https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf)\n",
    "\n",
    "Получение разреженного автоэнкодера при помощи:\n",
    "\n",
    "1. [L1-loss а](https://debuggercafe.com/sparse-autoencoders-using-l1-regularization-with-pytorch/)\n",
    "\n",
    "2. [KL-divergence](https://debuggercafe.com/sparse-autoencoders-using-kl-divergence-with-pytorch/)\n",
    "\n",
    "Удаление шума из\n",
    "1. [изображений](https://debuggercafe.com/autoencoder-neural-network-application-to-image-denoising/)\n",
    "2. [текста](https://debuggercafe.com/denoising-text-image-documents-using-autoencoders/)\n",
    "\n",
    "[Введение в автоэнкодеры на kaggle](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)\n",
    "\n",
    "<font size=\"5\"> Вариационные автоэнкодеры \n",
    "\n",
    "[Введение в автоэнкодеры, вариационные автоэнкодеры, PCA](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "\n",
    "Введение в автоэнкодеры на Хабрахабре\n",
    "1. [Введение](https://habr.com/ru/post/331382/)\n",
    "2. [Manifold learning и скрытые (latent) переменные](https://habr.com/ru/post/331500/)\n",
    "3. [Вариационные автоэнкодеры (VAE)](https://habr.com/ru/post/331552/)\n",
    "4. [Conditional VAE](https://habr.com/ru/post/331664/)\n",
    "5. [GAN(Generative Adversarial Networks)](https://habr.com/ru/post/332000/)\n",
    "6. [GAN + VAE](https://habr.com/ru/post/332074/)\n",
    "\n",
    "\n",
    "[Оригинальная статья по VAE](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "[Ali Ghodsi, Лекция по VAE](https://www.youtube.com/watch?v=uaaqyVS9-rM)\n",
    "[Irhum Shafkat, Введение в автоэнкодеры, векторная арифметика](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)\n",
    "\n",
    "[Jeremy Jordan, введение в автоэнкодеры](https://www.jeremyjordan.me/autoencoders/)\n",
    "\n",
    "[Jeremy Jordan, вариационные автоэнкодеры](https://www.jeremyjordan.me/variational-autoencoders/)\n",
    "\n",
    "[Туториал по VAE с arxiv](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "\n",
    "[Еще одно введение в вариационные автоэнкодеры](https://livebook.manning.com/book/deep-learning-with-python/chapter-8/)\n",
    "\n",
    "[Туториал по VAE от Google по tensorflow](https://www.tensorflow.org/tutorials/generative/cvae)\n",
    "\n",
    "[Векторная арифметика в VAE при генерации изображений](https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)\n",
    "\n",
    "[Генерация анимированных персонажей](https://mlexplained.wordpress.com/category/generative-models/vae/)\n",
    "\n",
    "[Генерация лиц, можно менять пол, заставлять знаменитостей улыбаться](https://towardsdatascience.com/variational-autoencoders-vaes-for-dummies-step-by-step-tutorial-69e6d1c9d8e9)\n",
    "\n",
    "[VAE на pytorch с пояснениями](https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/)\n",
    "\n",
    "\n",
    "[Введение в условные вариационные автоэнкодеры](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)\n",
    "\n",
    "[Репозиторий с различными модификациями вариационных автоэнкодеров](https://github.com/AntixK/PyTorch-VAE)\n",
    "\n",
    "\n",
    "Взгляд на VAE как на игру с двумя участниками:\n",
    "1. [часть 1](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b)\n",
    "2. [часть 2](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46)\n",
    "3. [часть 3](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600)\n",
    "\n",
    "\n",
    "<font size=\"5\"> KL-дивергенция \n",
    "\n",
    "[Википедия по дивергенции Кульбака-Лейблера](https://ru.wikipedia.org/wiki/Расстояние_Кульбака_—_Лейблера)\n",
    "[Мотивация KL-дивергенции](https://math.stackexchange.com/questions/90537/what-is-the-motivation-of-the-kullback-leibler-divergence)\n",
    "\n",
    "Объяснение проблем и разницы между KL-дивергенцией, дивергенцией Йенсена-Шеннона и расстоянием Вассерштейна:\n",
    "1. [часть 1, проблемы KL-дивергенции. Дивергенция Йенсена-Шеннона](https://www.youtube.com/watch?v=_z9bdayg8ZI) и \n",
    "2. [часть 2, проблемы KL-дивергенции и дивергенции Йенсена-Шеннона. Расстояние Вассерштейна](https://www.youtube.com/watch?v=y8LGAhzCOxQ)\n",
    "\n",
    "\n",
    "<font size=\"5\"> AAE \n",
    "\n",
    "Цикл статей по AAE:\n",
    "1. [Автоэнкодеры](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4)\n",
    "2. [Добавление дискриминатора](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9)\n",
    "3. [Разделение стиля и содержания при помощи AAE](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-3-disentanglement-of-style-and-content-89262973a4d7)\n",
    "4. [Semisupervised learning при помощи AAE](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-4-classify-mnist-using-1000-labels-2ca08071f95)\n",
    "\n",
    "[Примеры AAE на mlxnet](https://github.com/nicklhy/AdversarialAutoEncoder)\n",
    "\n",
    "[Здесь в 6 и 8 лекции тоже можно найти примеры](https://github.com/che-shr-cat/deep-learning-for-biology-hse-2019-course)\n",
    "\n",
    "<font size=\"5\"> Модификации автоэнкодеров\n",
    "\n",
    "[Contractive Autoencoders](http://www.icml-2011.org/papers/455_icmlpaper.pdf) — автоэнкодеры, родственные шумоподавляющим автокодировщикам.\n",
    "\n",
    "[Variational losssy autoencoder](https://arxiv.org/pdf/1611.02731.pdf) — один из типов VAE, который пытается решить проблему того, что сильный декодер может игнорировать латентное представление. \n",
    "\n",
    "[$\\beta$- VAE](https://arxiv.org/pdf/1804.03599.pdf) — еще одно возможное улучшение VAE\n",
    "\n",
    "[Wassershtein autoencoders](https://arxiv.org/pdf/1711.01558.pdf)\n",
    "\n",
    "[Concrete autoencoders](https://arxiv.org/abs/1901.09346) — якобы позволяют выделять наиболее важные признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> Примеры практического применения \n",
    "\n",
    "\n",
    "1. [Age Progression/Regression — предсказание того, как будет выглядеть человек в другом возрасте](https:/arxiv/abs/1702.08423)\n",
    "\n",
    "2. [druGAN, генерация новых химических веществ](https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.7b00346)\n",
    "\n",
    "3. [Генерация лекарств, специфически меняющих активность генов человека](https://www.frontiersin.org/articles/10.3389/fphar.2020.00269/full)\n",
    "\n",
    "4. [Генерация ингибиторов определенного белка](https://www.nature.com/articles/s41587-019-0224-x)\n",
    "\n",
    "5. [Получение латентных представлений транскриптомов](https://academic.oup.com/nar/article/48/10/e56/5814052)\n",
    "\n",
    "6. [MethylNet](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3443-8) — Использование метилирования генома для обучения латентного представления, помогающего в предсказании возраста и т.д\n",
    "\n",
    "7. [scVAE](https://academic.oup.com/bioinformatics/article-abstract/36/16/4415/5838187?redirectedFrom=fulltext) — получение данных об экспрессии генов из single cell данных\n",
    "\n",
    "8. [U-Net](https://arxiv.org/abs/1505.04597) — сегментация медицинских изображений\n",
    "9. [W-Net](https://arxiv.org/abs/1711.08506) — unsupervised сегментация медицинских изображений  "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
