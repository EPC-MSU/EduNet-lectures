{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Линейные модели</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регрессия** — это одна из трех базовых задач машинного обучения (классификация, регрессия, кластеризация).\n",
    "\n",
    "В задаче **регрессии** мы используем входные **признаки**, чтобы предсказать **целевые значения**. Например, чтобы предсказать цену жилья по его характеристикам (площадь, этаж, год постройки дома, высота потолков, район, ...). **Линейная регрессия** сводится к тому, чтобы провести “**линию наилучшего соответствия**” через набор точек данных. Она является простым предшественником нелинейных методов, которые используются для обучения нейронных сетей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Модель и ее параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель линейной регрессии — **поиск линии, которая наилучшим образом соответствует заданным точкам**. Напомним, что общее уравнение для прямой есть\n",
    "\n",
    "$$f(x) = w⋅x + b,$$\n",
    "\n",
    "где $w$ — характеризует наклон линии (в будущем мы будем называть значения $w$ весом, weight) а $b$ — её сдвиг по $y$ (bias). Таким образом, решение линейной регрессии определяет значения для $w$ и $b$ так, что $f (x)$ приближается как можно ближе к $y$. $w$ и $b$ — **параметры модели**.\n",
    "\n",
    "\n",
    "Отобразим на графике случайные точки, расположенные в окрестности $y = 3⋅x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 + 3 * x + (np.random.rand(100, 1) - 0.5)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что нам неизвестны параметры наклона и сдвига $w$ и $b$. Для их определения мы бы могли рассмотреть всевозможные прямые вида $f(x) = w⋅x + b$ и выбрать среди семейства прямых такую, которая лучше всего приближает имеющиеся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in np.arange(-5.0, 7.0, 1):\n",
    "    for b in [-1, 0, 1, 2, 3]:\n",
    "        y_predicted = b + w * x\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель** $f(x) = w⋅x + b$ задаёт параметрическое семейство функций, а **выбор \"правильного\" представителя** из **параметрического семейства** и называется **обучением** модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in np.arange(-5.0, 7.0, 1):\n",
    "    for b in [-1, 0, 1, 2, 3]:\n",
    "        y_predicted = b + w * x\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.plot(x, 2 + 3 * x, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь\n",
    "\n",
    "Как выбрать параметры?\n",
    "\n",
    "**Функция потерь** — это мера количества ошибок. Для задачи **регрессии** такой мерой может быть **расстояние** между предсказанным значением $f(х)$ и его фактическим значением. Распространенной функцией потерь является **средняя квадратичная ошибка** (MSE). Чтобы вычислить MSE, мы просто берем все значения ошибок, считаем квадраты их длин и усредняем.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы определяем ошибку модели на одном объекте как квадрат расстояния между предсказанием и истинным значением, а общая функция потерь будет задана выражением:\n",
    "\n",
    "$$l_i =|y_i - f(x_i)| $$\n",
    "\n",
    "$$ Loss = \\sum l_i^2 = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для прямой с параметрами $w=4$, $b = 2$ и $w=3$, $b = 2$ (верные значения):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_line(ax, x, y, w, b, color=\"r\"):\n",
    "    y_predicted = w * x + b\n",
    "    # line\n",
    "    ax.plot(x, y_predicted, color=color, alpha=0.5, label=f\"f(x)={w}x+{b}\")\n",
    "    # delta\n",
    "    for x_i, y_i, f_x in zip (x, y, y_predicted):\n",
    "        ax.vlines(x=x_i, ymin=min(f_x, y_i), ymax = max(f_x, y_i),\n",
    "                  ls='--', alpha=0.3)\n",
    "    # MSE\n",
    "    loss = np.sum((y - (w * x + b)) ** 2) / (len(x))\n",
    "    ax.set_title(f\"MSE = {loss:.3f}\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,  figsize=(11, 4))\n",
    "\n",
    "# plot x_i y_i (dots)\n",
    "for ax in axs:\n",
    "    ax.scatter(x, y, s=10)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([2, 6])\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "plot_delta_line(axs[0], x, y, w=4, b=2, color=\"r\")\n",
    "plot_delta_line(axs[1], x, y, w=3, b=2, color=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача **поиска оптимальных параметров** модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск локального минимума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как будет выглядеть ландшафт функции потерь для нашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(-10, 30, 1)\n",
    "b = np.arange(-10, 10, 1)\n",
    "\n",
    "w, b = np.meshgrid(w, b)\n",
    "\n",
    "loss = np.zeros_like(w)\n",
    "for i in range(w.shape[0]):\n",
    "    for j in range(w.shape[1]):\n",
    "        loss[i, j] = np.sum((y - (w[i, j] * x + b[i, j])) ** 2) / (len(x))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "surf = ax.plot_surface(w, b, loss, cmap=plt.cm.RdYlGn_r, alpha=0.5)\n",
    "\n",
    "ax.contourf(w, b, loss, zdir=\"z\", offset=-1, cmap=\"RdYlGn_r\", alpha=0.5)\n",
    "ax.set_zlim(0, 20)\n",
    "\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_title(\"MSE\")\n",
    "\n",
    "fig.colorbar(surf, location = 'left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимым (но недостаточным) [условием локального минимума](http://school-collection.edu.ru/catalog/res/a8dc6578-4819-4297-ab14-95ffab6fe8b6/view/#:~:text=%D0%B5%D1%81%D0%BB%D0%B8%20%D0%BF%D1%80%D0%B8%20%D0%BF%D0%B5%D1%80%D0%B5%D1%85%D0%BE%D0%B4%D0%B5%20%D1%87%D0%B5%D1%80%D0%B5%D0%B7%20%D1%82%D0%BE%D1%87%D0%BA%D1%83%20%D1%850%20%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%BD%D0%B0%D1%8F%20%D0%BC%D0%B5%D0%BD%D1%8F%D0%B5%D1%82%20%D1%81%D0%B2%D0%BE%D0%B9,%D1%82%D0%BE%D1%87%D0%BA%D0%B5%20%D1%850%20%D0%BD%D0%B5%D1%82%20%D1%8D%D0%BA%D1%81%D1%82%D1%80%D0%B5%D0%BC%D1%83%D0%BC%D0%B0.) дифференцируемой функции является равенство нулю частных производных:\n",
    "\n",
    "$$\t\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\frac{\\partial Loss}{\\partial w}=0,\n",
    "   \\\\\n",
    "   \\frac{\\partial Loss}{\\partial b}=0.\n",
    " \\end{cases}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Т.к. MSE для линейной регрессии - полином второй степени относительно $w$ и $b$, а полином второй степени не может иметь больше одного экстремума, то локальный минимум будет глобальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Метод наименьших квадратов\n",
    "\n",
    "Реализуем простейшую модель линейной регрессии с использованием библиотеки NumPy на датасете, определённом выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод наименьших квадратов: [МНК простейшие частные случаи](https://ru.wikipedia.org/wiki/Метод_наименьших_квадратов#Простейшие_частные_случаи). По сути метод наименьших квадратов - это решение системы уравнений выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_coef(x, y):\n",
    "    n = len(x)\n",
    "    assert n == len(y)\n",
    "    w = (n * sum(np.multiply(x, y)) - sum(x) * sum(y)) / (\n",
    "         n * sum(np.multiply(x, x)) - sum(x) ** 2\n",
    "    )\n",
    "    b = (sum(y) - w * sum(x)) / n\n",
    "    return w, b\n",
    "\n",
    "w, b = estimate_coef(x, y)\n",
    "\n",
    "y_predicted = w * x + b\n",
    "\n",
    "print(f\"Estimated coefficients:\\nb = {b[0]:.3f} \\nw = {w[0]:.3f}\")\n",
    "print(f\"Final equation: \\ny = {w[0]:.3f}x +{b[0]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.plot(x, y_predicted, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С одной из метрик регрессии мы уже познакомились: это  — $MSE$, которую мы минимизировали в методе наименьших квадратов. Стоит отметить что $MSE$ имеет [размерность](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8) квадрата размерности предсказываемого значения.\n",
    "\n",
    "$$ MSE  = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$\n",
    "\n",
    "Чтобы получить оценку ошибки той же размерности можно взять корень (root) от $MSE$. Это метрика $RMSE$:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{N} \\sum (y_i - f(x_i))^2}$$\n",
    "\n",
    "Или посчитать среднюю абсолютную ошибку $MAE$:\n",
    "\n",
    "$$ MAE = \\frac{1}{N} \\sum |y_i - f(x_i)|$$\n",
    "\n",
    "Существуют и более специфичные метрики, например $R^2$, которая принимает значения от $(-\\inf, 1]$, где $1$  —  наилучший вариант. $R^2$  называется [коэффициентом детерминации](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8) и характеризуют долю дисперсии целевого значения, которую объясняет модель.\n",
    "\n",
    "$$R^2 = 1 - \\frac{MSE}{\\sigma^2}=1 - \\frac{\\sum {(y_i-f(x_i))^2}}{\\sum{(y_i-\\bar{y})^2}}$$\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{N}\\sum {y_i}$$\n",
    "\n",
    "где $\\sigma^2$ - дисперсия.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def print_metrics(y_true, y_predicted):\n",
    "    print(f\"Mean squared error: {mean_squared_error(y_true, y_predicted):.3f}\")\n",
    "    print(\"Root mean squared error: \",\n",
    "          f\"{mean_squared_error(y_true, y_predicted, squared=False):.3f}\")\n",
    "    print(f\"Mean absolute error: {mean_absolute_error(y_true, y_predicted):.3f}\")\n",
    "    print(f\"R2 score: {r2_score(y_true, y_predicted):.3f}\")\n",
    "\n",
    "print_metrics(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее про метрики можно почитать: [тут](https://academy.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель линейной регрессии из библиотеки scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свою линейную регрессию мы написали, теперь изучим как работать с моделью из sklearn.\n",
    "\n",
    "Рассмотрим следующую задачу. Пусть мы хотим построить модель предсказания успеваемости студента на основе информации о величине потраченного им на изучение материала количества времени в часах. Это пример простейшей задачи линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет. Мы должны увидеть, что он содержит два числовых признака — часы и результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/student_scores.csv\"\n",
    ")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости одного от другого, а также отобразим распределения каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.jointplot(data=dataset, x=\"Scores\", y=\"Hours\", height=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.iloc[:, :-1].values  # column Hours\n",
    "y = dataset.iloc[:, 1].values  # column Score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель для линейной регрессии. Чтобы не писать с нуля, воспользуемся готовой моделью из библиотеки `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.linspace(min(x_train), max(x_train), 100)  # 100 dots at min to max\n",
    "y_pred = regressor.predict(x_points)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_train, y_train, \"o\", label=\"Scores\")\n",
    "plt.plot(\n",
    "    x_points,\n",
    "    y_pred,\n",
    "    label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_),\n",
    ")\n",
    "plt.title(\"Hours vs Percentage\", size=12)\n",
    "plt.xlabel(\"Hours Studied\", size=12)\n",
    "plt.ylabel(\"Percentage Score\", size=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем предсказание для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "x_points = np.linspace(min(x_test), max(x_test), 100)\n",
    "y_pred = regressor.predict(x_points)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_test, y_test, \"o\", label=\"Scores\")\n",
    "plt.plot(\n",
    "    x_points,\n",
    "    y_pred,\n",
    "    label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_),\n",
    ")\n",
    "plt.title(\"Hours vs Percentage\", size=12)\n",
    "plt.xlabel(\"Hours Studied\", size=12)\n",
    "plt.ylabel(\"Percentage Score\", size=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неплохо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики для наших значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практические особенности работы с линейными моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30 признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer()  # load data\n",
    "\n",
    "x = cancer.data  # features\n",
    "y = cancer.target  # labels(classes)\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "print(f\"x[0]: \\n {x[0]}\")\n",
    "print(f\"y[0]: \\n {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстрее и удобнее можно посмотреть на данные используя pandas. К тому же Colab добавил возможность визуализации данных (для этого можно тыкнуть синий значок диаграммы ▆ █ ▄  под таблицей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab делает не полную визуализацию признаков, но и на данных изображениях можно найти полезную **информацию о выбросах** (из графика **Values**), **плотности распределений** (из графика **Distributions**) и о наличии **зависимости между переменными** (из графика **2-d distributions**). Например, мы можем увидеть, что значения признаков *mean area* и *mean perimeter* имеют зависимость близкую к линейной, что не очень хорошо (почему - обсудим позже).\n",
    "\n",
    "Кроме того можно тыкнуть на рисунок, посмотреть и скопировать код визуализации, чтобы применить к другим данным или изменить под свои нужды.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк, в каждой из которой по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные диапазоны  значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.boxenplot(data=cancer_df, orient=\"h\", palette=\"Set2\", linewidth=0.4,\n",
    "                   flier_kws={'marker': 'o', 's': 3}, line_kws={'linewidth':1})\n",
    "ax.set(xscale=\"log\", xlim=(1e-4, 1e4), xlabel=\"Values\", ylabel=\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы адекватно сравнить данные между собой, нам следует использовать нормализацию.\n",
    "TODO:  описать, что в линейной модели у нас ответ строится в качестве взвешенной суммы признаков и практично иметь слагаемые в одном и том же масштабе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Выбор Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализацией** называется процедура приведения входных данных к **общим значениям математических статистик**.\n",
    "\n",
    "Нормализация строит **взаимно однозначное соответствие** между некоторыми размерными величинами (которые измеряются в метрах, килограммах, годах и т. п.) и их безразмерными аналогами. Исходные значения **можно восстановить**, зная статистики оригинальных данных и правило по которому делалась нормализация.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто используют следующие варианты нормализации:  **`MinMaxScaler`**, **`StandardScaler`**, **`RobustScaler`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные из имеющегося диапазона значений в диапазон от 0 до 1. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (например, масштабирование RGB пикселей)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_i=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "$z_i$ — масштабированное значение,\n",
    "$X_i$ — текущее значение,\n",
    "$X_{min}$ и $X_{max}$ — минимальное и максимальное значения имеющихся данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение 0 и стандартное отклонение 1. Большинство значений будет в  диапазоне от -1 до 1. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_i=\\frac{X_i-u}{s}$$\n",
    "\n",
    "$u$ — среднее значение (или 0 при `with_mean=False`) и $s$ — стандартное отклонение (или 0 при `with_std=False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И `StandardScaler`, и `MinMaxScaler` чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. k-й процентиль — это величина, равная или не превосходящая k процентов чисел во всем имеющемся распределении. Например, 50-й процентиль (медиана) распределения таков, что 50% чисел из распределения не меньше данного числа. Соответственно, `RobustScaler` не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_i=\\frac{X_i-X_{median}}{IQR}$$\n",
    "\n",
    "$X_{median}$ — значение медианы, $IQR$ — межквартильный диапазон, равный разнице между 75-ым и 25-ым процентилями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для случайного набора признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "random_names = random.sample(list(cancer.feature_names), 8)\n",
    "cut_df = cancer_df[random_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "def plot_norm(df, ax, title):\n",
    "    sns.boxenplot(df, orient=\"h\", palette=\"Set2\",  ax=ax, linewidth=0.2,\n",
    "                  flier_kws={'marker': 'o', 's': 5}, line_kws={'linewidth':1})\n",
    "    ax.set(xlabel=\"Values\", title=title)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2,  figsize=(10, 7))\n",
    "\n",
    "plot_norm(cut_df, axs[0][0], \"Original\")\n",
    "axs[0][0].set(xscale=\"log\", xlim=(1e-4, 1e4))\n",
    "\n",
    "min_max_x = MinMaxScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(min_max_x, columns=random_names), axs[0][1], \"MinMax\")\n",
    "\n",
    "std_x = StandardScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(std_x, columns=random_names), axs[1][0], \"Standard\")\n",
    "\n",
    "rob_x = RobustScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(rob_x, columns=random_names), axs[1][1], \"Robust\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.55, hspace=0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед выбором нормализации важно разобраться с природой выбросов. Для этого на выбросы нужно посмотреть, с точки зрения эксперта и попробовать определить, являются ли выбросы ошибкой при сборе данных или редкими случаями, которые необходимо сохранить.\n",
    "\n",
    "Мы не являемся экспертами в медицине и мало знаем о данных, поэтому мы будем считать что наши признаки имеют распределение близкое к нормальному (мы можем сделать такое предположение из центральной предельной теоремы в теории вероятности). Поэтому мы будем использовать `StandardScaler`. `StandardScaler` часто используют как нормировку по-умолчанию.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = StandardScaler().fit_transform(cancer_df)  # scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что они стали намного более сравнимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_norm, columns=cancer.feature_names).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.boxenplot(data=pd.DataFrame(x_norm, columns=cancer.feature_names),\n",
    "                   orient=\"h\", palette=\"Set2\", linewidth=0.4,\n",
    "                   flier_kws={'marker': 'o', 's': 3}, line_kws={'linewidth':1})\n",
    "ax.set(xlabel=\"Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема корреляции признаков  в случае линейных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто может оказаться, что признаковое описание объекта избыточно и между различными признаками имеются связи. Для устойчивости работы линейных моделей важно, чтобы среди признаков не было скоррелированных пар.\n",
    "\n",
    "Например, если мы будем решать задачу регрессии на наборе признаков $x_1 \\dots x_n$ среди которых есть связь $x_2 = 5 x_1$ и возьмём линейную модель вида\n",
    "$$\\large y = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b,$$\n",
    "то с учётом данной связи мы можем записать:\n",
    "$$\\large y = w_1 x_1 + w_2 (5x_1) + \\dots + w_n x_n + b = (w_1 + 5 w_2) x_1 +  w_3 x_3 + \\dots + w_n x_n + b.$$\n",
    "\n",
    "Таким образом, наша модель теперь учитывает признак $x_1$ с одним \"общим\" весом $(w_1 + 5 w_2)$, не смотря на то что он закодирован двумя независимыми параметрами. Решение, то есть набор весовых коэффициентов $w_i$ перестало быть единственным, так как мы теперь можем делать произвольные преобразования с числами $w_1$ и $w_2$ до тех пор пока $(w_1 + 5 w_2)$ остаётся неизменным:\n",
    "\n",
    "$$\\large (w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5\\cdot10^9 ,\\, w_2 \\rightarrow  w_2 - 10^9 \\} = (w_1 + 5 w_2 + 5 \\cdot 10^9 - 5 \\cdot 10^9) = (w_1 + 5 w_2).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чем это плохо?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае корреляции признаков задача не имеет единственного решения и не существует обратной матрицы, обеспечивающей аналитическое решение. Мы можем использовать градиентные методы (поговорим позже), для поиска решения, но\n",
    "при этом веса модели могут неконтролируемо расти. При этом **суммарный вклад** признаков может быть **мал**.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/carrelation_problem1.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем оценивать **важность признаков в линейной модели**, используя **веса** перед ними (признаки должны быть нормализованы). Чем больше модуль веса, тем больше вклад. Для коррелированных признаков важность будет переоценена.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/carrelation_problem2.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, нужно помнить, что диапазоны числовых переменных ограничены. При неконтруемом росте весов, значение может выйти за диапазон и превратиться в ~~тыкву~~ `Nan`.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/carrelation_problem3.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Что делать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализировать подобные зависимости можно при помощи построения матрицы попарных корреляций признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = cancer_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    vmax=1,\n",
    "    vmin=-1,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наших данных сильно скоррелированы данные о размерах опухоли (мы могли видеть это выше при визуализации 2-d distributions).\n",
    "\n",
    "В случае корреляции можно:\n",
    "- **добавить регуляризацию** (о том, что такое регуляризация мы поговорим дальше),\n",
    "- **оставить один признак**,\n",
    "- если есть сомнения, что при удалении признаков часть информации будет потеряна, можно оставить **один признак** неизменным и вычесть его из остальных (оставить только **разницу**). Неинформативные шумовые признаки можно удалить (как это делать вы узнаете на 4-й лекции).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим одномерный пример. У нас есть данные по массе мышей. Часть из них определена как мыши с нормальной массой тела, а часть — как мыши с ожирением. Чтобы их отделить друг от друга, нам достаточно одного критерия. Мы можем посмотреть на график и визуально определить предельную массу, после которой мышки будут жирненькими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(total_len=40):\n",
    "    x = np.hstack(\n",
    "        [\n",
    "            np.random.uniform(14, 21, total_len // 2),\n",
    "            np.random.uniform(24, 33, total_len // 2),\n",
    "        ]\n",
    "    )\n",
    "    y = np.hstack([np.zeros(total_len // 2), np.ones(total_len // 2)])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50, threshold=None, margin=None):\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s)\n",
    "    if threshold:\n",
    "        ax.axvline(threshold, color=\"red\", ls=\"dashed\")\n",
    "    if margin:\n",
    "        for line in margin:\n",
    "            ax.axvline(line, color=\"pink\", ls=\"dashed\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Normal\", \"Obese\"])\n",
    "    ax.set(xlabel=\"Mass, g\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data(x, y, threshold=21.5, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, пользуясь нашим простым критерием, попробуем классифицировать каких-то новых мышей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.random.uniform(14, 30, 5)\n",
    "\n",
    "\n",
    "def classify(x, threshold=21.5):\n",
    "    y = np.zeros_like(x)\n",
    "    y[x > threshold] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "threshold = 21.5\n",
    "x, y = generate_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data(x, y, threshold=threshold, total_len=total_len)\n",
    "ax = plot_data(x_test, classify(x_test, threshold), total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если наши мыши находятся тут?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([21.45, 22.5])\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data(x, y, total_len=total_len)\n",
    "ax = plot_data(x_test, classify(x_test), threshold=21.5, total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения нашего классификатора, все просто: больше порогового значения — толстые мыши, меньше — нормальные. Но, с точки зрения здравого смысла, логичнее было бы классифицировать обоих мышей как нормальных, так как они значительно ближе к нормальным, чем к ожиревшим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вооружившись этим новым знанием, попробуем классифицировать наших отъевшихся мышек по-умному. Возьмем крайние точки в каждом кластере. И в качестве порогового значения будем использовать среднее между ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_data(total_len=total_len)\n",
    "normal_limit = x[y == 0].max()  # extreme point for 'normal'\n",
    "obese_limit = x[y == 1].min()  # extreme point for 'obese'\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit])  # separated with mean value\n",
    "\n",
    "x_test = np.array([21.5, 23])\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data(\n",
    "    x, y, total_len=total_len, threshold=threshold, margin=[normal_limit, obese_limit]\n",
    ")\n",
    "ax = plot_data(\n",
    "    x_test,\n",
    "    classify(x_test, threshold=threshold),\n",
    "    total_len=total_len,\n",
    "    s=300,\n",
    "    threshold=threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать, насколько наша мышь близка к тому, чтобы оказаться в другом классе. Такое расстояние называется **margin**. И оно считается как $\\mathrm{margin} = |\\mathrm{threshold} - \\mathrm{observation}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = np.abs(x_test - threshold)\n",
    "print(margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если мы посчитаем margins для наших крайних точек `normal_limit` и `obese_limit`, мы найдем самое большое возможное значение margin для нашего классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_0 = np.abs(normal_limit - threshold)\n",
    "margin_1 = np.abs(obese_limit - threshold)\n",
    "print(margin_0, margin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой классификатор мы называем **Maximum Margin Classifier**. Он хорошо работает в случае, когда все данные размечены аккуратно. Теперь рассмотрим более реалистичный пример, где что-то пошло не так."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_data(total_len=40):\n",
    "    x = np.hstack(\n",
    "        [\n",
    "            np.random.uniform(14, 21, total_len // 2),\n",
    "            np.random.uniform(24, 33, total_len // 2),\n",
    "        ]\n",
    "    )\n",
    "    y = np.hstack([np.zeros(total_len // 2), np.ones(total_len // 2)])\n",
    "    indx = np.where(x == x[y == 1].min())[0]\n",
    "    y[indx] = 0\n",
    "    s = np.ones_like(x) * 50\n",
    "    s[indx] = 300\n",
    "    return x, y, s\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y, s = generate_realistic_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data(x, y, total_len=total_len, s=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае наш **Maximum Margin Classifier** работать не будет. Исходя из этого, мы можем прийти к выводу, что наш классификатор очень чувствителен к выбросам. Решение этой проблемы будет рассмотрено в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример, где мы измерили не только вес мышей, но и их длину от хвоста до носа. Мы можем применить метод Support Vector Classifier, и теперь классы разделяет не одно пороговое значение (по сути, точка), а линия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "def generate_2d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42)\n",
    "    x[:, 0] += 10\n",
    "    x[:, 1] += 20\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50, threshold=21.5):\n",
    "    ax = sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Normal\", \"Obese\"])\n",
    "    ax.set(xlabel=\"Mass, g\", ylabel=\"Length, cm\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_2d_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data(x, y, total_len=total_len)\n",
    "\n",
    "# Code for illustration, later we will understand how it works\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel=\"linear\", C=1000)\n",
    "clf.fit(x, y)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(\n",
    "    XX, YY, Z, colors=\"k\", levels=[-1, 0, 1], alpha=0.5, linestyles=[\"--\", \"-\", \"--\"]\n",
    ")\n",
    "# plot support vectors\n",
    "ax.scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=100,\n",
    "    linewidth=1,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы добавим еще одно измерение — возраст, мы обнаружим, что наши данные стали трехмерными, а разделяет их теперь не линия, а плоскость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42, n_features=3)\n",
    "    x[:, 0] += 10\n",
    "    x[:, 1] += 20\n",
    "    x[:, 2] += 10\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50, threshold=21.5):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(xs=x[:, 0], ys=x[:, 1], zs=x[:, 2], c=y, s=s, cmap=\"Set1\")\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    ax.plot_surface(XX, YY, XX * YY * 0.2, alpha=0.2)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Normal\", \"Obese\"])\n",
    "    ax.set(xlabel=\"Mass, g\", ylabel=\"Length, cm\", zlabel=\"Age, days\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_3d_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если бы у нас было 4 измерения и больше (например: вес, длина, возраст, кровяное давление), то многомерная плоскость, которая бы разделяла наши классы, называлась бы **гиперплоскость** (рисовать мы ее, конечно же, не будем). Чисто технически, и точка, и линия — тоже гиперплоскости. Но все же гиперплоскостью принято называть то, что нельзя нарисовать на бумаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Геометрическая интерпретация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрались с тем, что такое регрессия, вернемся к задаче классификации изображений из датасета CIFAR-10. Как можно применить регрессию для классификации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у нас есть только 2 класса. Как можно использовать регрессию для того, чтобы определить относится ли изображение к классу 0 или к классу 1? В упрощенном варианте задача будет состоять в том, чтобы провести разделяющую плоскость (прямую) между 2-мя классами. Например, мы можем провести прямую через 0.\n",
    "\n",
    "Такая прямая будет задана направляющим вектором $\\vec W$, число компонентов которого будет равно размерности пространства признаков. Уравнение гиперплоскоскости в этом случае имеет вид\n",
    "\n",
    "$$\\large \\sum_i W_i \\cdot x_i = 0$$\n",
    "или, что эквивалентно:\n",
    "$$\\large ( \\vec W , \\vec x) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/regression_for_classification_imgs.png\" width=\"270\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другую ситуацию. В этом случае мы не можем просто провести прямую через 0, но можем отступить от 0 на какое-то расстояние и провести ее там. Вспомним, что уравнение прямой это $y=wx+b$, где $b$ — это смещение (*bias*). Соответственно, если b != 0, то прямая через 0 проходить не будет, а будет проходить через значение b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/regression_for_classification_add_bias.png\" width=\"270\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, для классификации объектов на два класса нам нужно подобрать значение вектора $W$ и величину смещения $B$. Вместе они зададут гиперплоскость вида:\n",
    "$$\\large ( \\vec W , \\vec x) - B = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Classification Loss Visualization\n",
    "](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)\n",
    "\n",
    "\n",
    "Если у нас есть несколько классов, мы можем для каждого из них посчитать уравнение $y_{i} = w_{i}x_{i}+b_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/regression_for_classification_add_bias_add_multiclasses.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На картинке нас интересуют 3 класса. Соответственно, мы можем записать систему линейных уравнений:\n",
    "\n",
    "\\begin{cases}\n",
    "y_{0} = w_{0}x_{0} + b_{0} \\\\\n",
    "y_{1} = w_{1}x_{1} + b_{1} \\\\\n",
    "y_{2} = w_{2}x_{2} + b_{2} \\\\\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Единообразный подход к учету смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы их можем собрать в матрицу, тогда получится следующее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/scalar_product_add_bias.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть матрица коэффициентов, которые мы каким-то образом подобрали, пока ещё непонятно как. Есть вектор $x$, соответствующий изображению.\n",
    "\n",
    "Мы умножаем вектор на матрицу, получаем нашу гиперплоскость для четырехмерного пространства в данном случае. Чтобы оно не лежало в 0, мы должны добавить смещение. И мы можем сделать это после, но можно взять и этот вектор смещения (вектор **b**) просто приписать к матрице **W**.\n",
    "\n",
    "Что будет выходом такой конструкции? Мы умножили матрицу весов на наш вектор, соответствующий изображению, получили некоторый отклик. По этому отклику мы так же, как и при реализации метода ближайшего соседа можем судить: если он больше остальных, то мы предполагаем, что это кошка.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([56, 231, 24, 2])\n",
    "w_cat = np.array([0.2, -0.5, 0.1, 2.0])\n",
    "print(\"Image \", img)\n",
    "print(\"Weights \", w_cat)\n",
    "print(\"img * w_cat \", img * w_cat)\n",
    "print(\"sum \", (img * w_cat).sum())\n",
    "print(\"Add bias \", (img * w_cat).sum() + 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/img_to_function_get_scores.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим входное изображение как $x$, а шаблон для первого из классов как $w_0$.\n",
    "\n",
    "Элементы пронумеруем подряд 1,2,3 … $n$. То есть развернем матрицу пикселей изображения в вектор.\n",
    "\n",
    "Тогда результат сравнения изображения с этим шаблоном будет вычисляться по формуле: $x[0]*w_0[0] + x[1]*w_0[1] + … x[n-1]*w_0[n-1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/img_to_vector_to_compute_scalar_product.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/scalar_product_ways_to_use.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта простая модель лежит в основе практически всех сложных моделей, которые мы будем рассматривать дальше. Внутри будем также пользоваться скалярным произведением.\n",
    "\n",
    "В дальнейшем мы будем проходить сверточные сети, они работают очень похоже:\n",
    "мы тоже накладываем шаблон на некоторую матрицу и перемножаем элементы, затем складываем. Единственное отличие — обычно ядро свертки меньше, чем размер самого изображения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собирая все вместе, получаем какое-то компактное представление, что у нас есть некоторая функция, на вход которой мы подаем изображение, и у нее есть параметры (веса). Пока происходит просто умножение вектора на матрицу, в дальнейшем это может быть что-то более сложное, функция будет представлять какую-то более сложную модель. А на выходе (для классификатора) мы получаем числа, которые интерпретируют уверенность модели в том, что изображение принадлежит к определенному классу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/input_img_scalar_product_add_bias_get_scores.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, эти коэффициенты, которые являются весами модели, надо каким-то образом подбирать. Но прежде чем подбирать коэффициенты, давайте определимся со следующим: как мы будем понимать, что модель работает хорошо или плохо? Вывод модели представляет собой просто некоторый набор чисел. Но как эти числа следует правильно интерпретировать? Рассмотрим этот вопрос в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный классификатор с пороговой функций принятия решения. Hard Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример задачи классификации на два класса. У нас имеется:\n",
    "1. Набор данных из $N$ наблюдений.\n",
    "2. Для каждого из наблюдений нам известно признаковое описание объекта в виде набора вещественных чисел. То есть объекту под номером $i$ соответствует вектор $\\vec x_i$.\n",
    "3. Также для каждого наблюдения нам известна истинная метка класса. Мы знаем что объекту с признаковым описанием $\\vec x_i$ соответствует метка класса $y_i$. Будем считать, что метки классов принимают значения $y_i \\in \\pm 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим подобрать такие $\\vec W$ и $B$, чтобы можно было провести такие гиперплоскости:\n",
    "\n",
    "$$\\large (\\vec W \\vec x) - B  = 1$$\n",
    "**\"Лежащие на этой плоскости и выше объекты относятся к классу $+1$\"**\n",
    "\n",
    "$$\\large (\\vec W \\vec x) - B  = -1$$\n",
    "**\"Лежащие на этой плоскости и ниже объекты относятся к классу $-1$\"**\n",
    "\n",
    "Расстояние между двумя этими **жесткими** границами задаётся просто длиной вектора $\\vec W$ и равно $\\frac{2}{||\\vec W||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Условие того, что объекты лежат $(\\vec x_i, y_i)$ лежат по правильную сторону от разделяющих поверхностей можно записать в совместно:\n",
    "\n",
    "$$\\large y_i ((\\vec W, \\vec x_i) - B )\\ge 1,$$ которое должно выполняться для всех объектов $1 \\le i \\le N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди всех решений $\\vec W$ и $B$, которые удовлетворяют условию выше, мы хотим подобрать такое, при котором пороговые разделяющие поверхности будут находится дальше всего. Так как расстояние между ними равно $\\frac{2}{||\\vec W||}$, мы приходим к следующей задаче на условный экстремум:\n",
    "\n",
    "$$\\large ||\\vec W|| \\rightarrow \\min, \\; y_i ((\\vec W, \\vec x_i) - B )\\ge 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобная задача на условный экстремум может быть решена при помощи [метода множителей Лагранжа](https://en.wikipedia.org/wiki/Lagrange_multiplier):\n",
    "\n",
    "Найти $\\alpha_i$, $\\vec W$ и $B$, которые реализуют минимум функции потерь:\n",
    "\n",
    "$$\\large L =  (\\vec W, \\vec W) + \\sum_i \\alpha_i (y_i ((\\vec W, \\vec x_i) - B )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный классификатор с Hinge loss. Soft Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем модифицировать функцию потерь на одном примере, разрешив классификатору ошибаться и упорядочивать некоторые объекты по \"неправильную\" сторону от разделяющих поверхностей.\n",
    "Данная модификация называется [Hinge_loss](https://en.wikipedia.org/wiki/Hinge_loss):\n",
    "$$\\large L_i = \\max(0, 1 - y_i ((\\vec W, \\vec x_i) - B ))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим, как линейная модель классификации с Hinge loss работает на практике. Мы можем применить линейный классификатор в том числе и к изображениям — достаточно просто вытянуть изображение из тензора формата $(\\text{C}, \\text{H}, \\text{W})$ в $(\\text{C} \\cdot \\text{H} \\cdot \\text{W})$-мерный вектор. Применим линейный классификатор к нескольким изображениям из датасета CIFAR-10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/model_predicted_scores_for_10_classes.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии с тем, что мы уже делали, мы можем сравнивать отклик на ключевой класс  (про который нам известно, что он на изображении, так как у нас есть метка этого класса) с остальными. Соответственно, мы подали изображение кошки и получили на выход вектор. Чем больше значение, тем больше вероятность того, что, по мнению модели, на изображении этот класс. Для кошки в данном случае это значение 2.9. Хорошо это или плохо? Нельзя сказать, пока мы не проанализировали остальную часть вектора. Если бы мы могли посмотреть на все значения в векторе, мы бы увидели, что есть значения больше, то есть в данном случае модель считает, что это собака, а не кошка, потому что для собаки значение максимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основании этого можно построить некоторую оценку. Давайте смотреть на разницу правильного класса с неправильными. Насколько уверенность в кошке будет больше остальных, настолько хорошо работает наша модель.\n",
    "\n",
    "Но поскольку нам важна не работа модели на конкретном изображении, а важно оценить ее работу в целом, то эту операцию нужно проделать либо для всего датасета, либо для некоторой выборки, которую мы подаем на вход и подсчитываем средний показатель. Этот показатель (насколько хорошо работает модель), называется функцией потерь, или **loss function**. Называется она так потому, что она показывает не то, насколько хорошо работает модель, а то, насколько плохо.\n",
    "\n",
    "Дальше будет понятно, почему так удобнее (разница только в знаке). Как это посчитать для всего датасета?\n",
    "\n",
    "Мы каким-то образом считаем loss для конкретного изображения, потом усредняем по всем изображениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано: 3 учебных примера, 3 класса. При некотором W баллы f (a, W) = Wx равны:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/model_predicted_scores_for_3_classes.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь показывает, насколько хорош наш текущий классификатор.\n",
    "\n",
    "Дан датасет примеров:\n",
    "\n",
    "$\\begin{Bmatrix} (x_i,y_i)  \\end{Bmatrix}_{i=1}^N \t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где **$x_i$** — изображение, **$y_i$** — метка (число).\n",
    "\n",
    "Потери по набору данных — это среднее значение потерь для примеров:\n",
    "\n",
    "$ L = {1 \\over N}\\sum_iL_i(f(x_i,W),y_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим функцию потерь для одного примера $L_i(f(x_i,W),y_i)$:\n",
    "\n",
    "1. Вычислим вектор значений прогнозов классификатора $s = f(x_i, W)$.\n",
    "1. Для всех примеров рассмотрим разницу между оценкой на истинной категории и всеми оценками классификатора для неправильных категорий: $s_{y_i} - s_j$ для $j \\neq y_i$.\n",
    "1. Если получившаяся разница положительная и превышает некоторое пороговое значение («зазор»), которое мы установим равным $1$, то будем считать, что категория $j$ не мешает модели верно классифицировать входной объект, припишем категории $j$ нулевой вклад в $L_i(f(x_i,W),y_i)$.\n",
    "1. Если получившаяся разница не превосходит установленного нами единичного «зазора», то мы будем считать что ответ классификатора $s_j$ в категории $j$ мешает верной классификации входного объекта. В этом случае припишем для категории $j$ аддитивный вклад в $L_i(f(x_i,W),y_i)$ равный $s_j-s_{y_i}+1$.\n",
    "\n",
    "Описанную процедуру гораздо проще записать в виде формулы:\n",
    "\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}$\n",
    "\n",
    "$=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логика такая: если у нас уверенность модели в правильном классе большая, то модель работает хорошо и loss для данного конкретного примера должен быть равен нулю. Если есть класс, в котором модель уверена больше, чем в правильном, то loss должен быть не равен нулю, а отображать какую-то разницу, поскольку модель сильно ошиблась. При этом есть ещё одно соображение: что будет, если на выходе у правильного и ошибочного класса будут примерно равные веса? То есть, например, у кошки было бы 3.2, а у машины не 5.2, а 3.1. В этом случае ошибки нет, но понятно, что при небольшом изменении в данных (просто шум) скорее всего она появится.\n",
    "\n",
    "То есть модель плохо отличает эти классы. Поэтому мы и вводили некоторый зазор, который должен быть между правильным и неправильным ответом.\n",
    "\n",
    "Посмотрим на изображение снизу. У нас есть два класса: фиолетовые треугольники и синие квадраты, разделенные зазором. Также можем увидеть желтые треугольники и квадраты — это ошибочно распознанные классы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/svm_decision_boundary.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И тоже учитывать его в loss function: сравнивать его результат для правильного класса не с чистым выходом для другого, а добавить к нему некоторую дельту (в данном случае — 1(единица)). Смотрим: если разница больше 0, то модель работает хорошо и $L_i = 0$. Если нет, то мы возвращаем эту разницу, и loss будет складываться из этих индивидуальных разниц.\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}$\n",
    "\n",
    "$=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "Ниже пример того, как считается loss.\n",
    "\n",
    "Считаем функцию потерь для 1-ого изображения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/compute_loss_use_model_scores_for_1_example.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также считаем потери для 2-ого и 3-его изображения:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/compute_loss_use_model_scores_for_2_and_3_examples.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения потерь получились следующие:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/summary_losses_for_3_examples.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем среднее значение loss для всего датасета:\n",
    "\n",
    "$ L = {1 \\over N}\\sum_{i=1}^N L_i$\n",
    "\n",
    "$L={2.9 + 0 + 12.9 \\over 3} = 5.27$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM loss\n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как известно из курса математического анализа, производная вещественной функции одной переменной характеризует мгновенную скорость изменения значений данной функции. Если в какой-то точке производная положительна, то в этой окрестности функция может возрастать. В противном случае, если производная отрицательна, функция может убывать. Аналогичный смысл несёт градиент для вещественной скалярной функции многих переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём гладкую скалярную функцию многих вещественных переменных (таковой, например, является функция потерь) и вычислим производные по каждой из её переменных, считая остальные переменные зафиксированными (так называемые частные производные). Запишем получившиеся частные производные в виде вектора-столбца.\n",
    "Каждый из компонентов такого вектора будет характеризовать мгновенную скорость изменения функции вдоль направления одной из переменных. Если же посмотреть на такой вектор в целом, то он будет направлен в сторону наибольшего роста нашей скалярной функции, а его длина будет характеризовать скорость такого роста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, если в качестве скалярной функции $\\varphi$ взять высоту поверхности земли над уровнем моря,то $\\text{grad} \\varphi$ в каждой точке земной поверхности будет направлен в сторону самого крутого подъема и будет своей длиной указывать крутизну склона.\n",
    "\n",
    "Так, если $\\varphi = \\varphi(x_1 \\dots x_n)$  — функция $n$ переменных, то её градиентом называется $n$-мерный вектор:\n",
    "$$\\left(\\frac{\\partial\\varphi}{\\partial x_1},\\dots,\\frac{\\partial\\varphi}{\\partial x_n}\\right)^T$$\n",
    "\n",
    "Нас будет интересовать градиент функции потерь $L$, взятый по весам модели $W$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ — матрица(вектор) весов\n",
    "\n",
    "$L$ — функция потерь (скаляр)\n",
    "\n",
    "$\\partial W = W_2 - W_1$\n",
    "\n",
    "$\\partial L = L_2 - L_1$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W}=\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial W_1} \\\\\n",
    "\\frac{\\partial L}{\\partial W_2} \\\\\n",
    "... \\\\\n",
    "\\frac{\\partial L}{\\partial W_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Наша задача будет сводиться к тому, что мы будем искать градиент loss функции по весам, которые будут состоять из производной по каждому направлению. Поскольку у нас здесь числа, можно считать этот градиент численно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Численный расчет производной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/gradient_descent_analytical_calculation.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем градиент приближенно, воспользовавшись определением (в формуле аргумент обозначен как $x$, у нас же аргументом будет $W$):\n",
    "На нулевом шаге у нас есть $W_0$ найдем $L_0 = Loss(f(W_0,x))$\n",
    "Прибавим к первому элементу $W_0$ небольшую величину  $h$ = 0.0001 и получим новую матрицу весов $W_1$ отличающуюся от $W_0$ на один единственный элемент.\n",
    "\n",
    "Найдем Loss от $\\frac {W_1}  {L_1} = Loss(f(W_1,x))$\n",
    "По определению производной $\\frac {dL}{W_0} = \\frac{( L_1 - L_0 )}   {h}$\n",
    "\n",
    "Повторяя этот процесс для каждого элемента из $W$, найдем вектор частных производных, то есть градиент $\\frac{dL}{dW}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Плюсы:\n",
    "Это просто.\n",
    "\n",
    "Проблемы:\n",
    "\n",
    "1. Это очень долго, нам придется заново искать значение loss функции для каждого $W_i$.\n",
    "\n",
    "2. Это неточно, так как по определению приращение $h$ бесконечно мало, а мы используем конкретное, пусть и небольшое число. И если мы сделаем его слишком маленьким, то столкнемся с ошибками, связанными с округлением в памяти компьютера.\n",
    "\n",
    "Поэтому данный метод может использоваться как проверочный.\n",
    "А на практике вместо него используется **аналитический расчет градиента**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналитический расчет производной от функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые производные\n",
    "\n",
    "$$x' = \\frac {\\delta x} {\\delta x} = 1$$\n",
    "\n",
    "$$(x^2)' = \\frac {\\delta x^2} {\\delta x} = 2x$$\n",
    "\n",
    "$$(\\log x)'  = \\frac {\\delta \\log x} {\\delta x} = \\frac 1 x $$\n",
    "\n",
    "$$(e^x)'  = \\frac {\\delta e^x} {\\delta x} = e^x $$\n",
    "\n",
    "$$\\frac {\\delta cf(x)} {\\delta x}= c \\cdot \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$$\\frac {\\delta f(x) + c} {\\delta x}= \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$c$ — константа, не зависящая от $x$\n",
    "\n",
    "$$ \\frac {\\delta [f(x) + g(x)]} {\\delta x} = \\frac {\\delta f(x)} {\\delta x}  + \\frac {\\delta g(x)} {\\delta x} $$\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta x} = 2x  $$\n",
    "так как $y$ по отношению к $x$ — константа, и мы меняем только $x$:\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta y} = 3y^2  $$\n",
    "так как $x$ по отношению к $y$ — константа, и мы меняем только $y$:\n",
    "\n",
    "$$(e^y)'  = \\frac {\\delta e^y} {\\delta x} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-rule\n",
    "\n",
    "Производная функции $f(g)$:\n",
    "\n",
    "$$\\frac {\\delta f} {\\delta g}$$\n",
    "\n",
    "Пусть $g$  на самом деле не просто переменная, а зависит от $h$. Тогда производная от $f$ по $g$ **не меняется**, а производная $f$ по $h$ запишется следующим образом:\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h))} {\\delta h} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h}$$\n",
    "\n",
    "Пусть теперь $h$ зависит от $x$. Все аналогично\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h(x)))} {\\delta x} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h} \\frac {\\delta h} {\\delta x}$$\n",
    "\n",
    "Так можно делать до бесконечности, находя производную сколь угодно сложной функции. Важно, что мы можем считать градиенты частями: посчитать сначала $f$ по $g$, потом $g$ по $h$....\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x}$$\n",
    "\n",
    "$$h = x^2 + 5$$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac {\\delta log(h)} {\\delta h} \\frac {\\delta h} {\\delta x}$$\n",
    "\n",
    "$$\\frac {\\delta log(h)} {\\delta h} = \\frac 1 h$$\n",
    "\n",
    "$$\\frac  {\\delta h} {\\delta x} = 2x $$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac 1 {x^2 + 5} \\cdot 2x = \\frac {2x} {x^2 + 5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть MSE-loss\n",
    "\n",
    "$$loss = (y - \\hat{y})^2 $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta (y - \\hat{y})^2 } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = 2(y-\\hat{y}) \\cdot -1 = 2 (\\hat{y} - y)$$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = 2 x \\cdot (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE-loss\n",
    "$$MSE = \\frac 1 N \\sum_i(y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "$y_i$, $\\hat{y_i}$ — константы, не являются функциями друг от друга\n",
    "$$\\hat{y} = wx_i + b $$\n",
    "\n",
    "$$\\frac {\\delta MSE} {\\delta w} = \\frac 1 N \\sum \\frac {\\delta (y_i - \\hat{y_i}) ^2} {\\delta \\hat{y_i}} \\frac {\\delta \\hat{y_i}} {\\delta w}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть MAE-Loss\n",
    "\n",
    "$$loss = |y - \\hat{y}| $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}}$$\n",
    "\n",
    "Строго говоря, у модуля не существует производной в 0.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i for i in range(-5, 6)]\n",
    "y = [abs(i) for i in range(-5, 6)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, y, label=\"y = |x|\")\n",
    "plt.title(\"y = |x|\", size=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы можем сказать, что в этой точке производная равна 0.\n",
    "Если аргумент модуля меньше 0, то производная будет -1.\n",
    "\n",
    "Если больше +1\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} = - sign(y - \\hat{y}) =  sign(\\hat{y} - y)$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(-5, 1, 1)]\n",
    "y = [i * 0 - 1 for i in range(6)]\n",
    "x_1 = [i for i in range(0, 6)]\n",
    "y_1 = [i * 0 + 1 for i in range(0, 6)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, y, \"b\")\n",
    "plt.plot(x_1, y_1, \"b\")\n",
    "plt.plot(0, 0, \"ro\")\n",
    "plt.plot(0, 1, \"bo\")\n",
    "plt.plot(0, -1, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Loss\n",
    "\n",
    "$$b = max(x, y)$$\n",
    "\n",
    "$$b = x~~if~~x > y~~else~~y$$\n",
    "\n",
    "\n",
    "$$\\frac {\\delta b} {\\delta x} =  \\frac {\\delta x} {\\delta x}~~if~~x > y~~else~~\\frac {\\delta y} {\\delta x} = 1~~if~~x > y~~else~~0$$\n",
    "\n",
    "Если $x > y$, то он оказал влияние на $b$. Иначе его вклада в $b$ НЕ БЫЛО: градиент равен 0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из:\n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем:\n",
    "\n",
    "$ \\nabla_W\tL(W) = {1 \\over N}\\sum_{i=1}^N \\nabla_W L_i(x_i, y_i, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор шага обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/update_weghts_values.png\" width=\"450\">\n",
    "\n",
    "<!-- [Визуализация](https://docs.google.com/file/d/0Byvt-AfX75o1ZWxMRkxrUFJ2ZUE/preview)\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг обучения — некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время, чтобы тот же процесс не шел слишком медленно (как на графике слева)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/learning_rate_optimal_value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор слишком большого шага обучения может стать причиной нежелательного характера изменения значения функции потерь в процессе обучения. Вместо ожидаемого движения в сторону минимума, функция ошибок может начать периодически изменяться в течение цикла обучения или же вовсе продемонстрировать неограниченный рост."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве иллюстрации этого эффекта рассмотрим модельный пример. Предположим, мы решаем задачу регрессии и хотим восстановить значение неизвестной скалярной функции.\n",
    "\n",
    "Создадим синтетический датасет для функции\n",
    "$y(x_1, x_2, ... , x_n) = \\sum_i x_i^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(101)\n",
    "x_train = np.random.rand(1000, 5)\n",
    "noise = np.random.rand(1000, 1) / 1000\n",
    "y_train = np.expand_dims(np.sum(x_train**3, axis=-1), axis=1) + noise\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простейшую модель линейной регрессии. Используем MSE в качестве функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class LinearRegressor:\n",
    "    def __init__(self, in_features, out_features, batch_size, random_state=42):\n",
    "        self.in_features = in_features  # num of inputs\n",
    "        self.out_features = out_features  # num of outputs\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.W = (\n",
    "            np.random.randn(self.in_features + 1, self.out_features) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias\n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # reset loss\n",
    "        train_len = x_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)\n",
    "\n",
    "        for i in range(0, train_len, self.batch_size):\n",
    "            idx = indexes[i : i + self.batch_size]\n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "\n",
    "            x_batch = np.hstack([x_batch, np.ones((x_batch.shape[0], 1))])  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(x_batch, y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        MSE loss\n",
    "        \"\"\"\n",
    "        current_batch_size = x.shape[0]  # batch_size\n",
    "        loss = 0.0\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        for i in range(current_batch_size):\n",
    "            y_preds_on_batch = x[i].dot(self.W)  # vector of shape out_features\n",
    "            y_true_on_batch = y[i]\n",
    "            loss += np.sum((y_preds_on_batch - y_true_on_batch) ** 2)\n",
    "            # dW_{m, n} = 2 * x_m ((Wx)_n - y_n)\n",
    "            dW += np.meshgrid(y_preds_on_batch - y_true_on_batch, x[i])[0]\n",
    "\n",
    "        loss /= current_batch_size\n",
    "        dW /= current_batch_size\n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.append(x, 1)  # add 1 (bias)\n",
    "        scores = x.dot(self.W)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашу модель на синтетических данных и посмотрим, как будет изменяться значение функции потерь в ходе процесса обучения на 20 эпохах при использовании для обучения различных значений скорости обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def train_and_plot_history(learning_rates_list, batch_size=64, num_epochs=20):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    for lr_id, lr in tqdm(enumerate(learning_rates_list)):\n",
    "        regressor = LinearRegressor(\n",
    "            in_features=x_train.shape[1], out_features=y_train.shape[1], batch_size=42\n",
    "        )\n",
    "        loss_hist = np.zeros(num_epochs)\n",
    "        for epoch in range(num_epochs):\n",
    "            loss = regressor.fit(x_train, y_train, learning_rate=lr)\n",
    "            loss_hist[epoch] = loss\n",
    "        ax.plot(loss_hist, label=\"lr={:0.1e}\".format(lr))\n",
    "\n",
    "    ax.set_xticks(range(num_epochs))\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "    ax.set_ylabel(\"MSE loss\")\n",
    "    # ax.set_xlim([3, 8])\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При значениях скорости обучения в диапазоне $5 \\times 10^{-4} - 7 \\times 10^{-4}$ мы наблюдаем ожидаемый процесс обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot_history(learning_rates_list=np.linspace(start=5e-4, stop=7e-4, num=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понизив на несколько порядков скорость обучения, мы увидим, что за первые 20 эпох обучения модель существенно не смогла улучшить величину MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot_history(learning_rates_list=np.linspace(start=5e-9, stop=7e-9, num=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если же мы наоборот увеличим на несколько порядков скорость обучения, то заметим, что функция ошибок стала изменяться хаотично, и с дальнейшим ростом скорости обучения будет стремиться неограниченно ухудшаться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot_history(learning_rates_list=np.linspace(start=0.5, stop=0.55, num=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока изменения loss функции достаточно большие, сам по себе градиент тоже большой. За счет этого можно двигаться быстро. Когда он будет уменьшаться, мы должны оказаться рядом с этим минимумом, и шаг, который мы выбрали, не должен мешать этому процессу.\n",
    "\n",
    "Бывают ситуации, когда шаг можно менять в самом процессе обучения: в начале обучения модели шаг большой, потом, по мере того, как она сходится, чтобы более четко найти минимум, шаг можно изменить вручную. Но изначально его нужно каким-то образом подобрать, и это зависит от данных и от самой модели. Это тоже гиперпараметр, связанный с обучением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм стохастического градиентного спуска. Выбор размера батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост о стохастическом градиентом спуске](https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter2_stochastic-gradient-descent.html#motivation-for-stochastic-gradient-descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/define_size_of_batch.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы с самого начала говорили о выборках некоторого количества примеров. В дальнейшем мы будем называть их батчами. Батч (англ. *batch*) — это некоторое подмножество обучающей выборки фиксированного размера.\n",
    "\n",
    "При этом было не очень понятно, чем они мотивированы. Точнее, мы мотивировали это тем, что у нас много данных, и мы не сможем их обработать все, и это правда. Даже если мы сможем загрузить все данные в память, нам нужно будет загрузить их и использовать при расчете, в том числе градиента. Это ещё более затратно.\n",
    "\n",
    "Зачем батчи нужны при рассчете loss? Если мы посчитаем loss по одному изображению, скорее всего он будет очень специфичен, и это движение, которое произойдет, будет направлено в сторону минимума, потому что по этому конкретному изображению мы улучшим показатели, что не отражает обобщения всех данных, поэтому мы используем батчи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас в первую очередь интересует более общее направление, которое будет работать на большинстве наших данных. Поэтому, если мы будем оптимизировать модели, исходя из одного элемента данных, путь будет витиеватый и процесс будет происходить достаточно долго. Если же данные не помещаются в память, то **можно использовать батч того размера, который у нас есть**. Можно загрузить в память и считать по батчам градиент. В этом случае спуск будет более плавным, чем по одному изображению.\n",
    "\n",
    "Также при использовании всего датасета тоже есть свои минусы. **Не всегда загрузка всего датасета приводит к увеличению точности**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее:\n",
    "* [Методы оптимизации нейронных сетей\n",
    "](https://habr.com/ru/post/318970/)\n",
    "* [Обучение нейронной сети](https://alxmamaev.medium.com/deep-learning-на-пальцах-часть-1-341cfe021ef9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Сложность модели__ (*model complexity*) — важный гиперпараметр. В частности, для линейной модели, сложность может быть представлена количеством параметров, для полиномиальных моделей — степенью полинома, для деревьев решений — глубиной дерева и т.д.\n",
    "\n",
    "Сложность модели тесно связана с __ошибкой обобщения__ (_generalization error_). Ошибка обобщения отличается от ошибки обучения, измеряемой на тренировочных данных, тем, что позволяет оценить обобщающую способность модели, приобретенную в процессе обучения, давать точные ответы на неизвестных ей объектах. Cлишком простой модели не будет хватать мощности для обобщения сложной закономерности в данных, что приводит к большой ошибке обобщения, с другой стороны слишком сложная модель также приводит к большой ошибке обобщения за счет того, что в силу своей сложности модель начинает пытаться искать закономерности в шуме, добиваясь большей точности на тренировочных данных, теряя при этом часть обобщающей способности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/model_complexity.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проиллюстрируем описанное явление на примере полиномиальной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2 * np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "x_true = np.linspace(0, 2 * np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\", label=\"noisy data\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем аппроксимировать имеющуюся зависимость с помощью полиномиальной модели, используя шумные данные в качестве тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train = x.reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i, degree in enumerate([0, 1, 3, 9]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "    fig.add_subplot(2, 2, i + 1)\n",
    "    plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "    plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "    plt.plot(x_true, y_true, c=\"lime\")\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель может переобучаться, подстраиваясь под тренировочную выборку. В полиноме степень, и, как следствие, количество весов — это гиперпараметр, который можно подбирать на кросс-валидации, однако когда мы таким образом подбираем сложность модели, мы накладываем довольно грубое ограничение на обобщающую способность модели в целом. Вместо этого более разумным было бы оставить модель сложной, но использовать некий ограничитель (__регуляризатор__), который будет заставлять модель отдавать предпочтение выбору более простого обобщения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(9), LinearRegression())\n",
    "model_ridge = make_pipeline(PolynomialFeatures(9), Ridge(alpha=0.1))\n",
    "\n",
    "model.fit(x_train, y)\n",
    "y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "model_ridge.fit(x_train, y)\n",
    "y_plot_ridge = model_ridge.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "plt.plot(x_true, y_plot_ridge, c=\"black\", label=f\"M={degree}, alpha=0.1\")\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "poly_coef = model[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"Without regularization: \", eq)\n",
    "\n",
    "poly_coef = model_ridge[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"With regularization: \", eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что одним из \"симптомов\" переобучения являются аномально большие веса. Модель Ridge Regression, показанная в примере выше, использует L2-регуляризацию для борьбы с этим явлением.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/l2_regularization.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Regularization = weights decay\n",
    "\n",
    "Идея состоит в том, что мы можем наложить некоторое требование на сами веса. Дело в том, что можно получить один и тот же выход модели при разных весах (выход модели соответствует умножению весов на $x$), при разных $w$ выход может быть идентичен.\n",
    "\n",
    "Эти параметры задают некоторую аппроксимацию нашей целевой функции. Аппроксимировать функцию можно двумя способами:\n",
    "1. Использовать все имеющиеся данные и провести ее строго через все точки, которые нам известны;\n",
    "2. Использовать более простую функцию (в данном случае, линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым общим закономерностям, которые у них есть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Характерной чертой переобучения является второй сценарий, и сопровождается он, как правило, большими весами. Введение L2-регуляризации приводит к тому, что большие веса штрафуются и предпочтение отдается решениям, использующим малые значения весов.\n",
    "\n",
    "Модель может попробовать схитрить и по-другому: использовать все веса, все признаки, даже незначимые, но с маленькими коэффициентами. С этим L2-loss поможет хуже, так как он не сильнее штрафует мелкие веса. Результат его применения — малые значения весов, которые использует модель\n",
    "\n",
    "В этом случае на помощь приходит L1-loss, который штрафует вес за сам факт отличия его от нуля. Но и штрафует он все веса одинаково. Результат его применения — малое число весов, которые использует модель в принципе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/l1_and_l2_regularization.gif\" alt=\"alttext\" width=\"550\"/>\n",
    "\n",
    "\n",
    "Это лоссы можно комбинировать — получится Elastic Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda=$ regularization strength (hyperparameter)\n",
    "\n",
    "$L(W)=\\underbrace{\\frac1N\\sum_{i=1}^NL_i(f(x_i,W),y_i)}_{\\textbf{Data loss} }+\\underbrace {\\lambda R(W)}_{\\textbf{Regularization}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берем сумму всех весов по всей матрице $w$ и добавляем ее к loss. Соответственно, чем больше будет эта сумма, тем больше будет суммарный loss.\n",
    "\n",
    "В дальнейшем проблема с переобучением будет вставать довольно часто. Методов регуляризации модели существует достаточно много. Этот — один из базовых, который будет использоваться практически во всех оптимизаторах, с которыми познакомимся позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обобщенные линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные не всегда могут быть хорошо разделены (гипер)плоскостью. Например, рассмотрим следующее: у нас есть данные по дозировке лекарства и 2 класса — пациенты, которые поправились, и те, которым лучше не стало."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_patients_data(total_len=40):\n",
    "    x = np.random.uniform(0, 50, total_len)\n",
    "    y = np.zeros_like(x)\n",
    "    y[(x > 15) & (x < 35)] = 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Recover\", \"Sick\"])\n",
    "    ax.set(xlabel=\"dose, mg\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, мы не можем найти такое пороговое значение, которое будет разделять наши классы на больных и здоровых, а, следовательно, и Support Vector Classifier работать тоже не будет.  Для начала давайте преобразуем наши данные таким образом, чтобы они стали 2-хмерными. В качестве значений по оси Y будем использовать дозу, возведенную в квадрат (**доза**$^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    ax = sns.scatterplot(x=x[0, :], y=x[1, :], hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Recover\", \"Sick\"])\n",
    "    ax.set(xlabel=\"Dose, mg\")\n",
    "    ax.set(ylabel=\"Dose$^2$\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x_1, y = generate_patients_data(total_len=total_len)\n",
    "x_2 = x_1**2\n",
    "x = np.vstack([x_1, x_2])\n",
    "\n",
    "plot_data(x, y, total_len=40, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем вновь использовать Support Vector Classifier для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x, y, total_len=40, s=50)\n",
    "\n",
    "x_arr = np.linspace(0, 50, 50)\n",
    "xs = [x[0, :][y == 1].min(), x[0, :][y == 1].max()]\n",
    "ys = [x[1, :][y == 1].min(), x[1, :][y == 1].max()]\n",
    "\n",
    "# Calculate the coefficients.\n",
    "coefficients = np.polyfit(xs, ys, 1)\n",
    "\n",
    "# Let's compute the values of the line...\n",
    "polynomial = np.poly1d(coefficients)\n",
    "y_axis = polynomial(x_arr)\n",
    "\n",
    "# ...and plot the points and the line\n",
    "plt.plot(x_arr, y_axis, \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут возникает резонный вопрос: почему мы решили возвести в квадрат? Почему не в куб? Или, наоборот, не извлечь корень? Как нам решить, какое преобразование использовать?\n",
    "\n",
    "И у нас есть **вторая проблема** — а если перейти надо в пространство очень большой размерности? В этом случае наши данные очень сильно увеличатся в размере.\n",
    "\n",
    "Комбинация двух проблем дает нам **много сложности**: надо перебирать большое число возможных пространств большей размерности\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако основная фишка Support Vector Machine состоит в том, что внутри он работает на скалярных произведениях. И можно эти скалярные произведения считать, **не переходя в пространство большей размерности**\n",
    "\n",
    "Для этого SVM использует **Kernel Function**.\n",
    "\n",
    "Kernel Function может, например, быть полиномом (**Polynomial Kernel Function**), который имеет параметр $d$ — сколько размерностей выбрать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/svm_kernel_function.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры ядер :\n",
    "\n",
    "* $k(x_i, x_j) = (<x_i, x_j> + c)^d, с, d \\in \\mathbb{R}$ — полиномиальное ядро, считает расстояние между объектами в пространстве размерности d\n",
    "\n",
    "* $k(x_i, x_j) = \\frac{1}{z} e^{-\\frac{h(x_i, x_j)^2}{h}}$ — радиальная базисная функция RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, в случае SVM можно легко перебрать много таких пространств на кросс-валидации и выбрать более удобное.\n",
    "\n",
    "Более того, SVM может проверять пространства признаков бесконечного размера, если для такого пространства существует kernel function. Для решения практических задач иногда такие бесконечномерные пространства признаков могут оказаться удобными. Широко применяемое на практике RFB ядро как раз соответствует такому случаю бесконечномерного пространства признаков.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вероятностный подход в задаче классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный Байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно построить модель классификации, которая будет напрямую оценивать вероятность принадлежности объекта к интересующему нас классу просто на основе информации о распределении объектов по классам в обучающей выборке. Базовую идею такого примера легко продемонстрировать на следующем примере данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим `DataFrame` датасета [Wine](https://archive.ics.uci.edu/ml/datasets/wine), который являлся примером табличных данных в нашей первой лекции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine\n",
    "\n",
    "# Download dataset\n",
    "features, class_labels = load_wine(\n",
    "    return_X_y=True, as_frame=True\n",
    ")  # also we can get data in Bunch (dictionary) or pandas DataFrame\n",
    "\n",
    "wine_dataset = features\n",
    "wine_dataset[\"target\"] = class_labels\n",
    "\n",
    "wine_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём первый признак `alcohol`. По имеющийся таблице с данными легко построить функцию распределения $F(X)$, которая будет задавать вероятность $p(\\text{alcohol} = X)$ найти среди наших данных бутылку вина с параметром `alcohol` равным $X$.\n",
    "\n",
    "$$\\large p(\\text{alcohol} = X):$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.displot(wine_dataset, x=\"alcohol\", kind=\"kde\", fill=True, height=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "наш датасет содержит объекты 3 различных классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потому мы можем построить распределение объектов в обучающей выборке по признаку `alcohol` отдельно для каждого из этих трёх классов. Эти распределения зададут нам условную вероятность $p(\\text{alcohol} = X |\\text{target} = i)$ того что объект имеет значение признака `alcohol` равным $X$ при условии, что он относится к одному из классов с номером $i$.\n",
    "\n",
    "$$\\large p(\\text{alcohol} = X|\\text{target} = i):$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(wine_dataset, x=\"alcohol\", hue=\"target\", kind=\"kde\", fill=True, height=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи классификации нам теперь достаточно просто обернуть данную формулу для условной вероятности по [теореме Байеса](https://en.wikipedia.org/wiki/Bayes'_theorem) и получить вероятность того что объект принадлежит классу $i$ при условии того что признак `alcohol` у него принимает значение $X$:\n",
    "$$\\large p(\\text{target} = i | \\text{alcohol} = X) = \\frac{p(\\text{alcohol} = X | \\text{target} = i )p(\\text{target} = i )}{p(\\text{alcohol} = X)},$$\n",
    "\n",
    "где $p(\\text{target} = i)$ будет просто задавать как часто в датасете встречаются объекты класса $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы будем считать распределение объектов по значениям признаков независимыми (например, предполагая, что $p(\\text{alcohol}=X_1, \\text{malic_acid}=X_2) = p(\\text{alcohol}=X_1)p(\\text{malic_acid}=X_2)$), именно это допущение о независимости распределений и приводит к использованию слова *наивный* в названии подхода), то можем записать в общем случае:\n",
    "\n",
    "$$\\large p(\\text{target} = i |\\text{Features} = \\vec X ) = \\frac{p(\\text{Feature}_1 = X_1, \\text{Feature}_2 = X_2,\\dots,\\text{Feature}_n = X_n | \\text{target} = i )p(\\text{target} = i )}{p(\\text{Feature}_1 = X_1) p(\\text{Feature}_2 = X_2)\\dots p(\\text{Feature}_n = X_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы решаем задачу классификации на $k$ классов, то для объекта с набором признаком $\\vec X$ по формуле выше мы получим $k$ чисел, характеризующих вероятность принадлежности данного объекта к различным классам. Для финального принятия решения нам останется выбрать тот класс, для которого вероятность принадлежности наивысшая:\n",
    "\n",
    "$$\\large \\text{Prediction} = \\underset{i}{\\text{argmax}}{\\left(p(\\text{target} = i |\\text{Features} = \\vec X )\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивный Байесовский классификатор часто используют в задаче обнаружения спама и пример его использования в такой задаче может показаться ещё более наглядным.\n",
    "В рамках данной задачи у нас имеется:\n",
    "- Датасет из текстов сообщений с некоторым фиксированным словарём возможных слов.\n",
    "- Два класса сообщений: спам и нормальное.\n",
    "- Признаковое описание для каждого сообщения характеризует количество вхождений каждого из слов словаря в текст сообщения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе этой информации нам нужно научиться отделять нормальные письма от спама. Все письма состоят из 4-х слов: **‘Добрый’, ‘День’, ‘Гости’, ‘Деньги’**. При этом мы уже посчитали, сколько раз каждое слово встречается в каждом классе.\n",
    "\n",
    "Мы можем посчитать вероятность встретить слово **‘Добрый’** в нормальном письме: берем количество слов **‘Добрый’** и делим на количество слов во всех нормальных письмах (с повторениями). Аналогично для других слов.\n",
    "\n",
    "[Источник картинок](https://www.youtube.com/watch?v=O2L2Uv9pdDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/naive_bayes_1.png\" alt=\"alttext\" width=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем то же самое для слов из спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/naive_bayes_2.png\" alt=\"alttext\" width=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем вероятность того, чтобы письмо было нормальным. Для этого количество нормальных писем делим на общее количество писем. Аналогично для спама. Это — $p(\\text{target} = i)$ в формуле выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/naive_bayes_3.png\" alt=\"alttext\" width=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить вероятность нормального письма с фразой **‘Добрый День’** в \"наивном\" предположении мы можем перемножить вероятности нормального письма со словом **‘Добрый’** и нормального письма со словом **‘День’**. Это произведение будет $p(\\text{target} = i | \\text{Features} = \\vec X)$ в формуле выше.\n",
    "\n",
    "Считаем $p(\\text{Features} = \\vec X|\\text{target} = i) p(\\text{target} = i)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/naive_bayes_3_5.png\" alt=\"alttext\" width=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к нашему примеру с датасетом Wine и попробуем решить задачу классификации для него при помощи предложенного алгоритма:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обычно, разделим наш датасет на тренировочную и валидационную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features.values, class_labels.values, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём реализацию Наивного Байесовского классификатора из [библиотеки sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB). Обучим её на тренировочном датасете и измерим качество на отложенной валидационной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the model\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Calculate F1_score\n",
    "pred = model.predict(x_test)\n",
    "f1_score(y_test, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря простоте модельного датасета Wine наша наивная статистическая модель показала отличное качество работы, для решения большинства практических задач на реальных датасетах не следует ожидать настолько же высокого качества. Тем не менее, подход к решению задачи классификации, связанный с построением модели предсказания принадлежности объекта к имеющимся классам, оказался крайне конструктивным. Такой подход можно попробовать применить в общем случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-энтропия, как общая функция потерь для задач классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переход к вероятностям\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "[Видео от StatQuest, которое объясняет Softmax](https://www.youtube.com/watch?v=KpKog-L9veg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/scores_to_probability.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейти к вероятностям мы сможем, проведя с весами некоторые не очень сложные математические преобразования.\n",
    "\n",
    "На слайде выше показано, почему выходы модели часто называют [logit’ами](https://en.wikipedia.org/wiki/Logit). Если предположить, что у нас есть некая вероятность, от которой мы берем такую функцию (logit), то результат может принимать значение в любых вещественных числах. Мы можем считать, что выходы модели — это logit’ы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы могли бы просто взять индекс массива, в котором значение (logit) максимально. Предположим, что наша модель выдала следующие значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = [\n",
    "    5.1,  # cat\n",
    "    3.2,  # car\n",
    "    -1.7,  # frog\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, чтобы узнать какой класс наша сеть предсказала, мы могли бы просто взять `argmax` от наших `logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Predicted class = %i (Cat)\" % (np.argmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но от argmax нельзя посчитать градиент, так как производная от константы равна 0. Соответственно, если бы мы вставили производную от argmax в градиентный спуск, мы бы получили везде нули, и соответственно, наша модель бы вообще ничему не научилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(np.arange(3), [1, 0, 0], color=\"red\", s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А мы бы хотели получить не logit’ы, а настоящую вероятность на выходе модели. Да еще и таким образом, чтобы от наших вероятностей можно было посчитать градиент. Для этого мы можем применить к нашим логитам функцию **Softmax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/linear_classifier_softmax.png\" width=\"1024\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Отобразим наши logit’ы на значения $[0, +∞)$.\n",
    "\n",
    "Для этого возведем экспоненту (число Эйлера 2.71828) в **степень логита**. В результате, мы получим вектор гарантированно неотрицательных чисел (положительное число, возведенное в степень, даже отрицательную, даст положительное значение).\n",
    "\n",
    "2. Нормализуем.\n",
    "\n",
    "Чтобы  интерпретировать числа как вероятности, их сумма должна быть равна единице. Мы должны их нормализовать, то есть **поделить на сумму**.\n",
    "\n",
    "Это преобразование называется **Softmax функцией**. **Получаются вероятности**, то есть числа, которые можно интерпретировать, как вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Softmax}_\\text{кошка} = \\frac{e^{5.1}}{e^{5.1} + e^{3.2} + e^{-1.7}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return np.exp(logits) / np.sum(np.exp(logits))\n",
    "\n",
    "\n",
    "print(softmax(logits))\n",
    "print(\"Sum = %.2f\" % np.sum(softmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что Softmax никоим образом не поменял порядок значений. Самому большому logit'у соответствует самая большая вероятность, а самому маленькому, соответственно, самая маленькая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графиках. Возьмем массив случайных логитов и применим к ним softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_logits = np.linspace(-1, 1, 50)\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(6, 3))\n",
    "\n",
    "ax[0].plot(np.arange(50), rand_logits)\n",
    "ax[0].set_title(\"Logits\")\n",
    "ax[1].plot(np.arange(50), softmax(rand_logits))\n",
    "ax[1].set_title(\"Softmax\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Практическое вычисление SoftMax\n",
    "\n",
    "При вычислении экспоненты от выходов модели могут получиться очень большие числа в силу очень высокой скорости роста экспоненты. Этот факт необходимо учитывать, чтобы вычисления SoftMax были численно стабильны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "f = np.array([123, 456, 789])\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "print(f, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы регуляризовать вычисление, нам следует предварительно упростить возникающую в вычислении дробь. Для этого мы можем вычесть из каждого $s_i$ положительную константу, чтобы уменьшить значения экспонент. В качестве константы можно выбрать максимальный элемент этого вектора, тогда у нас гарантированно не будет очень больших чисел, и такой способ будет работать более стабильно.\n",
    "\n",
    "$$M = \\max_j s_{y_{j}}$$\n",
    "$$s^{new}_{y_{i}}  = s_{y_{i}} - M $$\n",
    "\n",
    "$$ \\dfrac {e^{s^{new}_{y_{i}}}} {\\sum_j e^{s^{new}_{y_{j}}}}  = \\dfrac {e^{s_{y_{i}} - M }} {\\sum_j e^{s_{y_{j}} - M }} = \\dfrac {e^{s_{y_{i}}}e ^ {-M}} {\\sum_j e^{s_{y_{j}}} e ^ {-M}} = \\dfrac {e ^ {-M} e^{s_{y_{i}}}} {e ^ {-M} \\sum_j e^{s_{y_{j}}} } = \\dfrac { e^{s_{y_{i}}}} { \\sum_j e^{s_{y_{j}}} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([123, 456, 789])\n",
    "f -= f.max()\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "print(f, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние (дивергенция) Кульбака — Лейблера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно сравнить полученные вероятности с истинными метками классов.\n",
    "\n",
    "В математической статистике и теории информации в мерой расхождения между двумя вероятностными распределениями $P$ и $Q$ является расстояние (дивергенция) Кульбака — Лейблера, вычисляемое по формуле\n",
    "$$D_{KL}(P||Q) = ∑_i P(i)\\log\\frac{P(i)}{Q(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разобраться, что значит эта формула на примере двух монеток:\n",
    "- настоящей с вероятностями орла и решки 0.5 и 0.5 соответственно,\n",
    "- фальшивой с вероятностями орла и решки 0.2 и 0.8 соответственно.\n",
    "\n",
    "Возьмем настоящую монету и произведем 10 бросков (выборок). Получили последовательность $\\color{blue}{О О} \\color{green}{Р} \\color{blue}{О О} \\color{green}{Р} \\color{blue}{О О О} \\color{green}{Р}$, где $\\color{blue}{O}$ - это орел, $\\color{green}{Р}$ - это решка.\n",
    "Посчитаем вероятности выбросить такую последовательность для настоящей и фальшивой монеты. Броски независимые, поэтому значения вероятностей перемножаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/kl_divergence_1.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем пропорцию вероятностей данной комбинации для настоящей монетки и для фальшивой (независимые случайные величины, вероятности перемножаются). Для заданных значений вероятностей пропорция будет примерно $149:1$.\n",
    "\n",
    "$$\\frac{\\color{blue}{p_1^{N_о}}\\color{green}{p_2^{N_р}}}\n",
    "{\\color{blue}{q_1^{N_о}}\\color{green}{q_2^{N_р}}}=\n",
    "\\frac{\\color{blue}{\\left(\\frac{1}{2}\\right)^{7}}\\color{green}{\\left(\\frac{1}{2}\\right)^{3}}}\n",
    "{\\color{blue}{\\left(\\frac{1}{5}\\right)^{7}}\\color{green}{\\left(\\frac{4}{5}\\right)^{3}}}\\approx \\frac{149}{1}$$\n",
    "\n",
    "Возьмем логарифм от этого значения (это позволит нам избавиться от степеней и заменить умножение сложением) и нормируем на количество бросков монетки $N=\\color{blue}{N_о}+\\color{green}{N_р}$.\n",
    "\n",
    "$$\\frac{\\color{blue}{N_о}}{N}\\log{\\color{blue}{p_1}}+\n",
    "\\frac{\\color{green}{N_р}}{N}\\log{\\color{green}{p_2}}-\n",
    "\\frac{\\color{blue}{N_о}}{N}\\log{\\color{blue}{q_1}}-\n",
    "\\frac{\\color{green}{N_р}}{N}\\log{\\color{green}{q_2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При увеличении количества бросков $N\\to∞$, так как мы бросали настоящую монетку\n",
    "\n",
    "$$\\frac{\\color{blue}{N_о}}{N} \\to \\color{blue}{p1}, \\frac{\\color{green}{N_р}}{N} \\to \\color{green}{p2} .$$\n",
    "\n",
    "Получаем расстояние Кульбака — Лейблера.\n",
    "\n",
    "$$D_{KL}(P||Q) = \\color{blue}{p_1} \\log{\\color{blue}{p_1}}\n",
    "+ \\color{green}{p_2}\\log{\\color{green}{p_2}}\n",
    "- \\color{blue}{p_1}\\log{\\color{blue}{q_1}}\n",
    "- \\color{green}{p_2}\\log{\\color{green}{q_2}}$$\n",
    "\n",
    "$$ = \\color{blue}{p_1} \\log{\\color{blue}{\\frac{p_1}{q_1}}}\n",
    "+ \\color{green}{p_2} \\log{\\color{green}{\\frac{p_2}{q_2}}}$$\n",
    "\n",
    "Обратим внимание, что если $P=Q$, то\n",
    "\n",
    "$$D_{KL}(P||Q) = ∑_i P(i)\\log\\frac{P(i)}{Q(i)} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Переход к оценке модели\n",
    "\n",
    "Мы научились определять близость двух распределений. Как это поможет нам оценить качество модели, если мы знаем, какие метки классов должны получиться?\n",
    "\n",
    "Пусть $P$ — вероятности истинных меток классов для объекта (1 для правильного класса, 0 для остальных), $Q(\\theta)$ — вероятности классов для объекта, предсказанные моделью с обучаемыми параметрами $\\theta$.\n",
    "\n",
    "Расстояние Кульбака — Лейблера между истинными и предсказанными значениями\n",
    "\n",
    "$$D_{KL}(P||Q(\\theta)) = ∑_i P(i)\\log\\frac{P(i)}{Q(i| \\theta)} $$\n",
    "\n",
    "$$ = ∑_i P(i)\\log{P(i))} -  ∑_i P(i)\\log{Q(i|\\theta)} $$\n",
    "\n",
    "$$ = - H(P) + H(P|Q(\\theta))$$\n",
    "\n",
    "Мы разбили сумму на две части, первая из которых называется **энтропией** $H(P)$ и не будет зависеть от модели, а вторая называется **кросс-энтропией** $H(P|Q(\\theta))$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Энтропия\n",
    "\n",
    "Понятие энтропии пришло из теории связи. Для расчета энтропии можно использовать формулу Шеннона:\n",
    "\n",
    "$$H(P)=-\\sum^C_{i=1}P(i)\\cdot log_{2}(P(i)),$$\n",
    "где C &mdash; количество классов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В формуле Шеннона не случайно используется логарифм по основанию 2. Она рассчитывает, **сколько бит информации минимально необходимо для передачи одного значения**, например, одного исхода броска монетки.\n",
    "\n",
    "Если монетка **всегда выдает орел**, то нам нет смысл передавать информацию, чтобы предсказать исход\n",
    "\n",
    "$$H(\\color{blue}{p_1 = 1}, \\color{green}{p_2 = 0}) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для настоящей монетки необходимо будет передавать 1 бит информации на бросок\n",
    "$$H(\\color{blue}{p_1 = 0.5},  \\color{green}{p_2 = 0.5})= log_{2}(2) = 1,$$\n",
    "\n",
    "А вот с поддельной монеткой получается интереснее: если сделать много бросков, можно выявить закономерность что решка выпадет чаще (вероятность трех решек подряд будет больше вероятности одного орла) и за счет этого сократить количество передаваемой информации так, чтобы на один бросок получалось меньше 1 бита. Как это сделать — отдельная область теории информации, называемая **кодирование источника**.\n",
    "$$H(\\color{blue}{q_1 = 0.2}, \\color{green}{q_2 = 0.8}) = 0.722$$\n",
    "\n",
    "__Энтропия__ — мера неуверенности, связанная с распределением $P$.\n",
    "Зная истинное распределение случайной величины, мы можем рассчитать его энтропию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы попытаемся использовать статистические данные, полученные для фальшивой монеты $Q$, для настоящей $P$, мы не получим выигрыш в количестве передаваемой информации:\n",
    "\n",
    "$$H(P||Q) = - \\sum^C_{i=1}P(i)\\cdot log_2(Q(i)) = 1.322$$\n",
    "\n",
    "Формула выше называется **кросс-энтропия**.\n",
    "\n",
    "**Кросс-энтропия** позволяет оценить ситуацию, когда мы аппроксимируем истинное распределение $P$ предсказанным распределением $Q$. Чем больше значения кросс-энтропии, тем больше расхождение между распределениями. Поэтому кросс-энтропию используют в качестве **функции потерь**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы показали выше, **кросс-энтропия** связана с **энтропией** и **расстоянием Кульбака — Лейблера**\n",
    "$$H(P||Q) = D_{KL}(P||Q) + H(P).$$\n",
    "\n",
    "Посчитаем значения для наших монеток с помощью кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal coin\n",
    "p1 = 0.5\n",
    "p2 = 0.5\n",
    "\n",
    "# fake coin\n",
    "q1 = 0.2\n",
    "q2 = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback–Leibler divergence\n",
    "div_kl = p1 * np.log2(p1 / q1) + p2 * np.log2(p2 / q2)\n",
    "print(f\"Dkl(P||Q) = {div_kl:.3f}\")\n",
    "\n",
    "# Entropy normal coin\n",
    "h_p = -p1 * np.log2(p1) - p2 * np.log2(p2)\n",
    "print(f\"H(P) = {h_p:.3f}\")\n",
    "\n",
    "# Entropy fake coin\n",
    "h_q = -q1 * np.log2(q1) - q2 * np.log2(q2)\n",
    "print(f\"H(Q) = {h_q:.3f}\")\n",
    "\n",
    "# Cross-entropy\n",
    "h_p_q = -p1 * np.log2(q1) - p2 * np.log2(q2)\n",
    "print(f\"H(P||Q) = {h_p_q:.3f}\")\n",
    "print(f\"H(P||Q) = Dkl(P||Q) + H(P) = {h_p+div_kl:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расчет функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к задаче классификации изображения. Рассчитаем для предсказания модели Cross-Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/logits_to_scores_to_probabilitys.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В расчетах выше мы использовали логарифм по основанию 2, что приближало нас к теории информации. Мы можем считать кросс-энтропию по натуральному логарифму, она будет отличаться умножением на константу.\n",
    "\n",
    "$$H(P||Q)=-1⋅\\log{0.869}-0\\cdot\\log{0.13}-0\\cdot\\log{0.001}\\approx0.14$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(pred_prob, true_prob):\n",
    "    return -np.dot(true_prob, np.log(pred_prob))\n",
    "\n",
    "\n",
    "print(f\"Logits = {logits}\")\n",
    "\n",
    "pred_prob = softmax(logits)\n",
    "print(f\"Predicted Probabilities = {pred_prob}\")\n",
    "\n",
    "true_prob = [1.0, 0.0, 0.0]\n",
    "print(f\"True Probabilities = {true_prob}\")\n",
    "\n",
    "print(f\"Cross-entropy loss = {cross_entropy_loss(pred_prob, true_prob):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-энтропия vs Hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-entropy / log loss**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/cross_entropy_plot_loss_with_probability.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Преимуществом Cross-Entropy loss (слева) по сравнению с кусочно-гладкой SVM-Loss (справа) является:\n",
    "- гладкость и отсутствие участка с плато (не имеет нулевых производных или неопределенных точек),\n",
    "- большой градиент для большого Loss, маленький вблизи 100% точности (для кусочно гладкой функции градиент ноль или константа).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиент функции потерь. Кросс-энтропия\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cross Entropy Loss](https://wandb.ai/wandb_fc/russian/reports/---VmlldzoxNDI4NjAw#:~:text=%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F%20%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C%20%D0%BF%D0%B5%D1%80%D0%B5%D0%BA%D1%80%D0%B5%D1%81%D1%82%D0%BD%D0%BE%D0%B9%20%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D0%B8%20%E2%80%93%20%D1%8D%D1%82%D0%BE,%2C%20%D0%B3%D0%B4%D0%B5%200%20%E2%80%93%20%D0%B8%D0%B4%D0%B5%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -L = \\sum_i y_i \\log p_i = \\sum_i y_i \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
    "\n",
    "$$s_{y_i} = w_i x$$\n",
    "\n",
    "$$ \\dfrac {\\partial L} {\\partial w_i} = \\dfrac {\\partial L} {\\partial s_{y_i}} \\dfrac {\\partial s_{y_i}} {\\partial w_i} $$\n",
    "\n",
    "$$\\dfrac {\\partial s_{y_i}} {\\partial w_i} = x$$\n",
    "\n",
    "Только один $y_k = 1$\n",
    "\n",
    "\n",
    "$$ -L = y_k \\log p_i = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
    "\n",
    "i = k\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_i}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_i} - \\log  \\sum_j e^{s_{y_j}}$$\n",
    "\n",
    "$$\\dfrac {\\partial -L} {\\partial s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\partial {\\sum_j e^{s_{y_j}}}} {\\partial s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\partial e^{s_{y_i}}} {\\partial s_{y_i}} = 1 - \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = 1 - p_i$$\n",
    "\n",
    "$$\\dfrac {\\partial L} {\\partial s_{y_j}} = p_i - 1 $$\n",
    "\n",
    "$$ \\dfrac {\\partial L_i} {\\partial w_i}  = \\dfrac {\\partial L} {\\partial s_{y_i}} \\dfrac {\\partial s_{y_i}} {\\partial w_i} = (p_i - 1) x $$\n",
    "\n",
    "i != k\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_k}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_k} - \\log  \\sum_j e^{s_{y_j}}$$\n",
    "\n",
    "$$\\dfrac {\\partial -L} {\\partial s_{y_i}} = - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\partial {\\sum_j e^{s_{y_j}}}} {\\partial s_{y_i}} =  \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\partial e^{s_{y_i}}} {\\partial s_{y_i}} = \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = - p_i$$\n",
    "\n",
    "$$\\dfrac {\\partial L} {\\partial s_{y_j}} = p_i $$\n",
    "$$ \\dfrac {\\partial L_i} {\\partial w_i}  = \\dfrac {\\partial L} {\\partial s_{y_i}} \\dfrac {\\partial s_{y_i}} {\\partial w_i} = p_i x $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде это будет выглядеть вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input batch of 2 vector with 4 elements\n",
    "x = np.array([[1, 2, 3, 4], [1, -2, 0, 0]])\n",
    "# Weights\n",
    "W = np.random.randn(3, 4)  # 3 class\n",
    "\n",
    "# model output\n",
    "logits = x.dot(W.T)\n",
    "print(\"Scores(Logits) \\n\", logits, \"\\n\")\n",
    "\n",
    "# Probabilities\n",
    "probs = softmax(logits)  # defined before\n",
    "print(\"Probs \\n\", probs, \"\\n\")\n",
    "\n",
    "# Ground true classes\n",
    "y = [0, 1]\n",
    "\n",
    "# Derivative\n",
    "probs[np.arange(2), y] = -1  # substract one from true class prob\n",
    "dW = x.T.dot(probs)  # dot product with input\n",
    "\n",
    "print(\"Grads dL/dW \\n\", dW)  # have same shape as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример обучения линейного классификатора с hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class LinearClassifier:\n",
    "    def __init__(self, labels, batch_size, imgsize=28, nchannels=1, random_state=42):\n",
    "        self.labels = labels  # classes names\n",
    "        self.classes_num = len(labels)  # num of classes\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.W = (\n",
    "            np.random.randn(nchannels * imgsize**2 + 1, self.classes_num) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias\n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # reset loss\n",
    "        train_len = x_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)\n",
    "\n",
    "        for i in range(0, train_len, self.batch_size):\n",
    "            idx = indexes[i : i + self.batch_size]\n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "\n",
    "            x_batch = np.hstack([x_batch, np.ones((x_batch.shape[0], 1))])  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(x_batch, y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        current_batch_size = x.shape[0]  # batch_size\n",
    "        loss = 0.0\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        for i in range(current_batch_size):\n",
    "            scores = x[i].dot(self.W)  # vector of shape 10\n",
    "            correct_class_score = scores[int(y[i])]\n",
    "            above_zero_loss_count = 0\n",
    "            for j in range(self.classes_num):\n",
    "                if j == y[i]:  # predict class\n",
    "                    continue\n",
    "                margin = scores[j] - correct_class_score + 1  # loss\n",
    "                if margin > 0:\n",
    "                    above_zero_loss_count += 1\n",
    "                    loss += margin  #\n",
    "                    dW[:, j] += x[i]  #\n",
    "            dW[:, int(y[i])] -= above_zero_loss_count * x[i]\n",
    "        loss /= current_batch_size\n",
    "        dW /= current_batch_size\n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.append(x, 1)  # add 1 (bias)\n",
    "        scores = x.dot(self.W)\n",
    "        return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для проверки качества полученной модели на валидационной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, x_test, y_test, noprint=False):\n",
    "    correct = 0\n",
    "    for i, img in enumerate(x_test):\n",
    "        index = model.forward(img)\n",
    "        correct += 1 if index == y_test[i] else 0\n",
    "        if noprint is False:\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                print(\"Accuracy {:.3f}\".format(correct / i))\n",
    "    return correct / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model_class, x_train, y_train, x_test, y_test, labels, imgsize, nchannels\n",
    "):\n",
    "    print(\"How learning quality depends of speed:\")\n",
    "\n",
    "    for lr in [1e-2, 1e-8]:\n",
    "        for bs in [256, 2048]:\n",
    "            print(\"-\" * 50, \"\\n\", \"learning_rate =\", lr, \"\\tbatch_size =\", bs)\n",
    "            print()\n",
    "            model = model_class(labels, bs, imgsize, nchannels)\n",
    "\n",
    "            best_accuracy = 0\n",
    "            for epoch in range(10):\n",
    "                loss = model.fit(x_train, y_train, learning_rate=lr)\n",
    "                accuracy = validate(model, x_test, y_test, noprint=True)\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_epoch = epoch\n",
    "                print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
    "\n",
    "            print()\n",
    "            print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST](https://paperswithcode.com/dataset/mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from IPython.display import clear_output\n",
    "\n",
    "dataset_train = MNIST(\"content\", train=True, download=True)\n",
    "dataset_test = MNIST(\"content\", train=False, download=True)\n",
    "\n",
    "x_train = dataset_train.data.numpy().reshape((-1, 28 * 28))\n",
    "y_train = np.array(dataset_train.targets)\n",
    "\n",
    "x_test = dataset_test.data.numpy().reshape((-1, 28 * 28))\n",
    "y_test = np.array(dataset_test.targets)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_train shape : {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(\n",
    "    LinearClassifier, x_train, y_train, x_test, y_test, np.arange(10), 28, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CIFAR-10](https://paperswithcode.com/dataset/cifar-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_exists = os.path.exists(\"/content/cifar-10-batches-py\")\n",
    "if file_exists == False:\n",
    "    #!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !wget https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/cifar-10-python.tar.gz\n",
    "    !tar -xzf cifar-10-python.tar.gz\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "x_train = np.zeros((0, 3072))\n",
    "y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    # raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    raw = unpickle(f\"cifar-10-batches-py/data_batch_{i}\")\n",
    "    x_train = np.append(x_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    y_train = np.append(y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "# test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "test = unpickle(\"cifar-10-batches-py/test_batch\")\n",
    "x_test = np.array(test[b\"data\"])\n",
    "y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_eng = [\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(\n",
    "    LinearClassifier, x_train, y_train, x_test, y_test, labels_eng, 32, 3\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
