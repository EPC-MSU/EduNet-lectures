{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Линейные модели</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы познакомимся с линейными моделями. Это простые модели, которые являются частью многих сложных моделей, о которых мы будем рассказывать на нашем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе линейных моделей лежит линейная функция\n",
    "$$\\large f(\\vec x) = (\\vec w, \\vec x) +b, $$\n",
    "где $(\\vec w, \\vec x)$ — скалярное произведение:\n",
    "\n",
    "$$\\large (\\vec w, \\vec x) = \\sum_{i=1}^n{w_ix_i} = w_1x_1+w_2x_2+...+w_ix_i+ ... + w_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/scalar_product_ways_to_use.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейные модели являются простыми предшественниками нейронных сетей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регрессия** — это одна из трех базовых задач машинного обучения (классификация, регрессия, кластеризация).\n",
    "\n",
    "В задаче **регрессии** мы используем входные **признаки**, чтобы предсказать **целевые значения**. Например, чтобы предсказать цену жилья по его характеристикам (площадь, этаж, год постройки дома, высота потолков, район, ...). **Линейная регрессия** сводится к тому, чтобы провести “**линию наилучшего соответствия**” через набор точек данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Модель и ее параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у нас есть набор точек $\\{(x_i, y_i)\\}$.\n",
    "\n",
    "Цель линейной регрессии — **поиск линии, которая наилучшим образом соответствует заданным точкам**. Напомним, что общее уравнение прямой:\n",
    "\n",
    "$$\\large f(x) = w⋅x + b,$$\n",
    "\n",
    "где $w$ — характеризует наклон линии (в будущем мы будем называть значения $w$ весом, weight) а $b$ — её сдвиг по оси $y$ (bias). Таким образом, решение линейной регрессии определяет значения для $w$ и $b$ так, что $f(x)$ приближается как можно ближе к $y(x)$. Здесь $w$ и $b$ — **параметры модели**.\n",
    "\n",
    "Отобразим на графике случайные точки, расположенные в окрестности $y(x) = 3⋅x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 + 3 * x + (np.random.rand(100, 1) - 0.5)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что нам неизвестны параметры наклона и сдвига $w$ и $b$. Для их определения мы бы могли рассмотреть все возможные прямые вида $f(x) = w⋅x + b$ и выбрать среди семейства прямых такую, которая лучше всего приближает имеющиеся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in np.arange(-5.0, 7.0, 1):\n",
    "    for b in [-1, 0, 1, 2, 3]:\n",
    "        y_predicted = b + w * x\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель** $f(x) = w⋅x + b$ задаёт параметрическое семейство функций, а **выбор \"правильного\" представителя** из **параметрического семейства** и называется **обучением** модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in np.arange(-5.0, 7.0, 1):\n",
    "    for b in [-1, 0, 1, 2, 3]:\n",
    "        y_predicted = b + w * x\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.plot(x, 2 + 3 * x, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выбрать параметры?\n",
    "\n",
    "**Функция потерь** позволяет вычислить меру количества ошибок. Для задачи **регрессии** такой мерой может быть **расстояние** между предсказанным значением $f(x)$ и его фактическим значением. Распространенной функцией потерь является **средняя квадратичная ошибка** (MSE). Чтобы вычислить MSE, мы просто берем все значения ошибок, считаем квадраты их длин и усредняем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы определяем ошибку модели на одном объекте как квадрат расстояния между предсказанием и истинным значением, а общая функция потерь будет задана выражением:\n",
    "\n",
    "$$l_i =|y_i - f(x_i)| $$\n",
    "\n",
    "$$ \\text{Loss} = \\sum l_i^2 = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для прямой с параметрами $w=4$, $b = 2$ и $w=3$, $b = 2$ (верные значения):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_line(ax, x, y, w, b, color=\"r\"):\n",
    "    y_predicted = w * x + b\n",
    "    # line\n",
    "    ax.plot(x, y_predicted, color=color, alpha=0.5, label=f\"f(x)={w}x+{b}\")\n",
    "    # delta\n",
    "    for x_i, y_i, f_x in zip(x, y, y_predicted):\n",
    "        ax.vlines(x=x_i, ymin=min(f_x, y_i), ymax=max(f_x, y_i), ls=\"--\", alpha=0.3)\n",
    "    # MSE\n",
    "    loss = np.sum((y - (w * x + b)) ** 2) / (len(x))\n",
    "    ax.set_title(f\"MSE = {loss:.3f}\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# plot x_i y_i (dots)\n",
    "for ax in axs:\n",
    "    ax.scatter(x, y, s=10)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([2, 6])\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "plot_delta_line(axs[0], x, y, w=4, b=2, color=\"r\")\n",
    "plot_delta_line(axs[1], x, y, w=3, b=2, color=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача **поиска оптимальных параметров** модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск локального минимума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как будет выглядеть ландшафт функции потерь для нашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(-10, 30, 1)\n",
    "b = np.arange(-10, 10, 1)\n",
    "\n",
    "w, b = np.meshgrid(w, b)\n",
    "\n",
    "loss = np.zeros_like(w)\n",
    "for i in range(w.shape[0]):\n",
    "    for j in range(w.shape[1]):\n",
    "        loss[i, j] = np.sum((y - (w[i, j] * x + b[i, j])) ** 2) / (len(x))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "surf = ax.plot_surface(w, b, loss, cmap=plt.cm.RdYlGn_r, alpha=0.5)\n",
    "\n",
    "ax.contourf(w, b, loss, zdir=\"z\", offset=-1, cmap=\"RdYlGn_r\", alpha=0.5)\n",
    "ax.set_zlim(0, 20)\n",
    "\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_title(\"MSE\")\n",
    "\n",
    "fig.colorbar(surf, location=\"left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимым (но недостаточным) условием локального минимума дифференцируемой функции является равенство нулю частных производных:\n",
    "\n",
    "$$\t\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\displaystyle\\frac{\\partial \\text{Loss}}{\\partial w}=0,\n",
    "   \\\\\n",
    "   \\displaystyle\\frac{\\partial \\text{Loss}}{\\partial b}=0.\n",
    " \\end{cases}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Т.к. MSE для линейной регрессии — полином второй степени относительно $w$ и $b$, а полином второй степени не может иметь больше одного экстремума, то локальный минимум будет глобальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Метод наименьших квадратов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем простейшую модель линейной регрессии с использованием библиотеки NumPy на датасете, определённом выше.\n",
    "\n",
    "Используем метод наименьших квадратов: [МНК, простейшие частные случаи 📚[wiki]](https://ru.wikipedia.org/wiki/Метод_наименьших_квадратов#Простейшие_частные_случаи).\n",
    "\n",
    "$$w = \\frac{n\\sum_{i=1}^nx_iy_i - (\\sum_{i=1}^nx_i)(\\sum_{i=1}^ny_i)}{n\\sum_{i=1}^nx_t^2 - (\\sum_{i=1}^n x_t)^2};$$\n",
    "\n",
    "$$b = \\frac{\\sum_{i=1}^ny_i - w(\\sum_{i=1}^nx_i)}{n}.$$\n",
    "\n",
    "По сути метод наименьших квадратов — это решение системы уравнений выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_coef(x, y):\n",
    "    n = len(x)\n",
    "    w = (n * sum(np.multiply(x, y)) - sum(x) * sum(y)) / (\n",
    "        n * sum(np.multiply(x, x)) - sum(x) ** 2\n",
    "    )\n",
    "    b = (sum(y) - w * sum(x)) / n\n",
    "    return w, b\n",
    "\n",
    "\n",
    "w, b = estimate_coef(x, y)\n",
    "\n",
    "y_predicted = w * x + b\n",
    "\n",
    "print(f\"Estimated coefficients:\\nb = {b[0]:.3f} \\nw = {w[0]:.3f}\")\n",
    "print(f\"Final equation: \\ny = {w[0]:.3f}x +{b[0]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.plot(x, y_predicted, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученное решение близко к исходной  зависимости $y=3\\cdot x + 2$.\n",
    "\n",
    "Для многомерного случая МНК можно записать [решение 📚[book]](https://sun.tsu.ru/mminfo/2016/Dombrovski/book/chapter-3/chapter-3-2.htm) в матричном виде:  \n",
    "$$\\vec w = (X^TX)^{-1}X^T\\vec y$$\n",
    "где $\\vec w$ — вектор параметров модели, включающий $b$ (при записи этого решения используется трюк “столбец единиц”, о котором мы поговорим чуть позже),\n",
    "$X$ — матрица входных признаков (с единичным столбцом),\n",
    "$\\vec y$ — вектор предсказываемых значений.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С одной из метрик регрессии мы уже познакомились: это $\\text{MSE}$, которую мы минимизировали в методе наименьших квадратов. Стоит отметить, что $\\text{MSE}$ имеет [размерность 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8) квадрата размерности предсказываемого значения.\n",
    "\n",
    "$$ \\text{MSE}  = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$\n",
    "\n",
    "Чтобы получить оценку ошибки той же размерности, можно взять корень (root) от $\\text{MSE}$. Это метрика $\\text{RMSE}$:\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum (y_i - f(x_i))^2}$$\n",
    "\n",
    "Или посчитать среднюю абсолютную ошибку $\\text{MAE}$:\n",
    "\n",
    "$$ \\text{MAE} = \\frac{1}{N} \\sum |y_i - f(x_i)|$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют и более специфичные метрики, например, $R^2$, которая принимает значения от $(-\\inf, 1]$, где $1$  —  наилучший вариант. $R^2$ называется [коэффициентом детерминации 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8) и характеризует долю дисперсии целевого значения, которую объясняет модель.\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\text{MSE}}{\\sigma^2}=1 - \\frac{\\sum {(y_i-f(x_i))^2}}{\\sum{(y_i-\\bar{y})^2}},$$\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{N}\\sum {y_i},$$\n",
    "\n",
    "где $\\sigma^2$ — дисперсия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/linear_regression.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Когда $R^2$ около нуля, модель плохо объясняет данные</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://xkcd.com/1725\">https://xkcd.com/1725</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def print_metrics(y_true, y_predicted):\n",
    "    print(f\"Mean squared error: {mean_squared_error(y_true, y_predicted):.3f}\")\n",
    "    print(\n",
    "        \"Root mean squared error: \",\n",
    "        f\"{mean_squared_error(y_true, y_predicted, squared=False):.3f}\",\n",
    "    )\n",
    "    print(f\"Mean absolute error: {mean_absolute_error(y_true, y_predicted):.3f}\")\n",
    "    print(f\"R2 score: {r2_score(y_true, y_predicted):.3f}\")\n",
    "\n",
    "\n",
    "print_metrics(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее про метрики можно почитать [тут 📚[book]](https://academy.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii). Там же вы можете найти информацию об относительных ошибках, выражаемых в процентах. Выбор метрики в реальной задаче зависит от традиции, сложившейся в области, поэтому для выбора метрик важно провести литературный обзор.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель линейной регрессии из библиотеки scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свою линейную регрессию мы написали. Теперь изучим, как работать с моделью из Sklearn.\n",
    "\n",
    "Рассмотрим следующую задачу. Пусть мы хотим построить модель предсказания успеваемости студента на основе информации о величине потраченного им на изучение материала количества времени в часах. Это пример простейшей задачи линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет [Students Score 🛠️[doc]](https://www.kaggle.com/datasets/shubham47/students-score-dataset-linear-regression). Датасет содержит два числовых значения — часы и результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/student_scores.csv\"\n",
    ")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости одного от другого, а также отобразим распределения каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.jointplot(data=dataset, x=\"Hours\", y=\"Scores\", height=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на train и test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.iloc[:, :-1].values  # column Hours\n",
    "y = dataset.iloc[:, 1].values  # column Score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае сложного, многомодового распределения значений целевой переменной в задаче регрессии может быть полезно задуматься о стратификации данных. Стратификация данных для задачи регрессии специфична и не реализована в Sklearn, о ней можно почитать в [Regression Analysis Based on Stratified Samples 🎓[article]](https://www.jstor.org/stable/2336525?seq=1), пример кода можно найти на [форуме ✏️[blog]](https://datascience.stackexchange.com/questions/33140/stratify-on-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель для линейной регрессии. Чтобы не писать с нуля, воспользуемся готовой моделью из библиотеки Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.linspace(min(x_train), max(x_train), 100)  # 100 dots at min to max\n",
    "y_pred = regressor.predict(x_points)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_train, y_train, \"o\", label=\"Scores\")\n",
    "plt.plot(\n",
    "    x_points,\n",
    "    y_pred,\n",
    "    label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_),\n",
    ")\n",
    "plt.title(\"Hours vs Percentage\", size=12)\n",
    "plt.xlabel(\"Hours Studied\", size=12)\n",
    "plt.ylabel(\"Percentage Score\", size=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем предсказание для тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "x_points = np.linspace(min(x_test), max(x_test), 100)\n",
    "y_pred = regressor.predict(x_points)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_test, y_test, \"o\", label=\"Scores\")\n",
    "plt.plot(\n",
    "    x_points,\n",
    "    y_pred,\n",
    "    label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_),\n",
    ")\n",
    "plt.title(\"Hours vs Percentage\", size=12)\n",
    "plt.xlabel(\"Hours Studied\", size=12)\n",
    "plt.ylabel(\"Percentage Score\", size=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неплохо.\n",
    "\n",
    "Посчитаем метрики для наших значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будьте осторожнее: модели отражают только те закономерности, которые видели в данных. Вероятность того, что студент, потративший на подготовку 20 часов, получит больше максимального балла, мала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/extrapolating.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://xkcd.com/605\">https://xkcd.com/605</a></em></center>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы решали задачу линейной регрессии аналитически (МНК), но это не всегда возможно по нескольким причинам:\n",
    "* Для аналитического решения нужно считать обратную матрицу, это вычислительно сложно и матрица бывает плохо определенной.\n",
    "* Данных может быть слишком много для того, чтобы их можно было одновременно положить в память для расчета обратной матрицы.\n",
    "* Модели могут быть слишком сложными для поиска аналитического решения. Более того, для сложных моделей ландшафт функции потерь может иметь сложный рельеф с несколькими локальными минимумами.\n",
    "\n",
    "Давайте поговорим о том, что делать в таком случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод, который мы будем использовать, называется **“метод градиентного спуска”**. Для начала вспомним, что такое **градиент**. Возьмем функцию двух переменных:\n",
    "\n",
    "$$\\large f(x, y) = \\sin(x\\cdot y)$$\n",
    "\n",
    "Она будет отличаться от функции потерь, которую мы визуализировали, тем, что у нее будет не один экстремум, а сложный рельеф. Рассчитаем ее на диапазоне значений от $0$ до $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = lambda x, y: np.sin(x * y)\n",
    "\n",
    "x = np.linspace(0, 4, 1000)\n",
    "y = np.linspace(0, 4, 1000)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "zz = f(xx, yy)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 7))\n",
    "\n",
    "\n",
    "def show_3d(xx, yy, zz, fig):\n",
    "    ax = fig.add_subplot(121, projection=\"3d\")\n",
    "    surf = ax.plot_surface(xx, yy, zz, cmap=plt.cm.RdYlGn_r)\n",
    "\n",
    "    ax.contourf(xx, yy, zz, zdir=\"zz\", offset=-2, cmap=\"RdYlGn_r\")\n",
    "    ax.set_zlim(-2, 2)\n",
    "\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"sin(xy)\")\n",
    "    fig.colorbar(surf, location=\"left\")\n",
    "\n",
    "\n",
    "show_3d(xx, yy, zz, fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eсли $\\varphi = \\varphi(\\vec{x})=\\varphi(x_1 \\dots x_n)$ — функция $n$ переменных, то её градиентом называется $n$-мерный вектор:\n",
    "$$\n",
    "\\nabla \\varphi(\\vec{x})=\n",
    "\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial\\varphi}{\\partial x_1}\\\\\n",
    "\\displaystyle\\frac{\\partial\\varphi}{\\partial x_2}\\\\\n",
    "...\\\\\n",
    "\\displaystyle\\frac{\\partial\\varphi}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем градиент нашей функции $f(x, y)$. Для этого воспользуемся [**таблицей производных** 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%BD%D1%8B%D1%85) и правилом вычисления [**производной сложной функции** 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%84%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D0%B9_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8) (Chain-rule):\n",
    "$$\\frac {\\partial f} {\\partial x} = \\frac {\\partial f} {\\partial t} \\cdot \\frac {\\partial t} {\\partial x}$$\n",
    "\n",
    "Это правило очень нам пригодится в будущем.\n",
    "\n",
    "$$\\nabla f(x, y)=\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial\\sin(xy)}{\\partial(xy)}\\cdot\\frac{\\partial(xy)}{\\partial x}\\\\\n",
    "\\displaystyle\\frac{\\partial\\sin(xy)}{\\partial(xy)}\\cdot\\frac{\\partial(xy)}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\cos(xy)\\cdot y\\\\\n",
    "\\cos(xy)\\cdot x\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Посчитаем градиент на том же диапазоне (сетка реже, т.к. мы будем рисовать не точки, а стрелочки):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradf = lambda x, y: (np.cos(x * y) * y, np.cos(x * y) * x)\n",
    "\n",
    "xsmall = np.linspace(0, 4, 15)\n",
    "ysmall = np.linspace(0, 4, 15)\n",
    "xxsmall, yysmall = np.meshgrid(xsmall, ysmall)\n",
    "gradx, grady = gradf(xxsmall, yysmall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как **значение градиента в точке** — это вектор, мы можем говорить о его **величине** и **направлении**. Визуализируем наши расчеты: посмотрим на ландшафт функции $f(x, y)$ и направления градиентов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "show_3d(xx, yy, zz, fig)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(\n",
    "    zz,\n",
    "    extent=(np.min(x), np.max(x), np.min(y), np.max(y)),\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.quiver(xxsmall, yysmall, gradx, grady)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке выше значения градиента в точке обозначены чёрными стрелочками. Можно заметить, что длина стрелок в  районе максимальных и минимальных значений функции **почти нулевая**, стрелки направлены в направлении возрастания значения функции и наиболее длинные стрелки находятся в области наиболее резкого изменения значений функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это проявление **свойств градиента**:\n",
    "* Направление $\\frac{\\nabla f}{||\\nabla f||}$ — сообщает нам направление максимального роста функции.\n",
    "\n",
    "*  Величина $||\\nabla f||$ — характеризует мгновенную скорость изменения значений функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим еще раз данные с зависимостью оценок студентов от времени подготовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/student_scores.csv\"\n",
    ")\n",
    "\n",
    "x = dataset.iloc[:, :-1].values  # column Hours\n",
    "y = dataset.iloc[:, 1].values  # column Score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем **код для интерактивной визуализации**. Он нужен только для объяснения и **не пригодится вам в работе**. Его разбирать мы не будем. Eсли интересно, можно изучить самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title *Code for interactive visual\n",
    "# source: https://github.com/TomasBeuzen/deep-learning-with-pytorch\n",
    "\n",
    "!wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/interactive_visualization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты рассмотрим одномерный случай. Будем подбирать только $w$, значение $b$ зафиксируем на уровне $2.83$. Визуализируем ошибку и значения $\\dfrac{\\partial \\text{Loss}}{\\partial w}$ для MSE Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_grid_search\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "slopes = np.arange(5, 15, 0.5)\n",
    "prediction = {f\"{w}\": w * x_train[:, 0] + 2.83 for w in slopes}\n",
    "mse = np.array([mean_squared_error(y_train, w * x_train[:, 0] + 2.83) for w in slopes])\n",
    "dmse_dw = np.array(\n",
    "    [(2 * x_train[:, 0] * (w * x_train[:, 0] + 2.83 - y_train)).mean() for w in slopes]\n",
    ")\n",
    "plot_grid_search(x_train[:, 0], y_train, slopes, prediction, mse, dmse_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Видно, что оптимальное значение наклона соответствует минимуму MSE и нулю частной производной $\\dfrac{\\partial \\text{Loss}}{\\partial w}$. Аналогично будет, если мы возьмем в качестве Loss MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "slopes = np.arange(5, 15, 0.5)\n",
    "prediction = {f\"{w}\": w * x_train[:, 0] + 2.83 for w in slopes}\n",
    "mae = np.array([mean_absolute_error(y_train, w * x_train[:, 0] + 2.83) for w in slopes])\n",
    "dmae_dw = np.array(\n",
    "    [\n",
    "        (x_train[:, 0] * np.sign(w * x_train[:, 0] + 2.83 - y_train)).mean()\n",
    "        for w in slopes\n",
    "    ]\n",
    ")\n",
    "plot_grid_search(x_train[:, 0], y_train, slopes, prediction, mae, dmae_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, т.к. градиент указывает направление наибольшего возрастания функции:\n",
    "\n",
    "* если $\\dfrac{\\partial \\text{Loss}}{\\partial w} < 0$, то нам имеет смысл “идти” в сторону возрастания $\\dfrac{\\partial \\text{Loss}}{\\partial w}$;\n",
    "\n",
    "* если $\\dfrac{\\partial \\text{Loss}}{\\partial w} > 0$ — в сторону убывания.\n",
    "\n",
    "**Метод градиентного спуска** — итеративный метод, идея которого заключается в том, чтобы небольшими шажками “идти” в **обратную от градиента сторону**:\n",
    "\n",
    "$$\\large \\vec w_{n+1} = \\vec w_{n} - α \\cdot \\nabla_{\\vec w_{n}} \\text{Loss},$$\n",
    "где $α$ — скорость обучения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем реализовать это в коде (для простоты только для $w$ при $b=2.83$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, w, b):\n",
    "    return 2 * (x * (w * x + b - y)).mean()\n",
    "\n",
    "\n",
    "def gradient_descent(x_train, y_train, x_test, y_test, w, alpha, b=2.83, iteration=10):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = [w]\n",
    "    mse_train = [mean_squared_error(y_train, w * x_train + b)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, w * x_test + b)]\n",
    "    prediction = {w: w * x_train + b}\n",
    "    print(\n",
    "        f\"Iteration 0: w = {w:.2f}, Loss_train = {mse_train[0]:.2f}, \"\n",
    "        f\"Loss_test = {mse_test[0]:.2f}.\"\n",
    "    )\n",
    "    for i in range(iteration):\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_train, y_train, w, b)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws.append(w)\n",
    "        mse_train.append(mean_squared_error(y_train, w * x_train + b))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, w * x_test + b))\n",
    "        prediction[w] = w * x_train + b\n",
    "        print(\n",
    "            f\"Iteration {i+1}: w = {w:.2f}, Loss_train = {mse_train[i]:.2f}, \"\n",
    "            f\"Loss_test = {mse_test[i]:3.2f}.\"\n",
    "        )\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашу модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.01, iteration=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем процесс обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_gradient_descent\n",
    "\n",
    "plot_gradient_descent(x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что за 7 эпох мы получили то же значение $w$, что получали при использовании `LinearRegression`. При этом мы пришли в минимум MSE и ноль градиента.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальности мы будем работать с функциями многих переменных, поэтому смотреть на сходимость по одной переменной — не самый оптимальный вариант. Более эффективно будет посмотреть на зависимость Loss от количества эпох для train и test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mse(mse_train, mse_test):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(mse_train, label=\"train\")\n",
    "    plt.plot(mse_test, label=\"test\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(\"iterations\", fontsize=12)\n",
    "    plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие графики называют кривыми обучения. Посмотрим на кривые обучения при нашей скорости обучения $α=0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что **Loss падает** как на **train**, так и на **test** выборке. Также мы можем сказать, что **сеть обучилась**: train и test **графики вышли на плато**. При этом не произошло **переобучение**: ошибка на **test** выборке **не начала расти** (про переобучение поговорим позже).\n",
    "\n",
    "В полученных графиках есть особенность, которая бросается в глаза опытному в обучении моделей человеку: **Loss на test выборке меньше, чем на train**. Это показатель того, что **с данными что-то не так**. Так бывает при утечке данных (об утечке данных вы подробнее узнаете в следующих лекциях), но в данном случае, test выборка просто слишком мала, чтобы отражать генеральную совокупность (всего 5 студентов, доверительный интервал для такого маленького количества объектов будет широкий)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор скорости обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Скорость (шаг) обучения** — некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время такой, чтобы тот же процесс не шел слишком медленно (как на графике слева)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/learning_rate_optimal_value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на скорость обучения на нашем примере. При **маленькой скорости обучения** мы будем очень медленно сходиться к минимуму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.0005, iteration=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спустя 30 итераций оранжевая прямая плохо отражает генеральную совокупность. Мы не достигли минимума MSE и нуля градиента.\n",
    "\n",
    "Посмотрим, как выглядят кривые обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель недообучена — значения Loss не вышли на плато.\n",
    "\n",
    "Посмотрим на **достаточно большую скорость обучения**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.027, iteration=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг, получаемый умножением градиента на скорость обучения, получается достаточно большим, чтобы “перескочить” локальный минимум, но при этом модель все-таки попадает в него. Кривые обучения при этом успешно выходят на плато."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В финале посмотрим на **очень большую скорость обучения**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0],\n",
    "    y_train,\n",
    "    x_test[:, 0],\n",
    "    y_test,\n",
    "    w=5,\n",
    "    alpha=0.034,\n",
    "    iteration=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг, который мы делаем, слишком большой. Мы не попадаем в локальный минимум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "По кривым обучения видно, что модель не сошлась: ошибка растет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор скорости обучения будет зависеть от модели и данных. В 7 лекции вы познакомитесь с различными модификациями метода градиентного спуска и узнаете больше о выборе скорости обучения, а пока ориентируйтесь на кривые обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Единый подход к учету смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока мы настраивали только одну переменную, но даже в случае предсказания оценки по времени подготовки у нас две переменные: вес $w$ и смещение $b$.\n",
    "\n",
    "Когда признаков станет больше, у нас получится “лапша” из слагаемых:\n",
    "$$y = b + w_1\\cdot x_1 + w_2\\cdot x_2 + w_3\\cdot x_3 + w_4\\cdot x_4 + w_5\\cdot x_5 + ... + w_n\\cdot x_n$$\n",
    "\n",
    "Нам бы хотелось записать их компактно, чтобы не усложнять код и использовать один и тот же код для данных с разным количеством признаков.  Для этого мы будем использовать матричное перемножение и трюк **“столбец единиц”**, который реализует **единый подход к учету смещения**.\n",
    "\n",
    "Обозначим вектор-столбец из настраиваемых параметров:\n",
    "$$\\vec w = \\begin{bmatrix}\n",
    "b \\\\ w \\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.5], [5]])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К матрице (в нашем случае был только один признак, поэтому у нас будет вектор-столбец) признаков слева \"дорисуем\" столбец единиц:\n",
    "$$X = \\begin{bmatrix}\n",
    "1 & X \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2.7 \\\\\n",
    "1 & 3.3 \\\\\n",
    "... & ...\\\\\n",
    "1 & 9.2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Предупреждение:** добавлять столбец единиц нужно, только если вы сами пишете модель. **Если вы пользуетесь готовыми моделями, в этом нет необходимости.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрицу $X$ можно матрично перемножить со столбцом $\\vec w$, т.к количество столбцов $X$ совпадает с количеством строк в $\\vec w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае:\n",
    "\n",
    "$$\\large \\vec y = b + w_1\\cdot x_1 + w_2\\cdot x_2 + w_3\\cdot x_3 + w_4\\cdot x_4 + w_5\\cdot x_5 + ... + w_n\\cdot x_n = X\\vec w $$  \n",
    "\n",
    "Эту формулу можно свести к нескольким символам кода (`@` — матричное умножение):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x_test @ w\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Необходимость нормализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем многомерный градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "y_test = np.expand_dims(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, w):\n",
    "    \"\"\"Gradient of mean squared error.\"\"\"\n",
    "    return 2 * (x.T @ (x @ w) - x.T @ y) / len(x)\n",
    "\n",
    "\n",
    "def gradient_descent(x_train, y_train, x_test, y_test, w, alpha, iteration=10):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = np.zeros((iteration + 1, 2))\n",
    "    ws[0] = w[:, 0]\n",
    "    mse_train = [mean_squared_error(y_train, x_train @ w)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, x_test @ w)]\n",
    "    prediction = {(w[0][0], w[1][0]): x_train @ w}\n",
    "\n",
    "    print(\n",
    "        f\"Iteration 0: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "        f\"Loss_train = {mse_train[0]:.2f}, \"\n",
    "        f\"Loss_test = {mse_test[0]:.2f}.\"\n",
    "    )\n",
    "\n",
    "    for i in range(iteration):\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_train, y_train, w)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws[i + 1] = w[:, 0]\n",
    "        mse_train.append(mean_squared_error(y_train, x_train @ w))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, x_test @ w))\n",
    "        prediction[(w[0][0], w[1][0])] = x_train @ w\n",
    "\n",
    "        print(\n",
    "            f\"Iteration {i+1}: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "            f\"Loss_train = {mse_train[i]:.2f}, \"\n",
    "            f\"Loss_test = {mse_test[i]:3.2f}.\"\n",
    "        )\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.5], [5]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    w,\n",
    "    0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы не дошли до оптимальной прямой $y = 9.68x+2.83$, которую вычисляли выше.\n",
    "\n",
    "При этом график Loss выглядит неплохо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такое поведение связано с ландшафтом функции потерь: значение ошибки по оси $b$ изменяется намного медленнее, чем по оси $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_grid_search_2d\n",
    "\n",
    "intercepts = np.arange(-7.5, 12.5, 0.1)  # b\n",
    "slopes = np.arange(5, 15, 0.1)  # w\n",
    "plot_grid_search_2d(x_train[:, 1], y_train, slopes, intercepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому основное изменение значений происходит вдоль оси $w$, а $b$ меняется слабо (значение $b$ далеко от ожидаемого)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_gradient_descent_2d\n",
    "\n",
    "plot_gradient_descent_2d(\n",
    "    x_train[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws,\n",
    "    slopes,\n",
    "    intercepts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы исправить ситуацию, применим `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(np.expand_dims(x_train[:, 1], axis=1)).flatten()\n",
    "x_test_scaled = scaler.transform(np.expand_dims(x_test[:, 1], axis=1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts = np.arange(40, 60, 0.1)  # b\n",
    "slopes = np.arange(15, 35, 0.1)  # w\n",
    "\n",
    "plot_grid_search_2d(x_train_scaled, y_train, slopes, intercepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = np.hstack(\n",
    "    (np.ones((len(x_train_scaled), 1)), np.expand_dims(x_train_scaled, axis=1)),\n",
    ")\n",
    "\n",
    "x_test_scaled = np.hstack(\n",
    "    (np.ones((len(x_test_scaled), 1)), np.expand_dims(x_test_scaled, axis=1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. диапазоны $x$ изменились, значения $w$ и $b$ тоже изменятся.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[57.0], [33.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train_scaled, y_train, x_test_scaled, y_test, w, 0.35, iteration=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что после нормализации мы сходимся к $y = 9.68x + 2.83$.  Для этого используем данные о матожидании и дисперсии из `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ws[-1][0] - ws[-1][1] * scaler.mean_ / (scaler.var_) ** 0.5\n",
    "w = ws[-1][1] / (scaler.var_) ** 0.5\n",
    "\n",
    "print(f\"y = {w[0]:.2f}x + {b[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По визуализации видно, что $w$ и $b$ изменяются во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws,\n",
    "    slopes,\n",
    "    intercepts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cтохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ✏️ Пост о стохастическом градиентом спуске](https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter2_stochastic-gradient-descent.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы обучали модель, рассчитывая градиент по **всей train выборке**. Это не всегда возможно:\n",
    "- данных может быть слишком много, чтобы загрузить их в память одновременно и рассчитать градиент,\n",
    "- мы можем хотеть дообучать модель на свежепришедших данных, которых может быть немного.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому появляется идея **стохастического градиентного спуска**: мы можем делать шаг обучения, рассчитывая градиент не по всей выборке (**batch**), а по нескольким случайно выбранным объектам (**mini-batch**) или даже по одному случайно выбранному объекту (**stochastic**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/define_size_of_batch.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно [показать 📚[book]](https://academy.yandex.ru/handbook/ml/article/shodimost-sgd), что **стохастический** (с размером $\\text{batch}=1$) **градиентный спуск сходится к минимуму (глобальному или локальному) функции потерь** с конечной точностью. Важным условием является **стохастичность**. Если мы будем использовать одну и ту же последовательность выборок, это приведет к накоплению ошибки и смещению результата.\n",
    "\n",
    "Добавим создание подвыборки к нашему алгоритму:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    w,\n",
    "    alpha,\n",
    "    iteration=10,\n",
    "    batch_size=None,\n",
    "):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = np.zeros((iteration + 1, 2))\n",
    "    ws[0] = w[:, 0]\n",
    "    mse_train = [mean_squared_error(y_train, x_train @ w)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, x_test @ w)]\n",
    "    prediction = {(w[0][0], w[1][0]): x_train @ w}\n",
    "\n",
    "    print(\n",
    "        f\"Iteration 0: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "        f\"Loss_train = {mse_train[0]:.2f}, \"\n",
    "        f\"Loss_test = {mse_test[0]:.2f}.\"\n",
    "    )\n",
    "\n",
    "    for i in range(iteration):\n",
    "        if not batch_size:\n",
    "            x_sample = x_train\n",
    "            y_sample = y_train\n",
    "        else:\n",
    "            indxs = np.random.choice(x_train.shape[0], batch_size)\n",
    "            x_sample = x_train[indxs, :]\n",
    "            y_sample = y_train[indxs, :]\n",
    "\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_sample, y_sample, w)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws[i + 1] = w[:, 0]\n",
    "        mse_train.append(mean_squared_error(y_train, x_train @ w))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, x_test @ w))\n",
    "        prediction[(w[0][0], w[1][0])] = x_train @ w\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Iteration {i+1}: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "                f\"Loss_train = {mse_train[i]:.2f}, \"\n",
    "                f\"Loss_test = {mse_test[i]:3.2f}.\"\n",
    "            )\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сравнить результаты, будем использовать одни и те же количество итераций и скорость обучения. Чтобы компенсировать стохастичность, возьмем маленькое значение $\\alpha$ и $100$ итераций.\n",
    "\n",
    "Для всего train сета мы посчитаем градиент для $20\\cdot100 = 2000$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[57.0], [33.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = stochastic_gradient_descent(\n",
    "    x_train_scaled,\n",
    "    y_train,\n",
    "    x_test_scaled,\n",
    "    y_test,\n",
    "    w,\n",
    "    0.02,\n",
    "    iteration=100,\n",
    "    batch_size=None,\n",
    ")\n",
    "\n",
    "f1 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws,\n",
    "    slopes,\n",
    "    intercepts,\n",
    "    mode=\"lines\",\n",
    "    title=\"Batch gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для стохастического градиентного спуска (размер $\\text{batch}=1$) мы посчитаем градиент для $1\\cdot100 = 100$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([[57.0], [33.0]])\n",
    "ws_stohastic, prediction, mse_train, dmse_train, mse_test = stochastic_gradient_descent(\n",
    "    x_train_scaled,\n",
    "    y_train,\n",
    "    x_test_scaled,\n",
    "    y_test,\n",
    "    w,\n",
    "    0.02,\n",
    "    iteration=100,\n",
    "    batch_size=1,\n",
    ")\n",
    "f2 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws_stohastic,\n",
    "    slopes,\n",
    "    intercepts,\n",
    "    mode=\"lines\",\n",
    "    title=\"Stochastic gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для стохастического спуска с $\\text{mini-batch}=5$ мы посчитаем градиент для $5\\cdot100=500$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([[57.0], [33.0]])\n",
    "(\n",
    "    ws_mini_batch,\n",
    "    prediction,\n",
    "    mse_train,\n",
    "    dmse_train,\n",
    "    mse_test,\n",
    ") = stochastic_gradient_descent(\n",
    "    x_train_scaled,\n",
    "    y_train,\n",
    "    x_test_scaled,\n",
    "    y_test,\n",
    "    w,\n",
    "    0.02,\n",
    "    iteration=100,\n",
    "    batch_size=5,\n",
    ")\n",
    "f3 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws_mini_batch,\n",
    "    slopes,\n",
    "    intercepts,\n",
    "    mode=\"lines\",\n",
    "    title=\"Mini-batch gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ищем минимум для всех данных.\n",
    "- Градиент, рассчитанный по одному объекту, будет специфичен. Трек обучения  в случае стохастического градиентного спуска будет запутанным, а итоговая ошибка будет расти с увеличением скорости обучения (мы взяли низкую скорость).\n",
    "- Градиент, рассчитанный по нескольким объектам будет давать лучшую оценку градиента для всех данных. Трек будет менее сложным.\n",
    "- Градиент, рассчитанный по всей выборке, будет давать наиболее точное направление."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_panel\n",
    "\n",
    "plot_panel(f1, f2, f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для **ускорения расчетов** рекомендовано использовать **максимальный размер mini-batch**, который помещается в память, но это не всегда дает лучший результат. На 7 лекции вы увидите, что для сложных моделей стохастичность, связанная с небольшим размером батча, может помочь выбраться из локального минимума и найти более глубокий.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Условия применимости линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Квартет Энскомбе (Anscombe’s quartet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы применили модель линейной регрессии, посчитали метрики и решили что наша модель достаточно хороша. Но так ли это на самом деле?\n",
    "\n",
    "В 1973 году Фрэнсис Энскомбе тоже [задумался](https://www.jstor.org/stable/2682899) над этим вопросом. Он предложил 4 набора данных с одинаковыми статистиками, которые показали некоторые типичные ошибки, возникающие при решении регрессионных задач. Посмотрим на эти наборы данных и мы (для простоты тут нет разделения на train/test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала посчитаем сатистики набора данных, коэффиценты линейной модели и коэффициент детерминации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "df = sns.load_dataset(\"anscombe\")\n",
    "\n",
    "grouped = df.groupby('dataset')\n",
    "\n",
    "summary_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        'mean_x',\n",
    "        'mean_y',\n",
    "        'std_x',\n",
    "        'std_y',\n",
    "        'correlation',\n",
    "        'slope',\n",
    "        'offset',\n",
    "        'R2',\n",
    "        'MSE',\n",
    "        'MAE'\n",
    "    ]\n",
    ")\n",
    "\n",
    "for key in grouped.groups.keys():\n",
    "    data = grouped.get_group(key)\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(data['x'].values.reshape(-1, 1), data['y'].values)\n",
    "    slope = regressor.coef_[0]\n",
    "    offset = regressor.intercept_\n",
    "    y_pred = regressor.predict(data['x'].values.reshape(-1, 1))\n",
    "    r2 = r2_score(data['y'].values, y_pred)\n",
    "    mse = mean_squared_error(data['y'].values, y_pred)\n",
    "    mae = mean_absolute_error(data['y'].values, y_pred)\n",
    "    summary_results.loc[key] = (\n",
    "        grouped.mean().loc[key]['x'],\n",
    "        np.round(grouped.mean().loc[key]['y'], 2),\n",
    "        np.round(grouped.std().loc[key]['x'], 5),\n",
    "        np.round(grouped.std().loc[key]['y'], 2),\n",
    "        np.round(grouped.corr().loc[(key, 'x')]['y'], 3),\n",
    "        np.round(slope, 3),\n",
    "        np.round(offset, 2),\n",
    "        np.round(r2, 2),\n",
    "        np.round(mse, 2),\n",
    "        np.round(mae, 2)\n",
    "    )\n",
    "\n",
    "summary_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что по статистикам наборы данных почти идентичны. Посмотрим на сами данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", data=df,\n",
    "           col_wrap=2, ci=None, palette=\"muted\", height=4,\n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1})\n",
    "plt.gcf().suptitle(\"Anscombe's Quartet\", x=0.5, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что мы видим:\n",
    "- Набор данных “I” - подходит для линейной регрессии,\n",
    "- Набор данных “II” - визуализирует ошибку спецификации. Мы предположили, что $y$ линейно зависит от $x$, а на самом деле зависимость квадратичная.\n",
    "- В наборе данных “III” присутствует выброс, который приводит к смещенной оценке. Нужно разобраться с природой выброса: если это ошибка в данных - удалить, если это реальное значение - необходимо расширение выборки и применение более сложной модели.\n",
    "- Набор данных  “IV” - плохо подходит для регрессии: 9 из 10 значений $x$ совпадают и дают разные значения $y$, что противоречит предположению, что $y = f(x)$.\n",
    "\n",
    "Эти проблемы легко визуализировать и заметить, если у нас только 1 признак, но что делать когда признаков много?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Анализ остатков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут нам нужно познакомиться с [теоремой Гаусса-Маркова](https://ru.wikipedia.org/wiki/Теорема_Гаусса_—_Маркова), согласно которой метод наименьших квадратов дает оптимальную несмещенную оценку я $y$, если:\n",
    "* модель данных правильно специфицированна (зависимость действительно линейная), спецификация модели:\n",
    "$$y_i = b+w_1·x_{i1}+w_2⋅x_{i2}+...+w_k⋅x_{ik}+ϵ_i$$\n",
    "где $(X_i, y_i)$ - пары признаков и целевых значений $i\\in(1, 2, ..., n)$,\n",
    "$b$ и $w_i$ - параметры модели, $ϵ_i$ - шум в определении $y_i$\n",
    "* все выборки $X_i$ детерминированы и не все равны между собой,\n",
    "* признаки линейно независимы,\n",
    "* ошибки не носят систематического характера, а именно гомоскедастичность (нулевое матожидание и одинаковая дисперсия ошибок для всех $i$) и отсуствие автокорреляции.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В условиях этой много говориться об ошибках. Давайте визуализируем ошибки/невязки/остатки $\\delta y = y_{true}-y_{pred}$ для квартета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(9, 7), nrows=2, ncols=2)\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "for key, ax in zip(\n",
    "    grouped.groups.keys(),\n",
    "     [axs[0][0], axs[0][1], axs[1][0], axs[1][1]]\n",
    "):\n",
    "    data = grouped.get_group(key)\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(data['x'].values.reshape(-1, 1), data['y'].values)\n",
    "    y_pred = regressor.predict(data['x'].values.reshape(-1, 1))\n",
    "    ax.scatter(data['x'].values, data['y'].values-y_pred)\n",
    "    ax.axhline(y=0, color=\"red\")\n",
    "    ax.set_title(key)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('dy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что мы видим: остатки  набора “I” распределены случайно, в то время, как остатки набора “II”, “III” и “IV” - подчиняются некоторой системе. **Вывод: визуализируя остатки можно сделать вывод о применимости линейной регресси.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы посмотрели на корреляцию остатков, кроме нее есть ограничение на **гомоскедастичность** (нулевое матожидание и одинаковая дисперсия ошибок для всех $i$). Попробуем применить линейную регрессию к данным построенным по правилу $y=10+0.5x$, дисперсия ошибки будет расти с $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = np.linspace(start=0, stop=10, num=40)\n",
    "y_true = 10 + 0.5*x\n",
    "y = y_true + rng.normal(loc=0, scale=0.1 + 0.1*x, size=x.shape[0])\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x[:, np.newaxis], y)\n",
    "y_pred = regressor.predict(x[:, np.newaxis])\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(9, 4), nrows=1, ncols=2)\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "axs[0].scatter(x, y)\n",
    "axs[0].plot(x, y_pred, label='y_pred', color='red')\n",
    "axs[0].plot(x, y_true, label='y_true', color='green')\n",
    "axs[0].set_title(\"Regression\")\n",
    "axs[0].set_xlabel('x')\n",
    "axs[0].set_ylabel('y')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].scatter(x, y-y_pred)\n",
    "axs[1].axhline(y=0, color='red')\n",
    "axs[1].set_title(\"Residuals\")\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('dy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Видим, что предсказание не совсем корректное. Это связано с вкладом больших ошибок. В таком случае имеет смысл посмотреть в сторону [взвешенного метода наименьших квадратов](https://stackoverflow.com/questions/35236836/weighted-linear-regression-with-scikit-learn) или подумать над [стабилизирующим преобразованием](https://ajeyvenkataraman.wordpress.com/2020/03/04/dealing-with-heteroscedasticity-in-python/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее про анализ остатков можно почитать в [книге](https://ema.drwhy.ai/residualDiagnostic.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема корреляции признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто может оказаться, что признаковое описание объекта избыточно и между различными признаками имеются связи. Для устойчивости работы линейных моделей важно, чтобы среди признаков не было линейных связей (скоррелированных пар).\n",
    "\n",
    "Например, если мы будем решать задачу регрессии на наборе признаков $x_1 \\dots x_n$, среди которых есть связь $x_2 = 5 x_1$, и возьмём линейную модель вида\n",
    "$$\\large y = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b,$$\n",
    "то с учётом данной связи мы можем записать:\n",
    "$$\\large y = w_1 x_1 + w_2 (5x_1) + \\dots + w_n x_n + b = (w_1 + 5 w_2) x_1 +  w_3 x_3 + \\dots + w_n x_n + b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, наша модель теперь учитывает признак $x_1$ с одним \"общим\" весом $(w_1 + 5 w_2)$, несмотря на то, что он закодирован двумя независимыми параметрами. Решение, то есть набор весовых коэффициентов $w_i$, перестало быть единственным, так как мы теперь можем делать произвольные преобразования с числами $w_1$ и $w_2$ до тех пор, пока $(w_1 + 5 w_2)$ остаётся неизменным:\n",
    "\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000 ,\\, w_2 \\rightarrow  w_2 - 1000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000 ,\\, w_2 \\rightarrow  w_2 - 1000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000000 ,\\, w_2 \\rightarrow  w_2 - 1000000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000000000 ,\\, w_2 \\rightarrow  w_2 - 1000000000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + Nan ,\\, w_2 \\rightarrow  w_2 + Nan\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чем это плохо?**\n",
    "\n",
    "В случае корреляции признаков задача не имеет единственного решения и не существует обратной матрицы, обеспечивающей аналитическое решение. Мы можем использовать градиентные методы для поиска решения, но\n",
    "при этом веса модели могут неконтролируемо расти. При этом **суммарный вклад** признаков может быть **мал**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/carrelation_problem1.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем оценивать **важность признаков в линейной модели**, используя **веса** перед ними (признаки должны быть нормализованы). Чем больше модуль веса, тем больше вклад. Для коррелированных признаков важность будет переоценена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/carrelation_problem2.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, нужно помнить, что диапазоны числовых переменных ограничены. При неконтролируемом росте весов значение может выйти за диапазон и превратиться в ~~тыкву~~ `Nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/carrelation_problem3.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Что делать?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30-ти признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer()  # load data\n",
    "\n",
    "x = cancer.data  # features\n",
    "y = cancer.target  # labels(classes)\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "print(f\"x[0]: \\n {x[0]}\")\n",
    "print(f\"y[0]: \\n {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстрее и удобнее можно посмотреть на данные, используя pandas. К тому же, Colab добавил возможность визуализации данных (для этого можно тыкнуть синий значок диаграммы ▆ █ ▄  справа от таблицы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cancer_df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab делает не полную визуализацию признаков, но и на данных изображениях можно найти полезную **информацию о выбросах** (из графика **Values**), **плотности распределений** (из графика **Distributions**) и о наличии **зависимости между переменными** (из графика **2-d distributions**). Например, мы можем увидеть, что значения признаков *mean area* и *mean perimeter* имеют зависимость, близкую к линейной, что не очень хорошо (почему, обсудим позже).\n",
    "\n",
    "Кроме того, можно тыкнуть на рисунок, посмотреть и скопировать код визуализации, чтобы применить к другим данным или изменить под свои нужды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем линейные зависимости между признаками при помощи построения матрицы попарных корреляций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cancer_df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = cancer_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    vmax=1,\n",
    "    vmin=-1,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наших данных сильно скоррелированы признаки, характеризующие размеры опухоли (мы могли видеть это выше при визуализации 2-d distributions).\n",
    "\n",
    "В случае корреляции можно:\n",
    "- **добавить регуляризацию**;\n",
    "- **оставить один признак**;\n",
    "- если есть вероятность, что при удалении признаков часть информации будет потеряна, можно оставить **один признак** неизменным и вычесть его из остальных (оставить только **разницу**). Неинформативные шумовые признаки можно удалить (как это делать, вы узнаете на 4-й лекции).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###L2 vs L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем ограничить значения весов, введя в функционал ошибок\n",
    "специальную добавку, называемую регуляризацией. L2-регуляризация имеет формулу:\n",
    "\n",
    "\n",
    "$$\\large L_2 = \\alpha \\sum_i w_i^2,$$\n",
    "где $\\alpha$ — это коэффициент регуляризации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Введение **L2-регуляризации** приводит к тому, что **большие веса больше штрафуются** и предпочтение отдается решениям, использующим **малые значения весов**. При этом модель будет **сохранять скоррелированные и неважные признаки с маленькими весами**.\n",
    "\n",
    "Это связано с градиентом $L_2$:\n",
    "$$\\large L'_{2w_i} = 2\\alpha w_i$$\n",
    "Он будет “тянуть” модель в сторону большого количества маленьких весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/l1_and_l2_regularization.gif\" alt=\"alttext\" width=\"550\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отбора признаков можно использовать L1-регуляризацию, она одинаково \"штрафует\" модель за любые ненулевые веса.\n",
    "\n",
    "$$\\large L_1 = \\alpha \\sum_i |w_i|$$\n",
    "$$\\large L_{1w_i}' = \\alpha, \\text{  где } w_i\\neq 0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения интуиции, что L1-регуляризация позволяет отбирать признаки, обычно используют картинку ниже. На ней член функции потерь, отвечающий за регуляризацию, жестко ограничен: $|L_\\text{reg}|≤1$. Вращающиеся овалы показывают, как Loss решаемой задачи изменяется в результате изменения входных данных и таргета.\n",
    "\n",
    "Голубая область — ограничение на значения весов, которое дает регуляризация:\n",
    "- Для **L2** это **окружность**.  Оранжевая точка — это минимальное значение для функции Loss с регуляризацией. Для **L2** она будет кататься **по касательной к окружности**.\n",
    "- Для **L1** ограничения на значения весов будут иметь **форму ромба**. При этом минимальное значение для функции Loss с регуляризацией будет зависать в **уголу ромба**, что соответствует **обнулению веса** одного из признаков.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/loss_landscape_with_regularization.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://people.eecs.berkeley.edu/~jrs/189/\">Introduction to Machine Learning\n",
    "</a></em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МНК с регуляризацией реализован в обобщенных линейных моделях, таких как [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#ridge) (L2 регуляризация), [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#lasso) (L1 регуляризация), [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#elasticnet) (L1 и L2).\n",
    "\n",
    "Подробнее про обощенные линейный модели читайте [тут](https://scikit-learn.org/stable/modules/linear_model.html) и [тут](https://scikit-learn.org/stable/auto_examples/index.html#generalized-linear-models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем разобраться, что делать в случае классификации.\n",
    "\n",
    "Рассмотрим задачу классификации на два класса. Например, у нас есть данные о лабораторных мышах. Часть из них имеет нормальную массу тела, а часть — мыши с ожирением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/svm_mouse_example.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак:\n",
    "1. У нас есть набор данных из $N$ объектов (мышей).\n",
    "2. Для каждого из объектов (мышей) нам известно признаковое описание объекта в виде набора вещественных чисел (вес, длина от носа до кончика хвоста, возраст и т.д.). То есть объекту под номером $i$ соответствует вектор $\\vec x_i$.\n",
    "3. Также для каждого объекта нам известна истинная метка класса. Мы знаем, что объекту с признаковым описанием $\\vec x_i$ соответствует метка класса $y_i$. Будем считать, что метки классов принимают значения:\n",
    "$$y_i =\n",
    "\\begin{cases}\n",
    "    1, & \\text{для пухляшей}, \\\\\n",
    "    0, & \\text{для всех остальных}.\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим **одномерный пример**. У нас есть данные только по **массе мышей**. Часть из них определена как мыши с нормальной массой тела, а часть — как мыши с ожирением.\n",
    "\n",
    "Попробуем отделить их друг от друга с использованием линейной регрессии и посмотрим на остатки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "custom_cmap = ListedColormap([\"#B8E1EC\", \"#bea6ff\", \"#FEE7D0\"])\n",
    "\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    x = np.hstack(\n",
    "        [\n",
    "            np.random.uniform(14, 21, total_len // 2),\n",
    "            np.random.uniform(24, 33, total_len // 2),\n",
    "        ]\n",
    "    )\n",
    "    y = np.hstack([np.zeros(total_len // 2), np.ones(total_len // 2)])\n",
    "    return x, y\n",
    "\n",
    "total_len = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "x, y = generate_data(total_len=total_len)\n",
    "x = x[:,  np.newaxis]\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x, y)\n",
    "y_pred = regressor.predict(x)\n",
    "\n",
    "x_model = np.linspace(14, 33, 300)\n",
    "x_model = x_model[:,  np.newaxis]\n",
    "y_model = regressor.predict(x_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_1d_linear_model(x, y, y_pred, x_model, y_model, label):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    fig.tight_layout(pad=4.0)\n",
    "\n",
    "    axs[0].plot(\n",
    "        x_model,\n",
    "        y_model,\n",
    "        label=label,\n",
    "        linewidth=1,\n",
    "    )\n",
    "\n",
    "    axs[0].set(ylabel=\"y\", xlabel=\"x\", title=label)\n",
    "    sns.scatterplot(x=x[:, 0], y=y, hue=y, s=50, ax=axs[0])\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    axs[0].legend(handles, [\"Model\", \"Normal\", \"Obese\"])\n",
    "\n",
    "\n",
    "    sns.scatterplot(x=x[:, 0], y=y-y_pred, hue=y, s=50, ax=axs[1])\n",
    "    axs[1].axhline(y=0, color='red')\n",
    "    axs[1].set(ylabel=\"dy\", xlabel=\"x\", title=\"Residuals\")\n",
    "    handles, labels = axs[1].get_legend_handles_labels()\n",
    "    axs[1].legend(handles, [\"Normal\", \"Obese\"])\n",
    "    plt.show()\n",
    "\n",
    "visual_1d_linear_model(x, y, y_pred, x_model, y_model, \"Linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остатки сильно скоррелированны. Мы можем внести разделяющее правило\n",
    "$$y_{pred}>0.5, \\text{class} = 1$$\n",
    "$$y_{pred}\\le0.5, \\text{class} = 0$$\n",
    "\n",
    "и считать $Loss=-\\text{accurecy}(y_{true}, y_{pred})$, но такое условие не диффернцируемо. Что делать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к вероятностям\n",
    "\n",
    "Давайте попробуем модифицировать простую линейную модель таким образом, чтобы мы могли трактовать выходы модели как вероятность. Начнем с задачи бинарной классификации. Линейная модель задается уравнением:\n",
    "$$s = f(\\vec x) = (\\vec{w}, \\vec{x}) + b$$\n",
    "\n",
    "Выходы такой модели принимают значения от $-∞$ до $+∞$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Нам бы хотелось получить на выходе вероятность того, что объект принадлежит к классу $1$. Вероятность принимает значения от $0$ до $1$. Нам нужна функция, которая спроецирует диапазон $(-∞,+∞)$ в диапазон $[0, 1]$.\n",
    "\n",
    "Такой функцией является сигмоида:\n",
    "$$p = \\sigma(s) = \\frac{1}{1+e^{-s}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При $s→-∞$: $p→0$.\n",
    "\n",
    "При $s→+∞$: $p→1$.\n",
    "\n",
    "При $s=0$: $p=0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/scores_to_probability.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения $s$ (score) также называют [logit-ом 📚[wiki]](https://en.wikipedia.org/wiki/Logit) (пер. “логарифм”). Это связано с тем, что если выразить logit $s$ через вероятность $p$, то получится формула:\n",
    "$$s(p) = \\log \\left(\\frac{p}{1-p}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно решить, какую функцию потерь использовать. **Вероятности** принимают значения **от $0$ до $1$**. Если мы будем использовать **MAE**, [Hinge loss 📚[wiki]](https://en.wikipedia.org/wiki/Hinge_loss) или **MSE**, **максимальным значением ошибки** на указанном диапазоне будет **$1$**. А нам хочется **максимально штрафовать** модель, если она выдает для правильного класса вероятность 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **Cross-Entropy loss** (или ее иногда называют log loss) в том, что мы хотим притянуть вероятность правильного класса к 1 и бесконечно штрафовать модель если вероятность истинного класса 0.\n",
    "\n",
    "Для этого можно использовать функцию логорифма. Она гладкая, и для нее: $\\log(1) = 0$, $\\log(0) = -∞$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/cross_entropy_plot_loss_with_probability.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы можем записать функцию ошибок, как:\n",
    "\n",
    "$$\\text{Loss} = - \\log(p^y_\\text{pred}),$$\n",
    "\n",
    "где $p^y_\\text{pred}$ — предсказанная вероятность для истиного класса. Минус нужен, т.к. $-\\log(0) = ∞$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель с сигмоидой и Cross-Entropy loss (log loss) называется логистической регрессией. Посмотрим, как логистическая регрессия справляется с задачей классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x, y)\n",
    "\n",
    "y_pred = classifier.predict(x)\n",
    "y_model = classifier.predict_proba(x_model)\n",
    "\n",
    "visual_1d_linear_model(x, y, y_pred, x_model, y_model[:, 1], \"Logistic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многоклассовая классификация\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу классификации на примере картинок, например, для датасета CIFAR-10 (он встретится вам в задании). У нас есть входное изображение, и мы хотим получить на выходе 10 чисел, обозначающих уверенность модели в принадлежности изображения к конкретному классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/img_to_function_get_scores.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем считать по logit-у для каждого класса:\n",
    "$$\\begin{matrix} s_1 = (\\vec{w_1}, \\vec{x}) + b_1\\\\ s_2 = (\\vec{w_2}, \\vec{x}) + b_2\\\\ ... \\\\ s_n = (\\vec{w_n}, \\vec{x}) + b_n \\end{matrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также мы можем записать формулы выше в виде матричного перемножения. Для этого нужно преобразовать матрицу пикселей в вектор признаков (просто вытянув массив в вектор), а строки с весами модели — в матрицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/scalar_product_add_bias.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае для непересекающихся классов результатом классификации будет класс, соответствующий наибольшему logit-у.\n",
    "\n",
    "В дальнейшем мы будем проходить нейронные сети, которые работают очень похоже.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[video] 📺 Объяснение SoftMax от StatQuest](https://www.youtube.com/watch?v=KpKog-L9veg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что наша модель выдала следующие значения logit-ов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "logits = [[5.1,    # cat\n",
    "           3.2,    # car\n",
    "          -1.7]]   # frog\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, чтобы узнать, какой класс наша сеть предсказала, мы могли бы просто взять `argmax` от наших `logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Predicted class = %i (Cat)\" % (np.argmax(logits, axis=1).squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От argmax нельзя посчитать градиент, так как производная от константы равна 0. Соответственно, если бы мы вставили производную от argmax в градиентный спуск, мы бы получили везде нули, и наша модель бы вообще ничему не научилась."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, мы бы хотели получить не logit’ы, а настоящую вероятность $p$ на выходе модели. Да еще и таким образом, чтобы от наших вероятностей можно было посчитать градиент. Для этого мы можем применить к нашим логитам функцию **SoftMax**:\n",
    "$$\\large p(y=k|x=x_i) = \\frac{e^{s_k(x_i)}}{\\sum_{j=1}^ne^{s_j(x_i)}},$$\n",
    "\n",
    "где $x_i$ — набор признаков, характеризующий один объект из выборки,\n",
    "$s_j(x_i)$ — logit для j-го класса для объекта $x_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Вероятности всегда неотрицательны**. Отобразим наши logit’ы на значения $[0, +∞)$.\n",
    "\n",
    "> Для этого возведем **экспоненту** (число Эйлера $e=2.71828$) **в степень логита**. В результате мы получим вектор гарантированно неотрицательных чисел (положительное число, возведенное в степень, даже отрицательную, даст положительное значение).\n",
    "\n",
    "2. Классы не пересекаются, **сумма вероятностей** по всем классам **равна единице**.\n",
    "\n",
    ">  Мы должны их **нормализовать**, то есть поделить на сумму.\n",
    "\n",
    "Это преобразование называется **SoftMax функцией**. **Получаются вероятности**, то есть числа, которые можно интерпретировать, как вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/linear_classifier_softmax.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\text{SoftMax}_\\text{кошка} = \\frac{e^{5.1}}{e^{5.1} + e^{3.2} + e^{-1.7}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "print(softmax(logits))\n",
    "print(\"Sum = %.2f\" % np.sum(softmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что SoftMax никоим образом не поменял порядок значений. Самому большому logit'у соответствует самая большая вероятность, а самому маленькому, соответственно, самая маленькая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Практическое вычисление SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вычислении экспоненты от выходов модели могут получиться очень большие числа в силу очень высокой скорости роста экспоненты. Этот факт необходимо учитывать, чтобы вычисления SoftMax были численно стабильны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "f = np.array([[123, 456, 789]])\n",
    "p = softmax(f)\n",
    "print(f\"logits = {f},\\nprobabilities = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы регуляризовать вычисление, нам следует предварительно упростить возникающую в вычислении дробь. Для этого мы можем вычесть из каждого $s_i$ положительную константу, чтобы уменьшить значения экспонент. В качестве константы можно выбрать максимальный элемент этого вектора, тогда у нас гарантированно не будет очень больших чисел, и такой способ будет работать более стабильно.\n",
    "\n",
    "$$\\large M = \\max_j s_{j}(x_i),$$\n",
    "\n",
    "$$\\large s^\\text{new}_{j}(x_i)  = s_{j}(x_i) - M,$$\n",
    "\n",
    "$$\\large \\dfrac {e^{s^\\text{new}_{k}(x_i)}} {\\sum_j e^{s^{new}_{j}(x_i)}}  = \\dfrac {e^{s_{k}(x_i) - M }} {\\sum_j e^{s_{j}(x_i) - M }} = \\dfrac {e^{s_{k}(x_i)}e ^ {-M}} {\\sum_j e^{s_{j}(x_i)} e ^ {-M}} = \\dfrac {e ^ {-M} e^{s_{k}(x_i)}} {e ^ {-M} \\sum_j e^{s_{j}(x_i)} } = \\dfrac { e^{s_{k}(x_i)}} { \\sum_j e^{s_{j}(x_i)} },$$\n",
    "\n",
    "где $x_i$ — набор признаков, характеризующий один объект из выборки,\n",
    "$s_j(x_i)$ — logit для $j$-го класса для объекта $x_i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "f = np.array([[123, 456, 789]])\n",
    "\n",
    "p = softmax(f)\n",
    "print(f\"new logits = {f},\\nprobabilities = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая последовательность преобразований будет выглядеть так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/linear_model_probability_pipeline.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно решить, какую функцию потерь использовать. **Вероятности** принимают значения **от $0$ до $1$**. Если мы будем использовать **MAE**, [Hinge loss 📚[wiki]](https://en.wikipedia.org/wiki/Hinge_loss) или **MSE**, **максимальным значением ошибки** на указанном диапазоне будет **$1$**. А нам хочется **максимально штрафовать** модель, если она выдает для правильного класса вероятность 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **Cross-Entropy loss** (или ее иногда называют log loss) в том, что мы хотим притянуть вероятность правильного класса к 1 и бесконечно штрафовать модель если вероятность истинного класса 0.\n",
    "\n",
    "Для этого можно использовать функцию логорифма. Она гладкая, и для нее: $\\log(1) = 0$, $\\log(0) = -∞$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/cross_entropy_plot_loss_with_probability.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы можем записать функцию ошибок, как:\n",
    "\n",
    "$$\\text{Loss} = - \\log(p^y_\\text{pred}),$$\n",
    "\n",
    "где $p^y_\\text{pred}$ — предсказанная вероятность для истиного класса. Минус нужен, т.к. $-\\log(0) = ∞$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к задаче классификации картинок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/logits_to_scores_to_probabilitys.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На картинке на входе был изображен кот. Мы можем преобразовать метки классов следующим образом:\n",
    "\n",
    "$$ y → p_\\text{true} $$\n",
    "\n",
    "$$ p_\\text{true_i}= \\begin{cases} 1 & \\text{для i=k, где k - номер истинного класса}, \\\\0 & \\text{для любого } i\\neq k.\n",
    " \\end{cases}$$\n",
    "Таким образом,\n",
    "$$ y → p_\\text{true} = [[1, 0, 0]]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае функция ошибки выше может быть записанна как [кросс-энтропия 📚[wiki]](https://en.wikipedia.org/wiki/Cross-entropy):\n",
    "$$\\text{Loss} = - \\log(p^y_\\text{true})= H(p_\\text{pred}||p_\\text{true})= - \\sum^C_{i=1}p_\\text{true_i}\\cdot \\log(p_\\text{pred_i}) = -1⋅\\log{0.87}-0\\cdot\\log{0.13}-0\\cdot\\log{0.001} = - \\log(p_\\text{cat}) = - \\log{0.87} \\approx0.14$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем это в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(pred_prob, true_prob):\n",
    "    # warning: this code does not take into account the possibility that\n",
    "    # pred_prob could be equal to zero: add epsilon = 1e-9 to fix it\n",
    "    return np.sum(-true_prob * np.log(pred_prob)) / pred_prob.shape[0]\n",
    "\n",
    "\n",
    "# 3 classes 2 items\n",
    "# fmt: off\n",
    "logits = np.array([\n",
    "    [5.1, 3.2, -1.7], # one item\n",
    "  # [2.1, 6.3,  1.5],  # second item\n",
    "])\n",
    "# fmt: on\n",
    "\n",
    "print(f\"Logits = \\n{logits}\\n\")\n",
    "\n",
    "pred_prob = softmax(logits)\n",
    "print(f\"Predicted Probabilities = \\n{pred_prob}\\n\")\n",
    "\n",
    "# 3 classes 2 items\n",
    "# fmt: off\n",
    "true_prob = np.array([\n",
    "    [1.0, 0.0, 0.0],  # one item\n",
    "  # [0.0, 1.0, 0.0],  # second item\n",
    "])\n",
    "# fmt: on\n",
    "\n",
    "print(f\"True Probabilities = \\n{true_prob}\\n\")\n",
    "\n",
    "print(f\"Cross-entropy loss = {cross_entropy_loss(pred_prob, true_prob):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Математический смысл\n",
    "\n",
    "**Кросс-энтропия** имеет глубокий математический смысл в теории информации и статистике. Она связана с [энтропией 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F) $H(P)$ и  [расстоянием Кульбака — Лейблера 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0) $D_{KL}(P||Q)$:\n",
    "\n",
    "\n",
    "$$H(P||Q) = D_{KL}(P||Q) + H(P)$$\n",
    "\n",
    "\n",
    "Подробнее об этом можно почитать тут:\n",
    "* [[colab] 🥨 Cross-entropy](https://colab.research.google.com/drive/1DFhT24njhb4LA2g6iJtl2diH-YPtD2Z4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи бинарной **классификации** или **multi-label классификации** (одному объекту может соответствовать несколько классов) используют модификацию cross-entropy, которую называют **binary cross-entropy**:\n",
    "\n",
    "$$\\text{Loss} =  - \\sum^C_{i=1}(p_\\text{true_i}\\cdot \\log(p_\\text{pred_i}) + (1-p_\\text{true_i})\\cdot \\log(1-p_\\text{pred_i}))$$\n",
    "\n",
    "В этом случае порог вероятности 0.5 может быть не оптимальным для определения класса. Подробнее про выбор оптимального порога рекомендуем почитать:\n",
    "* [[blog] ✏️ A Gentle Introduction to Threshold-Moving for Imbalanced Classification](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиент функции потерь. Кросс-энтропия\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая последовательность преобразований будет выглядеть так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/linear_model_probability_pipeline.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ✏️ Cross-Entropy Loss](https://wandb.ai/wandb_fc/russian/reports/---VmlldzoxNDI4NjAw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем производную от функции ошибок. Функция ошибок:\n",
    "\n",
    "$$ L = - \\sum_i p_\\text{true_i} \\log p_\\text{pred_i} = -\\sum_i p_\\text{true_i} \\log(\\dfrac {e^{s_i}} {\\sum_j e^{s_j}}),$$\n",
    "\n",
    "где $s_i$ — это логиты классов, получаемые из линейной модели:\n",
    "\n",
    "$$s_i = w_i x$$\n",
    "\n",
    "Для расчета градиента будем использовать chain rule:\n",
    "\n",
    "$$ \\dfrac {\\partial L} {\\partial w_i} = \\dfrac {\\partial L} {\\partial s_i} \\dfrac {\\partial s_i} {\\partial w_i} $$\n",
    "\n",
    "Градиент логитов по весам:\n",
    "\n",
    "$$\\dfrac {\\partial s_i} {\\partial w_i} = x$$\n",
    "\n",
    "У нас только одна истинная метка класса $p_\\text{true_k} = 1$, для $i\\neq k$ $p_\\text{true_k}=0$\n",
    "\n",
    "$$ L = -1⋅ \\log p_\\text{pred_k} = - \\log(\\dfrac {e^{s_k}} {\\sum_j e^{s_{j}}})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас два случая:\n",
    "1. **Производная по истинному логиту** (значение логита входит в числитель и знаменатель дроби).\n",
    "2. **Производная по остальным логитам** (значение логита входит только в знаменатель дроби).\n",
    "\n",
    "Начнем с истинного логита:\n",
    "1. Производная по $s_{k}$. Вынесем минус, чтобы не потерять:\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{k}}} {\\sum_j e^{s_{j}}}) = \\log e^{s_{k}} - \\log  \\sum_j e^{s_{j}}  = s_{k} - \\log  \\sum_j e^{s_{j}}$$\n",
    "\n",
    "$$\\dfrac {\\partial -L} {\\partial s_{k}} = 1 - \\dfrac 1 {\\sum_j e^{s_{j}}} \\cdot \\dfrac {\\partial {\\sum_j e^{s_{j}}}} {\\partial s_{k}} = 1 - \\dfrac 1 {\\sum_j e^{s_{j}}} \\cdot \\dfrac {\\partial e^{s_{k}}} {\\partial s_{k}} = 1 - \\dfrac {e^{s_{k}}} {\\sum_j e^{s_{j}}} = 1 - p_k$$\n",
    "\n",
    "Вспомним про минус:\n",
    "\n",
    "$$\\dfrac {\\partial L} {\\partial s_{j}} = p_k - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Для остальных логитов $i \\neq k$. Вынесем минус, чтобы не потерять:\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{i}}} {\\sum_j e^{s_{j}}}) = \\log e^{s_{k}} - \\log  \\sum_j e^{s_{j}}  = s_{k} - \\log  \\sum_j e^{s_{j}}$$\n",
    "\n",
    "$$\\dfrac {\\partial -L} {\\partial s_{i}} = - \\dfrac 1 {\\sum_j e^{s_{j}}} \\cdot \\dfrac {\\partial {\\sum_j e^{s_{j}}}} {\\partial s_{i}} =  \\dfrac 1 {\\sum_j e^{s_{i}}} \\cdot \\dfrac {\\partial e^{s_{i}}} {\\partial s_{i}} = \\dfrac {e^{s_{i}}} {\\sum_j e^{s_{j}}} = - p_i$$\n",
    "\n",
    "Вспомним про минус:\n",
    "\n",
    "$$\\dfrac {\\partial L} {\\partial s_{j}} = p_i $$\n",
    "\n",
    "Получаем:\n",
    "$$ \\dfrac {\\partial L} {\\partial s_{i}}  =  \\begin{cases} p_i - 1 & \\text{для i=k, где k - номер истинного класса}, \\\\p_i & \\text{для любого } i\\neq k.\n",
    " \\end{cases} $$\n",
    "\n",
    "Применим **chain rule**:\n",
    "\n",
    "$$ \\dfrac {\\partial L} {\\partial w_i}  = \\dfrac {\\partial L} {\\partial s_{i}} \\dfrac {\\partial s_{i}} {\\partial w_i}  =  \\begin{cases}(p_i - 1)x & \\text{для i=k, где k - номер истинного класса}, \\\\p_ix & \\text{для любого } i\\neq k.\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде это будет выглядеть вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input batch of 2 vector with 4 elements\n",
    "# fmt: off\n",
    "x = np.array([[1,  2, 3, 4],\n",
    "              [1, -2, 0, 0]])\n",
    "# fmt: on\n",
    "\n",
    "# Weights\n",
    "W = np.random.randn(3, 4)  # 3 class\n",
    "\n",
    "# model output\n",
    "logits = x.dot(W.T)\n",
    "print(\"Scores(Logits) \\n\", logits, \"\\n\")\n",
    "\n",
    "# Probabilities\n",
    "probs = softmax(logits)  # defined before\n",
    "print(\"Probs \\n\", probs, \"\\n\")\n",
    "\n",
    "# Ground true classes\n",
    "y = [0, 1]\n",
    "\n",
    "# Derivative\n",
    "dl_ds = probs.copy()\n",
    "dl_ds[np.arange(len(y)), y] += -1  # substract one from true class prob\n",
    "dW = x.T.dot(dl_ds)  # dot product with input\n",
    "\n",
    "print(\"Grads dL/dW \\n\", dW)  # have same shape as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основанный на расстоянии подход в задаче линейной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем разобраться, что делать в случае классификации.\n",
    "\n",
    "Рассмотрим задачу классификации на два класса. Например, у нас есть данные о лабораторных мышах. Часть из них имеет нормальную массу тела, а часть — мыши с ожирением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/svm_mouse_example.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак:\n",
    "1. У нас есть набор данных из $N$ объектов (мышей).\n",
    "2. Для каждого из объектов (мышей) нам известно признаковое описание объекта в виде набора вещественных чисел (вес, длина от носа до кончика хвоста, возраст и т.д.). То есть объекту под номером $i$ соответствует вектор $\\vec x_i$.\n",
    "3. Также для каждого объекта нам известна истинная метка класса. Мы знаем, что объекту с признаковым описанием $\\vec x_i$ соответствует метка класса $y_i$. Будем считать, что метки классов принимают значения:\n",
    "$$y_i =\n",
    "\\begin{cases}\n",
    "    +1, & \\text{для пухляшей}, \\\\\n",
    "    -1, & \\text{для всех остальных}.\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим **одномерный пример**. У нас есть данные только по **массе мышей**. Часть из них определена как мыши с нормальной массой тела, а часть — как мыши с ожирением.\n",
    "\n",
    "Чтобы их отделить друг от друга, нам достаточно одного критерия. Мы можем посмотреть на график и визуально определить предельную массу, после которой мыши считаются мышами с ожирением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "custom_cmap = ListedColormap([\"#B8E1EC\", \"#bea6ff\", \"#FEE7D0\"])\n",
    "\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    x = np.hstack(\n",
    "        [\n",
    "            np.random.uniform(14, 21, total_len // 2),\n",
    "            np.random.uniform(24, 33, total_len // 2),\n",
    "        ]\n",
    "    )\n",
    "    y = np.hstack([np.zeros(total_len // 2), np.ones(total_len // 2)])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data_1d(\n",
    "    x,\n",
    "    y,\n",
    "    total_len=40,\n",
    "    s=50,\n",
    "    threshold=None,\n",
    "    margin=None,\n",
    "    legend=[\"Normal\", \"Obese\"],\n",
    "    marker=\"o\",\n",
    "):\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s, marker=marker)\n",
    "    if threshold:\n",
    "        x_lim, y_lim = ax.get_xlim(), ax.get_ylim()\n",
    "        XX, YY = np.meshgrid(\n",
    "            np.linspace(x_lim[0], x_lim[1], 100), np.linspace(y_lim[0], y_lim[1], 100)\n",
    "        )\n",
    "        pred = np.sign(XX - threshold)\n",
    "        plt.contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "        ax.axvline(threshold, color=\"grey\")\n",
    "    if margin:\n",
    "        for line in margin:\n",
    "            ax.axvline(line, color=\"grey\", ls=\"dashed\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, legend)\n",
    "    ax.set(xlabel=\"Mass, g\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "np.random.seed(42)\n",
    "x, y = generate_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data_1d(x, y, threshold=21.5, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, пользуясь нашим простым критерием, попробуем классифицировать каких-то новых (тестовых) мышей $\\color{orange}{✭}$ $\\color{blue}{✭}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.random.uniform(14, 30, 5)\n",
    "\n",
    "\n",
    "def classify(x, threshold=21.5):\n",
    "    y = np.zeros_like(x)\n",
    "    y[x > threshold] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "threshold = 21.5\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data_1d(x, y, threshold=threshold, total_len=total_len)\n",
    "ax = plot_data_1d(\n",
    "    x_test, classify(x_test, threshold), total_len=total_len, s=500, marker=\"*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из тестовых мышей была классифицирована как мышь с ожирением ($\\color{orange}{✭}$ на границе), хотя она ближе по массе к мышам без ожирения из обучающей выборки $\\color{blue}{●}$. Не порядок!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вооружившись этим новым знанием, попробуем классифицировать мышек по-умному. Возьмем крайние точки в каждом кластере. И в качестве порогового значения будем использовать среднее между ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_limit = x[y == 0].max()  # extreme point for 'normal'\n",
    "obese_limit = x[y == 1].min()  # extreme point for 'obese'\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit])  # separated with mean value\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data_1d(\n",
    "    x, y, total_len=total_len, threshold=threshold, margin=[normal_limit, obese_limit]\n",
    ")\n",
    "ax = plot_data_1d(\n",
    "    x_test,\n",
    "    classify(x_test, threshold=threshold),\n",
    "    total_len=total_len,\n",
    "    s=500,\n",
    "    marker=\"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать, насколько наша мышь близка к тому, чтобы оказаться в другом классе. Такое расстояние называется **margin**. И оно считается как $\\mathrm{margin} = |\\mathrm{threshold} - \\mathrm{observation}|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = np.abs(x_test - threshold)\n",
    "print(margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если мы посчитаем margins для наших крайних точек `normal_limit` и `obese_limit`, мы найдем самое большое возможное значение margin для нашего классификатора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_0 = np.abs(normal_limit - threshold)\n",
    "margin_1 = np.abs(obese_limit - threshold)\n",
    "print(margin_0, margin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой классификатор мы называем **Maximum Margin Classifier**. Он хорошо работает в случае, когда классы не пересекаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример, где мы измерили не только вес мышей, но и их длину от хвоста до носа. Теперь не очевидно, какие точки кластеров у нас являются крайними и как провести разделяющую прямую, чтобы классы были максимально разнесены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "def generate_2d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42)\n",
    "    x[:, 0] += 10\n",
    "    x[:, 1] += 20\n",
    "    return x, y\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_2d_data(total_len=total_len)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, [\"Normal\", \"Obese\"])\n",
    "ax.set(xlabel=\"Mass, g\", ylabel=\"Length, cm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: метод множителей Лагранжа\n",
    "\n",
    "**Идея:** Мы умеем решать задачу классификации в одномерном случае. В многомерном случае попробуем свести задачу к одномерной. Для этого мы будем подбирать вектор весов и смещение чтобы точки преобразовывались, как на картинке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/svm_hard_margin.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобрать такое преобразование в многомерной задаче — значит провести **разделяющую гиперплоскость** так, чтобы:\n",
    "1. **Плюсы и минусы** лежали **по разные** стороны от этой плоскости.\n",
    "2. **Ближайшие** к плоскости **объекты** были от нее как можно **дальше**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Гиперплоскость однозначно задается вектором нормали $\\vec w$ и смещением $b$.** Мы ищем решение в виде $(\\vec w, \\vec x) + b$, где $(\\vec w, \\vec x)$ — это скалярное произведение, $\\vec w$ — вектор весов, а $b$ — смещение. Скалярное произведение вектора признаков на вектор нормали будет давать проекцию вектора признаков на вектор нормали (переход к 1d задаче)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектора нормали могут быть разной длины. Мы можем нормализовать вектор нормали (сделать длину вектора равной 1). Вместо этого в SVM (support vector machine) принято **фиксировать margin от −1 до 1** (это позволяет удобно записать слагаемое, отвечающее за максимальное удаление объектов от гиперплоскости в качестве слагаемого loss функции).\n",
    "\n",
    "Вы можете заметить, что **для построения зазора важны не все объекты**, а ограниченное количество объектов, чьи проекции попадают в точки 1 и −1. Такие объекты называются **опорными векторами (support vector)** т.к. в них будет упираться зор между классами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого:\n",
    "1. Мы хотим подобрать такие $\\vec  w$ и $b$, чтобы можно было провести такие гиперплоскости:\n",
    "$$\\large (\\vec w, \\vec x) + b  = 1$$\n",
    "\n",
    "- **Лежащие на этой плоскости и выше объекты относятся к классу $+1$:** $\\large (\\vec w, \\vec x_+) + b  \\ge 1$\n",
    "\n",
    "$$\\large (\\vec w, \\vec x) + b  = -1$$\n",
    "\n",
    "- **Лежащие на этой плоскости и ниже объекты относятся к классу $-1$:** $\\large (\\vec w, \\vec x_-) + b  \\le -1$\n",
    "\n",
    "    Условие того, что  $i$-й объект лежит по правильную сторону от разделяющих поверхностей, можно записать в совместное условие:\n",
    "\n",
    "    $$\\large y_i ((\\vec w, \\vec x_i) + b )\\ge 1,$$\n",
    "\n",
    "    которое должно выполняться для всех объектов $1 \\le i \\le N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Мы хотим разнести эти плоскости как можно дальше. Расстояние между двумя этими **жесткими** границами можно расписать через проекции **опорных** (лежащих на плоскости) **векторов** $\\vec x_{sv+}$ (дает проекцию в точку $1$) и $\\vec x_{sv1}$ (дает проекцию в точку $-1$):  \n",
    "\n",
    "$$\\text{margin} =\\frac{\\vec w}{||\\vec w||}(\\vec x_{sv+} - \\vec x_{sv-}) =\\frac{(\\vec w,\\vec x_{sv+})+b-((\\vec w,\\vec x_{sv-})+b)}{||\\vec w||} = \\frac{1-(-1)}{||\\vec w||} = \\frac{2}{||\\vec w||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы приходим к следующей задаче на экстремум:\n",
    "\n",
    "$$\\max \\frac{2}{||\\vec w||} \\Rightarrow \\min \\frac{1}{2} ||\\vec w||^2$$\n",
    "\n",
    "при условии:\n",
    "$$\\large y_i ((\\vec w, \\vec x_i) + b )\\ge 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача на условный экстремум для линейно разделимых классов может быть решена аналитически при помощи [метода множителей Лагранжа 📚[wiki]](https://en.wikipedia.org/wiki/Lagrange_multiplier) (позволяет преобразовать задачу условной оптимизации в задачу безусловной оптимизации).\n",
    "\n",
    "Найти $\\alpha_i$, $\\vec w$ и $b$, которые реализуют минимум функции потерь:\n",
    "\n",
    "$$\\large L =  \\frac{1}{2} ||\\vec w||^2 + \\sum_i \\alpha_i [y_i ((\\vec w, \\vec x_i) + b) - 1]$$\n",
    "\n",
    "$\\alpha_i\\geq0$ — множитель Лагранжа. Он будет не равен нулю только для **опорных векторов**.\n",
    "\n",
    "Подробный вывод этой формулы:\n",
    "* [[video] 📺 MIT: Support Vector Machines](https://www.youtube.com/watch?v=_PwhiWxHK8o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке Sklearn минимизация функции $L$ реализована через алгоритм Sequential Minimal Optimization, [предложенный Джоном Платтом в 1998 году 🎓[article]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf). Он работает очень быстро, потому что разбивает задачу на множество подзадач, решаемых аналитически (подробности в статье).\n",
    "\n",
    "Применим к мышкам метод `svm` из библиотеки Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Code for illustration, later we will understand how it works\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel=\"linear\", C=1000)\n",
    "clf.fit(x, y)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# first fig\n",
    "sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=axs[0])\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].legend(handles, [\"Normal\", \"Obese\"])\n",
    "axs[0].set(xlabel=\"Mass, g\", ylabel=\"Length, cm\")\n",
    "\n",
    "# plot the decision function\n",
    "delta = 0.5\n",
    "# create grid to evaluate model\n",
    "YY, XX = np.meshgrid(\n",
    "    np.linspace(x[:, 1].min() - delta, x[:, 1].max() + delta, 30),\n",
    "    np.linspace(x[:, 0].min() - delta, x[:, 0].max() + delta, 30),\n",
    ")\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "pred = np.sign(Z)\n",
    "axs[0].contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "# plot decision boget_xlimundary and margins\n",
    "axs[0].contour(\n",
    "    XX, YY, Z, colors=\"k\", levels=[-1, 0, 1], alpha=0.5, linestyles=[\"--\", \"-\", \"--\"]\n",
    ")\n",
    "# plot support vectors\n",
    "axs[0].scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=100,\n",
    "    linewidth=1,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "\n",
    "# second fig\n",
    "dec_val = clf.decision_function(x)\n",
    "sns.scatterplot(x=dec_val, y=np.zeros(len(x)), hue=y, ax=axs[1])\n",
    "\n",
    "x_lim, y_lim = axs[1].get_xlim(), axs[1].get_ylim()\n",
    "XX, YY = np.meshgrid(\n",
    "    np.linspace(x_lim[0], x_lim[1], 100), np.linspace(y_lim[0], y_lim[1], 100)\n",
    ")\n",
    "pred = np.sign(XX)\n",
    "axs[1].contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "axs[1].axvline(0, color=\"grey\")\n",
    "axs[1].axvline(-1, color=\"grey\", ls=\"dashed\")\n",
    "axs[1].axvline(1, color=\"grey\", ls=\"dashed\")\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(handles, [\"Normal\", \"Obese\"])\n",
    "axs[1].set(xlabel=\"wx+b\")\n",
    "\n",
    "sv = clf.decision_function(clf.support_vectors_)\n",
    "axs[1].scatter(\n",
    "    sv, np.zeros_like(sv), s=100, linewidth=1, facecolors=\"none\", edgecolors=\"k\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативным подходом к решению SVM задачи является Hinge loss с идеей которого можно ознакомиться по ссылке:\n",
    "* [[colab] 🥨 Hinge loss и accuracy](https://colab.research.google.com/drive/10STVnRISuC7DCJQ3x5XJgKG8gYTRqrSh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы добавим еще одно измерение — возраст, мы обнаружим, что наши данные стали трехмерными, а разделяет их теперь не линия, а плоскость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42, n_features=3)\n",
    "    x[:, 0] += 10\n",
    "    x[:, 1] += 20\n",
    "    x[:, 2] += 10\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50, threshold=21.5):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(\n",
    "        xs=x[:, 0], ys=x[:, 1], zs=x[:, 2], c=y, s=s, cmap=\"tab10\", vmin=0, vmax=9\n",
    "    )\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    ax.plot_surface(XX, YY, XX * YY * 0.2, alpha=0.2)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.set(xlabel=\"Mass, g\", ylabel=\"Length, cm\", zlabel=\"Age, days\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_3d_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если бы у нас было 4 измерения и больше (например: вес, длина, возраст, кровяное давление), то многомерная плоскость, которая бы разделяла наши классы, называлась бы **гиперплоскость** (рисовать мы ее, конечно же, не будем). Чисто технически, и точка, и линия — тоже гиперплоскости. Но все же гиперплоскостью принято называть то, что нельзя нарисовать на бумаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение задачи SVM, которое мы рассматривали, касалось задачи бинарной классификации. Однако мы часто будем работать с несколькими классами.\n",
    "\n",
    "Есть две основные стратегии расширения задачи SVM классификации с двух классов на несколько:\n",
    "* **One vs rest** (один против всех): каждый класс отделяется от всех других одной прямой (гиперплоскостью).\n",
    "* **One vs one** (один против одного): классы попарно отделяются друг от друга прямыми (гиперплоскостями)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датасет из 4 классов для демонстрации отличий между этими способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[1, 1], [1, -1], [-1, -1], [-1, 1]]\n",
    "\n",
    "x, y = make_blobs(n_samples=300, centers=centers, cluster_std=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_colors = [\"#1B1464\", \"#0961A5\", \"#754C24\", \"#006837\"]\n",
    "bright_colors = [\"#5D5DA6\", \"#2DA9E1\", \"#F9B041\", \"#4AAE4D\"]\n",
    "dull_cmap = ListedColormap([\"#D1D5ED\", \"#B8E1EC\", \"#FEE7D0\", \"#C9E3C8\"])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "# first fig\n",
    "sns.scatterplot(\n",
    "    x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=ax, palette=sns.color_palette(bright_colors)\n",
    ")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, [\"0\", \"1\", \"2\", \"3\"])\n",
    "ax.set(xlabel=\"feature 1\", ylabel=\"feature 2\")\n",
    "\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-2.5, 2.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs Rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация **one vs rest** реализована в Sklearn в классе `svm.LinearSVC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.LinearSVC(dual=\"auto\")\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядят разделяющие прямые и нормали к ним для нашей задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    x,\n",
    "    response_method=\"predict\",\n",
    "    cmap=dull_cmap,\n",
    "    alpha=0.8,\n",
    "    xlabel=\"feature 1\",\n",
    "    ylabel=\"feature 2\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "sns.scatterplot(\n",
    "    x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=ax, palette=sns.color_palette(bright_colors)\n",
    ")\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(-2.5, 2.5)\n",
    "# for visualization\n",
    "arrow_xs = [0.5, 0.5, -0.5, -0.5]\n",
    "for i in range(clf.coef_.shape[0]):\n",
    "    coef = clf.coef_[i]\n",
    "    w = -coef[0] / coef[1]\n",
    "    b = -clf.intercept_[0] / coef[1]\n",
    "    yy = w * xx + b\n",
    "    # normal\n",
    "    plt.arrow(\n",
    "        arrow_xs[i],\n",
    "        w * arrow_xs[i] + b,\n",
    "        coef[0] / 4,\n",
    "        coef[1] / 4,\n",
    "        edgecolor=dark_colors[i],\n",
    "        facecolor=bright_colors[i],\n",
    "        width=0.04,\n",
    "    )\n",
    "    # dividing line\n",
    "    plt.plot(xx, yy, dark_colors[i])\n",
    "\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-2.5, 2.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициенты `clf.coef_` возвращают вектор нормали. С помощью `clf.coef_` и `clf.intercept_` можно записать уравнение разделяющей прямой.\n",
    "\n",
    "Для 4 классов стратегия **one vs rest** даст 4 разделяющие прямые (гиперплоскости). Количество разделяющих прямых равно количеству классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия **one vs rest** позволяет обучать меньшее количество классификаторов, чем **one vs one**, но при **большом количестве классов** могут появляться проблемы, связанные с **сильным дисбалансом классов** при решении задачи “один против всех”. При большом количестве классов лучше использовать **one vs one** стратегию.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй стратегией многоклассовой классификации для SVM является **one vs one**, в которой классы разделяются попарно. Эта стратегия реализована в классе `svm.SVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    x,\n",
    "    response_method=\"predict\",\n",
    "    cmap=dull_cmap,\n",
    "    alpha=0.8,\n",
    "    xlabel=\"feature 1\",\n",
    "    ylabel=\"feature 2\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "sns.scatterplot(\n",
    "    x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=ax, palette=sns.color_palette(bright_colors)\n",
    ")\n",
    "\n",
    "# for visualization\n",
    "arrow_xs = [1, -0.1, 0, -0.17, -0.17, -1]\n",
    "colors_list = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "range_list = [(0, 2.5), (-0.3, 0.1), (-0.1, 0.5), (-1, -0.12), (-0.4, 0), (-2.5, 0)]\n",
    "\n",
    "for i in range(clf.coef_.shape[0]):\n",
    "    xx = np.linspace(*range_list[i])\n",
    "    coef = clf.coef_[i]\n",
    "    w = -coef[0] / coef[1]\n",
    "    b = -clf.intercept_[0] / coef[1]\n",
    "    yy = w * xx + b\n",
    "    # normal\n",
    "    plt.arrow(\n",
    "        arrow_xs[i],\n",
    "        w * arrow_xs[i] + b,\n",
    "        coef[0] / 4,\n",
    "        coef[1] / 4,\n",
    "        edgecolor=dark_colors[colors_list[i][0]],\n",
    "        facecolor=bright_colors[colors_list[i][0]],\n",
    "        width=0.04,\n",
    "    )\n",
    "    # dividing line\n",
    "    plt.plot(xx, yy, dark_colors[colors_list[i][1]])\n",
    "\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-2.5, 2.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для 4 классов стратегия **one vs one** даст 6 разделяющих прямых (гиперплоскости). Количество разделяющих прямыx:\n",
    "$$ \\frac{n_\\text{classes}\\cdot (n_\\text{classes}-1)}{2},$$\n",
    "гдe $n_\\text{classes}$ — количество классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Практические советы по использованию SVM:**\n",
    "\n",
    "* SVM делает **геометрическое разделение данных**, поэтому для адекватной работы модели важна **нормализация**.\n",
    "* В случае **дисбаланса классов** полезно использовать параметры `class_weight` и `sample_weight` ([подробнее 🛠️[doc]](https://scikit-learn.org/stable/modules/svm.html#unbalanced-problems)).\n",
    "* SVM может давать хорошее решение при небольшом количестве данных, в этом случае стоит попробовать **различные ядра** (про ядра вы узнаете ниже)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обобщенные линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полиномиальная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные не всегда могут быть **хорошо разделены (гипер)плоскостью**. Например, рассмотрим следующее: у нас есть данные по дозировке лекарства и 2 класса — пациенты, которые поправились, и те, которым лучше не стало."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def generate_patients_data(total_len=40):\n",
    "    x = np.random.uniform(0, 50, total_len)\n",
    "    y = np.zeros_like(x)\n",
    "    y[(x > 15) & (x < 35)] = 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Sick\", \"Recover\"])\n",
    "    ax.set(xlabel=\"dose, mg\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, мы не можем найти такое пороговое значение, которое будет разделять наши классы на больных и здоровых, а, следовательно, и Support Vector Classifier работать тоже не будет.  Для начала давайте преобразуем наши данные таким образом, чтобы они стали 2-хмерными. В качестве значений по оси Y будем использовать дозу, возведенную в квадрат (**доза**$^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    ax = sns.scatterplot(x=x[0, :], y=x[1, :], hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Sick\", \"Recover\"])\n",
    "    ax.set(xlabel=\"Dose, mg\")\n",
    "    ax.set(ylabel=\"Dose$^2$\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x_1, y = generate_patients_data(total_len=total_len)\n",
    "x_2 = x_1**2\n",
    "x = np.vstack([x_1, x_2])\n",
    "\n",
    "plot_data(x, y, total_len=40, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем вновь использовать Support Vector Classifier для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x, y, total_len=40, s=50)\n",
    "\n",
    "x_arr = np.linspace(0, 50, 50)\n",
    "xs = [x[0, :][y == 1].min(), x[0, :][y == 1].max()]\n",
    "ys = [x[1, :][y == 1].min(), x[1, :][y == 1].max()]\n",
    "\n",
    "# Calculate the coefficients.\n",
    "coefficients = np.polyfit(xs, ys, 1)\n",
    "\n",
    "# Let's compute the values of the line...\n",
    "polynomial = np.poly1d(coefficients)\n",
    "y_axis = polynomial(x_arr)\n",
    "\n",
    "# ...and plot the points and the line\n",
    "plt.plot(x_arr, y_axis, \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея **Kernel SVM** состоит в том, что **мы можем перейти в пространство большей размерности, в котором данные будут линейно разделимы**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут возникает резонный вопрос: **почему мы решили возвести в квадрат**? Почему не в куб? Или, наоборот, не извлечь корень? Как нам решить, какое преобразование использовать?\n",
    "\n",
    "И у нас есть вторая проблема — а если перейти надо в **пространство очень большой размерности**? В этом случае наши данные очень сильно **увеличатся в размере**.\n",
    "\n",
    "Комбинация двух проблем дает нам много сложности: надо **перебирать большое число возможных пространств большей размерности**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Обоснование Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако основная фишка **Support Vector Machine** состоит в том, что внутри он работает на скалярных произведениях. И можно эти **скалярные произведения** считать, **не переходя в пространство большей размерности**.\n",
    "\n",
    "Для этого SVM использует **Kernel Function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/svm_kernel_function.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы ввели $\\text{Loss}$ для **Hard Margin Classifier**:\n",
    "\n",
    "$$\\large \\text{Loss} =  \\frac{1}{2} ||\\vec w||^2 + \\sum_i \\alpha_i [y_i ((\\vec w, \\vec x_i) + b) - 1],$$\n",
    "\n",
    "где $\\alpha_i\\geq0$ — множитель Лагранжа. Он будет не равен нулю только для **опорных векторов**.\n",
    "\n",
    "С добавлением некоторых математических ограничений эту формулу можно [переписать ✏️[blog]](https://www.geeksforgeeks.org/dual-support-vector-machine/) в **дуальной форме**:\n",
    "\n",
    "$$\\large \\text{Loss} =  \\sum_i \\alpha_i+ \\sum_i \\sum_j \\alpha_i \\alpha_j\n",
    "y_i y_j (\\vec x_i, \\vec x_j) = \\sum_i \\alpha_i+ \\sum_i \\sum_j \\alpha_i \\alpha_j\n",
    "y_i y_j K(\\vec x_i, \\vec x_j),$$\n",
    "где $(\\vec x_i, \\vec x_j)$ — скалярное произведение.\n",
    "\n",
    "Для получения дуальной формы приравнивают нулю производные $\\dfrac {\\partial \\text{Loss}} {\\partial w}$ и $\\displaystyle\\frac {\\partial \\text{Loss}}{\\partial b}$ и подставляют их в исходную формулу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой формуле можно сделать **kernel trick** — заменить **скалярное произведение** на некоторую функцию от двух векторов, которую мы будем называть **Kernel function**.\n",
    "\n",
    "Решающее правило в таком случае будет выглядеть как:\n",
    "\n",
    "$$\\large y=f(x)=\\text{sign}(\\sum_{i=1}^nα_iy_iK(x, x_i)+b),$$\n",
    "\n",
    "где $x_i$ и $y_i$ — признаки и метки классов опорных векторов.\n",
    "\n",
    "Важно заметить, что  **дуальная форма** записи для **многоклассовой классификации** возможна только в случае **one vs one**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры ядер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для демонстрации возможностей Kernel SVM создадим датасет, который не разделяется линейными моделями. Для этого воспользуемся функцией `sklearn.datasets.make_circles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "x, y = make_circles(n_samples=500, factor=0.3, noise=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный датасет представляет собой две окружности с разными радиусами и общим центром, относящиеся к разным классам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=x[:, 0],\n",
    "    y=x[:, 1],\n",
    "    hue=y,\n",
    "    s=50,\n",
    "    ax=ax,\n",
    "    palette=sns.color_palette([\"#2DA9E1\", \"#F9B041\"]),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию визуализации разделяющего правила для SVM модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "def plot_svm(x, y, clf):\n",
    "    dull_cmap = ListedColormap([\"#B8E1EC\", \"#FEE7D0\"])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        x,\n",
    "        response_method=\"predict\",\n",
    "        cmap=dull_cmap,\n",
    "        alpha=0.8,\n",
    "        xlabel=\"feature 1\",\n",
    "        ylabel=\"feature 2\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=x[:, 0],\n",
    "        y=x[:, 1],\n",
    "        hue=y,\n",
    "        s=50,\n",
    "        ax=ax,\n",
    "        palette=sns.color_palette([\"#2DA9E1\", \"#F9B041\"]),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое ядро, которое мы рассмотрим, линейное. Оно задается формулой:\n",
    "$$\\large K(\\vec x_i, \\vec x_j) = (\\vec x_i, \\vec x_j)$$\n",
    "\n",
    "Линейным ядром является скалярное произведение векторов.\n",
    "\n",
    "Линейное ядро не способно справиться с такой задачей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее ядро, реализованное в библиотеке Sklearn — полиномиальное, оно задается формулой:\n",
    "$$K(\\vec x_i, \\vec x_j) = (\\gamma (\\vec x_i, \\vec x_j)+r)^d,$$\n",
    "где $d$ — настраиваемый параметр: степень полинома `degree`.\n",
    "\n",
    "Попробуем применить полиномиальное ядро к нашим данным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"poly\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полиномиальное ядро действует не совсем как полиномиальная модель.\n",
    "У модели не получилось разделить данные. Это связано с тем, что значение `degree` по умолчанию равно 3, поставим степень полинома 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"poly\", degree=2)\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод, который тут стоит сделать: для получения **оптимального результата** бывает полезным **настроить параметры ядра** с учетом даных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым  популярным ядром SVM является ядро радиальных базисных функций RBF, или гауссово ядро. Оно получено из гауссова  распределения, а гауссово распределение характерно для большого количества измеряемых величин. Данное ядро задается формулой:\n",
    "\n",
    "$$\\large K(\\vec x_i, \\vec x_j) = e^{-\\gamma{||\\vec x_i - \\vec x_j||^2}}$$\n",
    "\n",
    "Настраиваемыми параметрами модели являются `C` и `gamma`. `C` определяет степень гладкости поверхности принятия решений: чем больше `C`, тем сложнее поверхность и **выше вероятность переобучения** (про переобучение поговорим ниже), `gamma` определяет степень влияния одного обучающего примера на разделяющее правило ([подробнее 🛠️[doc]](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel)).\n",
    "\n",
    "SVM может проверять пространства признаков бесконечного размера, если для такого пространства существует kernel function. RBF ядро как раз [соответствует 📚[book]](https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf) такому случаю бесконечномерного пространства признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"rbf\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в Sklearn реализовано `sigma` ядро. Оно интересно больше с исторической точки зрения, т.к. эквивалентно модели нейрона — Перцептрону, о котором вы узнаете на 5-й лекции. [На практике 🎓[article]](https://home.work.caltech.edu/~htlin/publication/doc/tanh.pdf) оно в большинстве случаев проигрывает RBF ядру.\n",
    "\n",
    "$$\\large K(\\vec x_i, \\vec x_j) = \\tanh (\\gamma(\\vec x_i, \\vec x_j)+r)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Наивный Байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно построить модель классификатора, которая будет напрямую оценивать вероятность принадлежности объекта к интересующему нас классу просто на основе информации о распределении объектов по классам в обучающей выборке. Базовую идею такого примера легко продемонстрировать на следующих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример на табличных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим `DataFrame` датасета [Wine 🛠️[doc]](https://archive.ics.uci.edu/ml/datasets/wine), который являлся примером табличных данных в первой лекции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine\n",
    "\n",
    "# Download dataset\n",
    "features, class_labels = load_wine(\n",
    "    return_X_y=True, as_frame=True\n",
    ")  # also we can get data in Bunch (dictionary) or pandas DataFrame\n",
    "\n",
    "wine_dataset = features\n",
    "wine_dataset[\"target\"] = class_labels\n",
    "\n",
    "wine_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет содержит объекты 3 различных классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём первый признак `alcohol`. По имеющийся таблице с данными легко построить функцию распределения $f(x)$, которая будет задавать вероятность $p(\\text{alcohol} = x)$, и найти среди наших данных бутылку вина с параметром `alcohol`, равным $x$ (слева на графике ниже).\n",
    "\n",
    "Т.к. у нас три класса, мы можем построить распределение объектов в обучающей выборке по признаку `alcohol` отдельно для каждого из этих трёх классов. Эти распределения зададут нам условную вероятность $p(\\text{alcohol} = x |\\text{target} = i)$ того, что объект имеет значение признака `alcohol`, равное $x$, при условии, что он относится к одному из классов с номером $i$ (справа на графике ниже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.kdeplot(wine_dataset, x=\"alcohol\", fill=True, ax=axes[0])\n",
    "axes[0].set_title(\"p(alcohol=x)\")\n",
    "\n",
    "sns.kdeplot(\n",
    "    wine_dataset,\n",
    "    x=\"alcohol\",\n",
    "    hue=\"target\",\n",
    "    palette=sns.color_palette([\"#5D5DA6\", \"#2DA9E1\", \"#F9B041\"]),\n",
    "    fill=True,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"p(alcohol=x|target=i)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрев на плотности распределений по классам (график справа), мы можем предположить, что бутылка с значением $\\text{alcohol} = 11.3$ будет относиться к 1 классу.\n",
    "\n",
    "На языке формул наш “метод пристального вглядывания” можно записать с помощью формулы для условной вероятности по [теореме Байеса 📚[wiki]](https://en.wikipedia.org/wiki/Bayes'_theorem):\n",
    "\n",
    "$$\\large p(\\text{target} = i | \\text{alcohol} = x) = \\frac{p(\\text{alcohol} = x | \\text{target} = i )p(\\text{target} = i )}{p(\\text{alcohol} = x)},$$\n",
    "\n",
    "где $p(\\text{target} = i)$ — доля объектов класса $i$ в датасете, а $p(\\text{target} = i | \\text{alcohol} = x)$ — вероятность того, что объект принадлежит классу $i$, при условии того, что признак `alcohol` у него принимает значение $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы поняли, откуда в названии метода **Байес**. Теперь разберемся, **почему он “наивный”**.\n",
    "\n",
    "Мы использовали только один признак: `alcohol`. Всего же у нас 13 признаков.\n",
    "\n",
    "$$\\large p(\\text{target} = i |\\text{features} = \\vec x ) = \\frac{p(\\text{features} = \\vec x | \\text{target} = i )p(\\text{target} = i )}{p(\\text{features} = \\vec x)}$$\n",
    "\n",
    "**“Наивность”** Байеса состоит в том, что эта модель будет рассматривать признаки как **независимые случайные величины**:\n",
    "$$\\large p(\\text{features} = \\vec x)=p(\\text{feature}_1 = x_1) \\cdot p(\\text{feature}_2 = x_2)...p(\\text{feature}_n = x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы решаем задачу классификации на $k$ классов, то для объекта с набором признаком $\\vec x$ по формуле выше мы получим $k$ чисел, характеризующих вероятность принадлежности данного объекта к различным классам. Для финального принятия решения нам останется выбрать тот класс, для которого вероятность принадлежности наивысшая:\n",
    "\n",
    "$$\\large \\text{prediction} = \\underset{i}{\\text{argmax}}{\\left(p(\\text{target} = i |\\text{features} = \\vec x )\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к нашему датасету Wine и попробуем решить задачу классификации для него при помощи предложенного алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обычно, разделим наш датасет на тренировочную и валидационную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features.values, class_labels.values, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём реализацию Наивного Байесовского классификатора `GaussianNB` из [библиотеки Sklearn 🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB). `GaussianNB` **использует оценку распределения признаков с помощью** [Гауссового распределения 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%9D%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5). Обучим её на тренировочном датасете и измерим качество на отложенной валидационной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the model\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Calculate F1_score\n",
    "pred = model.predict(x_test)\n",
    "f1_score(y_test, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря простоте модельного датасета Wine наша наивная статистическая модель показала отличное качество работы. Это связано с “простотой” датасета — признаки классов имеют унимодальные распределения (один пик на плотности распределения), для более сложных данных (многомодальные распределения) такого не будет.\n",
    "\n",
    "Тем не менее, подход к решению задачи классификации, связанный с построением модели предсказания принадлежности объекта к имеющимся классам, оказался конструктивным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея применения наивного Байесовского классификатора в NLP задаче:\n",
    "* [[colab] 🥨 Naive Bayes NLP](https://colab.research.google.com/drive/1JUMs9TBWxlCTsqw2T-y3oxMe2WZxLYtJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практические особенности работы с линейными моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже обсудили, зачем нужна нормализация данных для линейной модели. В данном разделе мы обсудим виды нормализации более подробно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30-ти признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer()  # load data\n",
    "\n",
    "x = cancer.data  # features\n",
    "y = cancer.target  # labels(classes)\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "print(f\"x[0]: \\n {x[0]}\")\n",
    "print(f\"y[0]: \\n {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк, в каждой из которой по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные диапазоны  значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.boxenplot(\n",
    "    data=cancer_df,\n",
    "    orient=\"h\",\n",
    "    palette=\"Set2\",\n",
    "    linewidth=0.4,\n",
    "    flier_kws={\"marker\": \"o\", \"s\": 3},\n",
    "    line_kws={\"linewidth\": 1},\n",
    ")\n",
    "ax.set(xscale=\"log\", xlim=(1e-4, 1e4), xlabel=\"Values\", ylabel=\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная модель представляет собой **сумму взвешенных признаков**. Если мы приведем признаки к **единому масштабу**, мы сможем **оценить их вклад в модель** по значениям весов. Кроме того, работать с признаками в одном диапазоне вычислительно удобно. Для этого будем использовать нормализацию.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализацией** называется процедура приведения входных данных к **общим значениям математических статистик**.\n",
    "\n",
    "Нормализация строит **взаимно однозначное соответствие** между некоторыми размерными величинами (которые измеряются в метрах, килограммах, годах и т. п.) и их безразмерными аналогами. Исходные значения **можно восстановить**, зная статистики оригинальных данных и правило, по которому делалась нормализация.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто используют следующие варианты нормализации:  **`MinMaxScaler`**, **`StandardScaler`**, **`RobustScaler`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные из имеющегося диапазона значений в диапазон от $0$ до $1$. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (например, масштабирование RGB пикселей)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large z_i=\\frac{X_i-X_{\\min}}{X_{\\max}-X_{\\min}},$$\n",
    "\n",
    "где $z_i$ — масштабированное значение, $X_i$ — текущее значение, $X_{\\min}$ и $X_{\\max}$ — минимальное и максимальное значения имеющихся данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение $0$ и стандартное отклонение $1$. Большинство значений будет находиться в диапазоне от $-1$ до $1$. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large z_i=\\frac{X_i-u}{s},$$\n",
    "\n",
    "где $u$ — среднее значение (или $0$ при `with_mean=False`), $s$ — стандартное отклонение (или $0$ при `with_std=False`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И `StandardScaler`, и `MinMaxScaler` чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. $k$-й процентиль — это величина, равная или не превосходящая $k$ процентов чисел во всём имеющемся распределении. Например, 50-й процентиль (медиана) распределения таков, что 50% чисел из распределения не меньше данного числа.\n",
    "\n",
    "Соответственно, `RobustScaler` не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large z_i=\\frac{X_i-X_\\text{median}}{IQR},$$\n",
    "\n",
    "где $X_\\text{median}$ — значение медианы, $IQR$ — межквартильный диапазон, равный разнице между 75-ым и 25-ым процентилями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для случайного набора признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "random_names = random.sample(list(cancer.feature_names), 8)\n",
    "cut_df = cancer_df[random_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "\n",
    "def plot_norm(df, ax, title):\n",
    "    sns.boxenplot(\n",
    "        df,\n",
    "        orient=\"h\",\n",
    "        palette=\"Set2\",\n",
    "        ax=ax,\n",
    "        linewidth=0.2,\n",
    "        flier_kws={\"marker\": \"o\", \"s\": 5},\n",
    "        line_kws={\"linewidth\": 1},\n",
    "    )\n",
    "    ax.set(xlabel=\"Values\", title=title)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7))\n",
    "\n",
    "plot_norm(cut_df, axs[0][0], \"Original\")\n",
    "axs[0][0].set(xscale=\"log\", xlim=(1e-4, 1e4))\n",
    "min_max_x = MinMaxScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(min_max_x, columns=random_names), axs[0][1], \"MinMax\")\n",
    "\n",
    "std_x = StandardScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(std_x, columns=random_names), axs[1][0], \"Standard\")\n",
    "\n",
    "rob_x = RobustScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(rob_x, columns=random_names), axs[1][1], \"Robust\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.55, hspace=0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед выбором нормализации **важно разобраться с природой выбросов**. Для этого нужно посмотреть на выбросы с точки зрения эксперта и попробовать определить, являются ли выбросы ошибкой при сборе данных или редкими случаями, которые необходимо сохранить.\n",
    "\n",
    "Мы не являемся экспертами в медицине и мало знаем о данных, поэтому будем считать, что наши признаки имеют распределение, близкое к нормальному. Поэтому мы будем использовать `StandardScaler`. `StandardScaler` часто используют как нормировку по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = StandardScaler().fit_transform(cancer_df)  # scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что они стали намного более сравнимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_norm, columns=cancer.feature_names).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.boxenplot(\n",
    "    data=pd.DataFrame(x_norm, columns=cancer.feature_names),\n",
    "    orient=\"h\",\n",
    "    palette=\"Set2\",\n",
    "    linewidth=0.4,\n",
    "    flier_kws={\"marker\": \"o\", \"s\": 3},\n",
    "    line_kws={\"linewidth\": 1},\n",
    ")\n",
    "ax.set(xlabel=\"Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Борьба с переобучением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Сложность модели__ (*model complexity*) — важный гиперпараметр. В частности, для линейных моделей сложность может быть представлена **количеством параметров**, например, для полиномиальных моделей — степенью полинома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложность модели связана с **ошибкой обобщения** (_generalization error_):\n",
    "- **Cлишком простой модели** не будет хватать **количества параметров для обобщения сложной закономерности в данных**, что приведёт к большой ошибке обобщения.\n",
    "- **Избыточная сложность модели** также приводит к большой ошибке обобщения за счет того, что в силу своей сложности модель начинает **пытаться искать закономерности в шуме**, добиваясь большей точности на тренировочных данных, теряя при этом часть обобщающей способности.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/model_complexity.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры модели задают некоторую **аппроксимацию целевой функции**. Аппроксимировать целевую функцию можно несколькими способами, например:\n",
    "1. Использовать все имеющиеся данные и провести ее строго **через все точки**, которые нам известны ($f1$ на картинке);\n",
    "2. Использовать более простую функцию (в данном случае, линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым **общим закономерностям**, которые у них есть ($f2$ на картинке).\n",
    "\n",
    "Характерной чертой переобучения является первый сценарий, и сопровождается он, как правило, **большими весами**. Поэтому регуляризацию используют для борьбы с переобучением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/l2_regularization.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проиллюстрируем описанное явление на примере полиномиальной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2 * np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "x_true = np.linspace(0, 2 * np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\", label=\"noisy data\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем аппроксимировать имеющуюся зависимость с помощью полиномиальной модели, используя шумные данные в качестве тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train = x.reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i, degree in enumerate([0, 1, 3, 9]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "    fig.add_subplot(2, 2, i + 1)\n",
    "    plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "    plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "    plt.plot(x_true, y_true, c=\"lime\")\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель **переобучается, подстраиваясь под тренировочную выборку**. В полиноме степень/количество весов — это гиперпараметр, который можно подбирать на кросс-валидации, но **ограничивая количество параметров**, мы накладываем **ограничение на обобщающую способность модели** в целом. Вместо этого можно оставить модель сложной, но использовать **регуляризатор**, который будет заставлять модель отдавать предпочтение более простому обобщению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(9), LinearRegression())\n",
    "model_ridge = make_pipeline(PolynomialFeatures(9), Ridge(alpha=0.1))\n",
    "\n",
    "model.fit(x_train, y)\n",
    "y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "model_ridge.fit(x_train, y)\n",
    "y_plot_ridge = model_ridge.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "plt.plot(x_true, y_plot_ridge, c=\"black\", label=f\"M={degree}, alpha=0.1\")\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "poly_coef = model[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"Without regularization: \", eq)\n",
    "\n",
    "poly_coef = model_ridge[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"With regularization: \", eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что одним из \"симптомов\" переобучения являются аномально большие веса. Модель Ridge Regression, показанная в примере выше, использует L2-регуляризацию для борьбы с этим явлением.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Литература</font>\n",
    "\n",
    "<font size=\"5\">Линейная регрессия:</font>\n",
    "\n",
    "* [[book] 📚 Метрики классификации и регрессии](https://academy.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii)\n",
    "\n",
    "<font size=\"5\">Метод градиентного спуска:</font>\n",
    "* [[book] 📚 Градиентный спуск](https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter1_gradient-descent.html)\n",
    "* [[book] 📚 Стохастический градиентный спуск](https://tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter2_stochastic-gradient-descent.html)\n",
    "\n",
    "<font size=\"5\">SVM:</font>\n",
    "* [[video] 📺 Хорошее объяснение SVM](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "* [[book] 📚 SVM на стэнфордском курсе ](https://cs231n.github.io/linear-classify/#svm)\n",
    "\n",
    "<font size=\"5\">Обобщенные линейные модели:</font>\n",
    "* [[blog] ✏️ Дуальная форма SVM](https://www.geeksforgeeks.org/dual-support-vector-machine/)\n",
    "* [[git] 🐾 Продвинутый алгоритм, основанный на SVM](https://github.com/IvanoLauriola/MKLpy)\n",
    "\n",
    "<font size=\"5\">Вероятностный подход в задаче классификации:</font>\n",
    "* [[video] 📺 Naive Bayes in NLP](https://www.youtube.com/watch?v=O2L2Uv9pdDA)\n",
    "* Latent Dirichlet Allocation:\n",
    " * [[video] 📺 Latent Dirichlet Allocation (Part 1 of 2)](https://www.youtube.com/watch?v=T05t-SqKArY),\n",
    " * [[video] 📺 Training Latent Dirichlet Allocation: Gibbs Sampling (Part 2 of 2)](https://www.youtube.com/watch?v=BaM1uiCpj_E).\n",
    "* [[video] 📺 Объяснение SoftMax от StatQuest](https://www.youtube.com/watch?v=KpKog-L9veg)\n",
    "* [[video] 📺 Объяснение KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
