{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Линейные модели</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы познакомимся с линейными моделями. Это - простые модели, которые являются частью многих сложных моделей, которые мы будем рассказывать на нашем курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе линейных моделей лежит линейная функция\n",
    "$$f(\\vec x) = (\\vec w, \\vec x) +b $$\n",
    "где $(\\vec w, \\vec x)$  - скалярное произведение:\n",
    "\n",
    "$$(\\vec w, \\vec x) = \\sum_{i=1}^n{w_ix_i} = w_1x_1+w_2x_2+...+w_ix_i+ ... + w_nx_n$$\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/scalar_product_ways_to_use.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дальнейшем мы будем проходить **модель нейрона головного мозга**, которая ляжет в основу всех **нейронных сетей** от полносвязных и сверточных сетей, до сложных генеративных моделей и трансформеров. В ней мы встретимся с **линейным слоем**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регрессия** — это одна из трех базовых задач машинного обучения (классификация, регрессия, кластеризация).\n",
    "\n",
    "В задаче **регрессии** мы используем входные **признаки**, чтобы предсказать **целевые значения**. Например, чтобы предсказать цену жилья по его характеристикам (площадь, этаж, год постройки дома, высота потолков, район, ...). **Линейная регрессия** сводится к тому, чтобы провести “**линию наилучшего соответствия**” через набор точек данных. Она является простым предшественником нелинейных методов, которые используются для обучения нейронных сетей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Модель и ее параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель линейной регрессии — **поиск линии, которая наилучшим образом соответствует заданным точкам**. Напомним, что общее уравнение для прямой есть\n",
    "\n",
    "$$f(x) = w⋅x + b,$$\n",
    "\n",
    "где $w$ — характеризует наклон линии (в будущем мы будем называть значения $w$ весом, weight) а $b$ — её сдвиг по $y$ (bias). Таким образом, решение линейной регрессии определяет значения для $w$ и $b$ так, что $f (x)$ приближается как можно ближе к $y$. $w$ и $b$ — **параметры модели**.\n",
    "\n",
    "\n",
    "Отобразим на графике случайные точки, расположенные в окрестности $y = 3⋅x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 + 3 * x + (np.random.rand(100, 1) - 0.5)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что нам неизвестны параметры наклона и сдвига $w$ и $b$. Для их определения мы бы могли рассмотреть всевозможные прямые вида $f(x) = w⋅x + b$ и выбрать среди семейства прямых такую, которая лучше всего приближает имеющиеся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in np.arange(-5.0, 7.0, 1):\n",
    "    for b in [-1, 0, 1, 2, 3]:\n",
    "        y_predicted = b + w * x\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель** $f(x) = w⋅x + b$ задаёт параметрическое семейство функций, а **выбор \"правильного\" представителя** из **параметрического семейства** и называется **обучением** модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in np.arange(-5.0, 7.0, 1):\n",
    "    for b in [-1, 0, 1, 2, 3]:\n",
    "        y_predicted = b + w * x\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.plot(x, 2 + 3 * x, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь\n",
    "\n",
    "Как выбрать параметры?\n",
    "\n",
    "**Функция потерь**  позволяет вычислить меру количества ошибок. Для задачи **регрессии** такой мерой может быть **расстояние** между предсказанным значением $f(х)$ и его фактическим значением. Распространенной функцией потерь является **средняя квадратичная ошибка** (MSE). Чтобы вычислить MSE, мы просто берем все значения ошибок, считаем квадраты их длин и усредняем.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы определяем ошибку модели на одном объекте как квадрат расстояния между предсказанием и истинным значением, а общая функция потерь будет задана выражением:\n",
    "\n",
    "$$l_i =|y_i - f(x_i)| $$\n",
    "\n",
    "$$ Loss = \\sum l_i^2 = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для прямой с параметрами $w=4$, $b = 2$ и $w=3$, $b = 2$ (верные значения):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_line(ax, x, y, w, b, color=\"r\"):\n",
    "    y_predicted = w * x + b\n",
    "    # line\n",
    "    ax.plot(x, y_predicted, color=color, alpha=0.5, label=f\"f(x)={w}x+{b}\")\n",
    "    # delta\n",
    "    for x_i, y_i, f_x in zip (x, y, y_predicted):\n",
    "        ax.vlines(x=x_i, ymin=min(f_x, y_i), ymax = max(f_x, y_i),\n",
    "                  ls='--', alpha=0.3)\n",
    "    # MSE\n",
    "    loss = np.sum((y - (w * x + b)) ** 2) / (len(x))\n",
    "    ax.set_title(f\"MSE = {loss:.3f}\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,  figsize=(11, 4))\n",
    "\n",
    "# plot x_i y_i (dots)\n",
    "for ax in axs:\n",
    "    ax.scatter(x, y, s=10)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([2, 6])\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "plot_delta_line(axs[0], x, y, w=4, b=2, color=\"r\")\n",
    "plot_delta_line(axs[1], x, y, w=3, b=2, color=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача **поиска оптимальных параметров** модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск локального минимума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как будет выглядеть ландшафт функции потерь для нашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(-10, 30, 1)\n",
    "b = np.arange(-10, 10, 1)\n",
    "\n",
    "w, b = np.meshgrid(w, b)\n",
    "\n",
    "loss = np.zeros_like(w)\n",
    "for i in range(w.shape[0]):\n",
    "    for j in range(w.shape[1]):\n",
    "        loss[i, j] = np.sum((y - (w[i, j] * x + b[i, j])) ** 2) / (len(x))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "surf = ax.plot_surface(w, b, loss, cmap=plt.cm.RdYlGn_r, alpha=0.5)\n",
    "\n",
    "ax.contourf(w, b, loss, zdir=\"z\", offset=-1, cmap=\"RdYlGn_r\", alpha=0.5)\n",
    "ax.set_zlim(0, 20)\n",
    "\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_title(\"MSE\")\n",
    "\n",
    "fig.colorbar(surf, location = 'left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимым (но недостаточным) [условием локального минимума](http://school-collection.edu.ru/catalog/res/a8dc6578-4819-4297-ab14-95ffab6fe8b6/view/#:~:text=%D0%B5%D1%81%D0%BB%D0%B8%20%D0%BF%D1%80%D0%B8%20%D0%BF%D0%B5%D1%80%D0%B5%D1%85%D0%BE%D0%B4%D0%B5%20%D1%87%D0%B5%D1%80%D0%B5%D0%B7%20%D1%82%D0%BE%D1%87%D0%BA%D1%83%20%D1%850%20%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%BD%D0%B0%D1%8F%20%D0%BC%D0%B5%D0%BD%D1%8F%D0%B5%D1%82%20%D1%81%D0%B2%D0%BE%D0%B9,%D1%82%D0%BE%D1%87%D0%BA%D0%B5%20%D1%850%20%D0%BD%D0%B5%D1%82%20%D1%8D%D0%BA%D1%81%D1%82%D1%80%D0%B5%D0%BC%D1%83%D0%BC%D0%B0.) дифференцируемой функции является равенство нулю частных производных:\n",
    "\n",
    "$$\t\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\frac{\\partial Loss}{\\partial w}=0,\n",
    "   \\\\\n",
    "   \\frac{\\partial Loss}{\\partial b}=0.\n",
    " \\end{cases}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Т.к. MSE для линейной регрессии - полином второй степени относительно $w$ и $b$, а полином второй степени не может иметь больше одного экстремума, то локальный минимум будет глобальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Метод наименьших квадратов\n",
    "\n",
    "Реализуем простейшую модель линейной регрессии с использованием библиотеки NumPy на датасете, определённом выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод наименьших квадратов: [МНК простейшие частные случаи](https://ru.wikipedia.org/wiki/Метод_наименьших_квадратов#Простейшие_частные_случаи).\n",
    "\n",
    "$$w = \\frac{n\\sum_{i=1}^nx_iy_i - (\\sum_{i=1}^nx_i)(\\sum_{i=1}^ny_i)}{n\\sum_{i=1}^nx_t^2 - (\\sum_{i=1}^n x_t)^2};$$\n",
    "\n",
    "$$b = \\frac{\\sum_{i=1}^ny_i - w(\\sum_{i=1}^nx_i)}{n}.$$\n",
    "\n",
    "По сути метод наименьших квадратов - это решение системы уравнений выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_coef(x, y):\n",
    "    n = len(x)\n",
    "    assert n == len(y)\n",
    "    w = (n * sum(np.multiply(x, y)) - sum(x) * sum(y)) / (\n",
    "         n * sum(np.multiply(x, x)) - sum(x) ** 2\n",
    "    )\n",
    "    b = (sum(y) - w * sum(x)) / n\n",
    "    return w, b\n",
    "\n",
    "w, b = estimate_coef(x, y)\n",
    "\n",
    "y_predicted = w * x + b\n",
    "\n",
    "print(f\"Estimated coefficients:\\nb = {b[0]:.3f} \\nw = {w[0]:.3f}\")\n",
    "print(f\"Final equation: \\ny = {w[0]:.3f}x +{b[0]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.plot(x, y_predicted, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С одной из метрик регрессии мы уже познакомились: это  — $MSE$, которую мы минимизировали в методе наименьших квадратов. Стоит отметить что $MSE$ имеет [размерность](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8) квадрата размерности предсказываемого значения.\n",
    "\n",
    "$$ MSE  = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$\n",
    "\n",
    "Чтобы получить оценку ошибки той же размерности можно взять корень (root) от $MSE$. Это метрика $RMSE$:\n",
    "\n",
    "$$ RMSE = \\sqrt{\\frac{1}{N} \\sum (y_i - f(x_i))^2}$$\n",
    "\n",
    "Или посчитать среднюю абсолютную ошибку $MAE$:\n",
    "\n",
    "$$ MAE = \\frac{1}{N} \\sum |y_i - f(x_i)|$$\n",
    "\n",
    "Существуют и более специфичные метрики, например $R^2$, которая принимает значения от $(-\\inf, 1]$, где $1$  —  наилучший вариант. $R^2$  называется [коэффициентом детерминации](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8) и характеризуют долю дисперсии целевого значения, которую объясняет модель.\n",
    "\n",
    "$$R^2 = 1 - \\frac{MSE}{\\sigma^2}=1 - \\frac{\\sum {(y_i-f(x_i))^2}}{\\sum{(y_i-\\bar{y})^2}}$$\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{N}\\sum {y_i}$$\n",
    "\n",
    "где $\\sigma^2$ - дисперсия.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Когда $R^2$ около нуля - модель плохо объясняет данные.</center>\n",
    "\n",
    "<center><img src =\"https://imgs.xkcd.com/comics/linear_regression.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em> Source: https://xkcd.com/1725</em></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def print_metrics(y_true, y_predicted):\n",
    "    print(f\"Mean squared error: {mean_squared_error(y_true, y_predicted):.3f}\")\n",
    "    print(\"Root mean squared error: \",\n",
    "          f\"{mean_squared_error(y_true, y_predicted, squared=False):.3f}\")\n",
    "    print(f\"Mean absolute error: {mean_absolute_error(y_true, y_predicted):.3f}\")\n",
    "    print(f\"R2 score: {r2_score(y_true, y_predicted):.3f}\")\n",
    "\n",
    "print_metrics(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее про метрики можно почитать: [тут](https://academy.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель линейной регрессии из библиотеки scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свою линейную регрессию мы написали, теперь изучим как работать с моделью из sklearn.\n",
    "\n",
    "Рассмотрим следующую задачу. Пусть мы хотим построить модель предсказания успеваемости студента на основе информации о величине потраченного им на изучение материала количества времени в часах. Это пример простейшей задачи линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет. Датасет содержит два числовых значения — часы и результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/student_scores.csv\"\n",
    ")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости одного от другого, а также отобразим распределения каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.jointplot(data=dataset, x=\"Hours\", y=\"Scores\", height=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.iloc[:, :-1].values  # column Hours\n",
    "y = dataset.iloc[:, 1].values  # column Score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель для линейной регрессии. Чтобы не писать с нуля, воспользуемся готовой моделью из библиотеки `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.linspace(min(x_train), max(x_train), 100)  # 100 dots at min to max\n",
    "y_pred = regressor.predict(x_points)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_train, y_train, \"o\", label=\"Scores\")\n",
    "plt.plot(\n",
    "    x_points,\n",
    "    y_pred,\n",
    "    label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_),\n",
    ")\n",
    "plt.title(\"Hours vs Percentage\", size=12)\n",
    "plt.xlabel(\"Hours Studied\", size=12)\n",
    "plt.ylabel(\"Percentage Score\", size=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем предсказание для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "x_points = np.linspace(min(x_test), max(x_test), 100)\n",
    "y_pred = regressor.predict(x_points)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_test, y_test, \"o\", label=\"Scores\")\n",
    "plt.plot(\n",
    "    x_points,\n",
    "    y_pred,\n",
    "    label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_),\n",
    ")\n",
    "plt.title(\"Hours vs Percentage\", size=12)\n",
    "plt.xlabel(\"Hours Studied\", size=12)\n",
    "plt.ylabel(\"Percentage Score\", size=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неплохо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики для наших значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будьте осторожнее, модели отражают только те закономерности, которые видели в данных. Вероятность того, что студент, потративший на подготовку 20 часов получит больше максимального балла мала.\n",
    "\n",
    "<center><img src =\"https://imgs.xkcd.com/comics/extrapolating.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em> Source: https://xkcd.com/605</em></center>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы решали задачу линейной регрессии аналитически (МНК), но это не всегда возможно по нескольким причинам:\n",
    "* Для аналитического решения нужно считать обратную матрицу, это - вычислительно сложно и матрица бывает плохо определенной.\n",
    "* Данных может быть слишком много, чтобы их можно было одновременно положить в пямять для расчета обратной матрицы.\n",
    "* Модели могут быть слишком сложные для поиска аналитического решения. Более того, для сложных моделей  ландшафт функции потерь может иметь сложный рельеф с несколькими локальными минимумами.\n",
    "\n",
    "Давайте поговорим о том, что делать в таком случае.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод, который мы будем использовать, называется **“метод градиентного спуска”**. Для начала вспомним, что такое **градиент**. Возьмем функцию двух переменных:\n",
    "\n",
    "$$f(x, y) = \\sin(x\\cdot y)$$\n",
    "\n",
    "Она будет отличаться от функции потерь, которую мы визуализировали тем, что у нее не будет одного экстремума. Рассчитаем ее на диапазоне значений от 0 до 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = lambda x, y: np.sin(x * y)\n",
    "\n",
    "x = np.linspace(0, 4, 1000)\n",
    "y = np.linspace(0, 4, 1000)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "zz = f(xx, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eсли $\\varphi = \\varphi(\\vec{x})=\\varphi(x_1 \\dots x_n)$  — функция $n$ переменных, то её градиентом называется $n$-мерный вектор:\n",
    "$$\n",
    "\\nabla \\varphi(\\vec{x})=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial\\varphi}{\\partial x_1}\\\\\n",
    "\\frac{\\partial\\varphi}{\\partial x_2}\\\\\n",
    "...\\\\\n",
    "\\frac{\\partial\\varphi}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем градиент нашей функции $f(x, y)$. Для этого воспользуемся [**таблицей производных**](https://ru.wikipedia.org/wiki/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%BD%D1%8B%D1%85) и правилом вычисления [**производной сложной функции**](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%84%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D0%B9_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8) (Chain-rule) $\\frac {\\partial f} {\\partial x} = \\frac {\\partial f} {\\partial t} \\cdot \\frac {\\partial t} {\\partial x}$ - это правило очень нам пригодиться в будущем.\n",
    "\n",
    "$$\\nabla f(x, y)=\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x}\\\\\n",
    "\\frac{\\partial f}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\frac{\\partial\\sin(xy)}{\\partial(xy)}\\cdot\\frac{\\partial(xy)}{\\partial x}\\\\\n",
    "\\frac{\\partial\\sin(xy)}{\\partial(xy)}\\cdot\\frac{\\partial(xy)}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\cos(xy)\\cdot y\\\\\n",
    "\\cos(xy)\\cdot x\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Посчитаем градиент на том же диапазоне (сетка реже, т.к. мы будем рисовать не точки, а стрелочки):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradf = lambda x, y: (np.cos(x * y) * y, np.cos(x * y) * x)\n",
    "\n",
    "xsmall = np.linspace(0, 4, 15)\n",
    "ysmall = np.linspace(0, 4, 15)\n",
    "xxsmall, yysmall = np.meshgrid(xsmall, ysmall)\n",
    "gradx, grady = gradf(xxsmall, yysmall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как **значение градиента в точке** - это вектор, мы можем говорить о его **величине** и **направлении**. Так как значение градиента в точке - это вектор, мы можем говорить о его величине и направлении. Визуализируем наши расчеты: посмотрим на ландшафт функции $f(x, y)$ и направления градиентов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(11, 4))\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "surf = ax.plot_surface(xx, yy, zz, cmap=plt.cm.RdYlGn_r)\n",
    "\n",
    "ax.contourf(xx, yy, zz, zdir=\"zz\", offset=-2, cmap=\"RdYlGn_r\")\n",
    "ax.set_zlim(-2, 2)\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"sin(xy)\")\n",
    "\n",
    "fig.colorbar(surf, location = 'left')\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(zz, extent=(np.min(x), np.max(x), np.min(y), np.max(y)),\n",
    "          cmap=\"RdYlGn_r\", origin=\"lower\")\n",
    "ax.quiver(xxsmall, yysmall, gradx, grady)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке выше значения градиента в точке обозначены чёрными стрелочками. Можно заметить, что длина стрелок в  районе максимальных и минимальных значений функции - почти нулевая, стрелки направлены в направлении возрастания значения функции и наиболее длинные стрелки находятся в области наиболее резкого изменения значений функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это - проявление **свойств градиента**:\n",
    "* Направление $\\frac{\\nabla f}{||\\nabla f||}$ - сообщает нам направление максимального роста функции.\n",
    "\n",
    "*  Величина $||\\nabla f||$ -  характеризует мгновенную скорость изменения значений функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим еще раз данные с зависимостью оценок студентов от времени подготовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/student_scores.csv\"\n",
    ")\n",
    "\n",
    "x = dataset.iloc[:, :-1].values  # column Hours\n",
    "y = dataset.iloc[:, 1].values  # column Score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже **код для интерактивной визуализации**. Он нужен только для объяснения и **не пригодится вам в работе**. Его разбирать мы не будем, если интересно - можно изучить самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title *Code for interactive visual\n",
    "# source: https://github.com/TomasBeuzen/deep-learning-with-pytorch\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from torchvision import utils\n",
    "\n",
    "\n",
    "def plot_grid_search(x, y, slopes, prediction, loss, dloss_dw):\n",
    "    pred_df =  pd.DataFrame(prediction)\n",
    "    loss_df = pd.DataFrame({\"slope\": slopes, \"loss\": loss})\n",
    "    dloss_dw_df = pd.DataFrame({\"slope\": slopes, \"dloss/dw\": dloss_dw})\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=(\"Data & Fitted Line\",\n",
    "                        \"Loss\",\n",
    "                        \"dLoss/dw\")\n",
    "    )\n",
    "\n",
    "    # fig 1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=y, mode=\"markers\", marker=dict(size=10), name=\"Data\",\n",
    "                   line_color=\"#5D5DA6\",),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=pred_df.iloc[:, 0], line_color=\"#F9B041\", mode=\"lines\",\n",
    "                   line=dict(width=3), name=\"Fitted line\"),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    fig.update_xaxes(row=1, col=1, title=\"feature\")\n",
    "    fig.update_yaxes(row=1, col=1, title=\"target\")\n",
    "\n",
    "    # fig 2\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=loss_df[\"slope\"], y=loss_df[\"loss\"], mode=\"markers\",\n",
    "                   marker=dict(size=7), name=\"Loss\", line_color=\"#2DA9E1\"),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=loss_df.iloc[[0]][\"slope\"],\n",
    "            y=loss_df.iloc[[0]][\"loss\"],\n",
    "            line_color=\"red\",\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=14, line=dict(width=1, color=\"DarkSlateGrey\")),\n",
    "            name=\"Loss for line\"\n",
    "        ),\n",
    "        row=1,col=2,\n",
    "    )\n",
    "    fig.update_xaxes(row=1, col=2, title=\"w\")\n",
    "\n",
    "    # fig 3\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=dloss_dw_df[\"slope\"], y=dloss_dw_df[\"dloss/dw\"],\n",
    "                   mode=\"markers\", marker=dict(size=7), name=\"derivative\",\n",
    "                   line_color=\"#4AAE4D\"\n",
    "                   ),\n",
    "        row=1, col=3,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=dloss_dw_df.iloc[[0]][\"slope\"],\n",
    "            y=dloss_dw_df.iloc[[0]][\"dloss/dw\"],\n",
    "            line_color=\"yellow\",\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=14, line=dict(width=1, color=\"DarkSlateGrey\")),\n",
    "            name=\"derivative for Loss\"\n",
    "        ),\n",
    "        row=1, col=3,\n",
    "    )\n",
    "    fig.update_xaxes(row=1, col=3, title=\"w\")\n",
    "\n",
    "    # movement\n",
    "    frames = [\n",
    "        dict(\n",
    "            name=f\"{slope}\",\n",
    "            data=[\n",
    "                go.Scatter(x=x, y=y),\n",
    "                go.Scatter(x=x, y=pred_df[f\"{slope}\"]),\n",
    "                go.Scatter(x=loss_df[\"slope\"], y=loss_df[\"loss\"]),\n",
    "                go.Scatter(\n",
    "                    x=loss_df.iloc[[n]][\"slope\"],\n",
    "                    y=loss_df.iloc[[n]][\"loss\"],\n",
    "                ),\n",
    "                go.Scatter(x=dloss_dw_df[\"slope\"], y=dloss_dw_df[\"dloss/dw\"]),\n",
    "                go.Scatter(\n",
    "                    x=dloss_dw_df.iloc[[n]][\"slope\"],\n",
    "                    y=dloss_dw_df.iloc[[n]][\"dloss/dw\"],\n",
    "                ),\n",
    "            ],\n",
    "            traces=[0, 1, 2, 3, 4, 5],\n",
    "        )\n",
    "        for n, slope in enumerate(slopes)\n",
    "    ]\n",
    "\n",
    "    # slider\n",
    "    sliders = [\n",
    "        {\n",
    "            \"currentvalue\": {\n",
    "                \"font\": {\"size\": 16},\n",
    "                \"prefix\": \"w: \",\n",
    "                \"visible\": True,\n",
    "            },\n",
    "            \"pad\": {\"b\": 10, \"t\": 30},\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"args\": [\n",
    "                        [f\"{slope}\"],\n",
    "                        {\n",
    "                            \"frame\": {\n",
    "                                \"duration\": 0,\n",
    "                                \"easing\": \"linear\",\n",
    "                                \"redraw\": False,\n",
    "                            },\n",
    "                            \"transition\": {\"duration\": 0, \"easing\": \"linear\"},\n",
    "                        },\n",
    "                    ],\n",
    "                    \"label\": f\"{slope}\",\n",
    "                    \"method\": \"animate\",\n",
    "                }\n",
    "                for slope in slopes\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    fig.update(frames=frames), fig.update_layout(sliders=sliders)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_gradient_descent(x, y, slopes, prediction, mses, dmse_dws,\n",
    "                          w_range=(2.5, 17.5, 0.05)):\n",
    "    pred_df =  pd.DataFrame(prediction)\n",
    "    slope_range = np.arange(*w_range)\n",
    "    mse = []\n",
    "    for w in slope_range:\n",
    "        mse.append(mean_squared_error(y, w * x + 2.83))  # calc MSE\n",
    "    mse = pd.DataFrame({\"slope\": slope_range, \"squared_error\": mse})\n",
    "    iters = np.arange(len(dmse_dws)) + 1\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=(\"Data & Fitted Line\", \"Mean Squared Error\", \"dMSE/dw\" ),\n",
    "    )\n",
    "\n",
    "    # fig 1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=y, mode=\"markers\", marker=dict(size=10), name=\"Data\",\n",
    "                   line_color=\"#5D5DA6\",),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x, y=pred_df.iloc[:, 0], line_color=\"#F9B041\", mode=\"lines\",\n",
    "                   line=dict(width=3), name=\"Fitted line\"),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[2], y=[95], mode=\"text\", text=f\"<b>w = {slopes[0]:.2f}<b>\",\n",
    "            textfont=dict(size=16, color=\"DarkSlateGrey\"), showlegend=False,\n",
    "        ),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    fig.update_xaxes(row=1, col=1, title=\"feature\")\n",
    "    fig.update_yaxes(row=1, col=1, title=\"target\")\n",
    "\n",
    "    # fig 2\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=mse[\"slope\"], y=mse[\"squared_error\"], line_color=\"#2DA9E1\",\n",
    "                   line=dict(width=3), mode=\"lines\", name=\"MSE\"),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.array(slopes[:1]), y=np.array(mses[:1]), line_color=\"salmon\",\n",
    "            line=dict(width=4), mode=\"markers+lines\", name=\"Slope history\",\n",
    "            marker=dict(size=10, line=dict(width=1, color=\"DarkSlateGrey\")),\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.array(slopes[0]), y=np.array(mses[0]), line_color=\"red\",\n",
    "            mode=\"markers\", name=\"MSE for line\",\n",
    "            marker=dict(size=18, line=dict(width=1, color=\"DarkSlateGrey\")),\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    fig.update_xaxes(row=1, col=2, title=\"w\")\n",
    "\n",
    "    # fig 3\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=iters, y=dmse_dws, mode=\"markers\", line_color=\"DarkSlateGrey\",\n",
    "            marker=dict(size=3), name=\"Gradient values\"\n",
    "        ),\n",
    "        row=1, col=3,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.array(iters[0]), y=np.array(dmse_dws[0]), mode=\"markers\",\n",
    "            marker=dict(size=10, line=dict(width=1, color=\"DarkSlateGrey\")),\n",
    "            name=\"Gradient history\", line_color=\"#4AAE4D\"\n",
    "            ),\n",
    "        row=1, col=3,\n",
    "    )\n",
    "    fig.update_xaxes(row=1, col=3, title=\"iteration\")\n",
    "    # movement\n",
    "    frames = [\n",
    "        dict(\n",
    "            name=n,\n",
    "            data=[\n",
    "                go.Scatter(x=x, y=y),\n",
    "                go.Scatter(x=x, y=pred_df[slope]),\n",
    "                go.Scatter(text=f\"<b>w = {slope:.2f}<b>\"),\n",
    "                go.Scatter(x=mse[\"slope\"], y=mse[\"squared_error\"]),\n",
    "                go.Scatter(\n",
    "                    x=np.array(slopes[: n + 1]),\n",
    "                    y=np.array(mses[: n + 1]),\n",
    "                    mode=\"markers\" if n == 0 else \"markers+lines\",\n",
    "                ),\n",
    "                go.Scatter(x=np.array(slopes[n]), y=np.array(mses[n])),\n",
    "                go.Scatter(x=iters, y=dmse_dws),\n",
    "                go.Scatter(\n",
    "                    x=np.array(iters[: n + 1]),\n",
    "                    y=np.array(dmse_dws[: n + 1]),\n",
    "                    mode=\"markers\" if n == 0 else \"markers+lines\",\n",
    "                ),\n",
    "            ],\n",
    "            traces=[0, 1, 2, 3, 4, 5, 6, 7],\n",
    "        )\n",
    "        for n, slope in enumerate(slopes)\n",
    "    ]\n",
    "    # slider\n",
    "    sliders = [\n",
    "        {\n",
    "            \"currentvalue\": {\n",
    "                \"font\": {\"size\": 16},\n",
    "                \"prefix\": \"Iteration: \",\n",
    "                \"visible\": True,\n",
    "            },\n",
    "            \"pad\": {\"b\": 10, \"t\": 30},\n",
    "            \"steps\": [\n",
    "                {\n",
    "                    \"args\": [\n",
    "                        [n],\n",
    "                        {\n",
    "                            \"frame\": {\n",
    "                                \"duration\": 0,\n",
    "                                \"easing\": \"linear\",\n",
    "                                \"redraw\": False,\n",
    "                            },\n",
    "                            \"transition\": {\"duration\": 0, \"easing\": \"linear\"},\n",
    "                        },\n",
    "                    ],\n",
    "                    \"label\": n,\n",
    "                    \"method\": \"animate\",\n",
    "                }\n",
    "                for n in range(len(slopes))\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    fig.update(frames=frames), fig.update_layout(sliders=sliders)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_grid_search_2d(x, y, slopes, intercepts):\n",
    "    mse = np.zeros((len(slopes), len(intercepts)))\n",
    "    for i, slope in enumerate(slopes):\n",
    "        for j, intercept in enumerate(intercepts):\n",
    "            mse[i, j] = mean_squared_error(y, x * slope + intercept)\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=2,\n",
    "        subplot_titles=(\"Surface Plot\", \"Contour Plot\"),\n",
    "        specs=[[{\"type\": \"surface\"}, {\"type\": \"contour\"}]],\n",
    "    )\n",
    "    # fig 1\n",
    "    fig.add_trace(\n",
    "        go.Surface(\n",
    "            z=mse, x=intercepts, y=slopes, name=\"\", colorscale=\"RdYlGn_r\"\n",
    "        ),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    # fig 2\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            z=mse, x=intercepts, y=slopes, name=\"\", showscale=False,\n",
    "            colorscale=\"RdYlGn_r\",\n",
    "        ),\n",
    "        row=1, col=2,\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            zaxis=dict(title=\"MSE\"),\n",
    "            yaxis=dict(title=\"slope (w<sub>1</sub>)\"),\n",
    "            xaxis=dict(title=\"intercept (w<sub>0</sub>)\"),\n",
    "        ),\n",
    "        scene_camera=dict(eye=dict(x=2, y=1.1, z=1.2)),\n",
    "        margin=dict(l=0, r=0, b=60, t=90),\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        title=\"intercept (w<sub>0</sub>)\",\n",
    "        range=[intercepts.min(), intercepts.max()],\n",
    "        tick0=intercepts.max(),\n",
    "        row=1, col=2,\n",
    "        title_standoff=0,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title=\"slope (w<sub>1</sub>)\",\n",
    "        range=[slopes.min(), slopes.max()],\n",
    "        tick0=slopes.min(),\n",
    "        row=1, col=2,\n",
    "        title_standoff=0,\n",
    "    )\n",
    "    fig.update_layout(width=900, height=475, margin=dict(t=60))\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_gradient_descent_2d(x, y, ws, slopes, intercepts,\n",
    "                             title = \"Gradient Descent\", mode=\"markers+lines\"):\n",
    "    bs, ws = ws[:, 0], ws[:, 1]\n",
    "    mse = np.zeros((len(slopes), len(intercepts)))\n",
    "    for i, slope in enumerate(slopes):\n",
    "        for j, intercept in enumerate(intercepts):\n",
    "            mse[i, j] = mean_squared_error(y, x * slope + intercept)\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        subplot_titles=[title],\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Contour(\n",
    "            z=mse, x=intercepts, y=slopes, name=\"\", showscale=False,\n",
    "            colorscale=\"RdYlGn_r\",\n",
    "        ),\n",
    "        row=1, col=1,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bs, y=ws, mode=mode, line=dict(width=2.5),\n",
    "            line_color=\"coral\", marker=dict(\n",
    "                opacity=1,\n",
    "                size=np.linspace(19, 1, len(intercepts)),\n",
    "                line=dict(width=2, color=\"DarkSlateGrey\"),\n",
    "            ),\n",
    "            name=\"Descent Path\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[bs[0]], y=[ws[0]], mode=\"markers\",\n",
    "            marker=dict(size=20, line=dict(width=2, color=\"DarkSlateGrey\")),\n",
    "            marker_color=\"orangered\", name=\"Start\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[bs[-1]], y=[ws[-1]], mode=\"markers\",\n",
    "            marker=dict(size=20, line=dict(width=2, color=\"DarkSlateGrey\")),\n",
    "            marker_color=\"yellowgreen\", name=\"End\",\n",
    "        )\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        width=700, height=600, margin=dict(t=60),\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        title=\"intercept (w<sub>0</sub>)\",\n",
    "        range=[intercepts.min(), intercepts.max()],\n",
    "        tick0=intercepts.max(),\n",
    "        row=1, col=1,\n",
    "        title_standoff=0,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title=\"slope (w<sub>1</sub>)\",\n",
    "        range=[slopes.min(), slopes.max()],\n",
    "        tick0=slopes.min(),\n",
    "        row=1, col=1,\n",
    "        title_standoff=0,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def plot_panel(f1, f2, f3):\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=3,\n",
    "        subplot_titles=(\n",
    "            \"Gradient Descent\",\n",
    "            \"Stochastic Gradient Descent\",\n",
    "            \"Minibatch Gradient Descent\",\n",
    "        ),\n",
    "    )\n",
    "    for n, f in enumerate((f1, f2, f3)):\n",
    "        for _ in range(len(f.data)):\n",
    "            fig.add_trace(f.data[_], row=1, col=n + 1)\n",
    "    fig.update_layout(\n",
    "        width=1000, height=400, margin=dict(t=60), showlegend=False\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты рассмотрим одномерный случай. Будем подбирать только $w$, значение $b$ зафиксируем на уровне $2.83$. Визуализируем ошибку и значения $\\frac{\\partial Loss}{\\partial w}$ для MSE Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = np.arange(5, 15, 0.5)\n",
    "prediction = {\n",
    "    f\"{w}\": w*x_train[:, 0] + 2.83\n",
    "    for w in slopes\n",
    "}\n",
    "mse = np.array(\n",
    "    [mean_squared_error(y_train, w*x_train[:, 0] + 2.83)\n",
    "     for w in slopes]\n",
    ")\n",
    "dmse_dw = np.array(\n",
    "    [(2*x_train[:, 0]*(w*x_train[:, 0] + 2.83 - y_train)).mean()\n",
    "     for w in slopes]\n",
    ")\n",
    "plot_grid_search(x_train[:,0], y_train, slopes, prediction, mse, dmse_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Видно, что оптимальное значение наклона соответствует минимуму MSE и нулю частной производной $\\frac{\\partial Loss}{\\partial w}$. Аналогично будет, если мы будем возьмем в качестве Loss MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = np.arange(5, 15, 0.5)\n",
    "prediction = {\n",
    "    f\"{w}\": w*x_train[:, 0] + 2.83\n",
    "    for w in slopes\n",
    "}\n",
    "mae = np.array(\n",
    "    [mean_absolute_error(y_train, w*x_train[:, 0] + 2.83)\n",
    "     for w in slopes]\n",
    ")\n",
    "dmae_dw = np.array(\n",
    "    [(x_train[:, 0]*np.sign(w*x_train[:, 0] + 2.83 - y_train)).mean()\n",
    "    for w in slopes]\n",
    ")\n",
    "plot_grid_search(x_train[:,0], y_train, slopes, prediction, mae, dmae_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого т.к. градиент указывает направления наибольшего возрастания функции. Если $\\frac{\\partial Loss}{\\partial w} < 0$, то нам имеет смысл “идти” в сторону возрастания $\\frac{\\partial Loss}{\\partial w}$, если $\\frac{\\partial Loss}{\\partial w} > 0$ - в сторону убывания. **Метод градиентного спуска** - итеративный метод, идея которого заключается в том, чтобы небольшими шажками “идти” в **обратную от градиента сторону**:\n",
    "\n",
    "$$ \\vec w_{n+1} = \\vec w_{n} - α \\cdot \\nabla_{\\vec w_{n}} Loss$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $α$ - скорость обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/update_weghts_values.png\" width=\"450\"></center>\n",
    "\n",
    "<!-- [Визуализация](https://docs.google.com/file/d/0Byvt-AfX75o1ZWxMRkxrUFJ2ZUE/preview)\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем реализовать это в коде (для простоты только для $w$ при $b=2.83$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, w, b):\n",
    "    return 2 * (x * (w * x + b - y)).mean()\n",
    "\n",
    "def gradient_descent(x_train, y_train, x_test, y_test, w, alpha, b=2.83,\n",
    "                     iteration=10):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = [w]\n",
    "    mse_train = [mean_squared_error(y_train, w*x_train + b)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, w*x_test + b)]\n",
    "    prediction = {w: w*x_train + b}\n",
    "    print(f\"Iteration 0: w = {w:.2f}, Loss_train = {mse_train[0]:.2f}, \"\n",
    "          f\"Loss_test = {mse_test[0]:.2f}.\")\n",
    "    for i in range(iteration):\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_train, y_train, w, b)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws.append(w)\n",
    "        mse_train.append(mean_squared_error(y_train, w*x_train + b))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, w*x_test + b))\n",
    "        prediction[w] = w*x_train + b\n",
    "        print(f\"Iteration {i+1}: w = {w:.2f}, Loss_train = {mse_train[i]:.2f}, \"\n",
    "              f\"Loss_test = {mse_test[i]:3.2f}.\")\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашу модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.01, iteration=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем процесс обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(\n",
    "    x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что за 7 эпох мы мы получили то же значение $w$, что получали при использовании [`LinearRegression`](https://colab.research.google.com/drive/1QoqgO4nSSQcxKP6arDEcedw7I_XFq4zs#scrollTo=9E8205Ic5ujC). При этом мы пришли в минимум $MSE$ и ноль градиента.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальности мы будем работать с функциями многих переменных, поэтому смотреть на сходимость по одной переменной - не самый оптимальный вариант. Более эффективно будет посмотреть на зависимость Loss от количества эпох для train и test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mse(mse_train, mse_test):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(mse_train, label=\"train\")\n",
    "    plt.plot(mse_test, label=\"test\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt. xlabel('iterations', fontsize=12)\n",
    "    plt. ylabel('MSE Loss', fontsize=12)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие графики называют кривыми обучения.Посмотрим на кривые обучения для нашей скорости обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно что **Loss падает**, как на **train**, так и на **test** выборке. Также мы можем сказать, что **сеть обучалась**: train и test **графики вышли на плато**. При этом не произошло **переобучения**: ошибка на **test** выборке **не начала расти** (про переобучение поговорим позже).\n",
    "\n",
    "В полученных графиках есть особенность, которая бросается в глаза опытному в обучении моделей человеку: **Loss на test выборке меньше, чем на train**. Это - показатель того, что **с данными что-то не так**. Так бывает при утечки данных (об утечке данных вы подробнее узнаете в следующих лекциях) или если, как в данном случае, когда test выборка слишком мала, чтобы отражать генеральною совокупность (всего 4 студента, доверительный интервал для такого маленького количества объектов будет широкий)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор скорости обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Скорость (шаг) обучения**  — некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время, чтобы тот же процесс не шел слишком медленно (как на графике слева)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/learning_rate_optimal_value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на скорость обучения на нашем примере. При **маленькой скорости обучения** - мы будем очень медленно сходиться к минимуму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.0005, iteration=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(\n",
    "    x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спустя 30 итераций оранжевая прямая плохо отражает генеральную совокупность. Мы не достигли минимума MSE и нуля градиента.\n",
    "\n",
    "Посмотрим, как выглядят кривые обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель недообученна - значения Loss не вышло на плато.\n",
    "\n",
    "Посмотрим на **достаточно большую скорость обучения**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.027, iteration=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(\n",
    "    x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг, получаемый умножением градиента на скорость обучения, получается достаточно большим, чтобы “перескочить” локальный минимум, но при этом модель все-таки попадает в локальный минимум. Кривые обучения при этом успешно выходят на плато."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В финале посмотрим на **очень большую скорость обучения**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.034, iteration=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг который мы делаем слишком большой. Мы не попадаем в локальный минимум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(\n",
    "    x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "По кривым обучения видно, что модель не сошлась: ошибка растет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор скорости обучения будет зависеть от модели и данных. В 7 лекции вы познакомитесь с различными модификациями метода градиентного спуска и узнаете больше о выборе скорости обучения, а пока ориентируйтесь на кривые обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Единый подход к учету смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока мы настраивали только одну переменную, но даже в случае предсказания оценки по времени подготовки - у нас две переменные: вес $w$ и смещение $b$.\n",
    "\n",
    "Когда признаков станет больше - у нас получится “лапша” из слагаемых:\n",
    "$$y = b + w_1\\cdot x_1 + w_2\\cdot x_2 + w_3\\cdot x_3 + w_4\\cdot x_4 + w_5\\cdot x_5 + ... + w_n\\cdot x_n$$\n",
    "\n",
    "Нам бы хотелось записать их компактно, чтобы не усложнять код и использовать один и тот же код для данных с разным количеством признаков.  Для этого мы будем использовать матричное перемножение и трюк **“столбец единиц”**, который реализует **единый подход к учету смещения**.\n",
    "\n",
    "Обозначим вектор-столбец из настраиваемых параметров:\n",
    "$$\\vec w = \\begin{bmatrix}\n",
    "b \\\\ w \\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.5], [5]])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К матрице (в нашем случае был только один признак, поэтому у нас будет вектор-столбец) признаков слева \"дорисуем\" столбец единиц:\n",
    "$$X = \\begin{bmatrix}\n",
    "1 & X \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2.7 \\\\\n",
    "1 & 3.3 \\\\\n",
    "... & ...\\\\\n",
    "1 & 9.2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Предупреждение:** добавлять столбец единиц нужно только если вы сами пишите модель. **Если вы пользуетесь готовыми моделями - в этом нет необходимости.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.hstack((np.ones((len(x_train), 1)), x_train))\n",
    "x_test = np.hstack((np.ones((len(x_test), 1)), x_test))\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрицу $X$ можно матрично перемножить с столбцом $\\vec w$, т.к количество столбцов $X$ совпадает с количеством строк в $\\vec w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае:\n",
    "\n",
    "$$\\vec y = b + w_1\\cdot x_1 + w_2\\cdot x_2 + w_3\\cdot x_3 + w_4\\cdot x_4 + w_5\\cdot x_5 + ... + w_n\\cdot x_n = X\\vec w $$  \n",
    "\n",
    "Эту формулу можно свести к нескольким символам кода (`@` - матричное умножение):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x_test @ w\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Необходимость нормализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем многомерный градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "y_test = np.expand_dims(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, w):\n",
    "    \"\"\"Gradient of mean squared error.\"\"\"\n",
    "    return 2 * (x.T @ (x @ w) - x.T @ y) / len(x)\n",
    "\n",
    "def gradient_descent(x_train, y_train, x_test, y_test, w, alpha,\n",
    "                     iteration=10):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = np.zeros((iteration+1, 2))\n",
    "    ws[0] = w[:, 0]\n",
    "    mse_train = [mean_squared_error(y_train, x_train @ w)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, x_test @ w)]\n",
    "    prediction = {(w[0][0], w[1][0]): x_train @ w}\n",
    "\n",
    "    print(f\"Iteration 0: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "          f\"Loss_train = {mse_train[0]:.2f}, \"\n",
    "          f\"Loss_test = {mse_test[0]:.2f}.\")\n",
    "\n",
    "    for i in range(iteration):\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_train, y_train, w)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws[i+1] = w[:, 0]\n",
    "        mse_train.append(mean_squared_error(y_train, x_train @ w))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, x_test @ w))\n",
    "        prediction[(w[0][0], w[1][0])] = x_train @ w\n",
    "\n",
    "        print(f\"Iteration {i+1}: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "              f\"Loss_train = {mse_train[i]:.2f}, \"\n",
    "              f\"Loss_test = {mse_test[i]:3.2f}.\")\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.5], [5]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train, y_train, x_test, y_test, w, 0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы не дошли до оптимальной прямой $y = 9.68x+2.83$ ([вычисляли выше](https://colab.research.google.com/drive/1QoqgO4nSSQcxKP6arDEcedw7I_XFq4zs#scrollTo=DR5zQvUm5ujD&line=1&uniqifier=1)).\n",
    "\n",
    "При этом график Loss выглядит не плохо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такое поведение связано с ландшафтом функции потерь: значение ошибки по оси $w[0]$ измерняется намного медленнее, чем по $w[1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts = np.arange(-7.5, 12.5, 0.1) # b\n",
    "slopes = np.arange(5, 15, 0.1) # w\n",
    "plot_grid_search_2d(x_train[:, 1], y_train, slopes, intercepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому основное изменение значений происходит вдоль оси $w[1]$, а $w[0]$ меняется слабо (значение $b$ далеко от ожидаемого)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(\n",
    "    x_train[:, 1], y_train[:, 0], ws, slopes, intercepts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы исправить ситуацию применим `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(\n",
    "    np.expand_dims(x_train[:, 1], axis=1)\n",
    ").flatten()\n",
    "\n",
    "x_test_scaled = scaler.transform(\n",
    "    np.expand_dims(x_test[:, 1], axis=1)\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts = np.arange(40, 60, 0.1) # b\n",
    "slopes = np.arange(15, 35, 0.1) # w\n",
    "\n",
    "plot_grid_search_2d(x_train_scaled, y_train, slopes, intercepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled =  np.hstack(\n",
    "    (np.ones((len(x_train_scaled), 1)), np.expand_dims(x_train_scaled, axis=1)),\n",
    ")\n",
    "\n",
    "x_test_scaled =  np.hstack(\n",
    "    (np.ones((len(x_test_scaled), 1)), np.expand_dims(x_test_scaled, axis=1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. диапазоны x изменились - значения $w[0]$ и $w[1]$ тоже изменятся.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[57.0],[33.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train_scaled, y_train, x_test_scaled, y_test, w, 0.35, iteration=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что после нормализации мы сходимся к $y = 9.68x + 2.83$.  Для этого используем данные о матожидании и дисперсии из `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ws[-1][0] - ws[-1][1]*scaler.mean_/(scaler.var_)**0.5\n",
    "w = ws[-1][1]/(scaler.var_)**0.5\n",
    "\n",
    "print(f'y = {w[0]:.2f}x + {b[0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По визуализации видно, что $w[0]$ и $w[1]$ изменяются во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1], y_train[:, 0], ws, slopes, intercepts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cтохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост о стохастическом градиентом спуске](https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter2_stochastic-gradient-descent.html#motivation-for-stochastic-gradient-descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы обучали модель, рассчитывая градиент по **всей train выборке**. Это не всегда возможно:\n",
    "- данных может быть слишком много, чтобы загрузить их в память одновременно и рассчитать градиент,\n",
    "- мы можем хотеть дообучать модель на свеже пришедших данных, которых может приходить немного.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому появляется идея **стохастического градиентного спуска**: мы можем делать шаг обучения, рассчитывая градиент не по всей выборке (**batch**), а по нескольким случайно выбранным объектам (**mini-batch**) или даже по одному случайно выбранному объекту (**stochastic**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/define_size_of_batch.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно [показать](https://academy.yandex.ru/handbook/ml/article/shodimost-sgd), что **стохастический** (с размером batch 1) **градиентный спуск сходится к миниуму (глобальному или локальному) функции потерь** с конечной точностью. Важным условием является **стохастичность**. Если мы будем использовать одну и ту же последовательных выборок это приведет к накоплению ошибки и смещению результата.\n",
    "\n",
    "Добавим создание подвыборки к нашему алгоритму:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    x_train, y_train, x_test, y_test, w, alpha, iteration=10, batch_size=None,\n",
    "):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = np.zeros((iteration+1, 2))\n",
    "    ws[0] = w[:, 0]\n",
    "    mse_train = [mean_squared_error(y_train, x_train @ w)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, x_test @ w)]\n",
    "    prediction = {(w[0][0], w[1][0]): x_train @ w}\n",
    "\n",
    "    print(f\"Iteration 0: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "          f\"Loss_train = {mse_train[0]:.2f}, \"\n",
    "          f\"Loss_test = {mse_test[0]:.2f}.\")\n",
    "\n",
    "    for i in range(iteration):\n",
    "        if not batch_size:\n",
    "            x_sample = x_train\n",
    "            y_sample = y_train\n",
    "        else:\n",
    "            indxs = np.random.choice(x_train.shape[0], batch_size)\n",
    "            x_sample = x_train[indxs, :]\n",
    "            y_sample = y_train[indxs, :]\n",
    "\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_sample, y_sample, w)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws[i+1] = w[:, 0]\n",
    "        mse_train.append(mean_squared_error(y_train, x_train @ w))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, x_test @ w))\n",
    "        prediction[(w[0][0], w[1][0])] = x_train @ w\n",
    "        if (i+1)%10==0:\n",
    "            print(f\"Iteration {i+1}: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "                  f\"Loss_train = {mse_train[i]:.2f}, \"\n",
    "                  f\"Loss_test = {mse_test[i]:3.2f}.\")\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сравнить результаты будем использовать одно и то же количество итераций и скорость обучения. Чтобы компенсировать стохастичность возьмем маленькое значение $\\alpha$ и 100 итераций.\n",
    "\n",
    "Для всего train сета мы посчитаем градиент для $20\\cdot100 = 2000$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[57.0],[33.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = stochastic_gradient_descent(\n",
    "    x_train_scaled, y_train, x_test_scaled, y_test, w, 0.02, iteration=100,\n",
    "    batch_size=None,\n",
    ")\n",
    "\n",
    "f1 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1], y_train[:, 0], ws, slopes, intercepts, mode=\"lines\",\n",
    "    title=\"Batch gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для стохастического градиентного спуска (размер батча 1) мы посчитаем градиент для $1\\cdot100 = 100$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([[57.0],[33.0]])\n",
    "ws_stohastic, prediction, mse_train, dmse_train, mse_test = \\\n",
    "    stochastic_gradient_descent(\n",
    "        x_train_scaled, y_train, x_test_scaled, y_test, w, 0.02, iteration=100,\n",
    "        batch_size=1,\n",
    "    )\n",
    "f2 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1], y_train[:, 0], ws_stohastic, slopes, intercepts,\n",
    "    mode=\"lines\", title=\"Stochastic gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для стохастического спуска с mini-bach = 5 мы посчитаем градиент для $5\\cdot100=500$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([[57.0],[33.0]])\n",
    "ws_mini_batch, prediction, mse_train, dmse_train, mse_test = \\\n",
    "    stochastic_gradient_descent(\n",
    "        x_train_scaled, y_train, x_test_scaled, y_test, w, 0.02, iteration=100,\n",
    "        batch_size=5,\n",
    "    )\n",
    "f3 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1], y_train[:, 0], ws_mini_batch, slopes, intercepts,\n",
    "    mode=\"lines\", title=\"Mini-batch gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ищем минимум для всех данных.\n",
    "- Градиент, рассчитанный по одному объекту, будет специфичен. Трек обучения  в случае стохастического градиентного спуска будет запутанным, а итоговая ошибка будет расти с увеличением скорости обучения (мы взяли низкую скорость).\n",
    "- Градиент, рассчитанный по нескольким объектам будет давать лучшую оценку градиента для всех данных. Трек будет менее сложным.\n",
    "- Градиент, рассчитанный по всей выборкеб будет давать наиболее точное направление."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_panel(f1, f2, f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для **ускорения расчетов** рекомендовано использовать **максимальных размер mini-batch**, который помещается в память, но это не всегда дает лучший результат. На 7 лекции вы увидите, что для сложных моделей стохастичность, связанная с небольшим размером батча, может помочь выбраться из локального минимума и найти более глубокий.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Численный расчет производной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы знаем как **считать производную аналитически**, зная исходную функцию и используя таблицу производных. Но нам может быть полезно **оценить производную в точке**, ничего не зная о функции. Это часто используют для **тестирования правильности кода** для новых рукописных функций.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/gradient_descent_analytical_calculation.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем рассчитать градиент приближенно, воспользовавшись определением (в формуле аргумент обозначен как $x$, у нас же аргументом будет $W$):\n",
    "На нулевом шаге у нас есть $W_0$ найдем $L_0 = Loss(f(W_0,x))$\n",
    "Прибавим к первому элементу $W_0$ небольшую величину  $h$ = 0.0001 и получим новую матрицу весов $W_1$ отличающуюся от $W_0$ на один единственный элемент.\n",
    "\n",
    "Найдем Loss от $\\frac {W_1}  {L_1} = Loss(f(W_1,x))$\n",
    "По определению производной $\\frac {dL}{W_0} = \\frac{( L_1 - L_0 )}   {h}$\n",
    "\n",
    "Повторяя этот процесс для каждого элемента из $W$, найдем вектор частных производных, то есть градиент $\\frac{dL}{dW}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему мы будем считать производную **аналитически, а не численно**:\n",
    "\n",
    "1. Считать производную численно очень **долго**. Нам придется заново искать значение loss функции для каждого $W_i$.\n",
    "\n",
    "2. Считать производную численно **неточно**, так как по определению приращение $h$ бесконечно мало, а мы используем конкретное, пусть и небольшое число. И если мы сделаем его слишком маленьким, то столкнемся с ошибками, связанными с округлением в памяти компьютера.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем разобраться, что делать в случае случае классификации.\n",
    "\n",
    "Рассмотрим задачу классификации на два класса, например у нас есть данные о лабораторных мышах. Часть из них имеет нормальную массу тела, а часть - мыши с ожирением.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/svm_mouse_example.png\" width=\"800\">\n",
    "\n",
    "\n",
    "У нас есть:\n",
    "1. Набор данных из $N$ объектов (мышей).\n",
    "2. Для каждого из объекта (мыши) нам известно признаковое описание объекта в виде набора вещественных чисел (вес, длина от носа до кончика хвоста, возраст и т.д.). То есть объекту под номером $i$ соответствует вектор $\\vec x_i$.\n",
    "3. Также для каждого объекта нам известна истинная метка класса. Мы знаем что объекту с признаковым описанием $\\vec x_i$ соответствует метка класса $y_i$. Будем считать, что метки классов принимают значения $$y_i =\n",
    "\\begin{cases}\n",
    "    +1, & \\text{для толстеньких}, \\\\\n",
    "    -1, & \\text{для нормальных}.\n",
    "\\end{cases}$$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "В задаче регрессии мы использовали метрику $MSE$ в качестве функции потерь. Здесь мы можем сделать что-то подобное.\n",
    "\n",
    "Мы можем ввести пороговое решающее правило: определять метку класса по знаку линейной функции (случай $x = 0$ можем отнести к любому классу):\n",
    "\n",
    "$$y_i^{pred} = sign(\\vec{w}\\vec{x_i}+b)$$\n",
    "\n",
    "где $sign$ - сигнум функция знака.\n",
    "\n",
    "$$sign(x) =\n",
    "\\begin{cases}\n",
    "    +1, & x>0, \\\\\n",
    "    0 & x=0,\\\\\n",
    "    -1, & x<0.\n",
    "\\end{cases}$$\n",
    "\n",
    "Используя такое решающее правило мы можем посчитать метрику $accuracy$:\n",
    "\n",
    "$$accuracy=\\frac{\\sum_{i=1}^N [sign(\\vec{w}\\vec{x_i}+b)==y_i]}{N}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно максимизировать $accuracy$, а значит минимизировать $1-accuracy$. Действуя по аналогии с задачей регрессии мы могли бы задать функцию потерь следующим образом:\n",
    "$$Loss = 1-accuracy =  \\frac{\\sum_{i=1}^N \\overline{[sign(\\vec{w}\\vec{x_i}+b)==y_i]}}{N} = \\frac{\\sum_{i=1}^N l_i}{N}$$\n",
    "\n",
    "$$l_i = \\overline{[sign(\\vec{w}\\vec{x_i}+b)==y_i]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция $l_i$ будет представлять собой ступеньку (1 - там, где мы ошиблись и 0 - где класс определен правильно). Это плохо, т.к производная такой функции будет равна нулю почти везде, а это значит у нас будут проблемы с поиском минимума.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/hinge_loss.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем модифицировать функцию потерь задав оценку сверху для $l_i$ полученного из $accuracy$.\n",
    "\n",
    "$$\\large l_i = \\max(0, 1 - y_i ((\\vec w, \\vec x_i) + b ))$$\n",
    "\n",
    "Данная модификация будет входит в [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss):\n",
    "$$\\large Loss = \\frac{1}{2}||w||^2 + C\\frac{\\sum_{i=1}^N l_i}{N} $$\n",
    "\n",
    "где $C$ - обратный коэффициент регуляризации, гиперпараметр, значение по умолчанию в `sklearn`: `C=1.0`.\n",
    "\n",
    "Пока не очень понятно почему появился член с $w^2$, но он очень важен. Чтобы понять его назначение - рассмотрим задачу геометрически."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим **одномерный пример**. У нас есть данные только по **массе мышей**. Часть из них определена как мыши с нормальной массой тела, а часть — как мыши с ожирением.\n",
    "\n",
    "Чтобы их отделить друг от друга, нам достаточно одного критерия. Мы можем посмотреть на график и визуально определить предельную массу, после которой мышки будут жирненькими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "custom_cmap = ListedColormap(['#B8E1EC', '#bea6ff','#FEE7D0'])\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    x = np.hstack(\n",
    "        [\n",
    "            np.random.uniform(14, 21, total_len // 2),\n",
    "            np.random.uniform(24, 33, total_len // 2),\n",
    "        ]\n",
    "    )\n",
    "    y = np.hstack([np.zeros(total_len // 2), np.ones(total_len // 2)])\n",
    "    return x, y\n",
    "\n",
    "def plot_data_1d(x, y, total_len=40, s=50, threshold=None, margin=None,\n",
    "                 legend=[\"Normal\", \"Obese\"], marker='o'):\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s, marker=marker)\n",
    "    if threshold:\n",
    "        x_lim,  y_lim = ax.get_xlim(), ax.get_ylim()\n",
    "        XX, YY = np.meshgrid(np.linspace(x_lim[0], x_lim[1], 100),\n",
    "                             np.linspace(y_lim[0], y_lim[1], 100))\n",
    "        pred = np.sign(XX - threshold)\n",
    "        plt.contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "        ax.axvline(threshold, color=\"grey\")\n",
    "    if margin:\n",
    "        for line in margin:\n",
    "            ax.axvline(line, color=\"grey\", ls=\"dashed\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, legend)\n",
    "    ax.set(xlabel=\"Mass, g\")\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "np.random.seed(42)\n",
    "x, y = generate_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data_1d(x, y, threshold=21.5, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, пользуясь нашим простым критерием, попробуем классифицировать каких-то новых (тестовых) мышей $\\color{orange}{✭}$ $\\color{blue}{✭}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.random.uniform(14, 30, 5)\n",
    "\n",
    "def classify(x, threshold=21.5):\n",
    "    y = np.zeros_like(x)\n",
    "    y[x > threshold] = 1\n",
    "    return y\n",
    "\n",
    "total_len = 40\n",
    "threshold = 21.5\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data_1d(x, y, threshold=threshold, total_len=total_len)\n",
    "ax = plot_data_1d(x_test, classify(x_test, threshold), total_len=total_len,\n",
    "                  s=500, marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из тестовых мышей была классифицирована, как толстенькая ($\\color{orange}{✭}$ на границе), хотя она ближе по массе к мышам без ожирения из обучающей выборки $\\color{blue}{●}$. Не порядок!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вооружившись этим новым знанием, попробуем классифицировать наших отъевшихся мышек по-умному. Возьмем крайние точки в каждом кластере. И в качестве порогового значения будем использовать среднее между ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_limit = x[y == 0].max()  # extreme point for 'normal'\n",
    "obese_limit = x[y == 1].min()  # extreme point for 'obese'\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit])  # separated with mean value\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data_1d(x, y, total_len=total_len, threshold=threshold,\n",
    "                  margin=[normal_limit, obese_limit])\n",
    "ax = plot_data_1d(x_test, classify(x_test, threshold=threshold),\n",
    "                  total_len=total_len, s=500, marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать, насколько наша мышь близка к тому, чтобы оказаться в другом классе. Такое расстояние называется **margin**. И оно считается как $\\mathrm{margin} = |\\mathrm{threshold} - \\mathrm{observation}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = np.abs(x_test - threshold)\n",
    "print(margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если мы посчитаем margins для наших крайних точек `normal_limit` и `obese_limit`, мы найдем самое большое возможное значение margin для нашего классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_0 = np.abs(normal_limit - threshold)\n",
    "margin_1 = np.abs(obese_limit - threshold)\n",
    "print(margin_0, margin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой классификатор мы называем **Maximum Margin Classifier**. Он хорошо работает в случае, когда классы не пересекаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример, где мы измерили не только вес мышей, но и их длину от хвоста до носа. Теперь не очевидно, какие точки кластеров у нас являются крайними и как провести разделяющую прямую, чтобы классы были максимально разнесены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def generate_2d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42)\n",
    "    x[:, 0] += 10\n",
    "    x[:, 1] += 20\n",
    "    return x, y\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_2d_data(total_len=total_len)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, [\"Normal\", \"Obese\"])\n",
    "ax.set(xlabel=\"Mass, g\", ylabel=\"Length, cm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: Hard and Soft Margin Classifier\n",
    "\n",
    "Попробуем подойти к задаче с другой стороны (мы немного упростили визуализацию для удобства восприятия, но с мышками будет так же)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим провести разделяющую гиперплоскость таким образом, чтобы:\n",
    "1. Плюсы и минусы лежали по разные стороны от этой плоскости.\n",
    "2. Ближайшие к плоскости объекты были от нее как можно дальше.\n",
    "\n",
    "Второе условие дает нам максимальный зазор (*margin*), который мы тривиально находили в одномерном случае.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/svm_hard_margin.png\" width=\"1000\">\n",
    "\n",
    "Гиперплоскость однозначно задается вектором нормали $\\vec w$ и смещением $b$. Мы ищем решение в виде $(\\vec w, \\vec x) + b$, где $(\\vec w, \\vec x)$ - это скалярное произведение, $\\vec w$ - вектор весов, а $b$ - смещение. Скалярное произведение вектора признаков на вектор нормали будет давать проекцию вектора признаков на вектор нормали.  Мы не будем делать вектор $\\vec w$ единичным, вместо этого мы зафиксируем margin на проекции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого:\n",
    "1. Мы хотим подобрать такие $\\vec  w$ и $b$, чтобы можно было провести такие гиперплоскости:\n",
    "$$\\large (\\vec w, \\vec x) + b  = 1$$\n",
    "\n",
    "- **Лежащие на этой плоскости и выше объекты относятся к классу $+1$:** $\\large (\\vec w, \\vec x_+) + b  \\ge 1$\n",
    "\n",
    "$$\\large (\\vec w, \\vec x) + b  = -1$$\n",
    "\n",
    "- **Лежащие на этой плоскости и ниже объекты относятся к классу $-1$:** $\\large (\\vec w, \\vec x_-) + b  \\le -1$\n",
    "\n",
    "\n",
    "2. Мы хотим разнести эти плоскости как можно дальше. Расстояние между двумя этими **жесткими** границами можно расписать через проекции **опорных** (лежащих на плоскости) **векторов**: $(\\vec x_{sv+} - \\vec x_{sv-})\\frac{\\vec w}{||\\vec w||} = \\frac{2}{||\\vec w||}$\n",
    "\n",
    "Метод использующий **проекции опорных векторов для определения разделяющей гиперплоскости называется SVM** (*support vector machine*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Условие того, что  $i$-й объект лежит по правильную сторону от разделяющих поверхностей можно записать в совместно:\n",
    "\n",
    "$$\\large y_i ((\\vec w, \\vec x_i) + b )\\ge 1,$$\n",
    "\n",
    "которое должно выполняться для всех объектов $1 \\le i \\le N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди всех решений $\\vec w$ и $b$, которые удовлетворяют условию выше, мы хотим подобрать такое, при котором пороговые разделяющие поверхности будут находится дальше всего. Так как расстояние между ними равно $\\frac{2}{||\\vec w||}$, мы приходим к следующей задаче на экстремум\n",
    "\n",
    "$$max \\frac{2}{||\\vec w||} \\Leftrightarrow min \\frac{1}{2} ||\\vec w||^2$$\n",
    "\n",
    "при условии\n",
    "$$\\large y_i ((\\vec w, \\vec x_i) + b )\\ge 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобная задача на условный экстремум для линейно разделимых классов может быть решена аналитически при помощи [метода множителей Лагранжа](https://en.wikipedia.org/wiki/Lagrange_multiplier):\n",
    "\n",
    "Найти $\\alpha_i$, $\\vec w$ и $b$, которые реализуют минимум функции потерь:\n",
    "\n",
    "$$\\large L =  \\frac{1}{2} ||\\vec w||^2 + \\sum_i \\alpha_i [y_i ((\\vec w, \\vec x_i) + b) - 1]$$\n",
    "\n",
    "$\\alpha_i\\geq0$ - множитель Лагранжа. Он будет не равен нулю только для **опорных векторов**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае неразделимости классов, написанная выше формула переходит в **Hinge Loss**, $\\frac{1}{2} ||\\vec w||^2$  котором отвечает за максимальное разнесение классов. **SVM** c **Hinge Loss** называют **Soft Margin Classifier**.\n",
    "\n",
    "Применим к мышкам ним метод `svm` из библиотеки `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Code for illustration, later we will understand how it works\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel=\"linear\", C=1000)\n",
    "clf.fit(x, y)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,  figsize=(10, 3))\n",
    "\n",
    "# first fig\n",
    "sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=axs[0])\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].legend(handles, [\"Normal\", \"Obese\"])\n",
    "axs[0].set(xlabel=\"Mass, g\", ylabel=\"Length, cm\")\n",
    "\n",
    "# plot the decision function\n",
    "delta = 0.5\n",
    "# create grid to evaluate model\n",
    "YY, XX = np.meshgrid(np.linspace(x[:, 1].min()-delta, x[:, 1].max()+delta, 30),\n",
    "                     np.linspace(x[:, 0].min()-delta, x[:, 0].max()+delta, 30))\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "pred = np.sign(Z)\n",
    "axs[0].contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "# plot decision boget_xlimundary and margins\n",
    "axs[0].contour(\n",
    "    XX, YY, Z, colors=\"k\", levels=[-1, 0, 1], alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"]\n",
    ")\n",
    "# plot support vectors\n",
    "axs[0].scatter(\n",
    "    clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "    s=100, linewidth=1, facecolors=\"none\", edgecolors=\"k\"\n",
    ")\n",
    "\n",
    "# second fig\n",
    "dec_val = clf.decision_function(x)\n",
    "sns.scatterplot(x=dec_val, y=np.zeros(len(x)), hue=y, ax=axs[1])\n",
    "\n",
    "x_lim, y_lim  = axs[1].get_xlim(), axs[1].get_ylim()\n",
    "XX, YY = np.meshgrid(np.linspace(x_lim[0], x_lim[1], 100),\n",
    "                     np.linspace(y_lim[0], y_lim[1], 100))\n",
    "pred = np.sign(XX)\n",
    "axs[1].contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "axs[1].axvline(0, color=\"grey\")\n",
    "axs[1].axvline(-1, color=\"grey\", ls=\"dashed\")\n",
    "axs[1].axvline(1, color=\"grey\", ls=\"dashed\")\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(handles, [\"Normal\", \"Obese\"])\n",
    "axs[1].set(xlabel=\"wx+b\")\n",
    "\n",
    "sv = clf.decision_function(clf.support_vectors_)\n",
    "axs[1].scatter(sv, np.zeros_like(sv), s=100, linewidth=1,\n",
    "               facecolors=\"none\", edgecolors=\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видео с хорошим объяснением [SVM](https://youtu.be/_PwhiWxHK8o)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы добавим еще одно измерение — возраст, мы обнаружим, что наши данные стали трехмерными, а разделяет их теперь не линия, а плоскость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42, n_features=3)\n",
    "    x[:, 0] += 10\n",
    "    x[:, 1] += 20\n",
    "    x[:, 2] += 10\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50, threshold=21.5):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(xs=x[:, 0], ys=x[:, 1], zs=x[:, 2], c=y, s=s, cmap=\"tab10\",\n",
    "               vmin=0, vmax=9)\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    ax.plot_surface(XX, YY, XX * YY * 0.2, alpha=0.2)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.set(xlabel=\"Mass, g\", ylabel=\"Length, cm\", zlabel=\"Age, days\")\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_3d_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если бы у нас было 4 измерения и больше (например: вес, длина, возраст, кровяное давление), то многомерная плоскость, которая бы разделяла наши классы, называлась бы **гиперплоскость** (рисовать мы ее, конечно же, не будем). Чисто технически, и точка, и линия — тоже гиперплоскости. Но все же гиперплоскостью принято называть то, что нельзя нарисовать на бумаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение задачи SVM, которое мы рассматривали касалось задачи бинарной классификации. Мы часто будем работать с несколькими классами.\n",
    "\n",
    "Есть две основных стратегии расширения задачи SVM классификации с двух классов на несколько:\n",
    "* **one vs rest** (один против всех): каждый класс отделяется от всех других одной прямой (гиперплоскостью).\n",
    "* **one vs one** (один против одного): классы попарно отделяются друг от друга прямыми (гиперплоскостями).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датасет из 4 классов, для демонстрации отличий между этими способами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "centers = [[1, 1], [1, -1], [-1, -1], [-1, 1]]\n",
    "\n",
    "x, y = make_blobs(\n",
    "    n_samples=300, centers=centers, cluster_std=0.50, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "dark_colors = ['#1B1464','#0961A5', '#754C24', '#006837']\n",
    "bright_colors = ['#5D5DA6', '#2DA9E1', '#F9B041','#4AAE4D']\n",
    "dull_cmap = ListedColormap(['#D1D5ED','#B8E1EC','#FEE7D0','#C9E3C8'])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,  figsize=(5, 5))\n",
    "\n",
    "# first fig\n",
    "sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=ax,\n",
    "                palette=sns.color_palette(bright_colors))\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, [\"0\", \"1\", \"2\", \"3\"])\n",
    "ax.set(xlabel=\"feature 1\", ylabel=\"feature 2\")\n",
    "\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-2.5, 2.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs Rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация **one vs rest** реализована в `sklearn` в классе `svm.LinearSVC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как выглядят разделяющие прямые и нормали к ним для нашей задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,  figsize=(5, 5))\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    x,\n",
    "    response_method=\"predict\",\n",
    "    cmap=dull_cmap,\n",
    "    alpha=0.8,\n",
    "    xlabel=\"feature 1\",\n",
    "    ylabel=\"feature 2\",\n",
    "    ax = ax,\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=ax,\n",
    "                palette=sns.color_palette(bright_colors))\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(-2.5, 2.5)\n",
    "# for visualization\n",
    "arrow_xs = [0.5, 0.5, -0.5, -0.5]\n",
    "for i in range(clf.coef_.shape[0]):\n",
    "    coef = clf.coef_[i]\n",
    "    w = - coef[0]/coef[1]\n",
    "    b = - clf.intercept_[0]/coef[1]\n",
    "    yy = w * xx + b\n",
    "    # normal\n",
    "    plt.arrow(arrow_xs[i], w * arrow_xs[i] + b , coef[0]/4,  coef[1]/4,\n",
    "              edgecolor=dark_colors[i], facecolor=bright_colors[i],\n",
    "              width=.04)\n",
    "    # dividing line\n",
    "    plt.plot(xx, yy, dark_colors[i])\n",
    "\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-2.5, 2.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициенты `clf.coef_` - возвращают вектор нормали. С помощью `clf.coef_` и `clf.intercept_` можно записать уравнение разделяющей прямой.\n",
    "\n",
    "Для 4 классов стратегия **one vs rest** даст 4 разделяющих прямых (гиперплоскости). Количество разделяющих прямых равно количеству классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество модификаций SVM Loss для решения многоклассовой классификации. В литературе (и в задании) вы можете встретить следующую формулировку Loss для SVM задачи:\n",
    "\n",
    "$$ Loss = \\frac{1}{2} ||\\vec w||^2 + {1 \\over N}\\sum_iL_i(f(x_i,W),y_i),$$\n",
    "\n",
    "$$L_i = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$$\n",
    "\n",
    "где $s_j = f(x_i, W)_j$ - уравнение для $j$-го класса. $s_{y_i}$ - значение уравнения для истинного класса. Идея данной формулы аналогична one vs rest, но вместо абсолютных значений используется разница между предсказаниями для различных классов.\n",
    "\n",
    "Это формулировка появилась в [статье Weston and C. Watkins 1999 года](https://www.esann.org/sites/default/files/proceedings/legacy/es1999-461.pdf) и стала популянрной благодаря [cтендфорскому курсу](https://cs231n.github.io/linear-classify/#svm), но для нее нет реализации в `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стратегия **one vs rest** позволяет обучать меньшее количество классификаторов, чем **one vs one**, но при **большом количестве классов** могут появляться проблемы, связанные с **сильным дисбалансом классов** при решении задачи “один против всех”. При большом количестве классов лучше использовать **one vs one** стратегию.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй стратегией многоклассовой классификации для SVM является **one vs one**, в которой классы разделяются попарно. Эта стратегия реализована в классе `svm.SVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,  figsize=(5, 5))\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    x,\n",
    "    response_method=\"predict\",\n",
    "    cmap=dull_cmap,\n",
    "    alpha=0.8,\n",
    "    xlabel=\"feature 1\",\n",
    "    ylabel=\"feature 2\",\n",
    "    ax = ax,\n",
    ")\n",
    "\n",
    "# Plot the training points\n",
    "sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=ax,\n",
    "                palette=sns.color_palette(bright_colors))\n",
    "\n",
    "# for visualization\n",
    "arrow_xs = [1, -0.1, 0, -0.17, -0.17, -1]\n",
    "colors_list = [(0, 1), (0, 2), (0, 3),\n",
    "               (1, 2), (1, 3), (2, 3)]\n",
    "range_list = [(0, 2.5), (-0.3, 0.1), (-0.1, 0.5),\n",
    "              (-1, -0.12), (-0.4, 0), (-2.5, 0)]\n",
    "\n",
    "for i in range(clf.coef_.shape[0]):\n",
    "    xx = np.linspace(*range_list[i])\n",
    "    coef = clf.coef_[i]\n",
    "    w = - coef[0]/coef[1]\n",
    "    b = - clf.intercept_[0]/coef[1]\n",
    "    yy = w * xx + b\n",
    "    # normal\n",
    "    plt.arrow(arrow_xs[i], w * arrow_xs[i] + b , coef[0]/4,  coef[1]/4,\n",
    "              edgecolor=dark_colors[colors_list[i][0]],\n",
    "              facecolor=bright_colors[colors_list[i][0]],\n",
    "              width=.04)\n",
    "    # dividing line\n",
    "    plt.plot(xx, yy, dark_colors[colors_list[i][1]])\n",
    "\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-2.5, 2.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для 4 классов стратегия **one vs one** даст 6 разделяющих прямых (гиперплоскости). Количество разделяющих прямыx:\n",
    "$$ \\frac{n_{classes}\\cdot (n_{classes}-1)}{2}$$\n",
    "гдe $n_{classes}$ - колличество классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Практические советы по использованию SVM:**\n",
    "\n",
    "* SVM делает **геометрическое разделение данных**, поэтому для адекватной работы модели важна **нормализация**.\n",
    "* в случае **дисбаланса классов** полезно использовать параметры `class_weight` и  `sample_weight` подробнее об этом можно почитать по ссылке [по ссылке](https://scikit-learn.org/stable/modules/svm.html#unbalanced-problems).\n",
    "* SVM может давать хорошее решение при небольшом количестве данных, в этом случае стоит попробовать **различные ядра** (про ядра вы узнаете ниже).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обобщенные линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные не всегда могут быть **хорошо разделены (гипер)плоскостью**. Например, рассмотрим следующее: у нас есть данные по дозировке лекарства и 2 класса — пациенты, которые поправились, и те, которым лучше не стало."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_patients_data(total_len=40):\n",
    "    x = np.random.uniform(0, 50, total_len)\n",
    "    y = np.zeros_like(x)\n",
    "    y[(x > 15) & (x < 35)] = 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Sick\", \"Recover\"])\n",
    "    ax.set(xlabel=\"dose, mg\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, мы не можем найти такое пороговое значение, которое будет разделять наши классы на больных и здоровых, а, следовательно, и Support Vector Classifier работать тоже не будет.  Для начала давайте преобразуем наши данные таким образом, чтобы они стали 2-хмерными. В качестве значений по оси Y будем использовать дозу, возведенную в квадрат (**доза**$^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    ax = sns.scatterplot(x=x[0, :], y=x[1, :], hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Sick\", \"Recover\"])\n",
    "    ax.set(xlabel=\"Dose, mg\")\n",
    "    ax.set(ylabel=\"Dose$^2$\")\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "x_1, y = generate_patients_data(total_len=total_len)\n",
    "x_2 = x_1**2\n",
    "x = np.vstack([x_1, x_2])\n",
    "\n",
    "plot_data(x, y, total_len=40, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем вновь использовать Support Vector Classifier для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x, y, total_len=40, s=50)\n",
    "\n",
    "x_arr = np.linspace(0, 50, 50)\n",
    "xs = [x[0, :][y == 1].min(), x[0, :][y == 1].max()]\n",
    "ys = [x[1, :][y == 1].min(), x[1, :][y == 1].max()]\n",
    "\n",
    "# Calculate the coefficients.\n",
    "coefficients = np.polyfit(xs, ys, 1)\n",
    "\n",
    "# Let's compute the values of the line...\n",
    "polynomial = np.poly1d(coefficients)\n",
    "y_axis = polynomial(x_arr)\n",
    "\n",
    "# ...and plot the points and the line\n",
    "plt.plot(x_arr, y_axis, \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея **Kernel SVM** в том, что **мы можем перейти в пространство большей размерности в котором данные будут линейно разделимы**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут возникает резонный вопрос: **почему мы решили возвести в квадрат**? Почему не в куб? Или, наоборот, не извлечь корень? Как нам решить, какое преобразование использовать?\n",
    "\n",
    "И у нас есть вторая проблема — а если перейти надо в **пространство очень большой размерности**? В этом случае наши данные очень сильно **увеличатся в размере**.\n",
    "\n",
    "Комбинация двух проблем дает нам много сложности: надо **перебирать большое число возможных пространств большей размерности**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Обоснование Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако основная фишка **Support Vector Machine** состоит в том, что внутри он работает на скалярных произведениях. И можно эти **скалярные произведения** считать, **не переходя в пространство большей размерности**.\n",
    "\n",
    "Для этого SVM использует **Kernel Function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/svm_kernel_function.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы ввели $Loss$ для **Hard Margin Classifier**:\n",
    "\n",
    "$$\\large Loss =  \\frac{1}{2} ||\\vec w||^2 + \\sum_i \\alpha_i [y_i ((\\vec w, \\vec x_i) + b) - 1]$$\n",
    "\n",
    "$\\alpha_i\\geq0$ - множитель Лагранжа. Он будет не равен нулю только для **опорных векторов**.\n",
    "\n",
    "С добавлением некоторых математических ограничений эту формулу можно [переписать](https://www.geeksforgeeks.org/dual-support-vector-machine/) в **дуальной форме**:\n",
    "\n",
    "$$\\large Loss =  \\sum_i \\alpha_i+ \\sum_i \\sum_j \\alpha_i \\alpha_j\n",
    "y_i y_j (\\vec x_i, \\vec x_j) = \\sum_i \\alpha_i+ \\sum_i \\sum_j \\alpha_i \\alpha_j\n",
    "y_i y_j K(\\vec x_i, \\vec x_j)$$\n",
    "\n",
    "(для получения дуальной формы приравнивают нулю производные $\\frac {\\partial Loss} {\\partial w}$ и $\\frac {\\partial Loss} {\\partial b}$ подставляют в исходную формулу)\n",
    "\n",
    "где $(\\vec x_i, \\vec x_j)$ - скалярное произведение.\n",
    "`\n",
    "В этой формуле можно сделать **kernel trick** - заменить **скалярное произведение** на некоторую функцию от двух векторов, которую мы будем называть **Kernel function**.\n",
    "\n",
    "Важно заметить, что  **дуальная форма** записи для **многоклассовой классификации** возможна только в случае **one vs one**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры ядер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для демонстрации возможностей kerne SVM создадим датасет, который не разделяется линейными моделями. Для этого воспользуемся функцией `sklearn.datasets.make_circles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "x, y = make_circles(n_samples=500, factor=0.3, noise=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный датасет представляет собой две окружности с разными радиусами и общим центром, относящиеся к разным классам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1,  figsize=(5, 5))\n",
    "\n",
    "sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=ax,\n",
    "                palette=sns.color_palette(['#2DA9E1', '#F9B041']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию визуализации разделяющего правила для SVM модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "def plot_svm(x, y, clf):\n",
    "    dull_cmap = ListedColormap(['#B8E1EC','#FEE7D0'])\n",
    "    fig, ax = plt.subplots(1, 1,  figsize=(5, 5))\n",
    "\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        x,\n",
    "        response_method=\"predict\",\n",
    "        cmap=dull_cmap,\n",
    "        alpha=0.8,\n",
    "        xlabel=\"feature 1\",\n",
    "        ylabel=\"feature 2\",\n",
    "        ax = ax,\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=ax,\n",
    "                    palette=sns.color_palette(['#2DA9E1', '#F9B041']))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое ядро, которое мы рассмотрим - линейное, оно задается формулой:\n",
    "$$K(\\vec x_i, \\vec x_j) = (\\vec x_i, \\vec x_j)$$\n",
    "\n",
    "Линейным ядром является скалярное произведение векторов.\n",
    "\n",
    "Линейное ядро не способно справиться с такой задачей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее ядро, реализованное в библиотеке `sklearn` - полиномиальное, оно задается формулой:\n",
    "$$K(\\vec x_i, \\vec x_j) = (\\gamma (\\vec x_i, \\vec x_j)+r)^d$$\n",
    "где $d$ - настраиваемый параметр: степень полинома `degree`.\n",
    "Попробуем применить полиномиальное ядро к нашим данным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"poly\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У модели не получилось разделить данные, это связано с тем, что значение `degree` по-умолчанию равно 3. Попробуем увеличить степень полинома:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"poly\", degree=4)\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод который тут стоит сделать - для получения **оптимального результата** бывает полезным **настроить параметры ядра** с учетом даных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым  популярным ядром SVM является ядро радиальных базисных функций RBF, или гауссово ядро. Оно получено из гауссова  распределения, а гаусово распеление характерно для большого количества измеряемых величин. Данное ядро задается формулой:\n",
    "\n",
    "$$K(\\vec x_i, \\vec x_j) = e^{-\\gamma{||\\vec x_i - \\vec x_j||^2}}$$\n",
    "\n",
    "\n",
    "Настраиваемыми параметрами модели являются `C` и `gamma`, `C` - определяет степень гладкости поверхности принятия решений, чем больше `C` - тем сложнее поверхность и **выше вероятность переобучения** (про переобучение поговорим ниже). `gamma` - определяет  влияние одного обучающего примера [подробнее.](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel)\n",
    "\n",
    "\n",
    "SVM может проверять пространства признаков бесконечного размера, если для такого пространства существует kernel function. RFB ядро как раз [соответствует](https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf) такому случаю бесконечномерного пространства признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"rbf\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в `sklearn` реализовано `sigma` ядро. Оно интересно больше с исторической точки зрения, т.к. эквивалентно модели нейрона - Перцептрону, о котором вы узнаете на 5-й лекции. [На практике](https://home.work.caltech.edu/~htlin/publication/doc/tanh.pdf) оно в большинстве случаев проигрывает RBF ядру.\n",
    "\n",
    "$$K(\\vec x_i, \\vec x_j) = tanh (\\gamma(\\vec x_i, \\vec x_j)+r)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"sigmoid\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практические особенности работы с линейными моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже обсудили зачем нужна нормализация данных для линейной модели, в данном пункте мы обсудим виды нормализации более подробно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30 признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer()  # load data\n",
    "\n",
    "x = cancer.data  # features\n",
    "y = cancer.target  # labels(classes)\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "print(f\"x[0]: \\n {x[0]}\")\n",
    "print(f\"y[0]: \\n {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстрее и удобнее можно посмотреть на данные используя pandas. К тому же Colab добавил возможность визуализации данных (для этого можно тыкнуть синий значок диаграммы ▆ █ ▄  под таблицей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cancer_df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab делает не полную визуализацию признаков, но и на данных изображениях можно найти полезную **информацию о выбросах** (из графика **Values**), **плотности распределений** (из графика **Distributions**) и о наличии **зависимости между переменными** (из графика **2-d distributions**). Например, мы можем увидеть, что значения признаков *mean area* и *mean perimeter* имеют зависимость близкую к линейной, что не очень хорошо (почему - обсудим позже).\n",
    "\n",
    "Кроме того можно тыкнуть на рисунок, посмотреть и скопировать код визуализации, чтобы применить к другим данным или изменить под свои нужды.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк, в каждой из которой по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные диапазоны  значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.boxenplot(data=cancer_df, orient=\"h\", palette=\"Set2\", linewidth=0.4,\n",
    "                   flier_kws={'marker': 'o', 's': 3}, line_kws={'linewidth':1})\n",
    "ax.set(xscale=\"log\", xlim=(1e-4, 1e4), xlabel=\"Values\", ylabel=\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная модель представляет собой **сумму взвешенных признаков**. Если мы приведем признаки к **единому масштабу**, мы сможем **оценить их вклад в модель** по значениям весов. Кроме того, работать с признаками в одном диапазоне вычислительно удобно. Для этого будем использовать нормализацию.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализацией** называется процедура приведения входных данных к **общим значениям математических статистик**.\n",
    "\n",
    "Нормализация строит **взаимно однозначное соответствие** между некоторыми размерными величинами (которые измеряются в метрах, килограммах, годах и т. п.) и их безразмерными аналогами. Исходные значения **можно восстановить**, зная статистики оригинальных данных и правило по которому делалась нормализация.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто используют следующие варианты нормализации:  **`MinMaxScaler`**, **`StandardScaler`**, **`RobustScaler`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные из имеющегося диапазона значений в диапазон от 0 до 1. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (например, масштабирование RGB пикселей)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_i=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "$z_i$ — масштабированное значение,\n",
    "$X_i$ — текущее значение,\n",
    "$X_{min}$ и $X_{max}$ — минимальное и максимальное значения имеющихся данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение 0 и стандартное отклонение 1. Большинство значений будет в  диапазоне от -1 до 1. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_i=\\frac{X_i-u}{s}$$\n",
    "\n",
    "$u$ — среднее значение (или 0 при `with_mean=False`) и $s$ — стандартное отклонение (или 0 при `with_std=False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И `StandardScaler`, и `MinMaxScaler` чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. k-й процентиль — это величина, равная или не превосходящая k процентов чисел во всем имеющемся распределении. Например, 50-й процентиль (медиана) распределения таков, что 50% чисел из распределения не меньше данного числа. Соответственно, `RobustScaler` не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_i=\\frac{X_i-X_{median}}{IQR}$$\n",
    "\n",
    "$X_{median}$ — значение медианы, $IQR$ — межквартильный диапазон, равный разнице между 75-ым и 25-ым процентилями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для случайного набора признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "random_names = random.sample(list(cancer.feature_names), 8)\n",
    "cut_df = cancer_df[random_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "def plot_norm(df, ax, title):\n",
    "    sns.boxenplot(df, orient=\"h\", palette=\"Set2\",  ax=ax, linewidth=0.2,\n",
    "                  flier_kws={'marker': 'o', 's': 5}, line_kws={'linewidth':1})\n",
    "    ax.set(xlabel=\"Values\", title=title)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2,  figsize=(10, 7))\n",
    "\n",
    "plot_norm(cut_df, axs[0][0], \"Original\")\n",
    "axs[0][0].set(xscale=\"log\", xlim=(1e-4, 1e4))\n",
    "\n",
    "min_max_x = MinMaxScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(min_max_x, columns=random_names), axs[0][1], \"MinMax\")\n",
    "\n",
    "std_x = StandardScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(std_x, columns=random_names), axs[1][0], \"Standard\")\n",
    "\n",
    "rob_x = RobustScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(rob_x, columns=random_names), axs[1][1], \"Robust\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.55, hspace=0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед выбором нормализации важно разобраться с природой выбросов. Для этого на выбросы нужно посмотреть, с точки зрения эксперта и попробовать определить, являются ли выбросы ошибкой при сборе данных или редкими случаями, которые необходимо сохранить.\n",
    "\n",
    "Мы не являемся экспертами в медицине и мало знаем о данных, поэтому мы будем считать что наши признаки имеют распределение близкое к нормальному (мы можем сделать такое предположение из центральной предельной теоремы в теории вероятности). Поэтому мы будем использовать `StandardScaler`. `StandardScaler` часто используют как нормировку по-умолчанию.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = StandardScaler().fit_transform(cancer_df)  # scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что они стали намного более сравнимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_norm, columns=cancer.feature_names).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.boxenplot(data=pd.DataFrame(x_norm, columns=cancer.feature_names),\n",
    "                   orient=\"h\", palette=\"Set2\", linewidth=0.4,\n",
    "                   flier_kws={'marker': 'o', 's': 3}, line_kws={'linewidth':1})\n",
    "ax.set(xlabel=\"Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема корреляции признаков  в случае линейных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто может оказаться, что признаковое описание объекта избыточно и между различными признаками имеются связи. Для устойчивости работы линейных моделей важно, чтобы среди признаков не было скоррелированных пар.\n",
    "\n",
    "Например, если мы будем решать задачу регрессии на наборе признаков $x_1 \\dots x_n$ среди которых есть связь $x_2 = 5 x_1$ и возьмём линейную модель вида\n",
    "$$\\large y = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b,$$\n",
    "то с учётом данной связи мы можем записать:\n",
    "$$\\large y = w_1 x_1 + w_2 (5x_1) + \\dots + w_n x_n + b = (w_1 + 5 w_2) x_1 +  w_3 x_3 + \\dots + w_n x_n + b.$$\n",
    "\n",
    "Таким образом, наша модель теперь учитывает признак $x_1$ с одним \"общим\" весом $(w_1 + 5 w_2)$, не смотря на то что он закодирован двумя независимыми параметрами. Решение, то есть набор весовых коэффициентов $w_i$ перестало быть единственным, так как мы теперь можем делать произвольные преобразования с числами $w_1$ и $w_2$ до тех пор пока $(w_1 + 5 w_2)$ остаётся неизменным:\n",
    "\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000 ,\\, w_2 \\rightarrow  w_2 - 1000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000 ,\\, w_2 \\rightarrow  w_2 - 1000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000000 ,\\, w_2 \\rightarrow  w_2 - 1000000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000000000 ,\\, w_2 \\rightarrow  w_2 - 1000000000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + Nan ,\\, w_2 \\rightarrow  w_2 + Nan\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чем это плохо?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае корреляции признаков задача не имеет единственного решения и не существует обратной матрицы, обеспечивающей аналитическое решение. Мы можем использовать градиентные методы (поговорим позже), для поиска решения, но\n",
    "при этом веса модели могут неконтролируемо расти. При этом **суммарный вклад** признаков может быть **мал**.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/carrelation_problem1.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем оценивать **важность признаков в линейной модели**, используя **веса** перед ними (признаки должны быть нормализованы). Чем больше модуль веса, тем больше вклад. Для коррелированных признаков важность будет переоценена.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/carrelation_problem2.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, нужно помнить, что диапазоны числовых переменных ограничены. При неконтруемом росте весов, значение может выйти за диапазон и превратиться в ~~тыкву~~ `Nan`.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/carrelation_problem3.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Что делать?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализировать подобные зависимости можно при помощи построения матрицы попарных корреляций признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = cancer_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    vmax=1,\n",
    "    vmin=-1,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наших данных сильно скоррелированы данные о размерах опухоли (мы могли видеть это выше при визуализации 2-d distributions).\n",
    "\n",
    "В случае корреляции можно:\n",
    "- **добавить регуляризацию** (о том, что такое регуляризация мы поговорим дальше),\n",
    "- **оставить один признак**,\n",
    "- если есть сомнения, что при удалении признаков часть информации будет потеряна, можно оставить **один признак** неизменным и вычесть его из остальных (оставить только **разницу**). Неинформативные шумовые признаки можно удалить (как это делать вы узнаете на 4-й лекции).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Сложность модели__ (*model complexity*) — важный гиперпараметр. В частности, для линейной модели, сложность может быть представлена количеством параметров, для полиномиальных моделей — степенью полинома, для деревьев решений — глубиной дерева и т.д.\n",
    "\n",
    "Сложность модели тесно связана с __ошибкой обобщения__ (_generalization error_). Ошибка обобщения отличается от ошибки обучения, измеряемой на тренировочных данных, тем, что позволяет оценить обобщающую способность модели, приобретенную в процессе обучения, давать точные ответы на неизвестных ей объектах. Cлишком простой модели не будет хватать мощности для обобщения сложной закономерности в данных, что приводит к большой ошибке обобщения, с другой стороны слишком сложная модель также приводит к большой ошибке обобщения за счет того, что в силу своей сложности модель начинает пытаться искать закономерности в шуме, добиваясь большей точности на тренировочных данных, теряя при этом часть обобщающей способности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/model_complexity.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проиллюстрируем описанное явление на примере полиномиальной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2 * np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "x_true = np.linspace(0, 2 * np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\", label=\"noisy data\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем аппроксимировать имеющуюся зависимость с помощью полиномиальной модели, используя шумные данные в качестве тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train = x.reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i, degree in enumerate([0, 1, 3, 9]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "    fig.add_subplot(2, 2, i + 1)\n",
    "    plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "    plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "    plt.plot(x_true, y_true, c=\"lime\")\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель может переобучаться, подстраиваясь под тренировочную выборку. В полиноме степень, и, как следствие, количество весов — это гиперпараметр, который можно подбирать на кросс-валидации, однако когда мы таким образом подбираем сложность модели, мы накладываем довольно грубое ограничение на обобщающую способность модели в целом. Вместо этого более разумным было бы оставить модель сложной, но использовать некий ограничитель (__регуляризатор__), который будет заставлять модель отдавать предпочтение выбору более простого обобщения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(9), LinearRegression())\n",
    "model_ridge = make_pipeline(PolynomialFeatures(9), Ridge(alpha=0.1))\n",
    "\n",
    "model.fit(x_train, y)\n",
    "y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "model_ridge.fit(x_train, y)\n",
    "y_plot_ridge = model_ridge.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "plt.plot(x_true, y_plot_ridge, c=\"black\", label=f\"M={degree}, alpha=0.1\")\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "poly_coef = model[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"Without regularization: \", eq)\n",
    "\n",
    "poly_coef = model_ridge[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"With regularization: \", eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что одним из \"симптомов\" переобучения являются аномально большие веса. Модель Ridge Regression, показанная в примере выше, использует L2-регуляризацию для борьбы с этим явлением:\n",
    "\n",
    "$$Loss_{L_2} = Loss + \\alpha \\sum_i w_i^2$$\n",
    "где $\\alpha$ - это коэффициент регуляризации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/l2_regularization.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея состоит в том, что мы можем наложить некоторое **требование на сами веса**.\n",
    "\n",
    "Параметры модели задают некоторую **аппроксимацию целевой функции**. Аппроксимировать целевую функцию можно несколькими способами, например:\n",
    "1. Использовать все имеющиеся данные и провести ее строго **через все точки**, которые нам известны ($f1$ на картинке);\n",
    "2. Использовать более простую функцию (в данном случае, линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым **общим закономерностям**, которые у них есть ($f2$ на картинке)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Характерной чертой переобучения является первый сценарий, и сопровождается он, как правило, **большими весами**. Введение **L2-регуляризации** приводит к тому, что **большие веса больше штрафуются** и предпочтение отдается решениям, использующим **малые значения весов**. При этом модель будет **сохранять скоррелированные и неважные признаки с маленькими весами**.\n",
    "\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/l1_and_l2_regularization.gif\" alt=\"alttext\" width=\"550\"/></center>\n",
    "\n",
    "Для отбора признаков можно использовать L1-регуляризацию, она больше штрафует маленькие веса.\n",
    "\n",
    "$$Loss_{L_1} = Loss + \\alpha \\sum_i |w_i|$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения интуиции, что L1-регуляризация позволяет отбирать признаки обычно используют картинку ниже. Голубая область - ограничение на значения весов, которое дает регуляризация.\n",
    "\n",
    "Голубая область - ограничение на значения весов, которое дает регуляризация. Для **L2** - это **окружность**. Черная точка - это минимальное значение для функции Loss с регуляризацией. Для **L2** она будет лежать **на касательной к окружности**. Для **L1** - ограничения на значения весов будут иметь **форму ромба**. При этом минимальное значение для функции Loss с регуляризацией будет чаще попадать в **угол ромба**, что соответствует **обнулению веса** одного из признаков.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/l1_l2_regularization.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вероятностный подход в задаче классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный Байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно построить модель классификации, которая будет напрямую оценивать вероятность принадлежности объекта к интересующему нас классу просто на основе информации о распределении объектов по классам в обучающей выборке. Базовую идею такого примера легко продемонстрировать на следующем примере данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Табличные данные: датасет Wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим `DataFrame` датасета [Wine](https://archive.ics.uci.edu/ml/datasets/wine), который являлся примером табличных данных в нашей первой лекции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine\n",
    "\n",
    "# Download dataset\n",
    "features, class_labels = load_wine(\n",
    "    return_X_y=True, as_frame=True\n",
    ")  # also we can get data in Bunch (dictionary) or pandas DataFrame\n",
    "\n",
    "wine_dataset = features\n",
    "wine_dataset[\"target\"] = class_labels\n",
    "\n",
    "wine_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "наш датасет содержит объекты 3 различных классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём первый признак `alcohol`. По имеющийся таблице с данными легко построить функцию распределения $f(x)$, которая будет задавать вероятность $p(\\text{alcohol} = x)$ найти среди наших данных бутылку вина с параметром `alcohol` равным $x$ (рисунок слева).\n",
    "\n",
    "Т.к. у нас три класса, мы можем построить распределение объектов в обучающей выборке по признаку `alcohol` отдельно для каждого из этих трёх классов. Эти распределения зададут нам условную вероятность $p(\\text{alcohol} = x |\\text{target} = i)$ того что объект имеет значение признака `alcohol` равным $x$ при условии, что он относится к одному из классов с номером $i$ (рисунок справа)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.kdeplot(wine_dataset, x=\"alcohol\",\n",
    "            fill=True, ax=axes[0])\n",
    "axes[0].set_title(\"p(alcohol=x)\")\n",
    "\n",
    "sns.kdeplot(wine_dataset, x=\"alcohol\", hue=\"target\",\n",
    "            palette=sns.color_palette(['#5D5DA6', '#2DA9E1', '#F9B041']),\n",
    "            fill=True,\n",
    "            ax=axes[1])\n",
    "axes[1].set_title(\"p(alcohol=x|target=i)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрев на плотности распределений по классам (рисунок справа), мы можем предположить, что бутылка с значением $alcohol=11.3$ будет относиться к 1 классу.\n",
    "\n",
    "На языке формул наш “метод пристального вглядывания” можно записать с помощью формулы для условной вероятности по [теореме Байеса](https://en.wikipedia.org/wiki/Bayes'_theorem):\n",
    "\n",
    "$$\\large p(\\text{target} = i | \\text{alcohol} = x) = \\frac{p(\\text{alcohol} = x | \\text{target} = i )p(\\text{target} = i )}{p(\\text{alcohol} = x)},$$\n",
    "\n",
    "где $p(\\text{target} = i | \\text{alcohol} = X)$ - вероятность того что объект принадлежит классу $i$ при условии того что признак `alcohol` у него принимает значение $X$, а $p(\\text{target} = i)$ - как часто датасете встречаются объекты класса $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы поняли, откуда в названии метода **Байес**, теперь разберемся **почему он “наивный”**.\n",
    "\n",
    "Мы использовали только один признак: `alcohol`, всего же у нас 13 признаков.\n",
    "\n",
    "$$\\large p(\\text{target} = i |\\text{features} = \\vec x ) = \\frac{p(\\text{features} = \\vec x | \\text{target} = i )p(\\text{target} = i )}{p(\\text{features} = \\vec x)}$$\n",
    "\n",
    "**“Наивность”** Байеса состоит в том, что эта модель будет рассматривать признаки, как **независимые случайные величины**:\n",
    "$$\\large p(\\text{features} = \\vec x)=p(\\text{feature}_1 = x_1)\\cdotp(\\text{feature}_2 = x_2)...(\\text{feature}_n = x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы решаем задачу классификации на $k$ классов, то для объекта с набором признаком $\\vec x$ по формуле выше мы получим $k$ чисел, характеризующих вероятность принадлежности данного объекта к различным классам. Для финального принятия решения нам останется выбрать тот класс, для которого вероятность принадлежности наивысшая:\n",
    "\n",
    "$$\\large \\text{рrediction} = \\underset{i}{\\text{argmax}}{\\left(p(\\text{target} = i |\\text{features} = \\vec x )\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к нашему датасету Wine и попробуем решить задачу классификации для него при помощи предложенного алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обычно, разделим наш датасет на тренировочную и валидационную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features.values, class_labels.values, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём реализацию Наивного Байесовского классификатора из [библиотеки sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB). Обучим её на тренировочном датасете и измерим качество на отложенной валидационной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the model\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Calculate F1_score\n",
    "pred = model.predict(x_test)\n",
    "f1_score(y_test, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря простоте модельного датасета Wine наша наивная статистическая модель показала отличное качество работы, для решения большинства практических задач на реальных датасетах не следует ожидать настолько же высокого качества. Тем не менее, подход к решению задачи классификации, связанный с построением модели предсказания принадлежности объекта к имеющимся классам, оказался крайне конструктивным. Такой подход можно попробовать применить в общем случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP: задача определения спама"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивный Байесовский классификатор часто используют в задаче обнаружения спама и пример его использования в такой задаче может показаться ещё более наглядным.\n",
    "В рамках данной задачи у нас имеется:\n",
    "- Датасет из текстов сообщений с некоторым фиксированным словарём возможных слов.\n",
    "- Два класса сообщений: спам и нормальное.\n",
    "- Признаковое описание для каждого сообщения характеризует количество вхождений каждого из слов словаря в текст сообщения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе этой информации нам нужно научиться отделять нормальные письма от спама. Все письма состоят из 4-х слов: **‘Добрый’, ‘День’, ‘Гости’, ‘Деньги’**. При этом мы уже посчитали, сколько раз каждое слово встречается в каждом классе.\n",
    "\n",
    "Мы можем посчитать вероятность встретить слово **‘Добрый’** в нормальном письме: берем количество слов **‘Добрый’** и делим на количество слов во всех нормальных письмах (с повторениями). Аналогично для других слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/naive_bayes_1.png\" alt=\"alttext\" width=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем то же самое для слов из спама."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/naive_bayes_2.png\" alt=\"alttext\" width=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем вероятность того, чтобы письмо было нормальным. Для этого количество нормальных писем делим на общее количество писем. Аналогично для спама. Это — $p(\\text{target} = i)$ в формуле выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/naive_bayes_3.png\" alt=\"alttext\" width=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить вероятность нормального письма с фразой **‘Добрый День’** в \"наивном\" предположении мы можем перемножить вероятности нормального письма со словом **‘Добрый’** и нормального письма со словом **‘День’**. Это произведение будет $p(\\text{target} = i | \\text{Features} = \\vec X)$ в формуле выше.\n",
    "\n",
    "Считаем $p(\\text{Features} = \\vec X|\\text{target} = i) p(\\text{target} = i)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/naive_bayes_3_5.png\" alt=\"alttext\" width=900/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-энтропия, как общая функция потерь для задач классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переход к вероятностям\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "[Видео от StatQuest, которое объясняет Softmax](https://www.youtube.com/watch?v=KpKog-L9veg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/scores_to_probability.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейти к вероятностям мы сможем, проведя с весами некоторые не очень сложные математические преобразования.\n",
    "\n",
    "На слайде выше показано, почему выходы модели часто называют [logit’ами](https://en.wikipedia.org/wiki/Logit). Если предположить, что у нас есть некая вероятность, от которой мы берем такую функцию (logit), то результат может принимать значение в любых вещественных числах. Мы можем считать, что выходы модели — это logit’ы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы могли бы просто взять индекс массива, в котором значение (logit) максимально. Предположим, что наша модель выдала следующие значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = [\n",
    "    5.1,  # cat\n",
    "    3.2,  # car\n",
    "    -1.7,  # frog\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, чтобы узнать какой класс наша сеть предсказала, мы могли бы просто взять `argmax` от наших `logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Predicted class = %i (Cat)\" % (np.argmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но от argmax нельзя посчитать градиент, так как производная от константы равна 0. Соответственно, если бы мы вставили производную от argmax в градиентный спуск, мы бы получили везде нули, и соответственно, наша модель бы вообще ничему не научилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(np.arange(3), [1, 0, 0], color=\"red\", s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А мы бы хотели получить не logit’ы, а настоящую вероятность на выходе модели. Да еще и таким образом, чтобы от наших вероятностей можно было посчитать градиент. Для этого мы можем применить к нашим логитам функцию **Softmax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/linear_classifier_softmax.png\" width=\"1024\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Отобразим наши logit’ы на значения $[0, +∞)$.\n",
    "\n",
    "Для этого возведем экспоненту (число Эйлера 2.71828) в **степень логита**. В результате, мы получим вектор гарантированно неотрицательных чисел (положительное число, возведенное в степень, даже отрицательную, даст положительное значение).\n",
    "\n",
    "2. Нормализуем.\n",
    "\n",
    "Чтобы  интерпретировать числа как вероятности, их сумма должна быть равна единице. Мы должны их нормализовать, то есть **поделить на сумму**.\n",
    "\n",
    "Это преобразование называется **Softmax функцией**. **Получаются вероятности**, то есть числа, которые можно интерпретировать, как вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Softmax}_\\text{кошка} = \\frac{e^{5.1}}{e^{5.1} + e^{3.2} + e^{-1.7}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return np.exp(logits) / np.sum(np.exp(logits))\n",
    "\n",
    "\n",
    "print(softmax(logits))\n",
    "print(\"Sum = %.2f\" % np.sum(softmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что Softmax никоим образом не поменял порядок значений. Самому большому logit'у соответствует самая большая вероятность, а самому маленькому, соответственно, самая маленькая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графиках. Возьмем массив случайных логитов и применим к ним softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_logits = np.linspace(-1, 1, 50)\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(6, 3))\n",
    "\n",
    "ax[0].plot(np.arange(50), rand_logits)\n",
    "ax[0].set_title(\"Logits\")\n",
    "ax[1].plot(np.arange(50), softmax(rand_logits))\n",
    "ax[1].set_title(\"Softmax\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Практическое вычисление SoftMax\n",
    "\n",
    "При вычислении экспоненты от выходов модели могут получиться очень большие числа в силу очень высокой скорости роста экспоненты. Этот факт необходимо учитывать, чтобы вычисления SoftMax были численно стабильны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "f = np.array([123, 456, 789])\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "print(f, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы регуляризовать вычисление, нам следует предварительно упростить возникающую в вычислении дробь. Для этого мы можем вычесть из каждого $s_i$ положительную константу, чтобы уменьшить значения экспонент. В качестве константы можно выбрать максимальный элемент этого вектора, тогда у нас гарантированно не будет очень больших чисел, и такой способ будет работать более стабильно.\n",
    "\n",
    "$$M = \\max_j s_{y_{j}}$$\n",
    "$$s^{new}_{y_{i}}  = s_{y_{i}} - M $$\n",
    "\n",
    "$$ \\dfrac {e^{s^{new}_{y_{i}}}} {\\sum_j e^{s^{new}_{y_{j}}}}  = \\dfrac {e^{s_{y_{i}} - M }} {\\sum_j e^{s_{y_{j}} - M }} = \\dfrac {e^{s_{y_{i}}}e ^ {-M}} {\\sum_j e^{s_{y_{j}}} e ^ {-M}} = \\dfrac {e ^ {-M} e^{s_{y_{i}}}} {e ^ {-M} \\sum_j e^{s_{y_{j}}} } = \\dfrac { e^{s_{y_{i}}}} { \\sum_j e^{s_{y_{j}}} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([123, 456, 789])\n",
    "f -= f.max()\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "print(f, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние (дивергенция) Кульбака — Лейблера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно сравнить полученные вероятности с истинными метками классов.\n",
    "\n",
    "В математической статистике и теории информации в мерой расхождения между двумя вероятностными распределениями $P$ и $Q$ является расстояние (дивергенция) Кульбака — Лейблера, вычисляемое по формуле\n",
    "$$D_{KL}(P||Q) = ∑_i P(i)\\log\\frac{P(i)}{Q(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разобраться, что значит эта формула на примере двух монеток:\n",
    "- настоящей с вероятностями орла и решки 0.5 и 0.5 соответственно,\n",
    "- фальшивой с вероятностями орла и решки 0.2 и 0.8 соответственно.\n",
    "\n",
    "Возьмем настоящую монету и произведем 10 бросков (выборок). Получили последовательность $\\color{blue}{О О} \\color{green}{Р} \\color{blue}{О О} \\color{green}{Р} \\color{blue}{О О О} \\color{green}{Р}$, где $\\color{blue}{O}$ - это орел, $\\color{green}{Р}$ - это решка.\n",
    "Посчитаем вероятности выбросить такую последовательность для настоящей и фальшивой монеты. Броски независимые, поэтому значения вероятностей перемножаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/kl_divergence_1.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем пропорцию вероятностей данной комбинации для настоящей монетки и для фальшивой (независимые случайные величины, вероятности перемножаются). Для заданных значений вероятностей пропорция будет примерно $149:1$.\n",
    "\n",
    "$$\\frac{\\color{blue}{p_1^{N_о}}\\color{green}{p_2^{N_р}}}\n",
    "{\\color{blue}{q_1^{N_о}}\\color{green}{q_2^{N_р}}}=\n",
    "\\frac{\\color{blue}{\\left(\\frac{1}{2}\\right)^{7}}\\color{green}{\\left(\\frac{1}{2}\\right)^{3}}}\n",
    "{\\color{blue}{\\left(\\frac{1}{5}\\right)^{7}}\\color{green}{\\left(\\frac{4}{5}\\right)^{3}}}\\approx \\frac{149}{1}$$\n",
    "\n",
    "Возьмем логарифм от этого значения (это позволит нам избавиться от степеней и заменить умножение сложением) и нормируем на количество бросков монетки $N=\\color{blue}{N_о}+\\color{green}{N_р}$.\n",
    "\n",
    "$$\\frac{\\color{blue}{N_о}}{N}\\log{\\color{blue}{p_1}}+\n",
    "\\frac{\\color{green}{N_р}}{N}\\log{\\color{green}{p_2}}-\n",
    "\\frac{\\color{blue}{N_о}}{N}\\log{\\color{blue}{q_1}}-\n",
    "\\frac{\\color{green}{N_р}}{N}\\log{\\color{green}{q_2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При увеличении количества бросков $N\\to∞$, так как мы бросали настоящую монетку\n",
    "\n",
    "$$\\frac{\\color{blue}{N_о}}{N} \\to \\color{blue}{p1}, \\frac{\\color{green}{N_р}}{N} \\to \\color{green}{p2} .$$\n",
    "\n",
    "Получаем расстояние Кульбака — Лейблера.\n",
    "\n",
    "$$D_{KL}(P||Q) = \\color{blue}{p_1} \\log{\\color{blue}{p_1}}\n",
    "+ \\color{green}{p_2}\\log{\\color{green}{p_2}}\n",
    "- \\color{blue}{p_1}\\log{\\color{blue}{q_1}}\n",
    "- \\color{green}{p_2}\\log{\\color{green}{q_2}}$$\n",
    "\n",
    "$$ = \\color{blue}{p_1} \\log{\\color{blue}{\\frac{p_1}{q_1}}}\n",
    "+ \\color{green}{p_2} \\log{\\color{green}{\\frac{p_2}{q_2}}}$$\n",
    "\n",
    "Обратим внимание, что если $P=Q$, то\n",
    "\n",
    "$$D_{KL}(P||Q) = ∑_i P(i)\\log\\frac{P(i)}{Q(i)} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Переход к оценке модели\n",
    "\n",
    "Мы научились определять близость двух распределений. Как это поможет нам оценить качество модели, если мы знаем, какие метки классов должны получиться?\n",
    "\n",
    "Пусть $P$ — вероятности истинных меток классов для объекта (1 для правильного класса, 0 для остальных), $Q(\\theta)$ — вероятности классов для объекта, предсказанные моделью с обучаемыми параметрами $\\theta$.\n",
    "\n",
    "Расстояние Кульбака — Лейблера между истинными и предсказанными значениями\n",
    "\n",
    "$$D_{KL}(P||Q(\\theta)) = ∑_i P(i)\\log\\frac{P(i)}{Q(i| \\theta)} $$\n",
    "\n",
    "$$ = ∑_i P(i)\\log{P(i))} -  ∑_i P(i)\\log{Q(i|\\theta)} $$\n",
    "\n",
    "$$ = - H(P) + H(P|Q(\\theta))$$\n",
    "\n",
    "Мы разбили сумму на две части, первая из которых называется **энтропией** $H(P)$ и не будет зависеть от модели, а вторая называется **кросс-энтропией** $H(P|Q(\\theta))$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Энтропия\n",
    "\n",
    "Понятие энтропии пришло из теории связи. Для расчета энтропии можно использовать формулу Шеннона:\n",
    "\n",
    "$$H(P)=-\\sum^C_{i=1}P(i)\\cdot log_{2}(P(i)),$$\n",
    "где C &mdash; количество классов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В формуле Шеннона не случайно используется логарифм по основанию 2. Она рассчитывает, **сколько бит информации минимально необходимо для передачи одного значения**, например, одного исхода броска монетки.\n",
    "\n",
    "Если монетка **всегда выдает орел**, то нам нет смысла передавать информацию, чтобы предсказать исход\n",
    "\n",
    "$$H(\\color{blue}{p_1 = 1}, \\color{green}{p_2 = 0}) = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для настоящей монетки необходимо будет передавать 1 бит информации на бросок\n",
    "$$H(\\color{blue}{p_1 = 0.5},  \\color{green}{p_2 = 0.5})= log_{2}(2) = 1,$$\n",
    "\n",
    "А вот с поддельной монеткой получается интереснее: если сделать много бросков, можно выявить закономерность что решка выпадет чаще (вероятность трех решек подряд будет больше вероятности одного орла) и за счет этого сократить количество передаваемой информации так, чтобы на один бросок получалось меньше 1 бита. Как это сделать — отдельная область теории информации, называемая **кодирование источника**.\n",
    "$$H(\\color{blue}{q_1 = 0.2}, \\color{green}{q_2 = 0.8}) = 0.722$$\n",
    "\n",
    "__Энтропия__ — мера неуверенности, связанная с распределением $P$.\n",
    "Зная истинное распределение случайной величины, мы можем рассчитать его энтропию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы попытаемся использовать статистические данные, полученные для фальшивой монеты $Q$, для настоящей $P$, мы не получим выигрыш в количестве передаваемой информации:\n",
    "\n",
    "$$H(P||Q) = - \\sum^C_{i=1}P(i)\\cdot log_2(Q(i)) = 1.322$$\n",
    "\n",
    "Формула выше называется **кросс-энтропия**.\n",
    "\n",
    "**Кросс-энтропия** позволяет оценить ситуацию, когда мы аппроксимируем истинное распределение $P$ предсказанным распределением $Q$. Чем больше значения кросс-энтропии, тем больше расхождение между распределениями. Поэтому кросс-энтропию используют в качестве **функции потерь**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы показали выше, **кросс-энтропия** связана с **энтропией** и **расстоянием Кульбака — Лейблера**\n",
    "$$H(P||Q) = D_{KL}(P||Q) + H(P).$$\n",
    "\n",
    "Посчитаем значения для наших монеток с помощью кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal coin\n",
    "p1 = 0.5\n",
    "p2 = 0.5\n",
    "\n",
    "# fake coin\n",
    "q1 = 0.2\n",
    "q2 = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback–Leibler divergence\n",
    "div_kl = p1 * np.log2(p1 / q1) + p2 * np.log2(p2 / q2)\n",
    "print(f\"Dkl(P||Q) = {div_kl:.3f}\")\n",
    "\n",
    "# Entropy normal coin\n",
    "h_p = -p1 * np.log2(p1) - p2 * np.log2(p2)\n",
    "print(f\"H(P) = {h_p:.3f}\")\n",
    "\n",
    "# Entropy fake coin\n",
    "h_q = -q1 * np.log2(q1) - q2 * np.log2(q2)\n",
    "print(f\"H(Q) = {h_q:.3f}\")\n",
    "\n",
    "# Cross-entropy\n",
    "h_p_q = -p1 * np.log2(q1) - p2 * np.log2(q2)\n",
    "print(f\"H(P||Q) = {h_p_q:.3f}\")\n",
    "print(f\"H(P||Q) = Dkl(P||Q) + H(P) = {h_p+div_kl:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расчет функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к задаче классификации изображения. Рассчитаем для предсказания модели Cross-Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/logits_to_scores_to_probabilitys.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В расчетах выше мы использовали логарифм по основанию 2, что приближало нас к теории информации. Мы можем считать кросс-энтропию по натуральному логарифму, она будет отличаться умножением на константу.\n",
    "\n",
    "$$H(P||Q)=-1⋅\\log{0.869}-0\\cdot\\log{0.13}-0\\cdot\\log{0.001}\\approx0.14$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(pred_prob, true_prob):\n",
    "    return -np.dot(true_prob, np.log(pred_prob))\n",
    "\n",
    "\n",
    "print(f\"Logits = {logits}\")\n",
    "\n",
    "pred_prob = softmax(logits)\n",
    "print(f\"Predicted Probabilities = {pred_prob}\")\n",
    "\n",
    "true_prob = [1.0, 0.0, 0.0]\n",
    "print(f\"True Probabilities = {true_prob}\")\n",
    "\n",
    "print(f\"Cross-entropy loss = {cross_entropy_loss(pred_prob, true_prob):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-энтропия vs Hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-entropy / log loss**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/cross_entropy_plot_loss_with_probability.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Преимуществом Cross-Entropy loss (слева) по сравнению с кусочно-гладкой SVM-Loss (справа) является:\n",
    "- гладкость и отсутствие участка с плато (не имеет нулевых производных или неопределенных точек),\n",
    "- большой градиент для большого Loss, маленький вблизи 100% точности (для кусочно гладкой функции градиент ноль или константа).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиент функции потерь. Кросс-энтропия\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cross Entropy Loss](https://wandb.ai/wandb_fc/russian/reports/---VmlldzoxNDI4NjAw#:~:text=%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F%20%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C%20%D0%BF%D0%B5%D1%80%D0%B5%D0%BA%D1%80%D0%B5%D1%81%D1%82%D0%BD%D0%BE%D0%B9%20%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D0%B8%20%E2%80%93%20%D1%8D%D1%82%D0%BE,%2C%20%D0%B3%D0%B4%D0%B5%200%20%E2%80%93%20%D0%B8%D0%B4%D0%B5%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -L = \\sum_i y_i \\log p_i = \\sum_i y_i \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
    "\n",
    "$$s_{y_i} = w_i x$$\n",
    "\n",
    "$$ \\dfrac {\\partial L} {\\partial w_i} = \\dfrac {\\partial L} {\\partial s_{y_i}} \\dfrac {\\partial s_{y_i}} {\\partial w_i} $$\n",
    "\n",
    "$$\\dfrac {\\partial s_{y_i}} {\\partial w_i} = x$$\n",
    "\n",
    "Только один $y_k = 1$\n",
    "\n",
    "\n",
    "$$ -L = y_k \\log p_i = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
    "\n",
    "i = k\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_i}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_i} - \\log  \\sum_j e^{s_{y_j}}$$\n",
    "\n",
    "$$\\dfrac {\\partial -L} {\\partial s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\partial {\\sum_j e^{s_{y_j}}}} {\\partial s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\partial e^{s_{y_i}}} {\\partial s_{y_i}} = 1 - \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = 1 - p_i$$\n",
    "\n",
    "$$\\dfrac {\\partial L} {\\partial s_{y_j}} = p_i - 1 $$\n",
    "\n",
    "$$ \\dfrac {\\partial L_i} {\\partial w_i}  = \\dfrac {\\partial L} {\\partial s_{y_i}} \\dfrac {\\partial s_{y_i}} {\\partial w_i} = (p_i - 1) x $$\n",
    "\n",
    "i != k\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_k}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_k} - \\log  \\sum_j e^{s_{y_j}}$$\n",
    "\n",
    "$$\\dfrac {\\partial -L} {\\partial s_{y_i}} = - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\partial {\\sum_j e^{s_{y_j}}}} {\\partial s_{y_i}} =  \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\partial e^{s_{y_i}}} {\\partial s_{y_i}} = \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = - p_i$$\n",
    "\n",
    "$$\\dfrac {\\partial L} {\\partial s_{y_j}} = p_i $$\n",
    "$$ \\dfrac {\\partial L_i} {\\partial w_i}  = \\dfrac {\\partial L} {\\partial s_{y_i}} \\dfrac {\\partial s_{y_i}} {\\partial w_i} = p_i x $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде это будет выглядеть вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input batch of 2 vector with 4 elements\n",
    "x = np.array([[1, 2, 3, 4], [1, -2, 0, 0]])\n",
    "# Weights\n",
    "W = np.random.randn(3, 4)  # 3 class\n",
    "\n",
    "# model output\n",
    "logits = x.dot(W.T)\n",
    "print(\"Scores(Logits) \\n\", logits, \"\\n\")\n",
    "\n",
    "# Probabilities\n",
    "probs = softmax(logits)  # defined before\n",
    "print(\"Probs \\n\", probs, \"\\n\")\n",
    "\n",
    "# Ground true classes\n",
    "y = [0, 1]\n",
    "\n",
    "# Derivative\n",
    "probs[np.arange(2), y] = -1  # substract one from true class prob\n",
    "dW = x.T.dot(probs)  # dot product with input\n",
    "\n",
    "print(\"Grads dL/dW \\n\", dW)  # have same shape as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример обучения линейного классификатора с hinge loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class LinearClassifier:\n",
    "    def __init__(self, labels, batch_size, imgsize=28, nchannels=1, random_state=42):\n",
    "        self.labels = labels  # classes names\n",
    "        self.classes_num = len(labels)  # num of classes\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        self.W = (\n",
    "            np.random.randn(nchannels * imgsize**2 + 1, self.classes_num) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias\n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # reset loss\n",
    "        train_len = x_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)\n",
    "\n",
    "        for i in range(0, train_len, self.batch_size):\n",
    "            idx = indexes[i : i + self.batch_size]\n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "\n",
    "            x_batch = np.hstack([x_batch, np.ones((x_batch.shape[0], 1))])  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(x_batch, y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        current_batch_size = x.shape[0]  # batch_size\n",
    "        loss = 0.0\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        for i in range(current_batch_size):\n",
    "            scores = x[i].dot(self.W)  # vector of shape 10\n",
    "            correct_class_score = scores[int(y[i])]\n",
    "            above_zero_loss_count = 0\n",
    "            for j in range(self.classes_num):\n",
    "                if j == y[i]:  # predict class\n",
    "                    continue\n",
    "                margin = scores[j] - correct_class_score + 1  # loss\n",
    "                if margin > 0:\n",
    "                    above_zero_loss_count += 1\n",
    "                    loss += margin  #\n",
    "                    dW[:, j] += x[i]  #\n",
    "            dW[:, int(y[i])] -= above_zero_loss_count * x[i]\n",
    "        loss /= current_batch_size\n",
    "        dW /= current_batch_size\n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.append(x, 1)  # add 1 (bias)\n",
    "        scores = x.dot(self.W)\n",
    "        return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для проверки качества полученной модели на валидационной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, x_test, y_test, noprint=False):\n",
    "    correct = 0\n",
    "    for i, img in enumerate(x_test):\n",
    "        index = model.forward(img)\n",
    "        correct += 1 if index == y_test[i] else 0\n",
    "        if noprint is False:\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                print(\"Accuracy {:.3f}\".format(correct / i))\n",
    "    return correct / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model_class, x_train, y_train, x_test, y_test, labels, imgsize, nchannels\n",
    "):\n",
    "    print(\"How learning quality depends of speed:\")\n",
    "\n",
    "    for lr in [1e-2, 1e-8]:\n",
    "        for bs in [256, 2048]:\n",
    "            print(\"-\" * 50, \"\\n\", \"learning_rate =\", lr, \"\\tbatch_size =\", bs)\n",
    "            print()\n",
    "            model = model_class(labels, bs, imgsize, nchannels)\n",
    "\n",
    "            best_accuracy = 0\n",
    "            for epoch in range(10):\n",
    "                loss = model.fit(x_train, y_train, learning_rate=lr)\n",
    "                accuracy = validate(model, x_test, y_test, noprint=True)\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_epoch = epoch\n",
    "                print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
    "\n",
    "            print()\n",
    "            print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST](https://paperswithcode.com/dataset/mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "from IPython.display import clear_output\n",
    "\n",
    "dataset_train = MNIST(\"content\", train=True, download=True)\n",
    "dataset_test = MNIST(\"content\", train=False, download=True)\n",
    "\n",
    "x_train = dataset_train.data.numpy().reshape((-1, 28 * 28))\n",
    "y_train = np.array(dataset_train.targets)\n",
    "\n",
    "x_test = dataset_test.data.numpy().reshape((-1, 28 * 28))\n",
    "y_test = np.array(dataset_test.targets)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_train shape : {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(\n",
    "    LinearClassifier, x_train, y_train, x_test, y_test, np.arange(10), 28, 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CIFAR-10](https://paperswithcode.com/dataset/cifar-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_exists = os.path.exists(\"/content/cifar-10-batches-py\")\n",
    "if file_exists == False:\n",
    "    #!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !wget https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/cifar-10-python.tar.gz\n",
    "    !tar -xzf cifar-10-python.tar.gz\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "x_train = np.zeros((0, 3072))\n",
    "y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    # raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    raw = unpickle(f\"cifar-10-batches-py/data_batch_{i}\")\n",
    "    x_train = np.append(x_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    y_train = np.append(y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "# test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "test = unpickle(\"cifar-10-batches-py/test_batch\")\n",
    "x_test = np.array(test[b\"data\"])\n",
    "y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_eng = [\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_validate(\n",
    "    LinearClassifier, x_train, y_train, x_test, y_test, labels_eng, 32, 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Свалка\n",
    "Это рабочий заголовок, который будет удален перед чтением. Я переношу сюда картинки, которые не вписываются в лекцию по ходу правок, но могут пригодится в других разделах, чтобы они оставались навиду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы их можем собрать в матрицу, тогда получится следующее:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/scalar_product_add_bias.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть матрица коэффициентов, которые мы каким-то образом подобрали, пока ещё непонятно как. Есть вектор $x$, соответствующий изображению.\n",
    "\n",
    "Мы умножаем вектор на матрицу, получаем нашу гиперплоскость для четырехмерного пространства в данном случае. Чтобы оно не лежало в 0, мы должны добавить смещение. И мы можем сделать это после, но можно взять и этот вектор смещения (вектор **b**) просто приписать к матрице **W**.\n",
    "\n",
    "Что будет выходом такой конструкции? Мы умножили матрицу весов на наш вектор, соответствующий изображению, получили некоторый отклик. По этому отклику мы так же, как и при реализации метода ближайшего соседа можем судить: если он больше остальных, то мы предполагаем, что это кошка.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([56, 231, 24, 2])\n",
    "w_cat = np.array([0.2, -0.5, 0.1, 2.0])\n",
    "print(\"Image \", img)\n",
    "print(\"Weights \", w_cat)\n",
    "print(\"img * w_cat \", img * w_cat)\n",
    "print(\"sum \", (img * w_cat).sum())\n",
    "print(\"Add bias \", (img * w_cat).sum() + 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/img_to_function_get_scores.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим входное изображение как $x$, а шаблон для первого из классов как $w_0$.\n",
    "\n",
    "Элементы пронумеруем подряд 1,2,3 … $n$. То есть развернем матрицу пикселей изображения в вектор.\n",
    "\n",
    "Тогда результат сравнения изображения с этим шаблоном будет вычисляться по формуле: $x[0]*w_0[0] + x[1]*w_0[1] + … x[n-1]*w_0[n-1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/img_to_vector_to_compute_scalar_product.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/scalar_product_ways_to_use.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта простая модель лежит в основе практически всех сложных моделей, которые мы будем рассматривать дальше. Внутри будем также пользоваться скалярным произведением.\n",
    "\n",
    "В дальнейшем мы будем проходить сверточные сети, они работают очень похоже:\n",
    "мы тоже накладываем шаблон на некоторую матрицу и перемножаем элементы, затем складываем. Единственное отличие — обычно ядро свертки меньше, чем размер самого изображения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собирая все вместе, получаем какое-то компактное представление, что у нас есть некоторая функция, на вход которой мы подаем изображение, и у нее есть параметры (веса). Пока происходит просто умножение вектора на матрицу, в дальнейшем это может быть что-то более сложное, функция будет представлять какую-то более сложную модель. А на выходе (для классификатора) мы получаем числа, которые интерпретируют уверенность модели в том, что изображение принадлежит к определенному классу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/input_img_scalar_product_add_bias_get_scores.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, эти коэффициенты, которые являются весами модели, надо каким-то образом подбирать. Но прежде чем подбирать коэффициенты, давайте определимся со следующим: как мы будем понимать, что модель работает хорошо или плохо? Вывод модели представляет собой просто некоторый набор чисел. Но как эти числа следует правильно интерпретировать? Рассмотрим этот вопрос в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# старый мульткласс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/svm.html#unbalanced-problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим, как линейная модель классификации с Hinge loss работает на практике. Мы можем применить линейный классификатор в том числе и к изображениям — достаточно просто вытянуть изображение из тензора формата $(\\text{C}, \\text{H}, \\text{W})$ в $(\\text{C} \\cdot \\text{H} \\cdot \\text{W})$-мерный вектор. Применим линейный классификатор к нескольким изображениям из датасета CIFAR-10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/model_predicted_scores_for_10_classes.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии с тем, что мы уже делали, мы можем сравнивать отклик на ключевой класс  (про который нам известно, что он на изображении, так как у нас есть метка этого класса) с остальными. Соответственно, мы подали изображение кошки и получили на выход вектор. Чем больше значение, тем больше вероятность того, что, по мнению модели, на изображении этот класс. Для кошки в данном случае это значение 3.2. Хорошо это или плохо? Нельзя сказать, пока мы не проанализировали остальную часть вектора. Если бы мы могли посмотреть на все значения в векторе, мы бы увидели, что есть значения больше, то есть в данном случае модель считает, что это собака, а не кошка, потому что для собаки значение максимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основании этого можно построить некоторую оценку. Давайте смотреть на разницу правильного класса с неправильными. Насколько уверенность в кошке будет больше остальных, настолько хорошо работает наша модель.\n",
    "\n",
    "Но поскольку нам важна не работа модели на конкретном изображении, а важно оценить ее работу в целом, то эту операцию нужно проделать либо для всего датасета, либо для некоторой выборки, которую мы подаем на вход и подсчитываем средний показатель. Этот показатель (насколько хорошо работает модель), называется функцией потерь, или **loss function**. Называется она так потому, что она показывает не то, насколько хорошо работает модель, а то, насколько плохо.\n",
    "\n",
    "Дальше будет понятно, почему так удобнее (разница только в знаке). Как это посчитать для всего датасета?\n",
    "\n",
    "Мы каким-то образом считаем loss для конкретного изображения, потом усредняем по всем изображениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано: 3 учебных примера, 3 класса. При некотором W баллы f (a, W) = Wx равны:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/model_predicted_scores_for_3_classes.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь показывает, насколько хорош наш текущий классификатор.\n",
    "\n",
    "Дан датасет примеров:\n",
    "\n",
    "$\\begin{Bmatrix} (x_i,y_i)  \\end{Bmatrix}_{i=1}^N \t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где **$x_i$** — изображение, **$y_i$** — метка (число).\n",
    "\n",
    "Потери по набору данных — это среднее значение потерь для примеров:\n",
    "\n",
    "$ L = {1 \\over N}\\sum_iL_i(f(x_i,W),y_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим функцию потерь для одного примера $L_i(f(x_i,W),y_i)$:\n",
    "\n",
    "1. Вычислим вектор значений прогнозов классификатора $s = f(x_i, W)$.\n",
    "1. Для всех примеров рассмотрим разницу между оценкой на истинной категории и всеми оценками классификатора для неправильных категорий: $s_{y_i} - s_j$ для $j \\neq y_i$.\n",
    "1. Если получившаяся разница положительная и превышает некоторое пороговое значение («зазор»), которое мы установим равным $1$, то будем считать, что категория $j$ не мешает модели верно классифицировать входной объект, припишем категории $j$ нулевой вклад в $L_i(f(x_i,W),y_i)$.\n",
    "1. Если получившаяся разница не превосходит установленного нами единичного «зазора», то мы будем считать что ответ классификатора $s_j$ в категории $j$ мешает верной классификации входного объекта. В этом случае припишем для категории $j$ аддитивный вклад в $L_i(f(x_i,W),y_i)$ равный $s_j-s_{y_i}+1$.\n",
    "\n",
    "Описанную процедуру гораздо проще записать в виде формулы:\n",
    "\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}$\n",
    "\n",
    "$=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логика такая: если у нас уверенность модели в правильном классе большая, то модель работает хорошо и loss для данного конкретного примера должен быть равен нулю. Если есть класс, в котором модель уверена больше, чем в правильном, то loss должен быть не равен нулю, а отображать какую-то разницу, поскольку модель сильно ошиблась. При этом есть ещё одно соображение: что будет, если на выходе у правильного и ошибочного класса будут примерно равные веса? То есть, например, у кошки было бы 3.2, а у машины не 5.2, а 3.1. В этом случае ошибки нет, но понятно, что при небольшом изменении в данных (просто шум) скорее всего она появится.\n",
    "\n",
    "То есть модель плохо отличает эти классы. Поэтому мы и вводили некоторый зазор, который должен быть между правильным и неправильным ответом.\n",
    "\n",
    "Посмотрим на изображение снизу. У нас есть два класса: фиолетовые треугольники и синие квадраты, разделенные зазором. Также можем увидеть желтые треугольники и квадраты — это ошибочно распознанные классы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/svm_decision_boundary.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И тоже учитывать его в loss function: сравнивать его результат для правильного класса не с чистым выходом для другого, а добавить к нему некоторую дельту (в данном случае — 1(единица)). Смотрим: если разница больше 0, то модель работает хорошо и $L_i = 0$. Если нет, то мы возвращаем эту разницу, и loss будет складываться из этих индивидуальных разниц.\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}$\n",
    "\n",
    "$=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "Ниже пример того, как считается loss.\n",
    "\n",
    "Считаем функцию потерь для 1-ого изображения:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/compute_loss_use_model_scores_for_1_example.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также считаем потери для 2-ого и 3-его изображения:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/compute_loss_use_model_scores_for_2_and_3_examples.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения потерь получились следующие:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/summary_losses_for_3_examples.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем среднее значение loss для всего датасета:\n",
    "\n",
    "$ L = {1 \\over N}\\sum_{i=1}^N L_i$\n",
    "\n",
    "$L={2.9 + 0 + 12.9 \\over 3} = 5.27$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM loss\n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Геометрическая интерпретация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрались с тем, что такое регрессия, вернемся к задаче классификации изображений из датасета CIFAR-10. Как можно применить регрессию для классификации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у нас есть только 2 класса. Как можно использовать регрессию для того, чтобы определить относится ли изображение к классу 0 или к классу 1? В упрощенном варианте задача будет состоять в том, чтобы провести разделяющую плоскость (прямую) между 2-мя классами. Например, мы можем провести прямую через 0.\n",
    "\n",
    "Такая прямая будет задана направляющим вектором $\\vec W$, число компонентов которого будет равно размерности пространства признаков. Уравнение гиперплоскоскости в этом случае имеет вид\n",
    "\n",
    "$$\\large \\sum_i W_i \\cdot x_i = 0$$\n",
    "или, что эквивалентно:\n",
    "$$\\large ( \\vec W , \\vec x) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/regression_for_classification_imgs.png\" width=\"270\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другую ситуацию. В этом случае мы не можем просто провести прямую через 0, но можем отступить от 0 на какое-то расстояние и провести ее там. Вспомним, что уравнение прямой это $y=wx+b$, где $b$ — это смещение (*bias*). Соответственно, если b != 0, то прямая через 0 проходить не будет, а будет проходить через значение b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/regression_for_classification_add_bias.png\" width=\"270\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, для классификации объектов на два класса нам нужно подобрать значение вектора $W$ и величину смещения $B$. Вместе они зададут гиперплоскость вида:\n",
    "$$\\large ( \\vec W , \\vec x) - B = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Classification Loss Visualization\n",
    "](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)\n",
    "\n",
    "\n",
    "Если у нас есть несколько классов, мы можем для каждого из них посчитать уравнение $y_{i} = w_{i}x_{i}+b_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L02/out/regression_for_classification_add_bias_add_multiclasses.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На картинке нас интересуют 3 класса. Соответственно, мы можем записать систему линейных уравнений:\n",
    "\n",
    "\\begin{cases}\n",
    "y_{0} = w_{0}x_{0} + b_{0} \\\\\n",
    "y_{1} = w_{1}x_{1} + b_{1} \\\\\n",
    "y_{2} = w_{2}x_{2} + b_{2} \\\\\n",
    "\\end{cases}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
