{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Линейные модели</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня мы познакомимся с линейными моделями. Эти модели являются частью нейронных сетей, которым посвящена существенная часть нашего курса.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе линейных моделей лежит линейная функция\n",
    "$$\\large f(\\vec x) = (\\vec w, \\vec x) +b, $$\n",
    "где $(\\vec w, \\vec x)$ — скалярное произведение:\n",
    "\n",
    "$$\\large (\\vec w, \\vec x) = \\sum_{i=1}^n{w_ix_i} = w_1x_1+w_2x_2+...+w_ix_i+ ... + w_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/scalar_product_ways_to_use.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Регрессия** — это одна из трех базовых задач машинного обучения (классификация, регрессия, кластеризация).\n",
    "\n",
    "В задаче **регрессии** мы используем входные **признаки**, чтобы предсказать **целевые значения**. Например, чтобы предсказать температуру воздуха над населённым пунктом на следующий день, если известны различные характеристики на текущий момент времени (влажность воздуха, скорость и направление ветра, плотность застройки или наличие промышленных объектов, и т.п.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Модель и ее параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у нас есть набор точек $\\{(x_i, y_i)\\}$.\n",
    "\n",
    "Цель линейной регрессии — **поиск линии, которая наилучшим образом соответствует заданным точкам**. Напомним, что уравнение прямой:\n",
    "\n",
    "$$\\large f(x) = w⋅x + b,$$\n",
    "\n",
    "где $w$ — характеризует наклон линии (в будущем мы будем называть значения $w$ весом, weight) а $b$ — её сдвиг по оси $y$ (bias). Таким образом, решение линейной регрессии определяет значения для $w$ и $b$ так, что $f(x)$ приближается как можно ближе к $y(x)$. Здесь $w$ и $b$ — **параметры модели**.\n",
    "\n",
    "Отобразим на графике случайные точки, расположенные в окрестности $y(x) = 3⋅x + 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "y = 2 + 3 * x + (np.random.rand(100, 1) - 0.5)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что нам неизвестны параметры наклона и сдвига $w$ и $b$. Для их определения мы бы могли рассмотреть все возможные прямые вида $f(x) = w⋅x + b$ и выбрать среди семейства прямых такую, которая лучше всего приближает имеющиеся данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in np.arange(-5.0, 7.0, 1):\n",
    "    for b in [-1, 0, 1, 2, 3]:\n",
    "        y_predicted = b + w * x\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель** $f(x) = w⋅x + b$ задаёт параметрическое семейство функций, а **выбор \"правильного\" представителя** из **параметрического семейства** и называется **обучением** модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "for w in np.arange(-5.0, 7.0, 1):\n",
    "    for b in [-1, 0, 1, 2, 3]:\n",
    "        y_predicted = b + w * x\n",
    "        plt.plot(x, y_predicted, color=\"r\", alpha=0.3)\n",
    "plt.plot(x, 2 + 3 * x, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как выбрать параметры?\n",
    "\n",
    "**Функция потерь** позволяет вычислить меру количества ошибок. Для задачи **регрессии** такой мерой может быть **расстояние** между предсказанным значением $f(x)$ и его фактическим значением. Распространенной функцией потерь является **средняя квадратичная ошибка** (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы определяем ошибку модели на одном объекте как квадрат расстояния $l_i$ между предсказанием $f(x_i)$ и истинным значением $y_i$, а общая функция потерь будет задана выражением:\n",
    "\n",
    "$$l_i =|f(x_i)-y_i| $$\n",
    "\n",
    "$$ \\text{Loss} = \\frac{1}{N}\\sum l_i^2 = \\frac{1}{N} \\sum (f(x_i)-y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для прямой с параметрами $w=4$, $b = 2$ и $w=3$, $b = 2$ (верные значения):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta_line(ax, x, y, w, b, color=\"r\"):\n",
    "    y_predicted = w * x + b\n",
    "    # line\n",
    "    ax.plot(x, y_predicted, color=color, alpha=0.5, label=f\"f(x)={w}x+{b}\")\n",
    "    # delta\n",
    "    for x_i, y_i, f_x in zip(x, y, y_predicted):\n",
    "        ax.vlines(x=x_i, ymin=min(f_x, y_i), ymax=max(f_x, y_i), ls=\"--\", alpha=0.3)\n",
    "    # MSE\n",
    "    loss = np.sum((y - (w * x + b)) ** 2) / (len(x))\n",
    "    ax.set_title(f\"MSE = {loss:.3f}\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# plot x_i y_i (dots)\n",
    "for ax in axs:\n",
    "    ax.scatter(x, y, s=10)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([2, 6])\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "plot_delta_line(axs[0], x, y, w=4, b=2, color=\"r\")\n",
    "plot_delta_line(axs[1], x, y, w=3, b=2, color=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача **поиска оптимальных параметров** модели сводится к задаче **поиска минимума функции потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск локального минимума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как будет выглядеть ландшафт функции потерь для задачи линейной регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(-10, 30, 1)\n",
    "b = np.arange(-10, 10, 1)\n",
    "\n",
    "w, b = np.meshgrid(w, b)\n",
    "\n",
    "loss = np.zeros_like(w)\n",
    "for i in range(w.shape[0]):\n",
    "    for j in range(w.shape[1]):\n",
    "        loss[i, j] = np.sum((y - (w[i, j] * x + b[i, j])) ** 2) / (len(x))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "surf = ax.plot_surface(w, b, loss, cmap=plt.cm.RdYlGn_r, alpha=0.5)\n",
    "\n",
    "ax.contourf(w, b, loss, zdir=\"z\", offset=-1, cmap=\"RdYlGn_r\", alpha=0.5)\n",
    "ax.set_zlim(0, 20)\n",
    "\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"b\")\n",
    "ax.set_title(\"MSE\")\n",
    "\n",
    "fig.colorbar(surf, location=\"left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимым (но недостаточным) условием локального минимума дифференцируемой функции является равенство нулю частных производных:\n",
    "\n",
    "$$\t\\begin{equation*}\n",
    " \\begin{cases}\n",
    "   \\displaystyle\\frac{\\partial \\text{Loss}}{\\partial w}=0,\n",
    "   \\\\\n",
    "   \\displaystyle\\frac{\\partial \\text{Loss}}{\\partial b}=0.\n",
    " \\end{cases}\n",
    "\\end{equation*} $$\n",
    "\n",
    "Т.к. MSE для линейной регрессии — полином второй степени относительно $w$ и $b$, а полином второй степени не может иметь больше одного экстремума, то локальный минимум будет глобальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Метод наименьших квадратов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем простейшую модель линейной регрессии с использованием библиотеки NumPy на датасете, определённом выше.\n",
    "\n",
    "Используем метод наименьших квадратов: [МНК, простейшие частные случаи 📚[wiki]](https://ru.wikipedia.org/wiki/Метод_наименьших_квадратов#Простейшие_частные_случаи).\n",
    "\n",
    "$$w = \\frac{n\\sum_{i=1}^nx_iy_i - (\\sum_{i=1}^nx_i)(\\sum_{i=1}^ny_i)}{n\\sum_{i=1}^nx_t^2 - (\\sum_{i=1}^n x_t)^2};$$\n",
    "\n",
    "$$b = \\frac{\\sum_{i=1}^ny_i - w(\\sum_{i=1}^nx_i)}{n}.$$\n",
    "\n",
    "По сути метод наименьших квадратов — это решение системы уравнений выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_coef(x, y):\n",
    "    n = len(x)\n",
    "    w = (n * sum(np.multiply(x, y)) - sum(x) * sum(y)) / (\n",
    "        n * sum(np.multiply(x, x)) - sum(x) ** 2\n",
    "    )\n",
    "    b = (sum(y) - w * sum(x)) / n\n",
    "    return w, b\n",
    "\n",
    "\n",
    "w, b = estimate_coef(x, y)\n",
    "\n",
    "y_predicted = w * x + b\n",
    "\n",
    "print(f\"Estimated coefficients:\\nb = {b[0]:.3f} \\nw = {w[0]:.3f}\")\n",
    "print(f\"Final equation: \\ny = {w[0]:.3f}x +{b[0]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=10)\n",
    "plt.plot(x, y_predicted, color=\"g\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученное решение близко к исходной  зависимости $y=3\\cdot x + 2$.\n",
    "\n",
    "Для многомерного случая МНК можно записать [решение 📚[book]](https://sun.tsu.ru/mminfo/2016/Dombrovski/book/chapter-3/chapter-3-2.htm) в матричном виде:  \n",
    "$$\\vec w = (X^TX)^{-1}X^T\\vec y$$\n",
    "где $\\vec w$ — вектор параметров модели, включающий $b$ (при записи этого решения используется трюк “столбец единиц”, о котором мы поговорим чуть позже),\n",
    "$X$ — матрица входных признаков (с единичным столбцом),\n",
    "$\\vec y$ — вектор предсказываемых значений.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним метрики регрессии с предыдущей лекции.\n",
    "\n",
    "В методе наименьших квадратов мы минимизируем $\\text{MSE}$, эта метрика имеет [размерность 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D0%B8) квадрата размерности предсказываемого значения.\n",
    "\n",
    "$$ \\text{MSE}  = \\frac{1}{N} \\sum (y_i - f(x_i))^2$$\n",
    "\n",
    "Чтобы получить оценку ошибки той же размерности, можно взять корень (root) от $\\text{MSE}$. Это метрика $\\text{RMSE}$:\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum (y_i - f(x_i))^2}$$\n",
    "\n",
    "Или посчитать среднюю абсолютную ошибку $\\text{MAE}$:\n",
    "\n",
    "$$ \\text{MAE} = \\frac{1}{N} \\sum |y_i - f(x_i)|$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также мы будем работать с $R^2$, которая принимает значения от $(-\\inf, 1]$, где $1$  —  наилучший вариант. $R^2$  — [коэффициент детерминации 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%B4%D0%B5%D1%82%D0%B5%D1%80%D0%BC%D0%B8%D0%BD%D0%B0%D1%86%D0%B8%D0%B8). Она характеризует долю дисперсии целевого значения, которую объясняет модель.\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\text{MSE}}{\\sigma^2}=1 - \\frac{\\sum {(y_i-f(x_i))^2}}{\\sum{(y_i-\\bar{y})^2}},$$\n",
    "\n",
    "$$\\bar{y} = \\frac{1}{N}\\sum {y_i},$$\n",
    "\n",
    "где $\\sigma^2$ — дисперсия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "\n",
    "def print_metrics(y_true, y_predicted):\n",
    "    print(f\"Mean squared error: {mean_squared_error(y_true, y_predicted):.3f}\")\n",
    "    print(\n",
    "        \"Root mean squared error: \",\n",
    "        f\"{root_mean_squared_error(y_true, y_predicted):.3f}\",\n",
    "    )\n",
    "    print(f\"Mean absolute error: {mean_absolute_error(y_true, y_predicted):.3f}\")\n",
    "    print(f\"R2 score: {r2_score(y_true, y_predicted):.3f}\")\n",
    "\n",
    "\n",
    "print_metrics(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/linear_regression.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Когда $R^2$ около нуля, модель плохо объясняет данные</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://xkcd.com/1725\">https://xkcd.com/1725</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель линейной регрессии из библиотеки scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свою линейную регрессию мы написали. Теперь поработаем с моделью из Sklearn.\n",
    "\n",
    "Рассмотрим задачу предсказания успеваемости студента на основе информации о количестве потраченного им на изучение материала времени в часах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет [Students Score 🛠️[doc]](https://www.kaggle.com/datasets/shubham47/students-score-dataset-linear-regression). Датасет содержит два числовых значения — часы и результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/student_scores.csv\"\n",
    ")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости результата от затраченного времени, а также отобразим распределения каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.jointplot(data=dataset, x=\"Hours\", y=\"Scores\", height=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на train и test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.iloc[:, :-1].values  # column Hours\n",
    "y = dataset.iloc[:, 1].values  # column Score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае мультимодального (несколько пиков) распределения значений целевой переменной в задаче регрессии может быть полезно задуматься о стратификации данных. Стратификация данных для задачи регрессии специфична и не реализована в Sklearn, о ней можно почитать в [Regression Analysis Based on Stratified Samples 🎓[article]](https://www.jstor.org/stable/2336525?seq=1), пример кода можно найти на [форуме ✏️[blog]](https://datascience.stackexchange.com/questions/33140/stratify-on-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель для линейной регрессии. Чтобы не писать с нуля, воспользуемся готовой моделью из библиотеки Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.linspace(min(x_train), max(x_train), 100)  # 100 dots at min to max\n",
    "y_pred = regressor.predict(x_points)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_train, y_train, \"o\", label=\"Scores\")\n",
    "plt.plot(\n",
    "    x_points,\n",
    "    y_pred,\n",
    "    label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_),\n",
    ")\n",
    "plt.title(\"Hours vs Percentage\", size=12)\n",
    "plt.xlabel(\"Hours Studied\", size=12)\n",
    "plt.ylabel(\"Percentage Score\", size=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем предсказание для тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "x_points = np.linspace(min(x_test), max(x_test), 100)\n",
    "y_pred = regressor.predict(x_points)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_test, y_test, \"o\", label=\"Scores\")\n",
    "plt.plot(\n",
    "    x_points,\n",
    "    y_pred,\n",
    "    label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_),\n",
    ")\n",
    "plt.title(\"Hours vs Percentage\", size=12)\n",
    "plt.xlabel(\"Hours Studied\", size=12)\n",
    "plt.ylabel(\"Percentage Score\", size=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неплохо.\n",
    "\n",
    "Посчитаем метрики для наших значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)\n",
    "print_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будьте осторожнее: модели отражают только те закономерности, которые видели в данных. Вероятность того, что студент, потративший на подготовку 20 часов, получит больше максимального балла, мала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/extrapolating.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://xkcd.com/605\">https://xkcd.com/605</a></em></center>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы решали задачу линейной регрессии аналитически (МНК), но это не всегда возможно по нескольким причинам:\n",
    "* Для аналитического решения нужно считать обратную матрицу, это вычислительно сложно и матрица бывает плохо определенной.\n",
    "* Данных может быть слишком много для того, чтобы их можно было одновременно положить в память для расчета обратной матрицы.\n",
    "* Модели могут быть слишком сложными для поиска аналитического решения. Для сложных моделей ландшафт функции потерь может иметь рельеф с несколькими локальными минимумами. Например, при использовании более сложных функций потерь или в случае использования моделей, нелинейных по своим параметрам.\n",
    "\n",
    "Давайте поговорим о том, что делать в таком случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод, который мы будем использовать, называется **“метод градиентного спуска”**. Для начала вспомним, что такое **градиент**. Возьмем функцию двух переменных:\n",
    "\n",
    "$$\\large f(x, y) = \\sin(x\\cdot y)$$\n",
    "\n",
    "Она будет отличаться от функции потерь, которую мы визуализировали, тем, что у нее будет не один экстремум, а сложный рельеф. Рассчитаем ее на диапазоне значений от $0$ до $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = lambda x, y: np.sin(x * y)\n",
    "\n",
    "x = np.linspace(0, 4, 1000)\n",
    "y = np.linspace(0, 4, 1000)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "zz = f(xx, yy)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 7))\n",
    "\n",
    "\n",
    "def show_3d(xx, yy, zz, fig):\n",
    "    ax = fig.add_subplot(121, projection=\"3d\")\n",
    "    surf = ax.plot_surface(xx, yy, zz, cmap=plt.cm.RdYlGn_r)\n",
    "\n",
    "    ax.contourf(xx, yy, zz, zdir=\"zz\", offset=-2, cmap=\"RdYlGn_r\")\n",
    "    ax.set_zlim(-2, 2)\n",
    "\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(\"sin(xy)\")\n",
    "    fig.colorbar(surf, location=\"left\")\n",
    "\n",
    "\n",
    "show_3d(xx, yy, zz, fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eсли $\\varphi = \\varphi(\\vec{x})=\\varphi(x_1 \\dots x_n)$ — функция $n$ переменных, то её градиентом называется $n$-мерный вектор:\n",
    "$$\n",
    "\\nabla \\varphi(\\vec{x})=\n",
    "\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial\\varphi}{\\partial x_1}\\\\\n",
    "\\displaystyle\\frac{\\partial\\varphi}{\\partial x_2}\\\\\n",
    "...\\\\\n",
    "\\displaystyle\\frac{\\partial\\varphi}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем градиент нашей функции $f(x, y)$. Для этого воспользуемся [**таблицей производных** 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%BD%D1%8B%D1%85) и правилом вычисления [**производной сложной функции** 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%84%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D0%B9_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8) (Chain-rule):\n",
    "$$\\frac {\\partial f} {\\partial x} = \\frac {\\partial f} {\\partial t} \\cdot \\frac {\\partial t} {\\partial x}$$\n",
    "\n",
    "Это правило очень нам пригодится в будущем.\n",
    "\n",
    "$$\\nabla f(x, y)=\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial f}{\\partial x}\\\\\n",
    "\\displaystyle\\frac{\\partial f}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "\\displaystyle\\frac{\\partial\\sin(xy)}{\\partial(xy)}\\cdot\\frac{\\partial(xy)}{\\partial x}\\\\\n",
    "\\displaystyle\\frac{\\partial\\sin(xy)}{\\partial(xy)}\\cdot\\frac{\\partial(xy)}{\\partial y}\\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\cos(xy)\\cdot y\\\\\n",
    "\\cos(xy)\\cdot x\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Посчитаем градиент на том же диапазоне (сетка реже, т.к. мы будем рисовать не точки, а стрелочки):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradf = lambda x, y: (np.cos(x * y) * y, np.cos(x * y) * x)\n",
    "\n",
    "xsmall = np.linspace(0, 4, 15)\n",
    "ysmall = np.linspace(0, 4, 15)\n",
    "xxsmall, yysmall = np.meshgrid(xsmall, ysmall)\n",
    "gradx, grady = gradf(xxsmall, yysmall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как **значение градиента в точке** — это вектор, мы можем говорить о его **величине** и **направлении**. Визуализируем наши расчеты: посмотрим на ландшафт функции $f(x, y)$ и направления градиентов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "show_3d(xx, yy, zz, fig)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(\n",
    "    zz,\n",
    "    extent=(np.min(x), np.max(x), np.min(y), np.max(y)),\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.quiver(xxsmall, yysmall, gradx, grady)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке выше значения градиента в точке обозначены чёрными стрелочками. Можно заметить, что длина стрелок в  районе максимальных и минимальных значений функции **почти нулевая**, стрелки направлены в направлении возрастания значения функции и наиболее длинные стрелки находятся в области наиболее резкого изменения значений функции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это проявление **свойств градиента**:\n",
    "* Направление $\\frac{\\nabla f}{||\\nabla f||}$ — сообщает нам направление максимального роста функции.\n",
    "\n",
    "*  Величина $||\\nabla f||$ — характеризует мгновенную скорость изменения значений функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим еще раз данные с зависимостью оценок студентов от времени подготовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/student_scores.csv\"\n",
    ")\n",
    "\n",
    "x = dataset.iloc[:, :-1].values  # column Hours\n",
    "y = dataset.iloc[:, 1].values  # column Score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем **код для интерактивной визуализации**. Он нужен только для объяснения и **не пригодится вам в работе**. Его разбирать мы не будем. Eсли интересно, можно изучить самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title *Code for interactive visual\n",
    "# source: https://github.com/TomasBeuzen/deep-learning-with-pytorch\n",
    "\n",
    "!wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/interactive_visualization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты рассмотрим одномерный случай. Будем подбирать только $w$, значение $b$ зафиксируем на уровне $2.83$. Визуализируем ошибку и значения $\\dfrac{\\partial \\text{Loss}}{\\partial w}$ для MSE Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_grid_search\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "slopes = np.arange(5, 15, 0.5)\n",
    "prediction = {f\"{w}\": w * x_train[:, 0] + 2.83 for w in slopes}\n",
    "mse = np.array([mean_squared_error(y_train, w * x_train[:, 0] + 2.83) for w in slopes])\n",
    "dmse_dw = np.array(\n",
    "    [(2 * x_train[:, 0] * (w * x_train[:, 0] + 2.83 - y_train)).mean() for w in slopes]\n",
    ")\n",
    "plot_grid_search(x_train[:, 0], y_train, slopes, prediction, mse, dmse_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Видно, что оптимальное значение наклона соответствует минимуму MSE и нулю частной производной $\\dfrac{\\partial \\text{Loss}}{\\partial w}$. Аналогично будет, если мы возьмем в качестве Loss MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "slopes = np.arange(5, 15, 0.5)\n",
    "prediction = {f\"{w}\": w * x_train[:, 0] + 2.83 for w in slopes}\n",
    "mae = np.array([mean_absolute_error(y_train, w * x_train[:, 0] + 2.83) for w in slopes])\n",
    "dmae_dw = np.array(\n",
    "    [\n",
    "        (x_train[:, 0] * np.sign(w * x_train[:, 0] + 2.83 - y_train)).mean()\n",
    "        for w in slopes\n",
    "    ]\n",
    ")\n",
    "plot_grid_search(x_train[:, 0], y_train, slopes, prediction, mae, dmae_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, т.к. градиент указывает направление наибольшего возрастания функции:\n",
    "\n",
    "* если $\\dfrac{\\partial \\text{Loss}}{\\partial w} < 0$, то нам имеет смысл “идти” в сторону возрастания $\\dfrac{\\partial \\text{Loss}}{\\partial w}$;\n",
    "\n",
    "* если $\\dfrac{\\partial \\text{Loss}}{\\partial w} > 0$ — в сторону убывания.\n",
    "\n",
    "**Метод градиентного спуска** — итеративный метод, идея которого заключается в том, чтобы небольшими шажками “идти” в **обратную от градиента сторону**:\n",
    "\n",
    "$$\\large \\vec w_{n+1} = \\vec w_{n} - α \\cdot \\nabla_{\\vec w_{n}} \\text{Loss},$$\n",
    "где $α$ — скорость обучения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем реализовать это в коде (для простоты только для $w$ при $b=2.83$).\n",
    "\n",
    "Обратите внимание:\n",
    "- предсказание считается `w * x_test + b`\n",
    "- градиент считается `2 * (x * (w * x + b - y)).mean()`\n",
    "\n",
    "Через пепемножение скалярных величин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, w, b):\n",
    "    return 2 * (x * (w * x + b - y)).mean()\n",
    "\n",
    "\n",
    "def gradient_descent(x_train, y_train, x_test, y_test, w, alpha, b=2.83, iteration=10):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = [w]\n",
    "    mse_train = [mean_squared_error(y_train, w * x_train + b)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, w * x_test + b)]\n",
    "    prediction = {w: w * x_train + b}\n",
    "    print(\n",
    "        f\"Iteration 0: w = {w:.2f}, Loss_train = {mse_train[0]:.2f}, \"\n",
    "        f\"Loss_test = {mse_test[0]:.2f}.\"\n",
    "    )\n",
    "    for i in range(iteration):\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_train, y_train, w, b)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws.append(w)\n",
    "        mse_train.append(mean_squared_error(y_train, w * x_train + b))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, w * x_test + b))\n",
    "        prediction[w] = w * x_train + b\n",
    "        print(\n",
    "            f\"Iteration {i+1}: w = {w:.2f}, Loss_train = {mse_train[i]:.2f}, \"\n",
    "            f\"Loss_test = {mse_test[i]:3.2f}.\"\n",
    "        )\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашу модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.01, iteration=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем процесс обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_gradient_descent\n",
    "\n",
    "plot_gradient_descent(x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что за 7 эпох мы получили то же значение $w$, что получали при использовании `LinearRegression`. При этом мы пришли в минимум MSE и ноль градиента.\n",
    "\n",
    "\"Эпохой\" обучения мы будем называть последовательность шагов оптимизации, при которой мы использовали информацию обо всех данных из тренировочной выборки. В нашем примере мы использовали всю обучающую выборку на каждом шаге градиентного спуска, потому \"шаги градиентного спуска\" и \"эпохи\" в данном примере синонимичны.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальности мы будем работать с функциями многих переменных, поэтому смотреть на сходимость по одной переменной — не самый оптимальный вариант. Более эффективно будет посмотреть на зависимость Loss от количества эпох для train и test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mse(mse_train, mse_test):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(mse_train, label=\"train\")\n",
    "    plt.plot(mse_test, label=\"test\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel(\"iterations\", fontsize=12)\n",
    "    plt.ylabel(\"MSE Loss\", fontsize=12)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие графики называют кривыми обучения. Посмотрим на кривые обучения при нашей скорости обучения $α=0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что **Loss падает** как на **train**, так и на **test** выборке. Также мы можем сказать, что **сеть обучилась**: train и test **графики вышли на плато**. При этом не произошло **переобучение**: ошибка на **test** выборке **не начала расти** (про переобучение поговорим позже).\n",
    "\n",
    "В полученных графиках есть особенность, которая бросается в глаза опытному в обучении моделей человеку: **Loss на test выборке меньше, чем на train**. Это показатель того, что **с данными что-то не так**. Так бывает при утечке данных (об утечке данных вы подробнее узнаете в следующих лекциях), но в данном случае, test выборка просто слишком мала, чтобы отражать генеральную совокупность (всего 5 студентов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор скорости обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Скорость (шаг) обучения** — некоторый коэффициент, как правило, небольшой, который позволяет нам управлять размером шага. У нас есть локальный минимум, в который мы хотим попасть. Если мы сделаем слишком большой шаг, то мы его перескочим (график справа). Нужно подобрать шаг, который не позволит перескочить минимум, но в то же время такой, чтобы тот же процесс не шел слишком медленно (как на графике слева)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/learning_rate_optimal_value.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на скорость обучения на нашем примере. При **маленькой скорости обучения** мы будем очень медленно сходиться к минимуму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.0005, iteration=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спустя 30 итераций оранжевая прямая плохо отражает генеральную совокупность. Мы не достигли минимума MSE и нуля градиента.\n",
    "\n",
    "Посмотрим, как выглядят кривые обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель недообучена — значения Loss не вышли на плато.\n",
    "\n",
    "Посмотрим на **достаточно большую скорость обучения**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0], y_train, x_test[:, 0], y_test, w=5, alpha=0.027, iteration=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг, получаемый умножением градиента на скорость обучения, получается достаточно большим, чтобы “перескочить” локальный минимум, но при этом модель все-таки попадает в него. Кривые обучения при этом успешно выходят на плато."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В финале посмотрим на **очень большую скорость обучения**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train[:, 0],\n",
    "    y_train,\n",
    "    x_test[:, 0],\n",
    "    y_test,\n",
    "    w=5,\n",
    "    alpha=0.034,\n",
    "    iteration=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг, который мы делаем, слишком большой. Мы не попадаем в локальный минимум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(x_train[:, 0], y_train, slopes, prediction, mse_train, dmse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "По кривым обучения видно, что модель не сошлась: ошибка растет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор скорости обучения будет зависеть от модели и данных. В 7 лекции вы познакомитесь с различными модификациями метода градиентного спуска и узнаете больше о выборе скорости обучения, а пока ориентируйтесь на кривые обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Единый подход к учету смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока мы настраивали только одну переменную, но даже в случае предсказания оценки по времени подготовки у нас две переменные: вес $w$ и смещение $b$.\n",
    "\n",
    "Когда признаков станет больше, у нас получится “лапша” из слагаемых:\n",
    "$$y = b + w_1\\cdot x_1 + w_2\\cdot x_2 + w_3\\cdot x_3 + w_4\\cdot x_4 + w_5\\cdot x_5 + ... + w_n\\cdot x_n$$\n",
    "\n",
    "Нам бы хотелось записать их компактно, чтобы не усложнять код и использовать один и тот же код для данных с разным количеством признаков.  Для этого мы будем использовать матричное перемножение и трюк **“столбец единиц”**, который реализует **единый подход к учету смещения**.\n",
    "\n",
    "Обозначим вектор-столбец из настраиваемых параметров:\n",
    "$$\\vec w = \\begin{bmatrix}\n",
    "b \\\\ w \\\\\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.5], [5]])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К матрице (в нашем случае был только один признак, поэтому у нас будет вектор-столбец) признаков слева \"дорисуем\" столбец единиц:\n",
    "$$X = \\begin{bmatrix}\n",
    "1 & X \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 2.7 \\\\\n",
    "1 & 3.3 \\\\\n",
    "... & ...\\\\\n",
    "1 & 9.2 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Предупреждение:** добавлять столбец единиц нужно, только если вы сами пишете модель. **Если вы пользуетесь готовыми моделями, в этом нет необходимости.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.hstack((np.ones((x_train.shape[0], 1)), x_train))\n",
    "x_test = np.hstack((np.ones((x_test.shape[0], 1)), x_test))\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрицу $X$ можно матрично перемножить со столбцом $\\vec w$, т.к количество столбцов $X$ совпадает с количеством строк в $\\vec w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае:\n",
    "\n",
    "$$\\large \\vec y = b + w_1\\cdot x_1 + w_2\\cdot x_2 + w_3\\cdot x_3 + w_4\\cdot x_4 + w_5\\cdot x_5 + ... + w_n\\cdot x_n = X\\vec w $$  \n",
    "\n",
    "Эту формулу можно свести к нескольким символам кода (`@` — матричное умножение):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x_test @ w\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Необходимость нормализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем многомерный градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "y_test = np.expand_dims(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание:\n",
    "- предсказание считается `x_train @ w`\n",
    "- градиент считается `2 * (x.T @ (x @ w) - x.T @ y) / len(x)`\n",
    "\n",
    "Через перемножение матриц!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, w):\n",
    "    \"\"\"Gradient of mean squared error.\"\"\"\n",
    "    return 2 * (x.T @ (x @ w) - x.T @ y) / len(x)\n",
    "\n",
    "\n",
    "def gradient_descent(x_train, y_train, x_test, y_test, w, alpha, iteration=10):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = np.zeros((iteration + 1, 2))\n",
    "    ws[0] = w[:, 0]\n",
    "    mse_train = [mean_squared_error(y_train, x_train @ w)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, x_test @ w)]\n",
    "    prediction = {(w[0][0], w[1][0]): x_train @ w}\n",
    "\n",
    "    print(\n",
    "        f\"Iteration 0: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "        f\"Loss_train = {mse_train[0]:.2f}, \"\n",
    "        f\"Loss_test = {mse_test[0]:.2f}.\"\n",
    "    )\n",
    "\n",
    "    for i in range(iteration):\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_train, y_train, w)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws[i + 1] = w[:, 0]\n",
    "        mse_train.append(mean_squared_error(y_train, x_train @ w))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, x_test @ w))\n",
    "        prediction[(w[0][0], w[1][0])] = x_train @ w\n",
    "\n",
    "        print(\n",
    "            f\"Iteration {i+1}: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "            f\"Loss_train = {mse_train[i]:.2f}, \"\n",
    "            f\"Loss_test = {mse_test[i]:3.2f}.\"\n",
    "        )\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.5], [5]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    w,\n",
    "    0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы не дошли до оптимальной прямой $y = 9.68x+2.83$, которую вычисляли выше.\n",
    "\n",
    "При этом график Loss выглядит неплохо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такое поведение связано с ландшафтом функции потерь: значение ошибки по оси $b$ изменяется намного медленнее, чем по оси $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_grid_search_2d\n",
    "\n",
    "intercepts = np.arange(-7.5, 12.5, 0.1)  # b\n",
    "slopes = np.arange(5, 15, 0.1)  # w\n",
    "plot_grid_search_2d(x_train[:, 1], y_train, slopes, intercepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому основное изменение значений происходит вдоль оси $w$, а $b$ меняется слабо (значение $b$ далеко от ожидаемого)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_gradient_descent_2d\n",
    "\n",
    "plot_gradient_descent_2d(\n",
    "    x_train[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws,\n",
    "    slopes,\n",
    "    intercepts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы исправить ситуацию, применим `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(np.expand_dims(x_train[:, 1], axis=1)).flatten()\n",
    "x_test_scaled = scaler.transform(np.expand_dims(x_test[:, 1], axis=1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts = np.arange(40, 60, 0.1)  # b\n",
    "slopes = np.arange(15, 35, 0.1)  # w\n",
    "\n",
    "plot_grid_search_2d(x_train_scaled, y_train, slopes, intercepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = np.hstack(\n",
    "    (np.ones((len(x_train_scaled), 1)), np.expand_dims(x_train_scaled, axis=1)),\n",
    ")\n",
    "\n",
    "x_test_scaled = np.hstack(\n",
    "    (np.ones((len(x_test_scaled), 1)), np.expand_dims(x_test_scaled, axis=1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. диапазоны $x$ изменились, значения $w$ и $b$ тоже изменятся.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[57.0], [33.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = gradient_descent(\n",
    "    x_train_scaled, y_train, x_test_scaled, y_test, w, 0.35, iteration=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mse(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что после нормализации мы сходимся к $y = 9.68x + 2.83$.  Для этого используем данные о матожидании и дисперсии из `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ws[-1][0] - ws[-1][1] * scaler.mean_ / (scaler.var_) ** 0.5\n",
    "w = ws[-1][1] / (scaler.var_) ** 0.5\n",
    "\n",
    "print(f\"y = {w[0]:.2f}x + {b[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По визуализации видно, что $w$ и $b$ изменяются во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws,\n",
    "    slopes,\n",
    "    intercepts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cтохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ✏️ Пост о стохастическом градиентом спуске](https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter2_stochastic-gradient-descent.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы обучали модель, рассчитывая градиент по **всей train выборке**. То есть один шаг градиентного спуска мог быть \"долгим\" и **вычислительно сложным** из-за необходимости определения градиента функции потерь на всех потенциально многочисленных train объектах. Можем ли мы совершать шаги градиентного спуска \"чаще\" и существенно ускорить сходимость модели к верному ответу?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому появляется идея **стохастического градиентного спуска**: мы можем делать шаг обучения, рассчитывая градиент не по всей выборке (**batch**), а по нескольким случайно выбранным объектам (**mini-batch**) или даже по одному случайно выбранному объекту (**stochastic**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/define_size_of_batch.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно [показать 📚[book]](https://academy.yandex.ru/handbook/ml/article/shodimost-sgd), что **стохастический градиентный спуск сходится к минимуму** (глобальному или локальному) функции потерь, как и \"обычный\" градиентный спуск, при меньшем объеме вычислений.\n",
    "Важным условием является случайный выбор объектов. Если мы будем использовать одну и ту же последовательность выборок, это приведет к накоплению ошибки и смещению результата.\n",
    "\n",
    "Добавим создание подвыборки к нашему алгоритму:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    w,\n",
    "    alpha,\n",
    "    iteration=10,\n",
    "    batch_size=None,\n",
    "):\n",
    "    \"\"\"Gradient descent for optimizing slope in simple linear regression\"\"\"\n",
    "    # history\n",
    "    ws = np.zeros((iteration + 1, 2))\n",
    "    ws[0] = w[:, 0]\n",
    "    mse_train = [mean_squared_error(y_train, x_train @ w)]\n",
    "    dmse_train = []\n",
    "    mse_test = [mean_squared_error(y_test, x_test @ w)]\n",
    "    prediction = {(w[0][0], w[1][0]): x_train @ w}\n",
    "\n",
    "    print(\n",
    "        f\"Iteration 0: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "        f\"Loss_train = {mse_train[0]:.2f}, \"\n",
    "        f\"Loss_test = {mse_test[0]:.2f}.\"\n",
    "    )\n",
    "\n",
    "    for i in range(iteration):\n",
    "        if not batch_size:\n",
    "            x_sample = x_train\n",
    "            y_sample = y_train\n",
    "        else:\n",
    "            indxs = np.random.choice(x_train.shape[0], batch_size)\n",
    "            x_sample = x_train[indxs, :]\n",
    "            y_sample = y_train[indxs, :]\n",
    "\n",
    "        # adjust w based on gradient * learning rate\n",
    "        grad = gradient(x_sample, y_sample, w)\n",
    "        w -= alpha * grad  # adjust w based on gradient * learning rate\n",
    "        # history\n",
    "        ws[i + 1] = w[:, 0]\n",
    "        mse_train.append(mean_squared_error(y_train, x_train @ w))\n",
    "        dmse_train.append(grad)\n",
    "        mse_test.append(mean_squared_error(y_test, x_test @ w))\n",
    "        prediction[(w[0][0], w[1][0])] = x_train @ w\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Iteration {i+1}: b = {w[0][0]:.2f}, w = {w[1][0]:.2f}, \"\n",
    "                f\"Loss_train = {mse_train[i]:.2f}, \"\n",
    "                f\"Loss_test = {mse_test[i]:3.2f}.\"\n",
    "            )\n",
    "    return ws, prediction, mse_train, dmse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сравнить результаты, будем использовать одни и те же количество итераций и скорость обучения. Чтобы компенсировать стохастичность, возьмем маленькое значение $\\alpha$ и $100$ итераций.\n",
    "\n",
    "Для всего train сета мы посчитаем градиент для $20\\cdot100 = 2000$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[57.0], [33.0]])\n",
    "ws, prediction, mse_train, dmse_train, mse_test = stochastic_gradient_descent(\n",
    "    x_train_scaled,\n",
    "    y_train,\n",
    "    x_test_scaled,\n",
    "    y_test,\n",
    "    w,\n",
    "    0.02,\n",
    "    iteration=100,\n",
    "    batch_size=None,\n",
    ")\n",
    "\n",
    "f1 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws,\n",
    "    slopes,\n",
    "    intercepts,\n",
    "    mode=\"lines\",\n",
    "    title=\"Batch gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для стохастического градиентного спуска (размер $\\text{batch}=1$) мы посчитаем градиент для $1\\cdot100 = 100$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([[57.0], [33.0]])\n",
    "ws_stohastic, prediction, mse_train, dmse_train, mse_test = stochastic_gradient_descent(\n",
    "    x_train_scaled,\n",
    "    y_train,\n",
    "    x_test_scaled,\n",
    "    y_test,\n",
    "    w,\n",
    "    0.02,\n",
    "    iteration=100,\n",
    "    batch_size=1,\n",
    ")\n",
    "f2 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws_stohastic,\n",
    "    slopes,\n",
    "    intercepts,\n",
    "    mode=\"lines\",\n",
    "    title=\"Stochastic gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для стохастического спуска с $\\text{mini-batch}=5$ мы посчитаем градиент для $5\\cdot100=500$ точек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([[57.0], [33.0]])\n",
    "(\n",
    "    ws_mini_batch,\n",
    "    prediction,\n",
    "    mse_train,\n",
    "    dmse_train,\n",
    "    mse_test,\n",
    ") = stochastic_gradient_descent(\n",
    "    x_train_scaled,\n",
    "    y_train,\n",
    "    x_test_scaled,\n",
    "    y_test,\n",
    "    w,\n",
    "    0.02,\n",
    "    iteration=100,\n",
    "    batch_size=5,\n",
    ")\n",
    "f3 = plot_gradient_descent_2d(\n",
    "    x_train_scaled[:, 1],\n",
    "    y_train[:, 0],\n",
    "    ws_mini_batch,\n",
    "    slopes,\n",
    "    intercepts,\n",
    "    mode=\"lines\",\n",
    "    title=\"Mini-batch gradient descent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ищем минимум для всех данных.\n",
    "- Градиент, рассчитанный по одному объекту, будет специфичен. Трек обучения  в случае стохастического градиентного спуска будет запутанным, а итоговая ошибка будет расти с увеличением скорости обучения (мы взяли низкую скорость).\n",
    "- Градиент, рассчитанный по нескольким объектам будет давать лучшую оценку градиента для всех данных. Трек будет менее сложным.\n",
    "- Градиент, рассчитанный по всей выборке, будет давать наиболее точное направление (при отсутствии \"шума\" в обучающих данных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interactive_visualization import plot_panel\n",
    "\n",
    "plot_panel(f1, f2, f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для **ускорения расчетов** рекомендовано использовать **максимальный размер mini-batch**, который помещается в память, но это не всегда дает лучший результат. На 7 лекции вы увидите, что для сложных моделей стохастичность, связанная с небольшим размером батча, может помочь выбраться из локального минимума и найти более глубокий.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Условия применимости линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Квартет Энскомбе (Anscombe’s quartet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы применили модель линейной регрессии, посчитали метрики и решили что наша модель достаточно хороша. Но так ли это на самом деле?\n",
    "\n",
    "В 1973 году Фрэнсис Энскомбе тоже [задумался [art] 🎓](https://www.jstor.org/stable/2682899) над этим вопросом. Он предложил 4 набора данных с одинаковыми статистиками, которые показали некоторые типичные ошибки, возникающие при решении регрессионных задач. Посмотрим на эти наборы данных и мы (для простоты тут нет разделения на train/test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала посчитаем статистики набора данных, коэффиценты линейной модели и коэффициент детерминации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "df = sns.load_dataset(\"anscombe\")\n",
    "\n",
    "grouped = df.groupby(\"dataset\")\n",
    "\n",
    "summary_results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"mean_x\",\n",
    "        \"mean_y\",\n",
    "        \"std_x\",\n",
    "        \"std_y\",\n",
    "        \"correlation\",\n",
    "        \"slope\",\n",
    "        \"offset\",\n",
    "        \"R2\",\n",
    "        \"MSE\",\n",
    "        \"MAE\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for key in grouped.groups.keys():\n",
    "    data = grouped.get_group(key)\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(data[\"x\"].values.reshape(-1, 1), data[\"y\"].values)\n",
    "    slope = regressor.coef_[0]\n",
    "    offset = regressor.intercept_\n",
    "    y_pred = regressor.predict(data[\"x\"].values.reshape(-1, 1))\n",
    "    r2 = r2_score(data[\"y\"].values, y_pred)\n",
    "    mse = mean_squared_error(data[\"y\"].values, y_pred)\n",
    "    mae = mean_absolute_error(data[\"y\"].values, y_pred)\n",
    "    summary_results.loc[key] = (\n",
    "        grouped.mean().loc[key][\"x\"],\n",
    "        np.round(grouped.mean().loc[key][\"y\"], 2),\n",
    "        np.round(grouped.std().loc[key][\"x\"], 5),\n",
    "        np.round(grouped.std().loc[key][\"y\"], 2),\n",
    "        np.round(grouped.corr().loc[(key, \"x\")][\"y\"], 3),\n",
    "        np.round(slope, 3),\n",
    "        np.round(offset, 2),\n",
    "        np.round(r2, 2),\n",
    "        np.round(mse, 2),\n",
    "        np.round(mae, 2),\n",
    "    )\n",
    "\n",
    "summary_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что по статистикам наборы данных почти идентичны. Посмотрим на сами данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.lmplot(\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    col=\"dataset\",\n",
    "    hue=\"dataset\",\n",
    "    data=df,\n",
    "    col_wrap=2,\n",
    "    ci=None,\n",
    "    palette=\"muted\",\n",
    "    height=4,\n",
    "    scatter_kws={\"s\": 50, \"alpha\": 1},\n",
    ")\n",
    "plt.gcf().suptitle(\"Anscombe's Quartet\", x=0.5, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что мы видим:\n",
    "- Набор данных “I” - подходит для линейной регрессии,\n",
    "- Набор данных “II” - визуализирует ошибку спецификации. Мы предположили, что $y$ линейно зависит от $x$, а на самом деле зависимость квадратичная.\n",
    "- В наборе данных “III” присутствует выброс, который приводит к смещенной оценке. Нужно разобраться с природой выброса: если это ошибка в данных - удалить, если это реальное значение - необходимо расширение выборки и применение более сложной модели.\n",
    "- Набор данных  “IV” - плохо подходит для регрессии: 9 из 10 значений $x$ совпадают и дают разные значения $y$, что противоречит предположению, что $y = f(x)$.\n",
    "\n",
    "Эти проблемы легко визуализировать и заметить, если у нас только 1 признак, но что делать когда признаков много?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Анализ остатков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут нам нужно познакомиться с [теоремой Гаусса-Маркова](https://ru.wikipedia.org/wiki/Теорема_Гаусса_—_Маркова), согласно которой метод наименьших квадратов дает оптимальную несмещенную оценку $y$, если:\n",
    "* модель данных правильно специфицированна (зависимость действительно линейная), спецификация модели:\n",
    "$$y_i = b+w_1·x_{i1}+w_2⋅x_{i2}+...+w_k⋅x_{ik}+ϵ_i$$\n",
    "где $(X_i, y_i)$ - пары признаков и целевых значений $i\\in(1, 2, ..., n)$,\n",
    "$b$ и $w_i$ - параметры модели, $ϵ_i$ - шум в определении $y_i$\n",
    "* все выборки $X_i$ детерминированы и не все равны между собой,\n",
    "* признаки линейно независимы,\n",
    "* ошибки не носят систематического характера, а именно гомоскедастичность (нулевое матожидание и одинаковая дисперсия ошибок для всех $i$) и отсуствие автокорреляции.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В условиях этой много говориться об ошибках. Давайте визуализируем ошибки/невязки/остатки $\\delta y = y_{true}-y_{pred}$ для квартета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(8, 8), nrows=2, ncols=2)\n",
    "fig.tight_layout(pad=-4.0, w_pad=-3.5, h_pad=-6.0)\n",
    "\n",
    "for key, ax in zip(grouped.groups.keys(), [axs[0][0], axs[0][1], axs[1][0], axs[1][1]]):\n",
    "    data = grouped.get_group(key)\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(data[\"x\"].values.reshape(-1, 1), data[\"y\"].values)\n",
    "    y_pred = regressor.predict(data[\"x\"].values.reshape(-1, 1))\n",
    "    df = {\n",
    "        \"x\": data[\"x\"].values,\n",
    "        \"y\": data[\"y\"].values,\n",
    "        \"dy\": data[\"y\"].values - y_pred,\n",
    "    }\n",
    "    df = pd.DataFrame(df)\n",
    "    g = sns.JointGrid(\n",
    "        data=df,\n",
    "        x=\"y\",  # 'x'\n",
    "        y=\"dy\",\n",
    "        height=4,\n",
    "        ratio=4,\n",
    "    )\n",
    "    g.refline(y=0, color=\"red\")\n",
    "    g.plot_joint(sns.scatterplot)\n",
    "    g.plot_marginals(sns.kdeplot)\n",
    "    g.ax_marg_x.remove()\n",
    "    g.savefig(\"g.png\")\n",
    "    plt.close(g.fig)\n",
    "    ax.imshow(mpimg.imread(\"g.png\"))\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что мы видим: остатки  набора “I” распределены случайно, в то время, как остатки набора “II”, “III” и “IV” - подчиняются некоторой системе. **Вывод: визуализируя остатки можно сделать вывод о применимости линейной регресси.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы посмотрели на корреляцию остатков, кроме нее есть ограничение на **гомоскедастичность** (нулевое матожидание и одинаковая дисперсия ошибок для всех подвыборок). Попробуем применить линейную регрессию к данным построенным по правилу $y=10+0.5x$, дисперсия ошибки будет расти с $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = np.linspace(start=0, stop=10, num=40)\n",
    "y_true = 10 + 0.5 * x\n",
    "y = y_true + rng.normal(loc=0, scale=0.1 + 0.1 * x, size=x.shape[0])\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x[:, np.newaxis], y)\n",
    "y_pred = regressor.predict(x[:, np.newaxis])\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(9, 4), nrows=1, ncols=2)\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "axs[0].scatter(x, y)\n",
    "axs[0].plot(x, y_pred, label=\"y_pred\", color=\"red\")\n",
    "axs[0].plot(x, y_true, label=\"y_true\", color=\"green\")\n",
    "axs[0].set_title(\"Regression\")\n",
    "axs[0].set_xlabel(\"x\")\n",
    "axs[0].set_ylabel(\"y\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].scatter(y, y - y_pred)\n",
    "axs[1].axhline(y=0, color=\"red\")\n",
    "axs[1].set_title(\"Residuals\")\n",
    "axs[1].set_xlabel(\"y\")\n",
    "axs[1].set_ylabel(\"dy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Видим, что предсказание не совсем корректное. Это связано с вкладом больших ошибок. В таком случае имеет смысл посмотреть в сторону [взвешенного метода наименьших квадратов](https://stackoverflow.com/questions/35236836/weighted-linear-regression-with-scikit-learn) или подумать над [стабилизирующим преобразованием](https://ajeyvenkataraman.wordpress.com/2020/03/04/dealing-with-heteroscedasticity-in-python/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее про анализ остатков можно почитать в [книге](https://ema.drwhy.ai/residualDiagnostic.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема корреляции признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто может оказаться, что признаковое описание объекта избыточно и между различными признаками имеются связи. Для устойчивости работы линейных моделей важно, чтобы среди признаков не было линейных связей (скоррелированных пар).\n",
    "\n",
    "Например, если мы будем решать задачу регрессии на наборе признаков $x_1 \\dots x_n$, среди которых есть связь $x_2 = 5 x_1$, и возьмём линейную модель вида\n",
    "$$\\large y = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b,$$\n",
    "то с учётом данной связи мы можем записать:\n",
    "$$\\large y = w_1 x_1 + w_2 (5x_1) + \\dots + w_n x_n + b = (w_1 + 5 w_2) x_1 +  w_3 x_3 + \\dots + w_n x_n + b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, наша модель теперь учитывает признак $x_1$ с одним \"общим\" весом $(w_1 + 5 w_2)$, несмотря на то, что он закодирован двумя независимыми параметрами. Решение, то есть набор весовых коэффициентов $w_i$, перестало быть единственным, так как мы теперь можем делать произвольные преобразования с числами $w_1$ и $w_2$ до тех пор, пока $(w_1 + 5 w_2)$ остаётся неизменным:\n",
    "\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000 ,\\, w_2 \\rightarrow  w_2 - 1000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000 ,\\, w_2 \\rightarrow  w_2 - 1000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000000 ,\\, w_2 \\rightarrow  w_2 - 1000000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + 5000000000000 ,\\, w_2 \\rightarrow  w_2 - 1000000000000 \\} $$\n",
    "$$(w_1 + 5 w_2) = \\{w_1 \\rightarrow  w_1 + Nan ,\\, w_2 \\rightarrow  w_2 + Nan\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чем это плохо?**\n",
    "\n",
    "В случае корреляции признаков задача не имеет единственного решения и не существует обратной матрицы, обеспечивающей аналитическое решение. Мы можем использовать градиентные методы для поиска решения, но\n",
    "при этом веса модели могут неконтролируемо расти. При этом **суммарный вклад** признаков может быть **мал**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/carrelation_problem1.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем оценивать **важность признаков в линейной модели**, используя **веса** перед ними (признаки должны быть нормализованы). Чем больше модуль веса, тем больше вклад. Для коррелированных признаков важность будет переоценена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/carrelation_problem2.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, нужно помнить, что диапазоны числовых переменных ограничены. При неконтролируемом росте весов значение может выйти за диапазон и превратиться в ~~тыкву~~ `Nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/carrelation_problem3.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Что делать?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим **датасет Diabetes** ([Diabetes dataset 🛠️[doc]](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)), в котором таргетом является число, характеризующее стадию заболевания, а на признаками явдяются пол, возраст, иднекс массы тела и результаты анализа крови."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "x = diabetes.data  # features\n",
    "y = diabetes.target  # target\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "print(f\"x[0]: \\n {x[0]}\")\n",
    "print(f\"y[0]: \\n {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстрее и удобнее можно посмотреть на данные, используя pandas. К тому же Colab добавил возможность визуализации данных (для этого можно тыкнуть синий значок диаграммы ▆ █ ▄  справа от таблицы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diabetes_df = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "diabetes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab добавил возможность визуализации данных (для этого можно тыкнуть синий значок диаграммы ▆ █ ▄  справа от таблицы). Colab делает не полную визуализацию признаков, но и на данных изображениях можно найти полезную **информацию о выбросах** (из графика **Values**), **плотности распределений** (из графика **Distributions**) и о наличии **зависимости между переменными** (из графика **2-d distributions**).\n",
    "\n",
    "Кроме того, можно тыкнуть на рисунок, посмотреть и скопировать код визуализации, чтобы применить к другим данным или изменить под свои нужды. Так нажав на **2-d distributions**, скопировав код и поменяв названия признаков, мы можем увидеть, что значения признаков *s1* и *s2* имеют зависимость, близкую к линейной, что не очень хорошо (почему, обсудим позже).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "diabetes_df.plot(kind=\"scatter\", x=\"s1\", y=\"s2\", s=32, alpha=0.8)\n",
    "plt.gca().spines[\n",
    "    [\n",
    "        \"top\",\n",
    "        \"right\",\n",
    "    ]\n",
    "].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 441 строк, в каждой из которой по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные немного странно нормализованы (они нормализованны так, чтобы сумма квадратов значений в каждом столбце была единица), для линейной модели - это не очень хорошо, применим StandartScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(y_val.shape)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "x_train_norm = scaler_x.fit_transform(x_train)\n",
    "y_train_norm = scaler_y.fit_transform(y_train[:, np.newaxis])[:, 0]\n",
    "\n",
    "x_val_norm = scaler_x.transform(x_val)\n",
    "y_val_norm = scaler_y.transform(y_val[:, np.newaxis])[:, 0]\n",
    "\n",
    "diabetes_train_norm = pd.DataFrame(x_train_norm, columns=diabetes.feature_names)\n",
    "diabetes_train_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем линейные зависимости между признаками при помощи построения матрицы попарных корреляций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = diabetes_train_norm.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask,\n",
    "    cmap=cmap,\n",
    "    vmax=1,\n",
    "    vmin=-1,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы уже видели наших данных сильно скоррелированы признаки S1 и S2.\n",
    "\n",
    "В случае корреляции можно:\n",
    "- **добавить регуляризацию**;\n",
    "- **оставить один признак**;\n",
    "- если есть вероятность, что при удалении признаков часть информации будет потеряна, можно оставить **один признак** неизменным и вычесть его из остальных (оставить только **разницу**). Неинформативные шумовые признаки можно удалить (как это делать, вы узнаете на 4-й лекции).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###L2 vs L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем ограничить значения весов, введя в функционал ошибок\n",
    "специальную добавку, называемую регуляризацией. L2-регуляризация имеет формулу:\n",
    "\n",
    "\n",
    "$$\\large L_2 = \\alpha \\sum_i w_i^2,$$\n",
    "где $\\alpha$ — это коэффициент регуляризации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Введение **L2-регуляризации** приводит к тому, что **большие веса больше штрафуются** и предпочтение отдается решениям, использующим **малые значения весов**. При этом модель будет **сохранять скоррелированные и неважные признаки с маленькими весами**.\n",
    "\n",
    "Это связано с градиентом $L_2$:\n",
    "$$\\large L'_{2w_i} = 2\\alpha w_i$$\n",
    "Он будет “тянуть” модель в сторону большого количества маленьких весов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отбора признаков можно использовать L1-регуляризацию, она одинаково \"штрафует\" модель за любые ненулевые веса.\n",
    "\n",
    "$$\\large L_1 = \\alpha \\sum_i |w_i|$$\n",
    "$$\\large L_{1w_i}' = \\alpha, \\text{  где } w_i\\neq 0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как это работает на практике. Для этого применим для этого применим к данным Diabetes линейные модели с регуляризацией,а именнно [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#ridge) (L2 регуляризация), [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#lasso) (L1 регуляризация). Посмотрим на веса признаков при различных значениях $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "n_alphas = 200\n",
    "alphas = np.logspace(-5, 5, n_alphas)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(6, 6), nrows=2, ncols=1)\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "for i, model, model_name, xlim in zip(\n",
    "    [0, 1], [Ridge, Lasso], [\"Ridge (L2)\", \"Lasso (L1)\"], [[0.1, 100_000], [0.0001, 1]]\n",
    "):\n",
    "    coefs = []\n",
    "    coefs_corr = []\n",
    "    for a in alphas:\n",
    "        reg = model(alpha=a)\n",
    "        reg.fit(x_train_norm, y_train_norm)\n",
    "        coefs_corr.append(reg.coef_[4:6])\n",
    "        coefs.append(reg.coef_)\n",
    "    axs[i].plot(alphas, coefs, linestyle=\"--\", alpha=0.5)\n",
    "    axs[i].plot(alphas, coefs_corr, linewidth=2, label=[\"s1\", \"s2\"])\n",
    "    axs[i].set(\n",
    "        xscale=\"log\", title=model_name, xlabel=\"alpha\", ylabel=\"weights\", xlim=xlim\n",
    "    )\n",
    "    axs[i].legend()\n",
    "    axs[i].grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что при маленьком $\\alpha$ - веса не нулевые. При очень большом $\\alpha$ веса стремятся к 0 для всех признаков - это значит, что вклад в Loss регуляризации становится больше, чем вклад ошибки предсказания.\n",
    "\n",
    "Нам интересно поведение весов перед признаками  **s1** и  **s2**, т.к. они скоррелированны. В случае L2 веса принимают близкое небольшое значение и сходятся к 0 практически одновременно. В случае L1 - сначала к 0 сходится вес перед **s2**, затем вес перед **s1**.\n",
    "\n",
    "Интересно отметить, что коэффициенты $\\alpha$ при L1 и L2 отличаются по порядку величины. Это связано с тем, что L2 работает с квадратами весов, а L1 с модулями.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения интуиции, что L1-регуляризация позволяет отбирать признаки, часто используют картинку ниже. Функция ошибки задается как сумма ошибки предсказания $L_{\\text{err}}$ и регуляризации $L_\\text{reg}$.\n",
    "\n",
    "$$ Loss = L_{\\text{err}} + L_\\text{reg}$$\n",
    "где $L_\\text{reg}$ - L1 или L2 регуляризация.\n",
    "\n",
    "Зафиксируем ограничение $L_\\text{reg}=1$ и $\\alpha = 1$. В пространстве весов это даст для L2 ограничение:\n",
    "$$w_1^2+w_2^2 = 1$$\n",
    "Это - единичная окружность (голубая закрашенная область на рисунке ниже слева).\n",
    "\n",
    "Для L1 ограничение:\n",
    "$$|w_1|+|w_2| = 1$$\n",
    "Это - ромб (голубая закрашенная область на рисунке ниже справа).\n",
    "\n",
    "В реальной задаче значение $L_\\text{reg}$ будет меняться при минимизации Loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L02/loss_landscape_with_regularization.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://people.eecs.berkeley.edu/~jrs/189/\">Introduction to Machine Learning\n",
    "</a></em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вращающиеся овалы показывают, как член Loss, отвечающий за ошибку предсказания $L_{\\text{err}}$ (овалы  $L_{\\text{err}}=\\text{const}$, точка  $\\min{L_{\\text{err}}}$), изменяется в результате изменения значений признаков и таргета. Оранжевая точка — это минимальное значение для функции Loss с регуляризацией. Видим что:\n",
    "\n",
    "\n",
    "- Для **L2** оранжевая точка будет кататься **по касательной к окружности**.\n",
    "- Для **L1** оранжевая точка будет зависать в **уголу ромба**, что соответствует **обнулению веса** одного из признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем разобраться, что делать в случае классификации.\n",
    "\n",
    "Рассмотрим задачу классификации на два класса. Например, у нас есть данные о лабораторных мышах. Часть из них имеет нормальную массу тела, а часть — мыши с ожирением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/svm_mouse_example.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак:\n",
    "1. У нас есть набор данных из $N$ объектов (мышей).\n",
    "2. Для каждого из объектов (мышей) нам известно признаковое описание объекта в виде набора вещественных чисел (вес, длина от носа до кончика хвоста, возраст и т.д.). То есть объекту под номером $i$ соответствует вектор $\\vec x_i$.\n",
    "3. Также для каждого объекта нам известна истинная метка класса. Мы знаем, что объекту с признаковым описанием $\\vec x_i$ соответствует метка класса $y_i$. Будем считать, что метки классов принимают значения:\n",
    "$$y_i =\n",
    "\\begin{cases}\n",
    "    1, & \\text{для пухляшей}, \\\\\n",
    "    0, & \\text{для всех остальных}.\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим **одномерный пример**. У нас есть данные только по **массе мышей**. Часть из них определена как мыши с нормальной массой тела, а часть — как мыши с ожирением.\n",
    "\n",
    "Попробуем отделить их друг от друга с использованием линейной регрессии и посмотрим на остатки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "custom_cmap = ListedColormap([\"#B8E1EC\", \"#bea6ff\", \"#FEE7D0\"])\n",
    "\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    x = np.hstack(\n",
    "        [\n",
    "            np.random.uniform(14, 21, total_len // 2),\n",
    "            np.random.uniform(24, 33, total_len // 2),\n",
    "        ]\n",
    "    )\n",
    "    y = np.hstack([np.zeros(total_len // 2), np.ones(total_len // 2)])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "total_len = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "x, y = generate_data(total_len=total_len)\n",
    "x = x[:, np.newaxis]\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x, y)\n",
    "y_pred = regressor.predict(x)\n",
    "\n",
    "x_model = np.linspace(14, 33, 300)\n",
    "x_model = x_model[:, np.newaxis]\n",
    "y_model = regressor.predict(x_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visual_1d_linear_model(x, y, y_pred, x_model, y_model, label):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    fig.tight_layout(pad=4.0)\n",
    "\n",
    "    axs[0].plot(\n",
    "        x_model,\n",
    "        y_model,\n",
    "        label=label,\n",
    "        linewidth=1,\n",
    "    )\n",
    "\n",
    "    axs[0].set(ylabel=\"y\", xlabel=\"mass, g\", title=label)\n",
    "    sns.scatterplot(x=x[:, 0], y=y, hue=y, s=50, ax=axs[0])\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    axs[0].legend(handles, [\"Model\", \"Normal\", \"Obese\"])\n",
    "\n",
    "    sns.scatterplot(x=x[:, 0], y=y - y_pred, hue=y, s=50, ax=axs[1])\n",
    "    axs[1].axhline(y=0, color=\"red\")\n",
    "    axs[1].set(ylabel=\"dy\", xlabel=\"mass, g\", title=\"Residuals\")\n",
    "    handles, labels = axs[1].get_legend_handles_labels()\n",
    "    axs[1].legend(handles, [\"Normal\", \"Obese\"])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visual_1d_linear_model(x, y, y_pred, x_model, y_model, \"Linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остатки сильно скоррелированны. Мы можем внести разделяющее правило\n",
    "$$y_{pred}>0.5, \\text{class} = 1$$\n",
    "$$y_{pred}\\le0.5, \\text{class} = 0$$\n",
    "\n",
    "и считать $Loss=-\\text{accurecy}(y_{true}, y_{pred})$, но такое условие не диффернцируемо. Что делать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к вероятностям\n",
    "\n",
    "Давайте попробуем модифицировать простую линейную модель таким образом, чтобы мы могли трактовать выходы модели как вероятность. Начнем с задачи бинарной классификации. Линейная модель задается уравнением:\n",
    "$$s = f(\\vec x) = (\\vec{w}, \\vec{x}) + b$$\n",
    "\n",
    "Выходы такой модели принимают значения от $-∞$ до $+∞$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Нам бы хотелось получить на выходе вероятность того, что объект принадлежит к классу $1$. Вероятность принимает значения от $0$ до $1$. Нам нужна функция, которая спроецирует диапазон $(-∞,+∞)$ в диапазон $[0, 1]$.\n",
    "\n",
    "Такой функцией является сигмоида:\n",
    "$$p = \\sigma(s) = \\frac{1}{1+e^{-s}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При $s→-∞$: $p→0$.\n",
    "\n",
    "При $s→+∞$: $p→1$.\n",
    "\n",
    "При $s=0$: $p=0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/scores_to_probability.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения $s$ (score) также называют [logit-ом 📚[wiki]](https://en.wikipedia.org/wiki/Logit) (пер. “логарифм”). Это связано с тем, что если выразить logit $s$ через вероятность $p$, то получится формула:\n",
    "$$s(p) = \\log \\left(\\frac{p}{1-p}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно решить, какую функцию потерь использовать. **Вероятности** принимают значения **от $0$ до $1$**. Если мы будем использовать **MAE**, [Hinge loss 📚[wiki]](https://en.wikipedia.org/wiki/Hinge_loss) или **MSE**, **максимальным значением ошибки** на указанном диапазоне будет **$1$**. А нам хочется **максимально штрафовать** модель, если она выдает для правильного класса вероятность 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **log loss** (ее так же называют Cross-Entropy loss, почему - покажем ниже) в том, что мы хотим притянуть вероятность правильного класса к 1 и бесконечно штрафовать модель если вероятность истинного класса 0.\n",
    "\n",
    "Для этого можно использовать функцию логорифма. Она гладкая, и для нее: $\\log(1) = 0$, $\\log(0) = -∞$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/cross_entropy_plot_loss_with_probability.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы можем записать функцию ошибок, как:\n",
    "\n",
    "$$\\text{Loss} = - \\log(p^y_\\text{pred}),$$\n",
    "\n",
    "где $p^y_\\text{pred}$ — предсказанная вероятность для истиного класса. Минус нужен, т.к. $-\\log(0) = ∞$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель с сигмоидой и log loss называется логистической регрессией. Посмотрим, как логистическая регрессия справляется с задачей классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x, y)\n",
    "\n",
    "y_pred = classifier.predict(x)\n",
    "y_model = classifier.predict_proba(x_model)\n",
    "\n",
    "visual_1d_linear_model(x, y, y_pred, x_model, y_model[:, 1], \"Logistic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многоклассовая классификация\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу многоглассовой классификации на примере картинок, например, для датасета CIFAR-10 (он встретится вам в задании). У нас есть входное изображение, и мы хотим получить на выходе 10 чисел, обозначающих уверенность модели в принадлежности изображения к конкретному классу. При этом мы знаем, что наши данные устроенны так, что одно изображение может относиться только к одному классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/img_to_function_get_scores.png\" width=\"600\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем считать по logit-у для каждого класса:\n",
    "$$\\begin{matrix} s_1 = (\\vec{w_1}, \\vec{x}) + b_1\\\\ s_2 = (\\vec{w_2}, \\vec{x}) + b_2\\\\ ... \\\\ s_n = (\\vec{w_n}, \\vec{x}) + b_n \\end{matrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также мы можем записать формулы выше в виде матричного перемножения. Для этого нужно преобразовать матрицу пикселей в вектор признаков (просто вытянув массив в вектор), а строки с весами модели — в матрицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/scalar_product_add_bias.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае для непересекающихся классов результатом классификации будет класс, соответствующий наибольшему logit-у.\n",
    "\n",
    "В дальнейшем мы будем проходить нейронные сети, которые работают очень похоже.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[video] 📺 Объяснение SoftMax от StatQuest](https://www.youtube.com/watch?v=KpKog-L9veg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что наша модель выдала следующие значения logit-ов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "logits = [[5.1,    # cat\n",
    "           3.2,    # car\n",
    "          -1.7]]   # frog\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, чтобы узнать, какой класс наша сеть предсказала, мы могли бы просто взять `argmax` от наших `logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Predicted class = %i (Cat)\" % (np.argmax(logits, axis=1).squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От argmax нельзя посчитать градиент, так как производная от константы равна 0. Соответственно, если бы мы вставили производную от argmax в градиентный спуск, мы бы получили везде нули, и наша модель бы вообще ничему не научилась."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, мы бы хотели получить не logit’ы, а настоящую вероятность $p$ на выходе модели. Да еще и таким образом, чтобы от наших вероятностей можно было посчитать градиент. Для этого мы можем применить к нашим логитам функцию **SoftMax**:\n",
    "$$\\large p(y=k|x=x_i) = \\frac{e^{s_k(x_i)}}{\\sum_{j=1}^ne^{s_j(x_i)}},$$\n",
    "\n",
    "где $x_i$ — набор признаков, характеризующий один объект из выборки,\n",
    "$s_j(x_i)$ — logit для j-го класса для объекта $x_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция **SoftMax** вводится на основании двух свойств вероятностей:\n",
    "\n",
    "1. **Вероятности всегда неотрицательны**. Отобразим наши logit’ы на значения $[0, +∞)$.\n",
    "\n",
    "> Для этого возведем **экспоненту** (число Эйлера $e=2.71828$) **в степень логита**. В результате мы получим вектор гарантированно неотрицательных чисел (положительное число, возведенное в степень, даже отрицательную, даст положительное значение).\n",
    "\n",
    "2. Классы не пересекаются, **сумма вероятностей** по всем классам **равна единице**.\n",
    "\n",
    ">  Мы должны их **нормализовать**, то есть поделить на сумму.\n",
    "\n",
    "Получаются числа, которые можно интерпретировать, как **вероятности**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/linear_classifier_softmax.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\text{SoftMax}_\\text{кошка} = \\frac{e^{5.1}}{e^{5.1} + e^{3.2} + e^{-1.7}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "print(f\"logits: {logits},\")\n",
    "print(f\"probabilities: {softmax(logits)}\")\n",
    "print(\"Sum = %.2f\" % np.sum(softmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что SoftMax никоим образом не поменял порядок значений: самому большому logit'у соответствует самая большая вероятность, а самому маленькому, соответственно, самая маленькая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Практическое вычисление SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вычислении экспоненты от выходов модели могут получиться очень большие числа в силу очень высокой скорости роста экспоненты. Этот факт необходимо учитывать, чтобы вычисления SoftMax были численно стабильны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "f = np.array([[123, 456, 789]])\n",
    "p = softmax(f)\n",
    "print(f\"logits = {f},\\nprobabilities = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы регуляризовать вычисление, нам следует предварительно упростить возникающую в вычислении дробь. Для этого мы можем вычесть из каждого $s_i$ положительную константу, чтобы уменьшить значения экспонент. В качестве константы можно выбрать максимальный элемент этого вектора, тогда у нас гарантированно не будет очень больших чисел, и такой способ будет работать более стабильно.\n",
    "\n",
    "$$\\large M = \\max_j s_{j}(x_i),$$\n",
    "\n",
    "$$\\large s^\\text{new}_{j}(x_i)  = s_{j}(x_i) - M,$$\n",
    "\n",
    "$$\\large \\dfrac {e^{s^\\text{new}_{k}(x_i)}} {\\sum_j e^{s^{new}_{j}(x_i)}}  = \\dfrac {e^{s_{k}(x_i) - M }} {\\sum_j e^{s_{j}(x_i) - M }} = \\dfrac {e^{s_{k}(x_i)}e ^ {-M}} {\\sum_j e^{s_{j}(x_i)} e ^ {-M}} = \\dfrac {e ^ {-M} e^{s_{k}(x_i)}} {e ^ {-M} \\sum_j e^{s_{j}(x_i)} } = \\dfrac { e^{s_{k}(x_i)}} { \\sum_j e^{s_{j}(x_i)} },$$\n",
    "\n",
    "где $x_i$ — набор признаков, характеризующий один объект из выборки,\n",
    "$s_j(x_i)$ — logit для $j$-го класса для объекта $x_i$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "f = np.array([[123, 456, 789]])\n",
    "\n",
    "p = softmax(f)\n",
    "print(f\"logits = {f},\\nprobabilities = {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Замечание:** В реальных вычислениях, SoftMax часто объединяют с Cross-Entropy Loss, это позволяет уменьшить количество операций exp и log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая последовательность преобразований будет выглядеть так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/linear_model_probability_pipeline.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать log loss. Запишем функцию ошибок, как:\n",
    "\n",
    "$$\\text{Loss} = - \\log(p^y_\\text{pred}),$$\n",
    "\n",
    "где $p^y_\\text{pred}$ — предсказанная вероятность для истиного класса. Минус нужен, т.к. $-\\log(0) = ∞$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к задаче классификации картинок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/logits_to_scores_to_probabilitys.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На картинке на входе был изображен кот. Мы можем преобразовать метки классов следующим образом:\n",
    "\n",
    "$$ y → p_\\text{true} $$\n",
    "\n",
    "$$ p_\\text{true_i}= \\begin{cases} 1 & \\text{для i=k, где k - номер истинного класса}, \\\\0 & \\text{для любого } i\\neq k.\n",
    " \\end{cases}$$\n",
    "Таким образом,\n",
    "$$ y → p_\\text{true} = [[1, 0, 0]]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае функция ошибки выше может быть записанна как [кросс-энтропия 📚[wiki]](https://en.wikipedia.org/wiki/Cross-entropy):\n",
    "$$\\text{Loss} = - \\log(p^y_\\text{true})= H(p_\\text{pred}||p_\\text{true})= - \\sum^C_{i=1}p_\\text{true_i}\\cdot \\log(p_\\text{pred_i}) = -1⋅\\log{0.87}-0\\cdot\\log{0.13}-0\\cdot\\log{0.001} = - \\log(p_\\text{cat}) = - \\log{0.87} \\approx0.14$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем это в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(pred_prob, true_prob):\n",
    "    # warning: this code does not take into account the possibility that\n",
    "    # pred_prob could be equal to zero: add epsilon = 1e-9 to fix it\n",
    "    return np.sum(-true_prob * np.log(pred_prob)) / pred_prob.shape[0]\n",
    "\n",
    "\n",
    "# 3 classes 2 items\n",
    "# fmt: off\n",
    "logits = np.array([\n",
    "    [5.1, 3.2, -1.7], # one item\n",
    "  # [2.1, 6.3,  1.5],  # second item\n",
    "])\n",
    "# fmt: on\n",
    "\n",
    "print(f\"Logits = \\n{logits}\\n\")\n",
    "\n",
    "pred_prob = softmax(logits)\n",
    "print(f\"Predicted Probabilities = \\n{pred_prob}\\n\")\n",
    "\n",
    "# 3 classes 2 items\n",
    "# fmt: off\n",
    "true_prob = np.array([\n",
    "    [1.0, 0.0, 0.0],  # one item\n",
    "  # [0.0, 1.0, 0.0],  # second item\n",
    "])\n",
    "# fmt: on\n",
    "\n",
    "print(f\"True Probabilities = \\n{true_prob}\\n\")\n",
    "\n",
    "print(f\"Cross-entropy loss = {cross_entropy_loss(pred_prob, true_prob):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Математический смысл\n",
    "\n",
    "**Кросс-энтропия** имеет глубокий математический смысл в теории информации и статистике. Она связана с [энтропией 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F) $H(P)$ и  [расстоянием Кульбака — Лейблера 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0) $D_{KL}(P||Q)$:\n",
    "\n",
    "\n",
    "$$H(P||Q) = D_{KL}(P||Q) + H(P)$$\n",
    "\n",
    "\n",
    "Подробнее об этом можно почитать тут:\n",
    "\n",
    "[[colab] 🥨 Cross-entropy](https://colab.research.google.com/drive/1DFhT24njhb4LA2g6iJtl2diH-YPtD2Z4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи **бинарной классификации** или **multi-label классификации** (одному объекту может соответствовать несколько классов) можно использовать модификацию cross-entropy, которую называют **binary cross-entropy**:\n",
    "\n",
    "$$\\text{Loss} =  - \\sum^C_{i=1}(p_\\text{true_i}\\cdot \\log(p_\\text{pred_i}) + (1-p_\\text{true_i})\\cdot \\log(1-p_\\text{pred_i}))$$\n",
    "\n",
    "В этом случае порог вероятности 0.5 может быть не оптимальным для определения класса. Подробнее про выбор оптимального порога рекомендуем почитать:\n",
    "* [[blog] ✏️ A Gentle Introduction to Threshold-Moving for Imbalanced Classification](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиент функции потерь. Кросс-энтропия\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая последовательность преобразований будет выглядеть так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/linear_model_probability_pipeline.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ✏️ Cross-Entropy Loss](https://wandb.ai/wandb_fc/russian/reports/---VmlldzoxNDI4NjAw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем производную от функции ошибок. Функция ошибок:\n",
    "\n",
    "$$ L = - \\sum_i p_\\text{true_i} \\log p_\\text{pred_i} = -\\sum_i p_\\text{true_i} \\log(\\dfrac {e^{s_i}} {\\sum_j e^{s_j}}),$$\n",
    "\n",
    "где $s_i$ — это логиты классов, получаемые из линейной модели:\n",
    "\n",
    "$$s_i = w_i x$$\n",
    "\n",
    "Для расчета градиента будем использовать chain rule:\n",
    "\n",
    "$$ \\dfrac {\\partial L} {\\partial w_i} = \\dfrac {\\partial L} {\\partial s_i} \\dfrac {\\partial s_i} {\\partial w_i} $$\n",
    "\n",
    "Градиент логитов по весам:\n",
    "\n",
    "$$\\dfrac {\\partial s_i} {\\partial w_i} = x$$\n",
    "\n",
    "У нас только одна истинная метка класса $p_\\text{true_k} = 1$, для $i\\neq k$ $p_\\text{true_k}=0$\n",
    "\n",
    "$$ L = -1⋅ \\log p_\\text{pred_k} = - \\log(\\dfrac {e^{s_k}} {\\sum_j e^{s_{j}}})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас два случая:\n",
    "1. **Производная по истинному логиту** (значение логита входит в числитель и знаменатель дроби).\n",
    "2. **Производная по остальным логитам** (значение логита входит только в знаменатель дроби).\n",
    "\n",
    "Начнем с истинного логита:\n",
    "1. Производная по $s_{k}$. Вынесем минус, чтобы не потерять:\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{k}}} {\\sum_j e^{s_{j}}}) = \\log e^{s_{k}} - \\log  \\sum_j e^{s_{j}}  = s_{k} - \\log  \\sum_j e^{s_{j}}$$\n",
    "\n",
    "$$\\dfrac {\\partial -L} {\\partial s_{k}} = 1 - \\dfrac 1 {\\sum_j e^{s_{j}}} \\cdot \\dfrac {\\partial {\\sum_j e^{s_{j}}}} {\\partial s_{k}} = 1 - \\dfrac 1 {\\sum_j e^{s_{j}}} \\cdot \\dfrac {\\partial e^{s_{k}}} {\\partial s_{k}} = 1 - \\dfrac {e^{s_{k}}} {\\sum_j e^{s_{j}}} = 1 - p_k$$\n",
    "\n",
    "Вспомним про минус:\n",
    "\n",
    "$$\\dfrac {\\partial L} {\\partial s_{j}} = p_k - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Для остальных логитов $i \\neq k$. Вынесем минус, чтобы не потерять:\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{i}}} {\\sum_j e^{s_{j}}}) = \\log e^{s_{k}} - \\log  \\sum_j e^{s_{j}}  = s_{k} - \\log  \\sum_j e^{s_{j}}$$\n",
    "\n",
    "$$\\dfrac {\\partial -L} {\\partial s_{i}} = - \\dfrac 1 {\\sum_j e^{s_{j}}} \\cdot \\dfrac {\\partial {\\sum_j e^{s_{j}}}} {\\partial s_{i}} =  \\dfrac 1 {\\sum_j e^{s_{i}}} \\cdot \\dfrac {\\partial e^{s_{i}}} {\\partial s_{i}} = \\dfrac {e^{s_{i}}} {\\sum_j e^{s_{j}}} = - p_i$$\n",
    "\n",
    "Вспомним про минус:\n",
    "\n",
    "$$\\dfrac {\\partial L} {\\partial s_{j}} = p_i $$\n",
    "\n",
    "Получаем:\n",
    "$$ \\dfrac {\\partial L} {\\partial s_{i}}  =  \\begin{cases} p_i - 1 & \\text{для i=k, где k - номер истинного класса}, \\\\p_i & \\text{для любого } i\\neq k.\n",
    " \\end{cases} $$\n",
    "\n",
    "Применим **chain rule**:\n",
    "\n",
    "$$ \\dfrac {\\partial L} {\\partial w_i}  = \\dfrac {\\partial L} {\\partial s_{i}} \\dfrac {\\partial s_{i}} {\\partial w_i}  =  \\begin{cases}(p_i - 1)x & \\text{для i=k, где k - номер истинного класса}, \\\\p_ix & \\text{для любого } i\\neq k.\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде это будет выглядеть вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input batch of 2 vector with 4 elements\n",
    "# fmt: off\n",
    "x = np.array([[1,  2, 3, 4],\n",
    "              [1, -2, 0, 0]])\n",
    "# fmt: on\n",
    "\n",
    "# Weights\n",
    "W = np.random.randn(3, 4)  # 3 class\n",
    "\n",
    "# model output\n",
    "logits = x.dot(W.T)\n",
    "print(\"Scores(Logits) \\n\", logits, \"\\n\")\n",
    "\n",
    "# Probabilities\n",
    "probs = softmax(logits)  # defined before\n",
    "print(\"Probs \\n\", probs, \"\\n\")\n",
    "\n",
    "# Ground true classes\n",
    "y = [0, 1]\n",
    "\n",
    "# Derivative\n",
    "dl_ds = probs.copy()\n",
    "dl_ds[np.arange(len(y)), y] += -1  # substract one from true class prob\n",
    "dW = x.T.dot(dl_ds)  # dot product with input\n",
    "\n",
    "print(\"Grads dL/dW \\n\", dW)  # have same shape as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод опорных векторов (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы рассмотрели вероятностный подход в задаче линейной классификации, он нам еще не раз пригодиться, потому, что в нейронных сетях задача классификации решается именно таким образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо вероятностного подхода, существует еще геометрический подход: в нем мы предсказываем не вероятность отношения к классу, а разделяющие классы гиперплоскости. Этот подход имеет свои преимущества. Давайте разбираться.\n",
    "\n",
    "Замечание: в этом разделе мы не будем углубляться в математику SVM, более подробную информацию вы можете найти в [🥨 [colab] блокноте](https://colab.research.google.com/drive/1jDmQd35fyGauHbcO6jYyMH3-1pUAhSO_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к задаче классификации мышей. Рассмотрим **одномерный пример**. У нас есть данные только по **массе мышей**. Часть из них определена как мыши с нормальной массой тела, а часть — как мыши с ожирением. Данные выглядят например так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "custom_cmap = ListedColormap([\"#B8E1EC\", \"#bea6ff\", \"#FEE7D0\"])\n",
    "\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    x = np.hstack(\n",
    "        [\n",
    "            np.random.uniform(14, 21, total_len // 2),\n",
    "            np.random.uniform(24, 33, total_len // 2),\n",
    "        ]\n",
    "    )\n",
    "    y = np.hstack([np.zeros(total_len // 2), np.ones(total_len // 2)])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data_1d(\n",
    "    x,\n",
    "    y,\n",
    "    total_len=40,\n",
    "    s=50,\n",
    "    threshold=None,\n",
    "    margin=None,\n",
    "    legend=[\"Normal\", \"Obese\"],\n",
    "    marker=\"o\",\n",
    "):\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s, marker=marker)\n",
    "    if threshold:\n",
    "        x_lim, y_lim = ax.get_xlim(), ax.get_ylim()\n",
    "        XX, YY = np.meshgrid(\n",
    "            np.linspace(x_lim[0], x_lim[1], 100),\n",
    "            np.linspace(y_lim[0], y_lim[1], 100),\n",
    "        )\n",
    "        pred = np.sign(XX - threshold)\n",
    "        plt.contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "        ax.axvline(threshold, color=\"grey\")\n",
    "    if margin:\n",
    "        for line in margin:\n",
    "            ax.axvline(line, color=\"grey\", ls=\"dashed\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, legend)\n",
    "    ax.set(xlabel=\"Mass, g\", yticks=[])\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "np.random.seed(42)\n",
    "x, y = generate_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "ax = plot_data_1d(\n",
    "    x,\n",
    "    y,\n",
    "    total_len=total_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае если классы разделимы (существует пороговое значение, для которого все точки одного множества/класса расположены с одной стороны, а другого — с другой) - задача решается легко: мы находим мышь с ожирением с минимальной массой $\\min({m_\\text{obese}})$, мышь без ожирения максимальной массой $\\max({m_\\text{normal}})$. Порог выбираем посередине:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "np.random.seed(42)\n",
    "x, y = generate_data(total_len=total_len)\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "normal_limit = x[y == 0].max()  # extreme point for 'normal'\n",
    "obese_limit = x[y == 1].min()  # extreme point for 'obese'\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit])  # separated with mean value\n",
    "\n",
    "ax = plot_data_1d(\n",
    "    x, y, total_len=total_len, threshold=threshold, margin=[normal_limit, obese_limit]\n",
    ")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы выбрали решение для в котором порог максимально удален от $\\min({m_\\text{obese}})$ и $\\max({m_\\text{normal}})$. Такой подход называется Maximum Margin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_0 = np.abs(normal_limit - threshold)\n",
    "margin_1 = np.abs(obese_limit - threshold)\n",
    "print(margin_0, margin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что на значение порога влияют только крайние точки в каждом из множеств. Назовем эти точки **опорными**, т.к. в них “упирается” зазор между классами.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, пользуясь нашим простым критерием, попробуем классифицировать каких-то новых (тестовых) мышей $\\color{orange}{✭}$ $\\color{blue}{✭}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.random.uniform(14, 30, 5)\n",
    "\n",
    "\n",
    "def classify(x, threshold=21.5):\n",
    "    y = np.zeros_like(x)\n",
    "    y[x > threshold] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = plot_data_1d(\n",
    "    x, y, total_len=total_len, threshold=threshold, margin=[normal_limit, obese_limit]\n",
    ")\n",
    "ax = plot_data_1d(\n",
    "    x_test,\n",
    "    classify(x_test, threshold=threshold),\n",
    "    total_len=total_len,\n",
    "    s=500,\n",
    "    marker=\"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось неплохо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы умеем решать задачу классификации в одномерном случае. В многомерном случае попробуем свести задачу к одномерной. Для этого мы будем подбирать вектор весов и смещение чтобы точки преобразовывались, как на картинке:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/svm_hard_margin.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобрать такое преобразование в многомерной задаче — значит провести **разделяющую гиперплоскость** так, чтобы:\n",
    "1. Объекты разных классов лежали **по разные** стороны от этой плоскости.\n",
    "2. **Ближайшие** к плоскости **объекты** были от нее как можно **дальше**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Гиперплоскость однозначно задается вектором нормали $\\vec w$ и смещением $b$.** Скалярное произведение на вектор весов $\\vec w$ - это проекция на вектор нормали разделяющей плоскости. Побирая $\\vec w$ и $b$ мы сводим задачу у одномерной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зазор между классами будет опираться в объекты, которые мы назовем **опорными векторами**. Именно эти объекты проектируются преобразованием в точки 1 и -1, это обеспечивает удобство при математической записи преобразования и поиске решения (мера проекции опорного вектора = 1). Для более глубокого понимания SVM рекомендуем видео:\n",
    "* [[video] 📺 MIT: Support Vector Machines](https://www.youtube.com/watch?v=_PwhiWxHK8o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математика Support Vector Machines (в которую мы не будем погружаться в рамках этой лекции) крутится вокруг этих объектов. Функцию потерь и решающее правило [можно расписать ✏️[blog]](https://www.geeksforgeeks.org/dual-support-vector-machine/) через скалярное произведение с опорными векторами. Грубо мы можем сказать, что скалярное произведение - это [\"мера подобия\" 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C#:~:text=%D0%9A%D0%BE%D1%81%D0%B8%D0%BD%D1%83%D1%81%D0%BD%D0%BE%D0%B5%20%D1%81%D1%85%D0%BE%D0%B4%D1%81%D1%82%D0%B2%D0%BE%20%E2%80%94%20%D1%8D%D1%82%D0%BE%20%D0%BC%D0%B5%D1%80%D0%B0%20%D1%81%D1%85%D0%BE%D0%B4%D1%81%D1%82%D0%B2%D0%B0,%D0%B8%D0%B7%D0%BC%D0%B5%D1%80%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%BA%D0%BE%D1%81%D0%B8%D0%BD%D1%83%D1%81%D0%B0%20%D1%83%D0%B3%D0%BB%D0%B0%20%D0%BC%D0%B5%D0%B6%D0%B4%D1%83%20%D0%BD%D0%B8%D0%BC%D0%B8.) между векторами и \"сравнивая\" классифицируемый объект с опорными векторами мы можем определить его класс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу классификации мышей, в которой мы измерили не только вес мышей, но и их длину от хвоста до носа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "def generate_2d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42)\n",
    "    x[:, 0] += 10\n",
    "    x[:, 1] += 20\n",
    "    return x, y\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_2d_data(total_len=total_len)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, [\"Normal\", \"Obese\"])\n",
    "ax.set(xlabel=\"Mass, g\", ylabel=\"Length, cm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим к мышкам метод `svm` из библиотеки Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Code for illustration, later we will understand how it works\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel=\"linear\", C=1000)\n",
    "clf.fit(x, y)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# first fig\n",
    "sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=50, ax=axs[0])\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].legend(handles, [\"Normal\", \"Obese\"])\n",
    "axs[0].set(xlabel=\"Mass, g\", ylabel=\"Length, cm\")\n",
    "\n",
    "# plot the decision function\n",
    "delta = 0.5\n",
    "# create grid to evaluate model\n",
    "YY, XX = np.meshgrid(\n",
    "    np.linspace(x[:, 1].min() - delta, x[:, 1].max() + delta, 30),\n",
    "    np.linspace(x[:, 0].min() - delta, x[:, 0].max() + delta, 30),\n",
    ")\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "pred = np.sign(Z)\n",
    "axs[0].contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "# plot decision boget_xlimundary and margins\n",
    "axs[0].contour(\n",
    "    XX,\n",
    "    YY,\n",
    "    Z,\n",
    "    colors=\"k\",\n",
    "    levels=[-1, 0, 1],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"],\n",
    ")\n",
    "# plot support vectors\n",
    "axs[0].scatter(\n",
    "    clf.support_vectors_[:, 0],\n",
    "    clf.support_vectors_[:, 1],\n",
    "    s=100,\n",
    "    linewidth=1,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"k\",\n",
    ")\n",
    "\n",
    "# second fig\n",
    "dec_val = clf.decision_function(x)\n",
    "sns.scatterplot(x=dec_val, y=np.zeros(len(x)), hue=y, ax=axs[1])\n",
    "\n",
    "x_lim, y_lim = axs[1].get_xlim(), axs[1].get_ylim()\n",
    "XX, YY = np.meshgrid(\n",
    "    np.linspace(x_lim[0], x_lim[1], 100), np.linspace(y_lim[0], y_lim[1], 100)\n",
    ")\n",
    "pred = np.sign(XX)\n",
    "axs[1].contourf(XX, YY, pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "axs[1].axvline(0, color=\"grey\")\n",
    "axs[1].axvline(-1, color=\"grey\", ls=\"dashed\")\n",
    "axs[1].axvline(1, color=\"grey\", ls=\"dashed\")\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(handles, [\"Normal\", \"Obese\"])\n",
    "axs[1].set(xlabel=\"wx+b\", yticks=[])\n",
    "\n",
    "sv = clf.decision_function(clf.support_vectors_)\n",
    "axs[1].scatter(\n",
    "    sv, np.zeros_like(sv), s=100, linewidth=1, facecolors=\"none\", edgecolors=\"k\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Круги - опопрные вектора. Пунктиром показан зазор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Практические советы по использованию SVM:**\n",
    "\n",
    "* SVM делает **геометрическое разделение данных**, поэтому для адекватной работы модели важна **нормализация**.\n",
    "* В случае **дисбаланса классов** полезно использовать параметры `class_weight` и `sample_weight` ([подробнее 🛠️[doc]](https://scikit-learn.org/stable/modules/svm.html#unbalanced-problems)).\n",
    "* SVM работает и для классификации неразделимых данных (вводится ограничение, на сколько классы могут смешиваться, обычно за это отвечает параметр `C`).\n",
    "* SVM может давать хорошее решение при небольшом количестве данных, в этом случае стоит попробовать **различные ядра** (про ядра вы узнаете ниже)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обобщенные линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полиномиальная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные не всегда могут быть **хорошо разделены (гипер)плоскостью**. Например: у нас есть данные по дозировке лекарства и 2 класса — пациенты, которые поправились, и те, которым лучше не стало."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def generate_patients_data(total_len=40):\n",
    "    x = np.random.uniform(0, 50, total_len)\n",
    "    y = np.zeros_like(x)\n",
    "    y[(x > 15) & (x < 35)] = 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Sick\", \"Recover\"])\n",
    "    ax.set(xlabel=\"dose, mg\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, мы не можем найти такое пороговое значение, которое будет разделять наши классы на больных и здоровых, а, следовательно, и Support Vector Classifier работать тоже не будет.  Для начала давайте преобразуем наши данные таким образом, чтобы они стали 2-хмерными. В качестве значений по оси Y будем использовать дозу, возведенную в квадрат (**доза**$^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    ax = sns.scatterplot(x=x[0, :], y=x[1, :], hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [\"Sick\", \"Recover\"])\n",
    "    ax.set(xlabel=\"Dose, mg\")\n",
    "    ax.set(ylabel=\"Dose$^2$\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 40\n",
    "x_1, y = generate_patients_data(total_len=total_len)\n",
    "x_2 = x_1**2\n",
    "x = np.vstack([x_1, x_2])\n",
    "\n",
    "plot_data(x, y, total_len=40, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем вновь использовать Support Vector Classifier для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x, y, total_len=40, s=50)\n",
    "\n",
    "x_arr = np.linspace(0, 50, 50)\n",
    "xs = [x[0, :][y == 1].min(), x[0, :][y == 1].max()]\n",
    "ys = [x[1, :][y == 1].min(), x[1, :][y == 1].max()]\n",
    "\n",
    "# Calculate the coefficients.\n",
    "coefficients = np.polyfit(xs, ys, 1)\n",
    "\n",
    "# Let's compute the values of the line...\n",
    "polynomial = np.poly1d(coefficients)\n",
    "y_axis = polynomial(x_arr)\n",
    "\n",
    "# ...and plot the points and the line\n",
    "plt.plot(x_arr, y_axis, \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея **Kernel SVM** состоит в том, что **мы можем перейти в пространство большей размерности, в котором данные будут линейно разделимы**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Обоснование Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/svm_kernel_function.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже говорили, что **решающее правило SVM** можно записать через **скалярное произведение с опорными векторами**. В  Kernel SVM скалярное произведение заменяется на другую функцию подобия, которая называется **ядром/kernel**.\n",
    "\n",
    "[Подробнее 📚[book]](https://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture6.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры ядер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для демонстрации возможностей Kernel SVM создадим датасет, который не разделяется линейными моделями. Для этого воспользуемся функцией `sklearn.datasets.make_circles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "x, y = make_circles(n_samples=500, factor=0.3, noise=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный датасет представляет собой две окружности с разными радиусами и общим центром, относящиеся к разным классам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=x[:, 0],\n",
    "    y=x[:, 1],\n",
    "    hue=y,\n",
    "    s=50,\n",
    "    ax=ax,\n",
    "    palette=sns.color_palette([\"#2DA9E1\", \"#F9B041\"]),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию визуализации разделяющего правила для SVM модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "\n",
    "def plot_svm(x, y, clf):\n",
    "    dull_cmap = ListedColormap([\"#B8E1EC\", \"#FEE7D0\"])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        x,\n",
    "        response_method=\"predict\",\n",
    "        cmap=dull_cmap,\n",
    "        alpha=0.8,\n",
    "        xlabel=\"feature 1\",\n",
    "        ylabel=\"feature 2\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=x[:, 0],\n",
    "        y=x[:, 1],\n",
    "        hue=y,\n",
    "        s=50,\n",
    "        ax=ax,\n",
    "        palette=sns.color_palette([\"#2DA9E1\", \"#F9B041\"]),\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое ядро, которое мы рассмотрим, линейное. Оно задается формулой:\n",
    "$$\\large K(\\vec x_i, \\vec x_j) = (\\vec x_i, \\vec x_j)$$\n",
    "\n",
    "Линейным ядром является скалярное произведение векторов.\n",
    "\n",
    "Линейное ядро не способно справиться с такой задачей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее ядро, реализованное в библиотеке Sklearn — полиномиальное, оно задается формулой:\n",
    "$$K(\\vec x_i, \\vec x_j) = (\\gamma (\\vec x_i, \\vec x_j)+r)^d,$$\n",
    "где $d$ — настраиваемый параметр: степень полинома `degree`.\n",
    "\n",
    "Попробуем применить полиномиальное ядро к нашим данным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"poly\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полиномиальное ядро действует не совсем как полиномиальная модель.\n",
    "У модели не получилось разделить данные. Это связано с тем, что значение `degree` по умолчанию равно 3, поставим степень полинома 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"poly\", degree=2)\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод, который тут стоит сделать: для получения **оптимального результата** бывает полезным **настроить параметры ядра** с учетом даных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым  популярным ядром SVM является ядро радиальных базисных функций RBF, или гауссово ядро. Оно получено из гауссова  распределения, а гауссово распределение характерно для большого количества измеряемых величин. Данное ядро задается формулой:\n",
    "\n",
    "$$\\large K(\\vec x_i, \\vec x_j) = e^{-\\gamma{||\\vec x_i - \\vec x_j||^2}}$$\n",
    "\n",
    "Настраиваемыми параметрами модели являются `C` и `gamma`. `C` определяет степень гладкости поверхности принятия решений: чем больше `C`, тем сложнее поверхность и **выше вероятность переобучения** (про переобучение поговорим ниже), `gamma` определяет степень влияния одного обучающего примера на разделяющее правило ([подробнее 🛠️[doc]](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel)).\n",
    "\n",
    "SVM может проверять пространства признаков бесконечного размера, если для такого пространства существует kernel function. RBF ядро как раз [соответствует 📚[book]](https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf) такому случаю бесконечномерного пространства признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"rbf\")\n",
    "clf.fit(x, y)\n",
    "plot_svm(x, y, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в Sklearn реализовано `sigma` ядро. Оно интересно больше с исторической точки зрения, т.к. эквивалентно модели нейрона — Перцептрону, о котором вы узнаете на 5-й лекции. [На практике 🎓[article]](https://home.work.caltech.edu/~htlin/publication/doc/tanh.pdf) оно в большинстве случаев проигрывает RBF ядру.\n",
    "\n",
    "$$\\large K(\\vec x_i, \\vec x_j) = \\tanh (\\gamma(\\vec x_i, \\vec x_j)+r)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Наивный Байесовский классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно построить модель классификатора, которая будет напрямую оценивать вероятность принадлежности объекта к интересующему нас классу не на основе скалярных произведений и логитов, а на основе априорной информации о распределении объектов по классам в обучающей выборке. Это легко продемонстрировать на следующих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример на табличных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим `DataFrame` датасета [Wine 🛠️[doc]](https://archive.ics.uci.edu/ml/datasets/wine), который являлся примером табличных данных в первой лекции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine\n",
    "\n",
    "# Download dataset\n",
    "features, class_labels = load_wine(\n",
    "    return_X_y=True, as_frame=True\n",
    ")  # also we can get data in Bunch (dictionary) or pandas DataFrame\n",
    "\n",
    "wine_dataset = features\n",
    "wine_dataset[\"target\"] = class_labels\n",
    "\n",
    "wine_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет содержит объекты 3 различных классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём первый признак `alcohol`. По имеющийся таблице с данными легко построить функцию распределения $f(x)$, которая будет задавать вероятность $p(\\text{alcohol} = x)$, и найти среди наших данных бутылку вина с параметром `alcohol`, равным $x$ (слева на графике ниже).\n",
    "\n",
    "Т.к. у нас три класса, мы можем построить распределение объектов в обучающей выборке по признаку `alcohol` отдельно для каждого из этих трёх классов. Эти распределения зададут нам условную вероятность $p(\\text{alcohol} = x |\\text{target} = i)$ того, что объект имеет значение признака `alcohol`, равное $x$, при условии, что он относится к одному из классов с номером $i$ (справа на графике ниже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.kdeplot(wine_dataset, x=\"alcohol\", fill=True, ax=axes[0])\n",
    "axes[0].set_title(\"p(alcohol=x)\")\n",
    "\n",
    "sns.kdeplot(\n",
    "    wine_dataset,\n",
    "    x=\"alcohol\",\n",
    "    hue=\"target\",\n",
    "    palette=sns.color_palette([\"#5D5DA6\", \"#2DA9E1\", \"#F9B041\"]),\n",
    "    fill=True,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"p(alcohol=x|target=i)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрев на плотности распределений по классам (график справа), мы можем предположить, что бутылка с значением $\\text{alcohol} = 11.3$ будет относиться к 1 классу.\n",
    "\n",
    "На языке формул наш “метод пристального вглядывания” можно записать с помощью формулы для условной вероятности по [теореме Байеса 📚[wiki]](https://en.wikipedia.org/wiki/Bayes'_theorem):\n",
    "\n",
    "$$\\large p(\\text{target} = i | \\text{alcohol} = x) = \\frac{p(\\text{alcohol} = x | \\text{target} = i )p(\\text{target} = i )}{p(\\text{alcohol} = x)},$$\n",
    "\n",
    "где $p(\\text{target} = i)$ — доля объектов класса $i$ в датасете, а $p(\\text{target} = i | \\text{alcohol} = x)$ — вероятность того, что объект принадлежит классу $i$, при условии того, что признак `alcohol` у него принимает значение $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы поняли, откуда в названии метода **Байес**. Теперь разберемся, **почему он “наивный”**.\n",
    "\n",
    "Мы использовали только один признак: `alcohol`. Всего же у нас 13 признаков.\n",
    "\n",
    "$$\\large p(\\text{target} = i |\\text{features} = \\vec x ) = \\frac{p(\\text{features} = \\vec x | \\text{target} = i )p(\\text{target} = i )}{p(\\text{features} = \\vec x)}$$\n",
    "\n",
    "**“Наивность”** Байеса состоит в том, что эта модель будет рассматривать признаки как **независимые случайные величины**:\n",
    "\n",
    "$$\\large p(\\text{features} = \\vec x)=p(\\text{feature}_1 = x_1) \\cdot p(\\text{feature}_2 = x_2)...p(\\text{feature}_n = x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы решаем задачу классификации на $k$ классов, то для объекта с набором признаком $\\vec x$ по формуле выше мы получим $k$ чисел, характеризующих вероятность принадлежности данного объекта к различным классам. Для финального принятия решения нам останется выбрать тот класс, для которого вероятность принадлежности наивысшая:\n",
    "\n",
    "$$\\large \\text{prediction} = \\underset{i}{\\text{argmax}}{\\left(p(\\text{target} = i |\\text{features} = \\vec x )\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к нашему датасету Wine и попробуем решить задачу классификации для него при помощи предложенного алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обычно, разделим наш датасет на тренировочную и валидационную выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features.values, class_labels.values, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём реализацию Наивного Байесовского классификатора `GaussianNB` из [библиотеки Sklearn 🛠️[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB). `GaussianNB` **использует оценку распределения признаков с помощью** [Гауссового распределения 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%9D%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5). Обучим её на тренировочном датасете и измерим качество на отложенной валидационной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the model\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Calculate F1_score\n",
    "pred = model.predict(x_test)\n",
    "f1_score(y_test, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря простоте модельного датасета Wine наша наивная статистическая модель показала отличное качество работы. Это связано с “простотой” датасета — признаки классов имеют унимодальные распределения (один пик на плотности распределения), для более сложных данных (многомодальные распределения) такого не будет.\n",
    "\n",
    "Тем не менее, подход к решению задачи классификации, связанный с построением модели предсказания принадлежности объекта к имеющимся классам, оказался конструктивным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея применения наивного Байесовского классификатора в NLP задаче:\n",
    "* [[colab] 🥨 Naive Bayes NLP](https://colab.research.google.com/drive/1JUMs9TBWxlCTsqw2T-y3oxMe2WZxLYtJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практические особенности работы с линейными моделями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже обсудили, зачем нужна нормализация данных для линейной модели. В данном разделе мы обсудим виды нормализации более подробно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30-ти признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer()  # load data\n",
    "\n",
    "x = cancer.data  # features\n",
    "y = cancer.target  # labels(classes)\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")\n",
    "print(f\"x[0]: \\n {x[0]}\")\n",
    "print(f\"y[0]: \\n {y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cancer_df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк, в каждой из которой по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные диапазоны  значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.boxenplot(\n",
    "    data=cancer_df,\n",
    "    orient=\"h\",\n",
    "    palette=\"Set2\",\n",
    "    linewidth=0.4,\n",
    "    flier_kws={\"marker\": \"o\", \"s\": 3},\n",
    "    line_kws={\"linewidth\": 1},\n",
    ")\n",
    "ax.set(xscale=\"log\", xlim=(1e-4, 1e4), xlabel=\"Values\", ylabel=\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная модель представляет собой **сумму взвешенных признаков**. Если мы приведем признаки к **единому масштабу**, мы сможем **оценить их вклад в модель** по значениям весов. Кроме того, работать с признаками в одном диапазоне вычислительно удобно. Для этого будем использовать нормализацию.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализацией** называется процедура приведения входных данных к **общим значениям математических статистик**.\n",
    "\n",
    "Нормализация строит **взаимно однозначное соответствие** между некоторыми размерными величинами (которые измеряются в метрах, килограммах, годах и т. п.) и их безразмерными аналогами. Исходные значения **можно восстановить**, зная статистики оригинальных данных и правило, по которому делалась нормализация.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто используют следующие варианты нормализации:  **`MinMaxScaler`**, **`StandardScaler`**, **`RobustScaler`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные из имеющегося диапазона значений в диапазон от $0$ до $1$. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (например, масштабирование RGB пикселей)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large z_i=\\frac{X_i-X_{\\min}}{X_{\\max}-X_{\\min}},$$\n",
    "\n",
    "где $z_i$ — масштабированное значение, $X_i$ — текущее значение, $X_{\\min}$ и $X_{\\max}$ — минимальное и максимальное значения имеющихся данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение $0$ и стандартное отклонение $1$. Большинство значений будет находиться в диапазоне от $-1$ до $1$. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large z_i=\\frac{X_i-u}{s},$$\n",
    "\n",
    "где $u$ — среднее значение (или $0$ при `with_mean=False`), $s$ — стандартное отклонение (или $0$ при `with_std=False`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И `StandardScaler`, и `MinMaxScaler` чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. $k$-й процентиль — это величина, равная или не превосходящая $k$ процентов чисел во всём имеющемся распределении. Например, 50-й процентиль (медиана) распределения таков, что 50% чисел из распределения не меньше данного числа.\n",
    "\n",
    "Соответственно, `RobustScaler` не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large z_i=\\frac{X_i-X_\\text{median}}{IQR},$$\n",
    "\n",
    "где $X_\\text{median}$ — значение медианы, $IQR$ — межквартильный диапазон, равный разнице между 75-ым и 25-ым процентилями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для случайного набора признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "random_names = random.sample(list(cancer.feature_names), 8)\n",
    "cut_df = cancer_df[random_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "\n",
    "def plot_norm(df, ax, title):\n",
    "    sns.boxenplot(\n",
    "        df,\n",
    "        orient=\"h\",\n",
    "        palette=\"Set2\",\n",
    "        ax=ax,\n",
    "        linewidth=0.2,\n",
    "        flier_kws={\"marker\": \"o\", \"s\": 5},\n",
    "        line_kws={\"linewidth\": 1},\n",
    "    )\n",
    "    ax.set(xlabel=\"Values\", title=title)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 7))\n",
    "\n",
    "plot_norm(cut_df, axs[0][0], \"Original\")\n",
    "axs[0][0].set(xscale=\"log\", xlim=(1e-4, 1e4))\n",
    "min_max_x = MinMaxScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(min_max_x, columns=random_names), axs[0][1], \"MinMax\")\n",
    "\n",
    "std_x = StandardScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(std_x, columns=random_names), axs[1][0], \"Standard\")\n",
    "\n",
    "rob_x = RobustScaler().fit_transform(cut_df)\n",
    "plot_norm(pd.DataFrame(rob_x, columns=random_names), axs[1][1], \"Robust\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.55, hspace=0.35)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед выбором нормализации **важно разобраться с природой выбросов**. Для этого нужно посмотреть на выбросы с точки зрения эксперта и попробовать определить, являются ли выбросы ошибкой при сборе данных или редкими случаями, которые необходимо сохранить.\n",
    "\n",
    "Мы не являемся экспертами в медицине и мало знаем о данных, поэтому будем считать, что наши признаки имеют распределение, близкое к нормальному. Поэтому мы будем использовать `StandardScaler`. `StandardScaler` часто используют как нормировку по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = StandardScaler().fit_transform(cancer_df)  # scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что они стали намного более сравнимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_norm, columns=cancer.feature_names).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "ax = sns.boxenplot(\n",
    "    data=pd.DataFrame(x_norm, columns=cancer.feature_names),\n",
    "    orient=\"h\",\n",
    "    palette=\"Set2\",\n",
    "    linewidth=0.4,\n",
    "    flier_kws={\"marker\": \"o\", \"s\": 3},\n",
    "    line_kws={\"linewidth\": 1},\n",
    ")\n",
    "ax.set(xlabel=\"Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Борьба с переобучением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Сложность модели__ (*model complexity*) — важный гиперпараметр. В частности, для линейных моделей сложность может быть представлена **количеством параметров**, например, для полиномиальных моделей — степенью полинома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложность модели связана с **ошибкой обобщения** (_generalization error_):\n",
    "- **Cлишком простой модели** не будет хватать **количества параметров для обобщения сложной закономерности в данных**, что приведёт к большой ошибке обобщения.\n",
    "- **Избыточная сложность модели** также приводит к большой ошибке обобщения за счет того, что в силу своей сложности модель начинает **пытаться искать закономерности в шуме**, добиваясь большей точности на тренировочных данных, теряя при этом часть обобщающей способности.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/model_complexity.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры модели задают некоторую **аппроксимацию целевой функции**. Аппроксимировать целевую функцию можно несколькими способами, например:\n",
    "1. Использовать все имеющиеся данные и провести ее строго **через все точки**, которые нам известны ($f1$ на картинке);\n",
    "2. Использовать более простую функцию (в данном случае, линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым **общим закономерностям**, которые у них есть ($f2$ на картинке).\n",
    "\n",
    "Характерной чертой переобучения является первый сценарий, и сопровождается он, как правило, **большими весами**. Поэтому регуляризацию используют для борьбы с переобучением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L02/out/l2_regularization.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проиллюстрируем описанное явление на примере полиномиальной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, 2 * np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "x_true = np.linspace(0, 2 * np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\", label=\"noisy data\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем аппроксимировать имеющуюся зависимость с помощью полиномиальной модели, используя шумные данные в качестве тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_train = x.reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i, degree in enumerate([0, 1, 3, 9]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "    fig.add_subplot(2, 2, i + 1)\n",
    "    plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "    plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "    plt.plot(x_true, y_true, c=\"lime\")\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель **переобучается, подстраиваясь под тренировочную выборку**. В полиноме степень/количество весов — это гиперпараметр, который можно подбирать на кросс-валидации, но **ограничивая количество параметров**, мы накладываем **ограничение на обобщающую способность модели** в целом. Вместо этого можно оставить модель сложной, но использовать **регуляризатор**, который будет заставлять модель отдавать предпочтение более простому обобщению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(9), LinearRegression())\n",
    "model_ridge = make_pipeline(PolynomialFeatures(9), Ridge(alpha=0.1))\n",
    "\n",
    "model.fit(x_train, y)\n",
    "y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "model_ridge.fit(x_train, y)\n",
    "y_plot_ridge = model_ridge.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x_true, y_plot, c=\"red\", label=f\"M={degree}\")\n",
    "plt.plot(x_true, y_plot_ridge, c=\"black\", label=f\"M={degree}, alpha=0.1\")\n",
    "plt.scatter(x, y, s=50, facecolors=\"none\", edgecolors=\"b\")\n",
    "plt.plot(x_true, y_true, c=\"lime\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "poly_coef = model[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"Without regularization: \", eq)\n",
    "\n",
    "poly_coef = model_ridge[1].coef_\n",
    "\n",
    "eq = f\"y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x\"\n",
    "for i in range(2, 10):\n",
    "    eq += f\"+{round(poly_coef[i], 2)}*x^{i}\"\n",
    "\n",
    "print(\"With regularization: \", eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что одним из \"симптомов\" переобучения являются аномально большие веса. Модель Ridge Regression, показанная в примере выше, использует L2-регуляризацию для борьбы с этим явлением.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Литература</font>\n",
    "\n",
    "<font size=\"5\">Линейная регрессия:</font>\n",
    "\n",
    "* [[book] 📚 Метрики классификации и регрессии](https://academy.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii)\n",
    "\n",
    "<font size=\"5\">Метод градиентного спуска:</font>\n",
    "* [[book] 📚 Градиентный спуск](https://www.tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter1_gradient-descent.html)\n",
    "* [[book] 📚 Стохастический градиентный спуск](https://tomasbeuzen.com/deep-learning-with-pytorch/chapters/chapter2_stochastic-gradient-descent.html)\n",
    "\n",
    "<font size=\"5\">SVM:</font>\n",
    "* [[video] 📺 Хорошее объяснение SVM](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
    "* [[book] 📚 SVM на стэнфордском курсе ](https://cs231n.github.io/linear-classify/#svm)\n",
    "\n",
    "<font size=\"5\">Обобщенные линейные модели:</font>\n",
    "* [[blog] ✏️ Дуальная форма SVM](https://www.geeksforgeeks.org/dual-support-vector-machine/)\n",
    "* [[git] 🐾 Продвинутый алгоритм, основанный на SVM](https://github.com/IvanoLauriola/MKLpy)\n",
    "\n",
    "<font size=\"5\">Вероятностный подход в задаче классификации:</font>\n",
    "* [[video] 📺 Naive Bayes in NLP](https://www.youtube.com/watch?v=O2L2Uv9pdDA)\n",
    "* Latent Dirichlet Allocation:\n",
    " * [[video] 📺 Latent Dirichlet Allocation (Part 1 of 2)](https://www.youtube.com/watch?v=T05t-SqKArY),\n",
    " * [[video] 📺 Training Latent Dirichlet Allocation: Gibbs Sampling (Part 2 of 2)](https://www.youtube.com/watch?v=BaM1uiCpj_E).\n",
    "* [[video] 📺 Объяснение SoftMax от StatQuest](https://www.youtube.com/watch?v=KpKog-L9veg)\n",
    "* [[video] 📺 Объяснение KL Divergence](https://www.youtube.com/watch?v=SxGYPqCgJWM)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
