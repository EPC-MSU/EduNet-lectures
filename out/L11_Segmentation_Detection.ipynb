{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–¥–∞—á–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ö–æ–¥–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–Ω—è—Ç–∏–π –º—ã –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏ –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\n",
    "\n",
    "–ù–æ –ø–æ—Ä–æ–π –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∑–Ω–∞—Ç—å, —á—Ç–æ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –µ—Å—Ç—å –æ–±—ä–µ–∫—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞. –í–∞–∂–Ω–æ, –≥–¥–µ –∏–º–µ–Ω–Ω–æ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω –æ–±—ä–µ–∫—Ç. –í —Ä—è–¥–µ —Å–ª—É—á–∞–µ–≤ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –µ—â–µ –∏ —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –æ–±—ä–µ–∫—Ç–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ —Ä–µ—á—å –∏–¥–µ—Ç –æ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–æ–º —Å–Ω–∏–º–∫–µ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∫–ª–µ—Ç–æ–∫ —Ç–∫–∞–Ω–∏, –ø–æ–ª—É—á–µ–Ω–Ω–æ–º —Å –º–∏–∫—Ä–æ—Å–∫–æ–ø–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/classification_semantic_segmentation.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–≥–æ, –∫–∞–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –æ–±—ä–µ–∫—Ç–∞–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ ‚Äî —ç—Ç–æ –∑–∞–¥–∞—á–∞ **—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏** (segmentation).\n",
    "\n",
    "–ï—Å–ª–∏ –Ω–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –Ω–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã, –∞ —Ç–æ–ª—å–∫–æ —Ç–∏–ø (–∫–ª–∞—Å—Å) –æ–±—ä–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–º –∑–∞–Ω—è—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∏–∫—Å–µ–ª—å (–∫–∞–∫ –≤ —Å–ª—É—á–∞–µ —Å –∫–ª–µ—Ç–∫–∞–º–∏ –ø–æ–¥ –º–∏–∫—Ä–æ—Å–∫–æ–ø–æ–º), —Ç–æ –≥–æ–≤–æ—Ä—è—Ç –æ **—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏** (semantic segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –Ω–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã, –∏ –ø—Ä–∏ —ç—Ç–æ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∑–Ω–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–±–ª–∞—Å—Ç—å, –≤ –∫–æ—Ç–æ—Ä–æ–π –æ–±—ä–µ–∫—Ç –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω, —Ç–æ —ç—Ç–æ –∑–∞–¥–∞—á–∞ **–¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è** (Detection)\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ —Ç–∞–∫–æ–π –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∏—Ç–æ–≤ –Ω–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤–æ–º —Å–Ω–∏–º–∫–µ.\n",
    "\n",
    "–ï—Å–ª–∏ –∂–µ –≤–∞–∂–Ω—ã –∏ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã, –∏ –∏—Ö —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã, —Ç–æ —ç—Ç–æ —É–∂–µ –∑–∞–¥–∞—á–∞ **Instance segmentation**. –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è  –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–∞ –≤–∞–∂–Ω–æ –Ω–µ —Ç–æ–ª—å–∫–æ –∑–Ω–∞—Ç—å, —á—Ç–æ –ø–µ—Ä–µ–¥ –Ω–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π, –Ω–æ –∏ –æ—Ç–ª–∏—á–∏—Ç—å, –≥–¥–µ –∏–º–µ–Ω–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –±–ª–∏–∂–Ω–∏–π, –∞ –≥–¥–µ ‚Äî –¥–∞–ª—å–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset COCO ‚Äî Common Objects in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–µ–∂–¥–µ —á–µ–º –≥–æ–≤–æ—Ä–∏—Ç—å –æ —Å–ø–æ—Å–æ–±–∞—Ö —Ä–µ—à–µ–Ω–∏—è —ç—Ç–∏—Ö –∑–∞–¥–∞—á, –Ω–∞–¥–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–¥–µ–ª–∞–µ–º —ç—Ç–æ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ [COCO üõ†Ô∏è[doc]](https://cocodataset.org/).\n",
    "\n",
    "COCO ‚Äî –æ–¥–∏–Ω –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–∞—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ —Ç—Ä—ë—Ö—Å–æ—Ç —Ç—ã—Å—è—á –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞–∑–º–µ—á–µ–Ω–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:\n",
    "- –∫–∞—Ç–µ–≥–æ—Ä–∏–∏;\n",
    "- –º–∞—Å–∫–∏;\n",
    "- –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏–µ –±–æ–∫—Å—ã (*bounding boxes*);\n",
    "- –æ–ø–∏—Å–∞–Ω–∏—è (*captions*);\n",
    "- –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ (*keypoints*);\n",
    "- –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ.\n",
    "\n",
    "–§–æ—Ä–º–∞—Ç —Ä–∞–∑–º–µ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –≤ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, –Ω–µ—Ä–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏ –≤ –¥—Ä—É–≥–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –ö–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –æ–Ω —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ –∫–∞–∫ \"COCO format\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –¥–∞—Ç–∞—Å–µ—Ç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -qN \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "!wget -qN \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/annotations_trainval2017.zip\"\n",
    "!unzip -qn annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–∞–∫–µ—Ç `pycocotools`.\n",
    "\n",
    "[[blog] ‚úèÔ∏è –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–π COCO –¥–∞—Ç–∞—Å–µ—Ç —Å –Ω—É–ª—è](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ñ–æ—Ä–º–∞—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"cat\"])  #  Find category ID by tag\n",
    "print(\"class ID(cat) =\", catIds)\n",
    "\n",
    "imgIds = coco.getImgIds(catIds=catIds)  # Filtering dataset by category ID\n",
    "print(\"All images: %i\" % len(imgIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = coco.loadImgs(imgIds[0])  # 1 example\n",
    "img_metadata = img_list[0]\n",
    "img_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "I = coco2pil(img_metadata[\"coco_url\"])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = coco.loadCats(coco.getCatIds())  # loading categories\n",
    "num2cat = {}\n",
    "print(\"COCO categories: \")\n",
    "\n",
    "iterator = iter(cats)\n",
    "cat = next(iterator)\n",
    "for i in range(0, 91):\n",
    "    if i == cat[\"id\"]:\n",
    "        num2cat[cat[\"id\"]] = cat[\"name\"]\n",
    "        name = cat[\"name\"]\n",
    "        if i < 90:\n",
    "            cat = next(iterator)\n",
    "    else:\n",
    "        name = \"---\"\n",
    "\n",
    "    print(f\"{i:2}. {name:20}\", end=\"\")\n",
    "\n",
    "    if not i % 6:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞—Ç–µ–≥–æ—Ä–∏—è **0** –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –∫–ª–∞—Å—Å–∞ —Ñ–æ–Ω–∞. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–æ–º–µ—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ –∑–∞–Ω—è—Ç—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –Ω–∞–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"categories[2]: {cats[2]}\")\n",
    "print(f\"categories[3]: {cats[3]}\")\n",
    "\n",
    "nms = set([cat[\"supercategory\"] for cat in cats])\n",
    "print(\"COCO supercategories: \\n{}\".format(\"\\t\".join(nms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–º–∏–º–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–∞–º –¥–æ—Å—Ç—É–ø–Ω–∞ [—Ä–∞–∑–º–µ—Ç–∫–∞ üõ†Ô∏è[doc]]((https://cocodataset.org/#format-data). –î–∞–≤–∞–π—Ç–µ –µ—ë –∑–∞–≥—Ä—É–∑–∏–º –∏ –æ—Ç–æ–±—Ä–∞–∑–∏–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annIds = coco.getAnnIds(imgIds=img_metadata[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "plt.imshow(I)\n",
    "plt.axis(\"off\")\n",
    "coco.showAnns(anns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å —Ä–∞–∑–º–µ—Ç–∫—É –ø–∏–∫—Å–µ–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º. –¢–æ –µ—Å—Ç—å, –ø–∏–∫—Å–µ–ª–∏ –∏–∑ –æ–±—ä–µ–∫—Ç–æ–≤, –æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è –∫ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–º –∫–ª–∞—Å—Å–∞–º, –ø—Ä–∏–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –∫ –∫–ª–∞—Å—Å—É —ç—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞. –ö –ø—Ä–∏–º–µ—Ä—É, –º–æ–∂–Ω–æ —É–≤–∏–¥–µ—Ç—å –æ–±—ä–µ–∫—Ç—ã –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤: \"cat\" –∏ \"keyboard\".\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ —Ç–µ–ø–µ—Ä—å –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∏–∑ —á–µ–≥–æ —Å–æ—Å—Ç–æ–∏—Ç —Ä–∞–∑–º–µ—Ç–∫–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_anns(anns):\n",
    "    for i, a in enumerate(anns):\n",
    "        print(f\"\\n#{i}\")\n",
    "        for k in a.keys():\n",
    "            if k == \"category_id\" and num2cat.get(a[k], None):\n",
    "                print(k, \": \", a[k], num2cat[a[k]])  # Show cat. name\n",
    "            else:\n",
    "                print(k, \": \", a[k])\n",
    "\n",
    "\n",
    "dump_anns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–º–µ—Ç–∏–º, —á—Ç–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–æ–∂–µ—Ç —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –∫–∞–∂–¥–æ–µ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:\n",
    "* `segmentation` ‚Äî –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—Ä —á–∏—Å–µ–ª ($x$, $y$) ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫–∞–∂–¥–æ–π –∏–∑ –≤–µ—Ä—à–∏–Ω \"–æ–±–æ–ª–æ—á–∫–∏\" –æ–±—ä–µ–∫—Ç–∞;\n",
    "* `area` ‚Äî –ø–ª–æ—â–∞–¥—å –æ–±—ä–µ–∫—Ç–∞;\n",
    "* `iscrowd` ‚Äî –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä —Ç–æ–ª–ø–∞ –ª—é–¥–µ–π, –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥—Ä–∞–Ω–∏—Ü–∞—Ö –æ–±—ä–µ–∫—Ç–∞ (–º–∞—Å–∫–∞) —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ [RLE üìö[wiki]](https://en.wikipedia.org/wiki/Run-length_encoding) —Ñ–æ—Ä–º–∞—Ç–µ;\n",
    "* `image_id` ‚Äî –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –æ–ø–∏—Å—ã–≤–∞–µ–º—ã–π –æ–±—ä–µ–∫—Ç;\n",
    "* `bbox` ‚Äî *–±—É–¥–µ—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω –¥–∞–ª–µ–µ –≤ —Ö–æ–¥–µ –ª–µ–∫—Ü–∏–∏*;\n",
    "* `category_id` ‚Äî –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫ –∫–æ—Ç–æ—Ä–æ–π –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –¥–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç;\n",
    "* `id` ‚Äî –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–∞–º–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞.\n",
    "\n",
    "–ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø—Ä–∏–º–µ—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º `iscrowd = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"people\"])\n",
    "annIds = coco.getAnnIds(catIds=catIds, iscrowd=True)\n",
    "anns = coco.loadAnns(annIds[0:1])\n",
    "\n",
    "dump_anns(anns)\n",
    "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
    "I = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(I)\n",
    "coco.showAnns(anns)  # People in the stands\n",
    "seg = anns[0][\"segmentation\"]\n",
    "print(\"Counts\", len(seg[\"counts\"]))\n",
    "print(\"Size\", seg[\"size\"])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[video] üì∫ COCO Dataset Format ‚Äî Complete Walkthrough](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥—ã –∏–∑ `pycocotools`, –º–æ–∂–Ω–æ  –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –Ω–∞–±–æ—Ä –≤–µ—Ä—à–∏–Ω \"–æ–±–æ–ª–æ—á–∫–∏\" —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –≤ –±–æ–ª–µ–µ —É–¥–æ–±–Ω—ã–π, –Ω–æ –º–µ–Ω–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –≤–∏–¥ ‚Äî –º–∞—Å–∫—É –æ–±—ä–µ–∫—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "msk = np.zeros(seg[\"size\"])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        ann = anns[i]\n",
    "        msk = coco.annToMask(ann)\n",
    "        ax[row, col].imshow(msk, cmap=\"gray\")\n",
    "        ax[row, col].set_title(num2cat[anns[i][\"category_id\"]])\n",
    "        ax[row, col].axis(\"off\")\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –ø–æ–ø–∏–∫—Å–µ–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω–æ–π. –ö –ø—Ä–∏–º–µ—Ä—É, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ–ª–æ–≤–µ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏, —Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ—Å—Ç–æ –∫–∞–∫–∏–º-—Ç–æ –æ–±—Ä–∞–∑–æ–º –ø—Ä–æ–º–∞—Ä–∫–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∏—Ö, –ø–æ—Å–ª–µ —á–µ–≥–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞—à–∏—Ö \"–æ—Ç–º–µ—Ç–æ–∫\". –û–¥–Ω–∏–º –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–∞—Ä–∫–∏—Ä–æ–≤–∫–∏ —è–≤–ª—è–µ—Ç—Å—è \"–æ–±–≤–µ–¥–µ–Ω–∏–µ\" –æ–±—ä–µ–∫—Ç–∞ —Ä–∞–º–∫–æ–π (bounding box), –≤–Ω—É—Ç—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è. –¢–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—ä–µ–∫—Ç–∞—Ö —Ç–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö —Ñ–æ—Ä–º–∞—Ç–∞ COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "draw = ImageDraw.Draw(I)\n",
    "\n",
    "colors = {1: \"white\", 40: \"lime\"}  # person - white, glove - lime\n",
    "for ann in anns:\n",
    "    x, y, width, heigth = ann[\"bbox\"]  # bounding box here\n",
    "    color = colors.get(ann[\"category_id\"], None)\n",
    "    if color:\n",
    "        draw.rectangle((x, y, x + width, y + heigth), outline=color, width=2)\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è (*Semantic segmentation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ‚Äî –∑–∞–¥–∞—á–∞ –ø–æ–∏—Å–∫–∞ –≥—Ä—É–ø–ø –ø–∏–∫—Å–µ–ª–µ–π, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç –æ–¥–∏–Ω —Å–º—ã—Å–ª–æ–≤–æ–π –æ–±—ä–µ–∫—Ç.*\n",
    "\n",
    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —ç—Ç–æ –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫. –ï—Å—Ç—å –Ω–∞–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/semantic_segmentation_1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –µ—Å—Ç—å –º–∞—Å–∫–∞ $W √ó H$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/semantic_segmentation_2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∞—Å–∫–∞ –∑–∞–¥–∞–µ—Ç –∫–ª–∞—Å—Å –æ–±—ä–µ–∫—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è:\n",
    "[$(x,y) \\to$ `class_num`]\n",
    "\n",
    "–ù–∞–±–æ—Ä —Ç–∞–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–∞—Å–∫–∞–º–∏ ‚Äî —ç—Ç–æ –∏ –µ—Å—Ç—å –Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç, –Ω–∞ –Ω–µ–º –º—ã —É—á–∏–º—Å—è.\n",
    "\n",
    "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/semantic_segmentation_3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–∏ –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –º–µ—Ç–∫—É –∫–ª–∞—Å—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è (–º–∞—Å–∫—É)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ª—É—á–∏–º —Ç–∞–∫—É—é –º–∞—Å–∫—É –∏–∑ COCO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -qN \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "!wget -qN \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/annotations_trainval2017.zip\"\n",
    "!unzip -qn annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")\n",
    "clear_output()\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
    "I = coco2pil(img[\"coco_url\"])\n",
    "\n",
    "semantic_seg_person_mask = np.zeros(I.size[::-1], dtype=bool)  # WxH -> HxW\n",
    "\n",
    "for ann in anns:\n",
    "    msk = coco.annToMask(ann)  # HxW\n",
    "    if ann[\"category_id\"] == 1 and not ann[\"iscrowd\"]:  # single person:\n",
    "        # semantic_seg_person_mask = msk | semantic_seg_person_mask  # union\n",
    "        semantic_seg_person_mask += msk.astype(bool)\n",
    "\n",
    "semantic_seg_person_mask = semantic_seg_person_mask > 0  # binarize\n",
    "plt.imshow(semantic_seg_person_mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–ø–æ—Å–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ –ø–æ–¥—É–º–∞–µ–º –æ —Ç–æ–º, –∫–∞–∫ —Ç–∞–∫—É—é –∑–∞–¥–∞—á—É –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å.\n",
    "\n",
    "–ò–∑ —Å–∞–º–æ–π –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–¥–∞—á–∏ –≤–∏–¥–Ω–æ, —á—Ç–æ —ç—Ç–æ –∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –¢–æ–ª—å–∫–æ –Ω–µ\n",
    "–≤—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –∞ –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) –ù–∞–∏–≤–Ω—ã–π**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "–ü—Ä–æ—Å—Ç–µ–π—à–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º —Ä–µ—à–µ–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–æ–≥–æ \"—Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞\" ‚Äî –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ –±—É–¥—É—Ç –Ω–µ–±–æ–ª—å—à–∏–µ –∑–æ–Ω—ã, –æ–∫—Ä—É–∂–∞—é—â–∏–µ –∫–∞–∂–¥—ã–π –∏–∑ –ø–∏–∫—Å–µ–ª–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ö –∫–∞–∂–¥–æ–º—É –∏–∑ —Ç–∞–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å–≤—ë—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∞—è, –∫ –∫–∞–∫–æ–º—É –∫–ª–∞—Å—Å—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –ø–∏–∫—Å–µ–ª—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/naive_way_predict_pixel_class.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–±) –†–∞–∑—É–º–Ω—ã–π**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–Ω—è—Ç–Ω–æ, —á—Ç–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è $H*W$ –∑–∞–ø—É—Å–∫–æ–≤.\n",
    "\n",
    "–ú–æ–∂–Ω–æ –ø–æ–π—Ç–∏ –¥—Ä—É–≥–∏–º –ø—É—Ç–µ–º: –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø–æ –Ω–µ–π –¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –ø–∏–∫—Å–µ–ª–µ–π —Ä–∞–∑–æ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —ç—Ç–æ–≥–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–º–µ–Ω—è—Ç—å –ø—Ä–∏–≤—ã—á–Ω—É—é –Ω–∞–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "* —É–±—Ä–∞—Ç—å —Å–ª–æ–∏, —É–º–µ–Ω—å—à–∞—é—â–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã,\n",
    "* —É–±—Ä–∞—Ç—å –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π –≤ –∫–æ–Ω—Ü–µ, –∑–∞–º–µ–Ω–∏–≤ –µ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/reasonable_way_predict_pixel_class.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –≤—ã—Ö–æ–¥–∞ $(W, H)$ –±—É–¥—É—Ç —Ä–∞–≤–Ω—ã —à–∏—Ä–∏–Ω–µ –∏ –≤—ã—Å–æ—Ç–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "\n",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –±—É–¥–µ—Ç —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —É—á–∏–º—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ–≥–¥–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –∏–∑ –∫–∞—Ä—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–π –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è —Å–µ—Ç–∏ –∫–∞–∫ –Ω–µ–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ (score) –∫–∞–∂–¥–æ–≥–æ –∏–∑ –ø–∏–∫—Å–µ–ª–µ–π –∫ —Ç–æ–º—É –∏–ª–∏ –∏–Ω–æ–º—É –∫–ª–∞—Å—Å—É.\n",
    "\n",
    "–¢–æ –µ—Å—Ç—å –Ω–æ–º–µ—Ä –∫–∞–Ω–∞–ª–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –±—É–¥–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—É –æ–±—ä–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–æ–±—Ä–∞–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–π –ø–∏–∫—Å–µ–ª—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "last_layer_output = torch.randn((3, 32, 32))  # class_num, W,H\n",
    "print(\"Output of last layer shape\", last_layer_output.shape)  # activation slice\n",
    "mask = torch.argmax(last_layer_output, dim=0)  # class_nums prediction\n",
    "print(\"One class mask shape\", mask.shape)\n",
    "print(\"Predictions for all classes \\n\", mask[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –º–æ–∂–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å —Ç–∞–∫:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/semantic_mask2.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –Ω–∞ –≤—ã—Ö–æ–¥–µ —Å–µ—Ç–∏ –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤, —Ä–∞–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª–∞—Å—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–≤–µ—Ä—Ç–∫–∞ 1√ó1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/1x1_kernel_size_fully_connected_layer.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –ª–µ–∫—Ü–∏–∏ –ø—Ä–æ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —Å–µ—Ç–∏ –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏ –æ —Ç–æ–º, —á—Ç–æ —Å–≤–µ—Ä—Ç–∫—É 1√ó1 –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –∞–Ω–∞–ª–æ–≥ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–æ–≥–æ —Å–ª–æ—è. –ò–º–µ–Ω–Ω–æ —Ç–∞–∫ –æ–Ω–∞ —Ç—É—Ç –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç.\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
    "- —á—Ç–æ–±—ã —Ä–µ—Ü–µ–ø—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ –Ω–µ–π—Ä–æ–Ω–æ–≤ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–ª–æ—è—Ö –±—ã–ª–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Å —Ä–∞–∑–º–µ—Ä–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–Ω–æ–≥–æ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ–µ–≤ ($L$ —Ä–∞–∑ —Å–≤—ë—Ä—Ç–∫–∞ $3\\times3$ $\\to$ —Ä–µ—Ü–µ–ø—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ $(1+2L)$);\n",
    "- —Å–≤–µ—Ä—Ç–∫–∏ –º–µ–¥–ª–µ–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω—ã—Ö –∫–∞—Ä—Ç–∞—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–≤) –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å–≤–µ—Ä—Ç–æ—á–Ω—É—é —Å–µ—Ç—å, –Ω–æ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –∑–∞–º–µ–Ω–∏–º –Ω–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[arxiv] üéì Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)\n",
    "\n",
    "\n",
    "–°–æ–∫—Ä–∞—â–µ–Ω–Ω–æ FCN. –î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –ø—É—Ç–∞–Ω–∏—Ü—ã —Å Fully Connected Network, –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–º–µ–Ω—É—é—Ç MLP (Multilayer Perceptron).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞ –æ—Å–Ω–æ–≤—É –±–µ—Ä–µ—Ç—Å—è –æ–±—ã—á–Ω–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/fcn_backbone.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.v7labs.com/blog/semantic-segmentation-guide\">The Beginner‚Äôs Guide to Semantic Segmentation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫—É—é —Å–µ—Ç—å –º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å, –≤–∑—è–≤ –∑–∞ –æ—Å–Ω–æ–≤—É –¥—Ä—É–≥—É—é —Å–≤–µ—Ä—Ç–æ—á–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (*backbone*), –Ω–∞–ø—Ä–∏–º–µ—Ä, ResNet50 –∏–ª–∏ VGG16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/fcn_changes.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.v7labs.com/blog/semantic-segmentation-guide\">The Beginner‚Äôs Guide to Semantic Segmentation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò –∑–∞—Ç–µ–º –∑–∞–º–µ–Ω–∏—Ç—å –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏ –Ω–∞ —Å–≤–µ—Ä—Ç–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/fully_convolution_network_scheme.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1411.4038.pdf\"> Fully Convolutional Networks for Semantic Segmentation\n",
    "</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–æ–Ω—Ü–µ –¥–æ–±–∞–≤–∏—Ç—å `upsample` —Å–ª–æ–π –¥–æ –Ω—É–∂–Ω—ã—Ö –Ω–∞–º —Ä–∞–∑–º–µ—Ä–æ–≤.\n",
    "\n",
    "–ù–∞ –≤—Ö–æ–¥ —Ç–∞–∫–∞—è –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø–æ–ª—É—á–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.\n",
    "–î–ª—è –∑–∞–¥–∞—á —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ—Ä–µ –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≥—Ä–∞–Ω–∏—Ü–∞—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –†–∞–∑–∂–∏–º–∞—é—â–∏–π —Å–ª–æ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Å–ø–æ–º–Ω–∏–º, –∫–∞–∫ –ø–æ–≤—ã—à–∞—é—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ —É–∂–µ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–π–¥–µ–º –∫ –∫–∞—Ä—Ç–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "\n",
    "–î–æ–ø—É—Å—Ç–∏–º, —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º 2√ó2 –¥–æ —Ä–∞–∑–º–µ—Ä–∞ 4√ó4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/upsample.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/comparison_of_1d_and_2d_interpolation.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://en.wikipedia.org/wiki/Bilinear_interpolation\"> Bilinear_interpolation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è —á–µ—Ç—ã—Ä–µ—Ö —Å–æ—Å–µ–¥–Ω–∏—Ö –ø–∏–∫—Å–µ–ª–µ–π, —Ç–æ —Ç–∞–∫–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –±–∏–ª–∏–Ω–µ–π–Ω–æ–π. –í –∫–∞—á–µ—Å—Ç–≤–µ –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —ç—Ç–∏—Ö —á–µ—Ç—ã—Ä—ë—Ö –ø–∏–∫—Å–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def img_to_heatmap(img, ax, title):  # Magik method to show img as heatmap\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(title)\n",
    "    array = np.array(img)\n",
    "    array = array[None, None, :]\n",
    "    sns.heatmap(array[0][0], annot=True, ax=ax, lw=1, cbar=False)\n",
    "\n",
    "\n",
    "# Fake image\n",
    "raw = np.array([[1, 3, 0, 1], [3, 3, 3, 7], [8, 1, 8, 7], [6, 1, 1, 1]], dtype=np.uint8)\n",
    "pil = Image.fromarray(raw)\n",
    "\n",
    "interp_nn = pil.resize((8, 8), resample=Image.NEAREST)\n",
    "interp_bl = pil.resize((8, 8), resample=Image.BILINEAR)\n",
    "\n",
    "# plot result\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "img_to_heatmap(raw, ax[0], \"Raster dataset\")\n",
    "img_to_heatmap(interp_nn, ax[1], \"Nearest neighbor interpolation\")\n",
    "img_to_heatmap(interp_bl, ax[2], \"Bilinear interpolation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[–ë–∏–ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è üìö[wiki]](https://en.wikipedia.org/wiki/Bilinear_interpolation) –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç —Ä–µ–∑–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –º–µ—Ç–æ–¥–æ–º –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞.  –°—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –¥—Ä—É–≥–∏–µ –≤–∏–¥—ã –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ –±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–Ω–∏—Ö –ø–∏–∫—Å–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample –≤ PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö —á–µ–º—É –±—ã–ª —ç—Ç–æ—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä –æ–± —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫?\n",
    "\n",
    "–û–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è, –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature maps) –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ç–µ –∂–µ –º–µ—Ç–æ–¥—ã, —á—Ç–æ –∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.\n",
    "\n",
    "–î–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–∞—Ä—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–π), –≤ PyTorch –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª–∞—Å—Å `nn.Upsample` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html). –í –Ω—ë–º –¥–æ—Å—Ç—É–ø–Ω—ã –≤—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ –º–µ—Ç–æ–¥—ã –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ —Ç—Ä–∏–ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è ‚Äî –∞–Ω–∞–ª–æ–≥ –±–∏–ª–∏–Ω–µ–π–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–∫ –ø—Ä–∏–º–µ—Ä—É, –≤–∏–¥–µ–æ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] üõ†Ô∏è](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html?highlight=interp#torch.nn.functional.interpolate) `nn.functional.interpolate`\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `Upsample` –≤–Ω—É—Ç—Ä–∏ –Ω–∞—à–µ–≥–æ —Ä–∞–∑–∂–∏–º–∞—é—â–µ–≥–æ –±–ª–æ–∫–∞.\n",
    "\n",
    "–ó–∞–≥—Ä—É–∑–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/semantic_segmentation_1.png -O cat.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "def upsample(pil, ax, mode=\"nearest\"):\n",
    "    tensor = TF.to_tensor(pil)\n",
    "    # Create upsample instance\n",
    "\n",
    "    if mode == \"nearest\":\n",
    "        upsampler = nn.Upsample(scale_factor=2, mode=mode)\n",
    "    else:\n",
    "        upsampler = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
    "\n",
    "    tensor_128 = upsampler(tensor.unsqueeze(0))  # add batch dimension\n",
    "    # Convert tensor to pillow\n",
    "    img_128 = tensor_128.squeeze()\n",
    "    img_128_pil = TF.to_pil_image(img_128.clamp(min=0, max=1))\n",
    "    ax.imshow(img_128_pil)\n",
    "    ax.set_title(mode)\n",
    "\n",
    "\n",
    "# Load and show image in Pillow format\n",
    "pic = Image.open(\"cat.png\")\n",
    "pil_64 = pic.resize((64, 64))\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(15, 5))\n",
    "ax[0].imshow(pil_64)\n",
    "ax[0].set_title(\"Raw\")\n",
    "\n",
    "# Upsample with Pytorch\n",
    "upsample(pil_64, mode=\"nearest\", ax=ax[1])\n",
    "upsample(pil_64, mode=\"bilinear\", ax=ax[2])\n",
    "upsample(pil_64, mode=\"bicubic\", ax=ax[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–æ, —á—Ç–æ –≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –∫–∞–∂–¥–æ–µ –∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–º–µ—Ä–µ–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É–≤–µ–ª–∏—á–∏–ª–æ—Å—å –≤ 2 —Ä–∞–∑–∞, –Ω–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–æ–∑–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤ –∏–Ω–æ–µ, –≤ —Ç–æ–º —á–∏—Å–ª–µ –Ω–µ —Ü–µ–ª–æ–µ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑, –∏—Å–ø–æ–ª—å–∑—É—è –ø–∞—Ä–∞–º–µ—Ç—Ä `scale_factor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–ª–æ–∏ Upsample –æ–±—ã—á–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç –≤–º–µ—Å—Ç–µ —Å–æ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–º–∏, —ç—Ç–æ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± —É–≤–µ–ª–∏—á–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Upsample(scale_factor=2),\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# fmt: on\n",
    "\n",
    "dummy_input = torch.randn((0, 3, 32, 32))\n",
    "out = model(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î—Ä—É–≥–∏–µ —Å–ø–æ—Å–æ–±—ã \"—Ä–∞–∑–∂–∞—Ç—å\" –∫–∞—Ä—Ç—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxUnpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–º–∏–º–æ —Å–≤—ë—Ä—Ç–∫–∏, –Ω–∞ —ç—Ç–∞–ø–µ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–æ–∏ pooling'–∞. –ù–∞–∏–±–æ–ª–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è MaxPooling, —Å–æ—Ö—Ä–∞–Ω—è—é—â–∏–π –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞–∏–±–æ–ª—å—à–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤–Ω—É—Ç—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞. –î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –æ–±—Ä–∞—Ç–∏—Ç—å –¥–∞–Ω–Ω—É—é –æ–ø–µ—Ä–∞—Ü–∏—é —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏, –±—ã–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω MaxUnpooling —Å–ª–æ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/maxunpooling.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–Ω–Ω—ã–π —Å–ª–æ–π —Ç—Ä–µ–±—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ ‚Äî –ø—Ä–∏ –æ–±—Ä–∞—Ç–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –Ω–∞ –º–µ—Å—Ç–æ, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—ã–ª –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–µ–≥–º–µ–Ω—Ç–∞ –¥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –∫–∞–∂–¥–æ–º—É —Å–ª–æ—é MaxUnpooling –¥–æ–ª–∂–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Å–ª–æ–π MaxPooling, —á—Ç–æ –≤–∏–∑—É–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/downsample_and_upsample_layers.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∫ MaxPool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "[–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∫ MaxUnpool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(False, warn_only=False)\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "def tensor_show(tensor, title=\"\", ax=ax):\n",
    "    img = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
    "    ax.set_title(title + str(img.size))\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "pool = nn.MaxPool2d(\n",
    "    kernel_size=2, return_indices=True\n",
    ")  # False by default(get indexes to upsample)\n",
    "unpool = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "pil = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].set_title(\"original \" + str(pil.size))\n",
    "ax[0].imshow(pil)\n",
    "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
    "print(\"Orginal shape\", tensor.shape)\n",
    "\n",
    "# Downsample\n",
    "tensor_half_res, indexes1 = pool(tensor)\n",
    "tensor_show(tensor_half_res, \"1/2 down \", ax=ax[1])\n",
    "\n",
    "tensor_q_res, indexes2 = pool(tensor_half_res)\n",
    "tensor_show(tensor_q_res, \"1/4 down \", ax=ax[2])\n",
    "print(\"Downsample shape\", indexes2.shape)\n",
    "\n",
    "# Upsample\n",
    "tensor_half_res1 = unpool(tensor_q_res, indexes2)\n",
    "tensor_show(tensor_half_res1, \"1/2 up \", ax=ax[3])\n",
    "\n",
    "\n",
    "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
    "tensor_show(tensor_recovered, \"full size up \", ax=ax[4])\n",
    "print(\"Upsample shape\", tensor_recovered.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transposed convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–ø–æ—Å–æ–±—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–ª–∏, –Ω–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∏ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "\n",
    "–î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø–µ—Ä–∞—Ü–∏—é *Transposed convolution*, –≤ –∫–æ—Ç–æ—Ä–æ–π, –∫–∞–∫ –≤ –æ–±—ã—á–Ω–æ–π —Å–≤–µ—Ä—Ç–∫–µ, –µ—Å—Ç—å **–æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã**. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: *Fractionally strided convolution*.\n",
    "\n",
    "–ò–Ω–æ–≥–¥–∞ **–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ** –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è *–æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤–µ—Ä—Ç–∫–æ–π* –∏–ª–∏ *Deconvolution*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/simple_convolution.png\" width=\"700\"></center>\n",
    "<center><em>–û–±—ã—á–Ω–∞—è —Å–≤–µ—Ä—Ç–∫–∞</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø–µ—Ä–∞—Ü–∏—è –æ–±—ã—á–Ω–æ–π —Å–≤–µ—Ä—Ç–∫–∏ –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä-—è–¥—Ä–æ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–∞—Ä—Ç—ã, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ, –∞ –∑–∞—Ç–µ–º —Å–ª–æ–∂–µ–Ω–∏–µ, –ø—Ä–µ–≤—Ä–∞—â–∞—è **–æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç** –≤—Ö–æ–¥–∞ –≤ **–æ–¥–∏–Ω –ø–∏–∫—Å–µ–ª—å** –≤—ã—Ö–æ–¥–∞.\n",
    "\n",
    "Transposed convolution, –Ω–∞–æ–±–æ—Ä–æ—Ç, –ø—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –≤—Å–µ–º –ø–∏–∫—Å–µ–ª—è–º –≤—Ö–æ–¥–∞ –∏ —É–º–Ω–æ–∂–∞–µ—Ç –∏—Ö –Ω–∞ **–æ–±—É—á–∞–µ–º–æ–µ —è–¥—Ä–æ** —Å–≤–µ—Ä—Ç–∫–∏. –ü—Ä–∏ —ç—Ç–æ–º –∫–∞–∂–¥—ã–π **–æ–¥–∏–Ω–æ—á–Ω—ã–π –ø–∏–∫—Å–µ–ª—å** –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ **—Ñ—Ä–∞–≥–º–µ–Ω—Ç**. –¢–∞–º, –≥–¥–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–∞–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞, –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ø–∏–∫—Å–µ–ª—å–Ω–æ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è.\n",
    "\n",
    "–ï—Å–ª–∏ –≤—Ö–æ–¥ –∏–º–µ–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–∞–ª–æ–≤, —Ç–æ Transposed convolution –ø—Ä–∏–º–µ–Ω—è–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–π –æ–±—É—á–∞–µ–º—ã–π —Ñ–∏–ª—å—Ç—Ä –∫ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–∞–ª—É, –∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–º–º–∏—Ä—É–µ—Ç.\n",
    "\n",
    "–ü–∞—Ä–∞–º–µ—Ç—Ä `stride` –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–¥–≤–∏–≥ –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –Ω–∞ –≤—ã—Ö–æ–¥–µ. –ò—Å–ø–æ–ª—å–∑—É—è Transposed convolution —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º `stride = 2`, –º–æ–∂–Ω–æ –ø–æ–≤—ã—à–∞—Ç—å —Ä–∞–∑–º–µ—Ä –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ –≤ –¥–≤–∞ —Ä–∞–∑–∞, –¥–æ–±–∞–≤–ª—è—è –Ω–∞ –Ω–µ–µ –º–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/transposed_convolution_explained.png\" width=\"1024\"></center>\n",
    "\n",
    "<center><em>Transposed convolution</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—ã—á–Ω–æ–π —Å–≤–µ—Ä—Ç–∫–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä `padding` –≤ Transposed convolution –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–µ –∑–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–π –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∞, –Ω–∞–æ–±–æ—Ä–æ—Ç, –∑–∞ \"—Å—Ä–µ–∑–∞–Ω–∏–µ\" –≤–Ω–µ—à–Ω–µ–≥–æ –∫—Ä–∞—è –∫–∞—Ä—Ç—ã-–≤—ã—Ö–æ–¥–∞. –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–∞—Ä—Ç–∞ —Å—Ç—Ä–æ–∏—Ç—Å—è —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ —Å–æ—Å–µ–¥–Ω–∏—Ö –ø–∏–∫—Å–µ–ª–µ–π, –Ω–æ –ø–æ –ø–µ—Ä–∏–º–µ—Ç—Ä—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∏ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –±–æ–ª–µ–µ –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ `kernel_size` –≤—ã–±–∏—Ä–∞—é—Ç –∫—Ä–∞—Ç–Ω—ã–º `stride`, —á—Ç–æ–±—ã –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –ø—Ä–∏ —á–∞—Å—Ç–∏—á–Ω–æ–º –Ω–∞–ª–æ–∂–µ–Ω–∏–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n",
    "```\n",
    "kernel_size = 4\n",
    "stride = 2\n",
    "```\n",
    "–ü—Ä–∏ —Ç–∞–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö –∏–º–µ–µ—Ç —Å–º—ã—Å–ª —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å `padding=2`, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–≤–∞ –ø–∏–∫—Å–µ–ª—è —Å–æ –≤—Å–µ—Ö —Å—Ç–æ—Ä–æ–Ω –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –±–µ–∑ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ‚úèÔ∏è –ü—Ä–æ 2D —Å–≤–µ—Ä—Ç–∫–∏ —Å –ø–æ–º–æ—â—å—é –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü](https://www.baeldung.com/cs/convolution-matrix-multiplication)\n",
    "\n",
    "[[arxiv] üéì A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285v1.pdf) ‚Äî –æ—Ç–∫—É–¥–∞ —Å–ª–æ–≤–æ Transposed –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (—Ä–∞–∑–¥–µ–ª 4.1)\n",
    "\n",
    "[[doc] üõ†Ô∏è](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d) `torch.nn.ConvTranspose2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n",
    "                         stride=1, padding=0, ...)\n",
    "```\n",
    "–≥–¥–µ:\n",
    "* `in_channels`, `out_channels` ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ –≤ –≤—Ö–æ–¥–Ω–æ–π –∏ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,\n",
    "* `kernel_size` ‚Äî —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ —Å–≤–µ—Ä—Ç–∫–∏ Transpose convolution,\n",
    "* `stride` ‚Äî —à–∞–≥ —Å–≤–µ—Ä—Ç–∫–∏ Transpose convolution,\n",
    "* `padding`‚Äî —Ä–∞–∑–º–µ—Ä –æ—Ç—Å—Ç—É–ø–æ–≤, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—ã—Ö –ø–æ –∫—Ä–∞—è–º –≤—Ö–æ–¥–Ω–æ–π –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 16, 16, 16)  # define dummy input\n",
    "print(\"Original size\", input.shape)\n",
    "\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)  # define downsample layer\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)  # define upsample layer\n",
    "\n",
    "# let`s downsample and upsample input\n",
    "with torch.no_grad():\n",
    "    output_1 = downsample(input)\n",
    "    print(\"Downsampled size\", output_1.size())\n",
    "\n",
    "    output_2 = upsample(output_1, output_size=input.size())\n",
    "    print(\"Upsampled size\", output_2.size())\n",
    "\n",
    "# plot results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "sns.heatmap(input[0, 0, :, :], ax=ax[0], cbar=False, vmin=-2, vmax=2)\n",
    "ax[0].set_title(\"Input\")\n",
    "sns.heatmap(output_1[0, 0, :, :], ax=ax[1], cbar=False, vmin=-2, vmax=2)\n",
    "ax[1].set_title(\"Downsampled\")\n",
    "sns.heatmap(output_2[0, 0, :, :], ax=ax[2], cbar=False, vmin=-2, vmax=2)\n",
    "ax[2].set_title(\"Upsampled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–∏—Ä–∞–º–∏–¥–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ–∑–Ω–∏–∫–∞–µ—Ç –≤–æ–ø—Ä–æ—Å: –Ω–µ –ø–æ—Ç–µ—Ä—è–µ—Ç—Å—è –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–µ–ª–∫–∏—Ö –¥–µ—Ç–∞–ª—è—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–ª–æ–∫ —Å–µ—Ç–∏, –≥–¥–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ? –¢–∞–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\n",
    "\n",
    "–¢–µ, –∫—Ç–æ –∏–∑—É—á–∞–ª –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –ø–æ–º–Ω—è—Ç, —á—Ç–æ  –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤ –æ—Å–æ–±—ã—Ö —Ç–æ—á–µ–∫ ([SIFT üìö[wiki]](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform)) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–∞—è –ø–∏—Ä–∞–º–∏–¥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/pyramid_of_features.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–¥–µ—è —Å–æ—Å—Ç–æ–∏—Ç –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º —É–º–µ–Ω—å—à–µ–Ω–∏–∏ (–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏) –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è—Ö.\n",
    "\n",
    "–ü—Ä–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –º—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –ø–æ–ª—É—á–∞–µ–º –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ä–∞–∑–Ω—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/semantic_information.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ö –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∫–∞–∫ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–∞ –¥–ª—è –Ω–æ–≤—ã—Ö —Å–≤–µ—Ä—Ç–æ–∫, —Ç–∞–∫ –∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.\n",
    "\n",
    "–ù–∞ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã FPN-—Å–µ—Ç–∏:\n",
    "\n",
    "[[arxiv] üéì Feature Pyramid Networks for Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1612.03144)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/fcn_1.png\" width=\"1000\">\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf\"> Fully Convolutional Networks for Semantic Segmentation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤—Å–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±—É–¥—É—Ç —É–≤–µ–ª–∏—á–µ–Ω—ã –¥–æ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –æ–Ω–∏ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "* [[doc] üõ†Ô∏è Fully-Convolutional Network model with ResNet-50 and ResNet-101 backbones](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)\n",
    "* [[doc] üõ†Ô∏è Models and Pre-trained Weights](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
    "\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∞ –Ω–∞ —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ COCO train2017 (–Ω–∞ 20 –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–∞–∫ –∂–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ  Pascal VOC). –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª–∞—Å—Å—ã:\n",
    "\n",
    "`['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "# load resnet50\n",
    "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
    "    weights=\"FCN_ResNet50_Weights.DEFAULT\", num_classes=21\n",
    ")\n",
    "\n",
    "classes = [\n",
    "    \"__background__\",\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # ImageNet\n",
    "    ]\n",
    ")\n",
    "\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "input_tensor = transform(pil_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = fcn_model(input_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è 2 –º–∞—Å—Å–∏–≤–∞\n",
    "\n",
    "* out ‚Äî –∫–∞–∂–¥—ã–π –ø–∏–∫—Å–µ–ª—å –æ—Ç—Ä–∞–∂–∞–µ—Ç –Ω–µ–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—é –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"output keys: \", output.keys())  # Ordered dictionary\n",
    "print(\"out: \", output[\"out\"].shape, \"Batch, class_num, h, w\")\n",
    "\n",
    "output_predictions = output[\"out\"][0].argmax(0)  # for first element of batch\n",
    "print(f\"output_predictions: {output_predictions.shape}\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "indexes = output_predictions\n",
    "semantic_seg_person_predict = np.zeros(pil_img.size).astype(bool)\n",
    "\n",
    "# plot all classes predictions\n",
    "fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
    "i = 0  # counter\n",
    "for row in range(4):\n",
    "    for col in range(5):\n",
    "        mask = torch.zeros(indexes.shape)\n",
    "        mask[indexes == i] = 255\n",
    "\n",
    "        ax[row, col].set_title(classes[i])\n",
    "        ax[row, col].imshow(mask)\n",
    "        ax[row, col].axis(\"off\")\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_seg_person_predict = torch.zeros(indexes.shape)\n",
    "semantic_seg_person_predict[indexes == 15] = 1  # to obtain binary mask\n",
    "semantic_seg_person_predict = (\n",
    "    semantic_seg_person_predict.numpy()\n",
    ")  # for skliarn compability\n",
    "\n",
    "plt.imshow(semantic_seg_person_predict, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##–ú–µ—Ç—Ä–∏–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –æ—Ç –º–æ–¥–µ–ª–∏?\n",
    "\n",
    "–ë–∞–∑–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–æ–π —è–≤–ª—è–µ—Ç—Å—è Intersection over Union (IoU), –æ–Ω–∞ –∂–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–¥–∞([Jaccard index üìö[wiki]](https://en.wikipedia.org/wiki/Jaccard_index))\n",
    "\n",
    "–ò–º–µ–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (—Ñ–∏–æ–ª–µ—Ç–æ–≤–∞—è –º–∞—Å–∫–∞) –∏ —Ü–µ–ª–µ–≤–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞,  —Å–¥–µ–ª–∞–Ω–Ω–∞—è —á–µ–ª–æ–≤–µ–∫–æ–º (–∫—Ä–∞—Å–Ω–∞—è –º–∞—Å–∫–∞)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/iou_sample.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://datahacker.rs/deep-learning-intersection-over-union/\">Intersection over Union</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.\n",
    "\n",
    "**–î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –≤ –ø—Ä–∏–º–µ—Ä–µ –º–∞—Å–∫–∏ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–µ, –Ω–æ —Ç–∞ –∂–µ –ª–æ–≥–∏–∫–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å  –¥–ª—è –º–∞—Å–æ–∫ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã.*\n",
    "\n",
    "–ú–µ—Ç—Ä–∏–∫–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∫ –ø–ª–æ—â–∞–¥–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–≤—É—Ö –º–∞—Å–æ–∫:\n",
    "\n",
    "$$ \\large IoU = \\frac{|T \\cap P|}{|T \\cup P|} $$\n",
    "\n",
    "T ‚Äî True mask, P ‚Äî predicted mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/iou_formula.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –º–∞—Å–∫–∏ —Å–æ–≤–ø–∞–¥—É—Ç –Ω–∞ $100\\%$, —Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –±—É–¥–µ—Ç —Ä–∞–≤–Ω–æ $1$, –∏ —ç—Ç–æ –Ω–∞–∏–ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ü—Ä–∏ –ø—É—Å—Ç–æ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–∏ $\\text{IoU} $ –±—É–¥–µ—Ç –Ω—É–ª–µ–≤—ã–º. –ó–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ –ª–µ–∂–∞—Ç –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ $[0..1]$.\n",
    "\n",
    "–í —Ç–µ—Ä–º–∏–Ω–∞—Ö –æ—à–∏–±–æ–∫ –ø–µ—Ä–≤–æ–≥–æ/–≤—Ç–æ—Ä–æ–≥–æ —Ä–æ–¥–∞ $\\text{IoU}$  –º–æ–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å –∫–∞–∫:\n",
    "\n",
    "$$ \\large \\text{IoU} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP} + \\text{FN}} $$\n",
    "\n",
    "\n",
    "$\\text{TP}$ ‚Äî True positive ‚Äî –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (–æ–±–æ–∑–Ω–∞—á–µ–Ω–æ –∂–µ–ª—Ç—ã–º),\n",
    "\n",
    "$\\text{FP}$ ‚Äî False Positive (–æ—Å—Ç–∞—Ç–æ–∫ —Ñ–∏–æ–ª–µ—Ç–æ–≤–æ–≥–æ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞),\n",
    "\n",
    "$\\text{FN}$ ‚Äî False Negative (–æ—Å—Ç–∞—Ç–æ–∫ –∫—Ä–∞—Å–Ω–æ–≥–æ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ –±–∞–∑–µ —ç—Ç–æ–π –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç—Ä–æ–∏—Ç—Å—è —Ä—è–¥ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö –æ—Ç –Ω–µ–µ –º–µ—Ç—Ä–∏–∫, —Ç–∞–∫–∏—Ö –∫–∞–∫ Mean Average Precision, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤ —Ä–∞–∑–¥–µ–ª–µ –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.\n",
    "\n",
    "[[blog] ‚úèÔ∏è Intersection over Union](http://datahacker.rs/deep-learning-intersection-over-union/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"GT mask\")\n",
    "plt.imshow(semantic_seg_person_mask)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Predicted mask\")\n",
    "plt.imshow(semantic_seg_person_predict)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"GT & Predict overlap\")\n",
    "plt.axis(\"off\")\n",
    "tmp = semantic_seg_person_predict * 2 + semantic_seg_person_mask\n",
    "plt.imshow(tmp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∞–ª–∏–∑–∞—Ü–∏–∏:\n",
    "\n",
    "* [[doc] üõ†Ô∏è Jaccard score –≤ Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)\n",
    "* [[doc] üõ†Ô∏è SMP –≤ PyTorch](https://smp.readthedocs.io/en/latest/metrics.html#segmentation_models_pytorch.metrics.functional.iou_score)\n",
    "* [[doc] üõ†Ô∏è Jaccard score –≤ Torchmetrics](https://torchmetrics.readthedocs.io/en/stable/classification/jaccard_index.html)\n",
    "* [[doc] üõ†Ô∏è Jaccard index between two sets of boxes –≤ Torchvision](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# wait vectors, so we flatten the data\n",
    "y_true = semantic_seg_person_mask.flatten()\n",
    "y_pred = semantic_seg_person_predict.flatten()\n",
    "iou = jaccard_score(y_true, y_pred)\n",
    "\n",
    "print(f\"IoU = {iou:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "# or use:  smp.metrics.get_stats\n",
    "# https://smp.readthedocs.io/en/latest/metrics.html#segmentation_models_pytorch.metrics.functional.get_stats\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "iou = tp / (tp + fp + fn)\n",
    "print(f\"IoU = {iou:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î—Ä—É–≥–æ–π –ø–æ–ø—É–ª—è—Ä–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è $\\text{Dice}$ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç:\n",
    "\n",
    "$$ \\large \\text{Dice} = \\dfrac{2|A \\cap B|}{|A| + |B|} $$\n",
    "\n",
    "–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ –æ–Ω –ø–æ—Ö–æ–∂ –Ω–∞ $\\text{IoU}$, –Ω–æ –ø—Ä–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏–∏ —á–µ—Ä–µ–∑ –æ—à–∏–±–∫–∏ –ø–µ—Ä–≤–æ–≥–æ –∏ –≤—Ç–æ—Ä–æ–≥–æ —Ä–æ–¥–∞ –±—É–¥–µ—Ç –≤–∏–¥–Ω–æ, —á—Ç–æ –æ–Ω —Å–æ–≤–ø–∞–¥–µ—Ç —Å [F1-–º–µ—Ä–æ–π üõ†Ô∏è[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):\n",
    "\n",
    "$$ \\large \\text{Dice} =  \\frac{2|A \\cap B|}{|A| + |B|} = \\dfrac{2\\text{TP}}{2\\text{TP} + \\text{FP} + \\text{FN}} = \\text{F1_score} ‚àà [0,1]$$\n",
    "\n",
    "[[blog] ‚úèÔ∏è F1 Score = Dice Coefficient](https://chenriang.me/f1-equal-dice-coefficient.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "dice_smp = f1_score(y_true, y_pred)\n",
    "dice = 2 * tp / (2 * tp + fp + fn)\n",
    "print(f\"Dice = {dice:.2f}\")\n",
    "print(f\"F1_score = {dice_smp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –≤–∏–¥–Ω–æ, $\\text{IoU}$ —Å–∏–ª—å–Ω–µ–µ –Ω–∞–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞ –æ—à–∏–±–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ‚úèÔ∏è All the segmentation metrics!](https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/loss_overview.png\" width=\"900\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/JunMa11/SegLoss\"> Loss functions for image segmentation </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution-based loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫ –∫–∞–∫ –∑–∞–¥–∞—á–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–≤–æ–¥–∏—Ç—Å—è –∫ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Ç–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Cross-Entropy Loss, BCE, –∏–ª–∏ Focal Loss, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã –∑–Ω–∞–∫–æ–º—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Cross-Entropy (BCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –º–∞—Å–∫–∞ –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞(`target.shape` = 1√óH√óW), —Ç–æ –∑–∞–¥–∞—á–∞ —Å–≤–æ–¥–∏—Ç—Å—è –∫ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Ç–∞–∫ –∫–∞–∫ –∫–∞–∂–¥—ã–π –∫–∞–Ω–∞–ª –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è –≤—ã–¥–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞.\n",
    "\n",
    "–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å Softmax –≤ Cross-Entropy Loss –Ω–∞ —Å–∏–≥–º–æ–∏–¥—É, –∞ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å ‚Äî –Ω–∞ –±–∏–Ω–∞—Ä–Ω—É—é –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—é (BCE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mask_class_1 = torch.randint(0, 2, (1, 64, 64))  # [0 , 1]\n",
    "one_class_out = torch.randn(1, 1, 64, 64)\n",
    "print(mask_class_1.shape)\n",
    "print(one_class_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ–Ω—è–µ–º [BCE with Logits Loss üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "bce_loss_wl = nn.BCEWithLogitsLoss()  # Sigmoid inside\n",
    "loss = bce_loss_wl(\n",
    "    one_class_out, mask_class_1.float().unsqueeze(0)\n",
    ")  # both params must have equal size\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –º–æ–¥–µ–ª–∏ ‚Äî —ç—Ç–æ –°–∏–≥–º–æ–∏–¥–∞, —Ç–æ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [BCE Loss üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_one_class_out = one_class_out.sigmoid()\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "loss = bce_loss(\n",
    "    norm_one_class_out, mask_class_1.float().unsqueeze(0)\n",
    ")  # both params must have equal size\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cross-Entropy Loss üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ –±—É–¥–µ—Ç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "loss = cross_entropy(one_class_out, mask_class_1.float().unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫ –∫–∞–∫ Softmax –æ—Ç –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—Ö–æ–¥–∞ –≤—Å–µ–≥–¥–∞ —Ä–∞–≤–µ–Ω $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_class_out.softmax(dim=1).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilabel**\n",
    "\n",
    "–ï—Å–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–æ–≤ –∏ target –∏–º–µ–µ—Ç —Ñ–æ—Ä–º—É: N√óW√óH (multilabel), —Ç–æ –º–∞—Å–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –∫–∞–Ω–∞–ª–µ:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/semantic_mask2.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.jeremyjordan.me/semantic-segmentation/\"> An overview of semantic image segmentation </a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_class1 = torch.randint(0, 2, (1, 64, 64))  # [0 , 1]\n",
    "mask_class2 = torch.randint(0, 2, (1, 64, 64))\n",
    "\n",
    "target = torch.cat((mask_class1, mask_class2))\n",
    "\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–¥–∏–º, —á—Ç–æ —Ñ–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ñ–æ—Ä–º–æ–π —Ç–µ–Ω–∑–æ—Ä–∞ –º–∞—Å–æ–∫:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_class_out = torch.randn(1, 2, 64, 64)\n",
    "print(two_class_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –º–æ–∂–µ–º –ø–æ—Å—á–∏—Ç–∞—Ç—å [BCE üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–≤ target –≤–æ float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()  # Sigmoid inside\n",
    "float_target = target.float()  # add batch and convert ot float\n",
    "loss = bce_loss(\n",
    "    two_class_out, float_target.unsqueeze(0)\n",
    ")  # both params must have equal size\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–ª–∏ [Cross-Entropy Loss üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "# If containing class probabilities, same shape as the input and each value should be between [0,1][0,1].\n",
    "loss = cross_entropy(two_class_out, float_target.unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ —Å–æ–≤–ø–∞–¥—É—Ç, —Ç–∞–∫ –∫–∞–∫ –ø–æ—Å–ª–µ Sigmoid –∏ Softmax –ø–æ–ª—É—á–∞—Ç—Å—è —Ä–∞–∑–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –º–∞—Å–∫–∞ –∑–∞–¥–∞–Ω–∞ –æ–¥–Ω–∏–º –∫–∞–Ω–∞–ª–æ–º, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫–ª–∞—Å—Å—ã –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω—ã —Ü–µ–ª—ã–º–∏ —á–∏—Å–ª–∞–º–∏ (multiclass):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/semantic_mask1.png\" width=\"900\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.jeremyjordan.me/semantic-segmentation/\"> An overview of semantic image segmentation </a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_target = target.argmax(0)\n",
    "sq_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ –ª–æ–≥–∏—á–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `nn.CrossEntropyLoss`:\n",
    "\n",
    "```\n",
    "Input: Shape (C), (N,C) or (N,C,d1‚Äã,d2‚Äã,...,dK‚Äã) with K‚â•1 in the case of K-dimensional loss.\n",
    "\n",
    "Target: If containing class indices, shape (), (N) or (N,d1,d2,...,dK)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = cross_entropy(two_class_out, sq_target.unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-based loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö Region-based loss –æ—Ç–Ω–æ—Å—è—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Ü–µ–Ω–∫–∏ –ø–ª–æ—â–∞–¥–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–∞—Å–æ–∫.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç accuracy, —Ä–∞—Å—Å—á–µ—Ç IoU (Jaccard index):\n",
    "\n",
    "$\\large \\text{IoU} = \\text{JaccardIndex} = \\dfrac{  TP  }{TP + FP + FN} \\in [0,1]$\n",
    "\n",
    "–º–æ–∂–Ω–æ –ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ [–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–º –æ–±—Ä–∞–∑–æ–º üéì[arxiv]](https://arxiv.org/abs/1908.03851).\n",
    "\n",
    "–î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º —Ñ—É–Ω–∫—Ü–∏—é Sigmoid, –∞ –æ–ø–µ—Ä–∞—Ü–∏—é –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∑–∞–º–µ–Ω–∏—Ç—å –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω—ã–º —É–º–Ω–æ–∂–µ–Ω–∏–µ–º:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large \\text{IoU} = \\frac{|A \\cap B|}{|A \\cup B|} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ |A \\cap B| =\n",
    "\\begin{bmatrix}\n",
    "0.01 & 0.03 & 0.02 & 0.02 \\\\ 0.05 & 0.12 & 0.09 & 0.07 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix} * \\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix}\n",
    "\\xrightarrow{\\ element-wise \\ multiply \\ } \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix} \\xrightarrow{\\ sum \\ } 7.41$\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\qquad \\text{prediction} \\qquad \\qquad \\qquad \\quad \\text{target}$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ |A| =\n",
    "\\begin{bmatrix}\n",
    "0.01 & 0.03 & 0.02 & 0.02 \\\\ 0.05 & 0.12 & 0.09 & 0.07 \\\\ 0.89 & 0.85 & 0.88 & 0.91 \\\\ 0.99 & 0.97 & 0.95 & 0.97 \\end{bmatrix}\n",
    "\\xrightarrow{\\ sum \\ }  7.82 \\qquad \\qquad|B| =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{bmatrix} \\xrightarrow{\\ sum \\ }  8 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$|A \\cup B| = |A +B - A  \\cap B|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò —Ç–æ–≥–¥–∞ –º–µ—Ç—Ä–∏–∫—É –º–æ–∂–Ω–æ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–≤ –µ–µ:\n",
    "\n",
    "$\\large \\text{Jaccard Loss} = 1 - \\text{IoU}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í PyTorch —Ç–∞–∫–æ–π loss –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –ø–æ—ç—Ç–æ–º—É –¥–ª—è –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É [SMP üêæ[git]](https://github.com/qubvel/segmentation_models.pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q segmentation-models-pytorch\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "iou_loss = smp.losses.JaccardLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "print(\"IoU Loss\", iou_loss(two_class_out, float_target.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*–ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –Ω–∞ –≤—ã—Ö–æ–¥–µ –º–æ–¥–µ–ª–∏ —Å–∏–≥–º–æ–∏–¥—ã –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—â–µ–π –ª–æ–≥–∏—Ç—ã –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä `from_logits` —Å–ª–µ–¥—É–µ—Ç —É—Å—Ça–Ω–æ–≤–∏—Ç—å —Ä–∞–≤–Ω—ã–º `False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dice loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å Dice –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç:\n",
    "\n",
    "$$ \\large \\text{Dice} = \\dfrac{2|A \\cap B|}{|A| + |B|} $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large \\text{Dice} = \\frac{2 \\sum\\limits_{pixels}y_{true}y_{pred}}{\\sum\\limits_{pixels}y_{true} + \\sum\\limits_{pixels}y_{pred}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò –∑–∞—Ç–µ–º –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å:\n",
    "\n",
    "$ \\large \\text{DiceLoss} = 1 - \\text{Dice} = 1 - \\dfrac{2\\sum\\limits_{pixels}y_{true}y_{pred}}{\\sum\\limits_{pixels}y_{true} + \\sum\\limits_{pixels}y_{pred}} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Ç–∞–∫–∂–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ PyTorch, –ø–æ—ç—Ç–æ–º—É –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π [SMP üêæ[git]](https://github.com/qubvel/segmentation_models.pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = smp.losses.DiceLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "\n",
    "print(two_class_out.shape, target.shape)\n",
    "print(\"DICE Loss\", dice_loss(two_class_out, float_target.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ DiceLoss –∏ JaccardLoss –ø–æ—Ö–æ–∂–∏. –ù–æ JaccardLoss [—Å–∏–ª—å–Ω–µ–µ —à—Ç—Ä–∞—Ñ—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –≤—ã–±—Ä–æ—Å–∞—Ö ‚úèÔ∏è[blog]](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–•–æ—Ä–æ—à–∏–π –æ–±–∑–æ—Ä loss —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ [Loss functions for image segmentation üêæ[git]](https://github.com/JunMa11/SegLoss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net: Convolutional Networks for Biomedical Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–µ—Ç—å —Ç–∞–∫, —á—Ç–æ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –≤—ã—Ö–æ–¥–µ —Å–∂–∏–º–∞—é—â–∏—Ö –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∏–º —Ä–∞–∑–∂–∏–º–∞—é—â–∏—Ö –±–ª–æ–∫–æ–≤ –±—É–¥—É—Ç –∏–º–µ—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/efficient_way_predict_pixel_class.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–∑–Ω–∞–∫–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∏ —Å–∂–∞—Ç–∏–∏, —Å–∫–æ–ø–∏—Ä—É–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω–∏–º —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –≤ —Ä–∞–∑–∂–∏–º–∞—é—â–∏—Ö —Å–ª–æ—è—Ö, –≥–¥–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–º–µ—é—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/add_skip_connection.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫ –∂–µ, –∫–∞–∫ –∏ ResNet, —ç—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –Ω–æ—Å–∏—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ skip connection, –Ω–æ  –ø—Ä–∏–∑–Ω–∞–∫–∏  –Ω–µ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è, –∞ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω–∞—è –Ω–∞–º–∏ —Å—Ö–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ U-Net. –≠—Ç–∞ –ø–æ–ø—É–ª—è—Ä–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –±—ã–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –≤ [—Å—Ç–∞—Ç—å–µ üéì[arxiv]](https://arxiv.org/abs/1505.04597) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞  –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/unet_scheme.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ U-Net</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/1505.04597\">U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[git] üêæ –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ PyTorch](https://github.com/milesial/Pytorch-UNet)\n",
    "\n",
    "[[doc] üõ†Ô∏è U-Net –Ω–∞ PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ç–æ–∏—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–µ—Ä—ã–µ —Å—Ç—Ä–µ–ª–∫–∏ –Ω–∞ —Å—Ö–µ–º–µ: –æ–Ω–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –∫–æ–ø–∏–π —Ä–∞–Ω–µ–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å DenseNet. –ß—Ç–æ–±—ã —ç—Ç–æ –±—ã–ª–æ –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∫–∞—Ä—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö —Å–Ω–∏–∂–µ–Ω–∏—è –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π. –î–ª—è —ç—Ç–æ–π —Ü–µ–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–ø–µ—Ä–∞—Ü–∏—è—Ö `MaxPool` –∏ `MaxUnpool` ‚Äî –≤ –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö –≤ –¥–≤–∞ —Ä–∞–∑–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–æ–¥–µ –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–æ—Ç —Ç–∞–∫:\n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "    out1 = self.block1(x) #  ------------------------------>\n",
    "    out_pool1 = self.pool1(out1)\n",
    "\n",
    "    out2 = self.block2(out_pool1)\n",
    "    out_pool2 = self.pool2(out2)\n",
    "\n",
    "    out3 = self.block3(out_pool2)\n",
    "    out_pool3 = self.pool2(out3)\n",
    "\n",
    "    out4 = self.block4(out_pool3)\n",
    "    # return up\n",
    "    out_up1 = self.up1(out4)\n",
    "\n",
    "    out_cat1 = torch.cat((out_up1, out3), dim=1)\n",
    "    out5 = self.block5(out_cat1)\n",
    "    out_up2 = self.up2(out5)\n",
    "\n",
    "    out_cat2 = torch.cat((out_up2, out2), dim=1)\n",
    "    out6 = self.block6(out_cat2)\n",
    "    out_up3 = self.up3(out6)\n",
    "\n",
    "    out_cat3 = torch.cat((out_up3, out1), dim=1) # <-------\n",
    "    out = self.block7(out_cat3)\n",
    "\n",
    "    return out\n",
    "\n",
    "```\n",
    "–ü–æ—Å–ª–µ upsample –±–ª–æ–∫–æ–≤ ReLU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±–∑–æ—Ä DeepLabv3+ (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepLab ‚Äî —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–≤–∏–≤–∞–≤—à–µ–µ—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ —á–µ—Ç—ã—Ä—ë—Ö –ª–µ—Ç. –û—Å–Ω–æ–≤–æ–π –¥–∞–Ω–Ω–æ–≥–æ —Ä–æ–¥–∞ –º–æ–¥–µ–ª–µ–π —è–≤–ª—è–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ **atrous (dilated) convolutions** –∏, –Ω–∞—á–∏–Ω–∞—è —Å–æ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª–∏, **atrous spatial pyramid pooling**, –æ–ø–∏—Ä–∞—é—â–µ–π—Å—è –Ω–∞ **spatial pyramid pooling**.\n",
    "\n",
    "[[arxiv] üéì Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
    "\n",
    "[[doc] üõ†Ô∏è –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ PyTorch](https://pytorch.org/vision/stable/models.html#deeplabv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/deeplabv3_scheme.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atrous (Dilated) Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/dilated_convolution.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dilated convolution** (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–≤–µ—Ä—Ç–∫–∞) ‚Äî —ç—Ç–æ —Ç–∏–ø —Å–≤–µ—Ä—Ç–∫–∏, –∫–æ—Ç–æ—Ä—ã–π \"—Ä–∞–∑–¥—É–≤–∞–µ—Ç\" —è–¥—Ä–æ, –∫–∞–∫ –±—ã –≤—Å—Ç–∞–≤–ª—è—è –æ—Ç–≤–µ—Ä—Å—Ç–∏—è –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ —è–¥—Ä–∞. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä (—Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, **dilation**) —É–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è —è–¥—Ä–æ.\n",
    "\n",
    "\n",
    "–§–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤ —Ç–∞–∫–æ–π —Å–≤–µ—Ä—Ç–∫–µ –≤—Ö–æ–¥–Ω—ã–µ –ø–∏–∫—Å–µ–ª–∏ (–ø—Ä–∏–∑–Ω–∞–∫–∏) —É—á–∞—Å—Ç–≤—É—é—Ç —á–µ—Ä–µ–∑ –æ–¥–∏–Ω (–¥–≤–∞, —Ç—Ä–∏ ...).\n",
    "\n",
    "–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–≤–µ—Ä—Ç–∫–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å —Ä–µ—Ü–µ–ø—Ç–∏–≤–Ω–æ–µ –ø–æ–ª–µ –∏ —Ö–æ—Ä–æ—à–æ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Å–µ–±—è –ø—Ä–∏ [—Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π üéì[arxiv]](https://arxiv.org/pdf/1511.07122.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1,\n",
    "                padding=0, dilation=1, ...)\n",
    "```\n",
    "–≥–¥–µ:\n",
    "* `in_channels`, `out_channels` ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ –≤ –≤—Ö–æ–¥–Ω–æ–π –∏ –≤—ã—Ö–æ–¥–Ω–æ–π –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,\n",
    "* `kernel_size` ‚Äî —Ä–∞–∑–º–µ—Ä —è–¥—Ä–∞ —Å–≤–µ—Ä—Ç–∫–∏,\n",
    "* `stride` ‚Äî —à–∞–≥ —Å–≤–µ—Ä—Ç–∫–∏,\n",
    "* `padding` ‚Äî —Ä–∞–∑–º–µ—Ä –æ—Ç—Å—Ç—É–ø–æ–≤, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—ã—Ö –ø–æ –∫—Ä–∞—è–º –≤—Ö–æ–¥–Ω–æ–π –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,\n",
    "* `dilation` ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–≤–µ—Ä—Ç–∫–∏.\n",
    "\n",
    "[[doc] üõ†Ô∏è](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) `nn.Conv2d`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation models PyTorch (SMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[git] üêæ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞](https://github.com/qubvel/segmentation_models.pytorch#architectures-) –Ω–∞ –±–∞–∑–µ PyTorch  –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/smp.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "!pip install -q segmentation-models-pytorch\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–µ–∫–æ–¥–µ—Ä —Å —Ä–∞–∑–Ω—ã–º–∏ —ç–Ω–∫–æ–¥–µ—Ä–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "# 'mit_b0' Mix Vision Transformer Backbone from SegFormer pretrained on Imagenet\n",
    "preprocess_input = get_preprocessing_fn(\"mit_b0\", pretrained=\"imagenet\")\n",
    "\n",
    "# MixVisionTransformer encoder does not support in_channels setting other than 3\n",
    "# supported by FPN only for encoder depth = 5\n",
    "model = smp.FPN(\"mit_b0\", in_channels=3, classes=10, encoder_depth=5)\n",
    "\n",
    "# ... Train model on your dataset\n",
    "\n",
    "dummy_input = torch.randn([1, 3, 64, 64])\n",
    "\n",
    "mask = model(dummy_input)\n",
    "clear_output()\n",
    "print(mask.shape)  # torch.Size([1, 1, 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—É—â–µ—Å—Ç–≤—É–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ [pytorch-image-models üêæ[git]](https://github.com/huggingface/pytorch-image-models) (timm = Torch IMage Models), –≤ –∫–æ—Ç–æ—Ä–æ–π —Å–æ–±—Ä–∞–Ω–æ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.\n",
    "\n",
    "[–û–ø–∏—Å–∞–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ üõ†Ô∏è[doc]](https://huggingface.co/docs/timm/index) –∏ [–ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è üõ†Ô∏è[doc]](https://huggingface.co/docs/hub/timm) –≤ HuggingFace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model_names = timm.list_models(pretrained=True)\n",
    "print(\"Total pretrained models: \", len(model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ —à–∞–±–ª–æ–Ω—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = timm.list_models(\"*mobilenet*small*\")\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_mobilenet = timm.create_model(\"mobilenetv3_small_050\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = timm_mobilenet(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å SMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –∏–∑ timm –≤ –∫–∞—á–µ—Å—Ç–≤–µ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤.\n",
    "\n",
    "[[doc] üõ†Ô∏è –°–ø–∏—Å–æ–∫ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –º–æ–¥–µ–ª–µ–π](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n",
    "\n",
    "–ü—Ä–∏ —ç—Ç–æ–º –∫ –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–æ–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∫–ª–∞—Å—Å–∞ SMP, –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å `tu-`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_timm_model = smp.DeepLabV3(\"tu-mobilenetv3_small_050\", in_channels=3, classes=80)\n",
    "smp_timm_model.eval()\n",
    "print(\"Created DeepLab with mobileNet encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = smp_timm_model(dummy_input)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (Object detection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/detection.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Äî –∑–∞–¥–∞—á–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–π —Ç—Ä–µ–±—É–µ—Ç—Å—è\n",
    "–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏.\n",
    "\n",
    "–ü—Ä–∏ —ç—Ç–æ–º –≤—ã—á–∏—Å–ª—è—Ç—å —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏–µ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∏ (bounding boxes), –≤ –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞—Ö–æ–¥—è—Ç—Å—è –æ–±—ä–µ–∫—Ç—ã.\n",
    "\n",
    "–í –æ–±—â–µ–º —Å–ª—É—á–∞–µ –æ–±—ä–µ–∫—Ç—ã –º–æ–≥—É—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–ª–∞—Å—Å–∞–º, –∏ –æ–±—ä–µ–∫—Ç–æ–≤ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—á–Ω—ë–º —Å –ø—Ä–æ—Å—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏.\n",
    "\n",
    "–ü—É—Å—Ç—å –Ω–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç –æ–±—ä–µ–∫—Ç—ã —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞, –∏ –º—ã –∑–Ω–∞–µ–º, —á—Ç–æ —Ç–∞–∫–æ–π –æ–±—ä–µ–∫—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –µ—Å—Ç—å –∏ –æ–Ω –æ–¥–∏–Ω.\n",
    "\n",
    "–ö –ø—Ä–∏–º–µ—Ä—É, –º—ã —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–∏—Å—Ç–µ–º—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/predict_bounded_box_example.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/ftp/arxiv/papers/1807/1807.05786.pdf\"> MIDV-500: a dataset for identity document analysis and recognition\n",
    "on mobile devices in video stream</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –ø–æ–¥–∞—ë—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±–ª–∞—Å—Ç—å, –≤ –∫–æ—Ç–æ—Ä–æ–π –æ–±—ä–µ–∫—Ç –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω.\n",
    "–û–±–ª–∞—Å—Ç—å (bounding box) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –Ω–∞–±–æ—Ä–æ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤–µ—Ä—à–∏–Ω*. –°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ —ç—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –º–æ–¥–µ–ª—å.\n",
    "\n",
    "\\* *–ï—Å–ª–∏ –Ω–∞–ª–æ–∂–∏—Ç—å —É—Å–ª–æ–≤–∏–µ, —á—Ç–æ —Å—Ç–æ—Ä–æ–Ω—ã –º–Ω–æ–≥–æ—É–≥–æ–ª—å–Ω–∏–∫–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã —Å—Ç–æ—Ä–æ–Ω–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ç–æ –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º 2-—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –∑–∞–¥–∞—á—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–≤–µ—Å—Ç–∏ –∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Ç–æ –∑–¥–µ—Å—å –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–≥—Ä–µ—Å—Å–∏—é, –ø–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –Ω—É–∂–Ω–æ –Ω–µ –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞, –∞ –Ω–∞–±–æ—Ä —á–∏—Å–µ–ª.\n",
    "\n",
    "–í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π —ç—Ç–∏ —á–∏—Å–ª–∞ –º–æ–≥—É—Ç –Ω–µ—Å—Ç–∏ —Ä–∞–∑–Ω—ã–π —Å–º—ã—Å–ª, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n",
    "\n",
    "* –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ü–µ–Ω—Ç—Ä–∞ + —à–∏—Ä–∏–Ω–∞ –∏ –≤—ã—Å–æ—Ç–∞,\n",
    "* –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø—Ä–∞–≤–æ–≥–æ –≤–µ—Ä—Ö–Ω–µ–≥–æ –∏ –ª–µ–≤–æ–≥–æ –Ω–∏–∂–Ω–µ–≥–æ —É–≥–ª–æ–≤,\n",
    "* –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–µ—Ä—à–∏–Ω –º–Ω–æ–≥–æ—É–≥–æ–ª—å–Ω–∏–∫–∞ ...\n",
    "\n",
    "–ù–æ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ –∑–∞–¥–∞—á–∞ –æ—Å—Ç–∞–µ—Ç—Å—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º seeds –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fix_seeds():\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "fix_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ—à–∞–µ—Ç—Å—è –æ–Ω–∞ —Ç–∞–∫:\n",
    "\n",
    "–ë–µ—Ä–µ–º —Å–≤–µ—Ä—Ç–æ—á–Ω—É—é —Å–µ—Ç—å –∏ –º–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ —Å–æ–≤–ø–∞–¥–∞–ª–æ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å.\n",
    "\n",
    "–¢–∞–∫ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤—É—Ö —Ç–æ—á–µ–∫ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è —á–µ—Ç—ã—Ä–µ –≤—ã—Ö–æ–¥–∞ ( x1 , y1 , x2, y2 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "\n",
    "# load pretrained model\n",
    "resnet_detector = resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "\n",
    "# Change \"head\" to predict coordinates (x1,y1 x2,y2)\n",
    "resnet_detector.fc = nn.Linear(resnet_detector.fc.in_features, 4)  # x1,y1 x2,y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–∞–∫–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏–¥–µ—Ç—Å—è –∑–∞–º–µ–Ω–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –Ω–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é, –Ω–∞–ø—Ä–∏–º–µ—Ä, MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# This is a random example. Don't expect good results\n",
    "input = torch.rand((1, 3, 224, 224))\n",
    "target = torch.tensor([[0.1, 0.1, 0.5, 0.5]])  # x1,y1 x2,y2 or x,y w,h\n",
    "print(f\"Target: {target}\")\n",
    "output = resnet_detector(input)\n",
    "loss = criterion(output, target)\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–±—ã—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –æ—Ç –¥–ª–∏–Ω—ã –∏ —à–∏—Ä–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –¥–ª—è —á–µ–≥–æ –≤—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ –∫–∞–∫-—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–±–∞–≤–∏–≤ –ø–æ—Å–ª–µ–¥–Ω–∏–º —Å–ª–æ–µ–º —Å–∏–≥–º–æ–∏–¥—É:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_detector.fc = nn.Sequential(\n",
    "    nn.Linear(resnet_detector.fc.in_features, 4), nn.Sigmoid()\n",
    ")\n",
    "\n",
    "resnet_detector(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/predict_key_points.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è —Ç–æ—á–µ–∫ (Humphreys et al., 2020)</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/2011.12619\">Recent Progress in Appearance-based Action Recognition (Humphreys et al., 2020)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ —Ç–∞–∫–æ–º—É –ø—Ä–∏–Ω—Ü–∏–ø—É —Ä–∞–±–æ—Ç–∞—é—Ç –º–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫.\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä: –Ω–∞ –ª–∏—Ü–µ (facial landmarks) –∏–ª–∏ —Ç–µ–ª–µ —á–µ–ª–æ–≤–µ–∫–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤ –º—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞—É—á–∏–ª–∏—Å—å.\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å —É—Å–ª–æ–∂–Ω–∏–º –∑–∞–¥–∞—á—É: –æ–±—ä–µ–∫—Ç –æ—Å—Ç–∞–µ—Ç—Å—è –æ–¥–∏–Ω, –Ω–æ –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–ª–∞—Å—Å–∞–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/one_object.png\" width=\"650\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ –µ—Å—Ç—å –∫ –∑–∞–¥–∞—á–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è.\n",
    "\n",
    "–ó–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º—ã —É–º–µ–µ–º —Ä–µ—à–∞—Ç—å:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/class_prediction.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å—Ç–∞–µ—Ç—Å—è –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/multitask_loss_0.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å:\n",
    "\n",
    "*   –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –∫–ª–∞—Å—Å–∞–º,\n",
    "*   –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–µ–≥–æ –ø—Ä—è–º–æ–≥—É–ª—å–Ω–∏–∫–∞ (bounding box).\n",
    "\n",
    "–¢–æ–≥–¥–∞ –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è –±—É–¥–µ—Ç –∏–º–µ—Ç—å —Ä–∞–∑–º–µ—Ä:\n",
    "$$N + 4$$\n",
    "\n",
    "–≥–¥–µ $N$ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (1000 –¥–ª—è ImageNet),\n",
    "\n",
    "–∞ $4$ —á–∏—Å–ª–∞ ‚Äî —ç—Ç–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–¥–Ω–æ–≥–æ boundig box ($(x1,y1,x2,y2)$ –∏–ª–∏ $(x,y,w,h)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**–ö–∞–∫ –æ–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –¥–ª—è —Ç–∞–∫–æ–π –º–æ–¥–µ–ª–∏?**\n",
    "\n",
    "–ú–æ–∂–Ω–æ —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å loss –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ loss –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏.\n",
    "\n",
    "$ \\large L_{total} = L_{crossentropy}+L_{mse}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/multitask_loss.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–Ω–∞–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö loss –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä—è–¥–∫–∏, –ø–æ—ç—Ç–æ–º—É –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø–æ–¥–±–∏—Ä–∞—Ç—å –≤–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–∞–≥–∞–µ–º–æ–≥–æ.\n",
    "–í –æ–±—â–µ–º —Å–ª—É—á–∞–µ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –±—É–¥–µ—Ç –∏–º–µ—Ç—å –≤–∏–¥:\n",
    "\n",
    "$$\\large L_{total} = \\sum _iw_iL_i$$\n",
    "–≥–¥–µ $w_i$ ‚Äî –≤–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∫–∞–∂–¥–æ–π –∏–∑ —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å.\n",
    "\n",
    "–û–Ω–∏ —è–≤–ª—è—é—Ç—Å—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏ –∏ —Ç—Ä–µ–±—É—é—Ç –ø–æ–¥–±–æ—Ä–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥–±–æ—Ä –≤–µ—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –ø–æ–¥–±–∏—Ä–∞—Ç—å –≤–µ—Å–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ loss –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –î–ª—è —ç—Ç–æ–≥–æ –∫ –º–æ–¥–µ–ª–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–ª–æ–π:\n",
    "\n",
    "[[arxiv] üéì Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics (Alex Kendall et al., 2018)](https://arxiv.org/abs/1705.07115)\n",
    "\n",
    "[[git] üêæ –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ MultiTask learning](https://github.com/Hui-Li/multi-task-learning-example-PyTorch/blob/master/multi-task-learning-example-PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –±—ã—Ç—å, –µ—Å–ª–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ?\n",
    "\n",
    "\n",
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ –Ω—É–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã $(x1, y1, x2, y2)$ –∏ –∫–ª–∞—Å—Å $(0 .. N)$.\n",
    "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞–¥–æ —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å.\n",
    "\n",
    "–ù–æ –Ω–∞–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –∑–∞—Ä–∞–Ω–µ–µ, —Å–∫–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç–æ–≤ –±—É–¥–µ—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/object_detection_multiple_object.png\" width=\"1000\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Stanford University CS231n: Detection and Segmentation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ**\n",
    "\n",
    "–û–¥–Ω–∏–º –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∫–æ –≤—Å–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–º –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è–º –æ–±—ä–µ–∫—Ç–æ–≤. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç, –µ—Å—Ç—å –ª–∏ –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–¥–∏–Ω –∏–∑ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏—Ö –Ω–∞—Å –æ–±—ä–µ–∫—Ç–æ–≤. –ï—Å–ª–∏ –Ω–µ—Ç, —Ç–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ \"—Ñ–æ–Ω\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/naive_way_object_detection_multiple_object.gif\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Stanford University CS231n: Detection and Segmentation</a></em></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–±–ª–µ–º–æ–π –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ —è–≤–ª—è–µ—Ç—Å—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫ –æ–≥—Ä–æ–º–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —á—Ç–æ –∫—Ä–∞–π–Ω–µ –¥–æ—Ä–æ–≥–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ–∫–æ–Ω –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏, –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã:\n",
    "\n",
    "1. –ù–∞ –∏—Å—Ö–æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ–±–ª–∞—Å—Ç–∏, –≥–¥–µ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –æ–±—ä–µ–∫—Ç—ã (~2K) (R-CNN).\n",
    "2. –≠—Ç–∏ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å (–ø–æ–¥—Å–µ—Ç—å) (~2K) (Two stage detectors:  [Faster R-CNN üéì[arxiv]](https://arxiv.org/abs/1506.01497)).\n",
    "3. –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–∞–¥–∞—é—Ç—Å—è —Ü–µ–Ω—Ç—Ä—ã –∏ —Ä–∞–∑–º–µ—Ä—ã –æ–∫–æ–Ω, –∫–æ—Ç–æ—Ä—ã–µ –ø–ª–æ—Ç–Ω–æ –ø–æ–∫—Ä—ã–≤–∞—é—Ç –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ(~ 10K –æ–∫–æ–Ω), (One stage detector: [SSD üéì[arxiv]](https://arxiv.org/abs/1512.02325), YOLO).\n",
    "4. –ù–µ–π—Ä–æ—Å–µ—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞ –∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –≥—Ä–∞–Ω–∏—Ü –µ–≥–æ bounding box –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. (Anchor free: [FCOS üéì[arxiv]](https://arxiv.org/abs/1904.01355))\n",
    "5. –ï—Å–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–µ—Ç—å –Ω–∞ –±–∞–∑–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã ViT, —Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–ª–∞–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ patch (1K <) [(DETR) üéì[arxiv]](https://arxiv.org/abs/2005.12872).\n",
    "\n",
    "\n",
    "[[colab] ü•® Detectors history](https://colab.research.google.com/drive/1GWizPqYAZcYEn3yTvTSJsTzLBhVKjjza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/detector_types.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–±—â–∞—è —Å—Ö–µ–º–∞ —Ä–∞–±–æ—Ç—ã –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞**:\n",
    "\n",
    "* –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ–¥–∏–Ω–æ–∂–¥—ã –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –±–∞–∑–æ–≤—É—é —á–∞—Å—Ç—å —Å–µ—Ç–∏ (encoder),  –¥–∞–ª—å–Ω–µ–π—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–µ–ª–∞—é—Ç—Å—è –Ω–∞ –∫–∞—Ä—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "* –í–æ –≤—Å–µ—Ö —Å–ª—É—á–∞—è—Ö –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—Ç—Å—è —Å–º–µ—â–µ–Ω–∏–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö bounding boxes.\n",
    "* –î–ª—è –∫–∞–∂–¥–æ–≥–æ bounding box –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å.\n",
    "* –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –∑–∞–≤–µ–¥–æ–º–æ –±–æ–ª—å—à–µ bounding boxes, —á–µ–º –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/detector_scheme.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss\n",
    "\n",
    "–õ–æ—Å—Å —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –∏–∑ –ª–æ—Å—Å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ $L_{conf}$ –∏ –ª–æ—Å—Å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ $L_{loc}$\n",
    "\n",
    "$$\\large L_{det} = L_{conf} + \\alpha L_{loc}$$\n",
    "\n",
    "–ü—Ä–∏ —ç—Ç–æ–º –≤ $L_{loc}$ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è **–Ω–µ –≤—Å–µ** –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ bounding box, –∞ —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∏–ª—É—á—à–µ–º –æ–±—Ä–∞–∑–æ–º –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è —Å GT (bbox –∏–∑ —Ä–∞–∑–º–µ—Ç–∫–∏). –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º–æ–∂–µ—Ç –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –ø–æ—Ä–æ–≥—É –∏–ª–∏ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –¥—Ä—É–≥–∞—è –ø—Ä–æ–±–ª–µ–º–∞: –≤ —Ä–∞–π–æ–Ω–µ –æ–±—ä–µ–∫—Ç–∞ –∞–ª–≥–æ—Ä–∏—Ç–º –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤ (bounding box), –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–∏—á–Ω–æ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/non_max_suppression.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç –Ω–∏—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º\n",
    "NMS (Non maxima suppression). –ï–≥–æ –∑–∞–¥–∞—á–∞ ‚Äî –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç bounding boxes, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∫–ª–∞–¥–∞–≤–∞—é—Ç—Å—è –Ω–∞ –∏—Å—Ç–∏–Ω–Ω—ã–π:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/non_max_suppression_pseudo_code.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–¥–µ—Å—å $B$ ‚Äî —ç—Ç–æ –º–∞—Å—Å–∏–≤ –≤—Å–µ—Ö bounding boxes,  $C$ ‚Äî –º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–∞–ª–∏—á–∏—è –æ–±—ä–µ–∫—Ç–∞ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–º bounding box.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ IoU (same == IoU), –∞ –∑–Ω–∞—á–µ–Ω–∏–µ IoU ($\\lambda_{nms}$), –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º bounding boxes —Å—á–∏—Ç–∞—é—Ç—Å—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–∏–º–∏ –æ–¥–Ω–æ–º—É –æ–±—ä–µ–∫—Ç—É, —è–≤–ª—è–µ—Ç—Å—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º (—á–∞—Å—Ç–æ 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í PyTorch –∞–ª–≥–æ—Ä–∏—Ç–º NMS –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –º–æ–¥—É–ª–µ torchvision.ops\n",
    "\n",
    "`torchvision.ops.nms(boxes, scores, iou_threshold)`,\n",
    "–≥–¥–µ:\n",
    "* `boxes` ‚Äî –º–∞—Å—Å–∏–≤ bounding box,\n",
    "* `scores` ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞,\n",
    "* `iou_threshold` ‚Äî –ø–æ—Ä–æ–≥ IoU, NMS –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ—Ç –≤—Å–µ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –ø–æ–ª—è —Å $\\text{IoU} > \\text{iou_threshold}$.\n",
    "\n",
    "[[doc] üõ†Ô∏è NMS](https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html)\n",
    "\n",
    "[[git] üêæ Soft NMS](https://github.com/Gan4x4/ml_snippets/blob/main/CV/SoftNMS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone –¥–ª—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Aggregation Network Module(PAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í backbones, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö—Å—è –≤ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞—Ö –Ω–∞ –±–∞–∑–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π, –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –ø–æ–¥—Ö–æ–¥—ã, —Ä–∞–∑–≤–∏–≤–∞—é—â–∏–µ –∏–¥–µ–∏ FPN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/pan.png\" width=\"600\"></center>\n",
    "\n",
    "<center>Source: <a href =\"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Path_Aggregation_Network_CVPR_2018_paper.pdf\">Path Aggregation Network for Instance Segmentation<a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–∞) –°—Ç—Ä–æ–∏—Ç—Å—è –ø–∏—Ä–∞–º–∏–¥–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏ —ç—Ç–æ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ–ø–æ–ª–Ω—è—é—Ç—Å—è  –ø—Ä–∏–∑–Ω–∞–∫–∏–º–∏ —Å –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏—Ö —Å–ª–æ–µ–≤ (–∫—Ä–∞—Å–Ω—ã–π –ø—É–Ω–∫—Ç–∏—Ä).\n",
    "\n",
    "b) –ó–∞—Ç–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è FPN —Å—Ç—Ä–æ–∏—Ç—Å—è –µ—â–µ –æ–¥–Ω–∞ (Bottom-up), –∏ –æ–ø—è—Ç—å –Ω–æ–≤—ã–µ  –∫–∞—Ä—Ç—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ–ø–æ–ª–Ω—è—é—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –Ω–∞ –ø–µ—Ä–≤–æ–º —É—Ä–æ–≤–Ω–µ  (–∑–µ–ª–µ–Ω—ã–π –ø—É–Ω–∫—Ç–∏—Ä)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swin Transformer (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[arxiv] üéì Swin Transformer: Hierarchical Vision Transformer using Shifted Windows (Liu, Lin et al., 2021)](https://arxiv.org/abs/2103.14030)\n",
    "\n",
    "–ü—Ä–∏–º–µ–Ω—è—Ç—å ViT –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –∑–∞–¥–∞—á —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ —Å–ª–∏—à–∫–æ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö patch (16√ó16) –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ç–æ—á–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –æ–±—ä–µ–∫—Ç–æ–≤.\n",
    "\n",
    "–ê –ø—Ä–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ patch –±—É–¥–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –≤—Å–µ –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤, —Ç–∞–∫ –∫–∞–∫ —Å–ª–æ–∂–Ω–æ—Å—Ç—å self-attention $O(n^{2})$ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ –≤—Ö–æ–¥–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/swin_vs_vit.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/2103.14030.pdf\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—É –ø—Ä–∏ –ø–æ–º–æ—â–∏ –¥–≤—É—Ö —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–π.\n",
    "\n",
    "Self-attention –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–µ –∫–æ –≤—Å–µ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Å—Ä–∞–∑—É, –∞ –∫ –µ–≥–æ –±–æ–ª—å—à–∏–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º, –æ–∫–Ω–∞–º–∏.\n",
    "\n",
    "–ù–∞ –ø–µ—Ä–≤—ã–π –≤–∑–≥–ª—è–¥ —ç—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å–≤–µ—Ä—Ç–æ–∫, –ø—Ä–æ –∫–æ—Ç–æ—Ä—É—é –º—ã –≥–æ–≤–æ—Ä–∏–ª–∏:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/cnn_fail.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–∫–∞–∑—ã–≤–∞–µ–º —Å–µ—Ç–∏, –∫—É–¥–∞ —Å–º–æ—Ç—Ä–µ—Ç—å, –∏ —ç—Ç–æ –º–µ—à–∞–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –≤–∑–∞–∏–º–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É–≥–ª–∞—Ö –∫–∞—Ä—Ç—ã.\n",
    "\n",
    "–ß—Ç–æ–±—ã –Ω–µ –¥–æ–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã, –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–µ–¥—É—é—â–µ–º transformer-—Å–ª–æ–µ –æ–∫–Ω–æ —Å–¥–≤–∏–≥–∞–µ—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/swin_window_shift.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/2103.14030.pdf\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —Å–µ—Ç—å –º–æ–∂–µ—Ç –≤—ã—É—á–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –ª—é–±–æ–≥–æ –ø–∞—Ç—á–∞ –Ω–∞ –ª—é–±–æ–π. –ü—Ä–∏ —ç—Ç–æ–º –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–æ–≤ self-attention –±–ª–æ–∫–∞, –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–µ —Ä–∞—Å—Ç–µ—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–ª–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –∫–∞—Ä—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —É–º–µ–Ω—å—à–∞—é—Ç—Å—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–æ–º—É, –∫–∞–∫ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç—è—Ö. –î–ª—è —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–Ω—Ü–∏–ø FPN: –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç –∞–≥—Ä–µ–≥–∏—Ä—É—é—Ç—Å—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/swin_architecture.png\" width=\"1200\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/2103.14030.pdf\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch merging –∑–¥–µ—Å—å ‚Äî —ç—Ç–æ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –ø–æ–¥–∞—á–µ–π –Ω–∞ –≤—Ö–æ–¥ –ª–∏–Ω–µ–π–Ω–æ–≥–æ —Å–ª–æ—è.\n",
    "\n",
    "–§—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ 4-—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ 2√ó2√óC –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è. –ü–æ–ª—É—á–∞–µ–º –æ–¥–∏–Ω —Ç–µ–Ω–∑–æ—Ä 1√ó1√ó4C.\n",
    "\n",
    "–ó–∞—Ç–µ–º –ø–æ–¥–∞–µ–º –µ–≥–æ –Ω–∞ –≤—Ö–æ–¥ –ª–∏–Ω–µ–π–Ω–æ–º—É —Å–ª–æ—é, —É–º–µ–Ω—å—à–∞—é—â–µ–º—É —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤ –≤ 2 —Ä–∞–∑–∞, –ø–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é 1√ó1√ó2C.\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, —Ä–∞–∑–º–µ—Ä embedding –∑–¥–µ—Å—å –º–µ–Ω—è–µ—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/swin_result.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/2103.14030.pdf\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª–∏–ª –¥–æ—Å—Ç–∏—á—å SOTA —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∫ –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, —Ç–∞–∫ –∏ –≤ –∑–∞–¥–∞—á–∞—Ö –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.\n",
    "–ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É—é—Ç Swin Transformer –∫–∞–∫ backbone —Ä–µ—à–µ–Ω–∏—è —à–∏—Ä–æ–∫–æ–≥–æ –∫—Ä—É–≥–∞ –∑–∞–¥–∞—á CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>–ü–æ–ª–µ–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:</font>\n",
    "\n",
    "* [[arxiv] üéì You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640)\n",
    "* [[arxiv] üéì YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "* [[arxiv] üéì YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "* [[arxiv] üéì YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "* [[git] üêæ YOLOv5 (Glenn Jocher Ultralytics,  June 2020)](https://github.com/ultralytics/yolov5)\n",
    "* [[arxiv] üéì YOLOX: Exceeding YOLO Series in 2021 (Ge et al., June 2021)](https://arxiv.org/abs/2107.08430)\n",
    "* [[arxiv] üéì YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors(Chien-Yao Wang et. al. July 2022)](https://arxiv.org/abs/2207.02696)\n",
    "*  [[arxiv] üéì YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications(Chuyi Li et. al., Sept 2022)](https://arxiv.org/abs/2209.02976)\n",
    "*  [[git] üêæ YOLOv8,\tUltralytics, Dec. 2022](https://github.com/ultralytics/ultralytics), [üõ†Ô∏è[doc]](https://docs.ultralytics.com/)\n",
    "* [[arxiv] üéì YOLO6v3\tYOLOv6 v3.0: A Full-Scale Reloading Chuyi Li et. al., Dec. 2023](https://arxiv.org/abs/2301.05586)\n",
    "* [[blog] ‚úèÔ∏è YOLO-NAS: May 2023 ](https://deci.ai/blog/yolo-nas-object-detection-foundation-model/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.0/L11/YOLO-NAS.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://deci.ai/blog/yolo-nas-object-detection-foundation-model/\">YOLO-NAS by Deci Achieves SOTA Performance on Object Detection Using Neural Architecture Search</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è YOLO –≤—ã—à–ª–∞ –≤ —Ç–æ–º –∂–µ –≥–æ–¥—É, —á—Ç–æ –∏ SSD. –ù–∞ —Ç–æ—Ç –º–æ–º–µ–Ω—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–∏–≥—Ä—ã–≤–∞–ª SSD –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ –±–ª–∞–≥–æ–¥–∞—Ä—è —É—Å–∏–ª–∏—è–º Joseph Redmon –ø—Ä–æ–µ–∫—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª—Å—è –∏ —Ä–∞–∑–≤–∏–≤–∞–ª—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª–µ—Ç.\n",
    "\n",
    "3-—è –≤–µ—Ä—Å–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –æ–∫–∞–∑–∞–ª–∞—Å—å –Ω–∞—Å—Ç–æ–ª—å–∫–æ —É–¥–∞—á–Ω–æ–π, —á—Ç–æ –¥–∞–∂–µ –≤ 2021 –º–æ–∂–Ω–æ –±—ã–ª–æ –ø—Ä–æ—á–µ—Å—Ç—å: [\"YOLOv3, one of the most widely used detectors in industry\" üéì[arxiv]](https://arxiv.org/abs/2107.08430).\n",
    "\n",
    "–ü–æ—Å–ª–µ–¥—É—é—â–∏–µ –≤–µ—Ä—Å–∏–∏ —Å–æ–∑–¥–∞–≤–∞–ª–∏—Å—å —Ä–∞–∑–Ω—ã–º–∏ –∞–≤—Ç–æ—Ä–∞–º–∏. –ü–æ–ª–∞–≥–∞—é, —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—á–∏—Ç–∞—Ç—å –∏—Ö —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–∫–∞–º–∏ YOLOv3, –∞ –Ω–µ –Ω–æ–≤—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏. –î–∞–∂–µ –Ω—É–º–µ—Ä–∞—Ü–∏—è —É—Å–ª–æ–≤–Ω–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç–∞—Ç—å—è –ø—Ä–æ v7 –¥–∞—Ç–∏—Ä—É–µ—Ç—Å—è –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–µ–π –¥–∞—Ç–æ–π, —á–µ–º v6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –º–æ–∂–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [YOLO-NAS üõ†Ô∏è[doc]](https://docs.ultralytics.com/).\n",
    "\n",
    "–í –Ω–∞—Å—Ç–æ—è—â–∏–π –º–æ–º–µ–Ω—Ç –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ YOLO ‚Äî —ç—Ç–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∫ —Å–∫–æ—Ä–æ—Å—Ç–∏.\n",
    "\n",
    "[[colab] ü•® –ü–æ–¥—Ä–æ–±–Ω–µ–µ –ø—Ä–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã](https://colab.research.google.com/drive/1GWizPqYAZcYEn3yTvTSJsTzLBhVKjjza)\n",
    "\n",
    "–ú—ã –∂–µ –∑–∞–ø—É—Å—Ç–∏–º –æ–¥–Ω—É –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOLO-NAS**\n",
    "\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –º–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å –∫–∞–∫ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è [super-gradients üêæ[git]](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md), —Ç–∞–∫ –∏ –∏–∑ [ultralytics üõ†Ô∏è[doc]](https://docs.ultralytics.com/models/yolo-nas/).\n",
    "\n",
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è 1-–º, –∏ –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞–ø—É—Å—Ç–∏—Ç—å –µ–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q super_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Å—Ç–∞–Ω—Ü–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é.\n",
    "\n",
    "[[git] üêæ –ü–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π](https://github.com/Deci-AI/super-gradients/blob/master/documentation/source/model_zoo.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/yolo_nas_s.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training import models\n",
    "from super_gradients.common.object_names import Models\n",
    "\n",
    "yolo = models.get(Models.YOLO_NAS_M, pretrained_weights=\"coco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏–∑ COCO.\n",
    "\n",
    "–ò–∑ –∫–æ—Ä–æ–±–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö (–¥–∞–∂–µ url), –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –º–µ–Ω—è–µ—Ç —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏. –ü—Ä–∏ —ç—Ç–æ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É (–≤.—Ç.—á. –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é) –¥–µ–ª–∞—Ç—å –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.\n",
    "\n",
    "[[doc] üõ†Ô∏è Using Pretrained Models for Predictions](https://docs.deci.ai/super-gradients/latest/documentation/source/ModelPredictions.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Important note - When using batched input (4-dimensional np.ndarray or\n",
    "  torch.Tensor) formats, normalization and size preprocessing will be applied\n",
    "  to these inputs. This means that the input tensors should not be normalized\n",
    "  beforehand. Here is the example of incorrect code of using model.predict()\n",
    "\"\"\"\n",
    "\n",
    "results = yolo.predict(\n",
    "    \"http://images.cocodataset.org/val2017/000000448263.jpg\",  # baseball\n",
    "    conf=0.25,  # for NMS\n",
    "    iou=0.7,  # for NMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–∞—á–µ—Å—Ç–≤–µ [—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ üõ†Ô∏è[doc]](https://docs.ultralytics.com/modes/predict) –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model.predict() method returns an ImagesDetectionPrediction object, which contains the detection results for each image.\n",
    "\n",
    "print(len(results))  # contains detections for one image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£ –Ω–µ–≥–æ –µ—Å—Ç—å [–º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç üõ†Ô∏è[doc]](https://docs.ultralytics.com/reference/results/) –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö bounding box –ø–æ—Å–ª–µ NMS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction = results[0]\n",
    "\n",
    "class_names = image_prediction.class_names\n",
    "labels = image_prediction.prediction.labels\n",
    "confidence = image_prediction.prediction.confidence\n",
    "bboxes = image_prediction.prediction.bboxes_xyxy\n",
    "\n",
    "print(\"Classes\", class_names)\n",
    "print(\"Labels shape\", labels.shape)\n",
    "print(\"Confidence  shape\", confidence.shape)\n",
    "print(\"BBoxes  shape\", bboxes.shape)  # xyxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bounding box –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –º–µ—Ç–æ–¥ –∫–æ—Ç–æ—Ä—ã–π –æ—Ç—Ä–∏—Å—É–µ—Ç –∏—Ö –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction.show(box_thickness=2, show_confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L11/out/detection_instance_segmentation.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mask R-CNN** (Detectron) ‚Äî –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–∞—è, –≥–∏–±–∫–∞—è –∏ –æ–±—â–∞—è —Å—Ö–µ–º–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤. –ü–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –º–∞—Å–∫—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞.\n",
    "\n",
    "–ú–µ—Ç–æ–¥, –Ω–∞–∑–≤–∞–Ω–Ω—ã–π Mask R-CNN, —Ä–∞—Å—à–∏—Ä—è–µ—Ç Faster R-CNN, –∫–æ—Ç–æ—Ä—ã–π –º—ã –æ–±—Å—É–∂–¥–∞–ª–∏ —Ä–∞–Ω–µ–µ, –¥–æ–±–∞–≤–ª—è—è –≤–µ—Ç–≤—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–∞—Å–∫–∏ –æ–±—ä–µ–∫—Ç–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –≤–µ—Ç–≤—å—é –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è *bounding boxes*.\n",
    "\n",
    "[[git] üêæ –ö–æ–¥](https://github.com/facebookresearch/Detectron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/mask_r_cnn.png\" width=\"750\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\"> Detection and Segmentation </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] üõ†Ô∏è –ú–æ–¥–µ–ª—å Mask R-CNN](https://pytorch.org/vision/stable/models.html#mask-r-cnn)\n",
    "\n",
    "[[doc] üõ†Ô∏è –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ Mask R-CNN –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ PyTorch](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –∫–∞—Ä—Ç–∏–Ω–∫—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q \"http://images.cocodataset.org/val2017/000000448263.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "I = Image.open(\"000000448263.jpg\")\n",
    "t = ToTensor()(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from torchvision.models.detection import (\n",
    "    maskrcnn_resnet50_fpn,\n",
    "    MaskRCNN_ResNet50_FPN_Weights,\n",
    ")\n",
    "\n",
    "\n",
    "mask_rcnn = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "clear_output()\n",
    "mask_rcnn.eval()\n",
    "predictions = mask_rcnn(t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∂–¥—ã–π –∫–ª—é—á —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–Ω–∑–æ—Ä, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ—Ç–æ—Ä–æ–º —Ä–∞–≤–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0][\"masks\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_masks(masks, classes=None):\n",
    "    fig, ax = plt.subplots(nrows=4, ncols=8, figsize=(20, 10))\n",
    "    i = 0\n",
    "    for row in range(4):\n",
    "        for col in range(8):\n",
    "            if classes is not None:\n",
    "                ax[row, col].set_title(int(classes[i]))\n",
    "            ax[row, col].imshow(masks[i])\n",
    "            ax[row, col].axis(\"off\")\n",
    "            i += 1\n",
    "            if i >= len(masks):\n",
    "                plt.show()\n",
    "                return\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_masks(\n",
    "    predictions[0][\"masks\"].detach().squeeze(1), predictions[0][\"labels\"].detach()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∞—Å–∫–∏ –Ω–µ –±–∏–Ω–∞—Ä–Ω—ã–µ, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–µ–º –∏—Ö –±–∏–Ω–∞—Ä–∏–∑–æ–≤–∞—Ç—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = predictions[0][\"masks\"][2].detach().squeeze(0)\n",
    "mask = mask > 0.5\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£ YOLOv8 —Ç–∞–∫ –∂–µ –∑–∞—è–≤–ª–µ–Ω–∞ [–ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ üõ†Ô∏è[doc]](https://docs.ultralytics.com/tasks/segment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "yolo_seg = YOLO(\"yolov8m-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_results = yolo_seg(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_results[0].masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_results[0].masks.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_results[0].boxes.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "pil_with_bbox = seg_results[0].plot(boxes=True, masks=True)\n",
    "plt.imshow(pil_with_bbox[..., ::-1])  # BGR?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panoptic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] üõ†Ô∏è COCO panoptic](https://cocodataset.org/#panoptic-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/panoptic_segmentation.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://kharshit.github.io/blog/2019/10/18/introduction-to-panoptic-segmentation-tutorial\">Introduction to Panoptic Segmentation: A Tutorial</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∑–∞–¥–∞—á–∏ instance segmentation –∏ semantic segmentation.\n",
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∑–∞–¥–∞—é—Ç—Å—è –¥–≤–∞ –∑–Ω–∞—á–µ–Ω–∏—è: –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞ –∏ id –æ–±—ä–µ–∫—Ç–∞, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ø–∏–∫—Å–µ–ª—å.\n",
    "\n",
    "Id –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ç–æ–ª—å–∫–æ —É –æ–±—ä–µ–∫—Ç–æ–≤ —Ç–∞–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤, –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫ –∏–ª–∏ –º–∞—à–∏–Ω–∞. –ü–∏–∫—Å–µ–ª–∏, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ —Ç–∞–∫–∏–º —Å—É—â–Ω–æ—Å—Ç—è–º, –∫–∞–∫ –Ω–µ–±–æ –∏–ª–∏ –∑–µ–º–ª—è, id –Ω–µ –∏–º–µ—é—Ç.\n",
    "\n",
    "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∏–∫—Å–µ–ª—è –∏ –∫–ª–∞—Å—Å, –∏ id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OWL-ViT2 (Jul 2023)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer for Open-World Localization\n",
    "\n",
    "[[arxiv] üéì Scaling Open-Vocabulary Object Detection (Minderer et al., 2023)](https://arxiv.org/abs/2306.09683)\n",
    "\n",
    "[[doc] üõ†Ô∏è –ö–æ–¥](https://huggingface.co/docs/transformers/model_doc/owlv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OWL2 ‚Äî —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –æ—Å—É—â–µ—Å—Ç–≤–ª—è—é—â–∏–π –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–º—É —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É.\n",
    "\n",
    "–ú–æ–¥–µ–ª–∏ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –Ω–∞–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤. –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç bbox , –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Ç–µ–∫—Å—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://ml.gan4x4.ru/msu/dep-2.0/L11/owlv2_overview.png\" width=\"900\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/2306.09683\">Scaling Open-Vocabulary Object Detection (Minderer et al., 2023)</a></em></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Loss –¥–ª—è bbox —Ç–∞–∫–æ–π –∂–µ, –∫–∞–∫ –≤ [DETR üéì[arxiv]](https://arxiv.org/abs/2005.12872) (`bipartite matching`).\n",
    "*  –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —É—á–∞—Ç—Å—è –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ [CLIP üéì[arxiv]](https://arxiv.org/abs/2103.00020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –∏ `processor`.\n",
    "\n",
    "`processor` ‚Äî —ç—Ç–æ –∫–ª–∞—Å—Å, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from IPython.display import clear_output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\" \")\n",
    "clear_output()\n",
    "print(\"loaded\")  # suppress huge output of model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qN \"http://images.cocodataset.org/val2017/000000448263.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "img = Image.open(\"000000448263.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –Ω–∞–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"cap\", \"botle\", \"text\", \"boy\", \"player\"]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [img]\n",
    "batch_size = len(batch)\n",
    "with torch.inference_mode():\n",
    "    inputs = processor(\n",
    "        text=texts * batch_size,  # copy the same text for all images\n",
    "        images=[img],  # batch of images in PIL format\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)  # return object of type OwlViTObjectDetectionOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bboxex\", outputs[\"pred_boxes\"].shape)  # batch * total_boxes * 4 coords per box\n",
    "print(\"logits\", outputs[\"logits\"].shape)  # batch * total_boxes * text_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞**\n",
    "\n",
    "–ü—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–æ–≥—É—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å—Å—è (resize). –ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ bbox –Ω–∞ –≤—ã—Ö–æ–¥–µ, –≤ –º–µ—Ç–æ–¥ –º–æ–∂–Ω–æ –ø–æ–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = img.size[::-1]  # WH -> HW\n",
    "target_sizes = torch.tensor([size])\n",
    "print(target_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = processor.post_process_object_detection(\n",
    "    threshold=0.1,  # drop bbox with scores below 0.1\n",
    "    outputs=outputs,\n",
    "    target_sizes=target_sizes.to(device),\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.ops import box_convert\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def draw_bbox(img, bb, color, labels=None, xywh=True):\n",
    "    t_img = torch.tensor(img).permute(2, 0, 1)  # to tensor CHW\n",
    "    bb = bb[:, :4].clone().detach()  # take only coords\n",
    "    if xywh:\n",
    "        bb = box_convert(bb, \"xywh\", \"xyxy\")  # convert from COCO format\n",
    "    img_with_bb = draw_bounding_boxes(t_img, bb, colors=color, width=2, labels=labels)\n",
    "    return img_with_bb.permute(1, 2, 0).numpy()  # back to numpy HWC\n",
    "\n",
    "\n",
    "def show_owl_results(results):\n",
    "    labels = []\n",
    "    for i, text_idx in enumerate(results[0][\"labels\"]):\n",
    "        labels.append(texts[text_idx] + f\" {results[0]['scores'][i]:.2f}\")\n",
    "    image_with_bb = draw_bbox(\n",
    "        np.array(img), results[0][\"boxes\"], \"lime\", labels=labels, xywh=False\n",
    "    )\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_with_bb)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_owl_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_results = processor.post_process_object_detection(\n",
    "    threshold=0.07,  # change threshold\n",
    "    outputs=outputs,\n",
    "    target_sizes=target_sizes.to(device),\n",
    ")\n",
    "\n",
    "show_owl_results(coarse_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[colab] ü•® OWL-ViT minimal example](https://colab.research.google.com/github/google-research/scenic/blob/main/scenic/projects/owl_vit/notebooks/OWL_ViT_minimal_example.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SAM (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[arxiv] üéì Segment Anything (Kirillov et al., 2023)](https://arxiv.org/abs/2304.02643)\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–±–æ—Ä –º–∞—Å–æ–∫, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤—Ö–æ–¥—É. –ö–ª–∞—Å—Å—ã –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è.\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–∞ –º–æ–≥—É—Ç –ø–æ–¥–∞–≤–∞—Ç—å—Å—è:\n",
    "\n",
    "*  –Ω–∞–±–æ—Ä —Ç–æ—á–µ–∫,\n",
    "*  –Ω–∞–±–æ—Ä bounding box,\n",
    "*  –º–∞—Å–∫–∏,\n",
    "*  —Ç–µ–∫—Å—Ç (–ø–æ–¥–µ—Ä–∂–∫–∞ –≤ –∫–æ–¥–µ –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞),\n",
    "*  –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/sam_overview.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–∞–ª–∞—Å—å –Ω–∞ –æ–≥—Ä–æ–º–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ, —á–∞—Å—Ç–∏—á–Ω–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –≤ unsupervise —Ä–µ–∂–∏–º–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L11/sam_architecture.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£—Å—Ç–∞–Ω–æ–≤–∏–º –ø–∞–∫–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –∏–∑ [—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è Facebook Research üêæ[git]](https://github.com/facebookresearch/segment-anything#model-checkpoints):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT-H\n",
    "!wget -qN https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "# !wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/weights/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–µ–º encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "# model_type = 'vit_h'\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q \"http://images.cocodataset.org/val2017/000000448263.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(\"000000448263.jpg\")\n",
    "np_im = np.array(img)  # HWC format\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥ (–Ω–∞ CPU –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –¥–æ–ª–≥–æ) –∏ –ø—Ä–µ–¥—Å–∫–∞–∂–µ–º –≤—Å–µ –º–∞—Å–∫–∏.\n",
    "\n",
    "[[git] üêæ Automatically generating object masks with SAM (example)](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(np_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0][\"segmentation\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones(\n",
    "        (\n",
    "            sorted_anns[0][\"segmentation\"].shape[0],\n",
    "            sorted_anns[0][\"segmentation\"].shape[1],\n",
    "            4,\n",
    "        )\n",
    "    )\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann[\"segmentation\"]\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(img)\n",
    "show_anns(masks)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –ø–æ —Ç–æ—á–∫–∞–º](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb).\n",
    "–°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥. –û–Ω —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from segment_anything import SamPredictor\n",
    "\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(np_im)  # create embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, —É–∫–∞–∑–∞–≤ —Ç–æ—á–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –æ–±—ä–µ–∫—Ç—É –∏ —Ñ–æ–Ω—É:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=np.array([[200, 200], [1, 1]]),  # point coords\n",
    "    point_labels=np.array([1, 0]),  # 1 - object(foreground), 0 - background\n",
    "    # box\n",
    "    # mask_input\n",
    "    multimask_output=True,  # return 1 or 3 masks because of the ambiguous input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Masks count\", len(masks))\n",
    "print(\"Scores\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "show_mask(masks[2], plt.gca())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP ‚Äî mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP (Average Precision ‚Äî —Å—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å) ‚Äî —ç—Ç–æ –ø–ª–æ—â–∞–¥—å –ø–æ–¥ —Å–≥–ª–∞–∂–µ–Ω–Ω–æ–π PR-–∫—Ä–∏–≤–æ–π.\n",
    "\n",
    "m (mean) ‚Äî  —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–∞—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ IoU.\n",
    "\n",
    "–†–∞–∑–±–µ—Ä–µ–º—Å—è, —á—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision & recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–µ—Ç–∏ (—Ç.–µ. –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)\n",
    "\n",
    "**Recall** –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —Å–µ—Ç—å –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è (*positives*). –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –º–æ–∂–µ–º –Ω–∞–π—Ç–∏ 80% –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π –≤ –Ω–∞—à–∏—Ö K –ª—É—á—à–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Ç –∏—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:\n",
    "\n",
    "$\\displaystyle\\mathrm{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$\n",
    "\n",
    "$\\displaystyle\\mathrm{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$\n",
    "\n",
    "–≥–¥–µ $\\text{TP}$ ‚Äî True Positive, $\\text{TN}$ ‚Äî True Negative, $\\text{FP}$ ‚Äî False Positive, $\\text{FN}$ ‚Äî False Negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –ø—ã—Ç–∞–µ–º—Å—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —è–±–ª–æ–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è—Ö. –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, –º—ã –æ–±—Ä–∞–±–æ—Ç–∞–ª–∏ 20 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (–Ω–∞ 10 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è—Ö –ø–æ –æ–¥–Ω–æ–º—É —è–±–ª–æ–∫—É –∏ –Ω–∞ 10 —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è—Ö —è–±–ª–æ–∫ –Ω–µ—Ç) –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ —á—Ç–æ:\n",
    "\n",
    "* –≤ 7 —Å–ª—É—á–∞—è—Ö –Ω–∞—à–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞ —è–±–ª–æ–∫–æ —Ç–∞–º, –≥–¥–µ –æ–Ω–æ –±—ã–ª–æ –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ (True Positive);\n",
    "* –≤ 3 —Å–ª—É—á–∞—è—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞ —è–±–ª–æ–∫–æ —Ç–∞–º, –≥–¥–µ –æ–Ω–æ –±—ã–ª–æ (False Negative);\n",
    "* –≤ 4 —Å–ª—É—á–∞—è—Ö –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞ —è–±–ª–æ–∫–æ —Ç–∞–º, –≥–¥–µ –µ–≥–æ –Ω–µ –±—ã–ª–æ (False Positive);\n",
    "* –≤ 6 —Å–ª—É—á–∞—è—Ö –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∞, —á—Ç–æ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —è–±–ª–æ–∫ –Ω–µ—Ç (True Negative).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—á–∏—Ç–∞–µ–º precision –∏ recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(TP, FP):\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "\n",
    "pres = precision(TP=7, FP=4)\n",
    "rec = recall(TP=7, FN=3)\n",
    "\n",
    "print(\"Precision = %.2f\" % pres)\n",
    "print(\"Recall = %.2f\" % rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRCurve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–Ω–∞—è Precision –∏ Recall –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤, –º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º—É—é [PR-–∫—Ä–∏–≤—É—é ‚úèÔ∏è[blog]](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/).\n",
    "\n",
    "[[git] üêæ –ö–∞–∫ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å PR curve](https://github.com/Gan4x4/ml_snippets/blob/main/Training/PR_curve.ipynb).\n",
    "\n",
    "–ü—Ä–∏ –ø–æ–º–æ—â–∏ —Ç–∞–∫–∏—Ö –∫—Ä–∏–≤—ã—Ö —á–∞—Å—Ç–æ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã.\n",
    "\n",
    "–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π Precision –∏ Recall —Ç—Ä–µ–±—É—é—Ç—Å—è:\n",
    "\n",
    "*  Ground True –º–µ—Ç–∫–∏;\n",
    "*  —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 1, 0])  # Labels\n",
    "y_pred = np.array([0.8, 0.1, 0.2])  # Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å PR-–∫—Ä–∏–≤—É—é –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pr_curve(y_true, y_pred):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    plt.plot(recall, precision, marker=\"o\")\n",
    "    for i, t in enumerate(thresholds):\n",
    "        plt.annotate(str(t), (recall[i], precision[i]))\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xlabel(\"recall\")\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.title(\"PR curve\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pr_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU –≤–º–µ—Å—Ç–æ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–ü—Ä–∏ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —É –Ω–∞—Å  **–Ω–µ—Ç –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–∞** –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö bounding\n",
    "box.\n",
    "\n",
    "–ï—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–º bbox –≤ –≤–∏–¥–µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "detections = np.array(\n",
    "    [\n",
    "        [290, 50, 170, 160, 0.7, 53],  # x, y, w, h, confidence, class_num\n",
    "        [10, 200, 190, 180, 0.8, 53],\n",
    "        [310, 250, 120, 130, 0.75, 53],\n",
    "    ]\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=detections, columns=[\"x\", \"y\", \"width\", \"height\", \"confidence\", \"class_num\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–ò –µ—Å—Ç—å Ground True bounding box –∏–∑ —Ä–∞–∑–º–µ—Ç–∫–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qN \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "# !wget -qN \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/annotations_trainval2017.zip\"\n",
    "!unzip -qn annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")\n",
    "clear_output()\n",
    "\n",
    "apples_img_id = 60855  # if of some image with apples\n",
    "apple_cat_id = 53  # apple\n",
    "\n",
    "ann_id = coco.getAnnIds(imgIds=[apples_img_id])\n",
    "anns = coco.loadAnns(ann_id)\n",
    "\n",
    "gt_bbox = []\n",
    "for ann in anns:\n",
    "    if ann[\"category_id\"] == apple_cat_id:\n",
    "        gt_bbox.append(ann[\"bbox\"] + [ann[\"category_id\"]])\n",
    "\n",
    "\n",
    "pd.DataFrame(data=gt_bbox, columns=[\"x\", \"y\", \"width\", \"height\", \"class_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–¥–µ—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π IoU –º–µ–∂–¥—É —ç—Ç–∏–º–∏ bounding box –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤–º–µ—Å—Ç–æ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–∞ (True/False).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import skimage\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "\n",
    "def draw_bbox(img, bb_xywh, color):\n",
    "    t_img = torch.tensor(img).permute(2, 0, 1)  # to tensor CHW\n",
    "    xywh = torch.tensor(bb_xywh)[:, :4]  # take only coords\n",
    "    bb = box_convert(xywh, \"xywh\", \"xyxy\")  # convert from COCO format\n",
    "    img_with_bb = draw_bounding_boxes(t_img, bb, colors=color, width=4)\n",
    "    return img_with_bb.permute(1, 2, 0).numpy()  # back to numpy HWC\n",
    "\n",
    "\n",
    "img_info = coco.loadImgs(apples_img_id)\n",
    "\n",
    "img = skimage.io.imread(img_info[0][\"coco_url\"])\n",
    "img = draw_bbox(img, detections, \"blue\")\n",
    "img = draw_bbox(img, gt_bbox, \"lime\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ $\\text{IoU}$  –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ bbox,  –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Ñ—É–Ω–∫—Ü–∏–µ–π `torchvision.ops.box_iou` üõ†Ô∏è[[doc]](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html).\n",
    "\n",
    "–ü–µ—Ä–µ–¥ —ç—Ç–∏–º –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∫ `(x1,y1,x2,y2)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = box_convert(torch.tensor(gt_bbox)[:, :4], \"xywh\", \"xyxy\")\n",
    "pred = box_convert(torch.tensor(detections)[:, :4], \"xywh\", \"xyxy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò –ø–æ–ª—É—á–∏–º IoU –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–∞—Ä bbox:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "pairwise_iou = box_iou(gt, pred)\n",
    "print(pairwise_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç —Ç–æ–ª—å–∫–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou, _ = pairwise_iou.max(dim=0)\n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–ø–∏—à–µ–º —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü—É.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.tensor(detections)[:, 4]\n",
    "predictions = torch.vstack((predictions, iou)).T\n",
    "\n",
    "pd.DataFrame(data=predictions, columns=[\"confidence\", \"iou\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å IoU –∫–∞–∫ GT –º–µ—Ç–∫—É –∫–ª–∞—Å—Å–∞: –µ—Å–ª–∏ –æ–Ω –±–æ–ª—å—à–µ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä–æ–≥–∞, —Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–Ω–æ–µ.\n",
    "\n",
    "–í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ –µ—Å–ª–∏ $\\text{IoU}‚â•0.5$, —Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = iou > 0.5\n",
    "\n",
    "predictions = torch.vstack((predictions[:, 0], gt)).T\n",
    "detection_results = pd.DataFrame(data=predictions, columns=[\"confidence\", \"gt\"])\n",
    "detection_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å PR-–∫—Ä–∏–≤—É—é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "precision, recall, thresholds = precision_recall_curve(gt, predictions[:, 0])\n",
    "plt.plot(recall, precision, marker=\"o\")\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.title(\"PR curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP —ç—Ç–æ –ø–ª–æ—â–∞–¥—å –ø–æ–¥ PR –∫—Ä–∏–≤–æ–π.\n",
    "\n",
    "$\\large \\displaystyle AP = \\int_0^1p(r)dr$\n",
    "\n",
    "–ò–º–µ—è –∑–Ω–∞—á–µ–Ω–∏—è Precision –∏ Recall, –º–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å AP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap = average_precision_score(gt, predictions[:, 0])\n",
    "print(\"AP\", ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ä–∞–≤–Ω–∏–º —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —Å –ø–ª–æ—â–∞–¥—å—é –ø–æ–¥ –∫—Ä–∏–≤–æ–π:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "auc = metrics.auc(recall, precision)\n",
    "print(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –≤–∏–¥–∏–º, –æ–Ω–∏ –Ω–µ —Å–æ–≤–ø–∞–ª–∏. –î–µ–ª–æ –≤ —Ç–æ–º, —á—Ç–æ –ø—Ä–∏ –ø–æ–¥—Å—á–µ—Ç–µ AP –∫—Ä–∏–≤—É—é —Å–≥–ª–∞–∂–∏–≤–∞—é—Ç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_precision(precision):\n",
    "    smooth_prec = []\n",
    "    for i in range(1, len(precision) + 1):\n",
    "        max = precision[:i].max()\n",
    "        smooth_prec.append(max)\n",
    "    return smooth_prec\n",
    "\n",
    "\n",
    "detection_results = detection_results.sort_values(\"confidence\", ascending=False)\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    detection_results[\"gt\"], detection_results[\"confidence\"]\n",
    ")\n",
    "\n",
    "smoothed_precision = smooth_precision(precision)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall, precision, marker=\"o\")\n",
    "plt.plot(recall, smoothed_precision, marker=\"o\")\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.title(\"PR curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_auc = metrics.auc(recall, smoothed_precision)\n",
    "print(\"smooth AUC \", smooth_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –ø–æ—Å—á–∏—Ç–∞–ª–∏ AP –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞, –Ω–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏—Ö –º–Ω–æ–≥–æ.\n",
    "–õ–æ–≥–∏—á–Ω–æ —É—Å—Ä–µ–¥–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è AP –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤. –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ—Å–ª–µ —Ç–∞–∫–æ–≥–æ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –∫ –Ω–∞–∑–≤–∞–Ω–∏—é –º–µ—Ç—Ä–∏–∫–∏ –¥–æ–±–∞–≤–ª—è—é—Ç –ø—Ä–∏—Å—Ç–∞–≤–∫—É m(mean)\n",
    "\n",
    "*AP is averaged over all categories. Traditionally, this is called ‚Äúmean average precision‚Äù (mAP). We make no distinction between AP and mAP*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –≤—ã–±—Ä–∞–ª–∏ –¥–ª—è IoU –ø–æ—Ä–æ–≥ 0.5.\n",
    "\n",
    "–ù–æ –º–æ–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å AP –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤, –∏ –∑–∞—Ç–µ–º —Ç–æ–∂–µ —É—Å—Ä–µ–¥–Ω–∏—Ç—å. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫–∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥–∏.\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä AP@[.5:.95] —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ä–µ–¥–Ω–µ–º—É AP –¥–ª—è IoU –æ—Ç 0.5 –¥–æ 0.95 —Å —à–∞–≥–æ–º 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "t_gt_bbox = torch.tensor(gt_bbox)\n",
    "t_detections = torch.tensor(detections)\n",
    "\n",
    "map_obj = MeanAveragePrecision(\"xywh\", iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75])\n",
    "results = map_obj(\n",
    "    preds=[\n",
    "        {\n",
    "            \"boxes\": t_detections[:, :4],  # xywh\n",
    "            \"scores\": t_detections[:, 4],  # confidence\n",
    "            \"labels\": t_detections[:, 5].int(),  # class num\n",
    "        }\n",
    "    ],\n",
    "    target=[\n",
    "        {\n",
    "            \"boxes\": t_gt_bbox[:, :4],\n",
    "            \"labels\": t_gt_bbox[:, 4].int(),\n",
    "        }  # xywh  # class num\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"mAP@[0.5:0.75] = \", results[\"map\"].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "\n",
    "* –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ –∏ –æ–Ω–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ UNet.\n",
    "* –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ DeepLab –≤ –∫–∞—á–µ—Å—Ç–≤–µ baseline.\n",
    "* –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–∞–∫–µ—Ç—ã [SMP üêæ[git]](https://github.com/qubvel/segmentation_models.pytorch) –∏ [torchmetrics üêæ[git]](https://github.com/Lightning-AI/torchmetrics).\n",
    "* –î–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ YOLO-NAS –∏–∑ [super-gradients üêæ[git]](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md) –∏–ª–∏ [ultralytics üõ†Ô∏è[doc]](https://docs.ultralytics.com/models/yolo-nas/) —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤.\n",
    "* –ï—Å–ª–∏ –≤–∞—à–∞ –∑–∞–¥–∞—á–∞ —Å–≤—è–∑–∞–Ω–∞ —Å —Ç—Ä–µ–∫–∏–Ω–≥–æ–º, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—é.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"6\">–°–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã</font>\n",
    "\n",
    "<font size =\"5\">COCO:</font>\n",
    "* [[blog] ‚úèÔ∏è –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ —Ç–æ–º, –∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–π COCO –¥–∞—Ç–∞—Å–µ—Ç —Å –Ω—É–ª—è](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch).\n",
    "* [[doc] üõ†Ô∏è –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ —Ä–∞–∑–º–µ—Ç–∫–µ COCO](https://cocodataset.org/#format-data)\n",
    "* [[wiki] üìö Run-length encoding (RLE)](https://en.wikipedia.org/wiki/Run-length_encoding)?\n",
    "* [[video] üì∫ COCO Dataset Format ‚Äî Complete Walkthrough](https://www.youtube.com/watch?v=h6s61a_pqfM)\n",
    "\n",
    "<font size =\"5\">–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è:</font>\n",
    "\n",
    "* [[blog] ‚úèÔ∏è The Beginner‚Äôs Guide to Semantic Segmentation](https://www.v7labs.com/blog/semantic-segmentation-guide)\n",
    "* [[arxiv] üéì Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
    "* [[arxiv] üéì Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (He et al., 2014)](https://arxiv.org/abs/1406.4729)\n",
    "* [[blog] ‚úèÔ∏è An overview of semantic image segmentation](https://www.jeremyjordan.me/semantic-segmentation/)\n",
    "\n",
    "<font size =\"5\">–î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ</font>\n",
    "\n",
    "* [[arxiv] üéì Recent Progress in Appearance-based Action Recognition (Humphreys et al., 2020)](https://arxiv.org/abs/2011.12619)\n",
    "* [[article] üéì Selective Search for Object Recognition](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "* [[arxiv] üéì SSD: Single Shot MultiBox Detector (Liu et al., 2015)](https://arxiv.org/abs/1512.02325)\n",
    "* [[arxiv] üéì Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)\n",
    "* [[arxiv] üéì –ü—Ä–æ Focal Loss –æ—Ç Facebook AI Research](https://arxiv.org/abs/1708.02002)\n",
    "* [[arxiv] üéì Feature Pyramid Networks for Object Detection (Sergelius et al., 2016)](https://arxiv.org/abs/1612.0314)\n",
    "* [[blog] ‚úèÔ∏è What is Feature Pyramid Network (FPN)](https://www.fastpath.one/blog/feature-pyramid-network-fpn)\n",
    "* [[arxiv] üéì You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640)\n",
    "* [[arxiv] üéì YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "* [[arxiv] üéì YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "* [[arxiv] üéì YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "* [[git] üêæ YOLOv5 (Glenn Jocher)](https://github.com/ultralytics/yolov5)\n",
    "* [[arxiv] üéì YOLOX: Exceeding YOLO Series in 2021 (Ge et al., 2021)](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "<font size =\"5\">Hard Example Mining</font>\n",
    "\n",
    "* [[arxiv] üéì Training Region-based Object Detectors with Online Hard Example Mining (Shrivastava et al., 2016)](https://arxiv.org/abs/1604.03540)\n",
    "* [[arxiv] üéì Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors (Yu et al., 2018)](https://arxiv.org/abs/1804.04606)\n",
    "\n",
    "<font size =\"5\">–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:</font>\n",
    "\n",
    "* [[arxiv] üéì DINO: Emerging Properties in Self-Supervised Vision Transformers (Caron et al., 2021)](https://arxiv.org/abs/2104.14294)\n",
    "* [[video] üì∫ –û–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ DINO](https://www.youtube.com/watch?v=h3ij3F3cPIk)\n",
    "* [[arxiv] üéì U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597)\n",
    "* [[blog] ‚úèÔ∏è –ü—Ä–æ 2D —Å–≤–µ—Ä—Ç–∫–∏ —Å –ø–æ–º–æ—â—å—é –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü](https://www.baeldung.com/cs/convolution-matrix-multiplication)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
