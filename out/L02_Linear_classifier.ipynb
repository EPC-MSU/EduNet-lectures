{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Линейный классификатор </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ограничения алгоритма k-nearest neighbors (kNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжим с того места, где мы остановились в предыдущей лекции - классификация методом ближайших соседей (kNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метрики расстояния**\n",
    "\n",
    "В практических заданиях EX01 мы использовали kNN для классификации изображений CIFAR10 (набор фотографий разделенных на 10 классов). Мы выяснили, что точность метода классификации с помощью kNN с использованием расстояний L1 (*Manhatten distance* - сумма абсолютных разностей между пикселями) оставляет желать лучшего, и попробовали применить L2 (*Euclidian distance*) в надежде эту точность повысить. С метриками L1 и L2 мы будем сталкиваться часто: и в качестве loss функции, и в качестве регуляризации, поэтому познакомиться с ними полезно. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/l1_manhattan_and_l2_euclidian_distance.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "На практике, метод ближайших соседей для классификации используется крайне редко. Давайте разберемся почему.\n",
    "Проблема заключается в следующем: предположим, что точность классификации нас устраивает. Теперь давайте применим kNN на больших данных (e.g. миллион картинок). Для определения класса каждой из картинок, нам нужно сравнить ее со всеми другими картинками в базе данных, а такие расчеты, даже в существенно оптимизированном виде, занимают много времени. Мы же хотим, чтобы обученная модель работала быстро.\n",
    "\n",
    "Тем не менее, метод ближайших соседей используется в других задачах, где без него обойтись сложно. Например, в задаче распознавания лиц. Представим, что у нас у нас есть большая база данных с фотографиями лиц (например, по 5 разных фотографий всех сотрудников, которые работают в офисном здании, как на примере выше) и есть камера, установленная на входе в это здание. Мы хотим узнать, кто и во сколько пришел на работу. Для того чтобы понять кто прошел перед камерой, нам нужно зафиксировать лицо этого человека и сравнить его со всеми фотографиями лиц в базе. В такой формулировке мы не пытаемся определить конкретный класс фотографии, а всего лишь определяем “похож-не похож”. Мы смотрим на k ближайших соседей и, например, если из k соседей, 5 - это фотографии Джеки Чана, то, скорее всего, под камерой прошел именно он. В таких случаях kNN метод вполне полезен. Похожим образом работает и поиск дубликатов в базах данных.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/knn_for_classification.png\" width=\"450\">\n",
    "\n",
    "Примеры эффективной реализации метода на основе kNN:\n",
    "* [Facebook AI Research Similarity Search](https://github.com/facebookresearch/faiss) – разработка команды Facebook AI Research для быстрого поиска ближайших соседей и кластеризации в векторном пространстве. Высокая скорость поиска позволяет работать с очень большими данными – до нескольких миллиардов векторов.\n",
    "* Алгоритм поиска ближайших соседей [Hierarchical Navigable Small World](https://arxiv.org/abs/1603.09320). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практические аспекты работы с классификаторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков, соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30 признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n",
    "\n",
    "Можно иметь сколь угодно хороший алгоритм для классификации - но до тех пор, пока данные на входе - мусор, на выходе из нашего чудесного классификатора мы тоже будем получать мусор (*garbage in - garbage out*). Давайте разберемся, что конкретно надо сделать, чтобы kNN реально заработал.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer() # load data\n",
    "x = cancer.data # features\n",
    "y = cancer.target # labels(classes)\n",
    "print(f'x shape: {x.shape}, y shape: {y.shape}') \n",
    "print(f'x[0]: \\n {x[0]}') \n",
    "print(f'y[0]: \\n {y[0]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько данных в классе `0` и сколько данных в классе `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5)) # set fig size \n",
    "plt.bar(1, y[y == 1].shape, label=cancer.target_names[0]) # 1 label \n",
    "plt.bar(0, y[y == 0].shape, label=cancer.target_names[1]) # 0 label\n",
    "plt.title('Class balance')\n",
    "plt.ylabel('Num examples')\n",
    "plt.xticks(ticks=[1, 0], labels=['1', '0']) \n",
    "plt.legend(loc='upper left') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк в каждой, из которой, по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.boxenplot(data=pd.DataFrame(x), orient=\"h\", palette=\"Set2\")\n",
    "ax.set(xscale='log', xlim=(1e-4, 1e4), xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы адекватно сравнить данные между собой нам следует использовать нормализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Нормализация, выбор Scaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализацией называется процедура приведения входных данных к единому масштабу (диапазону) значений. Фактически, это означает построение взаимно однозначного соответствия между некоторыми размерными величинами (которые измеряются в метрах, килограммах, годах и т. п.) и их безразмерными аналогами, принимающими значение в строго определенном числовом диапазоне (скажем, на отрезке $[0,1]$). Преобразование данных к единому числовому диапазону (иногда говорят *домену*) позволяет считать их равноправными признаками и единообразно передавать их на вход модели. В некоторых источниках данная процедура явно называется *масштабирование*.\n",
    "\n",
    "$$\\text{scaling map} \\; : \\text{some arbitrary feature domain} \\rightarrow \\text{definite domain} $$\n",
    "\n",
    "Иногда под нормализацией данных понимают процедуру *стандартизации*, то есть приведение множеств значений всех признаков к стандартному нормальному распределению -- распределению, с нулевым среднем значением и единичной дисперсией.\n",
    "\n",
    "$$\\text{standartization map} : f_i \\rightarrow (f_i - \\text{mean} (\\{f_i\\})) \\cdot \\frac{1}{\\text{std} (\\{f_i\\})}$$\n",
    "\n",
    "\n",
    "Рассмотрим небольшой пример. Пусть у нас есть данные о некоторой группе людей, содержащие два признака: *возраст* (в годах) и *размер дохода* (в рублях). Возраст может измениться в диапазоне от 18 до 70 ( интервал 70-18 = 52). А доход от 30 000 р до 500 000 р (интервал 500 000 - 30 000 = 470 000). В таком варианте разница в возрасте имеет меньшее влияние, чем разница в доходе. Получается, что доход становится более важным признаком, изменения в котором влияют больше при сравнении схожести двух людей.\n",
    "\n",
    "Должно быть так, чтобы максимальные изменения любого признака в «основной массе объектов» были одинаковы. Тогда потенциально все признаки будут равноценны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось определиться с выбором инструмента, часто используют следующие варианты: `MinMaxScaler`, `StandardScaler`, `RobustScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для признака `data[:,0]`. **Обратите внимание на ось X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # setting the initialization parameter for random values\n",
    "\n",
    "# generate random values from 1 to 255, shape (30,1)\n",
    "test = x[:, 0].reshape(-1, 1)\n",
    "\n",
    "plt.figure(1, figsize=(30, 5))  \n",
    "plt.subplot(141)  # set location\n",
    "plt.scatter(test, range(len(test)), c=y)  \n",
    "plt.ylabel(\"Num examples\", fontsize=15)  \n",
    "plt.xticks(fontsize=15)  \n",
    "plt.yticks(fontsize=15)  \n",
    "plt.title(\"Non scaled data\", fontsize=18)  \n",
    "\n",
    "# scale data with MinMaxScaler\n",
    "test_scaled = MinMaxScaler().fit_transform(test)  \n",
    "plt.subplot(142)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"MinMaxScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with StandardScaler\n",
    "test_scaled = StandardScaler().fit_transform(test)  \n",
    "plt.subplot(143)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"StandardScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with RobustScaler\n",
    "test_scaled = RobustScaler().fit_transform(test)  \n",
    "plt.subplot(144)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"RobustScaler\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные в диапазоне от 0 до 1. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (e.g., масштабирование RGB пикселей)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$z=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "$X_{min}$ и $X_{max}$ задаются как минимальное и максимальное допустимое значение, по умолчанию:  $X_{min}=0$  и $X_{max}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение 0 и стандартное отклонение 1. Большинство значений будет в  диапазоне от -1 до 1. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-u}{s}$$\n",
    "\n",
    "$u$ — среднее значение (или 0 при `with_mean=False`) и $s$ — стандартное отклонение (или 0 при `with_std=False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И StandardScaler и MinMaxScaler очень чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. k-й процентиль – это величина, равная или не превосходящая k процентов чисел во всем имеющемся распределении. Например, 50-й процентиль (медиана) распределения таково, что 50% чисел из распределения не меньше данного числа. Соответственно, RobustScaler не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-X_{median}}{IQR}$$\n",
    "\n",
    "$X_{median}$ — значение медианы, $IQR$ — межквартильный диапазон равный разнице между 75-ым и 25-ым процентилями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нашей задачи по определению раковых опухолей обработаем наши 30 признаков с помощью StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm = StandardScaler().fit_transform(x)  # scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что они стали намного более сравнимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_norm).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxenplot(data=pd.DataFrame(x_norm), \n",
    "                   orient=\"h\", \n",
    "                   palette=\"Set2\")\n",
    "ax.set(xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим kNN для общей выборки данных, при разном значении количества соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  # array of the number of neighbors\n",
    "\n",
    "quality = np.zeros(\n",
    "    n_nei_rng.shape[0]\n",
    ")  \n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  # for all elements\n",
    "    # create knn for all num neighbors \n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_nei_rng[ind]\n",
    "    )  \n",
    "    knn.fit(x_norm, y)  \n",
    "    q = accuracy_score(y_pred=knn.predict(x_norm), y_true=y)  # accuracy\n",
    "    quality[ind] = q  # fill quality\n",
    "\n",
    "plt.figure(figsize=(8, 5))  \n",
    "plt.title(\"KNN on train\", size=20)  \n",
    "plt.xlabel(\"Neighbors\", size=15)  \n",
    "plt.ylabel(\"Accuracy\", size=15)  \n",
    "plt.plot(n_nei_rng, quality)  \n",
    "plt.xticks(n_nei_rng) \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество на 1 соседе - самое лучшее. Но это и понятно - ближайшим соседом элемента из обучающей выборки будет сам объект. Мы просто **запомнили** все объекты.\n",
    "\n",
    "Если теперь мы попробуем взять какой-то новый образец опухоли и классифицировать его - у нас скорее всего ничего не получится. В таких случаях мы говорим, что наша модель не умеет обобщать (*generalization*).\n",
    "\n",
    "Для того, чтобы знать заранее обобщает ли наша модель или нет, мы можем разбить все имеющиеся у нас данныe на 2 части. Но одной части мы будем обучать классификатор (*train set*), а на другой тестировать насколько хорошо он работает (*test set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data to train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(x_train)  \n",
    "x_train_norm = scaler.transform(x_train)  # scaling data\n",
    "x_test_norm = scaler.transform(x_test)  # scaling data\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  \n",
    "train_quality = np.zeros(n_nei_rng.shape[0])  # quality on train data\n",
    "test_quality = np.zeros(n_nei_rng.shape[0])  # quality on test data\n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=n_nei_rng[ind])  \n",
    "    knn.fit(x_train_norm, y_train)  \n",
    "    \n",
    "    # accuracy on train data\n",
    "    trq = accuracy_score(y_pred=knn.predict(x_train_norm), y_true=y_train)  \n",
    "    train_quality[ind] = trq  \n",
    "\n",
    "    # accuracy on test data\n",
    "    teq = accuracy_score(y_pred=knn.predict(x_test_norm), y_true=y_test)  \n",
    "    test_quality[ind] = teq  \n",
    "\n",
    "# accuracy plot  on train and test data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот, теперь мы видим, что 1 сосед был \"ложной тревогой\". Такие случаи мы называем *переобучением*. Чтобы действительно предсказывать что-то полезное, нам надо выбирать число соседей, начиная минимум с 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-валидация\n",
    "\n",
    "Каждая модель имеет как ряд **параметров**, которые она меняет в процессе обучения (например, веса модели), так и ряд **гиперпараметров**, которые влияют на то, каким способом модель меняет параметры в процессе обучения. \n",
    "\n",
    "В случае kNN параметры, строго говоря, отсутствует - модель просто запоминает объекты обучающей выборки. Особо упорные могут считать их параметрами. \n",
    "\n",
    "А вот гиперпараметры есть, даже несколько групп. Какие? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Число соседей \n",
    "2. Функция, которой считаем расстояние между объектами (L2=eucledian, L1=manhattan)\n",
    "3. Веса, с которыми складываем метки ближайших соседей\n",
    "4. Признаки! (но об этом с вами поговорим позже)\n",
    "5. Сама модель - мы могли выбрать не kNN, а нагуглить что-нибудь другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим еще раз на график, который нарисовали на прошлом шаге. Какое число соседей считать оптимальным? Метрика явно скачет? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Не понятно, насколько результат зависит от того, как нам повезло или не повезло с разбиением данных на обучение и тест. Может оказаться так, что для конкретного разбиения хорошо выбрать k=5, а для другого - k=7. \n",
    "\n",
    "Кроме того, опять же - фактически, мы сами выступаем в роли модели, которая учит гиперпараметры (а не параметры) под видимую ей выборку. \n",
    "\n",
    "Представим себе, что у нас есть 10000 моделей, полученных подкручиванием разных гиперпараметров (в том числе, выбором просто разного типа модели). Представим, что все эти модели не работают. Вообще. Представим так же, что каждая модель угадывает класс в задаче разделения на два класса с вероятностью 0.5 (и будем считать, что классы у нас сбалансированны - то есть 50% одного класса и 50% другого). \n",
    "Опять же, понятно, что классификация такими моделями ничем не лучше подбрасывания монетки. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_model(y_real):\n",
    "    # array with values True/False, for random choice by mask \n",
    "    guessed = np.random.choice(\n",
    "        [True,False],  \n",
    "        size=y_real.shape[0],\n",
    "        replace=True,\n",
    "    )\n",
    "    y_pred = np.zeros_like(y_real)  # zeros array, shape y_real\n",
    "    \n",
    "    # with mask 'guessed' assign values \n",
    "    y_pred[guessed] = y_real[guessed]  \n",
    "    y_pred[~guessed] = 1 - y_real[~guessed]  \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_num = 10000  # num of experiments\n",
    "best_quality = 0.5  # quality threshold for accuracy\n",
    "\n",
    "# array with random values in range 0, 1\n",
    "y_real = np.random.choice(\n",
    "    [0, 1], size=250, replace=True\n",
    ")  \n",
    "\n",
    "for i in range(models_num):  # for all expirements\n",
    "    y_pred = guess_model(y_real)  # predicted values\n",
    "    q = accuracy_score(y_pred=y_pred, y_true=y_real)  # accuracy\n",
    "    if q > best_quality:  \n",
    "        best_quality = q   \n",
    "print(f\"Best result {best_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы перебором всех возможных моделей вполне можем получить для абсолютно бесполезной модели приемлемое качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Получается, что если подбирать гиперпараметры модели на *train set*, то:\n",
    "1. Можно переобучитьcя, просто на более \"высоком\" уровне. Особенно если гиперпараметров у модели много и все они разнообразны\n",
    "2. Нельзя быть уверенным, что выбор параметров не зависит от разбиения на обучение и тест "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы:\n",
    "\n",
    "1. Подбираем гиперпараметры моделей на отдельном датасете, называемым валидационным. Получаем мы его разбиением обучающего датасета на собственно обучающий и валидационный \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/split_dataset_for_train_val_test.png\" width=\"700\">\n",
    "\n",
    "2. Чаще всего делаем несколько таких разбиений по какой-то схеме, чтобы получить уверенность оценок качества для моделей с разными гиперпараметрами - **кросс-валидация**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/cross_validation_on_train_data.png\" width=\"500\">\n",
    "\n",
    "Часто применяется следующий подход, называемый [K-Fold кросс-валидацией](https://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "\n",
    "Берется тренировочная часть датасета, разбивается на части - блоки. Дальше мы будем использовать для проверки первую часть (Fold 1), а на остальных учиться. И так последовательно для всех частей. В результате у нас будут информация о точности для разных фрагментов данных и уже на основании этого можно понять, насколько значение этого параметра, который мы проверяем, зависит или не зависит от данных. То есть если у нас от разбиения точность при одном и том же К меняться не будет, значит мы подобрали правильное К. Если она будет сильно меняться в зависимости от того, на каком куске данных мы проводим тестирование, значит, надо попробовать другое К и если ни при каком не получилось - то это такие данные.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем инструмент `GridSearchCV` для подбора оптимальных параметров модели.\n",
    "\n",
    "`GridSearchCV` автоматически перебирает все возможные комбинации из переданного ему  множества возможного параметров, для каждой такой возможной комбинации параметров проводит описанную выше процедуру K-Fold кросс-валидации и запоминает модель с лучшей комбинацией параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from IPython.display import clear_output\n",
    "\"\"\"\n",
    "Parameters for GridSearchCV:\n",
    "estimator — model\n",
    "cv — num of fold to cross-validation splitting \n",
    "param_grid — parameters names\n",
    "scoring — metrics \n",
    "n_jobs - number of jobs to run in parallel, -1 means using all processors.\n",
    "\"\"\"\n",
    "model = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    cv=KFold(3, shuffle=True, random_state=42),\n",
    "    param_grid={\n",
    "        \"n_neighbors\": np.arange(1, 31),\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs =- 1,\n",
    ")\n",
    "model.fit(x_train_norm, y_train)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем лучшие гиперпараметры для модели, которые подобрали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metric:\", model.best_params_[\"metric\"])\n",
    "print(\"Num neighbors:\", model.best_params_[\"n_neighbors\"])\n",
    "print(\"Weigths:\", model.best_params_[\"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Объект GridSearchCV можно использовать как обычную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "y_pred = model.predict(x_test_norm)\n",
    "print(f\"Percent correct predictions {np.round(accuracy_score(y_pred=y_pred, y_true=y_test) * 100, 2)} %\")\n",
    "print(f\"Percent correct predictions(balanced classes) {np.round(balanced_accuracy_score(y_pred=y_pred, y_true=y_test) * 100, 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем извлечь дополнительные данные о кроссвалидации и по ключу обратиться к результатам всех моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем для примера mean_test_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(model.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"mean_test_score\", size=20)\n",
    "plt.xlabel(\"Num of experiment\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим, например, при фиксированных остальных параметрах (равных лучшим параметрам), качество модели на валидации в зависимости от числа соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_means = []\n",
    "selected_std = []\n",
    "n_nei = []\n",
    "for ind, params in enumerate(model.cv_results_[\"params\"]):\n",
    "    if (\n",
    "        params[\"metric\"] == model.best_params_[\"metric\"]\n",
    "        and params[\"weights\"] == model.best_params_[\"weights\"]\n",
    "    ):\n",
    "        n_nei.append(params[\"n_neighbors\"])\n",
    "        selected_means.append(model.cv_results_[\"mean_test_score\"][ind])\n",
    "        selected_std.append(model.cv_results_[\"std_test_score\"][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим error bar, для сравнения разброса ошибки при разном количестве соседей Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"KNN CV, {params['metric']}, {params['weights']}\", size=20)\n",
    "plt.errorbar(n_nei, selected_means, yerr=selected_std, linestyle=\"None\", fmt=\"-o\")\n",
    "plt.xticks(n_nei)\n",
    "plt.ylabel(\"Mean_test_score\", size=15)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на самом деле большой разницы в числе соседей и нет. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Можно ли делать только кросс-валидацию (без теста)?**\n",
    "\n",
    "\n",
    "Нет, нельзя. Кросс-валидация не до конца спасает от подгона параметров модели под выборку, на которой она проводится. Оценка конечного качества модели должно производиться на отложенной тестовой выборке. Если у вас очень мало данных, можно рассмотреть [вложенную кросс-валидацию](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/), Речь об этом пойдет позже, в последующих лекциях. Но даже в этом случае придется анализировать поведение модели, чтобы показать, что она учит что-то разумное. Кстати, вложенную кросс-валидацию можно использовать, чтобы просто получить более устойчивую оценку поведения модели на тесте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/use_knn_to_comare_imgs_to_find_similar.png\" width=\"800\">\n",
    "\n",
    "Давайте подумаем, как избавиться от проблемы со скоростью K-Nearest Neighbors. Реализуя метод ближайшего соседа, мы сравнивали одно изображение со всеми, при этом мы предполагали, что изображения из одного класса будут чем-то похожи друг на друга, и поэтому разница будет меньше. Метод как-то работал. \n",
    "\n",
    "Чтобы ускорить этот процесс, мы можем взять весь блок изображений (справа), который относится, например, к машинам и усредним. Таким образом, получим некоторый шаблон для класса “автомобиль”. Скорее всего, работать такой подход будет не слишком здорово, но зато вместо тысяч изображений (в данном случае 50000) появится одно. Возможно, это поможет сильно сэкономить время."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к сравнению с шаблоном\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/one_template_for_each_class_for_classification.png\" width=\"450\">\n",
    "\n",
    "Идея в следующем: вместо того, чтобы сравнивать каждое изображение со всеми остальными по очереди, будем сравнивать изображения с шаблоном для каждого класса. Их будет всего 10 для данного датасета. Этот подход позволит радикально увеличить скорость. \n",
    "\n",
    "Проверим, что получится из этой идеи. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cкачиваем CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "\n",
    "dataset_train = CIFAR10(\"content\", train=True, download=True)\n",
    "dataset_test = CIFAR10(\"content\", train=False, download=True)\n",
    "\n",
    "x_train = dataset_train.data\n",
    "y_train = np.array(dataset_train.targets)\n",
    "\n",
    "x_test = dataset_test.data\n",
    "y_test = np.array(dataset_test.targets)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_train shape : {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cоздадим шаблоны посчитав среднее значение пикселя по всем изображениям одного класса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = []  \n",
    "\n",
    "for i in range(len(dataset_train.classes)):\n",
    "    imgs = x_train[y_train == i] # select images by class mask\n",
    "    mn = np.mean(imgs, 0)  \n",
    "    templates.append(mn.astype(int)) # convert to int for display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем полученные шаблоны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (20.0, 2.0))\n",
    "\n",
    "def show_templates(templates, labels):\n",
    "    for i, template in enumerate(templates):\n",
    "        plt.subplot(1, len(labels), i + 1)\n",
    "        plt.title(labels[i])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(template)\n",
    "\n",
    "show_templates(templates, dataset_train.classes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть 10 шаблонов, с которыми мы будем сравнивать изображение и на основании этого делать предсказание. На этих шаблонах можно увидеть очертания объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем классификатор, который будет сравнивать изображение с шаблоном, который генерируется во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateBasedClassifier():\n",
    "    def __init__(self, labels):\n",
    "        self.templates = []          \n",
    "        self.labels = labels\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.templates = []\n",
    "        for label_num in range(len(self.labels)):  \n",
    "          imgs = x_train[y_train == label_num] # select images by class mask\n",
    "          mn = np.mean(imgs, 0 )  \n",
    "          self.templates.append(mn)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute distance score\n",
    "        distances = np.sum(np.abs(self.templates - x), axis=(1 ,2, 3))  \n",
    "        return np.argmin(distances)  # return minimum score index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим сравнение на конкретном примере с кошкой. Построим изображения картинки, шаблона, а затем посчитаем расстояние L1 между ними. Чем желтее цвет - тем больше изображение похоже на шаблон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3, figsize=(10, 5))\n",
    "\n",
    "cat_class_num = 3\n",
    "imgs = x_train[y_train == cat_class_num] # images class 3(cat)\n",
    "\n",
    "img = imgs[1] # some image with cat\n",
    "template = templates[cat_class_num] \n",
    "residual = np.mean(np.abs(img-template), axis = -1) \n",
    "\n",
    "# Code for display\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(template)\n",
    "r_plot = ax[2].imshow(residual, cmap='inferno_r')\n",
    "\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(r_plot, cax=cax, orientation='vertical', label='L1')\n",
    "\n",
    "ax[0].set_title('Image')\n",
    "ax[1].set_title('Template')\n",
    "ax[2].set_title('D = Image - Template')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemplateBasedClassifier(dataset_train.classes)  \n",
    "model.fit(x_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказание и замерим время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def validate(model, x_test, y_test):\n",
    "    y_pred = []  # num of correct predictions\n",
    "    for i, img in enumerate(x_test):  # for all images\n",
    "        index = model.forward(img)  # predict class\n",
    "        y_pred.append(index)\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "start = time.perf_counter()  \n",
    "accuracy= validate(model, x_test, y_test)  \n",
    "tm = time.perf_counter() - start \n",
    "print(\n",
    "    \"\\nAccuracy {:.2f} Train {:d} /test {:d} in {:.1f} sec. speed {:.2f} samples per second.\".format(\n",
    "        accuracy, len(x_train), len(x_test), tm, len(x_test) / tm\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если бы мы обучали KNN на таком количестве данных, это заняло бы у нас четверть часа.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Переход к весам\n",
    "----\n",
    "Умножение вместо вычитания - для чего?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/compute_scalar_product_with_compare_with_template.png\" width=\"450\">\n",
    "\n",
    "В сравнении с шаблонами, мы вычитали нашу картинку из шаблона и таким образом смотрели, насколько они похожи (L1). Теперь давайте сделаем следующий, пока ничем не обоснованный, ход: оставим всё, как было, но заменим вычитание умножением. Логика в том, что не все пиксели одинаково важны. Вероятно, если на изображении совпадут какие-то пиксели, которые отвечают, например, за глаза кошки, это будет намного важнее, чем фон, который может быть точно таким же у собаки. Здесь будут какие-то важные особенности, которым можно придать больший вес.\n",
    "\n",
    "Если мы распишем в виде формул наложение шаблона на картинку таким образом с умножением, то получается, что мы скалярно перемножаем два вектора. Подробнее про [скалярное произведение](https://ru.wikipedia.org/wiki/Скалярное_произведение) векторов. Для того, чтобы получить вектор из изображения размерностью 32х32х3, достаточно \"выпрямить\" его, получим вектор размерностью 1х3072.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опять рассмотрим сравнение на конкретном примере с кошкой. Построим изображения картинки, шаблона, а затем посчитаем расстояние между ними, используя наш новый метод перемножения. Чем желтее цвет - тем больше изображение похоже на шаблон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3, figsize=(10, 5))\n",
    "\n",
    "residual = np.mean(np.abs(img * template), -1)\n",
    "\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(template)\n",
    "r_plot = ax[2].imshow(residual, cmap='inferno_r')\n",
    "\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(r_plot, cax=cax, orientation='vertical', label='Distance')\n",
    "\n",
    "ax[0].set_title('Image')\n",
    "ax[1].set_title('Template')\n",
    "ax[2].set_title('D = Image * Template')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Математическая запись"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/img_to_vector_to_compute_scalar_product.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "Обозначим входное изображение как $x$, а шаблон для первого из классов как $w_0$.\n",
    "\n",
    "Элементы пронумеруем подряд 1,2,3 … $n$. То есть развернем матрицу пикселей изображения в вектор. \n",
    "\n",
    "Тогда результат сравнения изображения с этим шаблоном будет вычисляться по формуле: $x[0]*w0[0] + x[1]*w0[1] + … x[n-1]*w0[n-1]$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/scalar_product_ways_to_use.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Эта простая модель лежит в основе практически всех сложных, которые мы будем рассматривать дальше. Внутри мы будем также пользоваться скалярным произведением.\n",
    "\n",
    "В дальнейшем мы будем проходить сверточные сети, они работают очень похоже:\n",
    "мы тоже накладываем шаблон на некоторую матрицу и перемножаем элементы, затем складываем. Единственное отличие – обычно ядро свертки меньше, чем размер самого изображения. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (метод опорных векторов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Отличное видео про SVM от Stat Quest которое все объясняет](https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим одномерный пример. У нас есть данные по массе мышей. Часть из них определена как нормальные, а часть как мыши с ожирением. Чтобы их отделить друг от друга, нам достаточно одного критерия. Мы можем посмотреть на график, и визуально определить предельную массу, после которой мышки будут жирненькими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    x = np.hstack([np.random.uniform(14, 21, total_len // 2), \n",
    "                   np.random.uniform(24, 33, total_len // 2)])\n",
    "    y = np.hstack([np.zeros(total_len // 2), \n",
    "                   np.ones(total_len // 2)])\n",
    "    return x, y\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50, threshold=None, margin=None):\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s)\n",
    "    if threshold:\n",
    "        ax.axvline(threshold, color='red', ls='dashed')\n",
    "    if margin:\n",
    "        for line in margin:\n",
    "            ax.axvline(line, color='pink', ls='dashed')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_data(total_len=total_len)\n",
    "ax = plot_data(x, y, threshold=21.5, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пользуясь нашим простым критерием, попробуем классифицировать каких-то новых мышей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.random.uniform(14, 30, 5)\n",
    "\n",
    "def classify(x, threshold=21.5):\n",
    "    y = np.zeros_like(x)\n",
    "    y[x > threshold] = 1\n",
    "    return y\n",
    "\n",
    "total_len = 40\n",
    "threshold = 21.5\n",
    "x, y = generate_data(total_len=total_len)\n",
    "ax = plot_data(x, y, threshold=threshold, total_len=total_len)\n",
    "ax = plot_data(x_test, classify(x_test, threshold), total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если наши мыши находятся тут?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([21.45, 22.5])\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)\n",
    "ax = plot_data(x_test, classify(x_test), threshold=21.5, total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения нашего классификатора - все четко. Больше порогового значения - значит перевес, меньше - значит нормальные. Но с точки зрения здравого смысла, логичнее было бы классифицировать обоих мышей как нормальных, так как они значительно ближе к нормальным, чем к ожиревшим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вооружившись этим новым знанием, попробуем классифицировать наших отъевшихся мышек по-умному. Возьмем крайние точки в каждом кластере. И в качестве порогового значения будем использовать среднее между ними"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_data(total_len=total_len)\n",
    "normal_limit = x[y==0].max() # extreme point for 'normal'\n",
    "obese_limit = x[y==1].min() # extreme point for 'obese'\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit]) # separated with mean value \n",
    "\n",
    "x_test = np.array([21.5, 23])\n",
    "ax = plot_data(x, y, total_len=total_len, threshold=threshold, margin=[normal_limit, obese_limit])\n",
    "ax = plot_data(x_test, classify(x_test, threshold=threshold), total_len=total_len, s=300, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать, насколько наша мышь близка к тому, чтобы оказаться в другом классе. Такое расстояние называется **margin**. И оно считается как $\\mathrm{margin} = |\\mathrm{threshold} - \\mathrm{observation}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = np.abs(x_test - threshold)\n",
    "print(margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если мы посчитаем margins для наших крайних точек `normal_limit` и `obese_limit`, мы найдем самое большое возможное значение margin для нашего классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_0 = np.abs(normal_limit - threshold)\n",
    "margin_1 = np.abs(obese_limit - threshold)\n",
    "print(margin_0, margin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой классификатор, мы называем **Maximum Margin Classifier**. Он хорошо работает в случае, когда все данные размечены аккуратно. Теперь рассмотрим более реалистичный пример, где что-то пошло не так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_data(total_len=40):\n",
    "    x = np.hstack([np.random.uniform(14, 21, total_len // 2), np.random.uniform(24, 33, total_len // 2)])\n",
    "    y = np.hstack([np.zeros(total_len // 2), np.ones(total_len // 2)])\n",
    "    indx = np.where(x == x[y==1].min())[0]\n",
    "    y[indx] = 0\n",
    "    s = np.ones_like(x) * 50\n",
    "    s[indx] = 300\n",
    "    return x, y, s\n",
    "\n",
    "total_len = 40\n",
    "x, y, s = generate_realistic_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len, s=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае, наш **Maximum Margin Classifier** работать не будет. Исходя из этого, мы можем прийти к выводу, что наш классификатор очень чувствителен к выбросам. Давайте подумаем можно ли это как-то исправить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы можем разрешить нашему классификатору ошибаться. Если мы будем использовать в качестве порогового значения не самое крайнее, а следующее за ним - мы промажем в классификации конкретно этой странной точки. Но в целом будем правы. Margin определенный таким образом, называется **Soft margin**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "x, y, s = generate_realistic_data(total_len=total_len)\n",
    "\n",
    "normal_limit = np.sort(x[y==0])[-2]\n",
    "obese_limit = np.sort(x[y==1])[1]\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit])\n",
    "\n",
    "x_test = np.array([20.5, 25])\n",
    "ax = plot_data(x, y, total_len=total_len, threshold=threshold)\n",
    "ax = plot_data(x_test, classify(x_test, threshold=threshold), total_len=total_len, \n",
    "               s=300, threshold=threshold, margin=[normal_limit, obese_limit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но почему мы решили взять именно следующее значение? Почему не через 2? Откуда мы знаем, что это лучший из возможных вариантов? А ни откуда не знаем. Чтобы узнать какой из margins лучше, нам стоит численно это проверить и посчитать, сколько раз мы ошибемся, если возьмем в качестве порогового значения между каждой парой точек. Для этого мы вновь воспользуемся *кросс-валидацией*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "x, y, s = generate_realistic_data(total_len=total_len)\n",
    "\n",
    "indxs = []\n",
    "accuracies = []\n",
    "thresholds = []\n",
    "\n",
    "x_test, y_test = generate_data(total_len=total_len)\n",
    "\n",
    "for i in range(total_len // 2):\n",
    "    for j in range(total_len // 2 - 1):\n",
    "        normal_limit = np.sort(x[y==0])[-i]\n",
    "        obese_limit = np.sort(x[y==1])[j]\n",
    "\n",
    "        threshold = np.mean([normal_limit, obese_limit])\n",
    "        \n",
    "        y_pred = classify(x_test, threshold=threshold)\n",
    "        accuracy = np.mean(y_test == y_pred)\n",
    "        indxs.append((-i, j))\n",
    "        accuracies.append(accuracy)\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "print(f'Accuracy = {np.max(accuracies) * 100}%')\n",
    "best_index = indxs[np.argmax(accuracies)][1]\n",
    "print(f'Best indexes', best_index)\n",
    "best_treshold = thresholds[np.argmax(accuracies)]\n",
    "print('Best treshhold value %.2f'% best_treshold)\n",
    "\n",
    "ax = plot_data(x, y, total_len=total_len, threshold=best_treshold)\n",
    "ax = plot_data(x_test, y_test, \n",
    "               total_len=total_len, \n",
    "               s=200, \n",
    "               threshold=best_treshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда для классификатора используется **Soft Margin** - такой классификатор называют **Soft Margin Classifier** или по-другому - **Support Vector Classifier**. По сути это уже SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример, где мы измерили не только вес мышей, но и их длину от хвоста до носа. Мы можем вновь применить наш метод Support Vector Classifier, и теперь классы разделяет не одно пороговое значение (по сути, точка), а линия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import svm\n",
    "\n",
    "def generate_2d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42)\n",
    "    x[:, 0] += 10 \n",
    "    x[:, 1] += 20 \n",
    "    return x, y\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50, threshold=21.5):\n",
    "    ax = sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g', ylabel='Length, cm');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_2d_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)\n",
    "\n",
    "# Code for illustration, later we will understand how it works\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(x, y)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы добавим еще одно измерение - возраст, мы обнаружим, что наши данные стали трехмерными, а разделяет их теперь не линия, а плоскость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_data(total_len=40):\n",
    "    x, y = make_blobs(n_samples=total_len, centers=2, random_state=42, n_features=3)\n",
    "    x[:,0] += 10 \n",
    "    x[:,1] += 20 \n",
    "    x[:,2] += 10\n",
    "    return x, y\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50, threshold=21.5):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(xs=x[:, 0], ys=x[:, 1], zs=x[:, 2], c=y, s=s, cmap='Set1')\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    ax.plot_surface(XX, YY, XX * YY * 0.2, alpha=0.2)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g', ylabel='Length, cm', zlabel='Age, days');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_3d_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если бы у нас было 4 измерения и больше (например: вес, длинна, возраст, кровяное давление), то многомерная плоскость, которая бы разделяла наши классы, называлась бы **гиперплоскость** (рисовать мы ее, конечно же, не будем). Чисто технически, и точка, и линия - тоже гиперплоскости. Но все же гиперплоскостью принято называть то, что нельзя нарисовать на бумаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные не всегда разделяются так хорошо как в случае нашего мышиного датасета. Например, рассмотрим следующее: у нас есть данные по дозировке лекарства и 2 класса - пациенты, которые поправились, и те, которым лучше не стало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patients_data(total_len=40):\n",
    "    x = np.random.uniform(0, 50, total_len)\n",
    "    y = np.zeros_like(x)\n",
    "    y[(x > 15) & (x < 35)] = 1\n",
    "    return x, y\n",
    "\n",
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=x, y=np.zeros(len(x)), hue=y, s=s)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Recover', 'Sick'])\n",
    "    ax.set(xlabel='dose, mg');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "x, y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(x, y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно мы не можем найти такое пороговое значение, которое будет разделять наши классы на больных и здоровых, а, следовательно, и Support Vector Classifier работать тоже не будет.  Для начала, давайте преобразуем наши данные таким образом, что бы они стали 2-х мерными. В качестве значений по оси Y будем использовать дозу возведенную в квадрат (**доза**$^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=x[0, :], y=x[1, :], hue=y, s=s)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Recover', 'Sick'])\n",
    "    ax.set(xlabel='Dose, mg');\n",
    "    ax.set(ylabel='Dose$^2$');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "x_1, y = generate_patients_data(total_len=total_len)\n",
    "x_2 = x_1 ** 2\n",
    "x = np.vstack([x_1, x_2])\n",
    "\n",
    "plot_data(x, y, total_len=40, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем вновь использовать Support Vector Classifier для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x, y, total_len=40, s=50)\n",
    "\n",
    "x_arr = np.linspace(0, 50, 50)\n",
    "xs = [x[0, :][y==1].min(), x[0, :][y==1].max()]\n",
    "ys = [x[1, :][y==1].min(), x[1, :][y==1].max()]\n",
    "\n",
    "# Calculate the coefficients.\n",
    "coefficients = np.polyfit(xs, ys, 1)\n",
    "\n",
    "# Let's compute the values of the line...\n",
    "polynomial = np.poly1d(coefficients)\n",
    "y_axis = polynomial(x_arr)\n",
    "\n",
    "# ...and plot the points and the line\n",
    "plt.plot(x_arr, y_axis, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут возникает резонный вопрос - почему мы решили возвести в квадрат? Почему не в куб? Или наоборот не извлечь корень? Как нам решить какое преобразование использовать?\n",
    "\n",
    "И у нас есть **вторая проблема** - а если перейти надо в пространство очень большой размерности? В этом случае наши данные очень сильно увеличатся в размере.\n",
    "\n",
    "Комбинация двух проблем дает нам **много радости** - надо перебирать большое число возможных пространств большей размерности\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Однако основная фишка Support Vector Machine состоит в том, что внутри он работает на скалярных произведениях. И можно эти скалярные произведения считать, **не переходя в пространство большей размерности**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого SVM использует **Kernel Function**. \n",
    "\n",
    "Kernel Function может, например, быть полиномом (**Polynomial Kernel Function**), который имеет параметр $d$ - сколько размерностей выбрать. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L02/out/svm_kernel_function.png\" width=\"700\">\n",
    "\n",
    "Примеры ядер :\n",
    "\n",
    "* $k(x_i, x_j) = (<x_i, x_j> + c)^d, с, d \\in \\mathbb{R}$ - полиномиальное ядро, считает расстояние между объектами в пространстве размерности d\n",
    "\n",
    "* $k(x_i, x_j) = \\frac{1}{z} e^{-\\frac{h(x_i, x_j)^2}{h}}$ - радиальная базисная функция RBF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, в случае SVM можно легко перебрать много таких пространств на кроссвалидации и выбрать более удобное. \n",
    "\n",
    "Более того, SVM может проверять пространства признаков бесконечного размера, если для такого пространства существует kernel function. Для решения практических задач иногда такие бесконечномерные пространства признаков могут оказаться удобными. Широко применяемое на практике RFB ядро как раз соответствует такому случаю бесконечномерного пространства признаков.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько другую задачу. Пусть мы хотим построить модель предсказания для величины успеваемости студента и хотим это делать на основе информации о величине потраченного им на изучения материала количества времени в часах. Это пример простейшей задачи линейной регрессии.\n",
    "\n",
    "**Регрессией** (регрессионным анализом) называется набор статистических методов, использующихся для определения характера связи между одной зависимой переменной (традиционно обозначаемой $Y$ и называемой также \"откликом\", \"результатом\" или \"лейблом\") и одной или рядом других независимых переменных (традиционно обозначаемых $X$ и называемых также \"предикторами\", \"ковариатами\" или \"признаками\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет. Мы должны увидеть, что он содержит два числовых признака - часы и результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"https://edunet.kea.su/repo/EduNet-web_dependencies/L02/student_scores.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости одного от другого, а так же отобразим распределения каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=dataset, x=\"Scores\", y=\"Hours\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.iloc[:, :-1].values # column Hours\n",
    "y = dataset.iloc[:, 1].values # column Score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель для линейной регрессии. Чтобы не писать с нуля, воспользуемся готовой моделью из библиотеки `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.linspace(min(x_train), max(x_train), 100)  # 100 dots at min to max\n",
    "y_pred = regressor.predict(x_points)  \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_train, y_train, \"o\", label=\"Scores\")\n",
    "plt.plot(x_points, y_pred, label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_ ))\n",
    "plt.title(\"Hours vs Percentage\", size=15)\n",
    "plt.xlabel(\"Hours Studied\", size=15)\n",
    "plt.ylabel(\"Percentage Score\", size=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем предсказание для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)  \n",
    "\n",
    "x_points = np.linspace(min(x_test), max(x_test), 100)  \n",
    "y_pred = regressor.predict(x_points) \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_test, y_test, \"o\", label=\"Scores\")\n",
    "plt.plot(x_points, y_pred, label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_ ))\n",
    "plt.title(\"Hours vs Percentage\", size=15)\n",
    "plt.xlabel(\"Hours Studied\", size=15)\n",
    "plt.ylabel(\"Percentage Score\", size=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неплохо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики для наших значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "print(\"Mean Absolute Error: %9.2f\" % metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(\"Mean Squared Error: %10.2f\" % metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: %5.2f\" % np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Геометрическая интерпретация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрались с тем, что такое регрессия и с чем ее едят, вернемся к нашим картинкам. Как можно применить регрессию для классификации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим у нас есть только 2 класса. Как можно использовать регрессию для того, чтобы определить относится ли изображение к классу 0 или к классу 1? В упрощенном варианте, задача будет состоять в том, чтобы провести разделяющую плоскость (прямую) между 2-мя классами. Например, мы можем провести прямую через 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/regression_for_classification_imgs.png\" width=\"270\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другую ситуацию, в этом случае, мы не можем просто провести прямую через 0. Но можем отступить от 0 на какое-то расстояние и провести ее там. Вспомним, что уравнение прямой это $y=wx+b$, где $b$ - это смещение (*bias*). Соответственно если b != 0, то прямая через 0 проходить не будет, а будет проходить через значение b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/regression_for_classification_add_bias.png\" width=\"270\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Classification Loss Visualization\n",
    "](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)\n",
    "\n",
    "\n",
    "Если у нас есть несколько классов (несколько шаблонов), мы можем для каждого из них посчитать уравнение $y_{i} = w_{i}x_{i}+b_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/regression_for_classification_add_bias_add_multiclasses.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На картинке нас интересуют 3 класса. Соответственно, мы можем записать систему линейных уравнений:\n",
    "\n",
    "\\begin{aligned}\n",
    "y_{0} = w_{0}x_{0} + b_{0} \\\\\n",
    "y_{1} = w_{1}x_{1} + b_{1} \\\\\n",
    "y_{2} = w_{2}x_{2} + b_{2} \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы их можем собрать в матрицу, тогда получится следующее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/scalar_product_add_bias.png\" width=\"750\">\n",
    "\n",
    "У нас есть матрица коэффициентов, которые мы каким-то образом подобрали, пока ещё не понятно как. Есть вектор $x$, соответствующий изображению. \n",
    "\n",
    "Мы умножаем вектор на матрицу, получаем нашу гиперплоскость для четырехмерного пространства в данном случае. Чтобы оно не лежало в 0, мы должны добавить смещение. И мы можем сделать это после, но можно взять и этот вектор смещения (вектор **b**) просто приписать к матрице **W**.\n",
    "\n",
    "Что будет выходом такой конструкции? Мы умножили матрицу весов на наш вектор, соответствующий изображению, получили некоторый отклик. По этому отклику мы так же, как и при реализации метода ближайшего соседа можем судить: если он больше остальных, то мы предполагаем, что это кошка.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([56, 231, 24, 2])\n",
    "w_cat = np.array([0.2, -0.5, 0.1, 2.0])\n",
    "print(\"Image \", img)\n",
    "print(\"Weights \", w_cat)\n",
    "print(\"img * w_cat \", img * w_cat)\n",
    "print(\"sum \", (img * w_cat).sum())\n",
    "print(\"Add bias \", (img * w_cat).sum() + 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/img_to_function_get_scores.png\" width=\"600\">\n",
    "\n",
    "\n",
    "Собирая все вместе, получаем какое-то компактное представление, что у нас есть некоторая функция, на вход которой мы подаем изображение, и у нее есть параметры (веса). Пока происходит просто умножение вектора на матрицу, в дальнейшем это может быть что-то более сложное, функция будет представлять какую-то более сложную модель. А на выходе (для классификатора) мы получаем числа, которые интерпретируют уверенность модели в том, что изображение принадлежит к определенному классу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/input_img_scalar_product_add_bias_get_scores.png\" width=\"450\">\n",
    "\n",
    "\n",
    "\n",
    "Соответственно, эти коэффициенты, которые являются весами модели, надо каким-то образом подбирать. Но прежде чем подбирать коэффициенты, давайте определимся со следующим: как мы будем понимать, что модель работает хорошо или плохо? Вывод модели представляет собой просто некоторый набор чисел. Но как эти числа следует правильно интерпретировать? Рассмотрим этот вопрос в следующем разделе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим как метод опорных векторов (SVM) работает на практике. Мы можем применить метод опорных векторов в том числе и к изображениям -- достаточно просто вытянуть изображение из тензора формата $(\\text{C}, \\text{H}, \\text{W})$ в $(\\text{C} \\cdot \\text{H} \\cdot \\text{W})$-мерный вектор. Применим SVM к нескольким изображениям из датасета CIFAR10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/model_predicted_scores_for_10_classes.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии с тем, что мы уже делали, мы можем сравнивать отклик на ключевой класс  (про который нам известно, что он на изображении, так как у нас есть метка этого класса) с остальными. Соответственно, мы подали изображение кошки и получили на выход вектор. Чем больше значение, тем больше вероятность того, что, по мнению модели, на изображении этот класс. Для кошки в данном случае это значение 2.9. Хорошо это или плохо? Нельзя сказать, пока мы не проанализировали остальную часть вектора. Если бы мы могли посмотреть на все значения в векторе, мы бы увидели, что есть значения больше, то есть в данном случае модель считает, что это собака, а не кошка, потому что для собаки значение максимально. \n",
    "\n",
    "На основании этого можно построить некоторую оценку. Давайте смотреть на разницу правильного класса с неправильными. В зависимости от того, насколько уверенность в кошке будет больше остальных, настолько будет наша модель хорошей.\n",
    "\n",
    "Но поскольку нам важна не работа модели на конкретном изображении, а важно оценить ее работу в целом, то эту операцию нужно проделать либо для всего датасета, либо для некоторой выборки, которую мы подаем на вход и подсчитываем средний показатель. Этот показатель (насколько хорошо работает модель), называется функцией потерь, или **loss функцией**. Называется она так, потому что она показывает не то, насколько хорошо работает модель, а то, насколько плохо. \n",
    "\n",
    "Дальше будет понятно, почему так удобнее (разница только в знаке). Как это посчитать для всего датасета?\n",
    "\n",
    "Мы каким-то образом считаем loss для конкретного изображения, потом усредняем по всем изображениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано: 3 учебных примера, 3 класса. При некотором W баллы f (a, W) = Wx равны:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/model_predicted_scores_for_3_classes.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь показывает, насколько хорош наш текущий классификатор.\n",
    "\n",
    "Дан датасет примеров:\n",
    "\n",
    "$\\begin{Bmatrix} (x_i,y_i)  \\end{Bmatrix}_{i=1}^N \t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где **$x_i$** изображение и **$y_i$** метка (число).\n",
    "\n",
    "Потери по набору данных — это среднее значение потерь для примеров:\n",
    "\n",
    "$ L = {1 \\over N}\\sum_iL_i(f(x_i,W),y_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим функцию потерь для одного примера $L_i(f(x_i,W),y_i)$:\n",
    "\n",
    "1. Вычислим вектор значений прогнозов классификатора $s = f(x_i, W)$.\n",
    "1. Для всех примеров рассмотрим разницу между оценкой на истинной категории и всеми оценками классификатора для неправильных категорий: $s_{y_i} - s_j$ для $j \\neq y_i$. \n",
    "1. Если получившаяся разница положительная и превышает некоторое пороговое значение («зазор»), которое мы установим равным $1$, то будем считать, что категория $j$ не мешает модели верно классифицировать входной объект, припишем категории $j$ нулевой вклад в $L_i(f(x_i,W),y_i)$.\n",
    "1. Если получившаяся разница не превосходит установленного нами единичного «зазора», то мы будем считать что ответ классификатора $s_j$ в категории $j$ мешает верной классификации входного объекта. В этом случае припишем для категории $j$ аддитивный вклад в $L_i(f(x_i,W),y_i)$ равный $s_j-s_{y_i}+1$.\n",
    "\n",
    "Описанную процедуру гораздо проще записать в виде формулы:\n",
    "\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}$\n",
    "\n",
    "$=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление функции потерь SVM \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Не самая мощная, но достаточно интуитивно понятная loss функция –  SVM loss. \n",
    " \n",
    "Логика такая: если у нас уверенность модели в правильном классе большая, то модель работает хорошо и loss для данного конкретного примера должен быть равен нулю. Если есть класс, в котором модель уверена больше, чем в правильном, то loss должен быть не равен нулю, а отображать какую-то разницу, поскольку модель сильно ошиблась. При этом есть ещё одно соображение: что будет, если на выходе у правильного и ошибочного класса будут примерно равные веса? То есть, например, у кошки было бы 3.2, а у машины не 5.2, а 3.1. В этом случае ошибки нет, но понятно, что при небольшом изменении в данных (просто шум) скорее всего она появится.\n",
    " \n",
    "\n",
    "\n",
    "То есть модель плохо отличает эти классы. Поэтому мы и вводили некоторый зазор, который должен быть между правильным и неправильным ответом. \n",
    "\n",
    "Посмотрим на изображение снизу. У нас есть два класса: фиолетовые треугольники и синие квадраты, разделенные зазором. Также можем увидеть желтые треугольники и квадраты - это ошибочно распознанные классы.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/svm_decision_boundary.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И тоже учитывать его в loss функции: то есть сравнивать его результат для правильного класса не с чистым выходом для другого, а добавить к нему некоторую дельту (в данном случае – 1(единица)). Смотрим: если разница больше 0, то модель работает хорошо и loss равно нулю. Если нет, то мы возвращаем эту разницу, и loss будет складываться из этих индивидуальных разниц.\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}$\n",
    "\n",
    "$=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "Ниже пример того, как считается loss.\n",
    "\n",
    "\n",
    "Считаем функцию потерь для 1-ого изображения:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/compute_loss_use_model_scores_for_1_example.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также считаем потери для 2-ого и 3-его изображения:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/compute_loss_use_model_scores_for_2_and_3_examples.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения losses получились следующие:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/summary_losses_for_3_examples.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем среднее значение loss для всего датасета:\n",
    "\n",
    "$ L = {1 \\over N}\\sum_{i=1}^N L_i$\n",
    "\n",
    "$L={2.9 + 0 + 12.9 \\over 3} = 5.27$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM loss\n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "\n",
    "Как записать это в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "file_exists = os.path.exists(\"/content/cifar-10-batches-py\")\n",
    "if file_exists == False:\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !tar -xzf cifar-10-python.tar.gz\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "x_train = np.zeros((0, 3072))\n",
    "y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    x_train = np.append(x_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    y_train = np.append(y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "x_test = np.array(test[b\"data\"])\n",
    "y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_eng = [\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(10, 3072) * 0.0001 # random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assa = np.maximum(0, W.dot(x_train[0]) - W.dot(x_train[0])[6] + 1)\n",
    "assa[6] = 0\n",
    "np.sum(assa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Li_svm(x, y, W):\n",
    "    y = int(y)  # to use like a index\n",
    "    scores = W.dot(x)  \n",
    "    margins = np.maximum(0, scores - scores[y] + 1)  # loss\n",
    "    margins[y] = 0  \n",
    "    return np.sum(margins)  \n",
    "\n",
    "\n",
    "L0 = Li_svm(x_train[0], y_train[0], W)\n",
    "print(\"Loss on first image\", L0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/losses_at_set_of_data.png\" width=\"900\">\n",
    "\n",
    "Собираем все вместе: считаем loss для всего датасета, усредняя ее.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как будет выглядеть график этой SVM-loss?\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/svm_plot.gif\" width=\"400\">\n",
    "\n",
    "\n",
    "Функция потерь для ситуации, когда зазор большой, будет равна 0. Она будет меняться только тогда, когда у нас есть ошибка. Причем будет меняться линейно.\n",
    "\n",
    "Как мы можем использовать эту loss функцию? Допустим, мы знаем, что у нас модель будет работать не очень хорошо (выдает большую ошибку). Можно попытаться найти минимум этой loss функции, то есть подобрать такое W, когда loss функция обратится в 0  (в данном случае), а в общем случае (если какое-то количество данных нельзя обратить в 0), мы можем попытаться найти ее минимум.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обновления весов методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/backpropagation_weight_optimization.png\" width=\"750\">\n",
    "\n",
    "Идея в следующем: у нас есть некоторая поверхность, на которой будет точка минимума функции. Мы должны обновлять веса таким образом, чтобы смещаться в сторону этой точки. Loss показывает, как возрастает функция потерь. Так как потери - это плохо, мы должны их минимизировать, то есть мы должны уменьшать веса в сторону, обратную росту этой функции.\n",
    "\n",
    "Если переходить на случай n-мерного (в данном случае трехмерного) пространства, здесь достаточно очевидна аналогия с поверхностью: у нас есть поверхность земли, которая описывается координатами $x, y, z$. Если мы движемся по этой поверхности, высота $(z)$ будет зависеть от $x, y$. \n",
    "\n",
    "$x, y$ - это координаты. Мы можем записать их как вектор. Наша функция будет работать с вектором координат и выдавать скаляр, третью координату. Если в качестве координат мы будем использовать веса нашей модели, а в качестве $z$ - loss, то аналогия станет полной. Наша задача сведется к тому, чтобы найти такой набор весов, при котором значение функции будет минимально. То есть мы окажемся в каком-то минимуме, где ошибка будет минимально возможна для этих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы предположить, где он может находиться, надо понимать, в какую сторону функция растет, а в какую - убывает. Для этого существует производная.\n",
    "\n",
    "Поскольку у нас функция от нескольких переменных, если мы будем брать от нее производную, у нас получится вектор частных производных, то есть градиент этой функции, который будет показывать, как она меняется в каждом направлении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент функции потерь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как известно из курса математического анализа, производная вещественной функции одной переменной характеризует мгновенную скорость изменения значений данной функции. Если в какой-то точке производная положительна, то в этой окрестности функция может возрастать. В противном случае, если производная отрицательна, функция может убывать. Аналогичный смысл несёт градиент для вещественной скалярной функции многих переменных.\n",
    "\n",
    "Возьмём гладкую скалярная функцию многих вещественных переменных (таковой, например, является функция потерь) и вычислим производные по каждой из её переменных, считая остальные переменные зафиксированными (так называемые частные производные). Запишем получившиеся частные производные в виде вектора-столбца.\n",
    "Каждый из компонентов такого вектора будет характеризовать мгновенную скорость изменения функции вдоль направления одной из переменных. Если же посмотреть на такой вектор в целом, то он будет направлен в сторону наибольшего роста нашей скалярной функции, а его длина будет характеризовать скорость такого роста.\n",
    "\n",
    "Например, если в качестве скалярной функции $\\varphi$ взять высоту поверхности земли над уровнем моря,то $\\text{grad} \\varphi$ в каждой точке земной поверхности будет направлен в сторону самого крутого подъема и будет своей длиной указывать крутизну склона.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так, если $\\varphi = \\varphi(x_1 \\dots x_n)$  — функция $n$ переменных, то её градиентом называется $n$-мерный вектор:\n",
    "$$\\left(\\frac{\\partial\\varphi}{\\partial x_1},\\dots,\\frac{\\partial\\varphi}{\\partial x_n}\\right)^T$$\n",
    "\n",
    "Нас будет интересовать градиент функции потерь $L$, взятый по весам модели $W$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$W$ - матрица(вектор) весов\n",
    "\n",
    "$L$ - функция потерь (скаляр)\n",
    "\n",
    "$\\partial W = W_2 - W_1$\n",
    "\n",
    "$\\partial L = L_2 - L_1$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W}=\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial W_1} \\\\\n",
    "\\frac{\\partial L}{\\partial W_2} \\\\\n",
    "... \\\\\n",
    "\\frac{\\partial L}{\\partial W_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Наша задача будет сводиться к тому, что мы будем искать градиент loss функции по весам, которые будут состоять из производной по каждому направлению. Поскольку у нас здесь числа, можно считать этот градиент численно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Численный расчет производной\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/gradient_descent_analytical_calculation.png\" width=\"650\">\n",
    "\n",
    "\n",
    "\n",
    "Посчитаем градиент приближенно, воспользовавшись определением (в формуле аргумент обозначен как $x$, у нас же аргументом будет $W$):\n",
    "На нулевом шаге у нас есть $W_0$ найдем $L_0 = Loss(f(W_0,x))$\n",
    "Прибавим к первому элементу $W_0$ небольшую величину  $h$ = 0.0001 и получим новую матрицу весов $W_1$ отличающуюся от $W_0$ на один единственный элемент.\n",
    "\n",
    "Найдем Loss от $\\frac {W_1}  {L_1} = Loss(f(W_1,x))$\n",
    "По определению производной $\\frac {dL}{W_0} = \\frac{( L_1 - L_0 )}   {h}$\n",
    "\n",
    "Повторяя этот процесс для каждого элемента из $W$, найдем вектор частных производных, то есть градиент $\\frac{dL}{dW}$.\n",
    "\n",
    "Плюсы:\n",
    "Это просто. \n",
    "\n",
    "Проблемы:\n",
    "\n",
    "1. Это очень долго, нам придется заново искать значение loss функции для каждого $W_i$.\n",
    "\n",
    "2. Это неточно, так как по определению приращение $h$ бесконечно мало, а мы используем конкретное пусть и небольшое число. И если мы сделаем его слишком маленьким, то столкнемся с ошибками связанными с округлением в памяти компьютера.\n",
    "\n",
    "\n",
    "Поэтому данный метод может использоваться как проверочный. \n",
    "А на практике вместо него используется **аналитический расчет градиента**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналитический расчет производной от функции потерь SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые производные \n",
    "\n",
    "$$x' = \\frac {\\delta x} {\\delta x} = 1$$ \n",
    "\n",
    "$$(x^2)' = \\frac {\\delta x^2} {\\delta x} = 2x$$\n",
    "\n",
    "$$(\\log x)'  = \\frac {\\delta \\log x} {\\delta x} = \\frac 1 x $$\n",
    "\n",
    "$$(e^x)'  = \\frac {\\delta e^x} {\\delta x} = e^x $$\n",
    "\n",
    "$$\\frac {\\delta cf(x)} {\\delta x}= c \\cdot \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$$\\frac {\\delta f(x) + c} {\\delta x}= \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$c$ - константа, не зависящая от $x$\n",
    "\n",
    "$$ \\frac {\\delta [f(x) + g(x)]} {\\delta x} = \\frac {\\delta f(x)} {\\delta x}  + \\frac {\\delta g(x)} {\\delta x} $$ \n",
    "\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta x} = 2x  $$\n",
    "так как $y$ по отношению к $x$ - константа и мы меняем только $x$\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta y} = 3y^2  $$\n",
    "так как $x$ по отношению к $y$ - константа и мы меняем только $y$\n",
    "\n",
    "$$(e^y)'  = \\frac {\\delta e^y} {\\delta x} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-rule\n",
    "\n",
    "Производная функции $f(g)$:\n",
    "\n",
    "$$\\frac {\\delta f} {\\delta g}$$\n",
    "\n",
    "Пусть $g$  на самом деле не просто переменная, а зависит от $h$. Тогда производная от $f$ по $g$ **не меняется**, а производная $f$ по $h$ запишется следующим образом:\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h))} {\\delta h} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h}$$\n",
    "\n",
    "Пусть теперь $h$ зависит от $x$. Все аналогично\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h(x)))} {\\delta x} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h} \\frac {\\delta h} {\\delta x}$$\n",
    "\n",
    "Так можно делать до бесконечности, находя производную сколь угодно сложной функции. И, что важно - мы можем считать градиенты частями - посчитать сначала $f$ по $g$, потом $g$ по $h$....\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x}$$\n",
    "\n",
    "$$h = x^2 + 5$$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac {\\delta log(h)} {\\delta h} \\frac {\\delta h} {\\delta x}$$ \n",
    "\n",
    "$$\\frac {\\delta log(h)} {\\delta h} = \\frac 1 h$$\n",
    "\n",
    "$$\\frac  {\\delta h} {\\delta x} = 2x $$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac 1 {x^2 + 5} \\cdot 2x = \\frac {2x} {x^2 + 5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть MSE-loss\n",
    "\n",
    "$$loss = (y - \\hat{y})^2 $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta (y - \\hat{y})^2 } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = 2(y-\\hat{y}) \\cdot -1 = 2 (\\hat{y} - y)$$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = 2 x \\cdot (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE-loss\n",
    "$$MSE = \\frac 1 N \\sum_i(y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "$y_i$ - константы\n",
    "$\\hat{y_i}$ - не являются функциями друг от друга \n",
    "$$\\hat{y} = wx_i + b $$\n",
    "\n",
    "$$\\frac {\\delta MSE} {\\delta w} = \\frac 1 N \\sum \\frac {\\delta (y_i - \\hat{y_i}) ^2} {\\delta \\hat{y_i}} \\frac {\\delta \\hat{y_i}} {\\delta w}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть MAE-Loss\n",
    "\n",
    "$$loss = |y - \\hat{y}| $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}}$$\n",
    "\n",
    "Строго говоря, у модуля не существует производной в 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i for i in range(-5, 6)]\n",
    "y = [abs(i) for i in range(-5, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label=\"y = |x|\")\n",
    "plt.title(\"y = |x|\", size=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы можем сказать, что в этой точке производная равна 0.\n",
    "Если аргумент модуля меньше 0, то производная будет -1.\n",
    "\n",
    "Если больше +1\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} = - sign(y - \\hat{y}) =  sign(\\hat{y} - y)$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(-5, 1, 1)]\n",
    "y = [i * 0 - 1 for i in range(6)]\n",
    "x_1 = [i for i in range(0, 6)]\n",
    "y_1 = [i * 0 + 1 for i in range(0, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, \"b\")\n",
    "plt.plot(x_1, y_1, \"b\")\n",
    "plt.plot(0, 0, \"ro\")\n",
    "plt.plot(0, 1, \"bo\")\n",
    "plt.plot(0, -1, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Loss\n",
    "\n",
    "$$b = max(x, y)$$\n",
    "\n",
    "$$b = x~~if~~x > y~~else~~y$$\n",
    "\n",
    "\n",
    "$$\\frac {\\delta b} {\\delta x} =  \\frac {\\delta x} {\\delta x}~~if~~x > y~~else~~\\frac {\\delta y} {\\delta x} = 1~~if~~x > y~~else~~0$$\n",
    "\n",
    "Если $x > y$, то он оказал влияние на $b$. Иначе, его вклада в $b$ НЕ БЫЛО - градиент равен 0\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из: \n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем:\n",
    "\n",
    "$ \\nabla_W\tL(W) = {1 \\over N}\\sum_{i=1}^N \\nabla_W L_i(x_i, y_i, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{blue}{\\text{*Не обязательное задание:}}$\n",
    "\n",
    "Поставлю дополнительно 20 баллов тому, кто графически оформит результаты этого эксперимента и напишет вывод о зависимости качества обучения от learning_rate и batch_size (возможно, нужно будет добавить варианты их сочетаний и/или увеличить количество эпох до 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "file_exists = os.path.exists(\"/content/cifar-10-batches-py\")\n",
    "if file_exists == False:\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !tar -xzf cifar-10-python.tar.gz\n",
    "    !mkdir content\n",
    "    !mv  cifar-10-batches-py content/cifar-10-batches-py\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "x_train = np.zeros((0, 3072))\n",
    "y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    x_train = np.append(x_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    y_train = np.append(y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(\"content/cifar-10-batches-py/test_batch\")\n",
    "x_test = np.array(test[b\"data\"])\n",
    "y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_eng = [\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LinearClassifier():\n",
    "    def __init__(self, labels, batch_size, random_state=42):\n",
    "        self.labels = labels  # classes names\n",
    "        self.classes_num = len(labels)  # num of classes\n",
    "\n",
    "        np.random.seed(random_state)  \n",
    "        self.W = (\n",
    "            np.random.randn(3073, self.classes_num) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias \n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # reset loss\n",
    "        train_len = x_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)  \n",
    "\n",
    "        for i in range(0, train_len, self.batch_size):  \n",
    "            idx = indexes[i : i + self.batch_size]   \n",
    "            x_batch = x_train[idx]  \n",
    "            y_batch = y_train[idx]  \n",
    "\n",
    "            x_batch = np.hstack([x_batch, np.ones((x_batch.shape[0], 1))])  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(x_batch, y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        current_batch_size = x.shape[0]  # batch_size\n",
    "        loss = 0.0  \n",
    "        dW = np.zeros(self.W.shape) \n",
    "        for i in range(current_batch_size):  \n",
    "            scores = x[i].dot(self.W)  # vector of shape 10\n",
    "            correct_class_score = scores[int(y[i])]  \n",
    "            above_zero_loss_count = 0  \n",
    "            for j in range(self.classes_num): \n",
    "                if j == y[i]:  # predict class\n",
    "                    continue\n",
    "                margin = scores[j] - correct_class_score + 1  # loss\n",
    "                if margin > 0:  \n",
    "                    above_zero_loss_count += 1  \n",
    "                    loss += margin  # \n",
    "                    dW[:, j] += x[i]  # \n",
    "            dW[:, int(y[i])] -= above_zero_loss_count * x[i]  \n",
    "        loss /= current_batch_size  \n",
    "        dW /= current_batch_size  \n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.append(x, 1)  # add 1 (bias)\n",
    "        scores = x.dot(self.W)  \n",
    "        return np.argmax(scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, x_test, y_test, noprint=False):\n",
    "    correct = 0  \n",
    "    for i, img in enumerate(x_test):  \n",
    "        index = model.forward(img)  \n",
    "        correct += (\n",
    "            1 if index == y_test[i] else 0\n",
    "        )  \n",
    "        if noprint is False:  \n",
    "            if i > 0 and i % 1000 == 0:  \n",
    "                print(\n",
    "                    \"Accuracy {:.3f}\".format(correct / i)\n",
    "                )  \n",
    "    return correct / len(y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How learning quality depends of speed:\")\n",
    "\n",
    "for lr in [1e-2, 1e-8]:\n",
    "    for bs in [256, 2048]:\n",
    "\n",
    "        print(\"-\" * 50, \"\\n\", \"learning_rate =\", lr, \"\\tbatch_size =\", bs)\n",
    "        print()\n",
    "        lc_model = LinearClassifier(labels_eng, batch_size=bs)\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(10):\n",
    "            loss = lc_model.fit(x_train, y_train, learning_rate=lr)\n",
    "            accuracy = validate(lc_model, x_test, y_test, noprint=True)\n",
    "            if best_accuracy < accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_epoch = epoch\n",
    "            print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор шага обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/update_weghts_values.png\" width=\"450\"> \n",
    "\n",
    "<!-- [Визуализация](https://docs.google.com/file/d/0Byvt-AfX75o1ZWxMRkxrUFJ2ZUE/preview)\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг обучения - некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время, чтобы тот же процесс не шел слишком медленно (как на графике слева)\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/learning_rate_optimal_value.png\" > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор слишком большого шага обучения может стать причиной нежелательного характера изменения значения функции потерь в процессе обучения. Вместо ожидаемого движения в сторону минимума, функция ошибок может начать периодически изменяться в течение цикла обучения или же вовсе продемонстрировать неограниченный рост."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве иллюстрации этого эффекта, рассмотрим модельный пример. Предположим, мы решаем задачу регрессии и хотим восстановить значение неизвестной скалярной функции. \n",
    "\n",
    "Создадим синтетический датасет для функции\n",
    "$y(x_1, x_2, ... , x_n) = \\sum_i x_i^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "x_train = np.random.rand(1000, 5)\n",
    "noise = np.random.rand(1000, 1) / 1000\n",
    "y_train = np.expand_dims(np.sum(x_train ** 3, axis=-1), axis=1) + noise\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простейшую модель линейной регрессии. Используем MSE в качестве функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor():\n",
    "    def __init__(self, in_features, out_features, batch_size, random_state=42):\n",
    "        self.in_features = in_features  # num of inputs\n",
    "        self.out_features = out_features  # num of outputs\n",
    "\n",
    "        np.random.seed(random_state)  \n",
    "        self.W = (\n",
    "            np.random.randn(self.in_features + 1, self.out_features) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias \n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # reset loss\n",
    "        train_len = x_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)  \n",
    "\n",
    "        for i in range(0, train_len, self.batch_size):  \n",
    "            idx = indexes[i : i + self.batch_size]   \n",
    "            x_batch = x_train[idx]  \n",
    "            y_batch = y_train[idx]  \n",
    "\n",
    "            x_batch = np.hstack(\n",
    "                [x_batch, np.ones((x_batch.shape[0], 1))]\n",
    "            )  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(x_batch, y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        '''\n",
    "        MSE loss\n",
    "        '''\n",
    "        current_batch_size = x.shape[0]  # batch_size\n",
    "        loss = 0.0  \n",
    "        dW = np.zeros(self.W.shape)\n",
    "        for i in range(current_batch_size):  \n",
    "            y_preds_on_batch = x[i].dot(self.W)  # vector of shape out_features\n",
    "            y_true_on_batch = y[i]\n",
    "            loss += np.sum((y_preds_on_batch - y_true_on_batch) ** 2)\n",
    "            # dW_{m, n} = 2 * x_m ((Wx)_n - y_n)\n",
    "            dW += np.meshgrid(y_preds_on_batch - y_true_on_batch, x[i])[0]\n",
    "            \n",
    "        loss /= current_batch_size  \n",
    "        dW /= current_batch_size  \n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.append(x, 1)  # add 1 (bias)\n",
    "        scores = x.dot(self.W)  \n",
    "        return np.argmax(scores)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим нашу модель на синтетических данных и посмотрим, как будет изменяться значение функции потерь в ходе процесса обучения на 20 эпохах при использовании для обучения различных значений скорости обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_and_plot_history(lerning_rates_list, batch_size=64, n_epochs=20):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    for lr_id, lr in tqdm(enumerate(lerning_rates_list)):\n",
    "        regressor = LinearRegressor(\n",
    "            in_features=x_train.shape[1], \n",
    "            out_features=y_train.shape[1],\n",
    "            batch_size=42\n",
    "        )\n",
    "        loss_hist = np.zeros(n_epochs)\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = regressor.fit(x_train, y_train, learning_rate=lr)\n",
    "            loss_hist[epoch] = loss\n",
    "        ax.plot(loss_hist, label='lr={:0.1e}'.format(lr))\n",
    "    \n",
    "    ax.set_xticks(range(n_epochs))\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('MSE loss')\n",
    "    # ax.set_xlim([3, 8])\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При значениях скорости обучения в диапазоне $5 \\times 10^{-4} - 7 \\times 10^{-4}$ мы наблюдаем ожидаемый процесс обучения модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot_history(\n",
    "    lerning_rates_list=np.linspace(start=5e-4, stop=7e-4, num=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понизив на несколько порядков скорость обучения, мы увидим, что за первые 20 эпох обучения модель существенно не смола улучшить величину MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot_history(\n",
    "    lerning_rates_list=np.linspace(start=5e-9, stop=7e-9, num=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если же мы наоборот увеличим на несколько порядков скорость обучения, то заметим что функция ошибок стала изменяться хаотично и с дальнейшим ростом скорости обучения будет стремиться неограниченно ухудшаться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot_history(\n",
    "    lerning_rates_list=np.linspace(start=0.5, stop=0.55, num=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока изменения loss функции достаточно большие, сам по себе градиент тоже большой. За счет этого можно двигаться быстро. Когда он будет уменьшаться, мы должны оказаться рядом с этим минимумом, и шаг, который мы выбрали, не должен мешать этому процессу. \n",
    "\n",
    "Бывают ситуации, когда шаг можно менять в самом процессе обучения, когда в начале обучения модели шаг большой, потом, по мере того, как она сходится, чтобы более четко найти минимум, шаг можно изменить вручную. Но изначально его нужно каким-то образом подобрать, и это зависит от данных и от самой модели. Это тоже гиперпараметр, связанный с обучением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор размера батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/define_size_of_batch.png\" width=\"500\"> \n",
    "\n",
    "Мы с самого начала говорили о выборках, некоторого количества примеров. В дальнейшем, мы будем называть их батчами. Батч (англ. *batch*) - это некоторое подмножество обучающей выборки фиксированного размера.\n",
    "\n",
    "При этом было не очень понятно, чем они мотивированны. Точнее, мы мотивировали это тем, что у нас много данных, и мы не сможем их обработать все, и это правда. Даже если мы сможем загрузить все данные в память, нам нужно будет загрузить их и использовать при расчете, в том числе градиента. Это ещё более затратно. \n",
    "\n",
    "Зачем батчи нужны при рассчете loss? Если мы посчитаем loss по одному изображению, скорее всего он будет очень специфичен, и это движение, которое произойдет, будет направлено в сторону минимума, потому что по этому конкретному изображению мы улучшим показатели, что не отражает обобщения всех данных, поэтому мы используем батчи.\n",
    "\n",
    "Нас в первую очередь интересует более общее направление, которое будет работать на большинстве наших данных. Поэтому если мы будем оптимизировать модели, исходя из одного элемента данных, путь будет витиеватый, и процесс будет происходить достаточно долго. Если же данные не помещаются в память, то **можно использовать батч того размера, который у нас есть**. Можно загрузить в память и считать по батчам градиент. В этом случае спуск будет более плавным, чем по одному изображению. \n",
    "\n",
    "Также при использовании всего датасета тоже есть свои минусы. **Не всегда загрузка всего датасета приводит к увеличению точности**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее: \n",
    "* [Методы оптимизации нейронных сетей\n",
    "](https://habr.com/ru/post/318970/)\n",
    "* [Обучение нейронной сети](https://alxmamaev.medium.com/deep-learning-на-пальцах-часть-1-341cfe021ef9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Сложность модели__ (*model complexity*) &mdash; важный гиперпараметр. В частности, для линейной модели, сложность может быть представлена количеством параметров, для полиномиальных моделей &mdash; степенью полинома, для деревьев решений &mdash; глубиной дерева и т.д.\n",
    "\n",
    "Сложность модели тесно связана с __ошибкой обобщения__ (_generalization error_). Ошибка обобщения отличается от ошибки обучения, измеряемой на тренировочных данных, тем, что позволяет оценить обобщающую способность модели, приобретенную в процессе обучения, давать точные ответы на неизвестных ей объектах. Cлишком простой модели не будет хватать мощности для обобщения сложной закономерности в данных, что приводит к большой ошибке обобщения, с другой стороны слишком сложная модель также приводит к большой ошибке обобщения за счет того, что в силу своей сложности модель начинает пытаться искать закономерности в шуме, добиваясь большей точности на тренировочных данных, теряя при этом часть обобщающей способности.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/model_complexity.png\" width=\"500\"> \n",
    "\n",
    "Проиллюстрируем описанное явление на примере полиномиальной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2 * np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "plt.scatter(x, y, s=50, facecolors='none', edgecolors='b', label='noisy data')\n",
    "\n",
    "x_true = np.linspace(0, 2 * np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "plt.plot(x_true, y_true, c='lime', label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем аппроксимировать имеющуюся зависимость с помощью полиномиальной модели, используя шумные данные в качестве тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x_train = x.reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, degree in enumerate([0, 1, 3, 9]):\n",
    "\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "    fig.add_subplot(2, 2, i + 1)\n",
    "    plt.plot(x_true, y_plot, c='red', label=f'M={degree}')\n",
    "    plt.scatter(x, y, s=50, facecolors='none', edgecolors='b')\n",
    "    plt.plot(x_true, y_true, c='lime')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель может переобучаться, подстраиваясь под тренировочную выборку. В полиноме степень, и как следствие количество весов &mdash; это гиперпараметр, который можно подбирать на кросс-валидации, однако, когда мы, таким образом, подбираем сложность модели мы налагаем довольно грубое ограничение на обобщающую способность модели в целом. Вместо этого более разумным было бы оставить модель сложной, но использовать некий ограничитель (__регуляризатор__), который будет заставлять модель отдавать предпочтение выбору более простого обобщения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(9), LinearRegression())\n",
    "model_ridge = make_pipeline(PolynomialFeatures(9), Ridge(alpha=0.1))\n",
    "\n",
    "model.fit(x_train, y)\n",
    "y_plot = model.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "model_ridge.fit(x_train, y)\n",
    "y_plot_ridge = model_ridge.predict(x_true.reshape(-1, 1))\n",
    "\n",
    "plt.plot(x_true, y_plot, c='red', label=f'M={degree}')\n",
    "plt.plot(x_true, y_plot_ridge, c='black', label=f'M={degree}, alpha=0.1')\n",
    "plt.scatter(x, y, s=50, facecolors='none', edgecolors='b')\n",
    "plt.plot(x_true, y_true, c='lime', label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "poly_coef = model[1].coef_\n",
    "\n",
    "eq = f'y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x'\n",
    "for i in range(2, 10):\n",
    "    eq += f'+{round(poly_coef[i], 2)}*x^{i}'\n",
    "    \n",
    "print('Without regularization: ', eq)\n",
    "\n",
    "poly_coef = model_ridge[1].coef_\n",
    "\n",
    "eq = f'y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x'\n",
    "for i in range(2, 10):\n",
    "    eq += f'+{round(poly_coef[i], 2)}*x^{i}'\n",
    "\n",
    "print('With regularization: ', eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что одним из \"симптомов\" переобучения являются аномально большие веса. Модель Ridge Regression, показанная в примере выше, использует L2 регуляризуцию для борьбы с этим явлением.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/l2_regularization.jpg\" width=\"700\"> \n",
    "\n",
    "\n",
    "L2 Regularization = weights decay\n",
    "\n",
    "Идея состоит в том, что мы можем наложить некоторое требование на сами веса. Дело в том, что можно получить один и тот же выход модели при разных весах (выход модели соответствует умножению весов на $x$), при разных $w$ выход может быть идентичен.\n",
    "\n",
    "Эти параметры задают некоторую аппроксимацию нашей целевой функции, аппроксимировать функцию можно двумя способами:\n",
    "1. Использовать все имеющиеся данные и провести ее строго через все точки, которые нам известны; \n",
    "2. Использовать более простую функцию: (в данном случае линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым общим закономерностям, которые у них есть. \n",
    "\n",
    "Характерной чертой переобучения является второй сценарий, и сопровождается он, как правило, большими весами. Введение L2-регуляризации приводит к тому, что большие веса штрафуются и предпочтение отдается решениям, использующим малые значения весов. \n",
    "\n",
    "Модель может попробовать схитрить и по-другому - использовать все веса, все признаки, даже незначимые, но с маленькими коэффициентами. С этим L2-loss поможет хуже, так как он не сильнее штрафует мелкие веса. Результатов его применения - малые значения весов, которые использует модель\n",
    "\n",
    "В этом случае на помощь приходит L1-loss, который штрафует вес за сам факт отличия его от нуля. Но и штрафует он все веса одинаково. Результат его применения - малое число весов, которые использует модель в принципе. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L02/out/l1_and_l2_regularization.gif\" alt=\"alttext\" width=\"550\"/>\n",
    "\n",
    "\n",
    "Это лоссы можно комбинировать - получится Elastic Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$\\lambda=$ regularization strength (hyperparameter)\n",
    "\n",
    "$L(W)=\\underbrace{\\frac1N\\sum_{i=1}^NL_i(f(x_i,W),y_i)}_{\\textbf{Data loss} }+\\underbrace {\\lambda R(W)}_{\\textbf{Regularization}}$\n",
    "\n",
    "Берем сумму всех весов по всей матрице $w$, и добавляем ее к loss. Соответственно, чем больше будет эта сумма, тем больше будет суммарный loss. \n",
    "\n",
    "\n",
    "В дальнейшем проблема с переобучением будет вставать довольно часто. Методов регуляризации модели существует достаточно много. Этот — один из базовых, который будет использоваться практически во всех оптимизаторах, с которыми познакомимся позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь Кросс-энтропия \n",
    "\n",
    "[Отличное видео от Statquest про энтропию](https://www.youtube.com/watch?v=YtebGVx-Fxw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "На Cross-entropy Loss построены практически все модели, про которые будет идти речь.\n",
    "\n",
    "Это оценка результата и обновление весов с использованием градиента. У нас один слой, поэтому мы достаточно легко посчитали от него градиент. Если слоев будет много, сделать это будет сложней.\n",
    "\n",
    "Рассмотрим ещё одну loss-функцию. \n",
    "\n",
    "SVM loss хороша тем, что интуитивно понятно, как ее посчитать. Но проблема состоит в том, что модель выдает некоторые числа, которые сами по себе невозможно интерпретировать. Сами по себе выходы модели мало что означают. Было бы неплохо, если бы мы сразу, глядя на выход модели для кошки, могли бы их как-то интерпретировать, не просматривая остальные веса. Хорошо бы, чтобы модель выдавала не какую-то абстрактную уверенность, а **вероятность** того, что по ее мнению, на картинке изображена кошка.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к вероятностям\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "[Видео от StatQuest, которое объясняет Softmax](https://www.youtube.com/watch?v=KpKog-L9veg)\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/scores_to_probability.png\" width=\"750\">\n",
    "\n",
    "Перейти к вероятностям мы сможем, проведя с весами некоторые не очень сложные математические преобразования. \n",
    "\n",
    "На слайде выше показано, почему выходы модели часто называют [logit’ами](https://en.wikipedia.org/wiki/Logit). Если предположить, что у нас есть некая вероятность, от которой мы берем такую функцию (logit), то она может принимать значения от нуля до бесконечности. Мы можем считать, что выходы модели - это logit’ы. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы могли бы просто взять индекс массива, в котором значение (logit) максимально. Предположим, что наша сеть выдала следующие значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = [\n",
    "    5.1, # cat\n",
    "    3.2, # car\n",
    "    -1.7, # frog\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, чтобы узнать какой класс наша сеть предсказала, мы могли бы просто взять `argmax` от наших `logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Predicted class = %i (Cat)' % (np.argmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но от argmax нельзя посчитать градиент, так как производная от константы равна 0. Соответственно, если бы мы вставили производную от argmax в градиентный спуск, мы бы получили везде нули, и соответственно, наша сеть бы вообще ничему не научилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(np.arange(3), [1, 0, 0], color='red', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А мы бы хотели получить не logit’ы, а настоящую вероятность на выходе модели. Да еще и таким образом, что бы от наших вероятностей можно было бы посчитать градиент. Для этого мы можем применить к нашим логитам функцию **Softmax**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/linear_classifier_softmax.png\" width=\"1300\">\n",
    "\n",
    "Мы можем провести над логитами операцию экспоненцирования, то есть число Эйлера (2.71828) **возвести в степень**, соответствующую этому выходу. В результате, мы получим вектор, гарантировано неотрицательных чисел (потому что мы ввели неположительное число в степень, пускай даже отрицательную, то есть выходы могут быть маленькие, но всегда положительные). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше, чтобы можно было интерпретировать эти числа как вероятности, их сумма должна быть равна единице. Мы должны их нормализовать, то есть **поделить на сумму**. Это преобразование называется **Softmax функцией**.\n",
    "\n",
    "**Получаются вероятности**, то есть числа, которые можно интерпретировать таким образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Softmax}_\\text{кошка} = \\frac{e^{5.1}}{e^{5.1} + e^{3.2} + e^{-1.7}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return np.exp(logits) / np.sum(np.exp(logits))\n",
    "\n",
    "print(softmax(logits))\n",
    "print('Sum = %.2f' % np.sum(softmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что Softmax, никоим образом не поменял порядок значений. Самому большому logit'у соответствует самая большая вероятность, а самому маленькому, соответственно самая маленькая"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графиках. Возьмем массив случайных логитов и применим к ним softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_logits = np.linspace(-1, 1, 50)\n",
    "fig,ax = plt.subplots(ncols=2)\n",
    "\n",
    "ax[0].plot(np.arange(50), rand_logits)\n",
    "ax[0].set_title('Logits')\n",
    "ax[1].plot(np.arange(50), softmax(rand_logits))\n",
    "ax[1].set_title('Softmax')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-entropy / log loss**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/cross_entropy_plot_loss_with_probability.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Теперь, с использованием такого вектора можно определить другую loss функцию. Если не вникать в детали, можно **взять от нее логарифм**. Тогда loss от такого выхода будет выглядеть вот таким образом:\n",
    "\n",
    "$ L_i =  -log(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}})$\n",
    "\n",
    "как нормализованный выход от верного класса на сумму остальных, к которому добавили минус. \n",
    "\n",
    "Получится график loss (слева). Его плюс заключается в том, что у него нет участка с плато, практически по всей длине функция получается гладкой, с хорошими мощными производными. Когда loss большой, производная тоже большая, и за счет этого можно быстро обучать модель. Для кусочно-линейной функции потерь она же равна константе.\n",
    "\n",
    "До тех пор, пока модель не будет работать с точностью 100%, то есть пока все остальные выходы не станут нулевыми, мы можем продолжить обучение, даже если у нас нет явной регуляризации.\n",
    "\n",
    "Осталось выяснить, почему такая простая на вид функция **называется так сложно** - Кросс-энтропия. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение Кросс-энтропии\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Начнем с простого примера, пусть у нас есть 10 точек со следующими значениями признака x: \n",
    "\n",
    "`x = [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]`\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/cross_entropy_ten_dots.png\">\n",
    "\n",
    "Пусть наши точки принадлежат двум классам: зеленый и красный:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/cross_entropy_two_classes.png\">\n",
    "\n",
    "Перед нами простая задача классификации: по признаку x предсказать класс наших точек. Мы можем переформулировать задачу как нахождение вероятности того, что точка зеленая или красная. В идеальной ситуации для зеленой точки вероятность того, что она зеленая равна 1, в то же время вероятность того, что красная точка &mdash; зеленая должна быть равна 0. \n",
    "\n",
    "Вероятности, которые выдает, обучаемая нами модель, зачастую далеки от идеальной ситуации. В нашем примере сравнить насколько сильно предсказанная вероятность класса отличается от вероятности, выдаваемой \"идеальной моделью\" можно за счет __бинарной кросс-энтропии__ частного случая кросс-энропии.\n",
    "\n",
    "$$H_p(q)=-\\frac{1}{N}\\sum^N_{i=1}y_i\\cdot log(p(y_i))+(1-y_i)\\cdot log(1-p(y_i))$$\n",
    "\n",
    "где $y$ &mdash; метка класса (1 для зеленого, 0 для красного), которую можно также интерпретировать как вероятность, предсказанную \"идеальной моделью\", $p(y)$ &mdash; вероятность того, что точка зеленая, предсказываемая оцениваемой моделью.\n",
    "\n",
    "Какое же отношение энтропия имеет к этой формуле? Давайте углубимся в детали.\n",
    "\n",
    "Поскольку $y$ представляет метку классов точек, то его распределение $q(y)$ выглядит следующим образом:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/cross_entropy_distribution.png\">\n",
    "\n",
    "__Энтропия__ &mdash; мера неуверенности, связанная с распределением $q(y)$. Какова была бы мера неуверенности распределения $q(y)$ если бы все точки были зелеными? Так как у нас бы не было сомнений насчет цвета точки (он всегда зеленый), значение энтропии было бы 0. Теперь представим другую ситуацию, пусть у нас поровну точек зеленого и красного цвета. Для нас это наихудшая ситуация, поскольку попытка определить цвет точки, по сути, представляет случайное угадывание. В этом случае энтропия вычисляется по формуле Хартли:\n",
    "\n",
    "$$H(q)=log(2)$$\n",
    "\n",
    "Мы рассмотрели два крайних случая, но как быть с промежуточными ситуациями? Для этих случаев мы можем использовать формулу Шеннона:\n",
    "\n",
    "$$H(q)=-\\sum^C_{c=1}q(y_c)\\cdot log(q(y_c))$$\n",
    "\n",
    "где C &mdash; количество классов. Нетрудно заметить, что формула Хартли является частным случаем формулы Шеннона.\n",
    "\n",
    "Таким образом, зная истинное распределение случайной величины, мы можем рассчитать его энтропию. А что будет если мы попытаемся аппроксимировать истинное распределение $q(y)$ некоторым другим распределением $p(y)$? Допустим, что наши цветные точки подчиняются этому распределению $p(y)$, также мы знаем, что исходят они из неизвестного нам истинного распределения $q(y)$, если мы посчитаем следующую энтропию, это и будет __кросс-энтропия__:\n",
    "\n",
    "$$H_p(q)=-\\sum^C_{c=1}q(y_c)\\cdot log(p(y_c))$$\n",
    "\n",
    "Если окажется, что распределения $p(y)$ и $q(y)$ совпадают, в этом случае энтропия $H(q)$ и кросс-энтропия $H_p(q)$ также будут совпадать. Однако в реальности такое случается редко и кросс-энтропия бывает больше энтропии истинного распределения\n",
    "\n",
    "$$H_p(q)-H(q)\\geq0$$\n",
    "\n",
    "Разница между кросс-энтропией и энтропией называется __дивергенцией Кульбака-Лейблера__, которая является мерой различия между двумя распределениями:\n",
    "\n",
    "$$D_{KL}(q||p)=H_p(q)-H(q)=\\sum^C_{c=1}q(y_c)\\cdot [log(q(y_c))-log(p(y_c))]$$\n",
    "\n",
    "Данная формула для дивергенции Кульбака-Лейблера по своей сути является записью выражения для __математического ожидания величины__ $log(q(y_c))-log(p(y_c)) = \\log(\\frac{q(y_c)}{p(y_c)})$ (__логарифма отношения вероятностей $p(y)$ и $q(y)$__) , __взятого по \"истинному\" распределению $q(y)$__. Это значит, что чем ближе распределение вероятности $p(y)$ к распределению $q(y)$, тем меньше будет значение дивергенции Кульбака-Лейблера и, следовательно, меньше значение кросс-энтропии. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При построении моделей классификации можно определить вероятностные распределения $p(x)$ и $q(x)$. Действительно, для обучающей выборки (для которой мы читаем известным номера правильных классов для всех объектов) мы можем приписать вероятность принадлежности некоторого объекта к его истинному классу равной $1$, а вероятность принадлежности ко всем другим классам положить равной $0$ . Например, в случае бинарной классификации $q(y)$ представляет собой просто индикатор принадлежности объекта к одному из классов:\n",
    "$$q(y) =  \\begin{cases} 1 \\, \\text{if $y \\in C$}  \\\\ 0 \\, \\text{else} \\end{cases}$$\n",
    "\n",
    "Если выходы модели принимают значения в вещественных числах, то их также можно интерпретировать как вероятности, переменив описанное выше SoftMax отображение. Если $\\vec z$ -- вектор выходов модели, то величины:\n",
    "$$ \\sigma(\\vec z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}} $$\n",
    "дадут искомое распределение $p(y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из определения выше можно легко получить выражение для кросс-энтропии пары вероятностных распрелелений, записанных через соответствующие энтропию и дивергенцию Кульбака-Лейблера:\n",
    "$$H_p(q) = D_{KL}(q||p) + H(q)$$.\n",
    "\n",
    "Важно заметить, что энтропия априорного распределения равна нулю $H(q)$ (так как равны нулю все слагаемые). То есть численное значение близости оприорного $q$ и оценочного $p$ распределений вероятности по дивергенции Кульбака-Лейбера даётся просто величиной кросс-энтропии $H_p(q)$. Мы хотим построить модель классификации, которая бы давала верную оценку принадлежности объектов классам — т. е. порождала $p(y)$ близкое к $q(y)$. Для этих целей мы стремимся __минимизировать кросс-энтропию__ при обучении модели.\n",
    "\n",
    "\n",
    "Таким образом, можно коротко сформулировать \"рецепт\" построения модели классификации с использованием кросс-энтропии в качестве функции потерь:\n",
    "\n",
    "* Построить априорное распределение вероятности принадлежности объектов классам для размеченных данных $Q$\n",
    "* Преобразовать \"сырые\" выходы модели в нормализованные и нормированные величины, которые можно будет интерпретировать в качестве оценок вероятностей принадлежностей исследуемых объектов к классам — построить оценочное распределение $P$\n",
    "* Сравнить априорные и оценочные вероятности принадлежности классам, вычислив между ними кросс-энтропию $H(P|Q)$\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/logits_to_scores_to_probabilitys.png\" width=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнение: кросс-энтропия с точки зрения теории информации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим следующий пример:\n",
    "\n",
    "Пусть у нас есть некоторые данные, которые мы передаем по каналу связи. Например, у нас есть метеостанция, которая сообщает прогноз погоды. Допустим, она может передавать 8 вариантов прогноза. Мы в каждый момент времени получаем от нее сообщение. Для передачи всех 8 возможных вариантов прогноза погоды наше сообщение должно содержать не менее 3 бит информации. Можем ли мы эффективнее распорядиться нашим каналом связи и в среднем тратить меньшее количество бит информации на передачу сообщений? [*Оказывается, это возможно*](https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding). Но необходимы некоторые дополнительные предположения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, мы знаем некую вероятность, с которой в нашем регионе может наступить хорошая либо плохая погода. Пусть у имеет место вероятностное распределение, по которому у нас, допустим, солнечное место, где почти не бывает бури. Например, как на нижнем правом изображении на следующей иллюстрации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/efficient_way_to_send_signals.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы знаем об этом, мы можем сэкономить на передаче информации. Мы можем закодировать наиболее вероятные сообщения двумя битами, причем придумать такие коды, чтобы они почти не пересекались. Они будут начинаться с нуля и передавать два бита вместо трех. Таким образом, можно сэкономить на передачи информации. То есть идея такая: **если мы знаем, что события маловероятны, мы можем кодировать их более длинной цепочкой, а более вероятные события - более короткими цепочками**. В этом случае в среднем количество информации, которое можно передать, сократится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся вновь к формуле Шеннона:\n",
    "\n",
    "$$H(q)=-\\sum_{x \\in X}q(x)\\cdot \\log(q(x)),$$\n",
    "\n",
    "в которую подставим изображенное на рисунке вероятностное распределение:\n",
    "\n",
    "$$H(Q)= - (2 \\cdot 0.35 \\cdot \\log_2(0.35) + 2 \\log_2 0.10 \\cdot \\log_2(0.10) + 2 \\cdot 0.04 \\cdot \\log_2(0.04) + 2 \\cdot 0.01 \\cdot \\log_2(0.01)) = 2.23.$$\n",
    "\n",
    "Мы получили число несколько меньшее $3$. [Можно показать](https://www.youtube.com/watch?v=YtebGVx-Fxw), что эта величина $2.23$ даёт оценку среднего количества информации, \"содержащегося\" в одном сообщении. На практике реализовать это, используя биты, возможно, не выйдет, но это свойство данного вероятностного распределения можно посчитать. Оно несёт в себе некоторое количество полезной информации, соответствующее этой энтропии.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предложенная кодовая система явно опирается на априорную информацию о распределении частот событий. Что будет, если частота событий изменится (или же мы просто будем использовать определенную выше систему кодирования для формирования сообщений о погоде в другом месте, где климат может отличаться; см. нижний левый фрагмент рисунка)? При смене местоположения метеостанции с сохранением схемы кодирования событий, у нас осталось неизменным априорное распределение $q$ частот погодных событий из старого места, но появилось некоторое новое \"оценочное\" распределение $p$ -- распределение частот погодных классов в новом месте. Вычислим кросс-энтропию между этими распределениями:\n",
    "\n",
    "$$H(P|Q)= - (2 \\cdot 0.35 \\cdot \\log_2(0.01) + 2 \\log_2 0.10 \\cdot \\log_2(0.04) + 2 \\cdot 0.04 \\cdot \\log_2(0.10) + 2 \\cdot 0.01 \\cdot \\log_2(0.35)) = 5.84.$$\n",
    "\n",
    "[В теории информации](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) кросс-энтропия между двумя распределениями вероятностей $p$ и $q$  по одному и тому же базовому набору событий измеряет среднее число битов, необходимых для идентификации события, взятого из набора, если схема кодирования, используемая для набора, оптимизирована для оценочного распределения вероятностей $q$, а не для истинного распределения $p$. Вычтя теперь из полученного значения $H(P|Q)$ величину информационной энтропии $H(Q)$ получим описанную выше дивергенцию Кульбака-Лейблера:\n",
    "\n",
    "$$D_{KL}(P|Q) = H(P|Q) - H(Q) \\approx 3.64$$,\n",
    "которая оказалась существенно отлична от нуля. Большое значение $D_{KL}(P|Q)$ позволяет нам убедиться в том, что вероятностные распределения $P$ и $Q$ существенно отличаются и нам не стоит переносить схему кодирования старой метеостанции на новую (подумайте, почему?). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент функции потерь Кросс-энтропии\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cross Entropy Loss](https://wandb.ai/wandb_fc/russian/reports/---VmlldzoxNDI4NjAw#:~:text=%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F%20%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8C%20%D0%BF%D0%B5%D1%80%D0%B5%D0%BA%D1%80%D0%B5%D1%81%D1%82%D0%BD%D0%BE%D0%B9%20%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D0%B8%20%E2%80%93%20%D1%8D%D1%82%D0%BE,%2C%20%D0%B3%D0%B4%D0%B5%200%20%E2%80%93%20%D0%B8%D0%B4%D0%B5%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -L = \\sum_i y_i \\log p_i = \\sum_i y_i \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
    "\n",
    "$$s_{y_i} = w_i x$$\n",
    "\n",
    "$$ \\dfrac {\\delta L} {\\delta w_i} = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} $$\n",
    "\n",
    "$$\\dfrac {\\delta s_{y_i}} {\\delta w_i} = x$$\n",
    "\n",
    "Только один $y_k = 1$\n",
    "\n",
    "\n",
    "$$ -L = y_k \\log p_i = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
    "\n",
    "i = k\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_i}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_i} - \\log  \\sum_j e^{s_{y_j}}$$\n",
    "\n",
    "$$\\dfrac {\\delta -L} {\\delta s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\delta {\\sum_j e^{s_{y_j}}}} {\\delta s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\delta e^{s_{y_i}}} {\\delta s_{y_i}} = 1 - \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = 1 - p_i$$\n",
    "\n",
    "$$\\dfrac {\\delta L} {\\delta s_{y_j}} = p_i - 1 $$\n",
    "\n",
    "$$ \\dfrac {\\delta L_i} {\\delta w_i}  = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} = (p_i - 1) x $$\n",
    "\n",
    "i != k\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_k}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_k} - \\log  \\sum_j e^{s_{y_j}}$$\n",
    "\n",
    "$$\\dfrac {\\delta -L} {\\delta s_{y_i}} = - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\delta {\\sum_j e^{s_{y_j}}}} {\\delta s_{y_i}} =  \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\delta e^{s_{y_i}}} {\\delta s_{y_i}} = \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = - p_i$$\n",
    "\n",
    "$$\\dfrac {\\delta L} {\\delta s_{y_j}} = p_i $$\n",
    "$$ \\dfrac {\\delta L_i} {\\delta w_i}  = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} = p_i x $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде это будет выглядеть вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input batch of 2 vector with 4 elements\n",
    "x = np.array([ \n",
    "              [1, 2, 3, 4], \n",
    "              [1, -2, 0, 0]\n",
    "            ])\n",
    "# Weights\n",
    "W = np.random.randn(3, 4) # 3 class\n",
    "\n",
    "# model output\n",
    "logits = x.dot(W.T)\n",
    "print(\"Scores(Logits) \\n\",logits,\"\\n\")\n",
    "\n",
    "# Probabilities\n",
    "probs = softmax(logits) #defined before\n",
    "print(\"Probs \\n\",probs,\"\\n\")\n",
    "\n",
    "# Ground true classes\n",
    "y = [0,1] \n",
    "\n",
    "# Derivative\n",
    "probs[np.arange(1),y] = -1 # substract one from true class prob\n",
    "dW = x.T.dot(probs) # dot product with input\n",
    "\n",
    "print(\"Grads dL/dW \\n\",dW) # have same shape as W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое вычисление SoftMax\n",
    "\n",
    "Когда мы возводим число в степень, может получиться очень большое число и произойти переполнение памяти. Неизвестно, какие будут у модели выходы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "f = np.array([123, 456, 789])\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы можем вычесть из каждого $s_i$ положительную константу, чтобы уменьшить значения экспонент. В качестве константы можно выбрать максимальный элемент этого вектора, тогда у нас гарантированно не будет очень больших чисел, и такой способ будет работать более стабильно.\n",
    "\n",
    "$$M = \\max_j s_{y_{j}}$$\n",
    "$$s^{new}_{y_{i}}  = s_{y_{i}} - M $$\n",
    "\n",
    "$$ \\dfrac {e^{s^{new}_{y_{i}}}} {\\sum_j e^{s^{new}_{y_{j}}}}  = \\dfrac {e^{s_{y_{i}} - M }} {\\sum_j e^{s_{y_{j}} - M }} = \\dfrac {e^{s_{y_{i}}}e ^ {-M}} {\\sum_j e^{s_{y_{j}}} e ^ {-M}} = \\dfrac {e ^ {-M} e^{s_{y_{i}}}} {e ^ {-M} \\sum_j e^{s_{y_{j}}} } = \\dfrac { e^{s_{y_{i}}}} { \\sum_j e^{s_{y_{j}}} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([123, 456, 789])\n",
    "f -= f.max()\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "print(f, p)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
