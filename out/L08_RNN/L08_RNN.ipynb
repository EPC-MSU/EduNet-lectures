{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные нейронные сети RNN\n",
    "\n",
    "без attention и без трансформеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Джон Хопфилд в 1982 предложил Сеть Хопфилда. В 1993 нейронная система запоминания и сжатия исторических данных смогла решить задачу «очень глубокого обучения», в которой в рекуррентной сети разворачивалось более 1000 последовательных слоёв."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_hist.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекуррентные сети: введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекуррентные нейронные сети (RNN) — это класс нейронных сетей, которые хороши для моделирования последовательных данных, таких как временные ряды или естественный язык."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/types3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если схематично, слой RNN использует цикл for для итерации по упорядоченной по времени последовательности, храня при этом во внутреннем состоянии, закодированную информацию о шагах, которые он уже видел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_01.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если стандартные методы машинного обучения работают с так называемыми структурированными типами данных, с объектами, у которых фиксировано признаковое описание, которые подаются на вход моделям, то рекуррентные нейронные сети используются с неструктурированными данными. Например, в распознавании речи, генерации музыки, машинном переводе, а также в задаче выделения именованных сущностей, о которых мы поговорим более подробно в следующем видео. Данные в таких задачах представляют собой текст без предобработки, звуковой ряд и другие данные, не имеющие признаковых описаний. Давайте обсудим, каков принцип работы нейронной сети, и как именно таким нейронным сетям удается работать с неструктурированными данными. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы имеем возможность сохранять информацию, сформированную при обработке одного слова, и использовать ее, когда мы анализируем дальнейшие слова. Видно, что при рассмотрении каждого слова происходят одни и те же процедуры. Нейронная сеть получает очередное слово, учитывает предыдущую активацию, формирует ответ. Это действие, происходящее в рамках одной ячейки нейронной сети. Так как вся сеть представляет собой объединение однотипных ячеек, то нейронная сеть называется рекуррентной. Рекуррентные нейронные сети не ограничиваются случаем, когда мы каждому входу нейронной сети должны сопоставить некоторый ответ, после чего анализировать следующий вход. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВОТ ОТСЮДА МОЖНО ВЗЯТЬ КОД И ТЕКСТ** \n",
    "\n",
    "https://habr.com/ru/post/487808/\n",
    "\n",
    "https://nplus1.ru/material/2016/11/04/recurrent-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества** \n",
    "\n",
    "• Возможность обработки входных данных любой длины\n",
    "\n",
    "• Размер модели не увеличивается с размером входного сигнала\n",
    "\n",
    "• Расчет учитывает историческую информацию\n",
    "\n",
    "• Веса распределяются во времени \n",
    "\n",
    "**Недостатки**\n",
    "\n",
    "• Вычисление происходит медленно\n",
    "\n",
    "• Трудность доступа к информации, полученной давным-давно\n",
    "\n",
    "• Не может учитывать какие-либо будущие входные данные для текущего состояния"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже знакомы с основными принципами построения архитектур для самых простых нейросетей. Обычные модели состоят из группы слоёв и принимают на вход объект фиксированного размера — например, изображение. Каждый слой применяет к этому объекту какие-либо преобразования, и на выходе мы получаем результат — для классификации это будет метка класса. Но в некоторых областях машинного обучения нам хотелось бы иметь большую гибкость в типах данных, которые могли бы обрабатывать наши модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именно это позволяют делать рекуррентные нейронные сети (Recurrent Neural Network, RNN). Иногда их делят на несколько разновидностей: “one to one”, “one to many”, “many to one” и “many to many”.\n",
    "\n",
    "Модели RNN в основном используются в области обработки естественного языка и распознавания речи. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_0.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество типов рекуррентных нейронных сетей. Простейший случай рекуррентной сети — «one to one», когда у нас есть всего один вход и один выход нейронной сети. Более сложной является реализация «one to many», когда у нас есть всего один вход и нам необходимо сформировать несколько выходов. Такой тип нейронной сети актуален, когда мы говорим о генерации музыки или текстов. Мы задаем начальное слово или начальный звук, а дальше модель начинает самостоятельно генерировать выходы, в качестве входа к очередной ячейке рассматривая выход с прошлой ячейки нейронной сети. Если мы рассматриваем задачу классификации, то актуальна схема «many to one». Мы должны проанализировать все входы нейронной сети и только в конце определиться с классом. Схему «many to many», когда количество выходов равно количеству входов нейронной сети, мы рассмотрели на примере с определением части речи. Такой вид используется также в задачах NER, которые мы обсудим в следующем видео. Ну и последней разновидностью нейронных сетей является сеть вида «many to many», когда количество выходов нейронной сети не равно количеству входов. Это актуально, к примеру, в машинном переводе, когда одна и та же фраза может иметь разное количество слов в разных языках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ИЛИ ВОТ ТАК**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К архитектурам “one to one” можно отнести модели, которые мы рассматривали ранее, — с определённым размером входных и выходных данных. В случае “one to many” при заранее заданном типе и размере входного объекта можно получить вывод разной длины. Такой подход применяется в популярной задаче описания изображений (image captioning). Вариант “many to one” работает ровно наоборот — мы подаём на вход данные нефиксированного размера и получаем их чётко определённые характеристики. Так, например, можно по фрагменту видео определять вид активностей или действия, которые в нём происходят.\n",
    "\n",
    "И, наконец, “many to many” архитектуры имеют варьирующиеся размеры как входных, так и выходных данных. К решаемым ими задачам относятся машинный перевод (исходная и переведённая фразы могут быть разной длины) и покадровая классификация видео.\n",
    "\n",
    "Рекуррентные нейросети очень полезны даже при решении задач “one to one”. Рассмотрим популярную проблему распознавания рукописных цифр. Вместо того, чтобы просто сделать один прямой проход и сразу выдать решение, рекуррентная сеть быстро «просматривает» различные части изображения. В терминологии этот процесс называется «проблеск» (glimpse). Сделав несколько таких проблесков, модель принимает окончательное решение о том, какое число изображено на фотографии. Это позволяет существенно повысить точность распознавания и лучше контролировать процесс обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, как же устроена рекуррентная нейросеть? Внутри архитектуры располагается базовая рекуррентная ячейка. Модель принимает некоторые входные данные x и отправляет их в RNN, которая имеет скрытое внутреннее состояние. Это состояние обновляется каждый раз, когда в RNN поступают новые данные. Часто нам необходимо, чтобы RNN генерировала некоторый вывод на каждом временном отрезке. Поэтому после чтения входных данных и обновления скрытого состояния RNN будет создавать выходные данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем подробнее разобраться, что же происходит в загадочном зелёном прямоугольнике с надписью RNN. Внутри него мы вычисляем рекуррентное соотношение с помощью функции f, которая зависит от весов w. Чтобы найти новое состояние ht, мы берём предыдущее скрытое состояние ht⁻1, а также текущий ввод xt. Когда мы отправляем в модель следующие входные данные, полученное нами скрытое состояние ht передаётся в эту же функцию, и весь процесс повторяется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы генерировать вывод в каждый момент времени, в модель добавляются полносвязные слои, которые постоянно обрабатывают состояния ht и выдают основанные на них прогнозы. При этом функция f и веса w остаются неизменными.\n",
    "\n",
    "Самая простая реализация рекуррентной сети будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_4.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тангенс здесь используется для введения нелинейности в систему. \n",
    "\n",
    "Концепция скрытого состояния, которое периодически возвращается к самому себе, может показаться немного запутанной. Более понятным будет представить рекуррентный процесс в виде развёртывания вычислительного графа за несколько временных шагов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возвращение к вычислительным графам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первом временном шаге у нас есть первоначальное скрытое состояние h0. Обычно оно инициализируется нулём. Также на вход подаются данные xt и веса W — всё это отправляется в функцию f, которая вычисляет новое состояние h1. И с каждыми новыми входными данными мы повторяем процесс. \n",
    "\n",
    "Очередное новое состояние даёт нам выходные данные yi, с помощью которых мы можем посчитать потери Li. Просуммировав все Li, мы получим полные потери нашей модели. Такой подход используется в “many to many” архитектурах и наглядно показан на рисунке ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_5.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом мы точно также можем применять здесь метод обратного распространения ошибки и вычисления градиентного спуска, как мы это делали в обычных полносвязных нейросетях. \n",
    "\n",
    "В случае, если мы хотим подавать на вход данные фиксированного размера и получать разнообразный вывод (“one to many”), из нашего графа исчезнут разные варианты xi. Модель будет выглядеть следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_6.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один вход ко многим выходам может применяться, например, для генерации аудиозаписи. На вход подаем жанр музыки, который хотим получить, на выходе получаем последовательность аудиозаписи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С “many to one” ситуация будет обратная:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_7.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Много входов и один выход может применяться, если мы хотим оценить тональность рецензии. На вход подаем слова рецензии, на выходе получаем оценку ее тональности: позитивная рецензия или негативная."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также есть более сложные модели “many to many”, применяющиеся в машинном переводе, — они называются “sequence to sequence”. По сути это комбинация методов “many to one” и “one to many”, которые располагаются друг за другом и называются энкодер и декодер соответственно. Энкодер получает данные различной длины — например, предложение на английском языке. С помощью скрытых состояний он формирует из исходных данных вектор, который затем передаётся в декодер. Последний, в свою очередь, генерирует из полученного вектора выходные данные — исходную фразу, переведённую на другой язык.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_8.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее о том, как выглядят нейросети для машинного перевода и, в частности, Google-переводчик, можно прочитать в статье Google преодолевает барьер между человеческим и машинным переводом. А мы вернёмся к распознаванию изображений и поговорим об image captioning.\n",
    "\n",
    "ссылка https://www.reg.ru/blog/google-preodolevaet-barer-mezhdu-chelovecheskim-i-mashinnym-perevodom/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часто используемые функции активации наиболее часто используемые функции активации используется в модулях РНН описаны ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_14.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исчезающий/взрывающийся градиент (Vanishing/exploding gradient) - явления исчезающего и взрывающегося градиента часто встречаются в контексте RNN. Причина, по которой они происходят, заключается в том, что трудно уловить долгосрочные зависимости из-за мультипликативного градиента, который может экспоненциально уменьшаться/увеличиваться по отношению к числу слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентное отсечение (Gradient clipping) - метод, используемый для решения проблемы взрывающегося градиента, иногда возникающей при выполнении обратного распространения. Ограничивая максимальное значение градиента, это явление контролируется на практике.\n",
    "\n",
    "\n",
    "Типы вентилей Для решения проблемы исчезающего градиента в некоторых типах РНН используются специальные вентили, которые обычно имеют четко определенную цель. Они обычно отмечаются \\GammaΓ и равны (см. формулу)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_15.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где W, U, bW,U,b -коэффициенты, характерные для ворот и \\sigmaσ функция сигмоиды. Основные из них суммированы в таблице."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть этой задачи заключается в том, чтобы нейросеть составила текстовое описание фотографии. Для этого необходимо сначала классифицировать объекты на изображении, а затем передать результат (одну или несколько меток) в языковую рекуррентную модель, которая сможет составить из них осмысленную фразу. При этом мы действуем точно так же, как в случае с обычной языковой моделью: преобразуем метку изображения в вектор, который обрабатывается декодером."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_9.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы рекуррентная сеть понимала, где именно начинается предложение, во время обучения на её вход подаётся стартовый опознавательный знак (<START> token). Для построения фразы используется заранее подготовленный словарь, например, из английских слов — и он может быть довольно большим.\n",
    "\n",
    "При переходе на каждое следующее скрытое состояние мы сохраняем как уже сгенерированные слова, так и информацию об изображении. В конце предложения в нейросеть отправляется финальный токен (<END>). Во время тестирования модель уже самостоятельно определяет, где должно начинаться и заканчиваться описание изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_10.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно подобные архитектуры создаются с помощью контролируемого обучения (supervized learning) — это означает, что в обучающих датасетах уже присутствуют как изображения, так и описания для них. Наиболее популярным и самым большим набором данных является Microsoft COCO. Помимо image captioning он также применяется для сегментации, поиска ключевых точек и даже построения трёхмерной модели человека на основе его позы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning + Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, основанные на внимании (attention) немного более продвинутые, чем обычные нейросети. Они могут концентрироваться на отдельных частях изображения, что позволяет избежать зашумления данных.\n",
    "\n",
    "Идея состоит в том, что свёрточная сеть теперь будет генерировать не один вектор, описывающий всё изображение, а набор векторов для нескольких участков исходного снимка. В дополнение к работе со словарём на каждом временном шаге модель также производит распределение по точкам на изображении, которые она обрабатывает в данный момент. Это позволяет ей научиться находить наиболее важные участки, на которых необходимо фокусироваться.\n",
    "\n",
    "После обучения модели можно увидеть, что она как бы переносит своё внимание по изображению для каждого генерируемого слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют также понятия мягкого и жёсткого внимания (soft and hard attention). При мягком внимании мы берём взвешенную комбинацию признаков по всему изображению, тогда как в случае жёсткого внимания мы заставляем модель выбирать только один небольшой участок для обработки на каждом временном шаге. При этом жёсткое внимание, строго говоря, не является дифференцируемой функцией. Поэтому для обучения такой модели необходимо использовать более изощрённые приёмы, чем обычное обратное распространение ошибки. Мы подробнее затронем эту тему в одной из следующих лекций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также нейросети, основанные на внимании, повсеместно используются для ответов на визуальные вопросы (Visual Question Answering). Цель этой задачи — обучить модель отвечать на вопрос по изображению. Например, она должна уметь не только называть сами объекты на фотографии, но и считать их, распознавать цвета и оценивать расположение относительно друг друга. Мы уже рассказывали о подобных архитектурах в статье о том, как такие нейросети могут помочь незрячим людям и о нейро-символическом мышлении.\n",
    "\n",
    "ссылка https://www.reg.ru/blog/nejroset-opisyvaet-mir-nezryachim-lyudyam/\n",
    "ссылка https://www.reg.ru/blog/uchim-nejroseti-rassuzhdat-o-tom-chto-oni-vidyat/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/L08_13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы знаете о том, что означает понятие внимания в машинном обучении. Также мы разобрали наиболее важные и набирающие популярность задачи, в которых используются рекуррентные нейронные сети. Если у вас возникли какие-либо вопросы — задавайте их в комментариях, мы обязательно ответим. В следующей лекции обсудим ещё несколько применений нейросетей в компьютерном зрении и подробно рассмотрим задачи детектирования и сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**вот отсюда можно взять слайды и текст можно взять, надо**\n",
    "\n",
    "http://www.machinelearning.ru/wiki/images/0/05/Mmta18-rnn.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример кода на Python с использованием библиотеки Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Импорты\n",
    " import numpy as np\n",
    " from keras.preprocessing import sequence\n",
    " from keras.models import Sequential\n",
    " from keras.layers import Dense, Activation, Embedding\n",
    " from keras.layers import LSTM\n",
    " from keras.datasets import imdb\n",
    " \n",
    " # Устанавливаем seed для обеспечения повторяемости результатов\n",
    " np.random.seed(42)\n",
    " \n",
    " # Указываем количество слов из частотного словаря, которое будет использоваться (отсортированы по частоте использования)\n",
    " max_features = 5000\n",
    " \n",
    " # Загружаем данные (датасет IMDB содержит 25000 рецензий на фильмы с правильным ответом для обучения и 25000 рецензий на фильмы с правильным ответом для тестирования)\n",
    " (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words = max_features)\n",
    " \n",
    " # Устанавливаем максимальную длину рецензий в словах, чтобы они все были одной длины\n",
    " maxlen = 80\n",
    " \n",
    " # Заполняем короткие рецензии пробелами, а длинные обрезаем\n",
    " X_train = sequence.pad_sequences(X_train, maxlen = maxlen)\n",
    " X_test = sequence.pad_sequences(X_test, maxlen = maxlen)\n",
    " \n",
    " # Создаем модель последовательной сети\n",
    " model = Sequential()\n",
    " # Добавляем слой для векторного представления слов (5000 слов, каждое представлено вектором из 32 чисел, отключаем входной сигнал с вероятностью 20% для предотвращения переобучения)\n",
    " model.add(Embedding(max_features, 32, dropout = 0.2))\n",
    " # Добавляем слой долго-краткосрочной памяти (100 элементов для долговременного хранения информации, отключаем входной сигнал с вероятностью 20%, отключаем рекуррентный сигнал с вероятностью 20%)\n",
    " model.add(LSTM(100, dropout_W = 0.2, dropout_U = 0.2))\n",
    " # Добавляем полносвязный слой из 1 элемента для классификации, в качестве функции активации будем использовать сигмоидальную функцию\n",
    " model.add(Dense(1, activation = 'sigmoid'))\n",
    " \n",
    " # Компилируем модель нейронной сети\n",
    " model.compile(loss = 'binary_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['accuracy'])\n",
    " \n",
    " # Обучаем нейронную сеть (данные для обучения, ответы к данным для обучения, количество рецензий после анализа которого будут изменены веса, число эпох обучения, тестовые данные, показывать progress bar или нет)\n",
    " model.fit(X_train, y_train, \n",
    "           batch_size = 64,\n",
    "           nb_epoch = 7,\n",
    "           validation_data = (X_test, y_test),\n",
    "           verbose = 1)\n",
    " \n",
    " # Проверяем качество обучения на тестовых данных (если есть данные, которые не участвовали в обучении, лучше использовать их, но в нашем случае таковых нет)\n",
    " scores = model.evaluate(X_test, y_test, batch_size = 64)\n",
    " print('Точность на тестовых данных: %.2f%%' % (scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**МОЖНО ЕЩЕ ВСТАВИТЬ ЧТО-ТО ОТСЮДА**\n",
    "\n",
    "**Языковое моделирование с помощью управляемых сверточных сетейА еще можно что-то взять отсюда**\n",
    "\n",
    "https://habr.com/ru/post/537968/\n",
    "\n",
    "**RNN: может ли нейронная сеть писать как Лев Толстой? (Спойлер: нет)**\n",
    "\n",
    "https://habr.com/ru/post/342738/\n",
    "\n",
    "**Поиск звуковых аномалий**\n",
    "\n",
    "https://habr.com/ru/post/315800/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L12 RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-011.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-012.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-013.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-013-1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-014.gif\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем обрабатывать последовательность векторов x, применяя рекуррентную формулу на каждом временном шаге:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-017.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-023.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-026.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-028.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-030.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-037.gif\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-045.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вперед через всю последовательность для вычисления потерь, затем назад через всю последовательность для вычисления градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-046.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run вперед и назад через куски последовательности вместо целой последовательности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-047.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переносите скрытые состояния вперед во времени навсегда, но только назад на некоторое меньшее количество шагов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-048.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-050.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-054.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-055.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-058.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-059.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-061.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-063.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-064.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-065.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-066.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-067.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-068.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-069.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-059.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-061.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-063.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-064.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-065.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-066.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-067.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-068.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-069.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-071.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-079.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We added two special rules to vocabulary <start> and <end>\n",
    "    \n",
    "Мы добавили в словарь две специальные команды <start> и <end>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда дает очень хорошие описания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-080.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-083.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-086.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-090.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-094.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still don't need to propagate through the weights (can potentially raise problems)\n",
    "\n",
    "Но нам все равно не нужно распространяться через веса (потенциально это может вызвать проблемы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-095.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но в Vanilla RNN был только один путь, но в LSTM есть highway для сокращения информации  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-098.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-101.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Были предприняты попытки с помощью эволюционного поиска найти более оптимальный юнит для RNN. \n",
    "\n",
    "*Поиск шел в пространстве формул для обновления состояний*\n",
    "\n",
    "Тем не менее, никакого значительного улучшения качва достигнуто не было относительно LSTM, именно поэтому LSTM является оптимальой архитектурой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-102.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture12-103.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rnn allows a lot of flexibility in architecture design\n",
    "* Vanilla RNNs are simple, but don't work very well\n",
    "* Common to use LSTM or GRU: additive interaction improve gradient flow\n",
    "* Backward flow of gradients in RNN are explode or vanish\n",
    "\n",
    "exploding is controlled with gradient clipping\n",
    "\n",
    "vanishing is controlled additive interactions (LSTM)\n",
    "\n",
    "* * Better/simpler architectures are hot topic of current research \n",
    "* Better understanding (both theoretical and empirical) is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rnn обеспечивает большую гибкость в архитектурном проектировании\n",
    "* Vanilla RNN намного проще, однако работают не очень хорошо\n",
    "* LSTM или GRU: аддитивное взаимодействие улучшает градиентный поток\n",
    "* Обратный поток градиентов в RNN делает explode или vanish\n",
    "\n",
    "explode управляется с помощью градиентного отсечения\n",
    "\n",
    "vanish - это контролируемые аддитивные взаимодействия (LSTM)\n",
    "\n",
    "* Лучшие/более простые архитектуры являются актуальной темой текущих исследований\n",
    "* Необходимо лучшее понимание (как теоретическое, так и эмпирическое)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L13 Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-Sequence with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-011.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-to-Sequence with RNNs and A,en.on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-017.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-020.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-021.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-025.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-028.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-031.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-036.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-037.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning with RNNs and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-038.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-039.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X, Attend, and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-044.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-045.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-048.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-050.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-056.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-063.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-070.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-072.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Self-Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-073.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-074.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-075.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Ways of Processing Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-084.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-085.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-092.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-094.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-095.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-100.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-104.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L08_RNN/img/498_FA2019_lecture13-105.png\" width=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
