{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (RNN)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ª–µ–∫—Ü–∏—è—Ö –º—ã —Ä–∞–±–æ—Ç–∞–ª–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ **—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã**, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ **–≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏**.\n",
    "\n",
    "–ï—Å–ª–∏ —ç—Ç–æ **—Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**, —Ç–æ –º—ã –∑–∞—Ä–∞–Ω–µ–µ –∑–Ω–∞–µ–º –¥–ª–∏–Ω—É –≤–µ–∫—Ç–æ—Ä-–æ–ø–∏—Å–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞, –∞ —Ç–∞–∫–∂–µ —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏: –æ–¥–Ω–æ —ç—Ç–æ —á–∏—Å–ª–æ –∏–ª–∏ –≤–µ–∫—Ç–æ—Ä.\n",
    "\n",
    "–í–µ—Ä–Ω–æ —ç—Ç–æ –∏ –ø—Ä–æ **–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è** ‚Äî –æ–±—ã—á–Ω–æ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —É—á–∏—Ç—Å—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è. –î–∞, –º—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è —Å–ø–æ—Å–æ–±–Ω–∞ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø–æ—á—Ç–∏ –ª—é–±–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, –Ω–æ –¥–æ–±–∏–≤–∞–µ–º—Å—è –º—ã —ç—Ç–æ–≥–æ –∑–∞ —Å—á–µ—Ç –≤—Å—Ç–∞–≤–∫–∏ —Å–ª–æ–µ–≤ **global pooling**, –∫–æ—Ç–æ—Ä—ã–µ  –ø—Ä–∏–≤–æ–¥—è—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –ø–æ–ª—É—á–µ–Ω–Ω–æ–µ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —á–∞—Å—Ç—å—é –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, –∫ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–Ω–∞–∫–æ –¥–∞–ª–µ–∫–æ **–Ω–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞**. –ö –ø—Ä–∏–º–µ—Ä—É, **—Ç–µ–∫—Å—Ç—ã**. –í–æ–∑—å–º–µ–º –≤—Å–µ –∞–±–∑–∞—Ü—ã –∏–∑ \"–í–æ–π–Ω—ã –∏ –ú–∏—Ä–∞\". –ö–∞–∫–∏–µ-—Ç–æ –±—É–¥—É—Ç –±–æ–ª—å—à–µ, –∫–∞–∫–∏–µ-—Ç–æ –º–µ–Ω—å—à–µ. –ê –µ—â—ë –Ω–∞–º –≤–∞–∂–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\n",
    "\n",
    "–ò–ª–∏ –µ—Å–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–±–∑–∞—Ü–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ **–∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ**? –¢–æ, —á—Ç–æ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã. –ê–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Ç–∞–∫–∂–µ –≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø–æ –¥–∞–Ω–Ω—ã–º –æ –∫—É—Ä—Å–µ –≤–∞–ª—é—Ç—ã –∑–∞ –ø—Ä–æ—à–ª—ã–π –≥–æ–¥ —Å–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å **–∫—É—Ä—Å –≤–∞–ª—é—Ç—ã –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –º–µ—Å—è—Ü** –ø–æ –¥–Ω—è–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/time_series_data.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–æ, –º–æ–∂–µ—Ç, —Å–ø—Ä–∞–≤–∏–º—Å—è —É–∂–µ –∏–º–µ—é—â–∏–º–∏—Å—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–µ–æ—Ä–∏—è –∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤:\n",
    "\n",
    "**–¢—Ä–µ–Ω–¥** ‚Äî –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞, –æ–ø–∏—Å—ã–≤–∞—é—â–∞—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Ä—è–¥–∞.\n",
    "\n",
    "**–°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å** ‚Äî –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞, –æ–±–æ–∑–Ω–∞—á–∞–µ–º–∞—è –∫–∞–∫ $Q$, –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è —Ä—è–¥–∞.\n",
    "\n",
    "**–û—à–∏–±–∫–∞ (random noise)** ‚Äî –Ω–µ–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è —Å–ª—É—á–∞–π–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞, –æ–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–µ—Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—ä—è—Å–Ω–∏–º—ã–µ –¥—Ä—É–≥–∏–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏.\n",
    "\n",
    "$$\\large y_i = T_i + Q_i + œµ_i$$\n",
    "\n",
    "**–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è** ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ –≤–µ–ª–∏—á–∏–Ω –æ–¥–Ω–æ–≥–æ —Ä—è–¥–∞. –≠—Ç–æ –æ–¥–∏–Ω –∏–∑ —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –≤ –∞–Ω–∞–ª–∏–∑–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞. –ß—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä—è–¥–æ–º –∏ –µ—ë —Å–¥–≤–∏–Ω—É—Ç–æ–π –∫–æ–ø–∏–µ–π –æ—Ç –≤–µ–ª–∏—á–∏–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å–¥–≤–∏–≥–∞. –°–¥–≤–∏–≥ —Ä—è–¥–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –ª–∞–≥–æ–º.\n",
    "\n",
    "**–ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è** ‚Äî –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –ª–∞–≥–∞—Ö, –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —É—á–∞—Å—Ç–∫–∏ —Å–∏–≥–Ω–∞–ª–∞. –ì—Ä–∞—Ñ–∏–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **–∫–æ—Ä—Ä–µ–ª–æ–≥—Ä–∞–º–º–æ–π**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/autocorrelation_graph.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/693562/\">–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π (1 —á–∞—Å—Ç—å)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–∏–º–µ—Ä**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ–∑—å–º–µ–º –¥–∞—Ç–∞—Å–µ—Ç [Air Passengers üõ†Ô∏è[doc]](https://www.kaggle.com/rakannimer/air-passengers), –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–∞—Å—Å–∞–∂–∏—Ä–æ–≤ –∑–∞ –∫–∞–∂–¥—ã–π –º–µ—Å—è—Ü.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/airline-passengers.csv\"\n",
    ")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = dataset.iloc[:, 1:2].values  # transform dataframe to numpy.array\n",
    "# plotting\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(training_data, label=\"Airline Passangers Data\")\n",
    "plt.title(\"Number of passengers per month\")\n",
    "plt.ylabel(\"#passengers\")\n",
    "plt.xlabel(\"Month\")\n",
    "labels_to_display = [i for i in range(training_data.shape[0]) if i % 12 == 0]\n",
    "plt.xticks(labels_to_display, dataset[\"Month\"][labels_to_display])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–°–¥–µ–ª–∞–µ–º —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–µ–µ**\n",
    "\n",
    "–ú—ã –º–æ–∂–µ–º –≤–∏–¥–µ—Ç—å —á–µ—Ç–∫—É—é —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –∏ —Ç–µ–Ω–¥–µ–Ω—Ü–∏—é –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –∑–Ω–∞—á–µ–Ω–∏–π.\n",
    "\n",
    "–¢–µ–Ω–¥–µ–Ω—Ü–∏—è –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å —è–≤–ª—è—é—Ç—Å—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫ –ª—é–±–æ–º—É –Ω–∞—à–µ–º—É –ø—Ä–æ–≥–Ω–æ–∑—É. –û–Ω–∏ –ø–æ–ª–µ–∑–Ω—ã, –Ω–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–¥–∞–ª–µ–Ω—ã, —á—Ç–æ–±—ã –∏–∑—É—á–∏—Ç—å –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏–≥–Ω–∞–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–º–æ—á—å —Å–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã.\n",
    "\n",
    "–ú–Ω–æ–≥–∏–µ –º–µ—Ç–æ–¥—ã –∏ –º–æ–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è—Ö –æ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏ —Ä—è–¥–∞, –ø–æ—ç—Ç–æ–º—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏ –¥–∞–≤–∞–π—Ç–µ –ø—Ä–æ–≤–µ–¥–µ–º [–æ–±–æ–±—â–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç –î–∏–∫–∫–∏-–§—É–ª–ª–µ—Ä–∞ üìö[wiki]](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D1%81%D1%82_%D0%94%D0%B8%D0%BA%D0%B8_%E2%80%94_%D0%A4%D1%83%D0%BB%D0%BB%D0%B5%D1%80%D0%B0) –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –µ–¥–∏–Ω–∏—á–Ω—ã—Ö –∫–æ—Ä–Ω–µ–π. –î–ª—è —ç—Ç–æ–≥–æ –≤ –º–æ–¥—É–ª–µ `statsmodels` –µ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏—è `adfuller()`.\n",
    "\n",
    "**–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º** –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è —Ä—è–¥, —É –∫–æ—Ç–æ—Ä–≥–æ **—Å—Ä–µ–¥–Ω–µ–µ, –¥–∏—Å–ø–µ—Ä—Å–∏—è –∏ –∏ –∫–æ–≤–∞—Ä–∏–∞—Ü–∏—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "test = sm.tsa.adfuller(training_data)\n",
    "print(\"adf: \", test[0])\n",
    "print(\"p-value: \", test[1])\n",
    "print(\"Critical values: \", test[4])\n",
    "if test[0] > test[4][\"5%\"]:\n",
    "    print(\"The time series is not stationary\")\n",
    "else:\n",
    "    print(\"The time series is stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ü–µ–Ω–∏–º —Ç—Ä–µ–Ω–¥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the trend\n",
    "trend = dataset[\"Passengers\"].rolling(window=12).mean()\n",
    "trend.plot(figsize=(12, 2))\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Trend\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã —É–±—Ä–∞—Ç—å —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å, –º—ã –º–æ–∂–µ–º –≤–∑—è—Ç—å —Å–µ–∑–æ–Ω–Ω—É—é —Ä–∞–∑–Ω–∏—Ü—É, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º—ã–º —Å–µ–∑–æ–Ω–Ω–æ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä—è–¥–∞–º.\n",
    "\n",
    "–ü–µ—Ä–∏–æ–¥ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–¥–∏–Ω –≥–æ–¥ (12 –º–µ—Å—è—Ü–µ–≤). –ü—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–¥ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–µ–∑–æ–Ω–Ω–æ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the seasonality\n",
    "seasonality = dataset[\"Passengers\"] - trend\n",
    "seasonality.plot(figsize=(12, 2))\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Seasonality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sm.tsa.adfuller(seasonality.dropna())\n",
    "print(\"adf: \", test[0])\n",
    "print(\"p-value: \", test[1])\n",
    "print(\"Critical values: \", test[4])\n",
    "if test[0] > test[4][\"5%\"]:\n",
    "    print(\"The time series is not stationary\")\n",
    "else:\n",
    "    print(\"The time series is stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å** (Autoregression method (AR))\n",
    "\n",
    "–õ–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å, –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ —è–≤–ª—è–µ—Ç—Å—è —Å—É–º–º–æ–π –ø—Ä–æ—à–ª—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, —É–º–Ω–æ–∂–µ–Ω–Ω—ã—Ö –Ω–∞ —á–∏—Å–ª–æ–≤–æ–π –º–Ω–æ–∂–∏—Ç–µ–ª—å.\n",
    "\n",
    "$$\\large X_t = C + œï_{t-1}X_{t-1}+œï_{t-2}X_{t-2}...+ œµ_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ú–µ—Ç–æ–¥ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ** (Moving average method (MA))\n",
    "\n",
    "–†–∞—Å—á–µ—Ç —Ç–æ—á–µ–∫ –ø—É—Ç–µ–º —Å–æ–∑–¥–∞–Ω–∏—è —Ä—è–¥–∞ –≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤ –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:\n",
    "\n",
    "$$\\large \\text{WWMA}_t=  \\frac {w_{t-1}p_{t-1} + w_{t-2}p_{t-2} ... w_{t-n+1}p_{t-n+1}} {w_{t-1} + w_{t-2} ... w_{n}} =  \\frac {\\sum ^ {n-1} _{i= 0} w_{t-i}p_{t-i}}{\\sum ^ {n-1} _{i= 0}w_{t-i}} $$\n",
    "\n",
    "–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ —á–∞—Å—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∞–∫—Ü–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/moving_average_method.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/693562/\">–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π (1 —á–∞—Å—Ç—å)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARMA**\n",
    "\n",
    "–î–ª—è **—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö** –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å **ARMA(p,q)**, –∫–æ—Ç–æ—Ä–∞—è –≤ –æ–±—â–µ–º –≤–∏–¥–µ –≤—ã–≥–ª—è–¥–∏—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "$$\\large X_t = œï_{t-1}X_{t-1} + ‚ãØ + œï_{t-p}X_{t-p}\t + Œµ_t + w_{t-1}p_{t-1} + ‚ãØ + w_{t-q}p_{t-q},$$\n",
    "\n",
    "–≥–¥–µ $œï_t$ ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π —á–∞—Å—Ç–∏ –º–æ–¥–µ–ª–∏, $Œµ_t$ ‚Äî –∑–Ω–∞—á–µ–Ω–∏—è –æ—à–∏–±–∫–∏ (–ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º–∏ —Å–ª—É—á–∞–π–Ω—ã–º–∏ –≤–µ–ª–∏—á–∏–Ω–∞–º–∏ –∏–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å –Ω—É–ª–µ–≤—ã–º —Å—Ä–µ–¥–Ω–∏–º), $w_j$ ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ.\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–µ —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ: –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å–∏–æ–Ω–Ω—É—é –∏ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ, –∫–æ—Ç–æ—Ä—ã–µ –≤ –º–æ–¥–µ–ª–∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω—ã $p$ –∏ $q$:\n",
    "\n",
    "* $p$ ‚Äî –ø–æ—Ä—è–¥–æ–∫ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏. –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –±—É–¥–µ—Ç –ª–∏ –æ—á–µ—Ä–µ–¥–Ω–æ–π —ç–ª–µ–º–µ–Ω—Ç —Ä—è–¥–∞ –±–ª–∏–∑–æ–∫ –∫ –∑–Ω–∞—á–µ–Ω–∏—é $X$, –µ—Å–ª–∏ –∫ –Ω–µ–º—É –±—ã–ª–∏ –±–ª–∏–∑–∫–∏ $p$ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "* $q$ ‚Äî –ø–æ—Ä—è–¥–æ–∫ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ. –ü–æ–∑–≤–æ–ª—è–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫–∞–∫ –ª–∏–Ω–µ–π–Ω—É—é –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –Ω–∞–±–ª—é–¥–∞–≤—à–∏—Ö—Å—è —Ä–∞–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–π –æ—à–∏–±–æ–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARIMA**\n",
    "\n",
    "–ï—Å–ª–∏ —Ä—è–¥ **–ø–æ—Å–ª–µ –≤–∑—è—Ç–∏—è d –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–∑–Ω–æ—Å—Ç–µ–π** —Å–≤–æ–¥–∏—Ç—Å—è –∫ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–º—É, —Ç–æ –¥–ª—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ, –æ–±–æ–∑–Ω–∞—á–∞–µ–º—É—é –∫–∞–∫ **ARIMA(p,d,q)**:\n",
    "\n",
    "$$ \\large (Œî^dX_t) = \\sum ^p _{t=1} œï_t(Œî^dX_{t-1}) + Œµ_t + \\sum ^q _{j=1} w_j(Œî^d Œµ_{t-j}) $$\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç—Ä–∏ —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ: –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å–∏–æ–Ω–Ω—É—é, —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏ **–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é**, –∫–æ—Ç–æ—Ä—ã–µ –≤ –º–æ–¥–µ–ª–∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω—ã $p, d$ –∏ $q$. –¢.–µ. –¥–æ–±–∞–≤–∏–ª–æ—Å—å:\n",
    "\n",
    "* $d$ ‚Äî –ø–æ—Ä—è–¥–æ–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç —Ä—è–¥–∞ –±–ª–∏–∑–æ–∫ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é –∫ $d$  –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∑–Ω–∞—á–µ–Ω–∏—è–º, –µ—Å–ª–∏ —Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –Ω–∏–º–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞.\n",
    "\n",
    "–†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç–∏: Seasonable ARIMA, ARIMAX –∏ –ø—Ä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ‚úèÔ∏è –û–±—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ Machinelearning Mastery](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã**\n",
    "\n",
    "* –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è **ACF** –ø–æ–º–æ–∂–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å $q$, —Ç. –∫. –ø–æ –µ–µ –∫–æ—Ä—Ä–µ–ª–æ–≥—Ä–∞–º–º–µ –º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤, —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω—ã—Ö –æ—Ç $0$, –≤ –º–æ–¥–µ–ª–∏ MA.\n",
    "\n",
    "* –ß–∞—Å—Ç–∏—á–Ω–æ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è **PACF** –ø–æ–º–æ–∂–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å $p$, —Ç. –∫. –ø–æ –µ–µ –∫–æ—Ä—Ä–µ–ª–æ–≥—Ä–∞–º–º–µ –º–æ–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞, —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–Ω–æ–≥–æ –æ—Ç $0$, –≤ –º–æ–¥–µ–ª–∏ **AR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first-order difference\n",
    "dfOrigin = dataset[\"Passengers\"]\n",
    "df1 = dfOrigin.diff(1).dropna()\n",
    "# second-order difference\n",
    "df2 = df1.diff(1).dropna()\n",
    "# third-order difference\n",
    "df3 = df2.diff(1).dropna()\n",
    "\n",
    "# plot three curves and check the stationary\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, sharex=True)\n",
    "\n",
    "ax1.plot(df1)\n",
    "ax1.set_title(\"first order\")\n",
    "\n",
    "ax2.plot(df2)\n",
    "ax2.set_title(\"second order\")\n",
    "\n",
    "ax3.plot(df3)\n",
    "ax3.set_title(\"third order\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def check_stationary(ts_data):\n",
    "    df_test = sm.tsa.adfuller(ts_data)\n",
    "    output = pd.Series(\n",
    "        df_test[0:4], index=[\"Test statistic\", \"p-value\", \"used_lag\", \"NOBS\"]\n",
    "    )\n",
    "    print(output)\n",
    "\n",
    "\n",
    "check_stationary(df1)\n",
    "check_stationary(df2)\n",
    "check_stationary(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ö–æ—Ä—Ä–µ–ª–æ–≥—Ä–∞–º–º–∞**\n",
    "\n",
    "–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –∫–∞–∂–¥–æ–≥–æ –æ—Ç—Å—Ä–æ—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –µ–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(dataset[\"Passengers\"].values.squeeze(), lags=20, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(dataset[\"Passengers\"], lags=20, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ –≥—Ä–∞—Ñ–∏–∫–µ –ø–æ–∫–∞–∑–∞–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–ø–∞–∑–¥—ã–≤–∞–Ω–∏—è –ø–æ –æ—Å–∏ $X$ –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø–æ –æ—Å–∏ $Y$ –º–µ–∂–¥—É $-1$ –∏ $1$ –¥–ª—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ –∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª–∞–≥–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –¢–æ—á–∫–∏ –Ω–∞–¥ —Å–∏–Ω–µ–π –æ–±–ª–∞—Å—Ç—å—é —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å. –≠—Ç–æ –∑–æ–Ω–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–∞ $0.05$.\n",
    "\n",
    "* –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è $1$ –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç—Å—Ç–∞–≤–∞–Ω–∏—è $0$ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ $100\\%$ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —Å —Å–∞–º–∏–º —Å–æ–±–æ–π.\n",
    "* –í—Ç–æ—Ä–∞—è —Ç–æ—á–∫–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ä–∞–π–æ–Ω–µ $0.75$, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å–ª–µ–¥—É—é—â–∞—è —Ç–æ—á–∫–∞ –Ω–∞ $75\\%$ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "data = dataset.iloc[:, 1:2]\n",
    "result = seasonal_decompose(\n",
    "    x=data, model=\"multiplicative\", extrapolate_trend=\"freq\", period=12\n",
    ")\n",
    "\n",
    "fig = result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –±–∏–±–ª–∏–æ—Ç–µ—á–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π ARIMA. –†–∞–∑–æ–±—å—ë–º –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ—Å—Ç–∞–≤–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –û–±—É—á–∏–º –º–æ–¥–µ–ª—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Month\"] = pd.to_datetime(dataset[\"Month\"], infer_datetime_format=True)\n",
    "dataset = dataset.set_index([\"Month\"])\n",
    "dataset.index = pd.DatetimeIndex(dataset.index.values, freq=dataset.index.inferred_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = (\n",
    "    dataset[0 : int(len(data) * 0.9)],\n",
    "    dataset[int(len(data) * 0.9) :],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "model = ARIMA(\n",
    "    train_data, order=(12, 2, 13), freq=dataset.index.inferred_freq\n",
    ")  # (p,d,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_data = train_data.index[-1] + pd.DateOffset(months=1)\n",
    "end_data = test_data.index[-1] + pd.DateOffset(months=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_fit.predict(start=start_data, end=end_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_data[\"Passengers\"], color=\"green\", label=\"Train\")\n",
    "plt.plot(test_data[\"Passengers\"], color=\"red\", label=\"Real\")\n",
    "plt.plot(predictions, color=\"blue\", label=\"Predicted\")\n",
    "plt.title(\"ARIMA with optimal parameters\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ü–µ–Ω–∏–º –º–µ—Ç—Ä–∏–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "# report performance\n",
    "mse = mean_squared_error(test_data[\"Passengers\"], predictions[:15])\n",
    "print(\"MSE: \" + str(mse))\n",
    "mae = mean_absolute_error(test_data[\"Passengers\"], predictions[:15])\n",
    "print(\"MAE: \" + str(mae))\n",
    "rmse = math.sqrt(mean_squared_error(test_data[\"Passengers\"], predictions[:15]))\n",
    "print(\"RMSE: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ‚úèÔ∏è Forecasting Time Series with Auto-Arima](https://www.alldatascience.com/time-series/forecasting-time-series-with-auto-arima/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –µ—Å—Ç—å, –æ–¥–Ω–∞–∫–æ –º–æ–∂–Ω–æ –ª–∏ –µ–≥–æ –∫–∞–∫-—Ç–æ —É–ª—É—á—à–∏—Ç—å? –î–∞ —Ç–∞–∫, —á—Ç–æ–±—ã –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "stepwise_model = auto_arima(\n",
    "    train_data,\n",
    "    start_p=1,\n",
    "    start_q=1,\n",
    "    max_p=3,\n",
    "    max_q=3,\n",
    "    m=12,\n",
    "    start_P=0,\n",
    "    seasonal=True,\n",
    "    d=1,\n",
    "    D=1,\n",
    "    trace=True,\n",
    "    error_action=\"ignore\",\n",
    "    suppress_warnings=True,\n",
    "    stepwise=True,\n",
    ")\n",
    "print(stepwise_model.aic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_forecast = stepwise_model.predict(start=start_data, n_periods=(len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_data[\"Passengers\"], color=\"green\", label=\"Train\")\n",
    "plt.plot(test_data[\"Passengers\"], color=\"red\", label=\"Real\")\n",
    "plt.plot(future_forecast, color=\"blue\", label=\"Predicted\")\n",
    "\n",
    "plt.title(\"ARIMA with optimal parameters\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Passengers\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_data[\"Passengers\"], future_forecast[: len(test_data)])\n",
    "print(\"MSE: \" + str(mse))\n",
    "mae = mean_absolute_error(test_data[\"Passengers\"], future_forecast[: len(test_data)])\n",
    "print(\"MAE: \" + str(mae))\n",
    "rmse = math.sqrt(\n",
    "    mean_squared_error(test_data[\"Passengers\"], future_forecast[: len(test_data)])\n",
    ")\n",
    "print(\"RMSE: \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ª—É—á–∏–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–∂–µ, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –≥–æ—Ä–∞–∑–¥–æ –±—ã—Å—Ç—Ä–µ–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ –Ω–∞ **train-val-test**.\n",
    "\n",
    "–ï—Å–ª–∏ –º—ã –ø–æ–¥–µ–ª–∏–º —Ä—è–¥ –Ω–∞ –æ—Ç—Ä–µ–∑–∫–∏, —Ç–æ—á–∫–∏ —Å–∫–ª–µ–π–∫–∏ –±—É–¥—É—Ç –ª–µ–≥–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –ª–∏–±–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º, –ª–∏–±–æ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –ø–æ –æ—Ç—Ä–µ–∑–∫—É. –ù—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∫—Ä—É–ø–Ω—ã–µ –æ—Ç—Ä–µ–∑–∫–∏ —Ä—è–¥–∞. –ò –ø–æ–º–Ω–∏—Ç—å –æ —Ç–æ–º, —á—Ç–æ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–µ–ª—å–∑—è ‚Äî –µ—Å—Ç—å \"–ø—Ä–æ—à–ª–æ–µ\" –∏ \"–±—É–¥—É—â–µ–µ\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/fixed_partitioning.jpg\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://yuting3656.github.io/yutingblog/coursera-tensorflow-developer-professional-certificate/sequences-time-series-and-prediction/week01-03\">Coursera: Sequences, Time Series and Prediction with TensorFlow</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–º–∏. –ò –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –¥—Ä. –£—Å–ª–æ–∂–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ (–∏–ª–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å–≤–µ–¥–µ–Ω–∏—è –∫ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–µ –Ω–µ–ø–æ–º–µ—Ä–Ω–æ –≤—ã—Å–æ–∫–∞). –ó–∞—á–∞—Å—Ç—É—é –≤ —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è –º–æ–≥—É—Ç –ø–æ–º–æ—á—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/standard_solar_spectrum.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.researchgate.net/publication/280041456_THERMAL_PERFORMANCE_OF_WHITE_SOLAR-REFLECTIVE_PAINTS_FOR_COOL_ROOFS_AND_THE_INFLUENCE_ON_THE_THERMAL_COMFORT_AND_BUILDING_ENERGY_USE_IN_HOT_CLIMATES?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ\">Research Gate</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ –±—É—Ä–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä**, –æ –∫–æ—Ç–æ—Ä–æ–π –±—É–¥–µ—Ç —Ä–∞—Å—Å–∫–∞–∑–∞–Ω–æ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –ª–µ–∫—Ü–∏–∏, **—Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏** –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å –≤ —à–∏—Ä–æ–∫–æ–º –ø–µ—Ä–µ—á–Ω–µ –∑–∞–¥–∞—á, –æ—Ç **—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏** –¥–æ **–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π** –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –û–±—â–µ–µ –¥–ª—è –∑–∞–¥–∞—á ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞ (**—Ç–æ–∫–µ–Ω–∞**), –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥—Ä—É–≥–∏—Ö —á–∞—Å—Ç–µ–π.\n",
    "\n",
    "–í –Ω–∞—Å—Ç–æ—è—â–∏–π –º–æ–º–µ–Ω—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏ –¥–µ—Ä–∂–∞—Ç –ø–µ—Ä–≤–µ–Ω—Å—Ç–≤–æ **–≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤**, –∞ —Ç–∞–∫–∂–µ —è–≤–ª—è—é—Ç—Å—è —á–∞—Å—Ç—å—é –≥–∏–±—Ä–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "–•–æ—Ç—è –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç RNN —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏, –≤–∞–∂–Ω–æ –ø–æ–º–Ω–∏—Ç—å –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö. –ü–æ —Å–≤–æ–µ–π —Å—É—Ç–∏ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Å–ª–æ–µ–≤ –≤ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º—É —É–≤–µ–ª–∏—á–µ–Ω–∏—é –æ–±—ä–µ–º–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å RNN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è**, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –æ—Å–Ω–æ–≤–∞–Ω—ã —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (RNN), —Å–æ—Å—Ç–æ–∏—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º: –≤–∑—è—Ç—å –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —á–µ—Ä–µ–∑ –æ–¥–Ω—É –∏ —Ç—É –∂–µ –Ω–µ–π—Ä–æ—Å–µ—Ç—å.\n",
    "–ù–æ –ø—Ä–∏ —ç—Ç–æ–º —Å–∞–º–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –∫—Ä–æ–º–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ) –±—É–¥–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –µ—â–µ –æ–¥–∏–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä ‚Äî –Ω–µ–∫–∏–π $h$, –∫–æ—Ç–æ—Ä—ã–π –≤ –Ω–∞—á–∞–ª–µ –±—É–¥–µ—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤–µ–∫—Ç–æ—Ä–æ–º –∏–∑ –Ω—É–ª–µ–π, –∞ –¥–∞–ª–µ–µ ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ –≤—ã–¥–∞–µ—Ç —Å–∞–º–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (**—Ç–æ–∫–µ–Ω–∞**).\n",
    "\n",
    "–¢–∞–∫–∂–µ –¥–∞–ª–µ–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–Ω—è—Ç–∏–µ **–Ω—É–ª–µ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞** ‚Äî —Ç–æ–∫–µ–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø–ª–∞—Ç–∫—É –∏ –Ω–µ –Ω–µ—Å—ë—Ç –Ω–∏–∫–∞–∫–æ–≥–æ —Å–º—ã—Å–ª–∞, –Ω–æ –∫–æ—Ç–æ—Ä—ã–π –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –º–æ–¥–µ–ª–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ —Å–∏–≥–Ω–∞–ª –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã.\n",
    "\n",
    "–í —Å–µ—Ç–∏ –ø–æ—è–≤–ª—è–µ—Ç—Å—è –Ω–æ–≤–∞—è —Å—É—â–Ω–æ—Å—Ç—å ‚Äî **hidden state** ($h$) ‚Äî –≤–µ–∫—Ç–æ—Ä, —Ö—Ä–∞–Ω—è—â–∏–π —Å–æ—Å—Ç–æ—è–Ω–∏–µ, —É—á–∏—Ç—ã–≤–∞—é—â–µ–µ –∏ –ª–æ–∫–∞–ª—å–Ω—ã–π, –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/rnn_basic_block.png\" width=\"700\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ë–∞–∑–æ–≤—ã–π RNN –±–ª–æ–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ä–∞–±–æ—Ç—É —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏:\n",
    "1. –ù–∞ –≤—Ö–æ–¥ –ø–æ—Å—Ç—É–ø–∞–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å $x = \\{x_1,...x_t,...,x_n\\}$, –≥–¥–µ $x_i$ ‚Äî –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏. –í —Ä—è–¥–µ —Å–ª—É—á–∞–µ–≤ —ç—Ç–æ—Ç –≤–µ–∫—Ç–æ—Ä –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å $1$.\n",
    "\n",
    "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç—É–ø–∏–≤—à–µ–≥–æ $x_t$ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ $h_t$, –∫–æ—Ç–æ—Ä–æ–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–µ–π –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è $h_{t-1}$ –∏ —Ç–µ–∫—É—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ $x_t$:\n",
    "$$\\large h_t = f_W(h_{t-1}, x_t),$$\n",
    "–≥–¥–µ $W$  ‚Äî —ç—Ç–æ –æ–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤–µ—Å–∞).\n",
    "\n",
    "3. –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–æ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞—é—â–µ–≥–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è  $x_i$, —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è –≤—ã—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å $y = \\{y_1,...y_t,...,y_k\\}$. –î–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è $y_t$ –≤ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –≤ –º–æ–¥–µ–ª—å –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω—ã –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏, –ø—Ä–∏–Ω–∏–º–∞—é—â–∏–µ –Ω–∞ –≤—Ö–æ–¥ —Ç–µ–∫—É—â–µ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ $h_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ—Å—Ç–µ–π—à–∞—è RNN**. –ú—ã –º–æ–∂–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–∞ $x$ –∑–∞ —Å—á–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π —Ñ–æ—Ä–º—É–ª—ã –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large h_t = f_W(h_{t-1}, x_t),$\n",
    "\n",
    "$\\large \\quad \\quad \\quad \\color{grey}{\\downarrow \\text{(—Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è bias)}}$\n",
    "\n",
    "$\\large h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large y_t = W_{hy}h_t.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û—Ç–ª–∏—á–∏–µ** –æ—Ç —Å–ª–æ–µ–≤, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –º—ã —É–∂–µ —Å—Ç–∞–ª–∫–∏–≤–∞–ª–∏—Å—å, —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ **–Ω–∞ –≤—ã—Ö–æ–¥–µ –º—ã –ø–æ–ª—É—á–∞–µ–º –¥–≤–∞ –æ–±—ä–µ–∫—Ç–∞** ‚Äî $y_t$ –∏ $h_t$:\n",
    "\n",
    "$y_t$ ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞,\n",
    "\n",
    "$h_t$ ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç, –≤ –∫–æ—Ç–æ—Ä–æ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ. –û–Ω –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í PyTorch –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è $h_t$ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥—É–ª—å `torch.nn.RNNCell` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html).\n",
    "\n",
    "$y_t$ –≤ –Ω–µ–º –Ω–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è: –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –¥–ª—è –µ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –≤ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π.\n",
    "\n",
    "**`input_size`** ‚Äî —Ä–∞–∑–º–µ—Ä —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–ª–æ—ë–≤, —ç—Ç–æ –≤—Å–µ–≥–¥–∞ –≤–µ–∫—Ç–æ—Ä, –∞ –Ω–µ —Ç–µ–Ω–∑–æ—Ä, –ø–æ—ç—Ç–æ–º—É **`input_size`** ‚Äî —Å–∫–∞–ª—è—Ä.\n",
    "\n",
    "**`hidden_size`** ‚Äî —Ç–æ–∂–µ —Å–∫–∞–ª—è—Ä. –û–Ω –∑–∞–¥–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ —Ç–æ–∂–µ —è–≤–ª—è–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–æ–º. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏ —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤ –≤ —Å–ª–æ–µ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "rnn_cell = torch.nn.RNNCell(input_size=3, hidden_size=2)\n",
    "dummy_sequence = torch.randn((1, 3))  # batch, input_size\n",
    "h = rnn_cell(dummy_sequence)\n",
    "print(\"Inital shape:\".ljust(17), f\"{dummy_sequence.shape}\")\n",
    "print(\"Resulting shape:\".ljust(17), f\"{h.shape}\")  # hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–Ω—É—Ç—Ä–∏ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–æ, —á—Ç–æ –æ–ø–∏—Å–∞–Ω–æ –≤ –∫–æ–¥–µ –Ω–∏–∂–µ.\n",
    "–î–ª—è –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ –æ–ø—É—â–µ–Ω–∞ –±–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞. –¢–∞–∫–∂–µ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø–æ–¥–æ–±–Ω—ã–π –∫–æ–¥ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞—Ä–∞–±–æ—Ç–∞–ª, [–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±–µ—Ä–Ω—É—Ç—å –≤–µ—Å–∞ ‚úèÔ∏è[blog]](https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter) –≤ `torch.nn.Parameter` –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –Ω—É–ª—è–º–∏, –Ω–æ –ª—É—á—à–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# Simple RNNcell without a bias and batch support\n",
    "class SimplifiedRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        # Init weight matrix, for simplicity omit bias\n",
    "        self.W_hx = (\n",
    "            torch.randn(input_size, hidden_size) * 0.0001\n",
    "        )  # hidden_size == number of neurons\n",
    "        self.W_hh = (\n",
    "            torch.randn(hidden_size, hidden_size) * 0.0001\n",
    "        )  # naive initialization\n",
    "        self.h0 = torch.zeros((hidden_size))  # Initial hidden state\n",
    "\n",
    "    def forward(self, x, h=None):  # Without a batch dimension\n",
    "        if h is None:\n",
    "            h = self.h0\n",
    "        h = torch.tanh(torch.matmul(self.W_hx.T, x) + torch.matmul(self.W_hh.T, h))\n",
    "        return h\n",
    "\n",
    "\n",
    "simple_rnn_cell = SimplifiedRNNCell(input_size=3, hidden_size=2)\n",
    "h = simple_rnn_cell(dummy_sequence[0])  # No batch\n",
    "print(f\"Out = h\\n{h.shape} \\n{h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–Ω–∞–∫–æ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–¥–∞ **–Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤**. –ò –Ω–∞–¥–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –∫ –∫–∞–∂–¥–æ–º—É.\n",
    "\n",
    "\n",
    "–ü–æ—ç—Ç–æ–º—É `torch.nn.RNNCell` –Ω–∞–ø—Ä—è–º—É—é –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è. –î–ª—è –Ω–µ–≥–æ –µ—Å—Ç—å –æ–±–µ—Ä—Ç–∫–∞ ‚Äî `torch.nn.RNN` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), –∫–æ—Ç–æ—Ä–∞—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ RNNCell –¥–ª—è –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN –±–ª–æ–∫ –≤ PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RNN: –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –±–∞—Ç—á, —Ä–∞–∑–º–µ—Ä –æ–±—ä–µ–∫—Ç–∞**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(input_size=3, hidden_size=2, batch_first=False)  # batch_first = True\n",
    "dummy_batched_seq = torch.randn((2, 1, 3))  # seq_len, batch, input_size\n",
    "out, h = rnn(dummy_batched_seq)\n",
    "\n",
    "print(\"Inital shape:\".ljust(20), f\"{dummy_batched_seq.shape}\")\n",
    "print(\"Resulting shape:\".ljust(20), f\"{out.shape}\")\n",
    "print(\"Hidden state shape:\".ljust(20), f\"{h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–Ω—É—Ç—Ä–∏ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ —Å–ª–µ–¥—É—é—â–µ–µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Simple RNN without batching\n",
    "class SimplifiedRNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn_cell = SimplifiedRNNCell(input_size, hidden_size)\n",
    "\n",
    "    # Without a batch dimension x have shape seq_len * input_size\n",
    "    def forward(self, x, h=None):\n",
    "        all_h = []\n",
    "        for i in range(x.shape[0]):  # iterating over timestamps\n",
    "            h = self.rnn_cell(torch.Tensor(x[i]), h)\n",
    "            all_h.append(h)\n",
    "        return np.stack(all_h), h\n",
    "\n",
    "\n",
    "simple_rnn = SimplifiedRNNLayer(input_size=4, hidden_size=2)\n",
    "\n",
    "sequence = np.array(\n",
    "    [[0, 1, 2, 0], [3, 4, 5, 0]]\n",
    ")  # batch with one sequence of two elements\n",
    "\n",
    "out, h = simple_rnn(sequence)\n",
    "print(\"Inital shape:\".ljust(20), f\"{sequence.shape}\")\n",
    "print(\"Resulting shape:\".ljust(20), f\"{out.shape}\")\n",
    "print(\"Hidden state shape:\".ljust(20), f\"{h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º—Å—è.\n",
    "\n",
    "–ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –¥–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:\n",
    "\n",
    "*   [1, 3, 2]\n",
    "*   [0, 4, 2]\n",
    "\n",
    "–ß—Ç–æ–±—ã –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —ç–ª–µ–º–µ–Ω—Ç \"3\", –Ω–∞–º –Ω—É–∂–µ–Ω hidden state, –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–π –ø–æ \"1\".\n",
    "\n",
    "–¢–æ –∂–µ —Å–∞–º–æ–µ –¥–ª—è \"4\" ‚Äî –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å \"0\". –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–π –æ—Å–∏ –º—ã –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –ø—Ä–∏–¥—ë—Ç—Å—è –¥–µ–ª–∞—Ç—å —ç—Ç–æ –ø–æ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–π. –ú—ã –º–æ–∂–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–µ—Ä–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–µ—Ä–≤–æ–π –∏ –≤—Ç–æ—Ä–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.\n",
    "\n",
    "–ö –¥–∞–Ω–Ω—ã–º –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –µ—â–µ –æ–¥–Ω–æ –∏–∑–º–µ—Ä–µ–Ω–∏–µ ‚Äî **—Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**. Batch –∏–∑ 5 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø–æ 6 –æ–±—ä–µ–∫—Ç–æ–≤ (—Ä–∞–∑–º–µ—Ä –æ–±—ä–µ–∫—Ç–∞ 3) –≤ –∫–∞–∂–¥–æ–π –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å —Ç–∞–∫ (–≤—Ä–µ–º—è –∏–¥—ë—Ç –ø–µ—Ä–≤–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é, –ø–æ—ç—Ç–æ–º—É –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ –∏–¥—ë–º \"—Å–≤–µ—Ä—Ö—É –≤–Ω–∏–∑\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/rnn_batch.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"https://www.researchgate.net/publication/284579100_Session-based_Recommendations_with_Recurrent_Neural_Networks\">Session-based Recommendations with Recurrent Neural Networks</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–Ω—É—Ç—Ä–∏ RNN-–º–æ–¥—É–ª—è —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/simple_rnn_h_state.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–µ—Å–∞ –ø—Ä–∏ —ç—Ç–æ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_seq = torch.randn((2, 1, 3))  #  seq_len, batch, input_size\n",
    "\n",
    "print(\"RNNCell\")\n",
    "rnn_cell = torch.nn.RNNCell(3, 2)\n",
    "print(\"Parameter\".ljust(10), \"Shape\")\n",
    "for t, p in rnn_cell.named_parameters():\n",
    "    print(t.ljust(10), p.shape)\n",
    "\n",
    "cell_out = rnn_cell(dummy_seq[0, :, :])  # take first element from sequence\n",
    "print()\n",
    "print(\"Result shape =\".ljust(20), cell_out.shape)\n",
    "print(\"Hidden state shape =\".ljust(20), cell_out.shape)  # one hidden state\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "print(\"RNN\")\n",
    "rnn = torch.nn.RNN(3, 2)\n",
    "print(\"Parameter\".ljust(15), \"Shape\")\n",
    "for t, p in rnn.named_parameters():\n",
    "    print(t.ljust(15), p.shape)\n",
    "\n",
    "out, h = rnn(dummy_seq)\n",
    "\n",
    "print()\n",
    "print(\"Result shape =\".ljust(20), out.shape)  # h for all timestamps element\n",
    "print(\"Hidden state shape =\".ljust(20), cell_out.shape)  # h for last element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ –æ–±—Ä–∞—Ç–∏–º—Å—è –∫ [PyTorch üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å—Ç—å —É –º–æ–¥—É–ª—è RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –°–ª–æ–∏ (Stacked RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN –±–ª–æ–∫–∏ –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –≤ —Å–ª–æ–∏, –Ω–∞–∫–ª–∞–¥—ã–≤–∞—è –∏—Ö –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞. –î–ª—è —ç—Ç–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ `torch.nn.RNN` –µ—Å—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç `num_layers`, —Å –ø–æ–º–æ—â—å—é –∫–æ—Ç–æ—Ä–æ–≥–æ –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤.\n",
    "\n",
    "–í –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –Ω–∏–∂–Ω–∏–π —Å–ª–æ–π (–∞ —ç—Ç–æ –≤—Å—ë –µ—â—ë –æ–¥–Ω–∞ RNN-—è—á–µ–π–∫–∞) –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±—É–∫–≤—É *h*, –ø–µ—Ä–µ–¥–∞—ë—Ç —Å–≤–æ–π hidden state –≤ —Å–∞–º—É —Å–µ–±—è (–Ω–∞–ø—Ä–∞–≤–æ, `h[0]`) –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç *–µ* –∏ —Ç.–¥. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —ç—Ç–∞ –∂–µ —è—á–µ–π–∫–∞ –ø–µ—Ä–µ–¥–∞—ë—Ç —Å–≤–æ—ë —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–∞ –≤—Ç–æ—Ä—É—é RNN-—è—á–µ–π–∫—É (–Ω–∞–≤–µ—Ä—Ö, `h[1]`), –∫–æ—Ç–æ—Ä–∞—è —É–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã –ø–µ—Ä–≤–æ–π —è—á–µ–π–∫–∏.\n",
    "\n",
    "–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —Ç–∞–∫–∞—è —Å—Ö–µ–º–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –≤–∑—Ä—ã–≤—É –∏–ª–∏ –∑–∞—Ç—É—Ö–∞–Ω–∏—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞, –ø—Ä–∏—á—ë–º –ø—Ä–∏ –ø—Ä–æ—Ö–æ–¥–µ –∫–∞–∫ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏, —Ç–∞–∫ –∏ –ø–æ –≤–µ—Ä—Ç–∏–∫–∞–ª–∏. –û–± —ç—Ç–æ–º –Ω–∏–∂–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/layers.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–∞—Ä–∞–º–µ—Ç—Ä **num_layers** –∑–∞–¥–∞—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ RNN-—è—á–µ–µ–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn((2, 1, 3))  # seq_len, batch, input_size\n",
    "rnn = torch.nn.RNN(3, 2, num_layers=3)\n",
    "\n",
    "# Weights matrix sizes not changed!\n",
    "for t, p in rnn.named_parameters():\n",
    "    print(t, p.shape)\n",
    "\n",
    "out, h = rnn(dummy_input)\n",
    "\n",
    "print()\n",
    "print(\"Out:\\n\", out.shape)  # Hidden states for all elements from top layer\n",
    "print(\"h:\\n\", h.shape)  # Hidden states for last element for all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- [[blog] ‚úèÔ∏è How to Remove Non-Stationarity From Time Series](https://www.kaggle.com/code/bextuychiev/how-to-remove-non-stationarity-from-time-series)\n",
    "- [[blog] ‚úèÔ∏è A Guide to Time Series Forecasting in Python](https://builtin.com/data-science/time-series-forecasting-python)\n",
    "- [[blog] ‚úèÔ∏è How to Check if Time Series Data is Stationary with Python?](https://www.geeksforgeeks.org/how-to-check-if-time-series-data-is-stationary-with-python/)\n",
    "- [[blog] ‚úèÔ∏è Complete Guide on Time Series Analysis in Python](https://www.kaggle.com/code/prashant111/complete-guide-on-time-series-analysis-in-python)\n",
    "- [[doc] üõ†Ô∏è Data transformations and forecasting models: what to use and when](https://people.duke.edu/~rnau/whatuse.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ –æ–±—â–µ–≥–æ —É –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏–∏ –¥–æ–º–æ—Ö–æ–∑—è–π—Å—Ç–≤–∞–º–∏, –æ—Ü–µ–Ω–∫–∏ —Ç—Ä–∞—Ñ–∏–∫–∞ –Ω–∞ –¥–æ—Ä–æ–≥–∞—Ö –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã, –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–∞–≤–æ–¥–∫–æ–≤ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ü–µ–Ω—ã, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –∞–∫—Ü–∏–∏ –±—É–¥—É—Ç —Ç–æ—Ä–≥–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ñ–æ–Ω–¥–æ–≤–æ–π –±–∏—Ä–∂–µ?\n",
    "\n",
    "–í—Å–µ –æ–Ω–∏ –ø–æ–ø–∞–¥–∞—é—Ç –ø–æ–¥ –ø–æ–Ω—è—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤! –í—ã –Ω–µ –º–æ–∂–µ—Ç–µ —Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ª—é–±–æ–π –∏–∑ —ç—Ç–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ ¬´–≤—Ä–µ–º—è¬ª. –ò –ø–æ –º–µ—Ä–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤ –º–∏—Ä–µ –≤–æ–∫—Ä—É–≥ –Ω–∞—Å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –≤—Å–µ –±–æ–ª—å—à–µ –∏ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ –≤–∞–∂–Ω–æ–π –æ–±–ª–∞—Å—Ç—å—é –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ ML –∏ DL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –ø–æ–¥—Ö–æ–¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[colab] ü•® Time Series Prediction with LSTM Using PyTorch](https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/pytorch/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb#scrollTo=NabsV8O5BBd5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –®–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–Ω–æ–≤–∞ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç [Air Passengers üõ†Ô∏è[doc]](https://www.kaggle.com/rakannimer/air-passengers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/airline-passengers.csv\"\n",
    ")\n",
    "\n",
    "training_data = dataset.iloc[:, 1:2].values  # transform dataframe to numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max normalization\n",
    "td_min = training_data.min()\n",
    "td_max = training_data.max()\n",
    "print(\"Initial statistics:\")\n",
    "print(\"Minimum value:\", repr(td_min).rjust(5))\n",
    "print(\"Maximum value:\", repr(td_max).rjust(5))\n",
    "\n",
    "training_data = (training_data - td_min) / (td_max - td_min)\n",
    "print(\"\\nResulting statistics:\")\n",
    "print(\"Minimum value:\", repr(training_data.min()).rjust(5))\n",
    "print(\"Maximum value:\", repr(training_data.max()).rjust(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–π –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–∫–æ–ª—å–∫—É –º—ã —Ö–æ—Ç–∏–º –Ω–∞—É—á–∏—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö, –Ω–∞–º –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º.\n",
    "\n",
    "–†–∞–∑–æ–±—å–µ–º –≤–µ—Å—å –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–∏–¥–∞\n",
    "\n",
    "$x \\to y$,\n",
    "\n",
    "–≥–¥–µ $x$ ‚Äî —ç—Ç–æ –ø–æ–¥–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø–∏—Å–∏ —Å 1-–π –ø–æ 8-—é, –∞ $y$ ‚Äî —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ 9-–π –∑–∞–ø–∏—Å–∏, —Ç–æ —Å–∞–º–æ–µ, –∫–æ—Ç–æ—Ä–æ–µ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sliding_windows(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data) - seq_length):\n",
    "        _x = data[i : (i + seq_length)]  # picking several sequential observations\n",
    "        _y = data[i + seq_length]  # picking the subsequent observation\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return torch.Tensor(np.array(x)), torch.Tensor(np.array(y))\n",
    "\n",
    "\n",
    "# set length of the ensemble; accuracy of the predictions and\n",
    "# speed perfomance almost always depend on it size\n",
    "seq_length = 8  # compare 2 and 32\n",
    "x, y = sliding_windows(training_data, seq_length)\n",
    "print(\"Example of the obtained data:\\n\")\n",
    "print(\"Data corresponding to the first x:\")\n",
    "print(x[0])\n",
    "print(\"Data corresponding to the first y:\")\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç–∞–∫–æ–º—É –ø–æ–¥—Ö–æ–¥—É –º—ã –º–æ–∂–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å RNN –º–æ–¥–µ–ª—å—é —Ç–∞–∫ –∂–µ, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–ª–∏ —Å–æ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –ø–æ–¥–∞–≤–∞—è –Ω–∞ –≤—Ö–æ–¥ —Ç–∞–∫—É—é –ø–æ–¥–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å + —Ä–µ–∑—É–ª—å—Ç–∞—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–∞–∑–æ–±—å–µ–º –Ω–∞ train –∏ test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç. –ù–µ–ª—å–∑—è –¥–æ–ø—É—Å—Ç–∏—Ç—å —É—Ç–µ—á–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö. –ß—Ç–æ –±—É–¥–µ—Ç, –µ—Å–ª–∏ –Ω–µ –æ—Ç—Å—Ç—É–ø–∏—Ç—å –Ω–∞ –¥–ª–∏–Ω—É `seq_length`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(y) * 0.8)\n",
    "\n",
    "x_train = x[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "x_test = x[train_size + seq_length :]\n",
    "y_test = y[train_size + seq_length :]\n",
    "\n",
    "print(\"Train data:\")\n",
    "print(\"x shape:\", x_train.shape)\n",
    "print(\"y shape:\", y_train.shape)\n",
    "\n",
    "print(\"\\nTest data:\")\n",
    "print(\"x shape:\", x_test.shape)\n",
    "print(\"y shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä **batch_first**. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∏–≤—ã—á–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AirTrafficPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        # hidden_size == number of neurons\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size, hidden_size=hidden_size, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # Predict only one value\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x: \",x.shape) # 108 x 8 x 1 : [batch_size, seq_len, input_size]\n",
    "        out, h = self.rnn(x)\n",
    "        # print(\"out: \", out.shape) # 108 x 8 x 4 : [batch_size, seq_len, hidden_size] Useless!\n",
    "        # print(\"h : \", h.shape) # 1 x 108 x 4 [ num_layers, batch_size, hidden_size]\n",
    "        y = self.fc(h)\n",
    "        # print(\"y\",y.shape) # 1 x 108 x 1\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë–ª–∞–≥–æ–¥–∞—Ä—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –Ω–µ –±—É–¥–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –Ω–∞ –ø—Ä–æ—à–µ–¥—à–∏—Ö –∑–∞–Ω—è—Ç–∏—è—Ö.\n",
    "\n",
    "–í —Å–∏–ª—É —Ç–æ–≥–æ, —á—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –º–∞–ª–µ–Ω—å–∫–∏–π –∏ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –ø–æ–º–µ—Å—Ç–∏–ª–∏—Å—å –≤ –æ–¥–∏–Ω batch, –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ batch-–∞–º –≤ —è–≤–Ω–æ–º –≤–∏–¥–µ –∑–¥–µ—Å—å –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_train(model, num_epochs=2000, learning_rate=0.01):\n",
    "    criterion = torch.nn.MSELoss()  # mean-squared error for regression\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred, h = model(x_train)  # we don't use h there, but we can!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # obtain the loss\n",
    "        loss = criterion(y_pred[0], y_train)  # for shape compatibility\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch: {epoch},\".ljust(15), \"loss: %1.5f\" % (loss.item()))\n",
    "\n",
    "\n",
    "print(\"Simple RNN training process with MSE loss:\")\n",
    "input_size = 1\n",
    "hidden_size = 8\n",
    "rnn = AirTrafficPredictor(input_size, hidden_size)\n",
    "time_series_train(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def time_series_plot(train_predict):\n",
    "    data_predict = train_predict.data\n",
    "    y_data_plot = y.data\n",
    "\n",
    "    # Denormalize\n",
    "    data_predict = data_predict[0] * (td_max - td_min) + td_min\n",
    "    y_data_plot = y_data_plot * (td_max - td_min) + td_min\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.axvline(x=train_size + seq_length, c=\"r\", linestyle=\"--\")\n",
    "    # shifting the curve as first y-value not correspond first value overall\n",
    "    plt.plot(np.arange(y_data_plot.shape[0]), y_data_plot)\n",
    "\n",
    "    plt.plot(np.arange(y_data_plot.shape[0]), data_predict)\n",
    "\n",
    "    plt.title(\"Number of passengers per month\")\n",
    "    plt.ylabel(\"#passengers\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    # plt.xticks(labels_to_display, dataset[\"Month\"][labels_to_display])\n",
    "\n",
    "    plt.legend([\"Train/Test separation\", \"Real\", \"Predicted\"])\n",
    "    plt.grid(axis=\"x\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "rnn.eval()\n",
    "train_predict, h = rnn(x)\n",
    "time_series_plot(train_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–¥–∏–º, —á—Ç–æ –º–æ–¥–µ–ª—å –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–µ–π.\n",
    "\n",
    "–í–æ—Ç —Ç–æ–ª—å–∫–æ —Å–µ–π—á–∞—Å –º—ã –¥–æ–ø—É—Å—Ç–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–∏–ø–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫.\n",
    "–ö–∞–∫ –Ω—É–∂–Ω–æ:\n",
    "\n",
    "1. –†–∞–∑–±–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏.\n",
    "2. –ù–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ –≤—ã—á–∏—Å–ª–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "3. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø–æ —Ç—Ä–µ–π–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞—Ö –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏.\n",
    "\n",
    "\n",
    "–í–µ—Ä–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º –≤ –ø—Ä–∏–º–µ—Ä–µ –Ω–∏–∂–µ, –≤ —Ä–∞–∑–¥–µ–ª–µ LSTM, –≥–¥–µ –±—É–¥–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º —Å–∏–Ω—É—Å–æ–∏–¥.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ–±–ª–µ–º—ã RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏, –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã —Å—Ä–∞–∑—É –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ —Å–µ—Ç—å –∏ –∑–∞—Ç–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç, –æ–¥–Ω–∞–∫–æ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:\n",
    "\n",
    " - –±–æ–ª—å—à–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ –ø–æ–º–µ—Å—Ç—è—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏;\n",
    " - —Ç–∞–∫ –∫–∞–∫ —Ü–µ–ø–æ—á–∫–∞ –±—É–¥–µ—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–π, –≤–æ–∑–Ω–∏–∫–Ω–µ—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏–µ/–≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞;\n",
    " - –ø–æ –º–µ—Ä–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª–∞ –ø–æ —Ü–µ–ø–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞—Ç–∏—Ä–∞–µ—Ç—Å—è.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–ø—É—Å—Ç–∏–º, —É –Ω–∞—Å –µ—Å—Ç—å –¥–ª–∏–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ï—Å–ª–∏ –º—ã —Å—Ä–∞–∑—É –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º, —Ç–æ –≤ –∫–∞–∂–¥—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω—É–∂–Ω–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–∏—Ç—å Loss. –ò –≤—Å–µ —è—á–µ–π–∫–∏ –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –≤–æ –≤—Ä–µ–º—è backpropogation. –í—Å–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω—É–∂–Ω–æ –ø–æ—Å—á–∏—Ç–∞—Ç—å. –í–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ—Ö–≤–∞—Ç–∫–æ–π –ø–∞–º—è—Ç–∏.\n",
    "\n",
    "–ï—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∫–æ–π –¥–ª–∏–Ω—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç RNN –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏. –ï—Å–ª–∏ –º—ã –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π —è—á–µ–π–∫–µ, –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —Å–∫–∞–∂–µ–º, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 —Å–ª–æ–≤–∞—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Tanh –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –∑–∞—Ç–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/backprop_through_time.png\"><center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2021/lecture_10.pdf\">CS231n: Recurrent Neural Network</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞—Ç—É—Ö–∞—é—â–∏–π/–≤–∑—Ä—ã–≤–∞—é—â–∏–π—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç (Vanishing/exploding gradient) ‚Äî —è–≤–ª–µ–Ω–∏—è –∑–∞—Ç—É—Ö–∞—é—â–µ–≥–æ –∏ –≤–∑—Ä—ã–≤–∞—é—â–µ–≥–æ—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ RNN. –ò –ø—Ä–∏ –±–æ–ª—å—à–æ–π –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–æ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫—Ä–∏—Ç–∏—á–Ω—ã–º. –ü—Ä–∏—á–∏–Ω–∞ –≤ —Ç–æ–º, —á—Ç–æ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –≤–µ–ª–∏—á–∏–Ω—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –æ—Ç —á–∏—Å–ª–∞ —Å–ª–æ—ë–≤ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è, –ø–æ—Å–∫–æ–ª—å–∫—É –≤–µ—Å–∞ —É–º–Ω–æ–∂–∞—é—Ç—Å—è –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ.\n",
    "\n",
    "$dL ‚àù (W)^N$.\n",
    "\n",
    "$W > 1$ => –≤–∑—Ä—ã–≤\n",
    "\n",
    "$W < 1$ => –∑–∞—Ç—É—Ö–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/simple_rnn_backprop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∏–∑ –ø—É—Ç–µ–π —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã ‚Äî **–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –æ—Ç—Å–µ—á–µ–Ω–∏–µ** (Gradient clipping) ‚Äî –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞, –ø–æ–∑–≤–æ–ª—è—è –∏–∑–±–µ–∂–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑—Ä—ã–≤–∞.\n",
    "\n",
    "–ê –æ—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å **–ø—Ä–æ–ø—É—Å–∫–∞–Ω–∏–µ** **–≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º**, –Ω–∞ —Å–∫–æ–ª—å–∫–æ-—Ç–æ —à–∞–≥–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞–∑–∞–¥ –∏–ª–∏ –≤–ø–µ—Ä—ë–¥, –∞ –Ω–µ —á–µ—Ä–µ–∑ –≤—Å—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å. –î–∞, –≥—Ä–∞–¥–∏–µ–Ω—Ç –±—É–¥–µ—Ç –Ω–µ —Å–æ–≤—Å–µ–º —Ç–æ—á–Ω–æ —Å—á–∏—Ç–∞—Ç—å—Å—è, –∏ –º—ã –±—É–¥–µ–º —Ç–µ—Ä—è—Ç—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ. –ù–æ —ç—Ç–æ –Ω–∞–º —Å–ø–∞—Å–∞–µ—Ç –ø–∞–º—è—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/truncated_backprop.png\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2021/lecture_10.pdf\">CS231n: Recurrent Neural Network</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—ã—á–Ω–∞—è RNN –∏–º–µ–ª–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–æ–±–ª–µ–º, –≤ —Ç–æ–º —á–∏—Å–ª–µ –≤ –Ω–µ–π –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ –∑–∞—Ç—É—Ö–∞–ª–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–ª–æ–≤–∞—Ö –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –ü–æ–º–∏–º–æ —ç—Ç–æ–≥–æ –±—ã–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞—Ç—É—Ö–∞–Ω–∏–µ–º/–≤–∑—Ä—ã–≤–æ–º —Å–∞–º–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.\n",
    "\n",
    "–≠—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã –±—ã–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ—à–µ–Ω—ã –≤ LSTM, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –≤ [Long Short-Term Memory (Hochreiter & Schmidhuber, 1997) üìö[article]](http://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "\n",
    "–í –æ–±—ã—á–Ω–æ–π RNN –≤ —è—á–µ–π–∫–µ –±—ã–ª —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø—É—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –º—ã —Å–ª–∏–≤–∞–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—É—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤, —Å —Ç–µ–∫—É—â–µ–π:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/simple_rnn_h_state.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ —ç—Ç–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ç–æ–∫–µ–Ω–∞—Ö –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ –∑–∞—Ç—É—Ö–∞–µ—Ç, –∏ —Ç–µ—Ä—è–µ—Ç—Å—è –æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏.\n",
    "\n",
    "–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —è—á–µ–π–∫–∏ LSTM –Ω–∞–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ. –ó–¥–µ—Å—å –µ—Å—Ç—å —Ü–µ–ª—ã—Ö 4 –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ—è, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/lstm_chain.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/lstm_chain_notation.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large f_t = œÉ(W_f \\cdot [h_{t-1}, x_t] + b_f) - \\text{forget  gate}$\n",
    "\n",
    "$\\large i_t = œÉ(W_i \\cdot [h_{t-1}, x_t] + b_i) - \\text{input gate}$\n",
    "\n",
    "$\\large C^\\prime_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_{C^\\prime}) - \\text{candidate cell state}$\n",
    "\n",
    "$\\large C_t = f_t\\otimes C_{t-1} + i_t \\otimes C^\\prime_t - \\text{cell state}$\n",
    "\n",
    "$\\large o_t = œÉ(W_o \\cdot [h_{t-1}, x_t] + b_o) - \\text{output gate}$\n",
    "\n",
    "$\\large h_t = o_t\\otimes \\tanh(C_t) - \\text{ block output}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ì–ª–∞–≤–Ω–æ–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ: –≤ LSTM –¥–æ–±–∞–≤–ª–µ–Ω –ø—É—Ç—å $c$, –∫–æ—Ç–æ—Ä—ã–π –ø–æ –∑–∞–¥—É–º–∫–µ –¥–æ–ª–∂–µ–Ω —ç—Ç–æ—Ç –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/lstm_c_state_highway.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –ø—É—Ç—å $c$ (cell state, –∏–Ω–æ–≥–¥–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è highway, –º–∞–≥–∏—Å—Ç—Ä–∞–ª—å)  –ø–æ–º–æ–≥–∞–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –≤—Å—Ç—Ä–µ—Ç–∏–≤—à—É—é—Å—è –≤ –∫–∞–∫–æ–π-—Ç–æ –º–æ–º–µ–Ω—Ç –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –≤—Å–µ –≤—Ä–µ–º—è, –ø–æ–∫–∞ —ç—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç—Å—è.\n",
    "\n",
    "–ü–æ —Ñ–æ—Ä–º—É–ª–∞–º —Ç–∞–∫–∂–µ –≤–∏–¥–Ω–æ, –∫–∞–∫ –≤–æ–∑—Ä–æ—Å–ª–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] üõ†Ô∏è LSTMCell –≤ PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
    "\n",
    "–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç RNNCell –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—Ö–æ–¥–æ–≤ –∏ –≤—ã—Ö–æ–¥–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "lstm_cell = torch.nn.LSTMCell(input_size=3, hidden_size=4)\n",
    "input = torch.randn(1, 3)  # batch, input_size\n",
    "h_0 = torch.randn(1, 4)\n",
    "c_0 = torch.randn(1, 4)\n",
    "h, c = lstm_cell(input, (h_0, c_0))  # second arg is tuple\n",
    "print(\"Shape of h:\", h.shape)  # batch, hidden_size\n",
    "print(\"Shape of c:\", c.shape)  # batch, hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM –≤ PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ç–ª–∏—á–∏–µ –æ—Ç RNN —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –∫—Ä–æ–º–µ $h$ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –µ—â–µ –∏ $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "lstm = nn.LSTM(input_size=4, hidden_size=5)\n",
    "input = torch.randn(3, 2, 4)  # seq_len, batch, input_size\n",
    "out, (h, c) = lstm(input)  # h and c returned in tuple\n",
    "\n",
    "print(\"Input shape:\".ljust(15), input.shape)\n",
    "print(\"Shape of h\".ljust(15), h.shape)  # batch, hidden_size\n",
    "print(\"Shape of c\".ljust(15), c.shape)  # batch, hidden_size\n",
    "print(\n",
    "    \"Output shape:\".ljust(15), out.shape\n",
    ")  # seq_len, batch, hidden_size : h for each element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ó–∞–º–µ—á–∞–Ω–∏—è**\n",
    "\n",
    "1. –°–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–¥–∞–≤–∞–µ–º—ã—Ö –≤ –Ω–µ–µ –¥–∞–Ω–Ω—ã—Ö. –° —Ä–æ—Å—Ç–æ–º –∞–Ω—Å–∞–º–±–ª—è –∏ —á–∏—Å–ª–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –∑–∞—É—á–∏–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏ —Ç–µ—Ä—è–µ—Ç—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é.\n",
    "2. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–∏–∫–ª–∏—á–Ω–æ—Å—Ç–∏ –≤ –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å) –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∞–Ω—Å–∞–º–±–ª—è (—Ç—É—Ç –≤–∏–¥–Ω–æ, —á—Ç–æ —Ü–∏–∫–ª –≤ —Å—Ä–µ–¥–Ω–µ–º —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 8 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤).\n",
    "3. –¢–∞–∫–∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–µ—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç —Ç–∏–ø–∞ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –í—ã –ø—Ä–∏–º–µ–Ω—è–µ—Ç–µ. –ù—É–∂–Ω–æ –∑–Ω–∞—Ç—å –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã scaler'–æ–≤ –∏ –Ω–µ —Å—Ç–µ—Å–Ω—è—Ç—å—Å—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –Ω–∏–º–∏:\n",
    " * [[blog] ‚úèÔ∏è Feature Scaling Data with Scikit-Learn for Machine Learning in Python](https://stackabuse.com/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python/)\n",
    " * [[blog] ‚úèÔ∏è Hands-On Machine Learning with Scikit-Learn and TensorFlow, —á.4](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch04.html)\n",
    "4. –ü—Ä–∏ –≤—Å–µ–π –≤—ã–≥–æ–¥–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –±—ã—Ç—å –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–º —Å –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π:\n",
    " * [[arxiv] üéì How to avoid machine learning pitfalls:\n",
    "a guide for academic researchers (Lones, 2023)](https://arxiv.org/pdf/2108.02497.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###–ü—Ä–∏–º–µ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ —Å—Ä–∞–∑—É –ø–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ —ç—Ç–∞ –∏–¥–µ—è —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ. –î–∞–ª–µ–µ –≤ –ª–µ–∫—Ü–∏–∏ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞–∑–±–µ—Ä—ë–º —Ä–∞–±–æ—Ç—É –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤.\n",
    "\n",
    "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º 30 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∏–Ω—É—Å–æ–∏–¥ —Å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —á–∞—Å—Ç–æ—Ç–æ–π –∏ –∞–º–ø–ª–∏—Ç—É–¥–æ–π, –Ω–æ –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è –Ω–µ–º–Ω–æ–≥–æ —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –Ω–∞ –æ—Å–∏ –•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 30  # number of samples\n",
    "L = 300  # length of each sample (number of values for each sine wave)\n",
    "T = 10  # width of the wave\n",
    "x = np.empty((N, L), np.float32)  # instantiate empty array\n",
    "x[:] = np.arange(L) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "y = np.sin(x / 1.0 / T).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–µ—Ç—å –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ 30 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∏–Ω—É—Å–æ–∏–¥–∞—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(y[0], label=\"Sequense1\")\n",
    "plt.plot(y[1], label=\"Sequense2\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–π–¥—ë–º—Å—è –ø–æ –µ–≥–æ —á–∞—Å—Ç—è–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_state=512):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_state = hidden_state\n",
    "        # lstm1, lstm2, linear are all layers in the network\n",
    "        self.lstm1 = nn.LSTMCell(1, self.hidden_state)\n",
    "        self.lstm2 = nn.LSTMCell(self.hidden_state, self.hidden_state)\n",
    "        self.linear = nn.Linear(self.hidden_state, 1)\n",
    "\n",
    "    def forward(self, y, future_preds=0):\n",
    "        outputs, n_samples = [], y.size(0)\n",
    "        h_t = torch.zeros(n_samples, self.hidden_state, dtype=torch.float32)\n",
    "        c_t = torch.zeros(n_samples, self.hidden_state, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(n_samples, self.hidden_state, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(n_samples, self.hidden_state, dtype=torch.float32)\n",
    "\n",
    "        for time_step in y.split(1, dim=1):\n",
    "            # N, 1\n",
    "            h_t, c_t = self.lstm1(\n",
    "                time_step, (h_t, c_t)\n",
    "            )  # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))  # new hidden and cell states\n",
    "            output = self.linear(h_t2)  # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "\n",
    "        for i in range(future_preds):\n",
    "            # this only generates future predictions if we pass in future_preds>0\n",
    "            # mirrors the code above, using last output/prediction as input\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output)\n",
    "        # transform list to tensor\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:\n",
    "\n",
    "* `time_step` ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –æ–ø–∏—Å—ã–≤–∞—é—â–∏—Ö –≤—Ö–æ–¥—è—â–∏–π –æ–±—ä–µ–∫—Ç $x$,\n",
    "* `hidden_state` ‚Äî —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ $h$.\n",
    "\n",
    "–ú—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–≤–∞ —Å–ª–æ—è RNN, –∏—Å–ø–æ–ª—å–∑—É—è –¥–≤–µ —è—á–µ–π–∫–∏ RNNCell.  –í –ø–µ—Ä–≤—É—é —è—á–µ–π–∫—É RNN –º—ã –ø–æ–¥–∞—ë–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–æ–º $1$.\n",
    "\n",
    "–í RNN –Ω–∞–º –Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –Ω–∞—Ä–µ–∑–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–∞–º –Ω–µ –Ω—É–∂–Ω–æ —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö, –ø–æ—Å–∫–æ–ª—å–∫—É –≤–µ–∫—Ç–æ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —è—á–µ–π–∫–∏ –∑–∞ –Ω–∞—Å.\n",
    "\n",
    "* `h_0` ‚Äî —Ç–µ–Ω–∑–æ—Ä, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –∫–æ—Ä—Ç–µ–∂–µ —Ä–∞–∑–º–µ—Ä–∞ `(batch, hidden_size).`\n",
    "\n",
    "–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –±–µ—Ä–µ–º `n_samples = x.size(0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥, –ø–æ–∂–∞–ª—É–π, —Å–∞–º—ã–π —Ç—Ä—É–¥–Ω—ã–π. –ú—ã –¥–æ–ª–∂–Ω—ã –ø–æ–¥–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π —Ñ–æ—Ä–º—ã. –≠—Ç–æ –±—É–¥–µ—Ç —Ç–µ–Ω–∑–æ—Ä –∏–∑ m —Ç–æ—á–µ–∫, –≥–¥–µ m ‚Äî —Ä–∞–∑–º–µ—Ä `train` –≤ –∫–∞–∂–¥–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.from_numpy(y[3:, :-1])\n",
    "b = a.split(1, dim=1)\n",
    "b[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—â—ë –æ–¥–∏–Ω –º–æ–º–µ–Ω—Ç: –º—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º $1$ —ç–ª–µ–º–µ–Ω—Ç –≤ —Å—Ç—Ä–æ–∫–µ, –∏, —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å $300$-–π —ç–ª–µ–º–µ–Ω—Ç (–∏ –ø–æ—Å—á–∏—Ç–∞—Ç—å –æ—à–∏–±–∫—É), –º—ã –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –Ω–∞ $299$-–º. –¢–æ –∂–µ —Å–∞–º–æ–µ –∫–∞—Å–∞–µ—Ç—Å—è –Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ü–æ –ø–µ—Ä–≤–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤—Ç–æ—Ä–æ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = torch.from_numpy(y[3:, :-1])  # (27, 299)\n",
    "train_target = torch.from_numpy(y[3:, 1:])  # (27, 299)\n",
    "test_input = torch.from_numpy(y[:3, :-1])  # (3, 299)\n",
    "test_target = torch.from_numpy(y[:3, 1:])  # (3, 299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏–±–ª–∏–∂–µ–Ω –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É.\n",
    "\n",
    "–î–æ–±–∞–≤–ª–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º—ã—Ö —Å–∏–Ω—É—Å–æ–∏–¥ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(yi, n, i, future):\n",
    "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(12, 3))\n",
    "\n",
    "    plt.title(f\"Step {i+1}\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    ax1.plot(np.arange(n), yi[0][:n], \"r\", linewidth=2.0)\n",
    "    ax1.plot(np.arange(n, n + future), yi[0][n:], \"r\" + \":\", linewidth=2.0)\n",
    "\n",
    "    ax2.plot(np.arange(n), yi[1][:n], \"g\", linewidth=2.0)\n",
    "    ax2.plot(np.arange(n, n + future), yi[1][n:], \"g\" + \":\", linewidth=2.0)\n",
    "\n",
    "    ax3.plot(np.arange(n), yi[2][:n], \"b\", linewidth=2.0)\n",
    "    ax3.plot(np.arange(n, n + future), yi[2][n:], \"b\" + \":\", linewidth=2.0)\n",
    "\n",
    "    plt.savefig(\"predict%d.png\" % i, dpi=200)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    model,\n",
    "    optimiser,\n",
    "    loss_fn,\n",
    "    train_input,\n",
    "    train_target,\n",
    "    test_input,\n",
    "    test_target,\n",
    "):\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        def closure():\n",
    "            optimiser.zero_grad()\n",
    "            out = model(train_input)\n",
    "            loss = loss_fn(out, train_target)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimiser.step(closure)\n",
    "        with torch.no_grad():\n",
    "            future = 100\n",
    "            pred = model(test_input, future_preds=future)\n",
    "            # use all pred samples, but only go to 299\n",
    "            loss = loss_fn(pred[:, :-future], test_target)\n",
    "            y = pred.detach().numpy()\n",
    "        # draw figures\n",
    "        n = train_input.shape[1]  # 299\n",
    "        draw(y, n, i, future)\n",
    "\n",
    "        # print the loss\n",
    "        out = model(train_input)\n",
    "        loss_print = loss_fn(out, train_target)\n",
    "        print(\"Step: {}, Loss: {}\".format(i, loss_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "model = LSTM()\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = optim.LBFGS(model.parameters(), lr=0.01)\n",
    "# optimiser = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "training_loop(\n",
    "    num_epochs,\n",
    "    model,\n",
    "    optimiser,\n",
    "    criterion,\n",
    "    train_input,\n",
    "    train_target,\n",
    "    test_input,\n",
    "    test_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–•–æ—Ä–æ—à–æ –±—ã –µ—â—ë –∫–∞–∫-—Ç–æ —É–ª—É—á—à–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/signal.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —á–µ—Ä–µ–∑ —Å–µ—Ç—å –¥–≤–∞ —Ä–∞–∑–∞: –≤ –ø—Ä—è–º–æ–º –∏ –æ–±—Ä–∞—Ç–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏. –î–ª—è —ç—Ç–æ–≥–æ —Å–æ–∑–¥–∞—ë—Ç—Å—è —Å–ª–æ–π, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –≤—Ö–æ–¥–Ω–æ–º—É, –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–≤—É—Ö —Å–ª–æ—ë–≤ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç—Å—è.\n",
    "\n",
    "[[blog] ‚úèÔ∏è A Beginner‚Äôs Guide on Recurrent Neural Networks with PyTorch](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/bidirectional.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn((2, 1, 3))  # seq_len, batch, input_size\n",
    "rnn = torch.nn.RNN(3, 2, bidirectional=True)\n",
    "\n",
    "for t, p in rnn.named_parameters():\n",
    "    print(t, p.shape)\n",
    "\n",
    "out, h = rnn(dummy_input)\n",
    "\n",
    "# Concatenated Hidden states from both layers\n",
    "print(\"Out:\\n\", out.shape)\n",
    "# Hidden states last element from  both : 2*num_layers*hidden_state\n",
    "print(\"h:\\n\", h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –≤—ã —É–∂–µ –º–æ–≥–ª–∏ –æ—Ç–º–µ—Ç–∏—Ç—å, RNN —è—á–µ–π–∫—É –º–æ–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –Ω–∞—á–∞–ª—å–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º. –ê –∑–∞—Ç–µ–º –ø–æ–¥–∞–≤–∞—Ç—å —Ç—É–¥–∞ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏. **–ò–ª–∏ –Ω–µ –ø–æ–¥–∞–≤–∞—Ç—å.**\n",
    "\n",
    "–ö–∞–∫ —Ç–∞–∫-—Ç–æ?\n",
    "\n",
    "–î–∞, —Ü–µ–ª—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è ‚Äî –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é. –î–æ–ø—É—Å—Ç–∏–º, –≤–∞—à–∏ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö —Å–æ 2 –ø–æ 11 –¥–µ–Ω—å, –∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–≤—è–∑–∞–Ω–∞ —Ç–æ–ª—å–∫–æ —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–æ 2 –ø–æ 11 –¥–µ–Ω—å. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤–∞—à–∏ –±–∞—Ç—á–∏ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é, –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞.\n",
    "\n",
    "–ü—Ä–∏ —ç—Ç–æ–º **–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ** –Ω—É–∂–Ω–æ **—Å–±—Ä–æ—Å–∏—Ç—å** —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –≤ –Ω–∏—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã. –í—ã –Ω–µ —Ö–æ—Ç–∏—Ç–µ, —á—Ç–æ–±—ã –≤–∞—à–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ –≤–∞—à–µ–≥–æ –ø—Ä–æ—à–ª–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–≤–ª–∏—è–ª–æ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–µ?\n",
    "\n",
    "[[blog] ‚úèÔ∏è –û–±—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ Stackexchange](https://stats.stackexchange.com/questions/470382/what-is-the-use-of-the-hidden-state-in-an-lstm-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU (Gated reccurent unit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∞–º–∞—è –∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è LSTM ‚Äî GRU. –û–Ω–∞ –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω–∞ –∑–∞ —Å—á–µ—Ç —Å–∏–ª—å–Ω—ã—Ö —É–ø—Ä–æ—â–µ–Ω–∏–π –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π LSTM.\n",
    "\n",
    "–ì–ª–∞–≤–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è: –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã forget –∏ input gates, —Å–ª–∏—Ç—ã $h_t$ –∏ $C_t$, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ–±—ã—á–Ω–æ–π LSTM —Ç–æ–ª—å–∫–æ —É—á–∞—Å—Ç–≤–æ–≤–∞–ª–∏ –≤ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥—Ä—É–≥ –¥—Ä—É–≥–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/gru_basic_block.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "$\\large r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$\n",
    "\n",
    "$\\large \\tilde h_t = tanh(W \\cdot [r_t * h_{t-1}, x_t])$\n",
    "\n",
    "$\\large h_t = (1-z_t) * h_{t-1} + z_t * \\tilde h_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = torch.nn.GRU(input_size=4, hidden_size=3)\n",
    "input = torch.randn(2, 1, 4)  # seq_len, batch, input_size\n",
    "h0 = torch.randn(1, 1, 3)\n",
    "output, h = gru(input, h0)\n",
    "\n",
    "print(\"Input shape:\".ljust(15), input.shape)\n",
    "print(\"Shape of h:\".ljust(15), h.shape)  # last h\n",
    "print(\"Output shape:\".ljust(15), output.shape)  # seq_len = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ–ø—ã—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π: –∏–Ω–æ–≥–¥–∞ –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç GRU, –∏–Ω–æ–≥–¥–∞ ‚Äî LSTM. –¢–æ—á–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç —É—Å–ø–µ—Ö–∞ —Å–∫–∞–∑–∞—Ç—å –Ω–µ–ª—å–∑—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#–¢–∏–ø—ã –∑–∞–¥–∞—á"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–µ—Ç—å –º–æ–∂–µ—Ç –≤—ã–¥–∞–≤–∞—Ç—å –Ω–µ–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ, –æ–¥–Ω–∞–∫–æ –º—ã –º–æ–∂–µ–º:\n",
    "\n",
    " 1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤—ã–¥–∞–Ω–Ω—ã–π –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º (–µ—Å–ª–∏ –Ω–∞–º –Ω—É–∂–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ) ‚Äî **many-to-one**.\n",
    "\n",
    " 2. –ü–æ–¥–∞–≤–∞—Ç—å –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å —Ç–æ–∫–µ–Ω—ã (–∫–æ–≥–¥–∞ –∫–æ–Ω—á–∏–ª—Å—è –∏—Å—Ö–æ–¥–Ω—ã–π —Å–∏–≥–Ω–∞–ª, –ø–æ–¥–∞–µ–º –Ω—É–ª–µ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã), –ø–æ–∫–∞ –æ–Ω–∞ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω, —Å–∏–º–≤–æ–ª–∏–∑–∏—Ä—É—é—â–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∫—É (**many-to-many, one-to-many**).\n",
    "\n",
    " 3. –î–µ–ª–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —á–∞—Å—Ç—å –≤—ã—Ö–æ–¥–æ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –≤ –Ω–∞—á–∞–ª–µ –µ—ë —Ä–∞–±–æ—Ç—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **¬´One to one¬ª** ‚Äî –æ–±—ã—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å, –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å RNN –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ.\n",
    "\n",
    "*   –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–π —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è **¬´one to many¬ª**, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ–≥–æ –æ–¥–∏–Ω –≤—Ö–æ–¥, –∏ –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—ã—Ö–æ–¥–æ–≤. –¢–∞–∫–æ–π —Ç–∏–ø –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∞–∫—Ç—É–∞–ª–µ–Ω, –∫–æ–≥–¥–∞ –º—ã –≥–æ–≤–æ—Ä–∏–º –æ **–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º—É–∑—ã–∫–∏** –∏–ª–∏ **—Ç–µ–∫—Å—Ç–æ–≤**. –ú—ã –∑–∞–¥–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –Ω–∞—á–∞–ª—å–Ω—ã–π –∑–≤—É–∫, –∞ –¥–∞–ª—å—à–µ –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Ö–æ–¥—ã, –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–∞ –∫ –æ—á–µ—Ä–µ–¥–Ω–æ–π —è—á–µ–π–∫–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è –≤—ã—Ö–æ–¥ —Å –ø—Ä–æ—à–ª–æ–π —è—á–µ–π–∫–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.\n",
    "\n",
    "*   –ï—Å–ª–∏ –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∑–∞–¥–∞—á—É **–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏**, —Ç–æ –∞–∫—Ç—É–∞–ª—å–Ω–∞ —Å—Ö–µ–º–∞ **¬´many to one¬ª**. –ú—ã –¥–æ–ª–∂–Ω—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –≤—Ö–æ–¥—ã –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –∏ —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å—Å—è —Å –∫–ª–∞—Å—Å–æ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/one_or_many_to_one_or_many_ways_1.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   –°—Ö–µ–º–∞ **¬´many to many¬ª**, –≤ –∫–æ—Ç–æ—Ä–æ–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ **—Ä–∞–≤–Ω–æ** –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–¥–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –û–±—ã—á–Ω–æ —ç—Ç–æ –∑–∞–¥–∞—á–∏ —Ç–∏–ø–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ –∏—Å—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, —É–∫–∞–∑–∞—Ç—å —Å—Ç–æ–ª–∏—Ü—ã –≥–æ—Ä–æ–¥–æ–≤, –Ω–∞–∑–≤–∞–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤, –≤–µ—â–µ—Å—Ç–≤ –∏ —Ç.–¥., —á—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∑–∞–¥–∞—á–∞–º –≤–∏–¥–∞ NER (Named entity recogition).\n",
    "\n",
    "*   –°—Ö–µ–º–∞ **¬´many to many¬ª**, –≤ –∫–æ—Ç–æ—Ä–æ–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ **–Ω–µ —Ä–∞–≤–Ω–æ** –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–¥–æ–≤. –≠—Ç–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ –≤ –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ, –∫–æ–≥–¥–∞ –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ —Ñ—Ä–∞–∑–∞ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö (—Ç.–µ. —ç—Ç–æ —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å—Ö–µ–º—É –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫-–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫). –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏–Ω—ã ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ. –° –ø–æ–º–æ—â—å—é —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –æ–Ω —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –∑–∞—Ç–µ–º –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –≤ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫. –ü–æ—Å–ª–µ–¥–Ω–∏–π, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ‚Äî –∏—Å—Ö–æ–¥–Ω—É—é —Ñ—Ä–∞–∑—É, –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—É—é –Ω–∞ –¥—Ä—É–≥–æ–π —è–∑—ã–∫."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/one_or_many_to_one_or_many_ways_2.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —Ä–∞–∑–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã. –°–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–∫–∏–π $h$, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∂–∞—Ç—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, —á—Ç–æ –±—ã–ª–æ –ø–æ–¥–∞–Ω–æ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –∞ –∑–∞—Ç–µ–º –ø–æ–¥–∞–µ–º –µ–≥–æ –≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å ¬´one to many¬ª, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç, –∫ –ø—Ä–∏–º–µ—Ä—É, –ø–µ—Ä–µ–≤–æ–¥ —Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ –±—ã–ª –ø–æ–¥–∞–Ω –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ä–µ–¥–∏ NLP-–∑–∞–¥–∞—á –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å —Ç–∞–∫–∏–µ –≥—Ä—É–ø–ø—ã:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/nlp_tasks.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤–≤–æ–¥–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –∏–º–µ—Ç—å —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –æ—à–∏–±–∫–∞–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ —Ä–∞–∑–º–µ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü; —Ç–µ–∫—Å—Ç –∏–∑ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π ‚Äî –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π, —Å–º–∞–π–ª–æ–≤, —Ö—ç—à—Ç–µ–≥–æ–≤, —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏ –ø—Ä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[git] üêæ RNN-walkthrough](https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb)\n",
    "\n",
    "–ù–∞—á–Ω–µ–º —Å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–∏ ‚Äî –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –ö–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤ —Å–µ–±–µ —Å–ª–µ–¥—É—é—â–∏–µ 5 –±–ª–æ–∫–æ–≤. –ö –Ω–∏–º, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏, –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É–Ω–∫—Ç—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/base_nlp_pipeline.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏:** –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Å–∏–º–≤–æ–ª –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "- –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
    "'hey how are you'\n",
    "\n",
    "- –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\n",
    "'hey how are yo'\n",
    "\n",
    "- –í–µ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:\n",
    "'u'\n",
    "\n",
    "–û—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ç–æ, —á—Ç–æ –º—ã –¥–µ–ª–∞–ª–∏ —Ä–∞–Ω–µ–µ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å —Ç–∞–∫:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/character_by_character_generation_example.gif\" width=\"400\">\n",
    "\n",
    "<em>Source: <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">The Unreasonable Effectiveness of Recurrent Neural Networks</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –Ω–∞—à–µ–π —É—á–µ–±–Ω–æ–π –∑–∞–¥–∞—á–µ –º—ã —Å–æ–∫—Ä–∞—Ç–∏–º –¥–æ –º–∏–Ω–∏–º—É–º–∞ –≤—ã–ø–æ–ª–Ω—è–µ–º—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/simple_pipeline.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "text = [\"hey how are you\", \"good i am fine\", \"have a nice day\"]\n",
    "\n",
    "# Join all the sentences together and extract the unique characters\n",
    "# from the combined sentences\n",
    "chars = set(\"\".join(text))\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "print(\"Dictionary for mapping character to the integer:\")\n",
    "pprint.pprint(char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–º–µ—Å—Ç–æ ASCII —Å–∏–º–≤–æ–ª–∞, –∫–∞–∂–¥–æ–π –±—É–∫–≤–µ –º—ã —Å–æ–ø–æ—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–º–µ—Ä."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN –¥–æ–ø—É—Å–∫–∞—é—Ç —Ä–∞–±–æ—Ç—É —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã. –ù–æ —á—Ç–æ–±—ã –ø–æ–º–µ—Å—Ç–∏—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ batch, –Ω–∞–¥–æ –∏—Ö –≤—ã—Ä–æ–≤–Ω—è—Ç—å.\n",
    "\n",
    "–û–±—ã—á–Ω–æ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–µ–ª–∞—é—Ç —Ä–∞–≤–Ω—ã–º —Å–∞–º–æ–º—É –¥–ª–∏–Ω–Ω–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Å—Ç–æ –¥–æ–ø–æ–ª–Ω—è—é—Ç –ø—Ä–æ–±–µ–ª–∞–º–∏ (–∏–ª–∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–∞–º–∏) –¥–æ —ç—Ç–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.  –¢–∞–∫–∂–µ —Ö–æ—Ä–æ—à–µ–π –∏–¥–µ–µ–π –±—É–¥–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Å–∏–º–≤–æ–ª–æ–º –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(sent) for sent in text]\n",
    "maxlen = max(lengths)\n",
    "print(f\"The longest string has {maxlen} characters.\\n\")\n",
    "\n",
    "print(f\"Initial texts:\\n{text}\")\n",
    "# A simple loop that loops through the list of sentences and adds\n",
    "# a ' ' whitespace until the length of the sentence matches\n",
    "# the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "    while len(text[i]) < maxlen:\n",
    "        text[i] += \" \"\n",
    "\n",
    "print(f\"Resulting texts:\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–∞ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –±–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∏–º–≤–æ–ª–∞:\n",
    "\n",
    "**'hey how are yo'**\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω:\n",
    "\n",
    "**'ey how are you'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "\n",
    "    # Remove first character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "\n",
    "    print(\"Input sequence:\".ljust(18), f\"'{input_seq[i]}'\")\n",
    "    print(\"Target sequence:\".ljust(18), f\"'{target_seq[i]}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å —Å–∏–º–≤–æ–ª—ã –Ω–∞–¥–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –≤ —á–∏—Å–ª–∞. –î–ª—è —ç—Ç–æ–≥–æ –º—ã —É–∂–µ –ø–æ—Å—Ç—Ä–æ–∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å.\n",
    "\n",
    "P.S. –ó–∞–ø—É—Å–∫–∞—Ç—å –±–ª–æ–∫ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "\n",
    "    print(\"Encoded input sequence:\".ljust(25), input_seq[i])\n",
    "    print(\"Encoded target sequence:\".ljust(25), target_seq[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –∏–∑ —á–∏—Å–µ–ª –Ω–∞–¥–æ —Å–¥–µ–ª–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–∞.\n",
    "\n",
    "–ü—Ä–∏—á–∏–Ω–∞ ‚Äî –ø–æ–¥–∞—á–∞ —á–∏—Å–µ–ª –¥–∞—Å—Ç –º–æ–¥–µ–ª–∏ –ª–æ–∂–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ –æ–± –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏ –ø–æ —Ç–∏–ø—É \"*–±—É–∫–≤–∞ –∞ –≤ –¥–≤–∞ —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –±—É–∫–≤—ã –±*\". –ù–∞–º –∂–µ –Ω—É–∂–Ω–æ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞—à–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤ –Ω–µ–∫–æ—Ç–æ—Ä–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/one_hot_encoding_softmax.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "\n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "\n",
    "\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\n",
    "    \"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(\n",
    "        input_seq.shape\n",
    "    )\n",
    ")\n",
    "print(input_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–ª–∏ –≤–µ–∫—Ç–æ—Ä–æ–º.\n",
    "–ù–µ —Å–ª–∏—à–∫–æ–º —ç–∫–æ–Ω–æ–º–Ω–æ, –∑–∞—Ç–æ —É–¥–æ–±–Ω–æ —É–º–Ω–æ–∂–∞—Ç—å –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É –≤–µ—Å–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–∏–º–µ—Ä: Language Modeling**\n",
    "\n",
    "–ö–æ–¥–∏—Ä—É–µ–º –±—É–∫–≤—ã –ø—Ä–∏ –ø–æ–º–æ—â–∏ **one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è** –∏ –ø–æ–¥–∞–µ–º –Ω–∞ –≤—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π.\n",
    "\n",
    "$\\begin{bmatrix} w_{11} & w_{12} & w_{13} & w_{14} \\\\ w_{21} & w_{22} & w_{23} & w_{24} \\\\ w_{31} & w_{32} & w_{33} & w_{34} \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 0\\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} w_{11} \\\\ w_{21} \\\\ w_{31}\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –æ–±—Ä–∞–±–æ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/language_modeling.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–º–Ω–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –Ω–∞ one-hot –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ –¥–æ—Å—Ç–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –Ω–µ–Ω—É–ª–µ–≤–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –∫–æ–ª–æ–Ω–∫—É –∏–∑ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤.\n",
    "–ü–æ—ç—Ç–æ–º—É —á–∞—Å—Ç–æ –≤–º–µ—Å—Ç–æ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –¥–≤—É—Ö –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–µ–≤ (one-hot + –ª–∏–Ω–µ–π–Ω–æ–≥–æ) –¥–µ–ª–∞—é—Ç –ø—Ä–æ—Å—Ç–æ —Å–ª–æ–π, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π **Embedding Layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensor\n",
    "import torch\n",
    "\n",
    "input_seq = torch.Tensor(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NextCharacterGenerator(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size=hidden_dim, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden_0 = torch.zeros(\n",
    "            1, batch_size, self.rnn.hidden_size\n",
    "        )  # 1 correspond to number of layers\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden_0)\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        # Need Only if n_layers > 1\n",
    "        out = out.contiguous().view(-1, self.rnn.hidden_size)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = NextCharacterGenerator(\n",
    "    input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1\n",
    ")\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 100\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Run\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    optimizer.zero_grad()  # Clears existing gradients from previous epoch\n",
    "    output, hidden = model(input_seq)\n",
    "    loss = criterion(output, target_seq.view(-1).long())\n",
    "    loss.backward()  # Does backpropagation and calculates gradients\n",
    "    optimizer.step()  # Updates the weights accordingly\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}/{num_epochs}\".ljust(20), end=\" \")\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "\n",
    "    out, hidden = model(character)\n",
    "    # print(out.shape)\n",
    "    # print(out)\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "\n",
    "def sample(model, out_len, start=\"hey\"):\n",
    "    model.eval()  # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for _ in range(size):\n",
    "        char, h = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "sample(model, 15, \"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    print(sample(model, 15, \"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–µ—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω—É–ª—è–º–∏ –∏ –Ω–∏–∫–∞–∫–æ–π —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –Ω–µ—Ç. –î–∞–∂–µ –µ—Å–ª–∏ –º—ã –¥–æ–±–∞–≤–∏–º –≤ –¥–∞—Ç–∞—Å–µ—Ç –µ—â—ë –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å *good*, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –∏–∑–º–µ–Ω–∏—Ç—Å—è. –¢–∞–∫–∂–µ —Å–µ—Ç—å –ø–µ—Ä–µ–æ–±—É—á–∏–ª–∞—Å—å –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —ç—Ç–∞–ø–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ ‚Äî **—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è**. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã ‚Äî –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Å–ª–æ–≤–∞. –ó–∞—Ç–µ–º —Å–æ–∑–¥–∞–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å, –≤ –∫–æ—Ç–æ—Ä—ã–π –∑–∞–Ω–æ—Å—è—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ª–µ–∫—Å–µ–º—ã, –≤—Å—Ç—Ä–µ—Ç–∏–≤—à–∏–µ—Å—è –≤ –∫–æ—Ä–ø—É—Å–µ –∏–ª–∏ —Ç–µ–∫—Å—Ç–µ. –ù–∞ —ç—Ç–∏—Ö —ç—Ç–∞–ø–∞—Ö –º–æ–∂–Ω–æ —Å—Ç–æ–ª–∫–Ω—É—Ç—å—Å—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞ 1. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è**\n",
    "\n",
    "–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ‚Äî –Ω–∞–∑–Ω–∞—á–∏—Ç—å –∫–∞–∂–¥–æ–º—É —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É —Å–ª–æ–≤—É —Å–≤–æ—ë —á–∏—Å–ª–æ. –ù–æ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º–∞: —Å–ª–æ–≤ –∏ –∏—Ö —Ñ–æ—Ä–º –º–∏–ª–ª–∏–æ–Ω—ã, –∏ –ø–æ—ç—Ç–æ–º—É —Å–ª–æ–≤–∞—Ä—å —Ç–∞–∫–∏—Ö —Å–ª–æ–≤ –ø–æ–ª—É—á–∏—Ç—Å—è —á–µ—Ä–µ—Å—á—É—Ä –±–æ–ª—å—à–∏–º, –∞ —ç—Ç–æ –±—É–¥–µ—Ç –∑–∞—Ç—Ä—É–¥–Ω—è—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ú–æ–∂–Ω–æ —Ä–∞–∑–±–∏–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞ —Å–ª–æ–≤–∞, –∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –±—É–∫–≤—ã (char-level tokenization), —Ç–æ–≥–¥–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ –±—É–¥–µ—Ç –≤—Å–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–µ—Å—è—Ç–∫–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –ù–û –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ —É–∂–µ —Å–∞–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –±—É–¥–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º, –∞ —ç—Ç–æ —Ç–æ–∂–µ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞ 2. –ë–æ–≥–∞—Ç–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è**\n",
    "\n",
    "\"–ù–µ–π—Ä–æ—Å–µ—Ç—å\", \"—Å–µ—Ç–∫–∞\", \"—Å–µ—Ç—å\" —è–≤–ª—è—é—Ç—Å—è —Ä–∞–∑–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –Ω–æ –∏–º–µ—é—Ç —Å—Ö–æ–∂–∏–π —Å–º—ã—Å–ª. –≠—Ç—É –ø—Ä–æ–±–ª–µ–º—É –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏ –≤—Å–µ–≥–¥–∞ —Ä–µ—à–∞–ª —ç—Ç–∞–ø **—Å—Ç–µ–º–º–∏–Ω–≥–∞** (—É–¥–∞–ª–µ–Ω–∏–µ —Å—É—Ñ—Ñ–∏–∫—Å–∞, –ø—Ä–∏—Å—Ç–∞–≤–∫–∏, –æ–∫–æ–Ω—á–∞–Ω–∏—è) –∏–ª–∏ **–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏** (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º–µ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞ 3. –°–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞**\n",
    "\n",
    "–ù–æ –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã —ç—Ç–∏ —ç—Ç–∞–ø—ã –Ω–µ —Ä–µ—à–∞—é—Ç. –í –≥–µ—Ä–º–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö (–≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –Ω–µ–º–µ—Ü–∫–æ–º, —à–≤–µ–¥—Å–∫–æ–º –∏ —Ç.–¥.) –æ—á–µ–Ω—å –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—é—Ç—Å—è –Ω–æ–≤—ã–µ —Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞. –ó–Ω–∞—á–µ–Ω–∏—è —Ç–∞–∫–∏—Ö —Å–ª–æ–≤ –≤—ã–≤–æ–¥—è—Ç—Å—è –∏–∑ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ò—Ö –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ –¥–æ–ª–≥–æ, –∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∏–∑ –Ω–∏—Ö –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ ¬´–±—É–º–∞–∂–Ω–æ–º¬ª —Å–ª–æ–≤–∞—Ä–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/swedish_word_example.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>–ü—Ä–∏–º–µ—Ä —à–≤–µ–¥—Å–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –≥–∞–µ—á–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –∫–æ–ª–µ—Å–∞ –º–æ—Ç–æ—Ü–∏–∫–ª–∞</a></em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://sysblok.ru/nlp/7250/\">–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —ç—Ç–∏–º–∏ —è–∑—ã–∫–∞–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–∞–∫–∂–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –Ω–∞ —ç—Ç–∞–ø–µ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è. –ü—Ä–∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É—é—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–æ, –µ—Å–ª–∏ –æ–Ω–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–æ—Å—å —á–∞—â–µ –ø—è—Ç–∏ —Ä–∞–∑), –ø–æ—ç—Ç–æ–º—É –Ω–µ –±—É–¥—É—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å —Ç–∞–∫–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –∏ —Å–ª–æ–∂–Ω–æ–µ —Å–ª–æ–≤–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞ 4: –ì—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤–∞**\n",
    "\n",
    "–î–ª—è –Ω–∞—Å, –ø—Ä–∏–≤—ã–∫—à–∏—Ö –∫ —è–∑—ã–∫–∞–º –µ–≤—Ä–æ–ø–µ–π—Å–∫–æ–≥–æ —Ç–∏–ø–∞, —Å–ª–æ–≤–æ ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –±—É–∫–≤ –º–µ–∂–¥—É –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è. –ù–æ –≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –º–Ω–æ–≥–∏–µ —Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –ø–∏—à—É—Ç—Å—è —Ä–∞–∑–¥–µ–ª—å–Ω–æ, –∞ –≤ —è–ø–æ–Ω—Å–∫–æ–º, –Ω–∞–æ–±–æ—Ä–æ—Ç, –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –≤–æ–æ–±—â–µ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤. –ü–æ—ç—Ç–æ–º—É —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞—Ç—å –±—ã–ª–æ –Ω–µ–ª–µ–≥–∫–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF** ‚Äî —Å–ø–æ—Å–æ–± —á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞, –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç  **–≤–∞–∂–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞**. –°–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –º–Ω–æ–∂–∏—Ç–µ–ª–µ–π $\\text{TF}$ –∏ $\\text{IDF}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/tf_idf.png\" width=\"500\"></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–≤–∞—è –∏–¥–µ—è **TF-IDF** ‚Äî **–µ—Å–ª–∏ —Å–ª–æ–≤–æ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –æ–Ω–æ –≤–∞–∂–Ω–æ–µ**. –ó–∞ —ç—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç $TF$.\n",
    "\n",
    "$\\text{TF}$ (term frequency) ‚Äî —á–∞—Å—Ç–æ—Ç–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ:\n",
    "\n",
    "$$\\large \\text{TF}(t, d) = \\frac{n_t}{\\sum_{k}n_k},$$\n",
    "\n",
    "$n_t$ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤–∞ $t$ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ $d$,\n",
    "\n",
    "$\\sum_{k}n_k$ ‚Äî –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤  $t$ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ $d$ —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Ç–æ—Ä–∞—è –∏–¥–µ—è **TF-IDF** ‚Äî **–µ—Å–ª–∏ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤–æ –º–Ω–æ–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –µ–≥–æ —Ü–µ–Ω–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–∞–µ—Ç—Å—è**.\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä: –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Ç–µ–∫—Å—Ç–æ–≤, –Ω–æ –Ω–µ –Ω–µ—Å—É—Ç —Å–º—ã—Å–ª–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏.\n",
    "\n",
    "–ó–∞ —ç—Ç–æ –æ—Ç–≤–µ—á–∞–µ—Ç $\\text{IDF}$.\n",
    "\n",
    "$\\text{IDF}$ (inverse document frequency) ‚Äî –ª–æ–≥–∞—Ä–∏—Ñ–º –æ–±—Ä–∞—Ç–Ω–æ–π —á–∞—Å—Ç–æ—Ç—ã –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ã.\n",
    "$$\\large \\text{IDF}(t, D) = \\log{\\frac{|D|}{|\\{d_i \\in D| t_i \\in d_i \\}|}},$$\n",
    "\n",
    "–≥–¥–µ $|D|$ ‚Äî —á–∏—Å–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏,\n",
    "$|\\{d_i \\in D| t_i \\in d_i \\}|$ ‚Äî —á–∏—Å–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å–æ —Å–ª–æ–≤–æ–º $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–æ–≥–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞:\n",
    "$$\\large \\text{TF-IDF}(t, d, D) = \\text{TF}(t,d)‚ãÖ\\text{IDF}(t, D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã `TfidfVectorizer` –∏–∑ [sklearn üõ†Ô∏è[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "–ö–∞–∂–¥–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä, —Ä–∞–≤–Ω—ã–π –¥–ª–∏–Ω–µ —Å–ª–æ–≤–∞—Ä—è. –ù–µ–Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –≤–∏–¥–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü `scipy.sparse.csr_matrix` [üõ†Ô∏è[doc]](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Tf-idf dictionary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Tf-idf dictionary len:\", len(vectorizer.get_feature_names_out()))\n",
    "print(\"Tf-idf shape:\", x.shape)\n",
    "print(\"Tf-idf type:\", type(x))\n",
    "print(\"Tf-idf values:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ**\n",
    "\n",
    "1. –¢–µ–∫—Å—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è. –ï—Å–ª–∏ –º—ã —Ä–µ—à–∞–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å N-–≥—Ä–∞–º–º–∞–º–∏, —Ç–æ –≤–º–µ—Å—Ç–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –ø–æ—Å–ª–µ –Ω–µ—ë –≤—ã–¥–µ–ª—è–µ–º N-–≥—Ä–∞–º–º—ã.\n",
    "\n",
    "2. –ü—Ä–æ—Ö–æ–¥–∏–º—Å—è –ø–æ –≤—Å–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º, –≤ –∫–∞–∫–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–Ω–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–æ—Å—å. –ï—Å–ª–∏ —É –Ω–∞—Å –±–æ–ª—å—à–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è, —Ç–æ —Å–ª–æ–≤–∞—Ä—å –º–æ–∂–µ—Ç –ø–æ–ª—É—á–∏—Ç—å—Å—è –ø—Ä–æ—Å—Ç–æ –≥–∏–≥–∞–Ω—Ç—Å–∫–∏–º, –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –º—ã —Ä–∞–±–æ—Ç–∞–µ–º —Å N-–≥—Ä–∞–º–º–∞–º–∏, –ø–æ—ç—Ç–æ–º—É –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏, –≤–æ –≤—Ä–µ–º—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–ª–∏ –ø–æ—Å–ª–µ —ç—Ç–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã, –º—ã –¥–æ–ª–∂–Ω—ã –≤—ã–±—Ä–æ—Å–∏—Ç—å –∏–∑ —Å–ª–æ–≤–∞—Ä—è –≤—Å—ë, —á—Ç–æ —Å—á–∏—Ç–∞–µ–º –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º ‚Äî —Å–ª–∏—à–∫–æ–º —Ä–µ–¥–∫–∏–µ –∏ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞.\n",
    "\n",
    "3. –ó–∞—Ç–µ–º –Ω–∞—á–∏–Ω–∞–µ–º —Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ—á–∫–∞ —ç—Ç–æ–π –º–∞—Ç—Ä–∏—Ü—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—É, –∞ –∫–∞–∂–¥—ã–π —Å—Ç–æ–ª–±–µ—Ü ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —ç—Ç–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –º—ã —Å—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –≤ –Ω—ë–º –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —è—á–µ–π–∫–∏ —Ç–∞–±–ª–∏—Ü—ã –≤–µ—Å–∞ —Å–ª–æ–≤ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º—É–ª–µ.\n",
    "\n",
    "4. –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É.\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, TF-IDF ‚Äî —ç—Ç–æ —Å–ø–æ—Å–æ–± –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –∏ –æ—Ç–±–æ—Ä–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è ‚Äî –Ω–µ —Ç–æ–ª—å–∫–æ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤. –ù–∞–¥–æ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ TF-IDF –Ω–∏–∫–∞–∫ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç–∫–µ –æ–±—ä–µ–∫—Ç–∞ ‚Äî —ç—Ç–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ, –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫. –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TF-IDF, –Ω–µ –∏–º–µ—è –º–µ—Ç–æ–∫, —Ç–æ –µ—Å—Ç—å –∑–∞–¥–∞—á–∞—Ö –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ ‚Äî –≤ —Ç–æ–º, —á—Ç–æ –º—ã —Ç–µ—Ä—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –µ—ë –∏—Å–ø–æ–ª—å–∑—É–µ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[wiki] üìö TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)\n",
    "\n",
    "[[doc] üõ†Ô∏è sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "[[doc] üõ†Ô∏è –ï—â—ë –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –∏ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "[[doc] üõ†Ô∏è TF-IDF Vectorizer scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ö–æ–≥–¥–∞ TF-IDF –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω:**\n",
    "\n",
    "1. **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏**: TF-IDF –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–º—ã—Å–ª–∞ —Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "2. **–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –¥–ª–∏–Ω–µ** –¥–æ–∫—É–º–µ–Ω—Ç–∞: –¥–ª–∏–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∏–º–µ—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è TF, –¥–∞–∂–µ –µ—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ä–µ–∂–µ. –í —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö TF-IDF –º–æ–∂–µ—Ç –Ω–µ–¥–æ–æ—Ü–µ–Ω–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/w2v_example.png\" width=\"350\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/585838/\">–°–µ–º–∞–Ω—Ç–∏–∫–∞ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è Word2Vec</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec ‚Äî –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –∏–∑ –¥–≤—É—Ö —Å–ª–æ–µ–≤, –∫–æ—Ç–æ—Ä–∞—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –µ–≥–æ –≤ —á–∏—Å–ª–æ–≤—ã–µ ‚Äú–≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ‚Äù —Å–ª–æ–≤–∞. –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ w2v ‚Äî —ç—Ç–æ –≥—Ä–æ–º–∞–¥–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ—Ä–ø—É—Å, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞ –≤—ã—Ö–æ–¥–µ –º—ã –ø–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ (–ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ), —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ—Ç–µ–Ω, –≥–¥–µ –∫–∞–∂–¥–æ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –≤ –∫–æ—Ä–ø—É—Å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–º –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞.\n",
    "\n",
    "–ú—ã —É—á–∏–º —Å–µ—Ç—å —Ç–∞–∫, —á—Ç–æ –≤–µ–∫—Ç–æ—Ä–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ—Ö–æ–∂–∏–º —Å–ª–æ–≤–∞–º, —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç—Å—è –±–ª–∏–∑–∫–æ –≤ –Ω–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ,  –∏ –Ω–∞–¥ –Ω–∏–º–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/w2v_example_2.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/446530/\">Word2vec –≤ –∫–∞—Ä—Ç–∏–Ω–∫–∞—Ö</a></em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ —Ç–æ—á–Ω–æ–º—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—é, –æ–¥–Ω–∞–∫–æ –ø–æ–ª—É—á–∞–µ–º—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è —Å–∞–º—ã–º–∏ –ø–æ—Ö–æ–∂–∏–º–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[colab] ü•® –ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é Word2Vec](https://colab.research.google.com/drive/1OJF0k-E60sp9Vyoj1yWuoedGTrwRiTyj?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–Ω–µ–µ –º—ã –ø—Ä–∏–º–µ–Ω—è–ª–∏ OneHotEncoding –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞—à–∏—Ö —Å–ª–æ–≤. –ü—Ä–æ–±–ª–µ–º—ã –≤–æ–∑–Ω–∏–∫–∞—é—Ç, –∫–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞—Å—Ç–∏ –∏ —É –Ω–∞—Å –≤–æ–∑–Ω–∏–∫–∞—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã.\n",
    "\n",
    "–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–±—ä–µ–∫—Ç—ã —É –Ω–∞—Å —Å—Ä–∞–∑—É –º–æ–≥—É—Ç –±—ã—Ç—å –±–ª–∏–∂–µ: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ \"–∫–æ—Ä–æ–ª—å\" –∏ \"–∫–æ—Ä–æ–ª–µ–≤–∞\" –æ—Ç–ª–∏—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ–ª–æ–º, —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ \"–∫–æ—Ä–æ–ª—å\" –∏ \"—Å—Ç—É–ª\" –∑–∞–º–µ—Ç–Ω–æ –≤—ã—à–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—ç—Ç–æ–º—É –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –Ω–∞—à–∏ —Å–ª–æ–≤–∞ –≤ –≤–µ–∫—Ç–æ—Ä–∞ –º–µ–Ω—å—à–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏ —ç—Ç–æ–º –±—É–¥—É—Ç —Å—Ä–∞–≤–Ω–∏–º—ã –º–µ–∂–¥—É —Å–æ–±–æ–π —Å –ø–æ–º–æ—â—å—é –º–æ–¥—É–ª—è `nn.Embedding` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
    "\n",
    "[[doc] üõ†Ô∏è –¢—É—Ç–æ—Ä–∏–∞–ª PyTorch –ø–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ NLP](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)\n",
    "\n",
    "[[blog] ‚úèÔ∏è NLP Course for you](https://lena-voita.github.io/nlp_course.html)\n",
    "\n",
    "[[git] üêæ –ö—É—Ä—Å –ø–æ NLP –æ—Ç –®–ê–î](https://github.com/yandexdataschool/nlp_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/word_representation_intro_min.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/lookup_table.gif\" width=\"600\">\n",
    "\n",
    "<em>Source: <a href=\"https://lena-voita.github.io/nlp_course/word_embeddings.html\">Lena Voita NLP Course</a></em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say you have 2 sentences (lowercased, punctuations removed):\n",
    "sentences = \"i am new to pytorch i am having fun\"\n",
    "\n",
    "words = sentences.split(\" \")\n",
    "\n",
    "print(f\"All words: {words} \\n\")\n",
    "\n",
    "vocab = set(words)  # create a vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary (unique words): {vocab} \\n\")\n",
    "print(f\"Vocabulary size: {vocab_size} \\n\")\n",
    "\n",
    "# map words to unique indices\n",
    "word2idx = {word: ind for ind, word in enumerate(vocab)}\n",
    "\n",
    "print(f\"Word-to-id dictionary: {word2idx} \\n\")\n",
    "\n",
    "encoded_sentences = [word2idx[word] for word in words]\n",
    "\n",
    "print(f\"Encoded sentences: {encoded_sentences}\")\n",
    "\n",
    "# let's say you want embedding dimension to be 3\n",
    "emb_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω —Ç–∞–∫:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "emb_layer = nn.Embedding(vocab_size, emb_dim)\n",
    "word_vectors = emb_layer(torch.LongTensor(encoded_sentences))\n",
    "\n",
    "print(f\"Shape of encoded sentences: {word_vectors.shape} \\n\")\n",
    "print(f\"Shape of weigths: {emb_layer.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠—Ç–æ—Ç –∫–æ–¥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é (—Å–æ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º 0 –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π 1). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–æ–∫–∞ —á—Ç–æ –Ω–∏–∫–∞–∫–æ–≥–æ —Ä–∞–∑–ª–∏—á–∏—è –∏–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –Ω–µ—Ç.\n",
    "\n",
    "`word_vectors` ‚Äî —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º (9, 3). 9 —Å–ª–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, —Ä–∞–∑–º–µ—Ä 3 –∑–∞–¥–∞–Ω –Ω–∞–º–∏.\n",
    "\n",
    "`emb_layer` –∏–º–µ–µ—Ç 1 –æ–±—É—á–∞–µ–º—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä `weight`, –∫–æ—Ç–æ—Ä—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True. –ú–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–∞–∫:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer.weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –º—ã –Ω–µ —Ö–æ—Ç–∏–º –æ–±—É—á–∞—Ç—å —ç—Ç–æ–π —Å–ª–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞—Ä–∞–Ω–µ–µ –æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏), –º—ã –º–æ–∂–µ–º –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –µ–≥–æ –≤–µ—Å–∞:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –≤–µ—Å–∞:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predefined weights\n",
    "weight = torch.FloatTensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "print(weight.shape)\n",
    "embedding = nn.Embedding.from_pretrained(weight)\n",
    "# get embeddings for ind 0 and 1\n",
    "embedding(torch.LongTensor([0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∫–∞—á–∞–µ–º —É–∂–µ –≥–æ—Ç–æ–≤—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ Word2Vec, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ Google News, —Å–æ—Å—Ç–æ—è—âe–º –∏–∑ 100 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Å–ª–æ–≤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this way for fast loading from Google Drive\n",
    "!gdown \"1ooLZnU8J54YzHtG5xXb3iDnY_TwIOY2K\"\n",
    "\n",
    "# Use this way for loading from our host\n",
    "# !wget https://edunet.kea.su/repo/EduNet-web_dependencies/weights/GoogleNews-vectors-negative300.bin.gz\n",
    "\n",
    "!gunzip -q GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wordvector_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(wordvector_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(word_vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding.from_pretrained(weight)\n",
    "\n",
    "input = torch.LongTensor([0, 1])\n",
    "\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ –º—ã –º–æ–∂–µ–º –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π [TorchText üõ†Ô∏è[doc]](https://pytorch.org/text/stable/index.html). –í–æ–∑—å–º—ë–º —Ç–∞–±–ª–∏—Ü—É –≤–µ—Å–æ–≤ –ø–æ–º–µ–Ω—å—à–µ, –≤—Å–µ–≥–æ 10000 –Ω–∞–∏–±–æ–ª–µ–µ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è —Å–ª–æ–≤. –ó–∞–¥–∞–¥–∏–º –¥–ª–∏–Ω—É —Ç–µ–Ω–∑–æ—Ä–∞ ‚Äî 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(\n",
    "    name=\"6B\", dim=50, max_vectors=10000\n",
    ")  # use 10k most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –º—ã —É–≤–∏–¥–∏–º, —á—Ç–æ 6–í ‚Äî —ç—Ç–æ [–ª–∏—à—å –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤–µ—Å–æ–≤ üõ†Ô∏è[doc]](https://pytorch.org/text/stable/_modules/torchtext/vocab/vectors.html#GloVe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_emb = nn.Embedding.from_pretrained(glove.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([0, 1])\n",
    "glove_emb(input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–¥ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —Å–æ —Å–ª–æ–µ–º `nn.Embedding` –≤—ã–≥–ª—è–¥–∏—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_with_Embedding_Layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RNN_with_Embedding_Layer, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN_with_Embedding_Layer(input_size=50, hidden_size=128, num_classes=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è ‚Äî –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä. –ö–∞–∫ –µ–≥–æ –≤—ã–±—Ä–∞—Ç—å?\n",
    "\n",
    "**–ó–∞–∫–æ–Ω –¶–∏–ø—Ñ–∞** ‚Äî —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞: –µ—Å–ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞ —è–∑—ã–∫–∞ –≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ —É–ø–æ—Ä—è–¥–æ—á–∏—Ç—å –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, —Ç–æ —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ –≤ —Ç–∞–∫–æ–º —Å–ø–∏—Å–∫–µ –æ–∫–∞–∂–µ—Ç—Å—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –µ–≥–æ –ø–æ—Ä—è–¥–∫–æ–≤–æ–º—É –Ω–æ–º–µ—Ä—É. –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¶–∏–ø—Ñ–∞:\n",
    "\n",
    "$$\\large f(\\text{rank}, s, N) = \\frac {1}{Z(s,N)\\text{rank}^s}$$\n",
    "\n",
    "–≥–¥–µ $\\text{rank}$ ‚Äî –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã, $s$ ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ —É–±—ã–≤–∞–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, $N$ ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤, $Z(s,N)= \\sum ^N _{i=1} i^{-s}$ ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–æ–Ω–Ω–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/zipfs_law.jpg\" width=\"500\"></center>\n",
    "\n",
    "[[arxiv] üéì Zipf‚Äôs law in 50 languages (YU et al.)](https://arxiv.org/ftp/arxiv/papers/1807/1807.01855.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–∑ —ç—Ç–æ–≥–æ –≤—Å–µ–≥–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–≤–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–∞:\n",
    "\n",
    "* —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤ –æ—á–µ–Ω—å –º–∞–ª–æ,\n",
    "* —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤ –æ—á–µ–Ω—å –º–Ω–æ–≥–æ.\n",
    "\n",
    "**–ß–∞—Å—Ç–æ—Ç–Ω—ã–µ** ‚Äî —Å–ª–∞–±–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã, —Ç–∞–∫ –∫–∞–∫ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤–æ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.\n",
    "\n",
    "**–†–µ–¥–∫–∏–µ** ‚Äî —Å–ª–æ–≤–∞ –æ—á–µ–Ω—å —Ä–µ–¥–∫–∏, –∏ –ø–æ—ç—Ç–æ–º—É –æ–Ω–∏ **–Ω–µ–Ω–∞–¥—ë–∂–Ω—ã** –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏ –ø—Ä–∏–Ω—è—Ç–∏–∏ —Ä–µ—à–µ–Ω–∏–π.\n",
    "\n",
    "–°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –Ω—É–∂–Ω–æ –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è –±–∞–ª–∞–Ω—Å–∞ **—á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏** –∏ **–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏**. –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –≤ —Ç–æ–º, —á—Ç–æ —á–µ–º —á–∞—â–µ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, —Ç–µ–º –±–æ–ª–µ–µ –æ–Ω–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Ç–µ–º –ª—É—á—à–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –µ–≥–æ —Ç–µ–º–∞—Ç–∏–∫—É. –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, —á–µ–º —ç—Ç–æ —Å–ª–æ–≤–æ —Ä–µ–∂–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ –∫–æ—Ä–ø—É—Å–µ, –≤ –≤—ã–±–æ—Ä–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ç–µ–º –æ–Ω–æ –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ. –ó–∞ —ç—Ç–æ—Ç –±–∞–ª–∞–Ω—Å –æ—Ç–≤–µ—á–∞—é—Ç –¥–≤–µ –≤–µ–ª–∏—á–∏–Ω—ã: TF –∏ IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –∞–ª–≥–æ—Ä–∏—Ç–º –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏ BPE –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º —É–∑–Ω–∞–≤–∞—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ —Å–ª–æ–≤ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º –æ–±—ä–µ–º–µ —Å–ª–æ–≤–∞—Ä—è.\n",
    "\n",
    "1.   –°–ª–æ–≤–æ = –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤\n",
    "2.   –°–ª–æ–≤–∞—Ä—å = –≤—Å–µ —Ç–æ–∫–µ–Ω—ã\n",
    "3.   –ü–æ–≤—Ç–æ—Ä—è—Ç—å, –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è:\n",
    "\n",
    "     –ù–∞–∑–Ω–∞—á–∞–µ–º –Ω–æ–≤—ã–º —Ç–æ–∫–µ–Ω–æ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–≤—É—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ\n",
    "–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —á–∞—â–µ –¥—Ä—É–≥–∏—Ö –ø–∞—Ä –≤ –∫–æ—Ä–ø—É—Å–µ (–≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ BPE –≤–æ–∑–º–æ–∂–Ω—ã —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã. –û–¥–∏–Ω –∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö ‚Äì –∏–¥—ë–º –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã, –Ω–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —Ç–æ–∫–µ–Ω."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/subword_tokenization.png\" width = \"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://alexanderdyakonov.wordpress.com/2019/11/29/—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è-–Ω–∞-–ø–æ–¥—Å–ª–æ–≤–∞-subword-tokenization/\">–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞ (Subword Tokenization)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠—Ç–æ—Ç –∂–µ —Å–ø–æ—Å–æ–± –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ—à–∏—Ç—å **–ø—Ä–æ–±–ª–µ–º—É** **OOV (out of vocabulary)**. –í –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å —Å–ª–æ–≤–∞ *Unfriendly*, –Ω–æ –ø–æ—Å–∫–æ–ª—å–∫—É **Unfriendly** = **Un** + **friend** + **ly**, –º—ã –º–æ–∂–µ–º —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å, —á—Ç–æ —Å–µ—Ç—å –±—É–¥–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å / –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ —Å–ª–æ–≤–æ —Ü–µ–ª–∏–∫–æ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/token_unfriendly.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.thoughtvector.io/blog/subword-tokenization/\">Subword Tokenization ‚Äî Handling Misspellings and Multilingual Data</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Ç—Ä–∞—Ç–∏—Ç—å –¥–Ω–∏ –Ω–∞ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤—Ä—É—á–Ω—É—é, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ —É–∂–µ –∏–º–µ—é—â–∏—Ö—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–í–∞–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç**: –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ `train-val-test`. –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å—Ç–æ–∏—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ `train`. –ü–æ—á–µ–º—É —Ç–∞–∫? –ö–æ–Ω–µ—á–Ω–∞—è —Ü–µ–ª—å –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ ‚Äî —ç—Ç–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å–µ—Ç—å –Ω–µ –≤–∏–¥–µ–ª–∞. –ü–æ—ç—Ç–æ–º—É –¥–ª—è –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω—è—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.\n",
    "\n",
    "–í –ª—é–±–æ–º —Å–ª—É—á–∞–µ, `test` –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç–¥–µ–ª–µ–Ω –æ—Ç –¥–∞–Ω–Ω—ã—Ö –µ—â–µ –¥–æ —Ç–æ–≥–æ, –∫–∞–∫ –æ–Ω–∏ –ø–æ–ø–∞–ª–∏ –≤ `DataLoader` –∏–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å.\n",
    "\n",
    "–î—Ä—É–≥–æ–µ –¥–µ–ª–æ, —á—Ç–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ —Ç–µ—Å—Ç–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –º–µ—Ç–æ–¥ –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Å–ª—É—á–∞–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ú–æ–∂–Ω–æ –≤–∑—è—Ç—å sample ‚Üí —Å–æ–∑–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –µ–≥–æ –∫–æ–ø–∏–π ‚Üí –ø–æ-—Ä–∞–∑–Ω–æ–º—É –∏—Ö –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å ‚Üí –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–ª–∞—Å—Å –Ω–∞ –∫–∞–∂–¥–æ–π –∏–∑ —ç—Ç–∏—Ö –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–ø–∏–π ‚Üí –∞ –ø–æ—Ç–æ–º –≤—ã–±—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π –∫–ª–∞—Å—Å –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ–º (—Ç–∞–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ [YOLOv5 üêæ[git]](https://github.com/ultralytics/yolov5/blob/d204a61834d0f6b2e73c1f43facf32fbadb6b284/models/yolo.py#L121), –æ –∫–æ—Ç–æ—Ä–æ–π —Ä–µ—á—å –ø–æ–π–¥–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –ª–µ–∫—Ü–∏—è—Ö)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê—É–¥–∏–æ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –∞—É–¥–∏–æ. –° –ø–æ–ª–Ω—ã–º —Å–ø–∏—Å–∫–æ–º –º–æ–∂–Ω–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è –∑–¥–µ—Å—å: [audiomentations üêæ[git]](https://github.com/iver56/audiomentations).\n",
    "\n",
    "–£—Å—Ç–∞–Ω–æ–≤–∏–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q audiomentations\n",
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/audio_example.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Get input audio\n",
    "input_audio = \"/content/audio_example.wav\"\n",
    "\n",
    "display(Audio(input_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "data, sr = librosa.load(\"/content/audio_example.wav\")  # sr - sampling rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background Noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import AddGaussianSNR\n",
    "\n",
    "augment = AddGaussianSNR(min_snr_in_db=3, max_snr_in_db=7, p=1)\n",
    "\n",
    "# Augment/transform the audio data\n",
    "augmented_data = augment(samples=data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°—Ä–∞–≤–Ω–∏–º –≤–æ–ª–Ω–æ–≤—ã–µ –∫–∞—Ä—Ç–∏–Ω—ã –∏ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import spectrogram\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def produce_plots(input_audio_arr, aug_audio, sr):\n",
    "    f, t, Sxx_in = spectrogram(\n",
    "        input_audio_arr, fs=sr\n",
    "    )  # Compute spectrogram for the original signal (f - frequency, t - time)\n",
    "    f, t, Sxx_aug = spectrogram(aug_audio, fs=sr)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 5))\n",
    "\n",
    "    ax[0, 0].plot(input_audio_arr)\n",
    "    ax[0, 0].set_xlim(0, len(input_audio_arr))\n",
    "    ax[0, 0].set_xticks([])\n",
    "    ax[0, 0].set_title(\"Original audio\")\n",
    "\n",
    "    ax[0, 1].plot(aug_audio)\n",
    "    ax[0, 1].set_xlim(0, len(input_audio_arr))\n",
    "    ax[0, 1].set_xticks([])\n",
    "    ax[0, 1].set_title(\"Augmented  audio\")\n",
    "\n",
    "    ax[1, 0].imshow(\n",
    "        np.log(Sxx_in),\n",
    "        extent=[t.min(), t.max(), f.min(), f.max()],\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"inferno\",\n",
    "    )\n",
    "    ax[1, 0].set_ylabel(\"Frequecny, Hz\")\n",
    "    ax[1, 0].set_xlabel(\"Time,s\")\n",
    "\n",
    "    ax[1, 1].imshow(\n",
    "        np.log(Sxx_aug, where=Sxx_aug > 0),\n",
    "        extent=[t.min(), t.max(), f.min(), f.max()],\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"inferno\",\n",
    "    )\n",
    "    ax[1, 1].set_ylabel(\"Frequecny, Hz\")\n",
    "    ax[1, 1].set_xlabel(\"Time,s\")\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import TimeStretch\n",
    "\n",
    "augment = TimeStretch(min_rate=0.8, max_rate=1.5, p=1)\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pitch Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import PitchShift\n",
    "\n",
    "augment = PitchShift(min_semitones=1, max_semitones=12, p=1)\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –°–æ–≤–º–µ—â–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –≤–º–µ—Å—Ç–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏, –º—ã –º–æ–∂–µ–º —Å–æ–≤–º–µ—â–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –≤–º–µ—Å—Ç–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise\n",
    "\n",
    "augment = Compose(\n",
    "    [\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1),\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.25, p=1),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–æ, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–≤—É–∫–∞ (–∏ –≤–æ–ª–Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤ —Ü–µ–ª–æ–º):\n",
    "- [[doc] üõ†Ô∏è torchaudio](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)\n",
    "- [[git] üêæ torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations)\n",
    "- [[git] üêæ AugLy](https://github.com/facebookresearch/AugLy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–µ–∫—Å—Ç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π —Ç–µ–∫—Å—Ç–∞. –° –ø–æ–ª–Ω—ã–º —Å–ø–∏—Å–∫–æ–º –º–æ–∂–Ω–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è –∑–¥–µ—Å—å: [nlpaug üêæ[git]](https://github.com/makcedward/nlpaug)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input text\n",
    "text = \"Hello, future of AI for Science! How are you today?\"\n",
    "print(f\"input text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å–∏–º–≤–æ–ª–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–º–µ–Ω–æ–π –Ω–∞ –ø–æ—Ö–æ–∂–µ –≤—ã–≥–ª—è–¥—è—â–∏–µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "augment = nac.OcrAug()\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–° –æ–ø–µ—á–∞—Ç–∫–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ —É—á–∏—Ç—ã–≤–∞—é—Ç —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –∫–ª–∞–≤–∏–∞—Ç—É—Ä–µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = nac.KeyboardAug()\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å–ª–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–° –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º–∏ –æ—à–∏–±–∫–∞–º–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "augment = naw.SpellingAug()\n",
    "augmented_text = augment.augment(text, n=3)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–æ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# model_type: word2vec, glove or fasttext\n",
    "augment = naw.ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"insert\")\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –∫–∞–∫–æ–π-–ª–∏–±–æ —è–∑—ã–∫, –∞ –∑–∞—Ç–µ–º –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –∏—Ö –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ —è–∑—ã–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞. –≠—Ç–æ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name=\"facebook/wmt19-en-de\", to_model_name=\"facebook/wmt19-de-en\"\n",
    ")\n",
    "augmented_text = back_translation_aug.augment(text)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instance Crossover Augmentation* ‚Äî —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Ç–æ–≥–æ –∂–µ –∫–ª–∞—Å—Å–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å—Ç—å –¥–≤–∞ –æ–±—ä–µ–∫—Ç–∞ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ \"–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤\":\n",
    "- <font color='red'>–æ—á–µ–Ω—å —É–¥–æ–±–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ. –ú–Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è</font>\n",
    "- <font color='green'>–∫–ª–∞—Å—Å! –û—Ç–ª–∏—á–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å</font>\n",
    "\n",
    "–¢–æ–≥–¥–∞ –º–æ–∂–Ω–æ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç —Ç–æ–≥–æ –∂–µ –∫–ª–∞—Å—Å–∞ –∏–∑ –∏—Ö —á–∞—Å—Ç–µ–π:\n",
    "- <font color='red'>–æ—á–µ–Ω—å —É–¥–æ–±–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ!</font> <font color='green'>–û—Ç–ª–∏—á–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∞–∂–Ω–æ: –ø—Ä–∏ –ª—é–±—ã—Ö –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è—Ö —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –µ—Å—Ç—å —à–∞–Ω—Å —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä–∞–Ω–Ω—ã–µ –∏ –Ω–µ–ª–æ–≥–∏—á–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö —Å–ª–µ–¥—É–µ—Ç —Å –æ—Å–æ–±–æ–π –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ  –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:\n",
    "\n",
    "- [[git] üêæ TextAugment](https://github.com/dsfsi/textaugment)\n",
    "- [[git] üêæ AugLy](https://github.com/facebookresearch/AugLy)\n",
    "\n",
    "[[blog] ‚úèÔ∏è –û–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏](https://amitness.com/2020/05/data-augmentation-for-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NLP –º–µ—Ç—Ä–∏–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[blog] ‚úèÔ∏è –¶–∏–∫–ª –ø–æ—Å—Ç–æ–≤ –æ–± NLP-–º–µ—Ç—Ä–∏–∫–∞—Ö](https://habr.com/ru/articles/745642/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ì–ª–æ–±–∞–ª—å–Ω–æ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–æ–∂–Ω–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ **—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ** –∏ **–Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ**.\n",
    "\n",
    "1. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç—ã –≤ —Ä–∞—Å—á–µ—Ç–µ –∏ –ø—Ä–æ–∑—Ä–∞—á–Ω—ã, –Ω–∞–∏–±–æ–ª–µ–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–∑ –Ω–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–æ –±—É–º–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π. –ß–∞—â–µ –≤—Å–µ–≥–æ —Ç–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –ø–æ–¥—Å—á–µ—Ç–µ —á–∏—Å–ª–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å–∏–º–≤–æ–ª–æ–≤ / —Å–ª–æ–≤ / c–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π ‚Äì –∏—Ö –Ω–∞–∑—ã–≤–∞—é—Ç ¬´lexic overlap metrics¬ª.\n",
    "\n",
    "2. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ 2016 –≥–æ–¥–∞ ‚Äì –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ. –ü–µ—Ä–≤—ã–º —à–∞–≥–æ–º –≤ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –≤ —Ä–∞—Å—á–µ—Ç–µ –º–µ—Ç—Ä–∏–∫ —Å—Ç–∞–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤ (embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L09/nlp_metrics.png\" width=\"1000\">\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/745642/\">–≠–≤–æ–ª—é—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–µ–≥–æ–¥–Ω—è –Ω–∞–∏–±–æ–ª–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ ‚Äì –º–µ—Ç—Ä–∏–∫–∏, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –¥–æ 2010 –≥–æ–¥–∞, –ø—Ä–∏—á–µ–º –≤ 99% —ç—Ç–æ BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–∏–±–æ–ª–µ–µ –∏–∑–≤–µ—Å—Ç–Ω—ã –∏ —É–ø–æ—Ç—Ä–µ–±–∏–º—ã —Ç–∞–∫–∏–µ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∫: BLEU, NIST, ROUGE, METEOR, TER, chrF, chrF++, RIBES.\n",
    "\n",
    "**BLEU**\n",
    "\n",
    "BLEU (BiLingual Evaluation Understudy) ‚Äî –º–µ—Ç—Ä–∏–∫–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –≤ IBM (Papineni et al.) –≤ 2001. –û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –ø–æ–¥—Å—á–µ—Ç–µ —Å–ª–æ–≤ (unigrams) –∏ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π (n‚Äëgrams) –∏–∑ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, —Ç–∞–∫–∂–µ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –≤ —ç—Ç–∞–ª–æ–Ω–µ. –î–∞–ª–µ–µ —ç—Ç–æ —á–∏—Å–ª–æ –¥–µ–ª–∏—Ç—Å—è –Ω–∞ –æ–±—â–µ–µ —á–∏—Å–ª–æ —Å–ª–æ–≤ –∏ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π –≤ –º–∞—à–∏–Ω–Ω–æ–º –ø–µ—Ä–µ–≤–æ–¥–µ ‚Äî –ø–æ–ª—É—á–∞–µ—Ç—Å—è precision. –ö –∏—Ç–æ–≥–æ–≤–æ–º—É precision –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ ‚Äî —à—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ç—å (brevity penalty), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ BLEU –¥–ª—è –∫—Ä–∞—Ç–∫–∏—Ö –∏ –Ω–µ–ø–æ–ª–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "candidate_corpus = [[\"My\", \"full\", \"pytorch\", \"test\"], [\"Another\", \"Sentence\"]]\n",
    "references_corpus = [\n",
    "    [[\"My\", \"full\", \"pytorch\", \"test\"], [\"Completely\", \"Different\"]],\n",
    "    [[\"No\", \"Match\"]],\n",
    "]\n",
    "print(bleu_score(candidate_corpus, references_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ó–∞–¥–∞—á–∞ Sequence-to-Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É **sequence-to-sequence**: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ $X$ –¥–ª–∏–Ω—ã $N$ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å $Y$ –¥–ª–∏–Ω—ã $T$. $T$ **–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —Ä–∞–≤–Ω–æ** $N$.\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä—ã **sequence-to-sequence** –∑–∞–¥–∞—á:\n",
    "*   –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥,\n",
    "*   –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å,\n",
    "*   –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏–ª–∏ –≤–∏–¥–µ–æ.\n",
    "\n",
    "–î–ª—è —Ä–µ—à–µ–Ω–∏—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á  –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤–µ **RNN**: **–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫** –∏ **–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫**.\n",
    "* –ó–∞–¥–∞—á–∞ **–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞**: –æ–±–æ–±—â–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ **–≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** $X = (x_1,..., x_N)$, —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–≤ **–≤–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** $C$ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.\n",
    "* –ó–∞–¥–∞—á–∞ **–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞**: –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ $C$, —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å **–≤—ã—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å** $Y = (y_1, ..., y_T)$.\n",
    "\n",
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –≤–µ–∫—Ç–æ—Ä–∞ $C$ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–µ **—Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ** –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ $h_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "* **–≤—Ö–æ–¥** ‚Äî –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å  $\\large x_1, \\dots, x_N$;\n",
    "* **–≤—ã—Ö–æ–¥** ‚Äî –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å  $\\large y_1, \\dots, y_T$.\n",
    "\n",
    "–ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç **–Ω—É–ª–µ–≤–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –≤–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** $\\large —Å$, –∫–æ—Ç–æ—Ä—ã–π —á–∞—Å—Ç–æ —Ä–∞–≤–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Å–∫—Ä—ã—Ç–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞.\n",
    "\n",
    "**–ö–æ–¥–∏—Ä–æ–≤—â–∏–∫:** $\\large h_i = f_w(x_i, h_{i_1})$\n",
    "\n",
    "**–î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫:** $\\large s_t = g_u(y_{t-1}, s_{t-1}, c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L09/out/seq_to_seq_with_rnn.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ —ç—Ç–æ–º –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∞—è **–ø—Ä–æ–±–ª–µ–º–∞:** –º—ã –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –±—É—Ç—ã–ª–æ—á–Ω–æ–µ –≥–æ—Ä–ª—ã—à–∫–æ ‚Äî –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ $h_N$. –ß—Ç–æ –±—É–¥–µ—Ç, –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 1000?\n",
    "\n",
    "**–ò–¥–µ—è:** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –¥–∞–Ω–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω **–≤–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã $c$, –≤ –∫–æ—Ç–æ—Ä—ã–π —Å–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ –≤—Å–µ–π **–≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** $(x_1,...,x_N)$.\n",
    "\n",
    "**–í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å** –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–∞–∫ –µ–¥–∏–Ω–∏—Ü—ã, —Ç–∞–∫ –∏ —Ç—ã—Å—è—á–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –í –∑–∞–¥–∞—á–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ **–≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é** –º–æ–∂–µ—Ç –±—ã—Ç—å:\n",
    "* –∫–æ—Ä–æ—Ç–∫–∞—è —Ñ—Ä–∞–∑–∞,\n",
    "* –∞–±–∑–∞—Ü ‚Äú–í–æ–π–Ω—ã –∏ –º–∏—Ä–∞‚Äù.\n",
    "\n",
    "–ö–æ–Ω—Ç–µ–∫—Å—Ç –≤–∞–∂–µ–Ω. –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–ª–∞–≥–æ–ª–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ –Ω—É–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å, –∫ –∫–∞–∫–æ–º—É —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–º—É –æ–Ω –æ—Ç–Ω–æ—Å–∏—Ç—Å—è, –∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∫–æ–Ω—Ü–∞ –∞–±–∑–∞—Ü–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–Ω–∏–º–∞—Ç—å, –æ —á–µ–º —à–ª–∞ —Ä–µ—á—å –≤ –µ–≥–æ –Ω–∞—á–∞–ª–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º—ã Sequence-to-Sequence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ —ç—Ç–æ–º –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã:\n",
    "- **–í–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** $c$ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –Ω–µ –º–æ–∂–µ—Ç –≤–º–µ—Å—Ç–∏—Ç—å –ª—é–±–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –ø–æ—ç—Ç–æ–º—É –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∫–∞—á–µ—Å—Ç–≤–æ –±—É–¥–µ—Ç —É—Ö—É–¥—à–∞—Ç—å—Å—è.\n",
    "- –ù–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ **—Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ** $s_t$ –¥–æ–ª–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, –∫–∞–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã **–≤—ã—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** —É–∂–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã. –ï—Å–ª–∏ $s_t$ –Ω–µ —Å–ø–æ—Å–æ–±–Ω–æ –≤–º–µ—Å—Ç–∏—Ç—å —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –∑–∞—Ü–∏–∫–ª–∏—Ç—å—Å—è –∏–ª–∏ –ø–æ—Ç–µ—Ä—è—Ç—å —á–∞—Å—Ç—å **–≤—ã—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**.\n",
    "\n",
    "**–í–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞** $c$ –∏ **—Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è** –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ $s_t$ —è–≤–ª—è—é—Ç—Å—è ‚Äú–±—É—Ç—ã–ª–æ—á–Ω—ã–º–∏ –≥–æ—Ä–ª—ã—à–∫–∞–º–∏‚Äù –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] üõ†Ô∏è –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Ç—É—Ç–æ—Ä–∏–∞–ª –æ—Ç PyTorch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "[[doc] üõ†Ô∏è Language translation with TorchText](\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ Unicode, –ø–æ—ç—Ç–æ–º—É –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –º—ã –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–∏–º–≤–æ–ª—ã Unicode –≤ ASCII, —Å–¥–µ–ª–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ—á–Ω—ã–º–∏ –∏ —É–±–µ—Ä—ë–º –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def normalizeStringRu(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-—è–ê-–Ø!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ–ª–∏–º —Ñ–∞–π–ª –Ω–∞ —Å—Ç—Ä–æ–∫–∏, –∞ —Å—Ç—Ä–æ–∫–∏ ‚Äî –Ω–∞ –ø–∞—Ä—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -q https://raw.githubusercontent.com/L1aoXingyu/seq2seq-translation/master/data/eng-fra.txt\n",
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/eng_rus_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = (\n",
    "        open(\"%s_%s_vocab.txt\" % (lang1, lang2), encoding=\"utf-8\")\n",
    "        .read()\n",
    "        .strip()\n",
    "        .split(\"\\n\")\n",
    "    )\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [l.split(\"\\t\")[:2] for l in lines]\n",
    "    eng = [normalizeString(s[0]) for s in pairs]\n",
    "    rus = [normalizeStringRu(s[1]) for s in pairs]\n",
    "    pairs = list(zip(rus, eng))\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å–æ–∫—Ä–∞—Ç–∏–º –¥–∞—Ç–∞—Å–µ—Ç –¥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–µ –¥–ª–∏–Ω–µ–µ 10 —Å–ª–æ–≤ –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \",\n",
    "    \"i m \",\n",
    "    \"he is\",\n",
    "    \"he s \",\n",
    "    \"she is\",\n",
    "    \"she s \",\n",
    "    \"you are\",\n",
    "    \"you re \",\n",
    "    \"we are\",\n",
    "    \"we re \",\n",
    "    \"they are\",\n",
    "    \"they re \",\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return (\n",
    "        len(p[0].split(\" \")) < MAX_LENGTH\n",
    "        and len(p[1].split(\" \")) < MAX_LENGTH\n",
    "        and p[1].startswith(eng_prefixes)\n",
    "    )\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"eng\", \"rus\", False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –≤ –Ω–∞—à–µ–º —Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–∏ –µ—Å—Ç—å –¥–≤–∞ —Å–ª–æ–≤–∞—Ä—è –∏ –Ω–∞–±–æ—Ä –ø–∞—Ä —Å—Ç—Ä–æ–∫.\n",
    "–û–ø—Ä–µ–¥–µ–ª–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫-–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–º–µ—Å—Ç–æ one_hot-–≤–µ–∫—Ç–æ—Ä–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è [—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ üõ†Ô∏è[doc]](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) —Ä–∞–∑–º–µ—Ä–æ–º 1√ó256 (hidden size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(\n",
    "            batch_size, 1, dtype=torch.long, device=device\n",
    "        ).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(\n",
    "                    -1\n",
    "                ).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return (\n",
    "            decoder_outputs,\n",
    "            decoder_hidden,\n",
    "            None,\n",
    "        )  # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –¥–ª—è –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏ –ø–∞—Ä—ã –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –∞ –Ω–∞ –≤—ã—Ö–æ–¥–µ –æ–∂–∏–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –ø–∞—Ä—ã –∏–∑ –¥—Ä—É–≥–æ–≥–æ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData(\"eng\", \"rus\", False)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, : len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, : len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(\n",
    "        torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device)\n",
    "    )\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, sampler=train_sampler, batch_size=batch_size\n",
    "    )\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion\n",
    "):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)), target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_dataloader,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_epochs,\n",
    "    learning_rate=0.001,\n",
    "    print_every=100,\n",
    "    plot_every=100,\n",
    "):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss = train_epoch(\n",
    "            train_dataloader,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            criterion,\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\n",
    "                \"%s (%d %d%%) %.4f\"\n",
    "                % (\n",
    "                    timeSince(start, epoch / num_epochs),\n",
    "                    epoch,\n",
    "                    epoch / num_epochs * 100,\n",
    "                    print_loss_avg,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%matplotlib inline\n",
    "plt.switch_backend(\"agg\")\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, num_epochs=80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids[0]:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –º–æ–∂–µ–º –æ—Ü–µ–Ω–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(\"RUS\", pair[0])\n",
    "        print(\"ENG\", pair[1])\n",
    "        output_words = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = \" \".join(output_words)\n",
    "        print(\"DNN\", output_sentence)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞</font>\n",
    "\n",
    "<font size=5>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö:</font>\n",
    "* [[blog] ‚úèÔ∏è –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π (1 —á–∞—Å—Ç—å)](https://habr.com/ru/articles/693562/)\n",
    "* [[blog] ‚úèÔ∏è Forecasting Time Series with Auto-Arima](https://www.alldatascience.com/time-series/forecasting-time-series-with-auto-arima/)\n",
    "\n",
    "<font size=5>–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏:</font>\n",
    "* [[article] üéì Session-based Recommendations with Recurrent Neural Networks](https://www.researchgate.net/publication/284579100_Session-based_Recommendations_with_Recurrent_Neural_Networks)\n",
    "* [[blog] ‚úèÔ∏è How to Remove Non-Stationarity From Time Series](https://www.kaggle.com/code/bextuychiev/how-to-remove-non-stationarity-from-time-series)\n",
    "* [[blog] ‚úèÔ∏è A Guide to Time Series Forecasting in Python](https://builtin.com/data-science/time-series-forecasting-python)\n",
    "* [[blog] ‚úèÔ∏è How to Check if Time Series Data is Stationary with Python?](https://www.geeksforgeeks.org/how-to-check-if-time-series-data-is-stationary-with-python/)\n",
    "* [[blog] ‚úèÔ∏è Complete Guide on Time Series Analysis in Python](https://www.kaggle.com/code/prashant111/complete-guide-on-time-series-analysis-in-python)\n",
    "* [[doc] üõ†Ô∏è Data transformations and forecasting models: what to use and when](https://people.duke.edu/~rnau/whatuse.htm)\n",
    "* [[colab] ü•® Time Series Prediction with LSTM Using PyTorch](https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/pytorch/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb#scrollTo=NabsV8O5BBd5)\n",
    "\n",
    "<font size=5>LSTM:</font>\n",
    "* [[article] üìö Long Short-Term Memory (Hochreiter & Schmidhuber, 1997)](http://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "* [[blog] ‚úèÔ∏è Feature Scaling Data with Scikit-Learn for Machine Learning in Python](https://stackabuse.com/feature-scaling-data-with-scikit-learn-for-machine-learning-in-python/)\n",
    "* [[blog] ‚úèÔ∏è Hands-On Machine Learning with Scikit-Learn and TensorFlow, —á.4](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch04.html)\n",
    "* [[arxiv] üéì How to avoid machine learning pitfalls:\n",
    "a guide for academic researchers (Lones, 2023)](https://arxiv.org/pdf/2108.02497.pdf)\n",
    "* [[blog] ‚úèÔ∏è A Beginner‚Äôs Guide on Recurrent Neural Networks with PyTorch](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n",
    "\n",
    "<font size=5>–ü—Ä–∏–º–µ—Ä –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞:</font>\n",
    "* [[git] üêæ RNN-walkthrough](https://github.com/gabrielloye/RNN-walkthrough/blob/master/main.ipynb)\n",
    "* [[blog] ‚úèÔ∏è The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "<font size=5>–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:</font>\n",
    "* [[wiki] üìö TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)\n",
    "* [[doc] üõ†Ô∏è sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "* [[doc] üõ†Ô∏è –ï—â—ë –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è –∏ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "* [[doc] üõ†Ô∏è TF-IDF Vectorizer scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)\n",
    "* [[colab] ü•® –ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é Word2Vec](https://colab.research.google.com/drive/1OJF0k-E60sp9Vyoj1yWuoedGTrwRiTyj?usp=sharing)\n",
    "* [[doc] üõ†Ô∏è –¢—É—Ç–æ—Ä–∏–∞–ª PyTorch –ø–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –≤ NLP](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)\n",
    "* [[blog] ‚úèÔ∏è NLP Course for you](https://lena-voita.github.io/nlp_course.html)\n",
    "* [[git] üêæ –ö—É—Ä—Å –ø–æ NLP –æ—Ç –®–ê–î](https://github.com/yandexdataschool/nlp_course)\n",
    "\n",
    "<font size=5>–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è:</font>\n",
    "* [[git] üêæ audiomentations](https://github.com/iver56/audiomentations)\n",
    "* [[doc] üõ†Ô∏è torchaudio](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)\n",
    "* [[git] üêæ torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations)\n",
    "* [[git] üêæ AugLy](https://github.com/facebookresearch/AugLy)\n",
    "* [[git] üêæ nlpaug](https://github.com/makcedward/nlpaug)\n",
    "* [[git] üêæ TextAugment](https://github.com/dsfsi/textaugment)\n",
    "* [[blog] ‚úèÔ∏è –û–±–∑–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏](https://amitness.com/2020/05/data-augmentation-for-nlp/)\n",
    "\n",
    "<font size=5>NLP –º–µ—Ç—Ä–∏–∫–∏:</font>\n",
    "* [[blog] ‚úèÔ∏è –¶–∏–∫–ª –ø–æ—Å—Ç–æ–≤ –æ–± NLP-–º–µ—Ç—Ä–∏–∫–∞—Ö](https://habr.com/ru/articles/745642/)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
