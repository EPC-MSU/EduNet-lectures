{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация признаков при помощи модели\n",
    "\n",
    "Если у вас есть модель, обученная на другом датасете, можно генерировать признаки при помощи нее. Например, при помощи случайного леса\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/generating_features_using_model.png\" width=\"700\">\n",
    "\n",
    "**Генерация бинарного признакового пространства с помощью RandomForest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление в модель признаков, полученных на основе другой модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "n_estimator = 10\n",
    "X, Y = make_classification(n_samples=80000) # define dummy dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)\n",
    "\n",
    "# It is important to train the ensemble of trees on a different subset\n",
    "# of the training data than the linear regression model to avoid\n",
    "# overfitting, in particular if the total number of leaves is\n",
    "# similar to the number of training samples\n",
    "X_train, X_train_lr, Y_train, Y_train_lr = train_test_split(\n",
    "    X_train, Y_train, test_size=0.5)\n",
    "\n",
    "\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
    "rf_enc = OneHotEncoder()\n",
    "rf_lm = LogisticRegression(max_iter=1000)\n",
    "rf.fit(X_train, Y_train)\n",
    "rf_enc.fit(rf.apply(X_train))\n",
    "rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), Y_train_lr)\n",
    "\n",
    "Y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\n",
    "fpr_rf_lm, tpr_rf_lm, _ = roc_curve(Y_test, Y_pred_rf_lm)\n",
    "\n",
    "# Supervised transformation based on gradient boosted trees\n",
    "grd = GradientBoostingClassifier(n_estimators=n_estimator)\n",
    "grd_enc = OneHotEncoder()\n",
    "grd_lm = LogisticRegression(max_iter=1000)\n",
    "grd.fit(X_train, Y_train)\n",
    "grd_enc.fit(grd.apply(X_train)[:, :, 0])\n",
    "grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]),\n",
    "           Y_train_lr)\n",
    "\n",
    "Y_pred_grd_lm = grd_lm.predict_proba(\n",
    "    grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
    "fpr_grd_lm, tpr_grd_lm, _ = roc_curve(Y_test, Y_pred_grd_lm)\n",
    "\n",
    "# The gradient boosted model by itself\n",
    "Y_pred_grd = grd.predict_proba(X_test)[:, 1]\n",
    "fpr_grd, tpr_grd, _ = roc_curve(Y_test, Y_pred_grd)\n",
    "\n",
    "# The random forest model by itself\n",
    "Y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(Y_test, Y_pred_rf)\n",
    "\n",
    "# Plot figure 1\n",
    "plt.figure(1, figsize=(10,10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
    "plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
    "plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Plot figure 2\n",
    "plt.figure(2, figsize=(10,10))\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
    "plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
    "plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# function for plotting principal components\n",
    "def plot_principal_components(data, model, scatter=True, legend=True):\n",
    "    W_pca = model.components_\n",
    "    if scatter:\n",
    "        plt.scatter(data[:,0], data[:,1])\n",
    "    plt.plot(data[:,0], -(W_pca[0,0]/W_pca[0,1])*data[:,0], color=\"c\")\n",
    "    plt.plot(data[:,0], -(W_pca[1,0]/W_pca[1,1])*data[:,0], color=\"c\")\n",
    "    if legend:\n",
    "        c_patch = mpatches.Patch(color='c', label='Principal components')\n",
    "        plt.legend(handles=[c_patch], loc='lower right')\n",
    "    # to better visualization:\n",
    "    plt.axis('equal')\n",
    "    limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1]))-0.5,\n",
    "              np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))+0.5]\n",
    "    plt.xlim(limits[0],limits[1])\n",
    "    plt.ylim(limits[0],limits[1])\n",
    "    plt.draw()\n",
    "\n",
    "# function for vector plotting\n",
    "def plot_components_vector(ax, data, vector, color, label=\"\", delta=0.5):\n",
    "    limits_x = [np.min(data[:,0])-delta, np.max(data[:,0])+delta]\n",
    "    limits_y = [np.min(data[:,1])-delta, np.max(data[:,1])+delta]\n",
    "    \n",
    "    if np.fabs(vector[1]) > 1e-5:\n",
    "        if np.fabs(vector[0]) > 1e-5:\n",
    "            x = np.arange(*limits_x, 0.1)\n",
    "            y = x * vector[1]/vector[0]\n",
    "        else:\n",
    "            y = np.arange(*limits_y, 0.1)\n",
    "            x = np.full_like(y, 0)\n",
    "    else:\n",
    "        x = np.arange(*limits_x, 0.1)\n",
    "        y = np.full_like(x, 0)\n",
    "        \n",
    "    ax.plot(x, y, color=color, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы семлировать многоразмерное нормальное распределение, можно использовать функцию:\n",
    "\n",
    "```python\n",
    "np.random.multivariate_normal(mu, covariance_matrix, size=500)\n",
    "```\n",
    "\n",
    "* mu - вектор средних значений (центр колокола);\n",
    "* covariance_matrix - матрица ковариации;\n",
    "* size - размер семплированной выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's sample data from multivariate normal distribution with covaraince matrix C \n",
    "\n",
    "fig, ax = plt.subplots(1, 1 , figsize = (8, 4))\n",
    "\n",
    "mu = np.zeros(2)\n",
    "C_simple = np.array([[4,0],\n",
    "                     [0,2]])\n",
    "\n",
    "data_simple = np.random.multivariate_normal(mu, C_simple, size=300)\n",
    "ax.scatter(data_simple[:,0], data_simple[:,1], alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, данные распределены в соответствии с двумерным распределением. А так как значения матрицы ковариации вне диагонали равны нулю, то корреляции между переменными нет, поэтому график не повернут.\n",
    "\n",
    "Давайте попробуем его повернуть. Для этого нам необходимо найти ковариационную матрицу с ненулевыми значениями вне диагонали.\n",
    "\n",
    "Так как матрица $ X^T X $ всегда положительно полуопределена (то есть собственные значения) > 0 (в самом деле $ w^T X^T X w = (Xw)^T Xw \\ge 0$) и симметрична (является эрмитовой), то необходимо просто подобрать подходящий X. \n",
    "Пусть это будет $ X = \\begin{pmatrix} 1 & 1 \\\\ 0 & 3 \\end{pmatrix}$. Тогда:\n",
    "$X^T X = \\begin{pmatrix} 1 & 1 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 2 & 3 \\\\ 3 & 9 \\end{pmatrix}$.\n",
    "\n",
    "Сгенерируем данные с такой ковариационной матрицей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1 , figsize = (8, 4))\n",
    "\n",
    "mu = np.zeros(2)\n",
    "\n",
    "C_angle = np.array([[2, 3], \n",
    "              [3, 9]])\n",
    "\n",
    "data_angle = np.random.multivariate_normal(mu, C_angle, size=300)\n",
    "ax.scatter(data_angle[:,0], data_angle[:,1], alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы знаем, теоретические истинные главные компоненты равны собственным векторам ковариационной матрицы. Давайте построим на наших графиках собственные вектора ковариационной матрицы и компоненты, которые нам выдаст в качестве своего ответа алгоритм PCA из библиотеки sklearn.\n",
    "\n",
    "Для поиска собственных значений можно использовать [функцию](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html)  \n",
    "```python\n",
    "np.linalg.eig(C)\n",
    "```\n",
    "\n",
    "\n",
    "Для PCA разложения функцию \n",
    "```python\n",
    "sklearn.decomposition.PCA\n",
    "```\n",
    "\n",
    "**Внимание! Attention! Achtung!** \n",
    "* np.linalg.eig возвращает вторым значением матрицу собственных векторов, в которой собственные вектора расположены по столбцам, т.е. v\\[:, i\\] есть собственный вектор соответствующий i-му собственному значению.\n",
    "* В то же время в поле класса PCA PCA.components_ лежат вектора главных компонент, которые располагаются там построчно (n_components, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_simple, eigen_vectors_simple = np.linalg.eig(C_simple)\n",
    "eigen_values_angle, eigen_vectors_angle = np.linalg.eig(C_angle)\n",
    "\n",
    "eigen_vectors_simple = eigen_vectors_simple.T\n",
    "eigen_vectors_angle = eigen_vectors_angle.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "# PCA\n",
    "pca_simple = sklearn.decomposition.PCA()\n",
    "pca_simple.fit(data_simple)\n",
    "\n",
    "pca_angle = sklearn.decomposition.PCA()\n",
    "pca_angle.fit(data_angle)\n",
    "\n",
    "# Let's compare it\n",
    "print('simple PCA components:\\n', pca_simple.components_)\n",
    "print('simple true components:\\n', eigen_vectors_simple)\n",
    "print('*' * 80)\n",
    "print('angle PCA components:\\n', pca_angle.components_)\n",
    "print('angle true components:\\n', eigen_vectors_angle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что PCA упорядочивает собственные вектора. Это значит, что собственный вектор соответствующий главной компоненте соответственно имеющей максимальную дисперсию будет находиться в первой строке. Этим объясняется разница во втором случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2 , figsize = (12, 6))\n",
    "\n",
    "ax1.scatter(data_simple[:,0], data_simple[:,1], alpha=0.2)\n",
    "plot_components_vector(ax1, data_simple, eigen_vectors_simple[0], 'g', 'True component 1')\n",
    "plot_components_vector(ax1, data_simple, eigen_vectors_simple[1], 'g', 'True component 2')\n",
    "\n",
    "plot_components_vector(ax1, data_simple, pca_simple.components_[0], 'r', 'PCA component 1')\n",
    "plot_components_vector(ax1, data_simple, pca_simple.components_[1], 'r', 'PCA component 2')\n",
    "\n",
    "s=15; ax1.set(xlim=(-s, s), ylim=(-s, s))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(data_angle[:,0], data_angle[:,1], alpha=0.2)\n",
    "plot_components_vector(ax2, data_angle, eigen_vectors_angle[0], 'g', 'True component 1')\n",
    "plot_components_vector(ax2, data_angle, eigen_vectors_angle[1], 'g', 'True component 2')\n",
    "\n",
    "plot_components_vector(ax2, data_angle, pca_angle.components_[0], 'r', 'PCA component 1')\n",
    "plot_components_vector(ax2, data_angle, pca_angle.components_[1], 'r', 'PCA component 2')\n",
    "ax2.legend()\n",
    "s=15; ax2.set(xlim=(-s, s), ylim=(-s, s))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы видите зеленые компоненты (истинные главные компоненты) полученные разложением ковариационной матрицы на собственные вектора отличаются от красных компонент, которые были получены вычислением PCA на данных. Это неудивительно, ибо данные были сгенерированы статистически. Если вы будете увеличивать размер сгенерированной выборки, то со временем разрешения экрана станет недостаточно для отображения разницы между PCA и реальными компонентами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также бывают ситуации, когда оптимально спроецировать не на некоторую плоскость, а на многообразие (кривая плоскость), как показано на картинке ниже.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/s_manifold.png\" width=650/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "В данном случае оптимально спроецировать на S-образную кривую. \n",
    "\n",
    "В связи с вышеописанными случаями, ниже мы рассмотрим более сильные методы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA Ядровой (нелинейный) метод главных компонент\n",
    "\n",
    "Как уже упоминалось, иногда невозможно захватить всю информацию линейной проекцией, хотя кривая поверхность с такой же размерностью это позволяет сделать. Одним из подходов к решению данной проблемы является задача перевода признаков в нелинейное пространство. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel trick\n",
    "\n",
    "\n",
    "Kernel Trick избегает явного перевода наших признаков в пространство новых признаков - ведь пространства бывают очень большие, а нам бы хотелось сэкономить память компьютера. \n",
    "Оказывается, для PCA не важны собственно признаки объектов, а важны скалярные произведения между объектами. \n",
    "\n",
    "И это скалярное произведением позволяет подсчитывать напрямую функция $k(\\mathbf {x} ,\\mathbf {x'})$, которую часто называют *ядром или ядерной функцией (kernel, kernel function)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бывают разные ядра, которые считают скалярное произведение в разных пространствах\n",
    "\n",
    "\n",
    "* $\\displaystyle k(x_i, x_j) = \\frac{1}{z} e^{-\\frac{h(x_i, x_j)^2}{h}}$ - радиальная базисная функция (RBF)\n",
    "* $k(x_i, x_j) = (<x_i, x_j> + c)^d; c, d \\in \\mathbb{R}$ - полиномиальное ядро \n",
    "* $k(x_i, x_j) = \\sigma((<x_i, x_j>)$ - ядро с функцией активации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Make_circles creates a data set of 400 points that form concentric circles with a gap of 50 points.\n",
    "# 2. The factor parameter controls the size of the inner circles.\n",
    "# 3. The noise parameter controls the amount of noise added to the data.\n",
    "# 4. The result is a 360-feature dataset of concentric circles with gaps.\n",
    "\n",
    "X, Y = make_circles(n_samples=400, factor=.3, noise=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем две концентрические окружности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.title(\"Original space\")\n",
    "reds = Y == 0\n",
    "blues = Y == 1\n",
    "\n",
    "plt.scatter(X[reds, 0], X[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X[blues, 0], X[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычный PCA не может их разделить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_pca[reds, 0], X_pca[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_pca[blues, 0], X_pca[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Projection by PCA\")\n",
    "plt.xlabel(\"1st principal component\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот KernelPCA  справляется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a PCA object to perform the PCA transformation \n",
    "#    using the RBF kernel (specified using kernel=\"rbf\").\n",
    "#    Setting fit_inverse_transform=True. This will make the object use the\n",
    "#    transformed data from the first step when transforming new, unseen data points.\n",
    "# 2. Let the PCA object fit and transform the data, \n",
    "#    then get the transformed data back.\n",
    "\n",
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(r\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя, конечно, и восстанавливать он будет не идеально - работал-то он по факту в пространстве бОльшей размерности и оси строил там"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The 'kpca' variable is a KernelPCA object that is initialized with 'n_components' set to 2. \n",
    "# 2. It then applies the kernel function specified in the 'kernel' variable  and then transforms the data based on the kernel, and gets the transformed data.\n",
    "# 3. It then returns the transformed data.\n",
    "# 4. Then we get the inverse transformation by simply calling \"kpca.inverse_transform(X_kpca)\"\n",
    "# 5. Finally, we plot the transformed data.\n",
    "\n",
    "X_back = kpca.inverse_transform(X_kpca)\n",
    "plt.scatter(X_back[reds, 0], X_back[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_back[blues, 0], X_back[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Original space after inverse transform\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Kernel PCA довольно чувствителен к выбору ядра.\n",
    "\n",
    "К примеру, для данных, расположенных на трех окружностях:\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/kernel_pca_three_circles.png\" width=260/>\n",
    "\n",
    "\n",
    "в зависимости от выбора ядра мы будем получать совершенно разные отображение в спрямляющее пространство:\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/kernel_pca_different_kernels.png\" width=600/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### $\\color{brown}{\\text{Это на потом}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Часть полей можно исключить (имя)\n",
    "\n",
    "2. Часть преобразовать в числа (пол, порт посадки ...)\n",
    "\n",
    "\n",
    "3. Непрерывные данные можно нормировать (здесь вместо этого берется квадратный корень из цены)\n",
    "\n",
    "4. На основании некоторых создать новые более полезные для модели (Номер кабины)\n",
    "\n",
    "\n",
    "\n",
    "cabin_data = array([\"C65\", \"\", \"E36\", \"C54\", \"B57 B59 B63 B66\"])\n",
    "->\n",
    "[['C', 65, 1], ['X', -1, 0], ['E', 36, 1], ['C', 54, 1], ['B', 57, 4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The categorical-to-numerical function\n",
    "# Changed to automatically add column names\n",
    "def cat_to_num(data): # one-hot encoding\n",
    "  categories = set(data) \n",
    "  features = {}\n",
    "  for cat in categories:\n",
    "      binary = (data == cat)\n",
    "      if len(set(binary)) == 1:\n",
    "        # Ignore features where all values equal \n",
    "        continue\n",
    "      new_key = f'{data.name}={cat}'\n",
    "\n",
    "      features[new_key] = binary.astype(\"int\")\n",
    "  return pd.DataFrame(features)\n",
    "\n",
    "def cabin_features(data):\n",
    "    features = []\n",
    "    for cabin in data:\n",
    "        cabins = str(cabin).split(\" \")\n",
    "        n_cabins = len(cabins)\n",
    "        # First char is the cabin_char\n",
    "        try:\n",
    "            cabin_char = cabins[0][0]\n",
    "        except IndexError:\n",
    "            cabin_char = \"X\"\n",
    "            n_cabins = 0\n",
    "        # The rest is the cabin number\n",
    "        try:\n",
    "            cabin_num = int(cabins[0][1:]) \n",
    "        except:\n",
    "            cabin_num = -1\n",
    "        # Add 3 features for each passanger\n",
    "        features.append( [cabin_char, cabin_num, n_cabins] )\n",
    "    features=np.array(features)\n",
    "    dic_of_features =  {\n",
    "        'Cabin_num' : features[:,1].astype(\"int\"),\n",
    "        'N_cabins' : features[:,2].astype(\"int\"),\n",
    "         }\n",
    "    out = pd.DataFrame(dic_of_features)\n",
    "    char_column = pd.DataFrame({'Cabin_char' : features[:,0]})\n",
    "    cabin_ch = cat_to_num(char_column['Cabin_char'])\n",
    "    return out.join(cabin_ch)\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"Takes a dataframe of raw data and returns ML model features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initially, we build a model only on the available numerical values\n",
    "    features = data.drop([\"PassengerId\", \"Survived\", \"Fare\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"], axis=1)\n",
    "    \n",
    "    # Setting missing age values to -1\n",
    "    features[\"Age\"] = data[\"Age\"].fillna(-1)\n",
    "    \n",
    "    # Adding the sqrt of the fare feature\n",
    "    features[\"sqrt_Fare\"] = np.sqrt(data[\"Fare\"])\n",
    "    \n",
    "    # Adding gender categorical value\n",
    "    features = features.join( cat_to_num(data['Sex']) )\n",
    "    \n",
    "    # Adding Embarked categorical value\n",
    "    features = features.join( cat_to_num(data['Embarked']) )\n",
    "\n",
    "    # Split cabin\n",
    "    features = features.join( cabin_features(data['Cabin']) )\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = prepare_data(dataset) #Create variable features\n",
    "features[:5] #Display first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь модель можно обучать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We make a 80/20% train/test split of the data\n",
    "features = prepare_data(dataset)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, dataset[\"Survived\"], test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "# Make predictions\n",
    "print('Accuracy of the model = %.2f' % model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bins = [0, 1, 5, 10, 25, 50, 100]\n",
    "dataset['binned_age'] = pd.cut(dataset['Age'], bins)\n",
    "dataset[['binned_age', 'Age']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['binned_age', 'Age']].to_numpy()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
