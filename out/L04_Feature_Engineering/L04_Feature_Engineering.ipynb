{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Feature Engineering</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных \n",
    "\n",
    ">Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine\" learning is basically feature engineering. @Andrew Ng\n",
    "\n",
    "Общая схема классического машинного обучения выглядит так. Даже в случае нейросетей некая предобработка исходных данных все равно не бывает лишней\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация признаков\n",
    "\n",
    "> Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. @Dr. Jason Brownlee\n",
    "\n",
    "**Генерация признаков** - процесс придумывания способов описания данных с помощью простых значений, которые должны отражать характеристики объектов исследований, через которые выражаются целевые значения \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально объекты в нашем датасете могут быть представлены в виде описаний, которые не являются признаковыми, либо, очевидно, требует некоторой предобработки:\n",
    "\n",
    "1. веб-страницы \n",
    "2. файлы \n",
    "3. ссылки на участников группы\n",
    "4. измерения в разных единицах (см, м, дц)\n",
    "и т.д \n",
    "\n",
    "Большая часть моделей неспособна работать с такими представлениями в сыром виде и или просто не запустится, либо будет выдавать неадекватные результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс создания признаков зависит от модели, которую мы собираемся использовать. Для одних моделей полезно добавить признаки, полученные делением/перемножением исходных. Другие модели могут провести эти операции сами и экономнее/менее переобучаясь. Как вариант, добавление признаков, явно зависящих от друг друга может даже мешать некоторым моделям. \n",
    "\n",
    "\n",
    "Например, плохая идея добавлять в обычную линейную модель как признаки X1 и X2, так и их сумму. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_2.png\" width=\"700\">\n",
    "\n",
    "Вообще говоря, надо понимать, что процесс feature engneering является критическим местом, bottleneck, в машинном обучении. Все, что ваша модель будет знать о данных решается на этом этапе. Больше, чем вы ей дадите - она не узнает. \n",
    "\n",
    "Если вы в данных дадите явную подсказку об ответе - то она будет использовать эту подсказку, а реальные закономерности может и не выучить. К примеру, можно дать ей в качестве признака id покупателя, который каждую неделю покупает одно и то же. Если таких ситуаций будет много, то она и выучит, что надо предсказывать все по id. Когда же к вам придет новый покупатель или у старого, что-то поменяется в поведении, модель начнет вести себя неадекватно. \n",
    "\n",
    "Точно такую же роль может сыграть информация о номере эксперимента, лаборатории, в которой его проводили, аспиранте, который его проводил и тд. \n",
    "\n",
    "Такая ситуация будет называться **data leakage**.\n",
    "\n",
    "\n",
    "Ну и понятно, что если вы дадите модели только нерелевантную информацию, она ничего из нее не вытащит. \n",
    "\n",
    "> At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. @ Prof. Pedro Domingos\n",
    "\n",
    "> The algorithms we used are very standard for Kagglers. …We spent most of our efforts in feature engineering. … We were also very careful to discard features likely to expose us to the risk of over-fitting our model.  @Xavier Conort, топ-участник Kaggle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы признаков \n",
    "\n",
    "Традиционно признаки делятся на \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вещественные \n",
    "Вещественные признаки бывают:\n",
    "\n",
    " * дискретные. Например - число лайков от пользователей\n",
    " \n",
    " <img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/social_media_likes.jpg\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * непрерывные. Например - температура\n",
    "\n",
    "\n",
    " <img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/18532010839_d5fee9ae9f_o.png\" width=\"600\">\n",
    " \n",
    "Понятно, что разделение часто условное. Тот же возраст можно посчитать и дискретной переменной (пользователь всегда нам сообщает свои полные года), и непрерывной (возраст можно считать с любой точностью, но никто не будет) )\n",
    "\n",
    "\n",
    "Также иногда вещественные признаки делят на относительные (считаются относительно чего-то, уже нормированные и тд)  и интервальные. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Категориальные \n",
    "\n",
    "\n",
    "\n",
    "Значение -  принадлежность к какой-то из категорий. Традиционно делятся на сильно отличающиеся по свойствам:\n",
    " * упорядоченные (ординальные) - для каждой пары возможных категорий можем сказать, какая больше, а какая меньше. Например - класс места. Или размер одежды\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_8.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * неупорядоченные (номинальные) - категории между собой несравнимы. Обычно нельзя сказать, что красный телефон больше синего. Или что солнечная погода больше снежной\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_7.png\" width=\"900\">\n",
    "\n",
    "\n",
    "\n",
    "Часто мы сталкиваемся с бинарными категориальными признаками - для которых известно только две возможных категории (например, биологический пол человека). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразования \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вещественных признаков \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинаризация \n",
    "\n",
    "Например, нам может быть не интересно, сколько конкретно раз встретилось явление в наблюдении - главное, что оно вообще встретилось. Тогда мы просто превращаем наш вещественный признак в бинарный \"было ли явление\", и работаем уже с ним. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Округление\n",
    "\n",
    "Часто данные до нас доходят с очень высокой точностью после запятой. Нужно ли это нашей модели - часто нет. Иногда по факту два наблюдения не различаются  по этому признаку (разница в пределах статошибки), но по признаку их отличить можно. Это может приводить к переобучению. В таких случаях разумно признаки округлить. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bining (Бинирование)\n",
    "\n",
    "Опять же, нам не интересны точные значения - например, что видео набрало 1000 лайков, а не 1001. \n",
    "\n",
    "К тому же, число просмотров/лайков некоторых видео может быть очень большим в сравнении с остальными, что будет приводить к неадекватному поведению. \n",
    "В итоге часть значений у нас встречается часто, а часть - очень редко. Это может приводить к неадекватному поведению модели. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed-width bining\n",
    "\n",
    "Просто бьем наши значения по диапазонам фиксированной длины. Так часто поступают с возрастом. \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_9.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaptive Binning\n",
    "\n",
    "Это не всегда работает хорошо. Например, распределение зарплат у нас очень сильно скошено вправо. \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_10.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И бинирование с фиксированной длиной бина нам не поможет справиться с редкими значениями.\n",
    "\n",
    "В этой ситуации помогает бинирование, например, по квантилям - когда границы бина представляют собой квантили. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_11.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логарифмирование\n",
    "\n",
    "С ситуацией, когда распределено скошено вправо работает и другой подход - прологаримфировать величину. \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_12.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Обобщением этого подхода явлется [Box-Cox Transform](https://www.statisticshowto.com/box-cox-transformation/#:~:text=A%20Box%20Cox%20transformation%20is,a%20broader%20number%20of%20tests.), общей целью которой является придать данным вид более похожий на нормальнео распределение, с которым работает бОльшее число моделей и сходимость лучше \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категориальных признаков "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding \n",
    "\n",
    "Просто берем и каждой категории однозначно сопоставляем число. \n",
    "Очень простой способ, если признак ординальный - будет работать почти всегда. \n",
    "\n",
    "Если же наш признак - номинальный, то могут возникнуть проблемы. Мы не можем сказать, что салатовый больше красного (в большинстве случаев). Но модель ничего про это не знает и после нашего кодирования спокойно такие сравнения может производить. Это может приводить к более низкому качеству модели и выучиванию ею неправильной информации. Кроме того, например, деревьям решений , чтобы выделить в таком случае конкретную категорию придется делать сразу несколько действий, которые, в силу жадности алгоритма их построения, могут и не быть найдены\n",
    "\n",
    "\n",
    "\n",
    " Некоторые модели (например, lightgbm) автоматически могут перекодировать все правильно, если им сообщить, что переданный признак - категориальный. Для некоторых это придется делать вручную. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding \n",
    "\n",
    "Схемой, которая часто используется на практике, является one-hot encoding. Он состоит том, что вместо одного категориального признака X создается набор бинарных категориальных признаков, которые отвечают на вопрос \"X == C? \", где C пробегает все возможные значения категориального признака. \n",
    "\n",
    "Теперь чтобы обусловиться на конкретное значение категориального признака, дереву решений достаточно задать один вопрос. \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_rgb.png\" width=\"450\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у такой схемы есть один минус - мы получаем линейно зависимые признаки. Это может плохо влиять на некоторые модели (для случая нейронных сетей - обычно нет, но полезно держать в голове). \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_3.png\" width=\"850\">\n",
    "\n",
    "Потому иногда одну из категорий исключают при кодировании, например, в примере выше можно исключить Fish, ведь если все три других признака-категории равны 0, то точно верно, что категория - Fish. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target encoding \n",
    "\n",
    "Кодируем каждую категорию каким-то численным параметром, характеризующим то, что мы предсказываем. Например, можно каждую категорию категориального признака заменять на среднее \n",
    "\n",
    "На самом деле, так просто делать нельзя, можно получить переобученную модель. Как делать - можете подробно посмотреть, к примеру, [здесь](https://github.com/Dyakonov/PZAD/blob/master/2020/PZAD2020_042featureengineering_07.pdf) или [здесь](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv?redirectTo=%2Flecture%2Fcompetitive-data-science%2Fconcept-of-mean-encoding-b5Gxv) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "Можно научить вашу модель саму сопоставлять каждой категории некий вектор определенной размерности. Для этого вначале сопоставляем каждой категории случайный вектор заданной длины. А далее изменяем этот вектор как обычные веса. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_4.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование циклических категориальных признаков\n",
    "\n",
    "В случае с такими признаками, как день недели или время суток, мы сталкиваемся с проблемой того, что нам нужно предложить кодирование, которое будет учитывать, что понедельник близок к воскресенью так же, как понедельник же ко вторнику, и тд. \n",
    "\n",
    "В случае деревьев решений и методов на них основанных можно \"забить\" - такие методы сами разберутся. Для некоторых других методов, тех же нейросетей, правильно кодирование может улучшить качество и сходимость. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Давайте нанесем наши категории, например, дни недели - на окружность. Как это сделать? \n",
    "Пусть понедельнику соответствует 1, а воскресенью - 7. Далеее посчитаем два таких вспомогательных признака по следующим формулам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "a = np.arange(1, 8)\n",
    "print(a)\n",
    "sina = np.sin(a * np.pi * 2 / np.max(a))\n",
    "cosa = np.cos(a * np.pi * 2 / np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(sina, cosa)\n",
    "for  i, z in enumerate( (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\") ):\n",
    "  plt.text(sina[i], cosa[i], s=z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать дальше? А по сути мы уже все сделали. Теперь расстояния между понедельником и вторником и воскресеньем и понедельником одинаковые:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mon_tue = (sina[1] - sina[0]) ** 2 + (cosa[1] - cosa[0]) ** 2\n",
    "dist_sun_mon = (sina[6] - sina[0]) ** 2 + (cosa[6] - cosa[0]) ** 2\n",
    "print(dist_mon_tue, dist_sun_mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "то же самое верно и для любых отстоящих друг от друга на одинаковое число дней\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mon_wed = (sina[2] - sina[0]) ** 2 + (cosa[2] - cosa[0]) ** 2\n",
    "dist_fri_sun = (sina[4] - sina[6]) ** 2 + (cosa[4] - cosa[6]) ** 2\n",
    "print(dist_mon_wed, dist_fri_sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, циклические признаки можно кодировать парой признаков - sin и cos, полученных по схеме, описанной выше. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Проблемы подхода\n",
    "\n",
    "1. Деревья решений могут решить задачу и так. А такое кодирование им, наоборот, будет мешать, т.к они работают с одним признаком за раз\n",
    "\n",
    "2. Надо понимать, что важность исходной категориальной фичи неочевидным образом делится между двумя полученными из нее таким образом. \n",
    "\n",
    "3. В некоторых задачах one-hot работает лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование взаимодействия признаков\n",
    "\n",
    "Признаки могут по-разному взаимодейстовать и некоторые модели в принципе не могут моделировать это взаимодействие. \n",
    "\n",
    "\n",
    "Взаимодействовать могут вещественные переменные и категориальные\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_st_vs_sp.png\" width=\"550\">\n",
    "\n",
    "**Сила vs. скорость**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "категориальные и категориальные \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_blood_int.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "вещественные и вещественные \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_SVM_8.png\" width=\"300\">\n",
    "\n",
    "\n",
    "Могут быть и более высокуровневые взаимодействия - взаимодействуют много разных признаков.\n",
    "\n",
    "Взаимодействия могут быть самые разные - много способов кодировать. Например, добавлять в число признаков их произведение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация признаков при помощи модели\n",
    "\n",
    "Если у вас есть модель, обученная на другом датасете, можно генерировать признаки при помощи нее. Например, при помощи случайного леса\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_RF.png\" width=\"700\">\n",
    "\n",
    "**Генерация бинарного признакового пространства с помощью RandomForest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Хорошие источники \n",
    "\n",
    "1. [Feature Selection for High-Dimensional Data](https://www.springer.com/gp/book/9783319218571)\n",
    "2. [How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science)\n",
    "3. **Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists Paperback** – April 14, 2018 by Alice Zheng , Amanda Casar\n",
    "4. [Сайт](https://dyakonov.org/) и [курс](https://github.com/Dyakonov/PZAD) Дьяконова\n",
    "5. Серия статей на towardsdatascience, [первая из серии](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "6. [A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "7. [Про кодирование циклических признаков](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример на Titanic\n",
    "\n",
    "Для иллюстрации будут использованны примеры из книги \"[Real-World Machine Learning](https://www.manning.com/books/real-world-machine-learning)\" из открытого [репозитория](https://github.com/brinkar/real-world-machine-learning)\n",
    "\n",
    "\n",
    "И датасет [Titanic](http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf\n",
    ")\n",
    "\n",
    "В отличие от датасетов с которыми мы работали до сих пор, это просто список пассажиров судна. Данные в нем не предобработанны и в сыром виде не могут быть использованны для обучения модели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/brinkar/real-world-machine-learning/master/data/titanic.csv\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"titanic.csv\")\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Часть полей можно исключить (имя)\n",
    "\n",
    "2. Часть преобразовать в числа (пол, порт посадки ...)\n",
    "\n",
    "\n",
    "3. Непрерывные данные можно нормировать (здесь вместо этого берется квадратный корень из цены)\n",
    "\n",
    "4. На основании некоторых создать новые более полезные для модели (Номер кабины)\n",
    "\n",
    "\n",
    "\n",
    "cabin_data = array([\"C65\", \"\", \"E36\", \"C54\", \"B57 B59 B63 B66\"])\n",
    "->\n",
    "[['C', 65, 1], ['X', -1, 0], ['E', 36, 1], ['C', 54, 1], ['B', 57, 4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The categorical-to-numerical function from chapter 2\n",
    "# Changed to automatically add column names\n",
    "def cat_to_num(data): # one-hot encoding\n",
    "  categories = set(data) # кошка, собака, енот a = 1 - b - c\n",
    "  features = {}\n",
    "  for cat in categories:\n",
    "      binary = (data == cat)\n",
    "      if len(set(binary)) == 1:\n",
    "        # Ignore features where all values equal \n",
    "        continue\n",
    "      new_key = f'{data.name}={cat}'\n",
    "\n",
    "      features[new_key] = binary.astype(\"int\")\n",
    "  return pd.DataFrame(features)\n",
    "\n",
    "def cabin_features(data):\n",
    "    features = []\n",
    "    for cabin in data:\n",
    "        cabins = str(cabin).split(\" \")\n",
    "        n_cabins = len(cabins)\n",
    "        # First char is the cabin_char\n",
    "        try:\n",
    "            cabin_char = cabins[0][0]\n",
    "        except IndexError:\n",
    "            cabin_char = \"X\"\n",
    "            n_cabins = 0\n",
    "        # The rest is the cabin number\n",
    "        try:\n",
    "            cabin_num = int(cabins[0][1:]) \n",
    "        except:\n",
    "            cabin_num = -1\n",
    "        # Add 3 features for each passanger\n",
    "        features.append( [cabin_char, cabin_num, n_cabins] )\n",
    "    features=np.array(features)\n",
    "    dic_of_features =  {\n",
    "        #'Cabin_char' : features[:,0],\n",
    "        'Cabin_num' : features[:,1].astype(\"int\"),\n",
    "        'N_cabins' : features[:,2].astype(\"int\"),\n",
    "         }\n",
    "    out = pd.DataFrame(dic_of_features)\n",
    "    char_column = pd.DataFrame({'Cabin_char' : features[:,0]})\n",
    "    cabin_ch = cat_to_num(char_column['Cabin_char'])\n",
    "    return out.join(cabin_ch)\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"Takes a dataframe of raw data and returns ML model features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initially, we build a model only on the available numerical values\n",
    "    features = data.drop([\"PassengerId\", \"Survived\", \"Fare\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"], axis=1)\n",
    "    \n",
    "    # Setting missing age values to -1\n",
    "    features[\"Age\"] = data[\"Age\"].fillna(-1)\n",
    "    \n",
    "    # Adding the sqrt of the fare feature\n",
    "    features[\"sqrt_Fare\"] = np.sqrt(data[\"Fare\"])\n",
    "    \n",
    "    # Adding gender categorical value\n",
    "    features = features.join( cat_to_num(data['Sex']) )\n",
    "    \n",
    "    # Adding Embarked categorical value\n",
    "    features = features.join( cat_to_num(data['Embarked']) )\n",
    "\n",
    "    # Split cabin\n",
    "    features = features.join( cabin_features(data['Cabin']) )\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = prepare_data(data)\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь модель обучать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We make a 80/20% train/test split of the data\n",
    "features = prepare_data(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data[\"Survived\"], test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.preprocessing \n",
    "\n",
    "Для целей предварительной обработки признаков существует множество инструментов, в том числе модуль preprocessing в пакете sklearn. \n",
    "\n",
    "Аналогичные подмодули или целые библиотеки есть и для разных задач, связанных с нейронными сетями (torchvision, torchaudio и прочее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = make_column_transformer(\n",
    "    (OneHotEncoder(sparse=False,\n",
    "                   handle_unknown='ignore'),\n",
    "     make_column_selector(dtype_include='category')),\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop([\"PassengerId\", \"Survived\", \"Fare\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"],\n",
    "                     axis=1)\n",
    "\n",
    "features['Cabin'] = data['Cabin'].fillna(\"X\").apply(lambda x: x[0]).astype(\"category\")\n",
    "\n",
    "def get_cab_num(cab):\n",
    "  try:\n",
    "    return int(cab.split()[0][1:])\n",
    "  except:\n",
    "    return -1 \n",
    "features['Cabin_num'] = data['Cabin'].fillna(\"X\").apply(lambda x: get_cab_num(x))\n",
    "\n",
    "features['N_cabins'] = data['Cabin'].fillna(\"X\").str.split(\" \").apply(lambda x: len(x))\n",
    "\n",
    "features['Sex'] = data['Sex'].astype(\"category\")\n",
    "\n",
    "features['Embarked'] = data['Embarked'].fillna(\"X\").astype(\"category\")\n",
    "features['sqrt_Fare'] = np.sqrt(data['Fare'])\n",
    "features['Age'] = data['Age'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, data[\"Survived\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder.fit(X_train)\n",
    "X_train = one_hot_encoder.transform(X_train)\n",
    "X_test = one_hot_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление в модель признаков, полученных на основе другой модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "n_estimator = 10\n",
    "X, y = make_classification(n_samples=80000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# It is important to train the ensemble of trees on a different subset\n",
    "# of the training data than the linear regression model to avoid\n",
    "# overfitting, in particular if the total number of leaves is\n",
    "# similar to the number of training samples\n",
    "X_train, X_train_lr, y_train, y_train_lr = train_test_split(\n",
    "    X_train, y_train, test_size=0.5)\n",
    "\n",
    "\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
    "rf_enc = OneHotEncoder()\n",
    "rf_lm = LogisticRegression(max_iter=1000)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_enc.fit(rf.apply(X_train))\n",
    "rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n",
    "\n",
    "y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\n",
    "fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n",
    "\n",
    "# Supervised transformation based on gradient boosted trees\n",
    "grd = GradientBoostingClassifier(n_estimators=n_estimator)\n",
    "grd_enc = OneHotEncoder()\n",
    "grd_lm = LogisticRegression(max_iter=1000)\n",
    "grd.fit(X_train, y_train)\n",
    "grd_enc.fit(grd.apply(X_train)[:, :, 0])\n",
    "grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]),\n",
    "           y_train_lr)\n",
    "\n",
    "y_pred_grd_lm = grd_lm.predict_proba(\n",
    "    grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\n",
    "fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n",
    "\n",
    "# The gradient boosted model by itself\n",
    "y_pred_grd = grd.predict_proba(X_test)[:, 1]\n",
    "fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "# The random forest model by itself\n",
    "y_pred_rf = rf.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "\n",
    "plt.figure(1, figsize=(10,10))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
    "plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
    "plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2, figsize=(10,10))\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\n",
    "plt.plot(fpr_grd, tpr_grd, label='GBT')\n",
    "plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (zoomed in at top left)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры данных которые нецелесообразноо отправлять в модель в сыром виде:\n",
    "\n",
    "### IP адрес: \n",
    "xxx.xxx.xxx.xxx -> регион, провайдер\n",
    "\n",
    "### Координаты атомов:\n",
    "[AlphaFold](https://yakovlev.me/para-slov-za-alphafold2/\n",
    ")\n",
    "\n",
    "\n",
    "\"как вообще математически можно выразить структуру белка. До того мы постоянно неявно подразумевали, что это координаты атомов (всех, каркасных, Cα или каких-то ещё), но на практике это очень неудачное представление, поскольку оно не единственное. Мы обычно считаем, что предсказание работает как некоторая детерминированная функция: принимает на вход последовательность и всегда возвращает один единственный ответ. Но какой из бесконечного набора координат \"канонический\"?\n",
    "\n",
    "\n",
    "\n",
    "\"... инвариантом структуры является матрица (таблица) всех попарных расстояний между атомами. \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлых лекциях вы познакомились с тем, что такое признаки. Сегодня мы попытаемся научиться отделять полезные признаки от бесполезных, а также понижать размерность пространства признаков.\n",
    "\n",
    "## Зачем отбирать признаки?\n",
    "\n",
    "\n",
    "*Во-первых*, признаков может быть слишком много, больше чем нужно. Это может возникнуть в ситуациях, когда используется вся имеющаяся на данный момент информация, потому что неизвестно, какая её часть может понадобиться, а какая — нет. В таких случаях можно повысить качество решения задачи, выбирая только действительно важные признаки. Существует другой подход: можно сформировать новые признаки на основе старых, таким образом признаков станет меньше, но их информативность сохранится.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Во-вторых*, существуют признаки, из-за которых при решении задачи возникает много проблем. Это шумовые признаки — признаки, которые не связаны с целевой переменной и никак не относятся к решаемой задаче. К сожалению, не всегда можно понять по обучающей выборке, что в ней присутствуют такие признаки.\n",
    "Полезно рассмотреть несколько примеров присутствия шумовых признаков в данных. Пусть в выборку добавляют 1000 признаков. Значения каждого признака генерируются из стандартного нормального распределения. Понятно, что эти признаки бесполезны, они никак не помогут решить задачу. Но, поскольку их много, может так оказаться (из соображений теории вероятностей), что один из них коррелирует с целевой переменной. При этом он будет коррелировать только на обучающей выборке, а на контрольной выборке корреляции наблюдаться не будет, поскольку признак абсолютно случайный. Однако внутри модели этот признак может быть учтён как важный и иметь какой-то вес. Получается, что модель зависит от признака, который никак не помогает решить задачу. Из-за этого качество модели и ее обобщающая способность окажутся ниже, чем хотелось бы. \n",
    "\n",
    "В более общем случае можно говорить, что в многоразмерном пространстве почти всегда можно найти корреляции. См. картинку.\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_correlations.png\" alt=\"alttext\" width=600/>\n",
    "\n",
    "[Больше подобных примеров](https://www.wnycstudios.org/podcasts/otm/articles/spurious-correlations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*В-третьих*, перед нами может стоять задача ускорения модели. Дело в том, что чем больше признаков, тем более сложная модель получается, и тем больше времени необходимо, чтобы построить прогноз. Существуют задачи, в которых прогнозы нужно строить очень быстро, например, выдача рекомендаций товаров на сайте интернет-магазина. Пользователь что-то ищет, нажимает на ссылку в поисковой выдаче и переходит на страницу интересующего его товара. На этой странице есть поле, в котором показываются рекомендации к этому товару, например похожие товары, которые должна выдавать модель. Важно, чтобы она выдавала рекомендации очень быстро, страница не должна долго загружаться, чтобы пользователь не подумал, что с сайтом что-то не так, и не ушел к конкуренту. В этом случае необходимо, чтобы модель была очень быстрой, и один из подходов к ускорению модели — это отбор признаков, которых достаточно, чтобы прогнозы были хорошими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полный перебор \n",
    "\n",
    "Можно попытаться перебрать все возможные комбинации признаков. Но даже для 100  признаков такой подход будет считаться до конца Вселенной. \n",
    "\n",
    "Потому прибегают к эвристикам, которые, очевидно, могут пропускать оптимальное решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Одномерный отбор признаков\n",
    "\n",
    "Самый простой подход к отбору признаков — это одномерный подход. В нём оценивается связь каждого признака с целевой переменной, например, измеряется корреляция. Такой подход — довольно простой, он не учитывает сложные закономерности, в нём все признаки считаются независимыми, тогда как в машинном обучении модели учитывают взаимное влияние признаков, их пар или даже более сложные действия на целевую переменную. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формализация задачи\n",
    "Пусть у нас есть N объектов с K признаками и для каждого объекта задана целевая переменная или ответ. Обозначим  матрицу объектов-признаков через $X \\in \\mathbf{R}^{N x M} $, а вектор ответов через $Y$. Для удобства введем следующие дополнительные обозначения:\n",
    "\n",
    "* $\\overline{X}_j$- среднее значение признака $j$ по всей выборке\n",
    "* $\\overline{Y}$- среднее значение целевой переменной на всей выборке\n",
    "\n",
    "Задача — оценить предсказательную силу (информативность) каждого признака, то есть насколько хорошо по данному признаку можно предсказывать целевую переменную. Данные оцененной информативности можно использовать, чтобы отобрать $K$ лучших признаков или признаки, у которых значение информативности больше порога (например, некоторой квантили распределения информативности).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Корреляция\n",
    "\n",
    "Один из самых простых методов измерения связи между признаком и ответами — это корреляция. Корреля́ция (от лат. correlatio «соотношение»), или корреляцио́нная зави́симость — статистическая взаимосвязь двух или более случайных величин (либо величин, которые можно с некоторой допустимой степенью точности считать таковыми). При этом изменения значений одной или нескольких из этих величин сопутствуют систематическому изменению значений другой или других величин.\n",
    "\n",
    "Коэффициент корреляции $R$ определяется формулой:\n",
    "\n",
    "$ R = \\frac{\\sum_{i=1}^{N} (X_{ij} - \\overline{X}_j)(Y_{i} - \\overline{Y})} {\\sqrt{ \\sum_{i=1}^{N}(X_{ij} - \\overline{X}_j)^2\\sum_{i=1}^{N} (Y_{i} - \\overline{Y})^2}} $\n",
    "\n",
    "\n",
    "Чем больше по модулю корреляция между признаком и целевой переменной, тем более информативным является данный признак. При этом она максимальна по модулю ($Rj$= ±1), если между признаком и целевой переменной есть **линейная связь**, то есть если целевую переменную можно строго линейно выразить через значение признака. Это означает, что корреляция измеряет только линейную информативность, то есть способность признака линейно предсказывать целевую переменную. Вообще говоря, корреляция рассчитана на вещественные признаки и вещественные ответы. Тем не менее, её можно использовать в случае, если признаки и ответы бинарные (имеет смысл кодировать бинарный признак с помощью значений ±1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/brinkar/real-world-machine-learning/master/data/titanic.csv\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The categorical-to-numerical function from chapter 2\n",
    "# Changed to automatically add column names\n",
    "def cat_to_num(data): # one-hot encoding\n",
    "  categories = set(data) # кошка, собака, енот a = 1 - b - c\n",
    "  features = {}\n",
    "  for cat in categories:\n",
    "      binary = (data == cat)\n",
    "      if len(set(binary)) == 1:\n",
    "        # Ignore features where all values equal \n",
    "        continue\n",
    "      new_key = f'{data.name}={cat}'\n",
    "\n",
    "      features[new_key] = binary.astype(\"int\")\n",
    "  return pd.DataFrame(features)\n",
    "\n",
    "def cabin_features(data):\n",
    "    features = []\n",
    "    for cabin in data:\n",
    "        cabins = str(cabin).split(\" \")\n",
    "        n_cabins = len(cabins)\n",
    "        # First char is the cabin_char\n",
    "        try:\n",
    "            cabin_char = cabins[0][0]\n",
    "        except IndexError:\n",
    "            cabin_char = \"X\"\n",
    "            n_cabins = 0\n",
    "        # The rest is the cabin number\n",
    "        try:\n",
    "            cabin_num = int(cabins[0][1:]) \n",
    "        except:\n",
    "            cabin_num = -1\n",
    "        # Add 3 features for each passanger\n",
    "        features.append( [cabin_char, cabin_num, n_cabins] )\n",
    "    features=np.array(features)\n",
    "    dic_of_features =  {\n",
    "        #'Cabin_char' : features[:,0],\n",
    "        'Cabin_num' : features[:,1].astype(\"int\"),\n",
    "        'N_cabins' : features[:,2].astype(\"int\"),\n",
    "         }\n",
    "    out = pd.DataFrame(dic_of_features)\n",
    "    char_column = pd.DataFrame({'Cabin_char' : features[:,0]})\n",
    "    cabin_ch = cat_to_num(char_column['Cabin_char'])\n",
    "    return out.join(cabin_ch)\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"Takes a dataframe of raw data and returns ML model features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initially, we build a model only on the available numerical values\n",
    "    features = data.drop([\"PassengerId\", \"Survived\", \"Fare\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"], axis=1)\n",
    "    \n",
    "    # Setting missing age values to -1\n",
    "    features[\"Age\"] = data[\"Age\"].fillna(-1)\n",
    "    \n",
    "    # Adding the sqrt of the fare feature\n",
    "    features[\"sqrt_Fare\"] = np.sqrt(data[\"Fare\"])\n",
    "    \n",
    "    # Adding gender categorical value\n",
    "    features = features.join( cat_to_num(data['Sex']) )\n",
    "    \n",
    "    # Adding Embarked categorical value\n",
    "    features = features.join( cat_to_num(data['Embarked']) )\n",
    "\n",
    "    # Split cabin\n",
    "    features = features.join( cabin_features(data['Cabin']) )\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = prepare_data(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data[\"Survived\"], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "correlations = []\n",
    "for column in features:\n",
    "  r , p_value = stats.pearsonr(X_train[column], y_train)\n",
    "  correlations.append((column,r))\n",
    "\n",
    "correlations.sort(key=lambda tup: abs(tup[1]),reverse=True)\n",
    "for name, r in correlations:\n",
    "  print(f'{name} : {r:.3f} ')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC-ROC\n",
    "\n",
    "\n",
    "Пусть решается задача бинарной классификации, и необходимо оценить важность признака $j$ для решения именно этой задачи. В этом случае можно попробовать построить классификатор, который использует лишь этот один признак $j$, и оценить его качество. Например, можно рассмотреть очень простой классификатор, который берёт значение признака $j$ на объекте, сравнивает его с порогом $t$, и если значение меньше этого порога, то он относит объект к первому классу, если же меньше порога — то к другому, нулевому или минус первому, в зависимости от того, как мы его обозначили. Далее, поскольку этот классификатор зависит от порога $t$, то его качество можно измерить с помощью таких метрик, как площадь под ROC-кривой или Precision-Recall кривой, а затем по данной площади отсортировать все признаки и выбирать лучшие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from scipy import stats\n",
    "\n",
    "features = prepare_data(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data[\"Survived\"], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "rocs = []\n",
    "for column in features:\n",
    "  # use feature as score directly\n",
    "  r1 = roc_auc_score(y_score=X_train[column], y_true=y_train) \n",
    "  # use feature as score in reversed manner\n",
    "  r2 = roc_auc_score(y_score=-X_train[column], y_true=y_train)\n",
    "   \n",
    "  r = max(r1, r2)\n",
    "  rocs.append((column,r))\n",
    "\n",
    "rocs.sort(key=lambda tup: max(1 - tup[1], tup[1]),reverse=True)\n",
    "for name, r in rocs:\n",
    "  print(f'{name} : {r:.3f} ')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Проблемы одномерного отбора признаков\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_multidim.png\" alt=\"alttext\" width=600/>\n",
    "\n",
    "\n",
    "У подхода, при котором важности всех признаков оцениваются по отдельности, есть свои недостатки. На левом рисунке изображена двумерная выборка, для которой необходимо решить задачу классификации. Если спроецировать данную выборку на ось абсцисс, то она будет разделима, хотя и будут присутствовать ошибки. Если же спроецировать данную выборку на ось ординат, то все объекты разных классов перемешаются, и выборка будет неразделима. В этом случае при использовании любого метода одномерного оценивания информативности первый признак будет информативен, а второй — совершенно неинформативен. Тем не менее, видно, что если использовать эти признаки одновременно, то классы будут разделимы идеально. На самом деле, второй признак важен, но он важен только в совокупности с первым, и методы одномерного оценивания информативности не способны это определить. На рисунке справа показана выборка, на которой одномерные методы оценки информативности работают ещё хуже. В этом случае, если спроецировать выборку на ось абсцисс или ординат, то объекты классов перемешаются, и в обоих случаях данные будут совершенно неразделимы. И, согласно любому из описанных методов, оба признака неинформативны. Тем не менее, если использовать их одновременно, то, например, решающее дерево может идеально решить данную задачу классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример: Влияние роста и веса при предсказании вероятности сердечного заболевания. Избыточный вес может являться важным фактором, но оценить является ли он избыточным или нормальным можно только зная рост пациента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Жадный отбор признаков\n",
    "\n",
    "\n",
    "Жадные методы отбора признаков, по сути своей, являются надстройками над методами обучения моделей. Они перебирают различные подмножества признаков и выбирают те из них, которое дает наилучшее качество определённой модели машинного обучения.\n",
    "Данный процесс устроен следующим образом. Обучение модели считается черным ящиком, который на вход принимает информацию о том, какие из его признаков можно использовать при обучении модели, обучает модель, и дальше каким-то методом оценивается качество такой модели, например, по отложенной выборке или кросс-валидации. Таким образом, задача, которую необходимо решить — это оптимизация функционала качества модели по подмножеству признаков.\n",
    "\n",
    "### Полный перебор\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_graphic-full.png\" alt=\"alttext\" width=600/>\n",
    "\n",
    "\n",
    "Самый простой способ решения данной задачи — это полный перебор всех подмножеств признаков и оценивание качества на каждом подмножестве. Итоговое подмножество — то, на котором качество модели наилучшее. Этот перебор можно структурировать и перебирать подмножества последовательно: сначала те, которые имеют мощность 1 (наборы из 1 признака), потом наборы мощности 2, и так далее. Это подход очень хороший, он найдет оптимальное подмножество признаков, но при этом очень сложный, поскольку всего таких подмножеств $2^d$, где $d$ — число признаков. Если признаков много — сотни или тысячи, то такой перебор невозможен: он займет слишком много времени, возможно, сотни лет или больше. Поэтому такой метод подходит либо при небольшом количестве признаков, либо если известно, что информативных признаков очень мало, единицы.\n",
    "\n",
    "\n",
    "### Жадное добавление\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_graphic-greedy.png\" alt=\"alttext\" width=600/>\n",
    "\n",
    "\n",
    "Если же признаков много и известно, что многие из них информативны, то нужно применять жадную стратегию. Жадная стратегия используется всегда, когда полный перебор не подходит для решения задачи. Например, может оказаться неплохой стратегия жадного наращивания (жадного добавления). Сначала находится один признак, который дает наилучшее качество модели (наименьшую ошибку $Q$):\n",
    "\n",
    "$i_1 = argmin Q(i)$. \n",
    "\n",
    "Тогда множество, состоящее из этого признака:\n",
    "\n",
    "$J_1 = {i_1}$\n",
    "\n",
    "Дальше к этому множеству добавляется еще один признак так, чтобы как можно сильнее уменьшить ошибку модели:\n",
    "\n",
    "$i_2 =argminQ(i_1,i)$, $J_2 ={i_1,i_2}$.\n",
    "\n",
    "Далее каждый раз добавляется по одному признаку, образуются множества J3 , J4 , . . . . Если в какой-то момент невозможно добавить новый признак так, чтобы уменьшить ошибку, процедура останавливается. Жадность процедуры заключается в том, что как только какой-то признак попадает в оптимальное множество, его нельзя оттуда удалить.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "#https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py\n",
    "\n",
    "#http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/#example-5-sequential-feature-selection-for-regression\n",
    "import pandas as pd\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sfs = SequentialFeatureSelector(LogisticRegression(max_iter=1000),\n",
    "                                k_features = 8,\n",
    "                                cv=5)\n",
    "sfs.fit(X_train,y_train)\n",
    "\n",
    "df =pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.errorbar(np.arange(1, 9),\n",
    "             df['avg_score'], \n",
    "             yerr=df['std_err'])\n",
    "plt.xticks(np.arange(1, 9), df[\"feature_idx\"],  rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD-DEL\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_graphic-non-greedy.png\" width=600/>\n",
    "\n",
    "\n",
    "Описанный выше подход довольно быстрый: в нем столько итераций, сколько признаков в выборке. Но при этом он слишком жадный, перебирается слишком мало вариантов и мы можем оказаться в плохой локальной точке. Процедуру можно усложнить. Один из подходов к усложнению — это алгоритм ADD-DEL, который не только добавляет, но и удаляет признаки из оптимального множества. Алгоритм начинается с процедуры жадного добавления. Множество признаков наращивается до тех пор, пока получается уменьшить ошибку, затем признаки жадно удаляются из подмножества, то есть перебираются все возможные варианты удаления признака, оценивается ошибка и удаляется тот признак, который приводит к наибольшему уменьшению ошибки на выборке. Эта процедура повторяет добавление и удаление признаков до тех пор, пока уменьшается ошибка. Алгоритм ADD-DEL всё еще жадный, но при этом он менее жадный, чем предыдущий, поскольку *может исправлять ошибки*, сделанные в начале перебора: если вначале был добавлен неинформативный признак, то на этапе удаления от него можно избавиться.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_graphic-add-del.png\" style=\"height: 470px;\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sffs = SequentialFeatureSelector(\n",
    "           LogisticRegression(max_iter=1000),\n",
    "           k_features = 8,\n",
    "           forward=True, \n",
    "           floating=True, # use ADD-DEL\n",
    "           verbose=0,\n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "\n",
    "sffs.fit(X_train.values,y_train)\n",
    "df = pd.DataFrame.from_dict(sffs.get_metric_dict()).T\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(np.arange(1, 9),\n",
    "             df['avg_score'], \n",
    "             yerr=df['std_err'])\n",
    "plt.xticks(np.arange(1, 9), df[\"feature_idx\"],  rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков на основе моделей\n",
    "\n",
    "### Использование весов признаков\n",
    "\n",
    "Во многих моделях (eg линейных) перед признаками стоят веса. Если признаки масштабированы, то веса при признаках можно интерпретировать как информативности: чем больше по модулю вес при признаке $j$, тем больший вклад этот признак вносит в ответ модели. Однако если признаки не масштабированы, то так использовать веса уже нельзя. Например, если есть два признака, и один по масштабу в 1000 раз меньше другого, то вес первого признака может быть очень большим, только чтобы признаки были одинаковыми по масштабу.\n",
    "Если необходимо обнулить как можно больше весов, чтобы линейная модель учитывала только те признаки, которые наиболее важны для нее, можно использовать L1-регуляризацию. Чем больше коэффициент при L1-регуляризаторе, тем меньше признаков будет использовать линейная модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим это на примере Линейного классификатора который мы конструировали на 2-м занятии.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/\n",
    "L04_skalyar.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "for i, w in enumerate(lr.coef_[0]):\n",
    "  print(X_train.columns[i],w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_selector = SelectFromModel(LogisticRegression(max_iter=1000))\n",
    "lr_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[lr_selector.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_selector.transform(X_train) # select only relevant features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "rf_selector  = SelectFromModel(rf)\n",
    "rf_selector.fit(X_train, y_train)\n",
    "\n",
    "X_train.columns[rf_selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков - это тоже выбор гиперпарметров\n",
    "\n",
    "Никогда не отбирайте признаки на том же наборе данных, на котором тестируетесь. Иначе получите завышенное качество вашей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обучения на большом числе бесполезных признаков\n",
    "\n",
    "Сгенерируем следущий датасет. \n",
    "\n",
    "У нас есть по 500 пациентов, больных и здоровых. \n",
    "Для каждого известно 100000 **случайных** бинарных признаков. \n",
    "Что будет, если мы попросим нашу модель научиться отделять здоровых от больных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "\n",
    "pat_cnt = 500\n",
    "snv_count = 100000\n",
    "top_k = 10\n",
    "\n",
    "genes = [f\"SNP{ind}\" for ind in range(snv_count)]\n",
    "healthy = pd.DataFrame(np.random.choice([0,1], \n",
    "                                        size=(pat_cnt, snv_count)), \n",
    "                       columns=genes)\n",
    "healthy['State'] = \"H\"\n",
    "diseased = pd.DataFrame(np.random.choice([0,1], \n",
    "                      size=(pat_cnt, snv_count)),\n",
    "                        columns=genes)\n",
    "diseased['State'] = \"D\"\n",
    "\n",
    "patients = pd.concat([healthy, diseased], axis=0)\n",
    "\n",
    "X = patients.drop(\"State\", axis=1)\n",
    "Y = patients['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Без отбора признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y == \"D\", \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_prauc = average_precision_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_accuracy = accuracy_score(y_pred=y_train_pred > 0.5, y_true=y_train)\n",
    "print(\"Train quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy: accuracy {train_accuracy:.02f}\")\n",
    "\n",
    "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "test_rocauc = roc_auc_score(y_score=y_test_pred, y_true=y_test)\n",
    "test_prauc = average_precision_score(y_score=y_test_pred, y_true=y_test)\n",
    "test_accuracy = accuracy_score(y_pred=y_test_pred > 0.5, y_true=y_test)\n",
    "print(\"Test quality:\")\n",
    "print(f\"ROCAUC : {test_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {test_prauc:.02f}\")\n",
    "print(f\"Accuracy: accuracy {test_accuracy:.02f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель идеально выучила данные обучения, но с тестом беда (как и должно быть)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### С неправильной процедурой отбора признаков\n",
    "\n",
    "Возьмем те признаки, для которых средняя разница для больных и здоровых максимальна. Заметьте, мы даже не используем чего-то сильно сложного. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = X[Y == \"H\"].mean(axis=0) - X[Y == 'D'].mean(axis=0)\n",
    "top = np.abs(diffs).sort_values(ascending=False)[0:top_k]\n",
    "genes = top.index\n",
    "\n",
    "X_selected = X[genes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, на качество модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_selected,\n",
    "                                                    Y == \"D\", \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_prauc = average_precision_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_accuracy = accuracy_score(y_pred=y_train_pred > 0.5, y_true=y_train)\n",
    "print(\"Train quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy: accuracy {train_accuracy:.02f}\")\n",
    "\n",
    "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_test_pred, y_true=y_test)\n",
    "train_prauc = average_precision_score(y_score=y_test_pred, y_true=y_test)\n",
    "train_accuracy = accuracy_score(y_pred=y_test_pred > 0.5, y_true=y_test)\n",
    "print(\"Test quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy: accuracy {train_accuracy:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внезапно, качество на тесте выглядит размным. Да, не очень классное, но есть. А должно быть соответствующее случайно модели - признаки-то случайные. \n",
    "\n",
    "Дело в том, что мы изначально выбрали те признаки, которые работали хорошо по слуучайным причинам на всем нашем искуственном датасете, а не на только на трейне. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### С правильной процедурой отбора признаков \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, \n",
    "                                                    Y == \"D\",\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "X_train1, X_train2, y_train1, y_train2 = train_test_split(X_train, \n",
    "                                                          y_train, \n",
    "                                                          test_size=0.8, \n",
    "                                                          random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбираем признаки на одном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = X_train1[np.logical_not(y_train1)].mean(axis=0) - X_train1[y_train1].mean(axis=0)\n",
    "top = np.abs(diffs).sort_values(ascending=False)[0:top_k]\n",
    "genes = top.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель на втором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train2[genes], y_train2)\n",
    "y_train_pred = model.predict_proba(X_train2)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем на третьем "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_test_pred, y_true=y_test)\n",
    "train_prauc = average_precision_score(y_score=y_test_pred, y_true=y_test)\n",
    "train_accuracy = accuracy_score(y_pred=y_test_pred > 0.5, y_true=y_test)\n",
    "print(\"Test quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy: accuracy {train_accuracy:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача понижении размерности\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто мы хотим данные из пространство высокой размерности преобразовать в пространство более низкой, с сохранением одного или нескольких свойств, например:\n",
    "\n",
    "Объекты реконструируются обратно почти без ошибки\n",
    "расстояние между объектами сохраняется\n",
    "Зачем это нужно? По многим причинам:\n",
    "\n",
    "1. Многие алгоритмы показывают себя плохо на простанствах большой размерности в принципе (проклятье размерности).\n",
    "\n",
    "2. Некоторые - просто будут значительно дольше работать, при этом качество их работы не изменится от уменьшения размерности.\n",
    "\n",
    "3. Понижение размерности позволяет использовать память более эффективно и подавать модели на обучение за один раз больше объектов.\n",
    "\n",
    "4. Помогает понижение размерности и избавится от шума, как мы обсудим дальше\n",
    "\n",
    "5. Задача визуализации - хочется взглянуть на наши объекты, а делать это в 100-мерном или 100000-мерном пространстве неудобно\n",
    "\n",
    "6. Удаление выбросов - в пространствах меньшей размерности можем их увидеть глазами, опять же. \n",
    "\n",
    "7. Можем увидеть закономерности в данных - что они бьются на явные кластера и тд. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold assumption \n",
    "Понятно, что при этом мы будем терять часть информации об объектах. Но мы считаем, что при правильных настройках алгоритма понижения размерности, потери будут незначитальны.\n",
    "\n",
    "Что нам позволяет это делать - мы предполагаем, что наши данные на самом деле лежат в пространстве меньшем, чем пространство исходных признаков. \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_manifold1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве случаев это действительно правда. Например, лица людей даже на фотографиях 300x300, очевидно, лежат в пространстве меньшей размерности, нежели 90000. Ведь не каждая матрица 300 на 300, заполненная какими-то значениями от 0 до 1, даст нам изображение человека\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_manifold2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Метод главных компонент)\n",
    "\n",
    "Анализ главных компонент - простейший линейный метод снижения размерности, описан Пирсоном в 1901 году.<br> \n",
    "* Pearson K. On Lines and Planes of Closest Fit to Systems of Points in Space. Philosophical Magazine 2. 1901; 559–572.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько способов сформулировать задачу метода главных компонент:\n",
    "\n",
    "* через максимизацию дисперсии;\n",
    "* через аппроксимацию данных линейными многообразиями меньшей размерности;\n",
    "* через приближение матрицы с рангом $k$;\n",
    "* через построения для данной многомерной случайной величины ортогонального преобразования координат, в результате которого корреляции между отдельными координатами обратятся в нуль.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_lec-1.png\" width=850/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Максимизация дисперсии выборки после понижения размерности\n",
    "\n",
    "\n",
    "Самый простой метод взгляда на метод главных компонент заключается в следующем: пусть имеется выборка, показанная на рисунке ниже, и требуется выбрать прямую, на которую можно будет эту выборку, максимизировав при этом дисперсию спроецированных данных. \n",
    "\n",
    "И действтительно, чем больше дисперсия выборки после проецирования на прямую, тем больше сохраняется информации. \n",
    "\n",
    "Красная прямая в этом случае будет **первой компонентой**. Часть выборочной дисперсии, лежащая вдоль главной компоненты (не обязательно первой) называется *объясненной дисперсией (explained variance)*.\n",
    "\n",
    "Зеленая, параллельная ей и объясняющая оставшуюся дисперсию - **второй компонентой**.\n",
    "\n",
    "Так как данные у нас вообще двумерные, то больше компонент не будет. Мы можем перейти теперь в пространство с двумя компонентами, при этом потерь информации не произойдет, но координатные оси в новом пространстве будут независимы, что порой тоже полезно. \n",
    "А можем оставить только первую ось, тогда информацию мы потеряем, но главная часть сохранится.\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_lec-6.png\" width=700/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В более многомерных случаях компонент будет больше - вплоть до размерности исходного пространства. Каждая следующая компонента будет перпендикулярна предыдущим и при этом объяснять меньше дисперсии, чем любая из них. \n",
    "\n",
    "Полезной информацией является *коэффициент объясненной дисперсии (explained variance ratio)* главной компоненты. Этот коэффициент является отношением между дисперсией главной компоненты и суммой дисперсий всех главных компонент. Он указывает долю выборочной дисперсии, которая лежит вдоль оси каждой главной компоненты.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_5.png\" width=750/>\n",
    "\n",
    "[источник](https://towardsdatascience.com/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с Титаником\n",
    "\n",
    "Что бы понять как использовать PCA на практике найдем главные компоненты для датасета Titanic и посмотрим,  как распределится между ними дисперсия.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модуле PCA, после fit можно получить explained variance ratio посредством обращения к полю explained_variance_ratio_, а explined_variance посредством обращения к полю explained_variance_. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Без стандартизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = prepare_data(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data[\"Survived\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "titanic_pca = sklearn.decomposition.PCA()\n",
    "titanic_pca.fit(X_train)\n",
    "print(titanic_pca.explained_variance_ratio_)\n",
    "#plt.bar(range(titanic_pca.explained_variance_.shape[0]),titanic_pca.explained_variance_)\n",
    "\n",
    "plt.bar(range(titanic_pca.explained_variance_ratio_.shape[0]),titanic_pca.explained_variance_ratio_)\n",
    "plt.title('Variance by components')\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Variance %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графика не совсем понятно, сколько компонент брать, резко доля объясняемой дисперсии меняется в районе 5-компоненты. Посмотрим, сколько компонент нужно модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "  titanic_pca = sklearn.decomposition.PCA(n_components=i)\n",
    "  titanic_pca.fit(X_train)\n",
    "\n",
    "  X_train_reduced = titanic_pca.transform(X_train)\n",
    "  model = LogisticRegression(max_iter=1000)\n",
    "  model.fit(X_train_reduced,y_train)\n",
    "\n",
    "  X_test_reduced = titanic_pca.transform(X_test)\n",
    "  print(f\"{i} first components\", model.score(X_test_reduced,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явно видим, что уже первых 7 компонент достаточно для достижения качества, которое далее не меняется. Почему же мы видим снижение уже после 2 компоненты?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы забыли сделать **стандартизацию** наших данных. \n",
    "\n",
    "В нашем датасете переменные имеют совершенно разные масштабы - из-за этого часть из них \"перетегягивает\" на себя всю дисперсию. \n",
    "\n",
    "В результате по доле дисперсии чего-либо судить о важности компонент нельзя. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Со стандартизацией "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предварительно стандартизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "titanic_pca = sklearn.decomposition.PCA()\n",
    "titanic_pca.fit(X_train)\n",
    "print(titanic_pca.explained_variance_ratio_)\n",
    "#plt.bar(range(titanic_pca.explained_variance_.shape[0]),titanic_pca.explained_variance_)\n",
    "\n",
    "plt.bar(range(titanic_pca.explained_variance_ratio_.shape[0]),titanic_pca.explained_variance_ratio_)\n",
    "plt.title('Variance by components')\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Variance %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь видим, что не только первые две компоненты дают вклад. Это происходит потому, что переменные с большим диапазоном значений не забивают остальные. \n",
    "\n",
    "Правда, теперь и значимого спада в доле объясняемой дисперсии мы долго не видим - потому встает вопрос, как выбрать число компонент так, чтобы взять нужное и отсечь ненужное"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как выбирать оптимальное число  компонент \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем датасет, на котором понижение размерности видно более явно. \n",
    "\n",
    "В данном датасете хранятся признаки (нам сейчас не важно, какие), характеризащию примерно 8000 клеток крови.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://kodomo.fbb.msu.ru/FBB/year_20/scRNAseq_CITEseq.txt -O scRNAseq_CITEseq.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scRNAseq = pd.read_csv('scRNAseq_CITEseq.txt',sep='\\t')\n",
    "\n",
    "X_scRNAseq = scRNAseq.iloc[:,0:(scRNAseq.shape[1]-1)]\n",
    "Y_scRNAseq = scRNAseq.iloc[:,scRNAseq.shape[1]-1]\n",
    "\n",
    "X_scRNAseq = np.log(X_scRNAseq + 1)\n",
    "scRNAseq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбирать число компонент можно по-разному\n",
    "\n",
    "#### По доле объясняемой дисперсии\n",
    "\n",
    "Часто берут минимальное число компонент, которое объясняет 95% дисперсии. Подход, очевидно, порочный (а почему не 90% или 99%), зато быстрый :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=X_train.shape[1])\n",
    "pca.fit(X_train)\n",
    "\n",
    "\n",
    "ths = 0.95\n",
    "total_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.plot(np.arange(1, total_explained.shape[0] + 1), total_explained)\n",
    "plt.axhline(xmin=0, xmax=1000, y=ths, c=\"red\", ls=\"--\")\n",
    "chosen_number = np.where(total_explained >= 0.95)[0][0] + 1\n",
    "plt.axvline(x=chosen_number, ymin=0, ymax=ths, c=\"red\", ls=\"--\")\n",
    "plt.xticks(np.arange(1, X_train.shape[1]))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взяли 15 компонент. Почему именно столько? Почему не 16? \n",
    "\n",
    "Непонятно. Просто ибо поставили такой порог. Могли поставить и другой, и тогда бы взяли больше или меньше компонент.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### По правилу локтя\n",
    "\n",
    "\n",
    "Можно построить график, отражающих сколько дисперсии объясняет каждая из компонент\n",
    "\n",
    "И на основе графика выбрать нужное число компонент. \n",
    "\n",
    "Записать это можно через собственные значения: \n",
    "\n",
    "Далее мы действуем по \"критерию крутого склона\" (\"критерий локтя\") - хотим найти такую компоненту, чтобы доля объяснямой ею дисперсии была резко больше доли, объясняемой следующей компонентой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея подхода простая - переходу от компонент, объясняющих что-то важное в данных к компонентам, объясняющим шум, должен сопровождаться резким снижением доли объясняемой дисперсии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или, можно сказать иначе - выбранные нами компоненты должны быть устойчивы к добавлению шума в данные. Если мы нашли резкий скачок в доли объясняемой дисперсии, то маловероятно, что добавление шума этот скачок нивелирует - наш способ отбора компонент устоячив к шуму"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В идеальном случае график будет выглядеть как-то так. Но практически таких склонов может не быть, может быть несколько и т.д.\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_scree.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = X_train.shape[1]\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(np.arange(1, n_comp + 1), explained)\n",
    "plt.plot(np.arange(1, n_comp + 1), explained)\n",
    "plt.xticks(np.arange(1, X_train.shape[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понять, какое число компонент стоит выбрать, из такого графика сложно. Наверно, стоит выбрать последний склон - это означает, что мы возьмем первые 12 компонент. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно применить критерий локтя и к нашему изначальному графику с суммарной долей объясненной дисперсии - опять же, ожидаем скачка. Но там скачок также трудно обнаружить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перестановочный метод\n",
    "\n",
    " 1) Перемешиваем значения каждого признака.\n",
    "\n",
    " 2)  Получаем матрицу признаков, которая не содержит никакой информации о манифолде\n",
    "\n",
    " 3) Делаем PCA\n",
    "\n",
    " 4) Любая explained variance - просто из-за природы данных\n",
    "\n",
    " 5) Делаем так 100-1000 раз\n",
    "\n",
    " 6) Пусть на реальных данных k-я компонента объясняет\n",
    "n% дисперсии.\n",
    "\n",
    " 7) Смотрим на распределение доли дисперсии,\n",
    "объясняемой k-компонентой для случайных данных\n",
    "(полученных перемешиванием).\n",
    "\n",
    " 8) Можем сравнить и принять решение, объясняет ли k-я\n",
    "компонента что-то реальное, или просто шум"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset):\n",
    "    random_data =  {}\n",
    "    for col in dataset.columns:\n",
    "        random_data[col] = np.random.permutation(dataset.loc[:, col].values)\n",
    "    random_data = pd.DataFrame(random_data)\n",
    "    return random_data\n",
    "\n",
    "def get_variance_by_chance(dataset, n_replics, n_components):\n",
    "    variance_explained_by_chance = np.zeros((n_replics, n_components))\n",
    "    for i in tqdm.tqdm_notebook(range(n_replics)):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{i} iter\")\n",
    "\n",
    "        random_data = shuffle_dataset(dataset)\n",
    "        random_pca = PCA(n_components=n_components)\n",
    "        random_pca.fit(random_data)\n",
    "        variance_explained_by_chance[i, :] = random_pca.explained_variance_ratio_\n",
    "    return variance_explained_by_chance\n",
    "\n",
    "def get_pc_variance(dataset, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(dataset)\n",
    "    return pca.explained_variance_ratio_\n",
    "\n",
    "def plot_mean_and_CI(ax, values, label, ci_level=0.95, alpha_transparency=0.5, color_mean=None, color_shading=None):\n",
    "    mean = values.mean(axis=0)\n",
    "    \n",
    "    std = values.std(axis=0)\n",
    "    n = values.shape[1]\n",
    "    se = std / np.sqrt(n) \n",
    "    \n",
    "    q_alpha = (1-ci_level) / 2\n",
    "    ci_num = np.abs(norm.ppf(q_alpha, loc=0, scale=1))\n",
    "    \n",
    "    lb = mean -  ci_num * se\n",
    "    ub = mean + ci_num * se\n",
    "    \n",
    "    # plot the shaded range of the confidence intervals\n",
    "    ax.fill_between(range(mean.shape[0]), ub, lb,\n",
    "                     color=color_shading, alpha=alpha_transparency)\n",
    "    # plot the mean on top\n",
    "    ax.plot(mean, c=color_mean, lw=3, label=label)\n",
    "    \n",
    "def plot_explained_variance(ax, variance):\n",
    "    ax.plot(variance, label='real', lw=3)\n",
    "    ax.scatter(np.arange(0, variance.shape[0]), variance)\n",
    "    \n",
    "def plot_variance_by_change(ax, variance_by_chance):\n",
    "    plot_mean_and_CI(ax, variance_by_chance, label='chance', color_mean='red', color_shading='red')\n",
    "\n",
    "def calc_permutat_pval(real_values, permut_values, eps=None):\n",
    "    eps = eps or (1/(permut_values.shape[0] * 10))\n",
    "    \n",
    "    p_values = np.zeros_like(real_values)\n",
    "    for i in range(0, p_values.shape[0], 1):\n",
    "        p_values[i] = (permut_values[:, i] >= real_values[i]).mean() + eps\n",
    "    return p_values\n",
    "\n",
    "def plot_explained_vs_chance(ax, explained_variance, variance_by_chance, dataset_name, step=1):\n",
    "    plot_explained_variance(ax, explained_variance)\n",
    "    plot_variance_by_change(ax, variance_by_chance)\n",
    "\n",
    "    ax.set_title(f'PCA {dataset_name}', size=35)\n",
    "    ax.set_xlabel(\"Component number\", size=25)\n",
    "    ax.set_ylabel(\"Explained variance ration\", size=25)\n",
    "    ax.set_xticks(np.arange(0, explained_variance.shape[0], step))\n",
    "    ax.set_xticklabels(np.arange(1, explained_variance.shape[0]+1, step), size=20)\n",
    "\n",
    "    ax.tick_params(labelsize=20, size=10)\n",
    "    ax.set_ylim(0, explained_variance[0] + 0.1 )\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "def plot_pval_plot(ax, p_values, dataset_name, alpha_level = 0.05, logscale=True, step=1):\n",
    "    if logscale:\n",
    "        p_values = -np.log10(p_values)\n",
    "        alpha_level = -np.log10(alpha_level)\n",
    "\n",
    "    \n",
    "    ax.set_title(f\"PC significance, {dataset_name}\", size=35)\n",
    "    ax.plot(p_values, lw=3)\n",
    "    ax.scatter(np.arange(0, p_values.shape[0]), p_values, lw=3)\n",
    "\n",
    "    ax.set_xlabel(\"Component number\", size=25)\n",
    "    ax.set_ylabel(\"-log(pvalue + eps)\", size=25)\n",
    "    ax.set_xticks(np.arange(0, p_values.shape[0], step))\n",
    "    \n",
    "    ax.set_xticklabels(labels = np.arange(1, p_values.shape[0]+1, step), size=20)\n",
    "    ax.tick_params(labelsize=20, size=10)\n",
    "    \n",
    "\n",
    "    ax.hlines(y=alpha_level, xmin=0, xmax=p_values.shape[0], color=\"red\", linestyles=\"dashed\", lw=3)\n",
    "    \n",
    "    \n",
    "def pca_analysis(ax1, ax2, dataset, title, n_components = None, n_replics = 1000, step=1):\n",
    "    n_components = n_components or dataset.shape[1]\n",
    "    explained_variance = get_pc_variance(dataset, n_components)\n",
    "    variance_by_chance = get_variance_by_chance(dataset, n_replics, n_components)\n",
    "    p_values = calc_permutat_pval(explained_variance, variance_by_chance)\n",
    "    plot_explained_vs_chance(ax1, explained_variance, variance_by_chance, title)\n",
    "    plot_pval_plot(ax2, p_values, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "f.set_figheight(20)\n",
    "f.set_figwidth(20)\n",
    "pca_analysis(ax1, ax2, pd.DataFrame(X_train), \"Titanic\", n_replics=10, n_components=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явно видим, что согласено этому подходу, надо взять 6 компонент. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вклад исходных признаков в компоненты\n",
    "\n",
    "Мы можем также посмотреть, какие признаки внесли вклад в какие компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_component = titanic_pca.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,10))\n",
    "\n",
    "b = sns.barplot(x=first_component, y=features.columns,  orient='h', hue=[ z < 0 for z in first_component],\n",
    "            palette=['blue', 'red'])\n",
    "b.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или абсолютные вклады"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (7,7))\n",
    "\n",
    "sns.barplot(x=np.abs(first_component), y=features.columns,  orient='h', color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что наибольший вклад в дисперсию вносят такие признаки как класс пассажира и класс кабины"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важность стандартизации\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем абсолютно случайную выборку - в ней нет никакой внутренней структуры. \n",
    "\n",
    "Но пусть у нас 5 признаков, описывающих выборку пришли из стандартного нормально распределения, а еще один - будет равновероятно принимать значение 0 и 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N = 200\n",
    "P = 5\n",
    "\n",
    "X = np.random.normal(size=[N,P])\n",
    "X = np.append(X, np.random.choice([0, 3], size = [N,1]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "low_d = pca.fit_transform(X)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(low_d[:,0], low_d[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без предварительной нормализации мы получили два явных кластера в данных. Только вот кластеров этих по идее быть не должно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xs = StandardScaler().fit_transform(X)\n",
    "low_d = pca.fit_transform(Xs)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(low_d[:,0], low_d[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После стандартизации кластеры исчезли. Потому у вас должны быть весомые аргументы для того, чтобы не применять стандартизацию или другой метод, переводящий ваши данные в одну шкалу со средним 0, перед применением PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с RNA-Seq - нахождение выбросов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные биологического анализа пациентов с раком и без него. \n",
    "\n",
    "В данном случае наши данные уже предварительно обработаны, потому стандартизацию проводить не будем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget  https://kodomo.fbb.msu.ru/FBB/year_20/ml/rnaseq_data.tab -O rnaseq_data.tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnadata = pd.read_table('rnaseq_data.tab', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnadata.columns = list(rnadata.columns[:-2]) + [\"dataset\", \"sample type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rnadata.drop(labels=['dataset', 'sample type'], axis=1)\n",
    "labels = rnadata.loc[:, ['dataset', 'sample type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_decomposer = PCA(n_components=2)\n",
    "pca_decomposer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = pca_decomposer.transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('PCA plot', size=24)\n",
    "plt.xlabel('PC1', size=16)\n",
    "plt.ylabel('PC2', size=16)\n",
    "sns.scatterplot(x=X_reduced[:, 0],\n",
    "                y=X_reduced[:, 1], \n",
    "                hue=labels['sample type']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даже не понимая, что за признаки у нас в колонках, мы видим несколько интересных вещей. \n",
    "\n",
    "Во-первых - синяя точка на оранжевой территории и оранжевая на синей - видимо, выбросы. \n",
    "\n",
    "Кроме этого мы видим, что почему-то у нас есть 4 почти равноудаленных кластера, по два на рак и нормальную ткань"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = pca_decomposer.transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('PCA plot', size=24)\n",
    "plt.xlabel('PC1', size=16)\n",
    "plt.ylabel('PC2', size=16)\n",
    "sns.scatterplot(x=X_reduced[:, 0], \n",
    "                y=X_reduced[:, 1], \n",
    "                hue=labels['sample type'], \n",
    "                style=labels['dataset']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что в пределах данных одного эксперимента, рак и опухоль разделяются хорошо. Но при этом данные между разными экспериментами отличаются так же, как и рак от опухоли. \n",
    "\n",
    "Этот эффект называется батч-эффектом и говорит о том, что нужно нормализовывать данные в пределах эксперимента каким-то хитрым образом, чтобы научить модель на новых данных отличать рак от нормальной ткани. \n",
    "\n",
    "Таким образом, с помощью PCA мы нашли выбросы и артефакт в данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с лицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим как PCA применялся для решения практической задачи: распознавания лиц*.\n",
    "\n",
    "Датасет:\n",
    "[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/)\n",
    "\n",
    "\n",
    "* Сейчас для этого используются более эффективные алгоритмы использующие CNN.\n",
    "\n",
    "Загрузим датасет и распакуем его на диск VM Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://conradsanderson.id.au/lfwcrop/ (LFWcrop Face Dataset, greyscale version)\n",
    "!wget http://conradsanderson.id.au/lfwcrop/lfwcrop_grey.zip\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dir='lfwcrop_grey/faces'\n",
    "\n",
    "# http://conradsanderson.id.au/lfwcrop/ (LFWcrop Face Dataset, greyscale version)\n",
    "!wget http://conradsanderson.id.au/lfwcrop/lfwcrop_grey.zip\n",
    "!unzip lfwcrop_grey.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.grid'] = False\n",
    "def show_faces(images, titles, h=64, w=64):\n",
    "    plt.figure(figsize=(16 , 4))\n",
    "    for i in range(min(images.shape[0],5)):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i])\n",
    "        \n",
    "\n",
    "# Get first 1000 files\n",
    "celebrity_photos=os.listdir(dir)[1:1001]\n",
    "celebrity_images=[dir+'/' + photo for photo in celebrity_photos]\n",
    "# Load iages from disk\n",
    "images=np.array([plt.imread(image) for image in celebrity_images], dtype=np.float64)\n",
    "# Extract real celebrity name from file name\n",
    "celebrity_names=[name[:name.find('0')-1].replace(\"_\", \" \") for name in celebrity_photos]\n",
    "print(images[0].shape)\n",
    "show_faces(images, celebrity_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка данных:\n",
    "Преобразуем изображения в вектора и центрируем их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretch to vector\n",
    "X = images.reshape(images.shape[0], 64*64)\n",
    "print(X.shape)\n",
    "mean = np.mean(X, axis=0)\n",
    "# Center: substract mean\n",
    "centered_faces = X-mean\n",
    "plt.imshow(mean.reshape(64, 64), cmap=plt.cm.gray )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем собственные вектора. Аналогия с фотороботом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "#n_components == min(n_samples, n_features)\n",
    "\n",
    "pca_faces = sklearn.decomposition.PCA() #1000x4096\n",
    "\n",
    "pca_faces.fit(centered_faces)\n",
    "eigenfaces = pca_faces.components_\n",
    "reshaped_eigenfaces = eigenfaces.reshape((1000, 64, 64))\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(reshaped_eigenfaces.shape[0])]\n",
    "show_faces(reshaped_eigenfaces, eigenface_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Восстановим лица с использованием n < 4096 компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(img, n_components):\n",
    "  # Generate embedding for first image using only 10 first components\n",
    "  img = img.reshape(64*64) - mean\n",
    "  emb = np.dot(img,eigenfaces[:n_components].T) #(1,4096) * (4096,1) \n",
    "  #print(emb,emb.shape) # 10 - 500 numbers only!\n",
    "\n",
    "  # Recover image from embeding\n",
    "  recovered_img = np.dot(emb,eigenfaces[:n_components]) \n",
    "  recovered_img += mean #shift by mean\n",
    "  return emb, recovered_img\n",
    "\n",
    "# Shome images recovered from embeddings of various sizes \n",
    "original_image = images[0]\n",
    "titles = []\n",
    "img_list = []\n",
    "for n in [10,25,100,500]:\n",
    "  embedding, recovered = create_embedding(original_image, n)\n",
    "  img_list.append(recovered)\n",
    "  titles.append(f\"Components {n}\")\n",
    "img_list.append(original_image)\n",
    "titles.append(\"Original\")\n",
    "\n",
    "show_faces(np.array(img_list), titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае требование на ортогональность компонент, выделяемых PCA фактичеески мешает нам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы  PCA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Интересное направление в данных может не совпадать с направлением максимальной дисперсии.\n",
    "\n",
    "Рассмотрим случай выборки, которая сгенерирована из двух вытянутых нормальных распределений, чьи основные оси неортогональны друг другу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.array([[10,0],[0,0.1]])\n",
    "mu = np.zeros(2)\n",
    "# rotate second normal distrbution by 60 degrees\n",
    "phi = np.pi/3\n",
    "rotation = np.array([[np.cos(phi), np.sin(phi)],\n",
    "                     [-np.sin(phi),np.cos(phi)]])\n",
    "\n",
    "data_1 = np.random.multivariate_normal(mu, C1, size=100)\n",
    "data_2 = np.dot(data_1, rotation)\n",
    "# generate data from two not orthogonal distributions\n",
    "data = np.vstack([data_1,\n",
    "                  data_2])\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "# ax.axis('equal')\n",
    "# ax.set_aspect('equal', 'box')\n",
    "ax.scatter(data[:,0], data[:,1])\n",
    "\n",
    "# plot real axis\n",
    "\n",
    "plot_components_vector(ax, data, np.array([1, 0]), color=\"green\", label=\"Ideal component 1\")\n",
    "plot_components_vector(ax, data, [np.cos(phi), np.sin(phi)], color=\"green\", label=\"Ideal component 2\")\n",
    "\n",
    "# plot PCA\n",
    "model = PCA(n_components=2)\n",
    "model.fit(data)\n",
    "W_pca = model.components_\n",
    "\n",
    "plot_components_vector(ax, data, W_pca[0], color=\"red\", label=\"PCA component 1\")\n",
    "plot_components_vector(ax, data, W_pca[1], color=\"red\", label=\"PCA component 2\")\n",
    "s = C1[0, 0] * 1.5\n",
    "ax.set(xlim=(-s, s), ylim=(-s, s))\n",
    "# ax.set_aspect('equal', 'box')\n",
    "plt.legend()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выбранные оси могут вообще не подходить для нашей задачи\n",
    "\n",
    "\n",
    "В примере ниже дисперсии не отражают интересующих нас направлений в данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting principal components\n",
    "def plot_principal_components(data, model, scatter=True, legend=True):\n",
    "    W_pca = model.components_\n",
    "    if scatter:\n",
    "        plt.scatter(data[:,0], data[:,1])\n",
    "    plt.plot(data[:,0], -(W_pca[0,0]/W_pca[0,1])*data[:,0], color=\"c\")\n",
    "    plt.plot(data[:,0], -(W_pca[1,0]/W_pca[1,1])*data[:,0], color=\"c\")\n",
    "    if legend:\n",
    "        c_patch = mpatches.Patch(color='c', label='Principal components')\n",
    "        plt.legend(handles=[c_patch], loc='lower right')\n",
    "    # сделаем графики красивыми:\n",
    "    plt.axis('equal')\n",
    "    limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1]))-0.5,\n",
    "              np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))+0.5]\n",
    "    plt.xlim(limits[0],limits[1])\n",
    "    plt.ylim(limits[0],limits[1])\n",
    "    plt.draw()\n",
    "\n",
    "# function for vector plotting\n",
    "def plot_components_vector(ax, data, vector, color, label=\"\", delta=0.5):\n",
    "    limits_x = [np.min(data[:,0])-delta, np.max(data[:,0])+delta]\n",
    "    limits_y = [np.min(data[:,1])-delta, np.max(data[:,1])+delta]\n",
    "    \n",
    "    if np.fabs(vector[1]) > 1e-5:\n",
    "        if np.fabs(vector[0]) > 1e-5:\n",
    "            x = np.arange(*limits_x, 0.1)\n",
    "            y = x * vector[1]/vector[0]\n",
    "        else:\n",
    "            y = np.arange(*limits_y, 0.1)\n",
    "            x = np.full_like(y, 0)\n",
    "    else:\n",
    "        x = np.arange(*limits_x, 0.1)\n",
    "        y = np.full_like(x, 0)\n",
    "        \n",
    "    ax.plot(x, y, color=color, label=label)\n",
    "    # ax.set_xlim(*limits_x)\n",
    "    # ax.set_ylim(*limits_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[0.5,0],[0,10]])\n",
    "mu1 = np.array([-2,0])\n",
    "mu2 = np.array([2,0])\n",
    "\n",
    "data = np.vstack([np.random.multivariate_normal(mu1, C, size=100),\n",
    "                  np.random.multivariate_normal(mu2, C, size=100)])\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.scatter(data[:,0], data[:,1])\n",
    "# обучим модель pca и построим главные компоненты\n",
    "model = PCA(n_components=2)\n",
    "model.fit(data)\n",
    "\n",
    "plot_components_vector(ax, data, model.components_[0], color=\"red\", label=\"PCA component 1\")\n",
    "plot_components_vector(ax, data, model.components_[1], color=\"green\", label=\"PCA component 2\")\n",
    "\n",
    "s = 15; ax.set(xlim=(-s, s), ylim=(-s, s))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что в данном случае метод главных компонент будет считать вертикальную компоненту более значимой для описания набора данных, чем горизонтальную. \n",
    "\n",
    "Но, например, в случае, когда данные из левого и правого кластера относятся к разным классам, для их линейной разделимости вертикальная компонента является шумовой. Несмотря на это, её метод главных компонент никогда шумовой не признает, и есть вероятность, что отбор признаков с его помощью выкинет из ваших данных значимые для решаемой вами задачи компоненты просто потому, что вдоль них значения имеют низкую дисперсию.\n",
    "\n",
    "Справляться с такими ситуациями могут некоторые другие методы уменьшения размерности данных, например, метод независимых компонент (Independent Component Analysis, ICA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Недостатки линейного PCA\n",
    "\n",
    "Как мы увидели в предыдущих примерах, обычный PCA далеко не всегда работает хорошо. В частности, могут быть ситуации, когда построенная PCA проекция не дает хорошего разбиения объектов на группы. Для набора картинок с написанными от руки цифрами  MNIST, PCA даст такой результат:\n",
    "\n",
    "Также бывают ситуации, когда оптимально спроецировать не на некоторую плоскость, а на многообразие (кривая плосоксть), как показано на картинке ниже.\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/S-manifold.png\" width=650/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "В данном случае оптимально спроецировать на S-образную кривую. \n",
    "\n",
    "В связи с вышеописанными случаями, ниже мы рассмотрим более сильные методы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA Ядровой (нелинейный) метод главных компонент\n",
    "\n",
    "Как уже упомяналось, иногда невозможно захватить всю информацию линейной проекцией, хотя кривая поверхность с такой же размерностью это позволяет сделать. Одним из подходов к решению данной проблемы является задача перевода признаков в нелинейное пространство. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel trick\n",
    "\n",
    "\n",
    "Kernel Trick избегает явного перевода наших признаков в пространство новых признаков - ведь пространства бывают очень большие, а нам бы хотелось сэкономить память компьютера. \n",
    "Оказывается, для PCA не важны собственно признаки объектов, а важны скалярные произведения между объектами. \n",
    "\n",
    "И это скалярное произведением позволяет подсчитывать напрямую функция $k(\\mathbf {x} ,\\mathbf {x'})$, которую часто называют *ядром или ядерной функцией (kernel, kernel function)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бывают разные ядра, которые считают скалярное произведение в разных пространствах\n",
    "\n",
    "* $k(x_i, x_j) = \\frac{1}{z} e^{-\\frac{h(x_i, x_j)^2}{h}}$ - радиальная базисная функция RBF\n",
    "* $k(x_i, x_j) = (<x_i, x_j> + c)^d, с, d \\in \\mathbb{R}$ - полиномиальное ядро\n",
    "* $k(x_i, x_j) = \\sigma((x_i, x_j>)$ - ядро с функцией активации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем две концентрические окружности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.title(\"Original space\")\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.scatter(X[reds, 0], X[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X[blues, 0], X[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычный PCA не может их разделить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_pca[reds, 0], X_pca[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_pca[blues, 0], X_pca[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Projection by PCA\")\n",
    "plt.xlabel(\"1st principal component\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот KernelPCA  справляется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(r\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя, конечно, и восстанавливать он будет не идеально - работал-то он по факту в пространстве бОльшей размерности и оси строил там"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_back = kpca.inverse_transform(X_kpca)\n",
    "plt.scatter(X_back[reds, 0], X_back[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_back[blues, 0], X_back[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Original space after inverse transform\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Kernel PCA довольно чувствителен к выбору ядра.\n",
    "\n",
    "К примеру, для данных, расположенных на трех окружностях:\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_PCA-3-circles.png\" width=260/>\n",
    "\n",
    "\n",
    "в зависимости от выбора ядра мы будем получать совершенно разные отображение в спрямляющее пространство:\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_PCA-kernels.png\" width=600/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы, основанные на сохранении расстояний\n",
    "\n",
    "Существуют и другие методы понижения размерности в данных. К примеру, t-SNE и UMAP. \n",
    "\n",
    "Они решают немного иначе поставленную задачу - из измерения новой размерности мы не должны легко переходить в старое измерение. Взамен этого требуется, чтобы сохранялись расстояния между объектами. Причем, особое внимание уделяется близким расстояниям. Далекие же могут не сохраняться.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "\n",
    "Идея состоит не в том что бы напрямую максимизировать дисперсиию, а найти такое пространство в котором расстояние между объектами будет сохраняться или по крайне мере не сильно меняться.\n",
    "\n",
    "При этом будем больше беспокоиться о расстоянии между близкими объектами, нежели о расстоянии между далекими\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описываем расстояния в исходном пространстве\n",
    "\n",
    "Для простоты будем в качестве исходного пространства рассматриать 2мерное"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne1.png\" width=1050/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее эти ненормированные расстояния нормируем на их сумму, чтобы для каждой точки похожести несли примерно одинаковый смысл. В итоге получается такая формула\n",
    "$$p_{j\\mid i}={\\frac {\\exp(-\\lVert \\mathbf{x}_{i}-\\mathbf {x}_{j}\\rVert^{2}/2\\sigma_{i}^{2})}{\\sum_{k\\neq i}\\exp(-\\lVert \\mathbf{x}_{i}-\\mathbf{x}_{k}\\rVert^{2}/2\\sigma_{i}^{2})}}$$\n",
    "\n",
    "Стандартное оттклонение будет различной для каждой точки. Оно подбирается бинарным поиском так, чтобы точки в областях с большей плотностью имели меньшую дисперсию. Для этого используется параметр perplexity, чем он больше - тем более далеким соседям уделяется внимание (стандартные отклонения становятся в целом больше)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить симметричные расстояния $p_{ij}$, их считают по следующей \n",
    "формуле \n",
    "\n",
    "$$p_{ij} = \\frac{p_{j\\mid i} + p_{i\\mid j}}{2N}$$\n",
    "\n",
    "В итоге получаем матрицу расстояний следущего вида:\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne_dist1.png\" width=650/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описываем расстояния в пространстве низкой размерности\n",
    "\n",
    "Обычно в слуучае tSNE используют простраства 2мерные и 3хмерные. Это вызвано во многом скоростью работы метода и тем, что он преимущественно используется для визуализации. \n",
    "\n",
    "Для простоты будем в качестве пространства низкой размерности рассматривать одномерное"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расположим точки, соответствующие точкам из исходного пространства случайным образом в одномерном пространстве. \n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne2.png\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точно также будем считать расстояние между выбранной точкой и остальными и использовать эти расстояния не в качестве $x$, а в качестве $y$, и, как следствие, similarity, будем использовать не нормальное распределение, а распределение Стьюдента. \n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne3.png\" width=350/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему? \n",
    "Оно более \"толстое\" в хвостах и потому дает возможность больше внимания уделять далеким точкам. Это позволяет компенсировать дисбаланс в распределении расстояний в пространстве большей и меньшей размерностей. Без этого точки у нас будут \"липнуть\" друг к другу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге мы получили две матрицы расстояний - в исходном и в новом пространстве\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne_two_dist.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизируем низкоразмерное представление"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Они абсолютно друг на друга непохожи, что логично - пока что наша проекция в новое пространство абсолютно случайное. Надо как-то править представление в новом пространстве. Именно для этого авторы и использовали плотности и прочее - чтобы задать теперь хорошую cost-функцию, которую затем нужно минимизировать\n",
    "\n",
    "Cost-функцией будет \n",
    "$$Cost = KL(P||Q) = \\sum_{i \\neq j}{p_{ij} log \\frac{p_{ij}}{q_{ij}}}$$\n",
    "\n",
    "Минимизируем это градиентым спуском. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, такое требование минимизировать такую cost-фуункцию говорит следующее: я хочу получить такое представление, чтобы объекты, которые находились в исходном пространстве близко, **вероятно**, находились и в представлении близко, а объекты, которые находились даеко - далеко. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример применения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшим размерность нашего \"клеточного\" набора данных предварительно при помощи PCA. \n",
    "Преследуем две цели - уменьшить время работы tSNE (который работает очень медленно) и убрать эффект шума на tSNE - он может на него реагировать, особенно при условии, что схождения к минимуму мы можем не дождаться\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import sklearn.manifold\n",
    "\n",
    "X_reduced = PCA(n_components = 6).fit_transform(X_scRNAseq)\n",
    "model = sklearn.manifold.TSNE(n_components = 2, \n",
    "             init = X_reduced[:, 0:2], # часто в качестве разумного приближения используют \n",
    "             perplexity=40, # важный параметр\n",
    "             verbose = 2)\n",
    "\n",
    "manifold = model.fit_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize = (20,15))\n",
    "plt.scatter(manifold[:, 0], manifold[:, 1], c = Y_scRNAseq, cmap = 'tab20', s = 20)\n",
    "plt.title('TSNE: scRNAseq', fontsize = 25); \n",
    "plt.xlabel(\"TSNE1\", fontsize = 22); plt.ylabel(\"TSNE2\", fontsize = 22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "И покрасим по разметке, которая нам известна из эксперимента. Видим, что разделение очень хорошее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важные параметры tSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perplexity \n",
    "\n",
    "Определяет то, как подбирается стандартное отклонение для распределения расстояний для каждой точки. Чем больше **perplexity** - тем более на глобальную структуру мы смотрим\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metric\n",
    "Как считаются расстояния между точками - **metric**. По-умолчанию используется евклидово расстояние, но часто помогают и другие (например, косинусное)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning_rate\n",
    "\n",
    "Шаг градиентного спуска, тоже влияет на полученное представление. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы tSNE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы продемонстрировать проблемы tSNE берем данные оригинальные данные в 2D пространстве и добавляем к ним новые признаки, взятые из нормального шума. \n",
    "Далее пытаемся восстановить изначальную структуру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стохастичность\n",
    "\n",
    "Низкоразмерное представление, которое вы получите, будет отличаться между запусками, если не зафиксиоровать **random seed**. Может отличаться довольно сильно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление новых точек\n",
    "\n",
    "Если у вас появились новые данные, то добавить их на  представление, полученно при помощи tSNE ранее - нетривиальная задача. \n",
    "Для разных областей есть свои \"подгоны\", но все это эвристика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Расстояния между кластерами точек могут ничего не значить (плохо сохраняются далекие расстояния)\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne_problem0.png\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Размеры кластеров ничего не значат\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne_problem3.png\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Можно увидеть артефактные кластеры \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne_problem2.png\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Можно увидеть не ту структуру, которая по идее должна быть \n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/tsne_problem1.png\" width=1000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Подробнее](https://distill.pub/2016/misread-tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование для кластеризации\n",
    "\n",
    "\n",
    "\n",
    "Из-за указанных  недостатков результат tSNE НЕЛЬЗЯ использовать для кластеризации. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация при помощи PCA \n",
    "\n",
    "Для того, чтобы tSNE сходился лучше и определеннее, в качестеве изначальных координат точек в новом пространстве можно использовать не слуучайный шум, а первые две компоненты PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Хорошее видео про t-SNE](youtube.com/watch?v=NEaUSP4YerM)\n",
    "\n",
    "[Статья по применению t-SNE в биологии](https://www.nature.com/articles/s41467-019-13056-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "[UMAP](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html) — uniform manifold approximation and projection. [Видео](https://www.youtube.com/watch?v=94ZMJ8tq1Wk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использует похожие на tSNE идею, но иначе, в результате чего получает много выгодных бонусов. \n",
    "\n",
    "Внутри себя метод строит граф, в котором ребрами соединены между собой k ближайших соседей. При этом эти ребра неравноправны - если для данной пары точек расстояние между ними сильно больше, чем расстояния между ними и другими точками - то и ребро будет иметь маленький вес. \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/L04_umap_graph.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее задача состоит в том, чтобы в пространстве более низкой размерность получился граф похожий на тот, который был в высокой размерностью. \n",
    "Для этого опять же, оптимизируем низкоразмерное представление градиентным спуском"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример применения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from umap import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_reduced = PCA(n_components = 9).fit_transform(X_scRNAseq)\n",
    "model = UMAP(n_components = 2, \n",
    "             min_dist = 1, \n",
    "             n_neighbors = 93,\n",
    "             init = X_reduced[:, 0:2], \n",
    "             # много где, не только он, рекомендуют использовать для инициализации UMAP и tSNE первые две компоненты PCA\n",
    "             n_epochs = 1000, \n",
    "             verbose = 2)\n",
    "umap = model.fit_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize = (20,15))\n",
    "plt.scatter(umap[:, 0], umap[:, 1], c = Y_scRNAseq, cmap = 'tab20', s = 20)\n",
    "plt.title('UMAP: scRNAseq', fontsize = 25); \n",
    "plt.xlabel(\"UMAP1\", fontsize = 22); plt.ylabel(\"UMAP2\", fontsize = 22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важные параметры\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_neighbors\n",
    "\n",
    "Число соседей, которые ищутся для каждой точки. Влияет на то, насколько глобально мы смотрим на структуру данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### min_dist\n",
    "\n",
    "Влияет на то, насколько близко могут находиться между собой точки в новом представлении. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metric\n",
    "\n",
    "Как считаются расстояния между точками. Опять же, по-умолчанию расстояние Евклидово, но это не всегда дает лучшший результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преимущества перед tSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скорость работы\n",
    "\n",
    "Выдает результаты, похожие на tSNE, но  работает в разы быстрее\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/UMAP-TSNE-performance.png\" width=450/>\n",
    "\n",
    "[Более подробно](https://umap-learn.readthedocs.io/en/latest/performance.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возможность проецировать новые точки\n",
    "UMAP может проецировать точки из новых датасетов на уже имеющееся представление. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Может объединять представления\n",
    "\n",
    "Если у вас есть признаки, сильно отличающиеся по своим свойствам, то можно построить для них представления отдельно, с разными метриками, а далее объединить их. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Более интуитивные параметры\n",
    "\n",
    "Параметры **n_neighbours** и **min_dist** намного понятнее их аналога - perplexity,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Можно использовать для отображения и в пространства бОльшей размерности\n",
    "\n",
    "\n",
    "\n",
    "Кроме того, его можно использовать для понижения размерности не только до 2-3 (в целях визуализации), но и для больших размерностей, с которыми потом работать другими методами (хотя здесь надо быть аккуратным, он тоже склоннен деформировать дальние расстояния)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Расстояния более информативны\n",
    "\n",
    "Плохая идея интерпретировать расстояния между кластерами и их размеры для 2D представлений в случае UMAP. \n",
    "\n",
    "Но в случае UMAP это менее выраженно. \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img_license/umap_dist.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Если же отображать в пространство размерности бОльшей, чем два, то можно получить и очень хорошие представления. Но это все эвристика - может и не повезти\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised UMAP\n",
    "\n",
    "UMAP позволяет передать в него метки объектов - вы можете получать представление, которое оптимизировано под имеющуюся у вас информацию о кластерах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi-Supervised UMAP\n",
    "\n",
    "UMAP позволяет передать в него метки **только для части** объектов - вы можете получать представление, которое оптимизировано под имеющуюся у вас информацию о кластерах, но при этом не оставляет без внимания объекты без меток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE и UMAP на цифрах\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, datasets\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE embedding of the digits dataset\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plot_embedding(X_tsne, \"t-SNE embedding of the digits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap = umap.UMAP(n_neighbors=5)\n",
    "X_umap = umap.fit_transform(X) # преобразовываем\n",
    "plot_embedding(X_umap, \"UMAP embedding of the digits\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
