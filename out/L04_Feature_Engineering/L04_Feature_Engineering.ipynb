{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для иллюстрации будут использованны примеры из книги \"[Real-World Machine Learning](https://www.manning.com/books/real-world-machine-learning)\" из открытого репозиторя:\n",
    "https://github.com/brinkar/real-world-machine-learning\n",
    "\n",
    "И датасет Titanic:\n",
    "http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf\n",
    "\n",
    "В отличие от датасетов с которыми мы работали до сих пор это просто список пассажиров судна. Данные в нем не предобработанны и в сыром виде не могут быть использованны для обучения модели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/brinkar/real-world-machine-learning/master/data/titanic.csv\n",
    "\n",
    "import pandas\n",
    "data = pandas.read_csv(\"titanic.csv\")\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Часть полей можно исключить (имя)\n",
    "\n",
    "2. Часть преобразовать в числа (пол, порт посадки ...)\n",
    "https://github.com/brinkar/real-world-machine-learning/blob/master/Chapter%203%20-%20Modeling%20and%20prediction.ipynb\n",
    "\n",
    "3. Непрерывные данные нормировать (здесь вместо этого берется квадратный корень из цены)\n",
    "\n",
    "4. На основании некоторых создать новые более полезные для модели (Номер кабины):\n",
    "cabin_data = array([\"C65\", \"\", \"E36\", \"C54\", \"B57 B59 B63 B66\"])\n",
    "->\n",
    "[['C', 65, 1], ['X', -1, 0], ['E', 36, 1], ['C', 54, 1], ['B', 57, 4]]\n",
    "\n",
    "https://github.com/brinkar/real-world-machine-learning/blob/master/Chapter%202%20-%20Data%20Processing.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The categorical-to-numerical function from chapter 2\n",
    "# Changed to automatically add column names\n",
    "def cat_to_num(data):\n",
    "  categories = set(data)\n",
    "  features = {}\n",
    "  for cat in categories:\n",
    "      binary = (data == cat)\n",
    "      if len(set(binary)) == 1:\n",
    "        # Ignore features where all values equal \n",
    "        continue\n",
    "      new_key = f'{data.name}={cat}'\n",
    "\n",
    "      features[new_key] = binary.astype(\"int\")\n",
    "  return pandas.DataFrame(features)\n",
    "\n",
    "def cabin_features(data):\n",
    "    features = []\n",
    "    for cabin in data:\n",
    "        cabins = str(cabin).split(\" \")\n",
    "        n_cabins = len(cabins)\n",
    "        # First char is the cabin_char\n",
    "        try:\n",
    "            cabin_char = cabins[0][0]\n",
    "        except IndexError:\n",
    "            cabin_char = \"X\"\n",
    "            n_cabins = 0\n",
    "        # The rest is the cabin number\n",
    "        try:\n",
    "            cabin_num = int(cabins[0][1:]) \n",
    "        except:\n",
    "            cabin_num = -1\n",
    "        # Add 3 features for each passanger\n",
    "        features.append( [cabin_char, cabin_num, n_cabins] )\n",
    "    features=np.array(features)\n",
    "    dic_of_features =  {\n",
    "        #'Cabin_char' : features[:,0],\n",
    "        'Cabin_num' : features[:,1].astype(\"int\"),\n",
    "        'N_cabins' : features[:,2].astype(\"int\"),\n",
    "         }\n",
    "    out = pandas.DataFrame(dic_of_features)\n",
    "    char_column = pandas.DataFrame({'Cabin_char' : features[:,0]})\n",
    "    cabin_ch = cat_to_num(char_column['Cabin_char'])\n",
    "    return out.join(cabin_ch)\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"Takes a dataframe of raw data and returns ML model features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initially, we build a model only on the available numerical values\n",
    "    features = data.drop([\"PassengerId\", \"Survived\", \"Fare\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"], axis=1)\n",
    "    \n",
    "    # Setting missing age values to -1\n",
    "    features[\"Age\"] = data[\"Age\"].fillna(-1)\n",
    "    \n",
    "    # Adding the sqrt of the fare feature\n",
    "    features[\"sqrt_Fare\"] = np.sqrt(data[\"Fare\"])\n",
    "    \n",
    "    # Adding gender categorical value\n",
    "    features = features.join( cat_to_num(data['Sex']) )\n",
    "    \n",
    "    # Adding Embarked categorical value\n",
    "    features = features.join( cat_to_num(data['Embarked']) )\n",
    "\n",
    "    # Split cabin\n",
    "    features = features.join( cabin_features(data['Cabin']) )\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = prepare_data(data)\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь модель обучать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We make a 80/20% train/test split of the data\n",
    "features = prepare_data(data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data[\"Survived\"], test_size=0.2, random_state=33)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры данных которые нецелесообразноо отправлять в модель в сыром виде:\n",
    "\n",
    "### IP адрес: \n",
    "xxx.xxx.xxx.xxx -> регион, провайдер\n",
    "\n",
    "### Координаты атомов:\n",
    "AlphaFold\n",
    "https://yakovlev.me/para-slov-za-alphafold2/\n",
    "\n",
    "\"как вообще математически можно выразить структуру белка. До того мы постоянно неявно подразумевали, что это координаты атомов (всех, каркасных, Cα или каких-то ещё), но на практике это очень неудачное представление, поскольку оно не единственное. Мы обычно считаем, что предсказание работает как некоторая детерминированная функция: принимает на вход последовательность и всегда возвращает один единственный ответ. Но какой из бесконечного набора координат \"канонический\"?\n",
    "\n",
    "\n",
    "\n",
    "\"... инвариантом структуры является матрица (таблица) всех попарных расстояний между атомами. \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлых лекциях вы познакомились с тем, что такое признаки. Сегодня мы попытаемся научиться отделять полезные признаки от бесполезных, а также понижать размерность пространства признаков.\n",
    "\n",
    "## Зачем отбирать признаки?\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_features.png\" alt=\"alttext\" width=760/>\n",
    "\n",
    "*Во-первых*, признаков может быть слишком много, больше чем нужно. Это может возникнуть в ситуациях, когда используется вся имеющаяся на данный момент информация, потому что неизвестно, какая её часть может понадобиться, а какая — нет. В таких случаях можно повысить качество решения задачи, выбирая только действительно важные признаки. Существует другой подход: можно сформировать новые признаки на основе старых, таким образом признаков станет меньше, но их информативность сохранится.\n",
    "\n",
    "*Во-вторых*, существуют признаки, из-за которых при решении задачи возникает много проблем. Это шумовые признаки — признаки, которые не связаны с целевой переменной и никак не относятся к решаемой задаче. К сожалению, не всегда можно понять по обучающей выборке, что в ней присутствуют такие признаки.\n",
    "Полезно рассмотреть несколько примеров присутствия шумовых признаков в данных. Пусть в выборку добавляют 1000 признаков. Значения каждого признака генерируются из стандартного нормального распределения. Понятно, что эти признаки бесполезны, они никак не помогут решить задачу. Но, поскольку их много, может так оказаться (из соображений теории вероятностей), что один из них коррелирует с целевой переменной. При этом он будет коррелировать только на обучающей выборке, а на контрольной выборке корреляции наблюдаться не будет, поскольку признак абсолютно случайный. Однако внутри модели этот признак может быть учтён как важный и иметь какой-то вес. Получается, что модель зависит от признака, который никак не помогает решить задачу. Из-за этого качество модели и ее обобщающая способность окажутся ниже, чем хотелось бы. \n",
    "\n",
    "В более общем случае можно говорить, что в многоразмерном пространстве почти всегда можно найти корреляции. См. картинку.\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_correlations.png\" alt=\"alttext\" width=600/>\n",
    "\n",
    "\n",
    "Больше подобных примеров можно найти на https://www.wnycstudios.org/podcasts/otm/articles/spurious-correlations.\n",
    "\n",
    "*В-третьих*, перед нами может стоять задача ускорения модели. Дело в том, что чем больше признаков, тем более сложная модель получается, и тем больше времени необходимо, чтобы построить прогноз. Существуют задачи, в которых прогнозы нужно строить очень быстро, например, выдача рекомендаций товаров на сайте интернет-магазина. Пользователь что-то ищет, нажимает на ссылку в поисковой выдаче и переходит на страницу интересующего его товара. На этой странице есть поле, в котором показываются рекомендации к этому товару, например похожие товары, которые должна выдавать модель. Важно, чтобы она выдавала рекомендации очень быстро, страница не должна долго загружаться, чтобы пользователь не подумал, что с сайтом что-то не так, и не ушел к конкуренту. В этом случае необходимо, чтобы модель была очень быстрой, и один из подходов к ускорению модели — это отбор признаков, которых достаточно, чтобы прогнозы были хорошими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Одномерный отбор признаков\n",
    "\n",
    "Самый простой подход к отбору признаков — это одномерный подход. В нём оценивается связь каждого признака с целевой переменной, например, измеряется корреляция. Такой подход — довольно простой, он не учитывает сложные закономерности, в нём все признаки считаются независимыми, тогда как в машинном обучении модели учитывают взаимное влияние признаков, их пар или даже более сложные действия на целевую переменную. Этот подход не всегда хорош, но иногда его можно использовать, чтобы ранжировать признаки, найти наиболее мощные среди них.\n",
    "\n",
    "### Формализация задачи\n",
    "Пусть у нас есть N объектов с K признаками и для каждого объекта задана целевая переменная или ответ. Обозначим  матрицу объектов-признаков через $X \\in \\mathbf{R}^{N x M} $, а вектор ответов через Y. Для удобства введем следующие дополнительные обозначения:\n",
    "\n",
    "* $\\overline{X}_j$- среднее значение признака j по всей выборке\n",
    "* $\\overline{Y}$- среднее значение целевой переменной на всей выборке\n",
    "\n",
    "Задача — оценить предсказательную силу (информативность) каждого признака, то есть насколько хорошо по данному признаку можно предсказывать целевую переменную. Данные оцененной информативности можно использовать, чтобы отобрать k лучших признаков или признаки, у которых значение информативности больше порога (например, некоторой квантили распределения информативности).\n",
    "\n",
    "### Корреляция\n",
    "\n",
    "Один из самых простых методов измерения связи между признаком и ответами — это корреляция. Корреля́ция (от лат. correlatio «соотношение»), или корреляцио́нная зави́симость — статистическая взаимосвязь двух или более случайных величин (либо величин, которые можно с некоторой допустимой степенью точности считать таковыми). При этом изменения значений одной или нескольких из этих величин сопутствуют систематическому изменению значений другой или других величин.\n",
    "\n",
    "Коэффициент корреляции R определяется формулой:\n",
    "\n",
    "$ R = \\frac{\\sum_{i=1}^{N} (X_{ij} - \\overline{X}_j)(Y_{i} - \\overline{Y})} {\\sqrt{ \\sum_{i=1}^{N}(X_{ij} - \\overline{X}_j)^2\\sum_{i=1}^{N} (Y_{i} - \\overline{Y})^2}} $\n",
    "\n",
    "\n",
    "Чем больше по модулю корреляция между признаком и целевой переменной, тем более информативным является данный признак. При этом она максимальна по модулю (Rj = ±1), если между признаком и целевой переменной есть **линейная связь**, то есть если целевую переменную можно строго линейно выразить через значение признака. Это означает, что корреляция измеряет только линейную информативность, то есть способность признака линейно предсказывать целевую переменную. Вообще говоря, корреляция рассчитана на вещественные признаки и вещественные ответы. Тем не менее, её можно использовать в случае, если признаки и ответы бинарные (имеет смысл кодировать бинарный признак с помощью значений ±1).\n",
    "\n",
    "BTW: не является истинной мерой расстояния, так как для нее не выполняется неравенство треугольника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from scipy import stats\n",
    "\n",
    "correlations = []\n",
    "for column in features:\n",
    "  r , p_value = stats.pearsonr(features[column], data[\"Survived\"])\n",
    "  correlations.append((column,r))\n",
    "\n",
    "correlations.sort(key=lambda tup: abs(tup[1]),reverse=True)\n",
    "for name, r in correlations:\n",
    "  print(f'{name} : {r:.3f} ')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC-ROC\n",
    "\n",
    "\n",
    "Пусть решается задача бинарной классификации, и необходимо оценить важность признака j для решения именно этой задачи. В этом случае можно попробовать построить классификатор, который использует лишь этот один признак j, и оценить его качество. Например, можно рассмотреть очень простой классификатор, который берёт значение признака j на объекте, сравнивает его с порогом t, и если значение меньше этого порога, то он относит объект к первому классу, если же меньше порога — то к другому, нулевому или минус первому, в зависимости от того, как мы его обозначили. Далее, поскольку этот классификатор зависит от порога t, то его качество можно измерить с помощью таких метрик, как площадь под ROC-кривой или Precision-Recall кривой, а затем по данной площади отсортировать все признаки и выбирать лучшие.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Проблемы одномерного отбора признаков\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_multidim.png\" alt=\"alttext\" width=600/>\n",
    "\n",
    "\n",
    "У подхода, при котором важности всех признаков оцениваются по отдельности, есть свои недостатки. На левом рисунке изображена двумерная выборка, для которой необходимо решить задачу классификации. Если спроецировать данную выборку на ось абсцисс, то она будет разделима, хотя и будут присутствовать ошибки. Если же спроецировать данную выборку на ось ординат, то все объекты разных классов перемешаются, и выборка будет неразделима. В этом случае при использовании любого метода одномерного оценивания информативности первый признак будет информативен, а второй — совершенно неинформативен. Тем не менее, видно, что если использовать эти признаки одновременно, то классы будут разделимы идеально. На самом деле, второй признак важен, но он важен только в совокупности с первым, и методы одномерного оценивания информативности не способны это определить. На рисунке справа показана выборка, на которой одномерные методы оценки информативности работают ещё хуже. В этом случае, если спроецировать выборку на ось абсцисс или ординат, то объекты классов перемешаются, и в обоих случаях данные будут совершенно неразделимы. И, согласно любому из описанных методов, оба признака неинформативны. Тем не менее, если использовать их одновременно, то, например, решающее дерево может идеально решить данную задачу классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример: Влияние роста и веса при предсказании вероятности сердечного заболевания. Избыточный вес может являться важным фактором, но оценить является ли он избыточным или нормальным можно только зная рост пациента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Жадный отбор признаков\n",
    "\n",
    "\n",
    "Жадные методы отбора признаков, по сути своей, являются надстройками над методами обучения моделей. Они перебирают различные подмножества признаков и выбирают то из них, которое дает наилучшее качество определённой модели машинного обучения.\n",
    "Данный процесс устроен следующим образом. Обучение модели считается черным ящиком, который на вход принимает информацию о том, какие из его признаков можно использовать при обучении модели, обучает модель, и дальше каким-то методом оценивается качество такой модели, например, по отложенной выборке или кросс-валидации. Таким образом, задача, которую необходимо решить — это оптимизация функционала качества модели по подмножеству признаков.\n",
    "\n",
    "### Полный перебор\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_graphic-full.png\" alt=\"alttext\" width=600/>\n",
    "\n",
    "\n",
    "Самый простой способ решения данной задачи — это полный перебор всех подмножеств признаков и оценивание качества на каждом подмножестве. Итоговое подмножество — то, на котором качество модели наилучшее. Этот перебор можно структурировать и перебирать подмножества последовательно: сначала те, которые имеют мощность 1 (наборы из 1 признака), потом наборы мощности 2, и так далее. Это подход очень хороший, он найдет оптимальное подмножество признаков, но при этом очень сложный, поскольку всего таких подмножеств $2^d$, где d — число признаков. Если признаков много — сотни или тысячи, то такой перебор невозможен: он займет слишком много времени, возможно, сотни лет или больше. Поэтому такой метод подходит либо при небольшом количестве признаков, либо если известно, что информативных признаков очень мало, единицы.\n",
    "\n",
    "\n",
    "### Жадное добавление\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_graphic-greedy.png\" alt=\"alttext\" width=600/>\n",
    "\n",
    "\n",
    "Если же признаков много и известно, что многие из них информативны, то нужно применять жадную стратегию. Жадная стратегия используется всегда, когда полный перебор не подходит для решения задачи. Например, может оказаться неплохой стратегия жадного наращивания (жадного добавления). Сначала находится один признак, который дает наилучшее качество модели (наименьшую ошибку Q):\n",
    "\n",
    "$i_1 = argmin Q(i)$. \n",
    "\n",
    "Тогда множество, состоящее из этого признака:\n",
    "\n",
    "$J_1 = {i_1}$\n",
    "\n",
    "Дальше к этому множеству добавляется еще один признак так, чтобы как можно сильнее уменьшить ошибку модели:\n",
    "\n",
    "$i_2 =argminQ(i_1,i)$, $J_2 ={i_1,i_2}$.\n",
    "\n",
    "Далее каждый раз добавляется по одному признаку, образуются множества J3 , J4 , . . . . Если в какой-то момент невозможно добавить новый признак так, чтобы уменьшить ошибку, процедура останавливается. Жадность процедуры заключается в том, что как только какой-то признак попадает в оптимальное множество, его нельзя оттуда удалить.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "#https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py\n",
    "\n",
    "#http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/#example-5-sequential-feature-selection-for-regression\n",
    "import pandas as pd\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector \n",
    "sfs = SequentialFeatureSelector(LogisticRegression(max_iter=1000),k_features = 8, cv=5)\n",
    "sfs.fit(X_train,y_train)\n",
    "\n",
    "pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD-DEL\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_graphic-non-greedy.png\" width=600/>\n",
    "\n",
    "\n",
    "Описанный выше подход довольно быстрый: в нем столько итераций, сколько признаков в выборке. Но при этом он слишком жадный, перебирается слишком мало вариантов и мы можем оказаться в плохой локальной точке. Процедуру можно усложнить. Один из подходов к усложнению — это алгоритм ADD-DEL, который не только добавляет, но и удаляет признаки из оптимального множества. Алгоритм начинается с процедуры жадного добавления. Множество признаков наращивается до тех пор, пока получается уменьшить ошибку, затем признаки жадно удаляются из подмножества, то есть перебираются все возможные варианты удаления признака, оценивается ошибка и удаляется тот признак, который приводит к наибольшему уменьшению ошибки на выборке. Эта процедура повторяет добавление и удаление признаков до тех пор, пока уменьшается ошибка. Алгоритм ADD-DEL всё еще жадный, но при этом он менее жадный, чем предыдущий, поскольку *может исправлять ошибки*, сделанные в начале перебора: если вначале был добавлен неинформативный признак, то на этапе удаления от него можно избавиться.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_graphic-add-del.png\" style=\"height: 470px;\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/#example-5-sequential-feature-selection-for-regression\n",
    " \n",
    "sffs = SequentialFeatureSelector(\n",
    "           LogisticRegression(max_iter=1000),\n",
    "           k_features = 7,\n",
    "           forward=True, \n",
    "           floating=True, \n",
    "           verbose=0,\n",
    "           scoring='accuracy',\n",
    "           cv=0)\n",
    "\n",
    "sffs.fit(X_train.values,y_train)\n",
    "pd.DataFrame.from_dict(sffs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков на основе моделей\n",
    "\n",
    "### Использование весов признаков\n",
    "\n",
    "Во многих моделях (eg линейных) перед признаками стоят веса. Если признаки масштабированы, то веса при признаках можно интерпретировать как информативности: чем больше по модулю вес при признаке j, тем больший вклад этот признак вносит в ответ модели. Однако если признаки не масштабированы, то так использовать веса уже нельзя. Например, если есть два признака, и один по масштабу в 1000 раз меньше другого, то вес первого признака может быть очень большим, только чтобы признаки были одинаковыми по масштабу.\n",
    "Если необходимо обнулить как можно больше весов, чтобы линейная модель учитывала только те признаки, которые наиболее важны для нее, можно использовать L1-регуляризацию. Чем больше коэффициент при L1-регуляризаторе, тем меньше признаков будет использовать линейная модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим это на примере Линейного классификатора который мы конструировали на 2-м занятии.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/\n",
    "L04_skalyar.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "for i, w in enumerate(lr.coef_[0]):\n",
    "  print(X_train.columns[i],w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача понижении размерности\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Метод главных компонент)\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_PCA-demonstrative.png\" width=500/>\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/PCA-main.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отображение в пространство меньшей размерности. постановка задачи\n",
    "\n",
    "Метод главных компонент (англ. principal component analysis, PCA) — один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации. Изобретён Карлом Пирсоном в 1901 году.\n",
    "\n",
    "Существует несколько способов сформулировать задачу метода главных компонент:\n",
    "\n",
    "* через максимизацию дисперсии;\n",
    "* через аппроксимацию данных линейными многообразиями меньшей размерности;\n",
    "* через приближение матрицы матрицы с рангом k;\n",
    "* через построения для данной многомерной случайной величины ортогонального преобразования координат, в результате которого корреляции между отдельными координатами обратятся в нуль.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_lec-1.png\" width=850/>\n",
    "\n",
    "\n",
    "\n",
    "Красота метода PCA заключается в том, что такие отличные постановки задачи из разных областей математики сводятся к одному и тому же решению. Рассмотрим некоторые из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод главных компонент как способ проекции данных на гиперплоскость\n",
    "\n",
    "Пусть имеется выборка, изображенная на рисунке ниже, и её необходимо спроецировать на некоторую прямую. В этом случае прямая будет тем лучше, чем меньше будет ошибка проецирования суммы по всей выборке расстояний от объекта до его проекции на эту прямую. Чем меньше эти расстояния, тем лучше прямая приближает данные, тем меньше будет ошибка и тем больше информации сохраняется. В идеальном случае прямая должна проходить через все объекты выборки, но в рассматриваемой ситуации это невозможно.\n",
    "\n",
    "В общем случае, когда признаков много, выборка проецируется на гиперплоскость. Из аналитической геометрии известно, что есть два способа задания гиперплоскости. Первый — с помощью вектора нормали, он использовался в линейных методах. Второй — с помощью направляющих векторов. Пусть в исходном пространстве размерности D строится гиперплоскость размерности D − 1, тогда, если выбрать на этой плоскости D линейно независимых векторов, то они будут однозначно задавать эту гиперплоскость. Если направляющие векторы составить в матрицу W, так что каждый столбец этой матрицы — это один направляющий вектор, то проекция точки $x_i$ на данную гиперплоскость будет вычисляться по формуле $ x_i ∗ W $. Тогда для того, чтобы уменьшить ошибку проецирования на гиперплоскость, необходимо минимизировать следующее выражение:\n",
    "\n",
    "$ \\sum_{i=1}^l \\lVert x_i - x_i * W \\rVert^2 \\to \\min_W $\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_lec-4.png\" width=650/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Максимизация дисперсии выборки после понижения размерности\n",
    "\n",
    "\n",
    "Иной взгляд на метод главных компонент заключается в следующем: пусть имеется выборка, показанная на рисунке ниже, и требуется выбрать прямую, на которую можно будет эту выборку, максимизировав при этом дисперсию спроецированных данных. Синяя прямая лучше подходит для решения данной задачи, поскольку при проецировании на нее сохраняется наибольший разброс.\n",
    "И действтительно, чем больше дисперсия выборки после проецирования на прямую, тем больше сохраняется информации. Для данного случая этот критерий хорошо подходит: дисперсия выборки после проецирования на синюю прямую будет гораздо больше, чем после проецирования на красную прямую.\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_lec-6.png\" width=700/>\n",
    "\n",
    "\n",
    "\n",
    "Формально дисперсию выборки после проецирования можно записать следующим образом:\n",
    "\n",
    "$ \\sum_{j=1}^d w_{j}^T X^T X w_{j} \\to \\max_{W}$\n",
    "\n",
    "Чем больше значение этой суммы, тем больше оказывается дисперсия выборки после проецирования на гиперплоскость, которая задается матрицей весов W. Таким образом, это выражение нужно максимизировать, чтобы сохранить как можно больше информации.\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_lec-8.png\" width=480/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание на то что выборка центрированна:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_covariation.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Решение\n",
    "\n",
    "#### Вывод решения\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_lec-10.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_first_component_1.png\" >\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_first_component_2.png\" >\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_first_component_3.png\" >\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ранее была описана формулировка задачи метода главных компонент, теперь необходимо её решить. Одна из постановок задачи метода главных компонент — это максимизация дисперсии:\n",
    "\n",
    "$ \n",
    "\\begin{cases} \n",
    "\\sum_{j=1}^d w_{j}^T X^T X w_{j} \\to \\max_{W} \\\\ \n",
    "W^T W = I\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "Второе условие здесь приведено для обеспечения единственности решения.\n",
    "\n",
    "Это является задачей оптимизации. Можно попытаться найти ее решение. (Если вы не знакомы с математическим анализом и далее ничего не поймете, то ничего страшного).\n",
    "\n",
    "В первой строке записана дисперсия после проецирования, а во второй — ограничение, обеспечивающее наличие единственного решения.\n",
    "\n",
    "В методе главных компонент есть один нюанс: выражение, через которое записана дисперсия, будет означать именно дисперсию выборки только в том случае, если матрица объекты-признаки центрирована (среднее каждого признака равно нулю). Далее считается, что выборка центрирована, и среднее из каждого столбца в матрице объекты-признаки уже вычли.\n",
    "\n",
    "Итак, чтобы разобраться, как устроено решение этой задачи, необходимо сначала рассмотреть простой частный случай: требуется найти ровно одну компоненту, на которую проецируется вся выборка, так, чтобы дисперсия после проецирования была максимальной:\n",
    "\n",
    "$ \n",
    "\\begin{cases} \n",
    "w_{1}^T X^T X w_{1} \\to \\max_{w_1} \\\\ \n",
    "w_1^T w_1 = 1\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Для решения подобных задач условной оптимизации необходимо выписать лагранжиан:\n",
    "\n",
    "....\n",
    "\n",
    "После преобразований получается следующее выражение:\n",
    "\n",
    "$X^T X w_1 = \\lambda w_1$\n",
    "\n",
    "Из него следует, что $w_1$ — это собственный вектор матрицы $X^T X$, а число $ \\lambda $ является собственным значением, соответствующим этому вектору. Домножим слева полученное выражение на $w_1^T$. так-как $w_1^T w_1 = 1$, то $ w_{1}^T X^T X w_{1} = \\lambda $, что говорит нам о том, что дисперсия выборки после проецирования будет равна собственному значению, соответствующему выбранному собственному вектору.\n",
    "\n",
    "Итак, в методе главных компонент первая компонента — это собственный вектор матрицы $X^T X$, который соответствует максимальному собственному значению этой матрицы. Стоит обратить внимание, что $X^T X$ — это матрица ковариации, то есть именно та матрица, которая характеризует дисперсию выборки.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_first_component_4.png\" width=300/>\n",
    "\n",
    "Визуально это выглядит следующим образом: есть облако точек (рисунок 3.8), и необходимо выбрать именно то направление, при проецировании на которое сохраняется как можно больше дисперсии. Это направление и будет задаваться первым собственным вектором матрицы ковариации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итог:\n",
    "\n",
    "\n",
    "## $XX^{T} = \\lambda W$\n",
    "\n",
    "\n",
    "Пример вычисления PCA на Python:\n",
    "https://cs231n.github.io/neural-networks-2/#datapre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Остаточная дисперсия и производные метрики\n",
    "\n",
    "На практике хочется как-то измерять значимость каждой компоненты. Для этого существует понятие остаточной дисперсии (explained variance).\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_explained-variance.png\" width=700/>\n",
    "\n",
    "\n",
    "\n",
    "### Практика: Ковариационные матрицы\n",
    "\n",
    "Итак, давайте для начала подробнее познакомимся с понятием ковариционной матрицы.\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_gaussian_probability.png\" width=600/>\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_covariance_matrix.png\" width=600/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn as sklearn\n",
    "import matplotlib.patches as mpatches\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting principal components\n",
    "def plot_principal_components(data, model, scatter=True, legend=True):\n",
    "    W_pca = model.components_\n",
    "    if scatter:\n",
    "        plt.scatter(data[:,0], data[:,1])\n",
    "    plt.plot(data[:,0], -(W_pca[0,0]/W_pca[0,1])*data[:,0], color=\"c\")\n",
    "    plt.plot(data[:,0], -(W_pca[1,0]/W_pca[1,1])*data[:,0], color=\"c\")\n",
    "    if legend:\n",
    "        c_patch = mpatches.Patch(color='c', label='Principal components')\n",
    "        plt.legend(handles=[c_patch], loc='lower right')\n",
    "    # сделаем графики красивыми:\n",
    "    plt.axis('equal')\n",
    "    limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1]))-0.5,\n",
    "              np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))+0.5]\n",
    "    plt.xlim(limits[0],limits[1])\n",
    "    plt.ylim(limits[0],limits[1])\n",
    "    plt.draw()\n",
    "\n",
    "# function for vector plotting\n",
    "def plot_components_vector(ax, data, vector, color, label=\"\", delta=0.5):\n",
    "    limits_x = [np.min(data[:,0])-delta, np.max(data[:,0])+delta]\n",
    "    limits_y = [np.min(data[:,1])-delta, np.max(data[:,1])+delta]\n",
    "    \n",
    "    if np.fabs(vector[1]) > 1e-5:\n",
    "        if np.fabs(vector[0]) > 1e-5:\n",
    "            x = np.arange(*limits_x, 0.1)\n",
    "            y = x * vector[1]/vector[0]\n",
    "        else:\n",
    "            y = np.arange(*limits_y, 0.1)\n",
    "            x = np.full_like(y, 0)\n",
    "    else:\n",
    "        x = np.arange(*limits_x, 0.1)\n",
    "        y = np.full_like(x, 0)\n",
    "        \n",
    "    ax.plot(x, y, color=color, label=label)\n",
    "    # ax.set_xlim(*limits_x)\n",
    "    # ax.set_ylim(*limits_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы семлировать многоразмерное нормальное распределение, можно использовать функцию\n",
    "\n",
    "```python\n",
    "np.random.multivariate_normal(mu, covariance_matrix, size=500)\n",
    "```\n",
    "\n",
    "* mu - вектор средних значений (центр колокола);\n",
    "* covariance_matrix - матрица ковариации;\n",
    "* size - размер семлированной выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's sample data from multivariate normal distribution with covaraince matrix C \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1 , figsize = (8, 4))\n",
    "\n",
    "mu = np.zeros(2)\n",
    "C_simple = np.array([[4,0],\n",
    "              [0,2]])\n",
    "\n",
    "data_simple = np.random.multivariate_normal(mu, C_simple, size=300)\n",
    "ax.scatter(data_simple[:,0], data_simple[:,1], alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, данные распределены в соответствии с двумерным распределением. А так как значения матрицы ковариации вне диагонали равны нулю, то корреляции между переменными нет, поэтому график не повернут.\n",
    "\n",
    "Давайте попробуем его повернуть. Для этого нам необходимо найти ковариационную матрицу с ненулевыми значениями вне диагонали.\n",
    "\n",
    "Так как матрица $ X^T X $ всегда положительно полуопределена (то есть собственные значения) > 0 (в самом деле $ w^T X^T X w = (Xw)^T Xw \\ge 0$) и симметрична (является эрмитовой), то необходимо просто подобрать подходящий X. \n",
    "Пусть это будет $ X = \\begin{pmatrix} 1 & 1 \\\\ 0 & 3 \\end{pmatrix}$. Тогда:\n",
    "$X^T X = \\begin{pmatrix} 1 & 1 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 2 & 3 \\\\ 3 & 9 \\end{pmatrix}$.\n",
    "\n",
    "Сгененрируем данные с такой ковариационной матрицей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1 , figsize = (8, 4))\n",
    "\n",
    "mu = np.zeros(2)\n",
    "\n",
    "C_angle = np.array([[2, 3], \n",
    "              [3, 9]])\n",
    "#C_angle = np.array([[9, 3], \n",
    "#              [3, 2]])   \n",
    "\n",
    "'''C_angle = np.array([[3, 3], \n",
    "                    [3, 6]])'''\n",
    "\n",
    "data_angle = np.random.multivariate_normal(mu, C_angle, size=300)\n",
    "ax.scatter(data_angle[:,0], data_angle[:,1], alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы знаем, теоретические истинные главные компоненты равны собственным векторам ковариационной матрицы. Давайте построим на наших графиках собственные вектора ковариационной матрицы и компоненты, которые нам выдаст в качестве своего ответа алгоритм PCA из библиотеки sklearn.\n",
    "\n",
    "Для поиска собственных значений можно использовать функцию  \n",
    "```python\n",
    "np.linalg.eig(C)\n",
    "```\n",
    "\n",
    "См. https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html\n",
    "\n",
    "Для PCA разложения функцию \n",
    "```python\n",
    "sklearn.decomposition.PCA\n",
    "```\n",
    "\n",
    "**Внимание! Attention! Achtung!** \n",
    "* np.linalg.eig возвращает вторым значением матрицу собственных векторов, в которой собственные вектора расположены по столбцам, т.е. v\\[:, i\\] есть собственный вектор соответсвующий i-му собственному значению.\n",
    "* В то же время в поле класса PCA PCA.components_ лежат вектора главных компонент, которые располагаются там построчно (n_components, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "eigen_values_simple, eigen_vectors_simple = np.linalg.eig(C_simple)\n",
    "eigen_values_angle, eigen_vectors_angle = np.linalg.eig(C_angle)\n",
    "\n",
    "eigen_vectors_simple = eigen_vectors_simple.T\n",
    "eigen_vectors_angle = eigen_vectors_angle.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca_simple = sklearn.decomposition.PCA()\n",
    "pca_simple.fit(data_simple)\n",
    "\n",
    "pca_angle = sklearn.decomposition.PCA()\n",
    "pca_angle.fit(data_angle)\n",
    "\n",
    "# Let's compare it\n",
    "print('simple PCA components:\\n', pca_simple.components_)\n",
    "print('simple true components:\\n', eigen_vectors_simple)\n",
    "print('*' * 80)\n",
    "print('angle PCA components:\\n', pca_angle.components_)\n",
    "print('angle true components:\\n', eigen_vectors_angle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что PCA упорядочивает собственные вектора. Это значит что собственный вектор соответствующий главной компоненте соответственно имеющей максимальную дисперсию будет находиться в первой строке. Этим объясняется разница вот второром случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2 , figsize = (12, 6))\n",
    "\n",
    "ax1.scatter(data_simple[:,0], data_simple[:,1], alpha=0.2)\n",
    "plot_components_vector(ax1, data_simple, eigen_vectors_simple[0], 'g', 'True component 1')\n",
    "plot_components_vector(ax1, data_simple, eigen_vectors_simple[1], 'g', 'True component 2')\n",
    "\n",
    "plot_components_vector(ax1, data_simple, pca_simple.components_[0], 'r', 'PCA component 1')\n",
    "plot_components_vector(ax1, data_simple, pca_simple.components_[1], 'r', 'PCA component 2')\n",
    "\n",
    "s=15; ax1.set(xlim=(-s, s), ylim=(-s, s))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(data_angle[:,0], data_angle[:,1], alpha=0.2)\n",
    "plot_components_vector(ax2, data_angle, eigen_vectors_angle[0], 'g', 'True component 1')\n",
    "plot_components_vector(ax2, data_angle, eigen_vectors_angle[1], 'g', 'True component 2')\n",
    "\n",
    "plot_components_vector(ax2, data_angle, pca_angle.components_[0], 'r', 'PCA component 1')\n",
    "plot_components_vector(ax2, data_angle, pca_angle.components_[1], 'r', 'PCA component 2')\n",
    "ax2.legend()\n",
    "s=15; ax2.set(xlim=(-s, s), ylim=(-s, s))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы видите зеленые компоненты (истинные главные компоненты) полученные разложением ковариационной матрицы на собственные вектора отличаются от красных компонент, которые были полученны вычислением PCA на данных. Это неудивительно, ибо данные были сгенерированы статистически. Если вы будете увеличивать размер сгенерированной выборки, то со временем разрешения экрана станет недостаточно для отображения разницы между PCA и реальными компонентами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика: explained variance ratio\n",
    "\n",
    "В модуле PCA, после fit можно получить explained variance ratio посредством обращения к полю explained_variance_ratio_, а explined_variance посредством обращения к полю explained_variance_. \n",
    "\n",
    "Давайте на практике убедимся, что explained_variance соответсвует собственным значениям, а explained_variance_ratio соответствует долям соответствующих собственных значений от общей суммы собственных значений.\n",
    "\n",
    "Для этого сгенерируем диагональную матрицу $ \\Lambda $ с элементами на диагонале равными 1,2, 3, ... N и произвольную ортогональную матрицу $Q$. Введем новую матрицу $ A = Q \\Lambda Q^T $. Можно доказать, что эта матрица симметрична и положительно определена, т.е. она является матрицей ковариаций. Сгенерируем данные по ней, применим к ней PCA разложение и посмотрим на explained_variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ortho_group\n",
    "\n",
    "dims = 5\n",
    "Q = ortho_group.rvs(dims)\n",
    "# check ortogonality\n",
    "print(np.dot(Q, Q.T).round(8))\n",
    "\n",
    "eig_vals = np.arange(1.0, dims + 1)\n",
    "EV = np.diag(eig_vals)\n",
    "# covariance matrix\n",
    "CM = np.linalg.multi_dot([Q, EV, Q.T])\n",
    "print(CM)\n",
    "\n",
    "# check eigenvalues\n",
    "print(np.linalg.eigvals(CM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.multivariate_normal(np.zeros(dims), CM, size=50000)\n",
    "\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca.fit(data)\n",
    "print('explained_variance_', np.sort(pca.explained_variance_).round(3))\n",
    "print('eigen_values       ', eig_vals)\n",
    "print('explained_variance_ratio_', np.sort(pca.explained_variance_ratio_).round(3))\n",
    "print('eig_vals / sum(eig_vals) ', np.sort(eig_vals/np.sum(eig_vals)).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с Титаником\n",
    "\n",
    "Что бы понять как использовать PCA на практике найдем главные компоненты для датасета Titanic и посмотрим на какое распредилится между ними дисперсия.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_pca = sklearn.decomposition.PCA()\n",
    "titanic_pca.fit(X_train)\n",
    "print(titanic_pca.explained_variance_ratio_)\n",
    "#plt.bar(range(titanic_pca.explained_variance_.shape[0]),titanic_pca.explained_variance_)\n",
    "\n",
    "plt.bar(range(titanic_pca.explained_variance_ratio_.shape[0]),titanic_pca.explained_variance_ratio_)\n",
    "plt.title('Vaiance by components')\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Variance %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графика большая часть дисперсии приходится на первые две компоненты. Можно предположить что остальные могут быть отброшенны без особого ушерба для точности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_first_eigenvectors = titanic_pca.components_[:6]\n",
    "X_train_reduced = np.dot(X_train,two_first_eigenvectors.T)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_reduced,y_train)\n",
    "\n",
    "X_test_reduced = np.dot(X_test,two_first_eigenvectors.T)\n",
    "print(model.score(X_test_reduced,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако в данном случае это не так. Во первых потому что значительная часть данных бинарные. Во вторых вероятно присутствует одна из проблем, которые мы рассмотрим ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с лицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим как PCA применялся для решения практической задачи: распознавания лиц*.\n",
    "\n",
    "Датасет:\n",
    "[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/)\n",
    "\n",
    "\n",
    "* Сейчас для этого используются более эффективные алгоритмы использующие CNN.\n",
    "\n",
    "Загрузим датасет и распакуем его на диск VM Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://conradsanderson.id.au/lfwcrop/ (LFWcrop Face Dataset, greyscale version)\n",
    "!wget http://conradsanderson.id.au/lfwcrop/lfwcrop_grey.zip\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dir='lfwcrop_grey/faces'\n",
    "\n",
    "# http://conradsanderson.id.au/lfwcrop/ (LFWcrop Face Dataset, greyscale version)\n",
    "!wget http://conradsanderson.id.au/lfwcrop/lfwcrop_grey.zip\n",
    "!unzip lfwcrop_grey.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.grid'] = False\n",
    "def show_faces(images, titles, h=64, w=64):\n",
    "    plt.figure(figsize=(16 , 4))\n",
    "    for i in range(min(images.shape[0],5)):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i])\n",
    "        \n",
    "\n",
    "# Get first 1000 files\n",
    "celebrity_photos=os.listdir(dir)[1:1001]\n",
    "celebrity_images=[dir+'/' + photo for photo in celebrity_photos]\n",
    "# Load iages from disk\n",
    "images=np.array([plt.imread(image) for image in celebrity_images], dtype=np.float64)\n",
    "# Extract real celebrity name from file name\n",
    "celebrity_names=[name[:name.find('0')-1].replace(\"_\", \" \") for name in celebrity_photos]\n",
    "print(images[0].shape)\n",
    "show_faces(images, celebrity_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка данных:\n",
    "Преобразуем изображения в вектора и центрируем их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretch to vector\n",
    "X = images.reshape(images.shape[0], 64*64)\n",
    "print(X.shape)\n",
    "mean = np.mean(X, axis=0)\n",
    "# Center: substract mean\n",
    "centered_faces = X-mean\n",
    "plt.imshow(mean.reshape(64, 64), cmap=plt.cm.gray )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем собственные вектора. Аналогия с фотороботом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "#n_components == min(n_samples, n_features)\n",
    "\n",
    "pca_faces = sklearn.decomposition.PCA() #1000x4096\n",
    "\n",
    "pca_faces.fit(centered_faces)\n",
    "eigenfaces = pca_faces.components_\n",
    "reshaped_eigenfaces = eigenfaces.reshape((1000, 64, 64))\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(reshaped_eigenfaces.shape[0])]\n",
    "show_faces(reshaped_eigenfaces, eigenface_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Восстановим лица с использованием n < 4096 компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(img, n_components):\n",
    "  # Generate embedding for first image using only 10 first components\n",
    "  img = img.reshape(64*64) - mean\n",
    "  emb = np.dot(img,eigenfaces[:n_components].T) #(1,4096) * (4096,1) \n",
    "  #print(emb,emb.shape) # 10 - 500 numbers only!\n",
    "\n",
    "  # Recover image from embeding\n",
    "  recovered_img = np.dot(emb,eigenfaces[:n_components]) \n",
    "  recovered_img += mean #shift by mean\n",
    "  return emb, recovered_img\n",
    "\n",
    "# Shome images recovered from embeddings of various sizes \n",
    "original_image = images[0]\n",
    "titles = []\n",
    "img_list = []\n",
    "for n in [10,25,100,500]:\n",
    "  embedding, recovered = create_embedding(original_image, n)\n",
    "  img_list.append(recovered)\n",
    "  titles.append(f\"Components {n}\")\n",
    "img_list.append(original_image)\n",
    "titles.append(\"Original\")\n",
    "\n",
    "show_faces(np.array(img_list), titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распознавание лиц.\n",
    "Теперь сравнивая вектора признаки в пространстве размерности 300 или 500 можно построить систему распознавания лиц.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пратика: проблемы с PCA\n",
    "\n",
    "Рассмотрим случай выборки, которая сгенерирована из двух вытянутых нормальных распределений, чьи основные оси неортогональны друг другу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.array([[10,0],[0,0.1]])\n",
    "# rotate second normal distrbution by 60 degrees\n",
    "phi = np.pi/3\n",
    "rotation = np.array([[np.cos(phi), np.sin(phi)],\n",
    "                     [-np.sin(phi),np.cos(phi)]])\n",
    "\n",
    "data_1 = np.random.multivariate_normal(mu, C1, size=100)\n",
    "data_2 = np.dot(data_1, rotation)\n",
    "# generate data from two not orthogonal distributions\n",
    "data = np.vstack([data_1,\n",
    "                  data_2])\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "# ax.axis('equal')\n",
    "# ax.set_aspect('equal', 'box')\n",
    "ax.scatter(data[:,0], data[:,1])\n",
    "\n",
    "# plot real axis\n",
    "\n",
    "plot_components_vector(ax, data, np.array([1, 0]), color=\"green\", label=\"Ideal component 1\")\n",
    "plot_components_vector(ax, data, [np.cos(phi), np.sin(phi)], color=\"green\", label=\"Ideal component 2\")\n",
    "\n",
    "# plot PCA\n",
    "model = PCA(n_components=2)\n",
    "model.fit(data)\n",
    "W_pca = model.components_\n",
    "\n",
    "plot_components_vector(ax, data, W_pca[0], color=\"red\", label=\"PCA component 1\")\n",
    "plot_components_vector(ax, data, W_pca[1], color=\"red\", label=\"PCA component 2\")\n",
    "s = C1[0, 0] * 1.5\n",
    "ax.set(xlim=(-s, s), ylim=(-s, s))\n",
    "# ax.set_aspect('equal', 'box')\n",
    "plt.legend()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values, eigen_vectors = np.linalg.eig(np.cov(data, rowvar=False))\n",
    "eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика: еще одна проблема с PCA\n",
    "Интересное направление в данных не совпадает с направлением максимальной дисперсии.\n",
    "\n",
    "В примере ниже дисперсии не отражают интересующих нас направлений в данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[0.5,0],[0,10]])\n",
    "mu1 = np.array([-2,0])\n",
    "mu2 = np.array([2,0])\n",
    "\n",
    "data = np.vstack([np.random.multivariate_normal(mu1, C, size=100),\n",
    "                  np.random.multivariate_normal(mu2, C, size=100)])\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "ax.scatter(data[:,0], data[:,1])\n",
    "# обучим модель pca и построим главные компоненты\n",
    "model = PCA(n_components=2)\n",
    "model.fit(data)\n",
    "\n",
    "plot_components_vector(ax, data, model.components_[0], color=\"red\", label=\"PCA component 1\")\n",
    "plot_components_vector(ax, data, model.components_[1], color=\"green\", label=\"PCA component 2\")\n",
    "\n",
    "s = 15; ax.set(xlim=(-s, s), ylim=(-s, s))\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что в данном случае метод главных компонент будет считать вертикальную компоненту более значимой для описания набора данных, чем горизонтальную. \n",
    "\n",
    "Но, например, в случае, когда данные из левого и правого кластера относятся к разным классам, для их линейной разделимости вертикальная компонента является шумовой. Несмотря на это, её метод главных компонент никогда шумовой не признает, и есть вероятность, что отбор признаков с его помощью выкинет из ваших данных значимые для решаемой вами задачи компоненты просто потому, что вдоль них значения имеют низкую дисперсию.\n",
    "\n",
    "Справляться с такими ситуациями могут некоторые другие методы уменьшения размерности данных, например, метод независимых компонент (Independent Component Analysis, ICA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Недостатки линейного PCA\n",
    "\n",
    "Как мы увидели в предыдущих примерах, обычный PCA далеко не всегда работает хорошо. В частности, могут быть ситуации, когда построенная PCA проекция не дает хорошего разбиения объектов на группы. Для набора картинок с написанными от руки цифрами  MNIST, PCA даст такой результат:\n",
    "\n",
    "Также бывают ситуации, когда оптимально спроецировать не на некоторую плоскость, а на многообразие (кривая плосоксть), как показано на картинке ниже.\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/S-manifold.png\" width=600/>\n",
    "\n",
    "\n",
    "В данном случае оптимально спроецировать на S-образную кривую. \n",
    "\n",
    "В связи с вышеописанными случаями, ниже мы рассмотрим более сильные методы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA Ядровой (нелинейный) метод главных компонент\n",
    "\n",
    "Как уже упомяналось, иногда невозможно захватить всю информацию линейной проекцией, хотя кривая поверхность с такой же размерностью это позволяет сделать. Одним из подходов к решению данной проблемы является задача перевода признаков в нелинейное пространство. \n",
    "\n",
    "### Kernel trick\n",
    "\n",
    "\n",
    "Kernel Trick избегает явного отображения, которое нужно для получения линейного обучающего алгоритма для нелинейной функции или границы решений. Для всех $\\mathbf{x} \\ и \\ \\mathbf{{x}'} $ во входном пространстве $ \\mathcal {X} $ некоторые функции $ k(\\mathbf {x} ,\\mathbf {x'}) \\colon   \\mathcal {X} \\times \\mathcal {X}\\to \\mathbb {R}$ )  могут быть представлены как скалярное произведение в другом пространстве $ \\mathcal  {V} $. \n",
    "То есть, $ k(\\mathbf {x} ,\\mathbf {x'}) = < \\phi(\\mathbf{x}), \\phi(\\mathbf{x'})>$, где $ \\phi \\colon {\\mathcal {X}}\\to {\\mathcal {V}} $, некоторая функция перевода функций из одного пространства в другое.\n",
    "\n",
    "\n",
    "Функцию $k(\\mathbf {x} ,\\mathbf {x'})$ часто называют *ядром или ядерной функцией (kernel, kernel function)*, а пространство V - *спрямляющим пространством*.\n",
    "\n",
    "\n",
    "\n",
    "Картинка с проекцией в более высокоразмерное пространство. \n",
    "\n",
    "После этого мы можем использовать обычный линейный PCA, но в высокоразмерном пространстве.\n",
    "\n",
    "В новом более высокоразмерном пространстве нам не нужно делать никаких вычислений (нужно вычислять только скалярные произведения).\n",
    "\n",
    "https://www.youtube.com/watch?v=HbDHohXPLnU&ab_channel=caltech\n",
    "\n",
    "\n",
    "Пример с разделением MNIST\n",
    "Пример с разделением radial kernel\n",
    "\n",
    "PCA довольно чувствителен к выбору ядра.\n",
    "\n",
    "К примеру, для данных, расположенных на трех окружностях:\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_PCA-3-circles.png\" width=260/>\n",
    "\n",
    "\n",
    "в зависимости от выбора ядра мы будем получать совершенно разные отображение в спрямляющее пространство:\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_PCA-kernels.png\" width=600/>\n",
    "\n",
    "\n",
    "Упомянуть про размерности\n",
    "\n",
    "Типы ядер:\n",
    "Картинка из ESL (p 549)\n",
    "Гауссово (см видео)\n",
    "\n",
    "* $k(x_i, x_j) = \\frac{1}{z} e^{-\\frac{h(x_i, x_j)^2}{h}}$ - радиальная базисная функция RBF\n",
    "* $k(x_i, x_j) = (<x_i, x_j> + c)^d, с, d \\in \\mathbb{R}$ - полиномиальное ядро\n",
    "* $k(x_i, x_j) = \\sigma((x_i, x_j>)$ - ядро с функцией активации\n",
    "\n",
    "Интуиция: точки, лежащие рядом друг с другом в исходном пространстве, должны иметь большие скалярные произведения\n",
    "(картинка с Гауссианой).\n",
    "\n",
    "\n",
    "В целом, нелинейные методы требуют больше данных как более гибкие алгоритмы. Поэтому лучше начинать с линейных и переходить к нелинейным только тогда, когда они действительно необходимы.\n",
    "\n",
    "Примеры\n",
    "https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие методы\n",
    "\n",
    "Существуют и другие, более сильные методы понижения размерности в данных. К примеру, t-SNE и UMAP. Мы не будем на них подробно останавливаться. Просто сравните картинки, проекций MNIST, полученные с помощью PCA, t-SNE и UMAP. А также сравните время работы, затраченное на каждый из этих методов.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "\n",
    "Идея состоит не в том что бы напрямую максимизировать дисперсиию, а найти такое пространство в котором расстояние между объектами будет сохраняться или по крайне мерее не сильно менятся.\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_manifold.jpg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_t-SNE.png\" >\n",
    "\n",
    "\n",
    "http://datareview.info/article/algoritm-t-sne-illyustrirovannyiy-vvodnyiy-kurs/\n",
    "\n",
    " При уменьшении размерности набора данных, если использовать гауссово распределение для точек данных и точек отображения, мы получим дисбаланс в распределении расстояний для соседей точек. Это объясняется тем, что распределение расстояний существенно отличается для пространства большой размерности и для пространства малой размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "UMAP — uniform manifold approximation and projection\n",
    "https://www.youtube.com/watch?v=94ZMJ8tq1Wk\n",
    "https://umap-learn.readthedocs.io/en/latest/how_umap_works.html\n",
    "\n",
    "\n",
    "Local connection assumption:\n",
    "\n",
    " \n",
    "Картинки с примерами разделений плоскости на PCA, t-SNE, UMAP\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/L04_MNIST.jpg\" >\n",
    "\n",
    "\n",
    "UMAP builds mathematical theory to justify the graph based approach\n",
    "\n",
    "UMAP understands the distance is involved we have obtained all the internal structure of these clusters and the global structure as well (пример test и trin set UMAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE vs UMAP\n",
    "\n",
    "As the size of a dataset increases the runtime of a given dimension reduction algorithm will increase at varying rates. If you ever want to run your algorithm on larger datasets you will care not just about the comparative runtime on a single small dataset, but how the performance scales out as you move to larger datasets. We can similate this by subsampling from MNIST digits (via scikit-learn’s convenient resample utility) and looking at the runtime for varying sized subsamples. Since there is some randomness involved here (both in the subsample selection, and in some of the algorithms which have stochastic aspects) we will want to run a few examples for each dataset size. We can easily package all of this up in a simple function that will return a convenient pandas dataframe of dataset sizes and runtimes given an algorithm.\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L04_Feature_Engineering/img/UMAP-TSNE-performance.png\" width=450/>\n",
    "\n",
    "Here we see UMAP’s advantages over t-SNE really coming to the forefront. While UMAP is clearly slower than PCA, its scaling performance is dramatically better than MulticoreTSNE, and, despite the impressive scaling performance of openTSNE, UMAP continues to outperform it. Based on the slopes of the lines, for even larger datasets the difference between UMAP and t-SNE is only going to grow.\n",
    "\n",
    "This concludes our look at scaling by dataset size. The short summary is that PCA is far and away the fastest option, but you are potentially giving up a lot for that speed. UMAP, while not competitive with PCA, is clearly the next best option in terms of performance among the implementations explored here. Given the quality of results that UMAP can provide we feel it is clearly a good option for dimension reduction.\n",
    "\n",
    "\n",
    "Более подробно: https://umap-learn.readthedocs.io/en/latest/performance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, datasets\n",
    "from matplotlib import offsetbox\n",
    "\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1., 1.]])  # just something big\n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "                X[i])\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE embedding of the digits dataset\n",
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plot_embedding(X_tsne, \"t-SNE embedding of the digits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap = umap.UMAP(n_neighbors=5)\n",
    "X_umap = umap.fit_transform(X) # преобразовываем\n",
    "plot_embedding(X_umap, \"UMAP embedding of the digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация\n",
    "\n",
    "Оба метода применяются в основном не для предобработки данных, а для визуализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy\n",
    "import torchvision\n",
    "\n",
    "mnist = torchvision.datasets.MNIST('mnist', train = False, download = True)\n",
    "#train_set.datanumpy()\n",
    "print(mnist.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательный метод для запуска Tensorboard в Colab\n",
    "\n",
    "# Fix https://stackoverflow.com/questions/60730544/tensorboard-colab-tensorflow-api-v1-io-gfile-has-no-attribute-get-filesystem\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "import os\n",
    "import torch\n",
    "# Запуск Tensorboard в Colab\n",
    "def reinit_tensorboard(clear_log = True):\n",
    "  # Лог-файлы читаются из этого каталога: \n",
    "  logs_base_dir = \"runs\"\n",
    "  if clear_log:\n",
    "    # Очистка логов\n",
    "    !rm -rfv {logs_base_dir}/*\n",
    "    os.makedirs(logs_base_dir, exist_ok=True)\n",
    "  # Магия Colab\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir {logs_base_dir}\n",
    "\n",
    "#, metadata=mnist.targets\n",
    "reinit_tensorboard()\n",
    "mnist_chunk = mnist.data[:1000]\n",
    "writer = SummaryWriter(comment = \"mnist\")\n",
    "writer.add_embedding(torch.reshape(mnist_chunk,(-1,28*28)),mnist.targets[:1000])\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "colab": {
   "name": "L04_Feature_Engineering_gan.ipynb",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
