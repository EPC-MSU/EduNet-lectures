{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточные нейронные сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полносвязная нейронная сеть \n",
    "Fully-connected Neural Network (FCN). В современных статьях чаще используется термин Multilayer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии мы рассмотрели следующий подход для классификации изображений:\n",
    "\n",
    "1. Превращаем исходные данные в вектор. \n",
    "\n",
    "***Примечание***: при обработке цветного изображения размером $32\\times32$ пикселя ($32\\times32\\times3$), размерность входного вектора будет равна $3072$. Однако, в общем случае модель получает на вход набор одноразмерных векторов (матрицу), потому при обработке одного изображения размер матрицы будет $3072\\times1$.\n",
    "\n",
    "2. Перемножаем матрицу данных с матрицей весов. Размер последней может быть, например, $100\\times3072$. Где $3072$ - размер входного вектора, а $100$ - количество признаков, которое мы хотим получить. Результат обработки одного изображения будет иметь размер $100\\times1$.\n",
    "\n",
    "3. Поэлементно применяем к полученной матрице нелинейную функцию (функцию активации), например Sigmoid или ReLu. Размерность данных при этом не меняется ($100\\times1$). В результате получаем вектор активаций или признаков.\n",
    "\n",
    "4. Используем полученные признаки как входные данные для нового слоя. Количество весов слоя будет зависеть от размерности входной матрицы и того, что мы хотим получить на выходе. Если мы делаем классификатор на $10$ классов, то матрица весов должна иметь размерность $10\\times100$, и на этом можно остановиться. Но в общем случае количество слоев может быть произвольным.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_1.png\" width=\"700\">\n",
    "\n",
    "\n",
    "На изображении представлена описанная выше нейронная сеть, функцией активации в которой является ReLu. Добавление второго слоя позволило модели использовать более одного шаблона на класс. Можно убедиться в этом, обучив модель на датасете CIFAR-10 и визуализировав веса первого слоя модели.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/mlp-templates.png\" width=\"400\">\n",
    "\n",
    "Засчёт создания нескольких шаблонов для каждого из классов, многослойные архитектуры в общем случае показывают более высокую, чем перцептроны, эффективность на задачах классификации изображений. Однако подход с использованием многослойного перцептрона также имеет свои недостатки. Чтобы обнаружить их, присмотримся к происходящему с данными при предобработке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нарушение связей между соседними пикселями\n",
    "\n",
    "Единственным этапом предобработки данных при использовании многослойного перцептрона является превращение изображения в вектор. Основной его проблемой является \"потеря связи\" между соседними пикселями, что показано на изображении ниже. В общем случае, значения из соседних пикселей могут оказаться на большом расстоянии внутри результирующего вектора. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_9.png\" width=\"700\">\n",
    "\n",
    "Словом, после преобразования не получает информацию о взаимном расположении значений на исходном изображении - при подборе весов никак не используется информация о том, что какие-то значения были \"близки\" друг к другу на исходном изображении. При этом мы, люди, понимаем важность взаимосвязей между пикселями.  \n",
    "\n",
    "К примеру, набор тёмных пикселей находящихся поблизости внутри изображения может сообщить нам, что там располагается некий тёмный объект. Если же мы преобразуем изображение в вектор и эти значения \"разбросает\" по нему без сохранения пространственной структуры, то новому наблюдателю будет очень сложно понять, что где-то на исходном изображении была тёмная зона.  \n",
    "\n",
    "Словом, передача нейросети информации о взаимном расположении пикселей может быть полезна. Чтобы понять, каким образом это можно сделать, рассмотрим как мозг обрабатывает визуальную информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рецептивные поля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе зрительного восприятия, человек обрабатывает информацию о свете, попадающем на сетчатку глаза. Фоторецепторы сетчатки (палочки и колбочки) передают информацию в более сложные клетки - биполярные. При этом, биполярные клетки получают информацию из располагающихся рядом рецепторных клеток, таким образом обрабатывая локальную информацию. На следующих уровнях обработки информации происходит аналогичная картина - клетки уровнем выше собирают информацию из нескольких более простых близко расположенных клеток. \n",
    "\n",
    "В результате, каждая из сложных клеток обрабатывают информацию с нескольких простых рецепторов расположенных рядом и возбуждается при определённой комбинации сигналов от них. Область сетчатки глаза, информация из которой попадает на нервную клетку, называется рецептивным полем этой клетки.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_10.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем импульсы от зрительных клеток последовательно обрабатываются различными отделами мозга - таламусом, зрительной корой и далее, после чего извлечённая информация используется человеком для принятия решений. Заметим, что при принятии решений мы можем использовать всю информацию о том, что видим, а также визуально распознавать такие сложные формы, как лица или объекты. Обе этих возможности существуют благодаря иерархической структуре обработке информации: от отдела к отделу мозгу рецептивные поля клеток увеличиваются, как и распознаваемые зрительные формы (от простейших - распознавания краёв и углов, до таких сложных, как лица).\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hubel & Wiesel,1959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Ice Cube Model](https://docplayer.ru/79903347-Kasyanov-evgeniy-dmitrievich-sankt-peterburg-nmic-pn-im-v-m-beh-k-m-n-fedotov-ilya-andreevich-ryazan-ryazgmu.html)\n",
    "\n",
    "Эта гипотетическая кубическая модель придумана для пояснения устройства клеток первичной визуальной коры, а именно  – как устроены предпочитаемые ориентации и, соответственно, реакции нейронов V1. Так, V1 можно условно поделить на кубы $2 мм^3$, каждый из которых получает сигналы от обоих глаз. Клетки с одинаковыми ориентационными предпочтениями формируют горизонтальные колонки, при этом соседние вертикальные колонки имеют слегка отличающиеся ориентационные предпочтения.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_14.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чувствительные к цветам клетки также собраны в столбцы (также их называют каплями, гиперколонками, шариками) 0,5 мм в диаметре в зонах соответствующих превалирующих глаз (картинка с цилиндрами). Каждый такой столбец содержит реагирующие либо на красно-зеленый, либо на сине-желтый контрасты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Свертка с фильтром\n",
    "\n",
    "\n",
    "Как реализовать идею на компьютере?\n",
    "\n",
    "Для этого испольльзуются так называемые фильтры.\n",
    "\n",
    "Допустим нам нужно найти на изображении некий объект (например рука).\n",
    "\n",
    "И у нас есть изображение этого объекта.\n",
    "\n",
    "Тогда мы можем накладывать прямоугольник с изображением интересующего нас объекта на изображение, во всех возможных положениях. В какой-то момент шаблон полностью совпадет с частью рисунка.\n",
    "\n",
    "\n",
    "Как определить что произошло совпадение?\n",
    "\n",
    "В простейшем варианте можно было бы просто посчитать попиксельную разницу, и сложить результат как это делалось в KNN.\n",
    "\n",
    "Тогда 0- соответствовал бы максимальному отклику.\n",
    "\n",
    "Однако на практике используют умножение. Благодаря этому, чем больше соответствует фрагмент изображения шаблону тем больше будет отклик.\n",
    "\n",
    "Это операция называется **\"свертка с фильтром\"**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-2.gif\" width=\"300\">\n",
    "\n",
    "*В демонстрации, зеленая секция это входное изображение 5x5. Желтая матрица это фильтр, или ядро фильтра 3x3.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # placeholder dor demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры 'hancrafted' фильтров\n",
    "\n",
    "Сами коэффициенты фильтра можно подбирать вручную. \n",
    "Многие известные фильтры подобранны таким образом. \n",
    "\n",
    "И использовать их можно не только для поиска объектов.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтр Гаусса\n",
    "\n",
    "Применяется для размытия изображений, он есть в большинстве фоторедакторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from skimage import data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = data.chelsea().mean(axis = 2).astype('int32')\n",
    "\n",
    "def im_show(arr):\n",
    "  plt.axis('off')\n",
    "  plt.imshow(arr, cmap='gray', vmin=0, vmax=255)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "im_show(cat)\n",
    "\n",
    "g_kernel5 = torch.tensor([\n",
    "                     [1,4,7,4,1],\n",
    "                     [4,16,26,16,4],\n",
    "                     [7,26,41,26,7],\n",
    "                     [4,16,26,16,4],\n",
    "                     [1,4,7,4,1]]\n",
    "                     ,dtype=torch.float) / 273 # sum of weights\n",
    "\n",
    "gaussian_filter = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=g_kernel5.shape, bias=False)\n",
    "gaussian_filter.weight.data = g_kernel5.unsqueeze(0).unsqueeze(0) #gaussian_kernel\n",
    "gaussian_filter.weight.requires_grad = False\n",
    "input_data = torch.tensor(cat).view(1,1,cat.shape[0],cat.shape[1]).type(torch.float)\n",
    "out = gaussian_filter(input_data)\n",
    "im_show(out[0, 0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтр Собеля\n",
    "\n",
    "На заре компьютерного зрения люди придумывали различные операции, которые будут находить важные элементы на изображении. К примеру, границы объекта на изображении. Примером подобной операции является фильтр Собеля. Он у Вас будет в практической работе.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/sobel_example.jpg\" width=\"800\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/sobel.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Дополнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Детектор границ Canny\n",
    "\n",
    "На самом деле, если мы будем применять данный алгоритм без подготовки изображения и дополнительных операций, то не получим хорошее качество детекции границ. Поэтому существуют детекторы границ, которые дополняют оператор Собеля (в алгоритме ниже оператор Собеля используется на шаге 2).\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/canny.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Детекторы углов\n",
    "\n",
    "Другими примерами подобных вручную созданных детекторов признаков являются всевозможные детекторы углов. Одним из самых известных среди них является детектор углов Харриса. Мы не будем на нем останавливаться, но вы может о нем прочесть самостоятельно.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Harris_Corner_Detector\n",
    "\n",
    "\n",
    "Как Вы увидите далее, сверточные нейронные сети, по факту автоматизируют процесс нахождения подобных признаков, высвобождая время исследователей и инженеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемещая различные ядра свертки по изображению, можно выполнять манипуляции с изображениями. Вот как работают многие инструменты, которые можно найти в программах для редактирования изображений, таких как Photoshop или Gimp:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Viola Jones ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточный слой нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим использовать фильтры в нашей модели. При хотим что бы сеть сама научилась подбирать коэффициенты ядер фильтров. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_9.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо вектора весов размером с целое изображение используем квадратную матрицу небольшого размера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/filter1.png\" width=\"700\">\n",
    "\n",
    "В результате получим число, которое будет характеризовать отклик на данный фильтр. \n",
    "\n",
    "Операция свертки является сводится к скалярному произведению, поэтому является линейной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Цветное/многоканальное изображение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если речь идет о цветном изображении, то входной тензор будет иметь измерение соответствующее числу цветовых каналов.\n",
    "\n",
    "$С\\times H\\times W$.\n",
    "\n",
    "Для RGB изображений число каналов равно $3$.\n",
    "\n",
    "CIFAR-10 изображения храняться в тензорах размером $3\\times32\\times32$.\n",
    "\n",
    "Тогда и ядро фильтра должно получить дополнительное измерение соответствующее числу каналов. \n",
    "\n",
    "Например матрица весов для фильтра $3\\times3$ будет иметь размерность $3\\times3\\times3$.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/conv1.png\" width=\"300\">\n",
    "\n",
    "Результатом свертки входного тензора с одним фильтром фильтром будет карта активации с глубиной $1$, вне зависимости от количества каналов.\n",
    "\n",
    "*При этом входной тензор не обязательно должен формироаться из пикселей изображения, а может быть выходом предидущего сверточнойго слоя. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_15-1.png\" width=\"200\">\n",
    "\n",
    "\n",
    "Во время прямого прохода мы смещаем фильтр по ширине и высоте входного представления и вычисляем скалярное произведение между значениями фильтра и подлежащими значениями входного представления в любой точке.\n",
    "\n",
    "В процессе перемещения фильтра формируеется 2х мерная карта активации, которая содержит результат применения данного фильтра к каждой из областей входного представления.\n",
    "\n",
    "В базовом случае фильтр смещается с шагов 1 но это - параметр который может меняться.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_15.png\" width=\"700\">\n",
    "\n",
    "Так же можно заметить что даже при использовании единичного шага ширина $W_{out}$ и высота $H_{out}$ **карты активаций** будет отличаться от **пространственных размерностей** исходного тензора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do it manually\n",
    "\n",
    "A = np.arange(9).reshape(3,3) # our matrix\n",
    "C = np.array([[-1, 0], [0, 1]]) # filter\n",
    "\n",
    "res = [[np.sum(A[:2, :2] * C), np.sum(A[:2, 1:3] * C)],\n",
    "       [np.sum(A[1:3, :2] * C), np.sum(A[1:3, 1:3] * C)]]\n",
    "\n",
    "print(\"Input: \\n\",A)\n",
    "print(\"Filter: \\n\",C)\n",
    "print(\"Result: \\n\",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фильтров может быть несколько.\n",
    "\n",
    "В результате обучения каждый должен обучиться реагировать на определенный паттерн во входном тензоре\n",
    "\n",
    "Применения каждый даст свою карту активации размерностью $1\\times H_{out}\\times W_{out}$\n",
    "\n",
    "Если объединить (стековать) эти карты между собой то получим тензор размерности $K\\times H_{out} \\times W_{out}$\n",
    "\n",
    "\n",
    "Где K - количество фильтров.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/depth.jpeg\" width=\"400\">\n",
    "\n",
    "\n",
    "На изображении результат приминения к изображению из CIFAR-10 сверточного слоя содержащего 5 фильтров.\n",
    "\n",
    "В документации Pytorch вместо K используется обозначение $С_{out}$ \n",
    "\n",
    "То есть результат применения фильтра трактуется аналогично цветовому каналу на самом первом слое.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим количество праметров и количество операций ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверточный слой = Сверка + активация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате прохода по изображению мы получаем карту активации, также известную как карта признаков. Этот процесс называется пространственной свёрткой\n",
    "\n",
    "К изображению можно применять множество фильтров и получать на выходе разные карты активации.\n",
    "\n",
    "\n",
    "По тем же соображениям что и для линейного слоя, на для каждого фильтра обучается еще обин коэффициент отвечающий за смежение (bias)\n",
    "\n",
    "\n",
    "При этом сама операция свертки остается линейной операцией, и к выходу сверточного слоя применяется функция активации (например ReLu).\n",
    "\n",
    "*Так как функция активации применяется поэлементно, не важно какой именно слой находился передней ней полносвязанный или сверточный. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "                           nn.Conv2d(3,6,3),\n",
    "                           nn.ReLU(), # Activation doesn't depend on input shape\n",
    "                           nn.Flatten(),\n",
    "                           nn.Linear(5400,100), # H = 1 + (32 - 3) / 1 \n",
    "                           nn.ReLU(),  # Activation doesn't depend on input shape\n",
    "                           nn.Linear(100,10)\n",
    "                           )\n",
    "dummy_input = torch.randn((1,3,32,32))\n",
    "out = net(dummy_input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение сверточного и полносвязанного слоев \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/conv_layer.png\" width=\"700\">\n",
    "\n",
    "- Сколько весов у такого слоя?\n",
    "- А сколько операций умножения потребуется сделать при прямом проходе?\n",
    "\n",
    "А теперь посчитаем эти же парметры для полносвязанной сети - сети?\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_1.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Память и быстродействия\n",
    "\n",
    "**Fully-connected Neural Network**\n",
    "3 слоя  \n",
    "$3\\times32\\times32 = 3072$  \n",
    "$3072\\to512$  \n",
    "$512\\to128$  \n",
    "$128\\to10$  \n",
    "\n",
    "$W = (3072 + 1) \\cdot 512 + (512 + 1) \\cdot 128 + (128 + 1) \\cdot 10 = 1\\,573\\,376 + 65\\,664 + 1290 = 1\\,640\\,330 \\approx 6.26 \\text{Mb}$ (10% от датасета CIFAR10!)  \n",
    "$\\text{FLOPS} = 3072 \\cdot 512 + 512 \\cdot 128 + 128 \\cdot 10 = 1\\,572\\,864 + 65\\,536 + 1280 = 1\\,639\\,680 \\approx 1.64e6$\n",
    "\n",
    "**Convolution Neural Network**\n",
    "4 слоя\n",
    "\n",
    "$3\\times32\\times32 * 10\\times3\\times3 \\to 10\\times32\\times32$  \n",
    "$10\\times32\\times32 * 128\\times3\\times3 \\to 128\\times32\\times32$  \n",
    "$128\\times32\\times32 * 512\\times3\\times3 \\to 512\\times32\\times32$  \n",
    "$512\\times32\\times32 = 524288$  \n",
    "$524288\\to10$\n",
    "\n",
    "$W = (3\\cdot3\\cdot3 + 1) \\cdot 10 + (10\\cdot3\\cdot3 + 1) \\cdot 128 + (128\\cdot3\\cdot3 + 1) \\cdot 512 + 524288 \\cdot 10 = 280 + 11\\,648 + 590\\,336 + 5\\,242\\,880 = 5\\,845\\,144 \\approx 22.30 \\text{Mb}$ \n",
    "\n",
    "$\\text{FLOPS} = 30\\cdot30\\cdot(3\\cdot3\\cdot(3\\cdot10 + 10\\cdot128 + 128\\cdot512)) + 524\\,288 \\cdot 10 = 900 \\cdot 9 \\cdot (30 + 1280 + 65\\,536) + 5\\,242\\,880 = 546\\,695\\,480 \\approx 5.47e8$\n",
    "\n",
    "\n",
    "Свертки содержат значительно меньшее количество обучаемых параметров (не похоже). Но требуют больше вычислительных ресурсов (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Параметры сверточного слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные параметры операции свертки слоя это:\n",
    "\n",
    "- Размер ядра фильтра ($F$)\n",
    "- Шаг ($\\text{stride}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_18.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность выходных данных напрямую зависит от этих параметров следующего слоя.\n",
    "\n",
    "Для тензоров у которых высота равна ширине ($H = W = N$)\n",
    "\n",
    "Вичислить пространственне размеры выхода можно по формуле:\n",
    "\n",
    "$$ N_{new} = 1 + \\frac{N - F}{\\text{stride}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_19.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "При некоторых комбинациях этих двух параметров часть данных может не попасть в свертку. \n",
    "\n",
    "$N = 7, F = 3$\n",
    "\n",
    "$\\displaystyle \\text{stride }1 \\implies \\frac{7 - 3}{1} + 1 = 5$ - ОК  \n",
    "\n",
    "$\\displaystyle \\text{stride }2 \\implies \\frac{7 - 3}{2} + 1 = 3$ - ОК\n",
    "\n",
    "stride 3 => (7 - 3)/3 + 1 = **2.33**  \n",
    "\n",
    "В данном случае часть изображения в свертку не попадет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create torch tensor 7x7\n",
    "input = torch.tensor([[[[1,1,1,1,1,1,99],\n",
    "                     [1,1,1,1,1,1,99],\n",
    "                     [1,1,1,1,1,1,99],\n",
    "                     [1,1,1,1,1,1,99],\n",
    "                     [1,1,1,1,1,1,99],\n",
    "                     [1,1,1,1,1,1,99]]]],dtype=torch.float)\n",
    "print(input)\n",
    "conv = torch.nn.Conv2d(in_channels=1, # Number of channels\n",
    "                       out_channels=1, # Number of filters\n",
    "                       kernel_size=3, \n",
    "                       stride = 3,\n",
    "                       bias = False # Don't use bias\n",
    "                       )\n",
    "print(conv.weight.shape) # shape = [1,1,3,3] batch, channels, H, W\n",
    "conv.weight  = torch.nn.Parameter(torch.ones((1,1,3,3))) # Replace randow weights to ones\n",
    "activation = conv(input) \n",
    "print(activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество фильтров\n",
    "\n",
    "Задача свертки научиться разпознавать паттерны которые будут часто встречаться на изображениях в нашем датасете.\n",
    "\n",
    "Поэтому в слой свертки добавляют не один а несколько фильтров.\n",
    "\n",
    "От количества фильтров будет зависеть количество каналов в выходном тензоре.\n",
    "\n",
    "Поэтому в классе Pytorch он на зывается out_channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv2d(in_channels=3, # Number of input channels (3 for RGB images)\n",
    "                       out_channels=64, # Number of filters\n",
    "                       kernel_size=3)\n",
    "\n",
    "dummy_input = torch.randn((1,3,32,32))\n",
    "out = conv(dummy_input)\n",
    "print(out.shape) # [1, 64, 30, 30] batch, C_out, H_out, W_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение размера за счет краев\n",
    "\n",
    "Как можно видеть из примера, при каждой операции свертки, пространственые размеры выхода уменьшаются.\n",
    "\n",
    "\n",
    "В результате этого пиксели, которые находились на границе изображения участвуют в меньшем количестве сверток, чем те что были ближе к центру. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_20.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расширение (padding)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы избавиться от этого эффекта используется дополнение входного тензора (англ. padding). Обозначается как P.\n",
    "\n",
    " Выходы с предыдущего слоя дополняются так, чтобы после свертки размер выходного тензора совпадал с входным.\n",
    "\n",
    " Такие свертки называют одинаковыми (англ. same convolution), а свертки без дополнения изображения называются правильными (англ. valid convolution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_21.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы добиться эффекта сохранения размера, при единичном шаге (stride == 1)\n",
    "\n",
    "padding (P) можно рассчитать по формуле \n",
    "\n",
    "P = (F-1)/2\n",
    "\n",
    "\n",
    "*F - размер фильтра*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Среди способов, которыми можно заполнить новые пиксели, можно выделить следующие:\n",
    "\n",
    "zero shift: 00[ABC]00;\n",
    "\n",
    "border extension: AA[ABC]CC;\n",
    "\n",
    "mirror shift: BA[ABC]CB;\n",
    "\n",
    "cyclic shift: BC[ABC]AB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input.shape,input)\n",
    "conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride = 1,padding = int((3-1)/2),padding_mode='zeros', bias = False)\n",
    "conv.weight  = torch.nn.Parameter(torch.ones((1,1,3,3))) # Replace random weights to ones one filter, one channel,  with 3x3 kernel \n",
    "activation = conv(input) \n",
    "print(activation.shape,activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Список параметров\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/conv_sum.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-3.gif\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Операция свертки в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_23.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/conv2_2.png\" width=\"700\">\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/conv2_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение сверточных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество слоев и размер фильтров\n",
    "\n",
    "* Сколько сверточных слоев следует добавлять в сеть?\n",
    "* Какой размер ядра фильтров использовать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если проводить аналогию я работой мозга, то фильтры на первых слоях сети должны реагировать на простые объекты занимающую малуч часть изображения. Или иметь малое рецептивное поле.\n",
    "\n",
    "\n",
    "На последних же слоях фильтры должны реагировать на признаки описывающие целые объекты, возможно занимающие все большую часть исходного изображения.\n",
    "\n",
    "Сответственно рецептивное поле фильтров на  последних слоях сети должно быть большим, сопоставимым с размером исходного изображения.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/r-field.png\" width=\"700\">\n",
    "\n",
    "Увеличить рецептивное поле можно двумя способами:\n",
    "\n",
    "* увеличивать размер ядра фильтра\n",
    "* Увеличивать количество слоев (см. схему)\n",
    "\n",
    "\n",
    "Второй случай требует большого количества слоев.\n",
    "И что бы уменьшить вычислительную сложность размерности входов   внутренних слоев последовательно уменьшают.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уменьшение размеров слоев \n",
    "\n",
    "- <font color=red >Не использовать расширение (padding)</font> \n",
    "- <font color=black >Увеличить шаг</font> \n",
    "- <font color=green >Свертка с фильтром 1x1</font> \n",
    "- <font color=green >Уплотнение (Субдискретизация, Pooling)</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Увеличение шага свертки\n",
    "\n",
    "Можно увеличить шаг свертки. Напрмер что бу уменьшить размер в две раза можно применить свертку с ядром 2x2 и шагом 2. \n",
    "Этот подход используется довольно часто. \n",
    "\n",
    "\n",
    "Но если задача только в уменьшении количества данных, то от требует дополнительных резурсов и памяти. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "conv = torch.nn.Conv2d(in_channels=3, # Number of input channels\n",
    "                       out_channels=64, # Number of filters\n",
    "                       kernel_size=2,\n",
    "                       stride = 2)\n",
    "\n",
    "dummy_input = torch.randn((1,3,32,32))\n",
    "out = conv(dummy_input)\n",
    "print(\"Input shape\",dummy_input.shape)\n",
    "print(\"Shape after conv with stride 2\",out.shape) # [1, 64, 16, 16] batch, C_out, H_out, W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Уплотнение (Субдискретизация, Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача такая же как при сжатии обычых изображений: \n",
    "\n",
    "уменьшить объем данных, потеряв как можно меньше значимой информаци.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_24_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "И способы используются аналогичные:\n",
    "\n",
    "**Max - pooling**\n",
    "Входной тензор делится на сегменты (например 2x2) в каждом канале каждого сегмента, выбирается максимальное значение.\n",
    "Из этих значение составляется выходной тензор.\n",
    "\n",
    "**Average pooling**\n",
    "Аналогично за тем исключением что вместо максимального используется среднее.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_25.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Можно не делить изображение на сегменты, а а применять к нему pooling - фильтр, c шагом больше 1-цы.\n",
    "\n",
    "Именно так операция рализуется в коде.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input = torch.tensor([[[\n",
    "                     [1,1,2,4],\n",
    "                     [5,6,7,8],\n",
    "                     [3,2,1,0],\n",
    "                     [1,2,3,4],\n",
    "                     ]]],dtype=torch.float)\n",
    "\n",
    "max_pool = torch.nn.MaxPool2d(2, stride=2)\n",
    "avg_pool = torch.nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "print(\"Max pooling\",max_pool(input))\n",
    "print(\"Average pooling\",avg_pool(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сответственно у слоя пулинга появляеются гиперпараметры:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/pool_sum.png\" width=\"700\">\n",
    "\n",
    "\n",
    "В отличие от сверточного слоя слой пулинга не содержит обучаемых параметров и работает существенно быстрее.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Свертка с фильтром 1x1\n",
    "\n",
    "Операция пулинга не влияет на количество каналов. А в его уплотнении тоже возникает потребность. Для этого применяют свертку размером 1x1.  \n",
    "\n",
    "Технически это обычный сверточный слой у которого размер ядра фильтра равен 1 и шаг равен 1. А количество выходных каналов задается количеством фильтров.\n",
    "\n",
    "Применение таких сверток позволяетпроизвольным образом менять количество каналов.\n",
    "\n",
    "\n",
    "*Фактически применение свертки 1x1 эквивалентно приминению линейного слоя к каждому из каналов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_24.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "conv = torch.nn.Conv2d(in_channels=64, # Number of input channels\n",
    "                       out_channels=32, # Number of filters\n",
    "                       kernel_size=1)\n",
    "\n",
    "dummy_input = torch.randn((1,64,56,56))\n",
    "out = conv(dummy_input)\n",
    "print(\"Input shape\",dummy_input.shape)\n",
    "print(\"Shape after 1x1 conv\",out.shape) # [1, 64, 56, 56] batch, C_out, H_out, W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Другие виды сверток\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свертка - это математическая операция, которая объединяет две функции в третью. Например, допустим, что у нас есть две данные функции, **е(т)** а также **г(т)** и мы заинтересованы в применении одного поверх другого и вычислении площади пересечения:**f (t) * g (t) = (f * g) (t)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае мы применяем **г(т)**(называется ядром) более **е(т)** и изменение ответа **(Е* г) (т)** согласно пересечению над областью обеих функций. Эта концепция свертки является наиболее используемой техникой обработки сигналов, поэтому она также применяется в компьютерном зрении, что можно рассматривать как обработку сигнала от нескольких датчиков RGB. Интуиция в эффективности свертки заключается в их способности фильтровать заданные входные сигналы в объединенный, более полезный результат.\n",
    "\n",
    "В конкретном случае изображений сигнал лучше понимается с точки зрения матриц, а не форм волны. Поэтому наши функциие **(т)а** также **г(т)** теперь станетизображение *(матрица)* а также Ядро *(матрица)* соответственно. \n",
    "Еще раз, мы можем применить свертку, сдвинув одну из матриц поверх другой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То, что вы видите выше, это просто матричное умножение скользящего окна на изображении с ядром, а затем сложение суммы. Сила сверток в контексте Computer Vision заключается в том, что они являются отличными экстракторами функций для области датчиков RGB. При индивидуальном рассмотрении каждый пиксель (датчик RGB) бессмысленно понимать, что содержит изображение. Именно отношения друг с другом в пространстве придают образу истинный смысл. Это относится к тому, как вы читаете эту статью на своем компьютере, когда пиксели, представляющие символы, и ваш мозг сопоставляют черные пиксели друг с другом в пространстве, чтобы сформировать концепцию символов.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/3d_conv.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "# With square kernels and equal stride\n",
    "m = nn.Conv3d(16, 33, 3, stride=2)\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n",
    "input = torch.randn(20, 16, 10, 50, 100)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Реализация\n",
    "\n",
    "Операция свертки может быть реализованна как скалярное произведение.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/cm1.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/cm2.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/cm3.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/cm4.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/cm5.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/cm6.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Справочник по сверткам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1603.07285v1.pdf\n",
    "\n",
    "https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
    "\n",
    "\n",
    "the best: \n",
    "https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сверточная сеть\n",
    "\n",
    "\n",
    "\n",
    "Теперь в ншем распоряжении появились все необходимые инструменты для создания сверточной сети.\n",
    "\n",
    "* в начале идут несколько сверточных слоев + функция активации за каждым слоем свертки\n",
    "* Затем слой пулинга\n",
    "* Затем еще несколько сверток\n",
    "...\n",
    "\n",
    "Этот набор может повторяться несколько раз, до тех пор пока пространственные размеры изображения не уменьшаться достаточно сильно  \n",
    "\n",
    "В конце добавляются один или несколько полносвязанных слоев.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/cnn.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/gan/lenet5.png\" width=\"700\">\n",
    "\n",
    "Примером сети посторенной по такой архитектуре является LeNet.\n",
    "Была разработана в 1989г. [Яном Ле Куном](https://en.wikipedia.org/wiki/Yann_LeCun). Сеть имела 5 слоев, из них 2 сверточных.\n",
    "\n",
    "Применялась в США для распознавания рукописных букв на почтовых конвертах до начала 2000г.\n",
    "\n",
    "Ниже ее реализация на Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension after pool\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x): # Expected input size 1x1x32x32\n",
    "        x = self.conv1(x) # 32 -> 30 because padding = 0\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2)) # 30 -> 15  Max pooling over a (2, 2) window\n",
    "        x = self.conv2(x) #   15 -> 13 ; 1  + ((N-F) / stride)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2) # 13->6 \n",
    "        x = torch.flatten(x,1) # 1x16x6x6 -> 576   ; alternatively use nn.Flatten()\n",
    "        x = F.relu(self.fc1(x)) # 576 -> 120\n",
    "        x = F.relu(self.fc2(x)) # 120 -> 84\n",
    "        x = self.fc3(x) # 84 -> 10\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберемся почему слои имеют именно такие параметры\n",
    "\n",
    "...\n",
    "\n",
    "Как видно данная реализация рассчитанна на одноканальные изображения размером 32x32. \n",
    "\n",
    "Что бы подавать на вход изображения размером например 28x28 (MNIST) необходимо поменять параметры линейных слоев.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для обучения не отличается от FC сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "target = torch.tensor([1])  # a dummy target, for example\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "\n",
    "loss = criterion(output, target)\n",
    "loss.backward() # backprop\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация\n",
    "\n",
    "Что бы понять на какие визуальные шаблоны обучилась реагировать модель можно визуализировать веса фильтров и карты ативаций .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-5.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация весов \n",
    "\n",
    "Веса фильтров на первом слое легко визуализировать. И результат легко интерпретируется, так как у фильтров такое же количество каналов как и у цветных изображений (3).\n",
    "\n",
    "Ниже приведен пример того, как это можно сделать для предобученной модели на Pytorch (Alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models,utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "#print(alexnet)\n",
    "weight_tensor = alexnet.features[0].weight.data\n",
    "print(weight_tensor.shape)\n",
    "\n",
    "img_grid = utils.make_grid(weight_tensor,pad_value=10)\n",
    "plt.imshow(np.transpose(img_grid , (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры визуализации весов фильтров первого слоя для нескольких популяьных моделей обученных на ImageNet.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-6.png\" width=\"700\">\n",
    "\n",
    "Видно что фильтры обучились реагировать на полосы или точки определенных цветов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация фильтров промежуточных слоев\n",
    "\n",
    "Но веса фильтров на скрытых слоях так просто интерпретировать. \n",
    "Количество каналов на скрытых слоях обычно сильно больше 3-x.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-7.png\" width=\"700\">\n",
    "\n",
    "Можно делать поканальную визуализацию, но она не слишком информативна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractor\n",
    "\n",
    "\n",
    "Альтернативным способом понять насколько эффективно сеть обучилась выделять признаки является анализ карт активаций.\n",
    "\n",
    "Например можно взять выход предпоследнего сверточного слоя и использовать его как вектор - признак описывающий объект.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-8.png\" width=\"700\">\n",
    "\n",
    "* По сути это проецирование данных в пространство меньшей размерности при помощи нейросети. Этот принцип используеся при решении множества задачь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объектов одного класса расстояние между векторами признаком должно быть маленьким. Если применить к полученным векторам алгоритм поиска ближайшего соседа, это еще один способ оценки качества работы модели.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-9.png\" width=\"700\">\n",
    "\n",
    "Как видно из иллюстрации качество работы намного лучше чем у KNN для пикселей. При этом фон и ориентация объекта перестали мешать классификации.\n",
    "\n",
    "\n",
    "*Подобный подход можно применять для кластеризации неразмеченных данных.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация карт активаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение значений активации нейронов в каждом слое сети в ответ на изображение или видео. \n",
    "\n",
    "\n",
    "Если на последних слоях сети, где рецептивные поля нейронов сравнимы с размерами изображения, то можно получить информацию о том, какие нейроны реагируют на какие типы изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение значений активации нейронов в каждом слое сети в ответ на изображение или видео.\n",
    "\n",
    "Если на последних слоях сети, где рецептивные поля нейронов сравнимы с размерами изображения, то можно получить информацию о том, какие нейроны реагируют на какие типы изображений.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-12.png\" width=\"700\">\n",
    "\n",
    "Похоже что этот нейрон научился распознавать лица."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Активация слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее очевидный метод визуализации заключается в том, чтобы показать активации сети во время прямого прохода. Для сетей ReLU активации обычно начинает выглядеть относительно сгущенной и плотной, но по мере развития обучения активации обычно становится более редкой и локализованной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Единственная опасная ловушка, которую можно легко заметить с помощью этой визуализации, заключается в том, что некоторые карты активации могут быть равны нулю для многих различных входов. Это может указывать на мертвые фильтры и может быть симптомом высокой скорости обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-14.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Окклюзирующие части изображения\n",
    "Предположим, что CNN относит изображение к классу \"собака\". Как мы можем быть уверены, что он действительно улавливает собаку на изображении, а не контекстуальные сигналы с фона или другого объекта? Один из способов исследовать, из какой части изображения исходит некоторое классификационное предсказание, заключается в построении графика вероятности интересующего класса (например, класса собак) в зависимости от положения объекта окклюдера. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы перебираем области изображения, устанавливаем патч изображения равным нулю и смотрим на вероятность класса. Мы можем визуализировать вероятность как 2-мерную тепловую карту. Этот подход был использован в визуализации и понимании сверточных сетей Мэтью Зейлера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cs231n.github.io/understanding-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-15.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-16.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-17.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-18.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-19.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-21.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-22.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-23.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Визуализация Модели класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее разберем методику визуализации моделей классов, изученную с помощью сверточных нейронных сетей. Этот метод визуализации состоит в численном генерировании изображения, которое является репрезентативным для класса CNN. (which is representative of the class in terms of the ConvNet class scoring model.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пускай Sc(I) - оценка класса c, вычисленная классификационным слоем CNN для изображения I. Мы хотели бы найти регуляризованное изображение L2, таким, чтобы показатель Sc высоким:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/1-1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где λ - параметр регуляризации. Локально-оптимальный показатель I может быть найден методом обратного распространения. Этот процесс напрямую связан с обучением CNN, где обратное распространение используется для оптимизации весов по слоям. Разница в том, что в нашем случае оптимизация выполняется по отношению к входному изображению, а веса фиксируются к тем, которые были найдены на этапе обучения.\n",
    "Мы инициализировали оптимизацию с нулевым изображением (в нашем случае CNN была обучена на данных изображения с нулевым центром), а затем добавили к результату среднее изображение обучающего набора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит также отметить, что мы использовали (ненормализованные) оценки классов Sc, а не апостериоры классов, возвращаемые слоем soft-max:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/1-2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Причина в том, что максимизация класса posterior может быть достигнута за счет минимизации баллов других классов. Таким образом, мы оптимизируем Sc для того, чтобы убедиться в том, что оптимизация концентрируется только на рассматриваемом классе Sc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1312.6034.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-24.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-25.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-26.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-27.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-28.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-29.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-30.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-31.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-32.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pytorch. Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_28.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
