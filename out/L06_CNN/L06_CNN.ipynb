{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Сверточные нейронные сети</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в сверточные нейронные сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полносвязная нейронная сеть \n",
    "Fully-connected Neural Network (FCN). В современных статьях чаще используется термин Multilayer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии мы рассмотрели следующий подход для классификации изображений:\n",
    "\n",
    "1. Превращаем исходные данные в вектор. \n",
    "\n",
    "***Примечание***: при обработке цветного изображения размером $32\\times32$ пикселя ($32\\times32\\times3$), размерность входного вектора будет равна $3072$. Однако, в общем случае модель получает на вход набор одноразмерных векторов (матрицу), потому при обработке одного изображения размер матрицы будет $3072\\times1$.\n",
    "\n",
    "2. Перемножаем матрицу данных с матрицей весов. Размер последней может быть, например, $100\\times3072$. Где $3072$ - размер входного вектора, а $100$ - количество признаков, которое мы хотим получить. Результат обработки одного изображения будет иметь размер $100\\times1$.\n",
    "\n",
    "3. Поэлементно применяем к полученной матрице нелинейную функцию (функцию активации), например Sigmoid или ReLu. Размерность данных при этом не меняется ($100\\times1$). В результате получаем вектор активаций или признаков.\n",
    "\n",
    "4. Используем полученные признаки как входные данные для нового слоя. Количество весов слоя будет зависеть от размерности входной матрицы и того, что мы хотим получить на выходе. Если мы делаем классификатор на $10$ классов, то матрица весов должна иметь размерность $10\\times100$, и на этом можно остановиться. Но в общем случае количество слоев может быть произвольным.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_1.png\" width=\"600\">\n",
    "\n",
    "\n",
    "На изображении представлена описанная выше нейронная сеть, функцией активации в которой является ReLu. Добавление второго слоя позволило модели использовать более одного шаблона на класс. Можно убедиться в этом, обучив модель на датасете CIFAR-10 и визуализировав веса первого слоя модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/mlp-templates.jpg\" width=\"700\">\n",
    "\n",
    "Засчёт создания нескольких шаблонов для каждого из классов, многослойные архитектуры в общем случае показывают более высокую, чем перцептроны, эффективность на задачах классификации изображений. Однако подход с использованием многослойного перцептрона также имеет свои недостатки. Чтобы обнаружить их, присмотримся к происходящему с данными при предобработке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нарушение связей между соседними пикселями\n",
    "\n",
    "Единственным этапом предобработки данных при использовании многослойного перцептрона является превращение изображения в вектор. Основной его проблемой является \"потеря связи\" между соседними пикселями, что показано на изображении ниже. В общем случае, значения из соседних пикселей могут оказаться на большом расстоянии внутри результирующего вектора. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_9.png\" width=\"600\">\n",
    "\n",
    "После преобразования модель не получает информацию о взаимном расположении значений на исходном изображении - при подборе весов никак не используется информация о том, что какие-то значения были \"близки\" друг к другу. При этом мы, люди, понимаем важность взаимосвязей между пикселями.  \n",
    "\n",
    "К примеру, набор тёмных пикселей находящихся поблизости внутри изображения может сообщить нам, что там располагается некий тёмный объект. Если же мы преобразуем изображение в вектор и эти значения \"разбросает\" по нему без сохранения пространственной структуры, то новому наблюдателю будет очень сложно понять, что где-то на исходном изображении была тёмная зона.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{brown}{\\text{Дополнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Рецептивные поля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе зрительного восприятия, человек обрабатывает информацию о свете, попадающем на сетчатку глаза. Фоторецепторы сетчатки (палочки и колбочки) передают информацию в более сложные клетки - биполярные. При этом, биполярные клетки получают информацию из располагающихся рядом рецепторных клеток, таким образом обрабатывая локальную информацию. На следующих уровнях обработки информации происходит аналогичная картина - клетки уровнем выше собирают информацию из нескольких более простых близко расположенных клеток. \n",
    "\n",
    "В результате, каждая из сложных клеток обрабатывают информацию с нескольких простых рецепторов расположенных рядом и возбуждается при определённой комбинации сигналов от них. Область сетчатки глаза, информация из которой попадает на нервную клетку, называется рецептивным полем этой клетки.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_10.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем импульсы от зрительных клеток последовательно обрабатываются различными отделами мозга - таламусом, зрительной корой и далее, после чего извлечённая информация используется человеком для принятия решений. Заметим, что при принятии решений мы можем использовать всю информацию о том, что видим, а также визуально распознавать такие сложные формы, как лица или объекты. Обе этих возможности существуют благодаря иерархической структуре обработке информации: от отдела к отделу мозгу рецептивные поля клеток увеличиваются, как и распознаваемые зрительные формы (от простейших - распознавания краёв и углов, до таких сложных, как лица).\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_11.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hubel & Wiesel,1959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06_13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Ice Cube Model](https://docplayer.ru/79903347-Kasyanov-evgeniy-dmitrievich-sankt-peterburg-nmic-pn-im-v-m-beh-k-m-n-fedotov-ilya-andreevich-ryazan-ryazgmu.html)\n",
    "\n",
    "Эта гипотетическая кубическая модель придумана для пояснения устройства клеток первичной визуальной коры, а именно  – как устроены предпочитаемые ориентации и, соответственно, реакции нейронов V1. Так, V1 можно условно поделить на кубы $2 мм^3$, каждый из которых получает сигналы от обоих глаз. Клетки с одинаковыми ориентационными предпочтениями формируют горизонтальные колонки, при этом соседние вертикальные колонки имеют слегка отличающиеся ориентационные предпочтения.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_14.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чувствительные к цветам клетки также собраны в столбцы (также их называют каплями, гиперколонками, шариками) 0,5 мм в диаметре в зонах соответствующих превалирующих глаз (картинка с цилиндрами). Каждый такой столбец содержит реагирующие либо на красно-зеленый, либо на сине-желтый контрасты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свертка с фильтром"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для реализации идеи локальности при обработке данных используется операция свёртки. Один из параметров свертки - \"ядро свёртки\", также называемое фильтром. По сути своей, операция свёртки - это та же самая взвешенная сумма с добавлением свободного члена, что и при работе перцептрона. Ключевое отличие между перцептроном и свёрткой заключается в том, что перцептрон получает на вход всё изображение, тогда как свёртка - небольшие фрагменты изображения. То есть, если при обработке перцептроном всё изображение сравнивалось с шаблонами, то теперь лишь небольшие фрагменты изображения сравниваются с шаблонами. \n",
    "\n",
    "К примеру, на GIF ниже можно увидеть, как фильтр размером $3\\times3$ применяется к одноканальному изображению размером $5\\times5$. Шаблон имеет форму x-образного креста. В правой части можно увидеть, насколько фрагмент изображения под фильтром совпадает с шаблоном внутри фильтра.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-2.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем это в коде:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "img = torch.Tensor([[1,1,1,0,0],\n",
    "                    [0,1,1,1,0],\n",
    "                    [0,0,1,1,1],\n",
    "                    [0,0,1,1,0],\n",
    "                    [0,1,1,0,0]])\n",
    "\n",
    "kernel = torch.Tensor([[1, 0, 1],\n",
    "                       [0, 1, 0],\n",
    "                       [1, 0, 1]])\n",
    "\n",
    "result = torch.zeros((3,3)) # img - kernel + 1 (5 - 3 + 1 = 3)\n",
    "\n",
    "for i in range(0, result.shape[0]):\n",
    "    for j in range(0, result.shape[1]):\n",
    "        segment = img[i:i+kernel.shape[0], \n",
    "                        j:j+kernel.shape[1]]\n",
    "        result[i, j] = torch.sum(segment * kernel)\n",
    "\n",
    "print(f'img shape: {img.shape}')\n",
    "print(f'kernel shape: {kernel.shape}')\n",
    "print(f'result shape: {result.shape}')\n",
    "print(f'result:\\n{result}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры 'hancrafted' фильтров\n",
    "\n",
    "В течение долгого времени веса в фильтрах подбирались вручную. Этого было достаточно для простой обработки изображений. О примерах мы поговорим далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтр Гаусса\n",
    "\n",
    "Данный фильтр является одним из наиболее известных и используется в компьютерном зрении для снижения уровня шума в изображении. Такое название он носит, поскольку веса внутри фильтра инициируются двумерной функцией Гаусса. Центр распределения находится в центре фильтра. \n",
    "\n",
    "Ниже показан пример использования фильтра Гаусса размером $5 \\times 5$ для размытия изображения. Размер фильтров может варьироваться от задачи к задаче - то есть, в некоторых случаях использование фильтра $5 \\times 5$ даёт слишком сильное размытие, из-за чего может быть использован фильтр меньшего размера: к примеру, $4 \\times 4$ или $3 \\times 3$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "from skimage import data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_cat = data.chelsea().mean(axis = 2).astype('int32')\n",
    "\n",
    "def im_show(img):\n",
    "  plt.axis('off')\n",
    "  plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "  plt.show()\n",
    "\n",
    "print(f'img_cat shape: {img_cat.shape}  -original-')\n",
    "im_show(img_cat)\n",
    "\n",
    "g_kernel5 = torch.tensor([[1,4,7,4,1],\n",
    "                          [4,16,26,16,4],\n",
    "                          [7,26,41,26,7],\n",
    "                          [4,16,26,16,4],\n",
    "                          [1,4,7,4,1]]\n",
    "                          ,dtype=torch.float) / 273 # sum of weights\n",
    "\n",
    "gaussian_filter = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=g_kernel5.shape, bias=False)\n",
    "gaussian_filter.weight.data = g_kernel5.unsqueeze(0).unsqueeze(0) # gaussian_kernel shape: [1,1,5,5]\n",
    "\n",
    "input = torch.FloatTensor(img_cat).unsqueeze(0).unsqueeze(0) # image to torch tensor shape: [1,1,300,451]\n",
    "out = gaussian_filter(input)\n",
    "out = out.squeeze(0).squeeze(0)\n",
    "\n",
    "print(f'out shape: {out.detach().numpy().shape}  -blurred-')\n",
    "im_show(out.detach().numpy()) # use .detach() to detached from the current graph(Requires grad = False) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтр Собеля\n",
    "\n",
    "На заре компьютерного зрения, людям приходилось явно выполнять feature engeneering при обработке изображений. То есть, было необходимо извлекать важную информацию из изображений. Довольно важной информацией на изображении являются границы - они могут быть использованы для большого количества задач. К примеру, для поиска дорожной разметки на изображении. Как видно на изображении ниже, фильтры - в частности, фильтр Собеля - могут прекрасно справляться с поиском границ. С данным фильтром Вы ещё раз столкнётесь в ходе практической работы.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/sobel_example.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{brown}{\\text{Дополнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Детектор границ Canny\n",
    "\n",
    "На самом деле, если мы будем применять упомянутый выше алгоритм без подготовки изображения и дополнительных операций, то не получим хорошее качество обнаружения границ. Поэтому существуют детекторы границ, дополняющие оператор Собеля либо иные операторы (в алгоритме ниже оператор Собеля используется на шаге 2).\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/canny.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Детекторы углов\n",
    "\n",
    "Другим примером созданных вручную экстракторов признаков являются всевозможные детекторы углов. Одним из самых известных среди них является детектор углов Харриса. Мы не будем на нем останавливаться, но Вы можете прочесть о нем самостоятельно.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Harris_Corner_Detector\n",
    "\n",
    "\n",
    "Как Вы увидите далее, сверточные нейронные сети по факту автоматизируют процесс извлечения подобных признаков, высвобождая время исследователей и инженеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемещая различные ядра свертки по изображению, можно выполнять манипуляции с изображениями. Вот как работают многие инструменты, которые можно найти в программах для редактирования изображений, таких как Photoshop или Gimp:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточный слой нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основные параметры свёртки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как уже упоминалось ранее, по сути операция свёртки представляет из себя операцию взвешенной суммы со свободным членом, что довольно сильно напоминает перцептрон с тем лишь отличием, что последний применяется ко всем данным, а не к их части. Тем не менее, подобно линейному слою, операцию свёртки можно встроить в нейросеть и, путём градиентного спуска, подбирать веса. Таким образом выполняется автоматический подбор весов свёртки.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/filter1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем операцию свертки с помощью линейного слоя:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "local_linear = nn.Linear(9, 1, bias = False) # 9 = 3*3 (weights shape: (3,3))\n",
    "\n",
    "img = torch.Tensor([[1,1,1,0,0],\n",
    "                    [0,1,1,1,0],\n",
    "                    [0,0,1,1,1],\n",
    "                    [0,0,1,1,0],\n",
    "                    [0,1,1,0,0]])\n",
    "# kernel weights\n",
    "weights = torch.Tensor([[1, 0, 1],\n",
    "                       [0, 1, 0],\n",
    "                       [1, 0, 1]])\n",
    "\n",
    "local_linear.weight = nn.Parameter(weights.reshape(-1)) # set weights \n",
    "\n",
    "result = torch.zeros((3,3)) # img - kernel + 1 (5 - 3 + 1 = 3)\n",
    "\n",
    "for i in range(0, result.shape[0]):\n",
    "    for j in range(0, result.shape[1]):\n",
    "        segment = img[i:i+weights.shape[0], \n",
    "                        j:j+weights.shape[1]].reshape(-1)\n",
    "        result[i, j] = local_linear(segment)\n",
    "\n",
    "print(f'img shape: {img.shape}')\n",
    "print(f'weights shape: {weights.shape}')\n",
    "print(f'result shape: {result.shape}')\n",
    "print(f'result:\\n{result}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка цветных/многоканальных изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее речь шла об одноканальных изображениях, однако в реальной жизни мы чаще сталкиваемся с RGB изображениями, в которых более одного канала - сразу три. В общем случае, изображение имеет размер $C \\times H\\times W$. К примеру, изображения из датасета CIFAR-10 имеют размер $3\\times32\\times32$.  \n",
    "\n",
    "При обработке подобных изображений, используется уже не матрица, а трёхмерный тензор. Ниже демонстрируется процесс обработки фрагмента трёхмерного изображения фильтром размерности $3\\times3$:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/conv1.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результатом свертки входного тензора будет карта активации с глубиной $1$, вне зависимости от количества его каналов. \n",
    "\n",
    "Важен тот факт, что входной тензор не обязательно должен формироваться из количества каналов изображения, а может быть выходом предыдущего свёрточного слоя.\n",
    "\n",
    "Во время прямого прохода фильтр перемещается по ширине и высоте входного представления и вычисляются скалярные произведения между значениями фильтра и подлежащими значениями входного представления. В процессе формируется двумерная карта активации, которая содержит результат применения данного фильтра к каждой из областей входного представления. Сама карта активации и соответствие фрагмента входного представления значению карты активации показаны на изображении выше, тогда как процесс перемещения ядра свёртки продемонстрирован ниже:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_15-1.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img = torch.randn((3, 8, 8)) # 3-num of channels, (8,8)-img size\n",
    "kernel = torch.randn((3, 3, 3)) # 3-num of filters, (3,3)-kernel size\n",
    "\n",
    "result = torch.zeros(6, 6) # 8 - 3 + 1 = 6\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "  for j in range(result.shape[1]):\n",
    "    segment = img[:, \n",
    "                    i:i+kernel.shape[0], \n",
    "                    j:j+kernel.shape[1]]\n",
    "    result[i, j] = torch.sum(segment * kernel)\n",
    "\n",
    "print(f'img shape: {img.shape}')\n",
    "print(f'kernel shape: {kernel.shape}')\n",
    "print(f'result shape: {result.shape}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование нескольких фильтров\n",
    "\n",
    "Подобно тому, как в перцептроне использовались несколько выходных нейронов, чтобы передать информацию о сходстве входного вектора с различными шаблонами, можно использовать несколько фильтров при свёртке. То есть, выполнять свёртку несколько $(C_{out}$, по количеству фильтров) раз с разными фильтрами. В результате появится несколько карт активации, имеющих размер $1\\times H_{out}\\times W_{out}$. При объединении этих карт будет получен тензор размерности $C_{out}\\times H_{out} \\times W_{out}$, где $C_{out}$ - количество фильтров, а также количество \"каналов\" в полученном представлении. Полученное представление может быть без проблем передано для следующей операции свёртки. \n",
    "\n",
    "На изображении ниже продемонстрирован результат применения сверточного слоя, содержащего $5$ фильтров, к изображению из CIFAR-10. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/depth.png\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В свёрточном слое указаны параметры, необходимые для создания ядра свёртки - количество каналов во входном представлении `in_channels` и размер ядра свёртки `kernel_size`. Также в нём необходимо указывать количество используемых фильтров `out_channels`. Стоит отметить, что, в отличие от полносвязанного слоя, свёрточный слой не требует информации о количестве значений во входном представлении и может быть использован как для представлений $C_{in} \\times 32 \\times 32$, так и $C_{in} \\times 100 \\times 100$. Словом, представления могут иметь практически любой размер, главное чтобы он был больше размера ядра свёртки."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conv = torch.nn.Conv2d(in_channels=3, # Number of input channels (3 for RGB images)\n",
    "                       out_channels=5, # Number of filters/output channels\n",
    "                       kernel_size=3)\n",
    "\n",
    "img = torch.randn((1,3,100,100)) # 1-batch size, 3-num of channels, (100,100)-img size\n",
    "print(f'img shape: {img.shape}')\n",
    "\n",
    "out = conv(img)\n",
    "print(f'out shape: {out.shape}') # [1, 5, 98, 98]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уменьшение размера представлений\n",
    "\n",
    "Заметим, что при свёртке ширина $W_{out}$ и высота $H_{out}$ **карты активаций** будут отличаться от **пространственных размерностей** $W_{in}$ и $H_{in}$ исходного тензора. К примеру, при обработке трёхканального тензора размера $32 \\times 32$ ядром размера $5 \\times 5$, можно будет произвести лишь $27$ $(32 - 5)$ сдвигов по вертикали и столько же - по горизонтали. Но при этом размер итоговой карты активаций будет равен $28 \\times 28$, поскольку первый ряд либо столбец можно получить без сдвигов по вертикали либо горизонтали, соответственно. При повторном применении фильтра, размер каждой из сторон уменьшится ещё на $4$. Общая формула нового значения $N'$ пространственной размерности $N$ при размере стороны фильтра $F$ вычисляется по следующей формуле: $N' = N - F + 1$.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conv_1 = torch.nn.Conv2d(in_channels=3, # Number of input channels (3 for RGB images)\n",
    "                       out_channels=6, # Number of filters/output channels\n",
    "                       kernel_size=5)\n",
    "\n",
    "conv_2 = torch.nn.Conv2d(in_channels=6, # Number of input channels (3 for RGB images)\n",
    "                              out_channels=10, # Number of filters/output channels\n",
    "                              kernel_size=5)\n",
    "\n",
    "img = torch.randn((1,3,32,32)) # 1-batch size, 3-num of channels, (32,32)-img size\n",
    "print(f'img shape: {img.shape}')\n",
    "\n",
    "out_1 = conv_1(img)\n",
    "print(f'out_1 shape: {out_1.shape}') # [1, 6, 28, 28]\n",
    "\n",
    "out_2 = conv_2(out_1)\n",
    "print(f'out_2 shape: {out_2.shape}') # [1, 10, 24, 24]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что при уменьшении размера представлений пиксели, находящиеся около краёв, участвуют в значительно меньшем количестве свёрток, чем пиксели в середине, хотя информация в них не обязательно менее ценна, чем информация из центральных пикселей. К примеру, пиксель в верхнем левом углу представления вне зависимости от размера фильтра будет принимать участие лишь в одной свёртке и информация о нём будет сохранена лишь в верхнем левом углу нового представления. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расширение (padding)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для борьбы с описанной выше проблемой применяется дополнение входного тензора (англ. padding). В ходе него ширина и высота тензора увеличиваются засчёт приписывания столбцов и строк с некими значениями. К примеру, на изображении ниже перед свёрткой ядром размера $3\\times3$ был применён padding нулями.  \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_21.png\" width=\"250\">\n",
    "\n",
    "На примере убедимся, что это позволит нам сохранить пространственные размерности тензоров."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img = torch.randn((1, 1, 5, 5)) # create random image\n",
    "print(f'Original tensor:\\nshape:{img.shape}:\\n {img}')\n",
    "\n",
    "# add zeros to image manually\n",
    "padded_img = torch.zeros((1, 1, 7, 7)) # create zeros array to insert image in center\n",
    "padded_img[:, :, 1:-1, 1:-1] += img # insert image, we get image arounded by zeros\n",
    "print(f'\\nPadded tensor:\\nshape:{padded_img.shape}:\\n {padded_img}')\n",
    "\n",
    "# define two conv layers to check results\n",
    "conv_3 = torch.nn.Conv2d(in_channels=1, \n",
    "                        out_channels=1, \n",
    "                        kernel_size=3)\n",
    "conv_5 = torch.nn.Conv2d(in_channels=1, \n",
    "                        out_channels=1, \n",
    "                        kernel_size=5)\n",
    "# use layers separately, to compare results\n",
    "conved_3 = conv_3(img)\n",
    "conved_5 = conv_5(img)\n",
    "\n",
    "conved_pad_3 = conv_3(padded_img)\n",
    "conved_pad_5 = conv_5(padded_img)\n",
    "\n",
    "print('\\n\\nOriginal shape:', img.shape)\n",
    "print('Shape after convolution layer(kernel 3x3):', conved_3.shape)\n",
    "print('Shape after convolution layer(kernel 5x5):', conved_5.shape)\n",
    "\n",
    "print('\\n\\nPadded shape:', padded_img.shape)\n",
    "print('Shape after convolution with padding(kernel 3x3):', conved_pad_3.shape)\n",
    "print('Shape after convolution with padding(kernel 5x5):', conved_pad_5.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что дополнение одним рядом и одним столбцом не является универсальным решением: для фильтра размером 5 размер выходного тензора всё равно отличается от входного. Если мы немного видоизменим полученную выше формулу (используя размер дополнения $P$), то получим следующую формулу: $N' = N + 2\\cdot P - F + 1$.  \n",
    "Видно, что чтобы пространствнные размеры не изменялись ($N' = N$), для разных размеров фильтра требуются разные размеры паддинга. В общем случае, для размера фильтра $F$, требуемый размер дополнения  $\\displaystyle P = \\frac{F-1}{2}$. \n",
    "\n",
    "\n",
    "Важными терминами, связанными с дополнением, являются \"same padding\" - использование дополнения(размер выходного тензора не отличается от размера входного тензора), а также \"valid padding\" - отсутствие дополнения.\n",
    "\n",
    "Ниже можно увидеть пример обработки RGB изображения с same padding (с использованием 0):\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-3.gif\" width=\"780\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем padding используя инструментарий torch и сравним его с ручным добавлением padding:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img = torch.randn((1, 1, 5, 5)) # define random image\n",
    "\n",
    "# add zeros manually\n",
    "padded_img = torch.zeros((1, 1, 7, 7)) \n",
    "padded_img[:, :, 1:-1, 1:-1] += img\n",
    "\n",
    "# conv layer without padding (padding=0 by default)\n",
    "conv_3 = torch.nn.Conv2d(in_channels=1, \n",
    "                        out_channels=1, \n",
    "                        kernel_size=3,\n",
    "                        padding = 0)\n",
    "\n",
    "# conv layer with padding = 1 (add zeros)\n",
    "conv_3_padded = torch.nn.Conv2d(in_channels=1, \n",
    "                                out_channels=1, \n",
    "                                kernel_size=3,\n",
    "                                padding = 1) # Padding added 1 zeros line to all four sides of the input\n",
    "\n",
    "print(\"Explicitly padded:\\n\", conv_3(padded_img))\n",
    "print(\"\\nImplicitly padded:\\n\", conv_3_padded(img))\n",
    "print(\"\\nExplicitly padded=Implicitly padded\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование свёрточных слоёв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свёрточный слой = свёртка + активация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку операция свертки является линейной, аналогично полносвязному слою, к выходу сверточного слоя применяется* функция активации (например, ReLu).\n",
    "\n",
    "*Так как функция активации применяется к тензору поэлементно, не важно какую имено форму имеет тензор, а значит и какой слой находился передней ней: полносвязанный или сверточный. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input = torch.randn((1,3,32,32))\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "                           nn.Conv2d(in_channels = 3,\n",
    "                                     out_channels = 6,\n",
    "                                     kernel_size = 3), # after conv shape: [1,6,30,30]\n",
    "                           nn.ReLU(), # Activation doesn't depend on input shape\n",
    "                           nn.Flatten(), # 6*30*30 = 5400\n",
    "                           nn.Linear(5400,100), \n",
    "                           nn.ReLU(),  # Activation doesn't depend on input shape\n",
    "                           nn.Linear(100,10) # 10 classes, like a cifar10\n",
    "                           )\n",
    "\n",
    "out = model(input)\n",
    "print(f'out shape: {out.shape}')\n",
    "print(f'out:\\n{out}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что внутри нейросети появляется новый слой - `nn.Flatten()`. Он необходим, поскольку полносвязанный слой принимает на вход набор векторов значений, тогда как свёрточный слой возвращает набор трёхмерных тензоров. `nn.Flatten()` необходим, чтобы преобразовать набор тензоров в набор векторов. Эту же операцию можно было бы выполнить с помощью метода `.view()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общая структура свёрточной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросетевая модель из предыдущего примера позволяет в общем случае понять структуру свёрточных нейронных сетей: после некоторого количества свёрточных слоёв, извлекающих локальную пространственную информацию, идут полносвязанные слои (как минимум в количестве одного), сопоставляющие извлечённую информацию. \n",
    "\n",
    "Внутри свёрточных слоёв происходит следующий процесс: первые слои нейронных сетей имеют малые рецептивные поля, т.е. им соответствует малая площадь на исходном изображении. Такие нейроны могут активироваться лишь на некие простые шаблоны, по типу углов или освещённости.\n",
    " Активации уровнем выше уже имеют большие рецептивные поля, в результате чего в картах активации может сохраняться информация о более нетривиальных геометрических фигурах. С каждым слоем свёрточной нейронной сети рецептивные поля увеличиваются, увеличивается и сложность шаблонов, на которые может реагировать слой. В некоторых случаях, рецептивное поле нейронов может быть размером со всё исходное изображение. Пример можно увидеть на схеме ниже.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/r-field.png\" width=\"700\">\n",
    "\n",
    "Если изначально рецептивное поле имело размер $1\\times1$, то после свёртки фильтром $K\\times K$, оно стало иметь размер $K \\times K$. То есть, рецептивное поле каждого нейрона увеличилось на $K-1$ по каждому из направлений. Не сложно самостоятельно убедиться, что данная закономерность сохранится при дальнейшем применении фильтров того или иного размера.\n",
    "\n",
    "Однако, при больших размерах изображений (к примеру, $1024\\times1024$), нейронная сеть становится очень глубокой, что приводит к появлению большого количества параметров. Тем не менее, рецептивное поле каждого из нейронов растить нужно, что делается путём искусственного увеличения рецептивного поля каждого из элементов тензора. В принципе, это можно сделать двумя принципиально разными способами, о примере каждого из которых мы и поговорим:  \n",
    "1. Доработать операцию свёртки, чтобы рецептивное поле росло быстрее\n",
    "2. Добавить некую промежуточную операцию, которая бы увеличивала рецептивные поля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг свёртки (Stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вариантом доработки свёртки с целью увеличения рецептивного поля нейрона является изменение шага фильтра. Ранее фильтры двигались с шагом 1 по горизонтали и 1 по вертикали, то есть зоны применения фильтров были сдвинуты друг относительно друга на 1 строку либо столбец. При увеличении шага, количество применений фильтра изменяется. Свёртка массива $5\\times5$ фильтром размером $3\\times3$ с шагом $2$ по вертикали и горизонтали.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/Convolution_arithmetic.gif\" width=\"350\">\n",
    "\n",
    "Казалось бы, с учеличением шага $S$ рецептивное поле не выросло - как увеличивалось с $1$ до $K$, так и увеличивается. Однако обратим внимание на иное: если раньше размерность $N$ становилась $N - F + 1$, то теперь она станет $\\displaystyle 1 + \\frac{N-F}{S}$. В результате, если раньше следующий фильтр с размером $K'$ имел рецептивное поле в $\\displaystyle N \\cdot \\frac{K'}{N'} = N \\cdot \\frac{K'}{N - F + 1}$, то теперь $\\displaystyle N \\cdot \\frac{K'}{N'} = N \\cdot \\frac{K'}{1 + \\frac{N-F}{S}}$. Понятно, что $\\displaystyle \\frac{K'}{N - F + 1} \\leq \\frac{K'}{1 + \\frac{N-F}{S}}$, потому рецептивное поле каждого нейрона увеличивается.\n",
    "\n",
    "При этом важно заметить, что в некоторых случаях часть данных может не попасть в свёртку. К примеру, при $N = 7,\\, K = 3,\\, S = 3$. В данном случае, $\\displaystyle N' = 1 + \\frac{7 - 3}{3} = 2\\frac13.$ В подобных ситуациях часть изображения не захватывается, в чём мы можем убедиться на наглядном примере:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create torch tensor 7x7\n",
    "input = torch.tensor([[[[1,1,1,1,1,1,99],\n",
    "                        [1,1,1,1,1,1,99],\n",
    "                        [1,1,1,1,1,1,99],\n",
    "                        [1,1,1,1,1,1,99],\n",
    "                        [1,1,1,1,1,1,99],\n",
    "                        [1,1,1,1,1,1,99],\n",
    "                        [1,1,1,1,1,1,99]]]],dtype=torch.float)\n",
    "\n",
    "print(f'input shape: {input.shape}')\n",
    "\n",
    "conv = torch.nn.Conv2d(in_channels=1, # Number of channels\n",
    "                       out_channels=1, # Number of filters\n",
    "                       kernel_size=3, \n",
    "                       stride = 3,\n",
    "                       bias = False # Don't use bias\n",
    "                       )\n",
    "conv.weight  = torch.nn.Parameter(torch.ones((1,1,3,3))) # Replace randow weights to ones\n",
    "out = conv(input) \n",
    "\n",
    "print(f'out shape: {out.shape}')\n",
    "print(f'out:\\n{out}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уплотнение (Субдискретизация, Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим вариантом стремительного увеличения размера рецептивного поля является использование дополнительных слоёв, требующих меньшее количество вычислительных ресурсов. Слои субдискретизации прекрасно выполняют эту функцию - подобно свёртке, производится разбиение изображения на небольшие сегменты, внутри которых выполняются операции, не требующие использования обучаемых весов. Два популярных примера подобных операций: получение максимального значения (max pooling) и получение среднего значения (average pooling). \n",
    "\n",
    "Аналогично разбиению на сегменты при свёртке, pooling имеют два параметра: размер фильтра $F$ (то есть, каждого из сегментов) и шаг $S$ (stride). Аналогичным свёрткам, при применении пуллинга формула размера стороны будет $\\displaystyle N' = 1+ \\frac{N-F}{K}.$ \n",
    "\n",
    "Ниже приведён пример использования обоих пуллингов при обработке массива.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_25.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем это в коде:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# create tensor 4x4\n",
    "input = torch.tensor([[[\n",
    "                     [1,1,2,4],\n",
    "                     [5,6,7,8],\n",
    "                     [3,2,1,0],\n",
    "                     [1,2,3,4],\n",
    "                     ]]],dtype=torch.float)\n",
    "\n",
    "max_pool = torch.nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size = 2, stride=2)\n",
    "\n",
    "print(\"Input:\\n\",input)\n",
    "print(\"Max pooling:\\n\",max_pool(input))\n",
    "print(\"Average pooling:\\n\",avg_pool(input))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно отметить**, что, в отличие от свёрток, уплотнение выполняется по каждому из каналов отдельно, в результате чего количество каналов не меняется, в отличие от применения фильтра при свёртке. К примеру, ниже можно увидеть визуализация применения max pooling к одному из каналов тензора, имеющего $64$ канала. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_24_1.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свёртка фильтром $1\\times1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оба упомянутых выше метода позволяют значительно снизить количество весов в модели, путём быстрого увеличения рецептивного поля нейронов, что даёт возможность не делать модель очень глубокой. Однако если мы посмотрим на количество весов в слое нейронной сети, то заметим, что проблема может быть не только в глубине модели. \n",
    "\n",
    "Вспомним, что по сути своей при свёртке значения сегмента пропускаются через перцептрон с $C_{out}$ выходами и $C_{in} \\cdot F_1 \\cdot F_2$ входами, где $F_1$ и $F_2$ обозначают размер фильтра модели. Итого оказывается, что модель имеет $C_{out} \\cdot C_{in} \\cdot F_1 \\cdot F_2$ весов и $C_{out}$ свободных членов, что в сумме даёт нам $C_{out} \\cdot (C_{in} \\cdot F_1 \\cdot F_2 + 1)$ параметров. Каждая из четырёх переменных оказывает значительное влияние на количество весов. Уменьшение $F_1$ и $F_2$ может привести к уменьшению рецептивных полей в свёрточных слоях выше, что повлечёт увеличение количества слоёв и, скорее всего, увеличение общей сложности модели. Уменьшение $C_{out}$ может привести к ухудшению обобщающей способности моделей. В связи с этим всем, был придуман трюк для уменьшения $C_{in}$ - свёртка фильтром $1\\times1$.\n",
    "\n",
    "Идея заключается в следующем: для каждого элемента в карте активаций $H_{in}\\times W_{in}$ есть вектор размерностью $C_{in}$. Элементы этого вектора сообщают, насколько сильно рецептивное поле соответствует каждому из $C_{in}$ шаблонов. Возможно, стоит попробовать найти некие полезные комбинации данных \"признаков\" внутри каждого элемента представления. Для этого прекрасно подходит поэлементно применённый перцептрон с $C_{temp}$ выходами, также являющийся набором из $C_{temp}$ фильтров свёртки размером $C_{in}\\times1\\times1$. Ниже приведён пример применения такого фильтра с целью снижения количества каналов.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_24.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conv = torch.nn.Conv2d(in_channels=64, # Number of input channels\n",
    "                       out_channels=32, # Number of filters\n",
    "                       kernel_size=1)\n",
    "\n",
    "input = torch.randn((1,64,56,56))\n",
    "out = conv(input)\n",
    "\n",
    "print(\"Input shape:\",input.shape)\n",
    "print(\"Shape after 1x1 conv:\",out.shape) # [1, 32, 56, 56] batch, C_out, H_out, W_out"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте выведем формулу, которая покажет, насколько использование промежуточного свёрточного слоя с $C_{temp}$ слоями уменьшит количество параметров при свёртке с $C_{out}$ фильтрами $F_1\\times F_2$.\n",
    "\n",
    "**Количество параметров при наивном решении:**  \n",
    "$Param_{naive} = C_{out} \\cdot (C_{in} \\cdot F_1 \\cdot F_2 + 1) = C_{in} \\cdot (C_{out} \\cdot F_1 \\cdot F_2) + C_{out}.$\n",
    "\n",
    "**Количество параметров при решении с $1\\times1$:**  \n",
    "$Param_{1\\times1} = C_{temp} \\cdot (C_{in}\\cdot1\\cdot1 + 1) + C_{out} \\cdot (C_{temp} \\cdot F_1 \\cdot F_2 + 1) = C_{temp} \\cdot (1 + C_{in} + C_{out}\\cdot F_1 \\cdot F_2) + C_{out}.$\n",
    "\n",
    "Не сложно заметить, что $\\displaystyle \\frac{Param_{1\\times1}}{Param_{naive}} \\approx \\frac{C_{temp}}{C_{in}}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{brown}{\\text{Дополнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение свёрточного и полносвязанного слоев \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/conv_layer.png\" width=\"550\">\n",
    "\n",
    "Мы успели довольно подробно рассмотреть свёрточный слой и заметить большое количество его сходств с полносвязным слоем (перцептроном). Теперь давайте попробуем сравнить эти два слоя по требуемому количеству параметров и операций умножения.  \n",
    "\n",
    "В первую очередь, давайте определимся с размерами входного и получаемого тензоров. Пусть на вход передаётся $C_{in}\\times H_{in}\\times W_{in}$. На выходе пусть будет $K$ нейронов для полносвзяного слоя и $C_{out}\\times H_{out} \\times W_{out}$ для свёрточного слоя (фильтр имеет размер $C_{in} \\times F_1 \\times F_2$). $F_1$ и $F_2$ - размер ядра свёртки по высоте и ширине, соответственно.\n",
    "\n",
    "Для простоты расчётов давайте примем, что шаг фильтра равен $1$ как по горизонтали, так и по вертикали. В таком случае, $H_{out} = H_{in} - F_1 + 1$, а $W_{out} = W_{in} - F_2 + 1$.\n",
    "  \n",
    "##### Количество параметров:  \n",
    "***Полносвязанный слой:***  \n",
    "Данный слой требует по параметру (весу) для всех связей между входными и выходными нейронами, то есть $C_{in} \\cdot H_{in} \\cdot W_{in} \\cdot K$. Помимо этого, каждый из выходных нейронов имеет свободный член, общее количество которых $K$. Итого, количество обучаемых параметров: $(C_{in} \\cdot H_{in} \\cdot W_{in} + 1) \\cdot K$.\n",
    "\n",
    "***Свёрточный слой:***  \n",
    "Для свёрточного слоя параметры связаны лишь с ядрами свёртки - внутри каждого ядра находятся $C_{in} \\cdot F_1 \\cdot F_2$ параметров, общее их количество - $C_{out}$. Помимо этого, каждое из ядер имеет свой собственный свободный член, потому общее количество обучаемых параметров: $(C_{in} \\cdot F_1 \\cdot F_2 + 1) \\cdot C_{out}$.\n",
    "\n",
    "***Сравнение количества параметров:***  \n",
    "Поскольку количество свободных членов мало относительно количества весов, мы опутим их в этих расчётах. \n",
    "$$Comp_{param} = \\frac{C_{in} \\cdot H_{in} \\cdot W_{in} \\cdot K}{C_{in} \\cdot F_1 \\cdot F_2 \\cdot C_{out}} = \\frac{H_{in} \\cdot W_{in} \\cdot K}{F_1 \\cdot F_2 \\cdot C_{out}}.$$  \n",
    "\n",
    "\n",
    "##### Количество умножений: \n",
    "***Полносвязанный слой:***  \n",
    "В данном слое каждый вес используется лишь один раз, в результате общее количество умножений - $C_{in} \\cdot H_{in} \\cdot W_{in} \\cdot K$.  \n",
    "\n",
    "***Свёрточный слой:***\n",
    "Заметим, что в отличие от перцептрона, свёрточная нейронная сеть использует каждый вес несколько раз, а именно - при подсчёте каждого из элементов карты активации. Размер карты активаций: $H_{out} \\times W_{out}$. В итоге оказывается, что операция умножения применяется $C_{in} \\cdot F_1 \\cdot F_2 \\cdot C_{out} \\cdot H_{out} \\cdot W_{out}$.\n",
    "\n",
    "***Сравнение количества умножений:***\n",
    "$$Comp_{mult} = \\frac{C_{in} \\cdot H_{in} \\cdot W_{in} \\cdot K}{C_{in} \\cdot F_1 \\cdot F_2 \\cdot C_{out} \\cdot H_{out} \\cdot W_{out}} = \\frac{H_{in} \\cdot W_{in} \\cdot K}{F_1 \\cdot F_2 \\cdot C_{out} \\cdot H_{out} \\cdot W_{out}} = \\frac{Comp_{param}}{H_{out} \\cdot W_{out}}.$$\n",
    "\n",
    "##### Выводы:\n",
    "\n",
    "Заметим, что выигрыш по количеству параметров при использовании свёрточного слоя омрачается большим количеством операций перемножения. Это было проблемой в течение долгого времени, пока вычисление операции свёртки не перевели на видеокарты (Graphical Processing Unit). При выполнении свёртки одного сегмента не требуется информация о результатах свёртки на другом сегменте, потому чисто теоретически можно выполнять данные операции параллельно, с чем как раз прекрасно справляются видеокарты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Другие виды сверток\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свертка - это математическая операция, которая объединяет две функции в третью. Например, допустим, что у нас есть две данные функции, **е(т)** а также **г(т)** и мы заинтересованы в применении одного поверх другого и вычислении площади пересечения:**f (t) * g (t) = (f * g) (t)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае мы применяем **г(т)**(называется ядром) более **е(т)** и изменение ответа **(Е* г) (т)** согласно пересечению над областью обеих функций. Эта концепция свертки является наиболее используемой техникой обработки сигналов, поэтому она также применяется в компьютерном зрении, что можно рассматривать как обработку сигнала от нескольких датчиков RGB. Интуиция в эффективности свертки заключается в их способности фильтровать заданные входные сигналы в объединенный, более полезный результат.\n",
    "\n",
    "В конкретном случае изображений сигнал лучше понимается с точки зрения матриц, а не форм волны. Поэтому наши функциие **(т)а** также **г(т)** теперь станетизображение *(матрица)* а также Ядро *(матрица)* соответственно. \n",
    "Еще раз, мы можем применить свертку, сдвинув одну из матриц поверх другой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То, что вы видите выше, это просто матричное умножение скользящего окна на изображении с ядром, а затем сложение суммы. Сила сверток в контексте Computer Vision заключается в том, что они являются отличными экстракторами функций для области датчиков RGB. При индивидуальном рассмотрении каждый пиксель (датчик RGB) бессмысленно понимать, что содержит изображение. Именно отношения друг с другом в пространстве придают образу истинный смысл. Это относится к тому, как вы читаете эту статью на своем компьютере, когда пиксели, представляющие символы, и ваш мозг сопоставляют черные пиксели друг с другом в пространстве, чтобы сформировать концепцию символов.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "conv = nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = conv(input)\n",
    "print(output.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одномерная операция свёртки используется для данных, имеющих последовательную структуру - текстов, аудиозаписей, цифровых сигналов. Как правило, данную структуру можно представить в виде временного измерения.  \n",
    "Двумерная операция свёртки, о которой мы сегодня много говорили, применяется для обработки данных, имеющих пространственную структуру - то есть, играют роль взаимные расположения как по вертикали, так и по горизонтали. (добавить пример не изображения, но каналы + пространственная размерность + время!)  \n",
    "Трёхмерная версия свёртки используется, когда данные имеют три независимых \"пространственных\" компоненты. Простейшим примером являются видео: к двумерной структуре самих изображений добавляется координата времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/3d_conv.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "# torch.nn.Conv3d(in_channels, \n",
    "                  out_channels, \n",
    "                  kernel_size, \n",
    "                  stride=1, \n",
    "                  padding=0, \n",
    "                  dilation=1, \n",
    "                  groups=1, \n",
    "                  bias=True, \n",
    "                  padding_mode='zeros')\n",
    " \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# With cubic kernels and same stride\n",
    "conv = nn.Conv3d(in_channels = 16, \n",
    "                 out_channels = 33, \n",
    "                 kernel_size = 3, \n",
    "                 stride=2)\n",
    "\n",
    "# non-square kernels and unequal stride and with padding\n",
    "conv = nn.Conv3d(in_channels = 16, \n",
    "                 out_channels = 33, \n",
    "                 kernel_size = (3, 5, 2), \n",
    "                 stride=(2, 1, 1),\n",
    "                 padding=(4, 2, 0))\n",
    "\n",
    "input = torch.randn(20, 16, 10, 50, 100)\n",
    "out = conv(input)\n",
    "\n",
    "print('out shape: ',out.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Справочник по сверткам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1603.07285v1.pdf\n",
    "\n",
    "https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
    "\n",
    "\n",
    "the best: \n",
    "https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сверточная сеть\n",
    "\n",
    "Теперь мы можем более детально взглянуть на типичную архитектуру свёрточной нейронной сети. Как ранее уже обсуждалось, в первую очередь необходимо последовательностью свёрточных слоёв и уплотнений достичь того, чтобы каждый элемент карты активации имел большое рецептивное поле, а значит мог отвечать за большие и сложные шаблоны. Затем данные карты активаций выпрямляются в вектора и передаются в полносвязанные слои, последовательность которых, используя глобальную информацию, возвращает значение целевой переменной.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06_27.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/lenet5.png\" width=\"900\">\n",
    "\n",
    "Примером сети посторенной по такой архитектуре является LeNet.\n",
    "Была разработана в 1989г [Яном Ле Куном](https://en.wikipedia.org/wiki/Yann_LeCun). Сеть имела 5 слоев с обучаемыми весами, из них 2 сверточных.\n",
    "\n",
    "Применялась в США для распознавания рукописных чисел на почтовых конвертах до начала 2000г.\n",
    "\n",
    "Ниже представлена ее реализация на Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension after pool\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x): # Expected input size ..x1x32x32\n",
    "        x = self.conv1(x) # 32 -> 28 because padding = 0\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.avg_pool2d(x, (2, 2)) # 28 -> 14  Avg pooling over a (2, 2) window\n",
    "        x = self.conv2(x) #   14 -> 10 ; 1  + ((N-F) / stride)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.avg_pool2d(x, 2) # 13->6 \n",
    "        x = torch.flatten(x,1) # ..x16x5x5 -> ..x400   ; alternatively use nn.Flatten()\n",
    "        x = F.sigmoid(self.fc1(x)) # 400 -> 120\n",
    "        x = F.sigmoid(self.fc2(x)) # 120 -> 84\n",
    "        x = self.fc3(x) # 84 -> 10\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchsummary import summary\n",
    "print('LeNet architecture')\n",
    "print(summary(model, (1,32,32), device='cpu'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, данная модель рассчитанна на одноканальные изображения размером 32x32. Чтобы подавать на вход изображения иного размера - к примеру, $28\\times28$ для MNIST, необходмо изменить лишь параметры полносвязанных слоёв, поскольку операции свёртки и пуллинга могут использоваться на любых представлениях, пока их размеры $\\geq$ соответствующих размеров фильтров и потому их менять нет необходимости.\n",
    "\n",
    "[LeNet torch documentation](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс обучения свёрточной нейронной сети повторяет процесс обучения полносвязанной нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "labels = torch.tensor([1])  # a dummy labels, for example\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = model(input)\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "loss.backward() # backprop\n",
    "optimizer.step()    # Does the update"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам может быть интересно, на какую информацию обращает внимание модель в процессе работы - на какие визульные шаблоны реагирует, насколько они трактуемы нами? \n",
    "\n",
    "Чтобы ответить на все эти вопросы, можно визуализировать карты активаций и веса фильтров свёртки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-5.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация весов \n",
    "\n",
    "Веса фильтров на первом слое легко визуализировать. И результат легко интерпретируется, так как у фильтров такое же количество каналов, как и у цветных изображений (3).\n",
    "\n",
    "Ниже приведен пример того, как это можно сделать для обученной модели в Pytorch (Alexnet)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision import models,utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True) # \n",
    "weight_tensor = alexnet.features[0].weight.data\n",
    "print('weights shape',weight_tensor.shape)\n",
    "\n",
    "img_grid = utils.make_grid(weight_tensor,pad_value=10)\n",
    "plt.imshow(np.transpose(img_grid, (1, 2, 0)))\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Видно, что модели научились улавливать простые геометрические формы - края под разными углами, точки того или иного цвета... Тем не менее, фильтры AlexNet'а оказались настолько большими, что частично захватили не только простую локальную информацию - можно заметить, что некоторые из них ловят сложные градиенты или решётки - это вызвано их сравнительно большим размером.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация фильтров промежуточных слоев\n",
    "\n",
    "К сожалению, выполнить ту же операцию для фильтров на скрытых слоях едва ли представляется разумным - в отличие от приятного трёхканального вида фильтров, который легко можно трактовать и визуализировать, фильтры поздних слоёв имеют гораздо больше каналов, что делает их удобное отображение практически невозможным. Пожалуй, единственным вариантом отображения является поканальное отображение весов, которое довольно сложно трактовать, в чём можно убедиться, взглянув на пример ниже.\n",
    "\n",
    "**Higher Layer: Visualize Filter**\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-7.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractor\n",
    "\n",
    "\n",
    "Альтернативным способом понимания эффективности обучения сети является анализ карт активаций с последних слоёв нейронной сети. Поскольку в процессе работы нейросети создаются промежуточные представления, имеющие меньшую размерность, весь процесс представляет из себя проекирование данных в пространство меньшей размерности. Новое представление может быть использовано для решения различных задач не только с помощью линейных слоёв нейросети, но и с помощью других методов машинного обучения.\n",
    "\n",
    "В следующих примерах мы будем использовать представление, полученное на последнем скрытом слое свёрточной нейронной сети.\n",
    "\n",
    "**Последний слой**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-8.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы убедиться в полезности полученных представлений, выполним поиск ближайшего соседа для изображения в исходном виде, пиксельном пространстве. В результате будут получены изображения, не всегда относящиеся к тому же классу, что и исходное изображение.\n",
    "\n",
    "Если же мы используем предобученную свёрточную нейронную сеть для получения представлений изображений в более низкой размерности, и затем применим алгоритм поиска соседей, то \"близко\" друг к другу окажутся изображения, принадлежащие одному классу, даже если есть визуально более похожие изображения из иных классов - фон и прочие неважные части изображения имеют меньшее влияние на представление. Успешное извлечение признаков отображает успешное обучение модели. Один из вариантов выполнения кластеризации изображений - кластеризация промежуточных представлений свёрточных нейронных сетей.\n",
    "\n",
    "**Последний слой: ближайшие соседи**\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-9.jpg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация карт активаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На последних слоях свёрточной нейронной сети размеры рецептивных полей нейронов становятся сравнимы с размером исходного изображения, потому при визуализации их карт активации становится понятно, какие нейроны реагируют на какие части изображений.\n",
    "\n",
    "К примеру, на изображении ниже активация выделенного нейрона достигнута благодаря пикселям, примерно соответствующим расположению лица человека, потому можно предположить, что он научился находить лица на изображении. Более подробно об этом можно почитать в [данной статье](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf).\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\color{brown}{\\text{Дополнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximally Activating Patches**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-13.jpg\" width=\"900\">\n",
    "\n",
    "\n",
    "*Выберите слой и канал; например, conv5 имеет размер 128 x 13 x 13, выберите канал 17/128;*\n",
    "\n",
    "*Пропустите через сеть множество изображений, запишите значения выбранного канала;*\n",
    "\n",
    "*Визуализируйте участки изображения, соответствующие максимальным активациям.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё один способ интерпретации нейронов выполнен следующим образом: через обученную сеть пропускается большое количество изображений, для каждого из которых записываются значения активаций на определённом канале определённого представления. К примеру, на изображении выше были взяты несколько каналов из `conv5` слоя и для каждого из изображений были записаны значения активаций. \n",
    "\n",
    "Затем для каждого из интересующих нас каналов производится поиск изображений, на которых активация данного канала была максимальной, после чего визуализируется рецептивное поле значений, максимально сильно активировавших тот или иной нейрон. \n",
    "\n",
    "Первый из визуализированных каналов реагирует на тёмную окружность на белом фоне, являющуюся глазом либо носом животного. Четвёртый канал реаширует на текст. \n",
    "\n",
    "Визуализации в нижней части изображения соответствуют объектам - их рецептивное поле ещё выше. Видно, что одни нейроны реагируют на машины, другие - на людей и так далее. Словом, модель выделяет довольно интерпретируемые объекты, чего и стоило ожидать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Активация слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее очевидный метод визуализации заключается в том, чтобы показать активации сети во время прямого прохода. Для сетей ReLU активации обычно начинает выглядеть относительно сгущенной и плотной, но по мере развития обучения активации обычно становится более редкой и локализованной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Единственная опасная ловушка, которую можно легко заметить с помощью этой визуализации, заключается в том, что некоторые карты активации могут быть равны нулю для многих различных входов. Это может указывать на мертвые фильтры и может быть симптомом высокой скорости обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Pixels Matter? Saliency via Occlusion**\n",
    "\n",
    "Mask part of the image before feeding to CNN, NN, check how much predicted probabilities change\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-14.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Окклюзирующие части изображения\n",
    "Предположим, что CNN относит изображение к классу \"собака\". Как мы можем быть уверены, что он действительно улавливает собаку на изображении, а не контекстуальные сигналы с фона или другого объекта? Один из способов исследовать, из какой части изображения исходит некоторое классификационное предсказание, заключается в построении графика вероятности интересующего класса (например, класса собак) в зависимости от положения объекта окклюдера. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы перебираем области изображения, устанавливаем патч изображения равным нулю и смотрим на вероятность класса. Мы можем визуализировать вероятность как 2-мерную тепловую карту. Этот подход был использован в визуализации и понимании сверточных сетей Мэтью Зейлера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cs231n.github.io/understanding-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which Pixels Matter? Saliency via Occlusion**\n",
    "\n",
    "Mask part of the image before feeding to CNN, check how much predicted probabilities change\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-15.jpg\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обратное распространение для определения важности\n",
    "\n",
    "***Примечание:*** подробнее ознакомиться с информацией об изложенных ниже методах можно в [данной статье](https://arxiv.org/pdf/1312.6034.pdf).\n",
    "\n",
    "Ключевым элементом другого варианта определения важности различных частей на изображении является использование обратного распространения ошибки. В отличие от обычного обучения, где мы считаем неизменным входное изображение и пытаемся считать градиент по параметрам модели, в данном случае мы фиксируем или замораживаем значения весов нейросети и считаем градиент по входному изображению. \n",
    "\n",
    "Модуль градиента в данном случае будет показывать, сколь сильно изменение значения данного пикселя изменит предсказание метки класса в ту или иную сторону. Нам важен лишь сам факт сильного влияния, потому при определении важности стоит смотреть на **модули** значений частных производных по каждому из значений внутри изображений. В результате каждый пиксель будет представлен тремя неотрицательными числами и для простоты визуализации мы можем выбрать лишь максимальное значение для каждого из пикселей, в результате получая одноканальное изображение с важностью пикселей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which pixels matter? Saliency via Backprop**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-17.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На следующем изображении можно увидеть примеры применения описанной выше последовательности действий к различным изображениям. Как видно, модель действительно обращает внимание на важные фрагменты изображения, практически игнорируя фон и прочую информацию с изображений, не относящуюся к классам напрямую. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-18.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После получения карт важности, можно попробовать выполнить сегментацию объектов на изображении: если мы предполагаем, что модель обучилась и \"важными\" для неё будут лишь осмысленные объекты, а не некий шум, то при использовании карт важности в качестве масок сегментации мы должны получать неплохие результаты, что можно увидеть на изображении ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-19.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичную операцию можно выполнять, но считать градиенты не по конечным предсказаниям классов, а по активациям некого нейрона. То есть, мы смотрим, какие части рецептивного поля нейрона стоит изменять, чтобы значение активации возросло. \n",
    "\n",
    "Чтобы изображение получилось более аккуратным, стоит через каждую ReLU пропускать лишь положительные значения градиента, в тех нейронах, которые не были занулены в ходе прямого прохода сигнала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intermediate Features via (guided) backprop**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img_license/L06-21.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ранее, мы возьмём фрагменты изображения, которые максимально активировали те или иные нейроны в нашей нейросети. На них мы выполним ту же самую операцию, что и для целых изображений, за исключением использования максимального значения производной по каждому из пикселей - теперь мы представляем производные в виде RGB изображений. \n",
    "\n",
    "На примерах ниже видно, что использование градиентов помогает ещё лучше понять, на что же смотрит нейросеть в спорных, сложных для трактовки ситуациях - к примеру, в нижнем ряду на первом изображении.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-22.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный метод может быть использован на любом уровне представлений нейронной сети, в том числе и на самых высокоуровневых, способных замечать некие сложные формы и объекты. Видно, что некоторые нейроны научились обнаруживать практически любых собак на изображении. Другие обращают внимание на лицо и важные его части."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-23.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация Модели класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее разберем методику визуализации моделей классов, изученную с помощью сверточных нейронных сетей. Этот метод визуализации состоит в численном генерировании изображения, которое является репрезентативным для класса CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $S_c(I)$ - оценка класса c, вычисленная классификационным слоем CNN для изображения $I$. Мы хотели бы найти регуляризованное изображение $L_2$, таким, чтобы показатель $S_c$ высоким:\n",
    "\n",
    "$$\\arg \\max_{I} S_c(I) - \\lambda||I||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "где $\\lambda$ - параметр регуляризации. Локально-оптимальный показатель $I$ может быть найден методом обратного распространения. Этот процесс напрямую связан с обучением CNN, где обратное распространение используется для оптимизации весов по слоям. Разница в том, что в нашем случае оптимизация выполняется по отношению к входному изображению, а значения весов устанавливаются на те, которые были найдены на этапе обучения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-24.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы инициализировали оптимизацию нулевым изображением (в нашем случае CNN была обучена на данных изображения с нулевым средним), а затем добавили к результату среднее изображение обучающего набора. После этого мы можем считать частые производные предсказаний по значениям изображения и выполнить обновление в сторону **увеличения** предсказанной вероятности класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1. Initialize image to zeros*\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-25.png\" width=\"1000\">\n",
    "\n",
    "*Repeat:*\n",
    "*2. Forward image to compute current scores*\n",
    "\n",
    "*3. Backprop to get gradient of neuron value with respect to image pixels*\n",
    "\n",
    "*4. Make a small update to the image*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит также отметить, что мы использовали (ненормализованные) оценки классов $S_c$, а не апостериоры классов, возвращаемые слоем soft-max:\n",
    "\n",
    "$$P_c = \\frac{\\exp S_c}{\\sum_c\\exp S_c}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Причина в том, что максимизация класса posterior может быть достигнута за счет минимизации баллов других классов. Таким образом, мы оптимизируем $S_c$ для того, чтобы убедиться в том, что оптимизация концентрируется только на рассматриваемом классе $S_c$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На двух изображениях ниже можно увидеть примеры сгенерированных изображений, максимизирующих предсказания по тем или иным классам. На изображениях ниже можно заметить очертания целевых объектов - к примеру, на сгенерированном для класса \"хаски\" изображении отчётливо виден силуэт собаки и характерные элементы окраски. Аналогиная ситуация наблюдается и для класса \"гусь\", но там видно уже несколько различных вариантов расположения гуся на изображении. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-27.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-28.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный процесс генерации изображений можно с лёгкостью улучшить - стоит лишь периодически использовать фильтр Гаусса (о котором мы сегодня говорили) для сглаживания изображения, а также занулять отрицательные значения пикселей и частных производных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-29.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку при данном процессе вносится небольшая неопределённость, могут быть сгенерированы различные изображения в зависимости от того, когда была выполнен набор описанных выше. На изображениях ниже видны примеры работы дополненного алгоритма обработки. Как видно, на изображениях гораздо лучше читаются силуэты целевых объектов, а также окружающего их фона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-30.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-31.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тот же самый процесс может быть применён при максимизации значений внутри фильтров, таким образом чтобы было легче понять, на что активируется тот или иной нейрон. В примере ниже приведены примеры для разных свёрточных слоёв, начиная от второго - с малым рецептивным полем - и заканчивая пятым, имеющим сравнительно большое рецептивное поле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing CNN Features: Gradient Ascent**\n",
    "\n",
    "Use the same approach to visualize intermediate features\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L06_CNN/img/L06-32.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример сверточной сети на датасете mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from IPython.display import clear_output\n",
    "# transforms for data\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "train_set = MNIST(root='./MNIST', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./MNIST', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "clear_output()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем сверточную сеть:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_model, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*32, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.conv_stack(x) \n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим обучение:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # change run time to gpu to fast training\n",
    "\n",
    "model = CNN_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "loss_hist = [] # for plotting\n",
    "for epoch in range(num_epochs):\n",
    "    hist_loss = 0\n",
    "    for _, batch in enumerate(train_loader, 0): # get batch\n",
    "        # parse batch \n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        # sets the gradients of all optimized tensors to zero.\n",
    "        optimizer.zero_grad() \n",
    "        # get outputs\n",
    "        Y_pred = model(imgs) \n",
    "        # calculate loss\n",
    "        loss = criterion(Y_pred, labels)\n",
    "        # calculate gradients\n",
    "        loss.backward() \n",
    "        # performs a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        hist_loss += loss.item()\n",
    "    loss_hist.append(hist_loss /len(train_loader))\n",
    "    print(f\"Epoch={epoch} loss={loss_hist[epoch]:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график обучения:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(num_epochs), loss_hist)\n",
    "plt.xlabel(\"Epochs\",fontsize=15)\n",
    "plt.ylabel(\"Loss\",fontsize=15)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посчитаем accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calaculate_accuracy(model, data_loader):\n",
    "    correct, total = 0, 0 \n",
    "    with torch.no_grad(): \n",
    "        for batch in data_loader: # get batch\n",
    "            imgs, labels = batch # parse batch\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            Y_pred = model.forward(imgs) # get output\n",
    "            _, predicted = torch.max(Y_pred.data, 1) # get predicted class\n",
    "            total += labels.size(0) # all examples\n",
    "            correct += (predicted == labels).sum().item() # correct predictions \n",
    "    return correct / total "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "acc_train = calaculate_accuracy(model, train_loader)\n",
    "print(f\"Accuracy train = {acc_train}\")\n",
    "acc_test = calaculate_accuracy(model, test_loader)\n",
    "print(f\"Accuracy test = {acc_test}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы сравним результат с моделью, которую мы делали на прошлом занятии то можем увидеть как вырасла точность и уменьшилась ошибка на обучении(точность вырасла на ~10%, ошибка уменьшилась с 0.4 до ~ 0.01)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {}
}
