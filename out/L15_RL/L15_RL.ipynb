{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º\n",
    "–ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏, —Ç–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç—É –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏, –∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –ø—Ä–∏–º–µ–Ω—è–µ–º –µ—ë –∫ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º. –ò–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏, –∫–æ–≥–¥–∞ –æ–±—É—á–∞–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è MNIST, –ø–æ–¥–∞–≤–∞—è –Ω–∞ –≤—Ö–æ–¥ —Å–µ—Ç–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä –∏ —Å—á–∏—Ç–∞—è –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∏ –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É –∏–∑–≤–µ—Å—Ç–Ω—ã–º –ª—ç–π–±–ª–æ–º —Ü–∏—Ñ—Ä—ã –∏ –≤—ã—Ö–æ–¥–æ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.\n",
    "### –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è\n",
    "–í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö —É –Ω–∞—Å –Ω–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –º–æ–≥–ª–∏ –±—ã –∑–∞—Ä–∞–Ω–µ–µ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å. –ù–æ, –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á, –º–æ–∂–Ω–æ –æ–±–æ–π—Ç–∏—Å—å –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏. –ü—Ä–∏–º–µ—Ä–æ–º —Ç–∞–∫–æ–π –∑–∞–¥–∞—á–∏ —è–≤–ª—è–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.\n",
    "### –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º\n",
    "–í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è –Ω–∞–º –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç. –í —Ç–æ –∂–µ –≤—Ä–µ–º—è —É –Ω–∞—Å –Ω–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É. –ü—Ä–∏ —ç—Ç–æ–º –º—ã –º–æ–∂–µ–º –ø–æ—Å—Ç—Ñ–∞–∫—Ç—É–º –æ—Ü–µ–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –æ—Ü–µ–Ω–∫—É –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–∞ —á–∞—â–µ —Å–æ–≤–µ—Ä—à–∞–ª–∞ –∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –∏ —Ä–µ–∂–µ - –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ. –í –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ —Ç–∞–∫—É—é –æ—Ü–µ–Ω–∫—É –Ω–∞–∑—ã–≤–∞—é—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º (reward), –∞ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç—Ä–æ–∏—Ç—Å—è —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã —ç—Ç–æ –º–æ–¥–µ–ª—å —Å—Ç—Ä–µ–º–∏–ª–∞—Å—å –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∞–µ–º–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-7.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-17.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è: –∞–≥–µ–Ω—Ç, —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã, —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã\n",
    "\n",
    "–ê–≥–µ–Ω—Ç –∏ —Å—Ä–µ–¥–∞ - –∫–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ª–µ–Ω–∏–µ–º.\n",
    "\n",
    "**–ê–≥–µ–Ω—Ç** - –ø—Ä–æ–≥—Ä–∞–º–º–∞, –ø—Ä–∏–Ω–∏–º–∞—é—â–∞—è —Ä–µ—à–µ–Ω–∏–µ –æ –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å—Ä–µ–¥—ã.\n",
    "\n",
    "**–°—Ä–µ–¥–∞** - —ç—Ç–æ –º–∏—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω \"–≤—ã–∂–∏–≤–∞—Ç—å\", —Ç.–µ. –≤—Å—ë, —Å —á–µ–º –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—Ä—è–º–æ –∏–ª–∏ –∫–æ—Å–≤–µ–Ω–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å. –°—Ä–µ–¥–∞ –æ–±–ª–∞–¥–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º (State), –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ —Å—Ä–µ–¥—É, —Å–æ–≤–µ—Ä—à–∞—è –∫–∞–∫–∏–µ-—Ç–æ –¥–µ–π—Å—Ç–≤–∏—è (Actions), –ø–µ—Ä–µ–≤–æ–¥—è —Å—Ä–µ–¥—É –ø—Ä–∏ —ç—Ç–æ–º –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –¥—Ä—É–≥–æ–µ –∏ –ø–æ–ª—É—á–∞—è –∫–∞–∫–æ–µ-—Ç–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ. –°—Ä–µ–¥–∞ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ - –≤–µ–∫—Ç–æ—Ä –≤ —ç—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.\n",
    "\n",
    "–í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏, –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –Ω–∞–±–ª—é–¥–∞—Ç—å –ª–∏–±–æ –ø–æ–ª–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã, –ª–∏–±–æ —Ç–æ–ª—å–∫–æ –Ω–µ–∫–æ—Ç–æ—Ä—É—é –µ–≥–æ —á–∞—Å—Ç—å. –í–æ –≤—Ç–æ—Ä–æ–º —Å–ª—É—á–∞–µ, –∞–≥–µ–Ω—Ç—É –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∫–∞–∫–æ–µ-—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –ø–æ –º–µ—Ä–µ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "**–§—É–∫–Ω—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã** - –≤–≤–æ–¥–∏–º–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º —Ñ–æ—Ä–º—É–ª–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –æ–∂–∏–¥–∞–Ω–∏—è —ç—Ç–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø—É—Ç—å –∫ –Ω–∞–∏–ª—É—á—à–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞. –≠—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–æ–≥ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –±–µ–∑ –∫–æ—Ç–æ—Ä–æ–π –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ —á–µ–º—É —É—á–∏—Ç—å—Å—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —à–∞—Ö–º–∞—Ç–∞—Ö –∏—Å—Ç–∏–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ —ç—Ç–æ –ø–æ–±–µ–¥–∞, –Ω–æ –≤–∑—è—Ç–∞—è —Ñ–∏–≥—É—Ä–∞ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞ —Ç–æ–∂–µ —Ü–µ–Ω–Ω–∞ –∏ –¥–æ–ª–∂–Ω–∞ —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –Ω–∞–≥—Ä–∞–¥—É, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø–æ–¥—Å–∫–∞–∑–∞—Ç—å –∞–≥–µ–Ω—Ç—É, —á—Ç–æ –±—Ä–∞—Ç—å —á—É–∂–∏–µ —Ñ–∏–≥—É—Ä—ã –ø–æ–ª–µ–∑–Ω–æ. –ú–æ–∂–µ—Ç –ª–∏ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞—Ç—å –º–∞—Ç, –ø–æ–∑–∞—Ä–∏–≤—à–∏—Å—å –Ω–∞ –Ω–µ–∑–∞—â–∏—â–µ–Ω–Ω—É—é —Ñ–∏–≥—É—Ä—É? –î–∞, —Ä–æ–≤–Ω–æ –∫–∞–∫ –∏ –Ω–µ–æ–ø—ã—Ç–Ω—ã–π —à–∞—Ö–º–∞—Ç–Ω—ã–π –∏–≥—Ä–æ–∫. –ü–æ–ø—ã—Ç–∫–∞ –ø–µ—Ä–µ–¥–∞—Ç—å —á–µ—Ä–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ–æ—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∫ –ø–æ–ª—É—á–µ–Ω–∏—é –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **reward shaping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û—Ç–ª–∏—á–∏–µ –æ—Ç supervised learning\n",
    "\n",
    "–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/rl_idea.png\" width=\"700\"/>\n",
    "\n",
    "–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/sv_idea.png\" width=\"700\"/>\n",
    "\n",
    "–†–∞–∑–Ω–∏—Ü–∞:\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/rl_rand.png\" width=\"700\"/>\n",
    "\n",
    "- –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å: –ø–µ—Ä–µ—Ö–æ–¥—ã –∏ –Ω–∞–≥—Ä–∞–¥—ã —Å–ª—É—á–∞–π–Ω—ã\n",
    "- –û—Ç–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å r_t+1 –º–æ–∂–µ—Ç –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç a_t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è\n",
    "\n",
    "* –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏, –ë–ü–õ\n",
    "* –ò–≥—Ä—ã\n",
    "* –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –≤ —É—Å–ª–æ–≤—Ç—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º–∏\n",
    "* drug discovery: –æ–±—ã—á–Ω–æ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç—Ä–∞–∂–∞—é—â–∏–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ –Ω–µ–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω–∏—Ü—Ä—É–µ–º—ã –∏ –º–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ, –≤–≤–æ–¥—è RL\n",
    "* –ù–µ —Ç–∞–∫ –¥–∞–≤–Ω–æ –æ–¥–Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∞—è –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ RL –¥–ª—è –æ–ø—Ç–∏–º–∞–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–∫–∞—Ç–Ω–æ–≥–æ —Å—Ç–∞–Ω–∞\n",
    "* NAS (Neural Architecture Search): –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ç–æ–ø–æ–ª–æ–≥–∏–π —Å–µ—Ç–∏ –º–æ–∂–Ω–æ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É —Å –ø–æ–º–æ—â—å—é RL, –Ω–∞–≥—Ä–∞–∂–¥–∞—è –Ω–∞—à–µ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∑–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Ö—Ö–æ—Ä–æ—à–∏—Ö —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏ –Ω–∞–∫–∞–∑—ã–≤–∞—è –∑–∞ –ø–ª–æ—Ö–∏–µ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–∞—Ç–∞—Å–µ—Ç—ã\n",
    "\n",
    "OpenAI \n",
    "\n",
    "Environments: \n",
    "\n",
    "https://gym.openai.com/envs/#classic_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision process (MDP) \n",
    "\n",
    "–ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-25_.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "–ì—Ä–∞—Ñ.\n",
    "\n",
    "- –≤–µ—Ä—à–∏–Ω–∞  == —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã\n",
    "- —Ä–µ–±—Ä–æ = –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏ ~= –¥–µ–π—Å—Ç–≤–∏–µ\n",
    "\n",
    "\n",
    "–ö—Ä–æ–º–µ —Ç–æ–≥–æ:\n",
    "- –ö–∞–∂–¥—ã–µ –ø–µ—Ä–µ—Ö–æ–¥ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è –Ω–∞–≥—Ä–∞–¥–æ–π (reward)\n",
    "- –ü–µ—Ä–µ—Ö–æ–¥—ã —Å–ª—É—á–∞–π–Ω—ã \n",
    "- –°—É—â–µ—Å—Ç–≤—É—é—Ç —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä: –ø—Ä–æ–≥—É–ª–∫–∞ –ø–æ –ª—å–¥—É."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov property\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-4.png\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "\n",
    "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ —Ç–µ–∫—É—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.\n",
    "\n",
    "–≠—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø—Ä–∏ –æ–±—Ö–æ–¥–µ —Ç–∞–∫–∏—Ö –≥—Ä–∞—Ñ–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ù–∞–≥—Ä–∞–¥–∞ (Reward)\n",
    "\n",
    "–ö–∞–∂–¥—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –∞—Å—Å–æ—Ü–∏–∏—Ä—É–µ—Ç—Å—è —Å –Ω–∞–≥—Ä–∞–¥–æ–π R.\n",
    "–æ–Ω–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫ –Ω—É–ª–µ–≤–æ–π, –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π.\n",
    "\n",
    "\n",
    "–ù–∞–ø—Ä–∏–º–µ—Ä –≤ MDP –æ–ø–∏—Å—ã–≤–∞—é—â–µ–º –∏–≥—Ä—É –≤ —à–∞—Ö–º–∞—Ç—ã –∏–ª–∏ GO –Ω–∞–≥—Ä–∞–¥–∞ –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ \"–ø–∞—Ä—Ç–∏—è –≤—ã–∏–≥—Ä–∞–Ω–Ω–∞\"\n",
    "\n",
    "–ù–∞ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ö–æ–¥–∞—Ö –Ω–∞–≥—Ä–∞–¥–∞ –±—É–¥–µ—Ç –Ω—É–ª–µ–≤–æ–π.\n",
    "\n",
    "–î–ª—è –ø–µ—Ä–µ–º–µ—â–∞—é—â–µ–≥–æ—Å—è —Ä–æ–±–æ—Ç–∞ –Ω–∞–≥—Ä–∞–¥–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π, —Ç–∞–∫ –∫–∞–∫ –Ω–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —ç–Ω–µ—Ä–≥–∏—è. –∏.—Ç.–ø. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "–ü—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (discounting)\n",
    "\n",
    "\n",
    "–ß—Ç–æ –ª—É—á—à–µ: \"–ü–æ–ª—É—á–∏—Ç—å –º–∏–ª–ª–∏–æ–Ω –¥–æ–ª–ª–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ 100 –ª–µ—Ç –∏–ª–∏ $100 —Å–µ–π—á–∞—Å\"\n",
    "\n",
    "–ß–∞—Å—Ç–æ –≤—Ä–µ–º—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞–≥—Ä–∞–¥—ã –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-12.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "–ü–æ—ç—Ç–æ–º—É –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –∫—É–º–º—É–ª—è—Ç–∏–≤–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —à–∞–≥–µ t (Gt) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è $\\gamma$ \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-15.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-24_.png\"  style=\"width: 500px;\"/>\n",
    "\n",
    "–ê–ª–≥–æ—Ä–∏—Ç–º:\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/mdp.png\"  style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ—Ä \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" />\n",
    "\n",
    "\n",
    "- —á–µ—Ä–Ω—ã–µ –±–µ–∑–∑–Ω–∞–∫–æ–≤—ã–µ —á–∏—Å–ª–∞ - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤\n",
    "- –∑–Ω–∞–∫–æ–≤—ã–µ —ç—Ç–æ –Ω–∞–≥—Ä–∞–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –ø—Ä–∏–º–µ—Ä–æ–º:\n",
    "https://github.com/yandexdataschool/Practical_RL"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
    "!touch .setup_complete\n",
    "\n",
    "# After this setup we can import mdp package"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from mdp import MDP \n",
    "\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "# We can now use MDP just as any other gym environment:\n",
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# MDP methods\n",
    "\n",
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "\n",
    "# state, action, next_state\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "\n",
    "# get_transition_prob(self, state, action, next_state)\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "import mdp as mdp_package\n",
    "display(mdp_package.plot_graph(mdp))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym\n",
    "\n",
    "\n",
    "\n",
    "–í –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –∫ –∫–æ–¥—É –≤—ã—à–µ, –≥–æ–≤–æ—Ä–∏–ª–æ—Å—å –æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Gym\n",
    "\n",
    "*\"Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.\"*\n",
    "\n",
    "\n",
    "\n",
    "form [openai](https://openai.com/projects/)\n",
    "https://gym.openai.com/docs/\n",
    "\n",
    "\n",
    "–°–ø–∏—Å–æ–∫ –æ–∫—Ä—É–∂–µ–Ω–∏–π:\n",
    "\n",
    "https://gym.openai.com/envs/#classic_control\n",
    "\n",
    "\n",
    "https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\n",
    "\n",
    "...\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –æ–∫—Ä—É–∂–µ–Ω–∏—è** \n",
    "–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≥–æ—Ç–æ–≤–∞—è —Å—Ä–µ–¥–∞: –º–∞—à–∏–Ω–∫–∞ –¥–æ–ª–∂–Ω–∞ –∑–∞–µ—Ö–∞—Ç—å –Ω–∞ –≥–æ—Ä–∫—É. –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è. –ê –∞–≥–µ–Ω—Ç - —Å–ª—É—á–∞–π–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ. –û–Ω–æ –Ω–∏—á–µ–º—É –Ω–µ —É—á–∏—Ç—Å—è, –ª–∏—à—å —Ö–∞–æ—Ç–∏—á–µ—Å–∫–∏ –≤—ã–¥–∞—ë—Ç –¥–µ–π—Å—Ç–≤–∏—è, –Ω–æ —ç—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≥–¥–µ –≤–∑—è—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install necessary package for show visualization in Colab\n",
    "# https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab\n",
    "\n",
    "!apt-get install -y xvfb python-opengl\n",
    "!pip install gym pyvirtualdisplay"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define some helper functions and init virtual display\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def query_environment(name):\n",
    "  env = gym.make(name)\n",
    "  spec = gym.spec(name)\n",
    "  print(f\"Action Space: {env.action_space}\")\n",
    "  print(f\"Observation Space: {env.observation_space}\")\n",
    "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "  print(f\"Reward Range: {env.reward_range}\")\n",
    "  print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
    "\n",
    "def draw(env):\n",
    "  screen = env.render(mode='rgb_array')\n",
    "  plt.imshow(screen)\n",
    "  ipythondisplay.clear_output(wait=True)\n",
    "  ipythondisplay.display(plt.gcf())\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "query_environment(\"MountainCar-v0\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gym/wiki/MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "MAX_NUM_EPISODES = 1\n",
    "\n",
    "for episode in range(MAX_NUM_EPISODES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n",
    "    step = 0\n",
    "    while not done:\n",
    "        draw(env)       \n",
    "        action = env.action_space.sample() \n",
    "\n",
    "        # Sample random action.\n",
    "        # This will be replaced by our agent's action\n",
    "        # when we # start developing the agent algorithms\n",
    "\n",
    "        #action = int(input()) 0,1,2\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # Send the action to the environment and receive       \n",
    "        # the next_state, reward and whether done or not\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        obs = next_state\n",
    "        \n",
    "        if step > 50:\n",
    "          # Forse exit in Colab demo\n",
    "          done = True\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"\\n Episode #{} ended in {} steps. total_reward={}\".format(episode, step+1,\n",
    "total_reward))\n",
    "env.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "wrapped_env = gym.wrappers.Monitor(env, 'test', force = True)\n",
    "wrapped_env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "  action = env.action_space.sample()\n",
    "  observation, reward, done, info = wrapped_env.step(action)\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "wrapped_env.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –≠–ø–∏–∑–æ–¥\n",
    "\n",
    "\n",
    "–î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (–∫–æ–Ω–µ—Ü –∏–≥—Ä—ã)\n",
    "\n",
    "`done`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏\n",
    "\n",
    "–ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∫–æ—Ç–æ—Ä–æ–π –±—É–¥–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ (Gt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### –ü–æ–ª–∏—Ç–∏–∫–∞\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-26.png\" alt=\"Drawing\" width=\"500\">\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –µ–µ —Ä–µ—à–µ–Ω–∏—è –±—É–¥–µ—Ç \"–ü–æ–ª–∏—Ç–∏–∫–∞\" –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ a –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è s.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/policy1.png\"  style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "*–°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –ø–æ–ª–∏—Ç–∏–∫–∞, –∫–∞–∫ —Å–ª–µ–¥—É–µ—Ç –∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∂–µ—Å—Ç–∫–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π. –í–ø–æ–ª–Ω–µ –≤–æ—Ö–º–æ–∂–Ω–æ, —á—Ç–æ –æ–Ω–∞ –±—É–¥–µ—Ç —Å–ª—É—á–∞–π–Ω–æ–π. –≠—Ç–æ –º–æ–∂–µ—Ç, –∫ –ø—Ä–∏–º–µ—Ä—É, –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è –ø—Ä–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö —Å—Ä–µ–¥—ã.*\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/policy2.png\"  style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value - function\n",
    "\n",
    "–î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –≤–≤–æ–¥—è—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–∏ V –∏ Q \n",
    "\n",
    "\n",
    "\n",
    "V(s) - –∫–∞–∫–æ–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å* –µ—Å–ª–∏ –Ω–∞—á–∞—Ç—å –¥–≤–∏–≥–∞—Ç—å—Å—è –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è s.\n",
    "\n",
    "* –í —Ç–µ—á–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏\n",
    "\n",
    "Q(s,a) -  –∞–Ω–ª–æ–≥–∏—á–Ω–æ, –Ω–æ –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ —á—Ç–æ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ s (s0 = s) –±—ã–ª–æ –≤—ã–±—Ä–∞–Ω–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-28.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "–ü–æ–¥ $E_\\pi$ –º—ã –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–∏—Ç–∫–∏ –∏ —Å—Ä–µ–¥—ã\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-29_.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "–í –≤–µ—Ä—à–∏–Ω–∞—Ö –∑–Ω–∞—á–µ–Ω–∏—è V –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ $v_\\pi$ (–µ—Ä—Ö—É –∫–∞—Ä—Ç–∏–Ω–∫–∏)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ—à–µ–Ω–∏–µ\n",
    "\n",
    "–ó–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–µ—à–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö V* –∏ Q* - —Ñ—É–Ω–∫—Ü–∏–∏.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-37.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation\n",
    "\n",
    "–î–ª—è –µ–µ —Ä–µ—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ [–ë–µ–ª–º–∞–Ω–∞](https://en.wikipedia.org/wiki/Richard_E._Bellman)\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/bellman.png\" /> \n",
    "\n",
    "Q —Ñ—É–Ω–∫—Ü–∏—è —É–±–∏—Ä–∞–µ—Ç —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –ø–µ—Ä–≤–æ–º —à–∞–≥–µ. –° –ø–æ–º–æ—â—å—é –Ω–µ–µ –º—ã —Ñ–∏–∫—Å–∏—Ä—É–µ–º –Ω–∞—à –ø–µ—Ä–≤—ã–π —à–∞–≥ –∏–∑ —ç—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.\n",
    "\n",
    "\n",
    "–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º—ã –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å Q –∏ V —Ç–∞–∫:\n",
    "\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n",
    "–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è V –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Q \n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-39_.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ q –∑–∞–¥–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –í–∞—Ä–∏–∞–Ω—Ç—ã —Ä–µ—à–µ–Ω–∏–π\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-48.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "\n",
    "–ê–ª–≥–æ—Ä–∏—Ç–º\n",
    "\n",
    "1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "- –°–æ–∑–¥–∞–µ–º –º–∞—Å—Å–∏–≤ V —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–∞–≤–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ—Å—Ç–æ—è–Ω–∏–π.\n",
    "\n",
    "- –ó–∞–ø–æ–ª–Ω—è–µ–º –µ–≥–æ –Ω—É–ª—è–º–∏\n",
    "\n",
    "\n",
    "2. –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ (Policy evaluation)\n",
    "\n",
    "- –î–ª—è –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å—á–∏—Ç–∞–µ–º Q(s,a)\n",
    "\n",
    "- –û–±–Ω–æ–≤–ª—è–µ–º –º–∞—Å—Å–∏–≤ V[s] -> max(Q(s,a))\n",
    "\n",
    "\n",
    "*–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç Policy iteration —Å–∞–º–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ –≤ –ø–∞–º—è—Ç–∏ –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—å—Å—è, –æ–Ω–∞ –≥–µ–Ω–µ–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ Q-—Ñ—É–Ω–∫—Ü–∏–∏.\n",
    "\n",
    "\n",
    "3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ (Policy improvement)\n",
    "\n",
    "- –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—á–∏—Ç–∞–µ–º Q(s,a) –¥–ª—è –≤—Å–µ—Ö a\n",
    "\n",
    "- –í—ã–±–∏—Ä–∞–µ–º a –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ Q(s,a) - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\n",
    "\n",
    "- –ï—Å–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —à–∞–≥—É 2, –∏–Ω–∞—á–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è\n",
    "\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "r(s,a,s') - –Ω–∞–≥—Ä–∞–¥–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –≤—ã–±–æ—Ä—É –¥–µ–π—Å—Ç–≤–∏—è a –≤ s\n",
    "s - –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ \n",
    "s' - –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n",
    "s' - –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "  \"\"\" \n",
    "  Computes Q(s,a) as in formula above \n",
    "\n",
    "  mdp : MDP object\n",
    "  state_values : dictionayry of { state_i : V_i }\n",
    "  state: string id of current state\n",
    "  gamma: float discount coeff\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  next_states = mdp.get_next_states(state, action)\n",
    "\n",
    "  Q = 0.0\n",
    "\n",
    "  for next_state in next_states.keys():\n",
    "    p = next_states[next_state] # alternatively p = mdp.get_transition_prob(state, action, next_state)\n",
    "    Q += p * (mdp.get_reward(state, action, next_state) + gamma * state_values[next_state])\n",
    "  return Q \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "      return 0 # Game over\n",
    "    \n",
    "    q_max = float('-inf')\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "      q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "      q_max = max(q_max, q)\n",
    "    return q_max"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import mdp as MDP\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "display(MDP.plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "\n",
    "    new_state_values = {}\n",
    "    for s in state_values.keys():\n",
    "      new_state_values[s] = get_new_state_value(mdp,state_values,s,gamma)   \n",
    "\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s])\n",
    "               for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–¥–µ—Å—å –Ω–µ—Ç –Ω–∏–∫–∞–∫–æ–≥–æ \"—Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\"!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(MDP.plot_graph_with_state_values(mdp, state_values))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    \n",
    "    best_action = None\n",
    "    q_max = float('-inf')\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "      q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "      if q > q_max:\n",
    "        best_action = a\n",
    "        q_max = q\n",
    "\n",
    "    return best_action"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(MDP.plot_graph_optimal_strategy_and_state_values(mdp, state_values, get_action_value))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q - Learning\n",
    "\n",
    "–í —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–¥—Ö–æ–¥ –æ–ø–∏—Å–∞–Ω–Ω—ã–π –≤—ã—à–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ù–∞–º –∑–∞—Ä–∞–Ω–µ–µ –Ω–µ –∏–∑–≤–µ—Å—Ç–Ω—ã –Ω–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning\n",
    "\n",
    "–º—ã –Ω–µ –∑–Ω–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤. –ú—ã –º–æ–∂–µ–º –ø–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –Ω–∏—Ö.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-19.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-31.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Q - —à—Ç—Ä–∏—Ö —Ä–∞—Å—Å—á–µ—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, Q –±–µ–∑ —à—Ç—Ä–∏—Ö–∞ —Ç–æ —á—Ç–æ –±—ã–ª–æ –≤ —Ç–∞–±–ª–∏—Ü–µ, –∞–ª—å—Ñ–∞ —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "**temporal difference**\n",
    "\n",
    "–ü—É—Å—Ç—å —É –Ω–∞—Å –µ—Å—Ç—å –æ—Ü–µ–Ω–∫–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è ùëÜùë° –∏ –¥–µ–π—Å—Ç–≤–∏—è ùê¥ùë° ùëÑ(ùëÜùë°,ùê¥ùë°). –°–¥–µ–ª–∞–µ–º –æ–¥–∏–Ω —à–∞–≥, –ø–æ–ª—É—á–∏–º —Ä–µ–∞–ª—å–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É r, –ø–æ—Å–º–æ—Ç—Ä–∏–º –æ—Ü–µ–Ω–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ ùëöùëéùë•ùëÑ(ùëÜùë°+1,ùê¥). –ù–æ –≤ –∏–¥–µ–∞–ª–µ ùëü+ùëöùëéùë•ùëÑ(ùëÜùë°,ùê¥)=ùëÑ(ùëÜùë°,ùê¥ùë°), –Ω–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑–Ω–∏—Ü–∞, —Ç–æ –µ–µ –º–æ–∂–Ω–æ –ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å.\n",
    "\n",
    "–ò–¥–µ—è TD –≤ —Ç–æ–º, —á—Ç–æ–±—ã —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –Ω–∞—à–µ–π –ø—Ä–æ—à–ª–æ–π –æ—Ü–µ–Ω–∫–æ–π –∏ —É—Ç–æ—á–Ω–µ–Ω–Ω–æ–π –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
    "\n",
    "Temporal - —Ç–∞–∫-–∫–∞–∫ –µ—Å—Ç—å –æ—Ü–µ–Ω–∫–∞ —Å–µ–π—á–∞—Å –∏ –µ—Å—Ç—å —É—Ç–æ—á–Ω–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω —à–∞–≥\n",
    "\n",
    "–í —Ç–µ–æ—Ä–∏–∏ –º–æ–∂–Ω–æ —Å–º–æ—Ç—Ä–µ—Ç—å —á–µ—Ä–µ–∑ n-—à–∞–≥–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning\n",
    "\n",
    "–ï—Å—Ç—å –∑–∞–¥–∞—á–∏ –≥–¥–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –æ–≥—Ä–æ–º–Ω–æ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω—ã–º.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-6.png\" style=\"width: 500\"/>\n",
    "\n",
    "–Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã —É—á–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º –∏–≥—Ä–∞—Ç—å –≤ –ø—Ä–æ—Å—Ç—É—é –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—É—é –∏–≥—Ä—É, —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –±—É–¥–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —ç–∫—Ä–∞–Ω–∞ (210x160).\n",
    "\n",
    "–í —Ç–∞–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö Q- —Ñ—É–Ω–∫—Ü–∏—é –Ω–µ —Å—á–∏—Ç–∞—é—Ç –≤ —è–≤–Ω–æ–º –≤–∏–¥–µ, –∞ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É—é—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é.\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-10.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "–ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ —á—Ç–æ –±—ã –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–ª–∞ Q -–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –¥–µ–π—Å—Ç–≤–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-11.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-14.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "$\\epsilon -greedy$ –Ω—É–∂–Ω–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—Ä–µ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–º–∞**:\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-16.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "–í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –æ–∫—Ä—É–∂–µ–Ω–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –ø–æ–ª—É—á–∞–µ–º–∞—è –∞–≥–µ–Ω—Ç–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –Ω–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ. –¢.–µ. –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π (—á—Ç–æ –ø–æ–Ω—è—Ç–Ω–æ –∏–∑ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç.–∫. –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –æ–∫—Ä—É–∂–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è RL, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –Ω–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã). –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ —É—Ö—É–¥—à–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –Ω–∞–º –Ω—É–∂–µ–Ω —Å–ø–æ—Å–æ–±, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—É—Å—Ç—Ä–∞–Ω–∏—Ç—å –∏–ª–∏ —Å–Ω–∏–∑–∏—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏). \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-62.png\" alt=\"Drawing\" />\n",
    "\n",
    "–û–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ **–ø—Ä–æ–∏–≥—Ä—ã–≤–∞–Ω–∏—è –æ–ø—ã—Ç–∞ (experience replay)**. –°—É—Ç—å —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≤ —Ç–æ–º, —á—Ç–æ –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å–æ—Å—Ç–æ—è–Ω–∏–µ, –¥–µ–π—Å—Ç–≤–∏—è, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ) –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–º –±—É—Ñ–µ—Ä–µ –∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –º–∏–Ω–∏-–±–∞—Ç—á–∏ –∏–∑ —ç—Ç–æ–≥–æ –±—É—Ñ–µ—Ä–∞.\n",
    "\n",
    "–¢–∞–∫ –∂–µ **experience replay** –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–π –ø—Ä–æ—à–ª—ã–π –æ–ø—ã—Ç.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ—Ä c CartPole DQN\n",
    "\n",
    "Deep Q Network  = DQN \n",
    "\n",
    "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
    "\n",
    "CartPole is the simplest one. It should take several minutes to solve it.\n",
    "\n",
    "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "################################################\n",
    "# For CartPole\n",
    "################################################\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    \n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/atari_wrappers.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/utils.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/replay_buffer.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/framebuffer.py\n",
    "\n",
    "!pip install gym[box2d]\n",
    "\n",
    "!touch .setup_complete\n",
    "\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gym\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env = make_env()\n",
    "env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map observations to state q-values.\n",
    "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device\n",
    "print(state_shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        \n",
    "        \n",
    "        # Define NN\n",
    "        ##############################################\n",
    "        hidden_size = 150\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        ##############################################\n",
    "\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        \n",
    "        ##############################################\n",
    "        qvalues = self._nn(state_t)\n",
    "        ##############################################\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2 and \n",
    "            qvalues.shape[0] == state_t.shape[0] and \n",
    "            qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer and Target Networks\n",
    "\n",
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "\n",
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Loss\n",
    "\n",
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # with torch.no_grad():\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    ##############################################\n",
    "    next_state_values = predicted_next_qvalues.max(axis=-1)[0]\n",
    "    ##############################################\n",
    "\n",
    "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
    "        \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    ###############################################\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * is_not_done\n",
    "    ##############################################\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
    "            \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim() == 1, \\\n",
    "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
    "            \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# your favourite random seed\n",
    "seed = 2\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = make_env(seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∞–≥–µ–Ω—Ç –¥–ª—è –∏–≥—Ä—ã, –≤—Ç–æ—Ä–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü–µ—Ä–µ–æ–¥–∏—á–µ—Å–∫–∏ –≤–µ—Å–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–∞–µ–Ω—Ç–∞ –∫–æ–ø–∏—Ä—É—é—Ç—Å—è –≤ \"–∏–≥—Ä–æ–∫–∞\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \n",
    "    hint: use agent.sample.actions\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    # <YOUR CODE>\n",
    "    for _ in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        \n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        # action = action.argmax(axis=-1)[0]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        sum_rewards += reward\n",
    "        \n",
    "        exp_replay.add(s, action, reward, state, done)\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        \n",
    "        s = state\n",
    "        \n",
    "    \n",
    "\n",
    "    return sum_rewards, s"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import src.L15_RL.lib.utils\n",
    "import utils\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "state = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print('less that 100 Mb RAM available, freezing')\n",
    "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        # <YOUR CODE: sample batch_size of data from experience replay>\n",
    "        s,a,r,next_s, is_done = exp_replay.sample(batch_size)\n",
    "        # loss = <YOUR CODE: compute TD loss>\n",
    "        loss = compute_td_loss(s, a, r, next_s, is_done, agent, target_network)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            # <YOUR CODE>\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(evaluate(\n",
    "                make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues(\n",
    "                [make_env(seed=step).reset()]\n",
    "            )\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "final_score = evaluate(\n",
    "  make_env(),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)\n",
    "assert final_score > 300, 'not good enough for DQN'\n",
    "print('Well done')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/approaches.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞:\n",
    "\n",
    "* Barto Sutton Reinforcement Learning: An Introduction (–∫–ª–∞—Å—Å–∏–∫–∞ —Ç–µ–æ—Ä–∏–∏)\n",
    "* Lapan Deep Reinforcement Learning Hands-On Second Edition (–ö–Ω–∏–≥–∞ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ RL)\n",
    "* Sudharsan Ravichandiran Hands-On Reinforcement Learning with Python (–¢–æ–∂–µ –º–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤)\n",
    "\n",
    "–ü—Ä–æ—á–∞—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞:\n",
    "\n",
    "* Alex Zai and Brandon Brown Deep Reinforcement Learning in Action\n",
    "\n",
    "\n",
    "–ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏:\n",
    "* https://www.alexirpan.com/2018/02/14/rl-hard.html Deep Reinforcement Learning Doesn't Work Yet\n",
    "* https://openai.com/blog/faulty-reward-functions/  Faulty Reward Functions in the Wild"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "L15_RL_gan.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 }
}
