{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "relative-skill",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Рекомендованная литература:\n",
    "\n",
    "* Barto Sutton Reinforcement Learning: An Introduction (классика теории)\n",
    "* Lapan Deep Reinforcement Learning Hands-On Second Edition (Книга с большим количеством примеров по RL)\n",
    "* Sudharsan Ravichandiran Hands-On Reinforcement Learning with Python (Тоже много примеров)\n",
    "\n",
    "Прочая дополнительная литература:\n",
    "\n",
    "* Alex Zai and Brandon Brown Deep Reinforcement Learning in Action\n",
    "\n",
    "\n",
    "Полезные ссылки:\n",
    "* https://www.alexirpan.com/2018/02/14/rl-hard.html Deep Reinforcement Learning Doesn't Work Yet\n",
    "* https://openai.com/blog/faulty-reward-functions/  Faulty Reward Functions in the Wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "impressed-screen",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5da78187a78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "        \n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/atari_wrappers.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/utils.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/replay_buffer.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/framebuffer.py\n",
    "\n",
    "    !pip install gym[box2d]\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hearing-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-worst",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-welsh",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Обучение с учителем\n",
    "Если у нас есть набор примеров с правильными ответами, то мы используем эту выборку для обучения нашей модели, а после обучения, применяем её к неразмеченным данным. Именно этот подход мы использовали, когда обучали классификатор для MNIST, подавая на вход сети картинки с изображениями рукописных цифр и считая градиент для подстройки весов на основе разницы между известным лэйблом цифры и выходом нейросети.\n",
    "### Обучение без учителя\n",
    "В некоторых случаях у нас нет размеченных данных, на которых мы могли бы заранее обучить модель. Но, при решении некоторых задач, можно обойтись без размеченной выборки. Примером такой задачи является задача кластеризации.\n",
    "### Обучение с подкреплением\n",
    "В некоторых случаях существующие методы обучения без учителя нам не подходят. В то же время у нас нет возможности создать качественную обучающую выборку. При этом мы можем постфактум оценить действия нашей модели и использовать эту оценку подстроить модель так, чтобы она чаще совершала желательные действия и реже - нежелательные. В литературе такую оценку называют вознаграждением (reward), а обучение строится таким образом, чтобы это модель стремилась максимизировать получаемое вознаграждение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-spice",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-7.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-17.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-holmes",
   "metadata": {},
   "source": [
    "## Терминология: агент, функция награды, состояние среды\n",
    "\n",
    "Агент и среда - ключевые понятия в обучении с подкрелением.\n",
    "\n",
    "**Агент** - программа, принимающая решение о дальнейших действиях на основе информации о состоянии среды.\n",
    "\n",
    "**Среда** - это мир, в котором агент должен \"выживать\", т.е. всё, с чем агент может прямо или косвенно взаимодействовать. Среда обладает состоянием (State), агент может влиять на среду, совершая какие-то действия (Actions), переводя среду при этом из одного состояния в другое и получая какое-то вознаграждение. Среда описывается пространством возможных состояний. Конкретное состояние - вектор в этом пространстве.\n",
    "\n",
    "В зависимости от конкретной задачи, агент может наблюдать либо полное состояние среды, либо только некоторую его часть. Во втором случае, агенту может потребоваться какое-то внутреннее представление полного состояния, которое будет обновляться по мере получения новых данных.\n",
    "\n",
    "**Фукнция награды** - вводимая программистом формула вычисления ценности действия на основе финального результата, ожидания этого результата, промежуточных результатов и любых других параметров, которые будут подсказывать путь к наилучшей последовательности действий агента. Это некоторый аналог функции потерь, без которой непонятно чему учиться. Например, в шахматах истинная награда это победа, но взятая фигура соперника тоже ценна и должна увеличивать награду, если мы хотим подсказать агенту, что брать чужие фигуры полезно. Может ли после этого агент получать мат, позарившись на незащищенную фигуру? Да, ровно как и неопытный шахматный игрок. Попытка передать через дополнительные неосновные награды подсказки к получению основной награды называется **reward shaping**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-posting",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-8.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-9.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-quarter",
   "metadata": {},
   "source": [
    "### MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-designation",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-24_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-4.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-silver",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-25_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-gateway",
   "metadata": {},
   "source": [
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-12.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-13.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-vermont",
   "metadata": {},
   "source": [
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-15.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-17.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-occasion",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-26.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-audio",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-28.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-29_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-phase",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-30.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-31.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-essay",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-33_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-35__.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-florida",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-32_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-34_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-madagascar",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-36.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Если все записать в матричной форме, то можно увидеть, что Bellman Expectaion Equation имеет решение (что нельзя сказать о поиске оптимальной политике, как мы увидим ниже)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-sunday",
   "metadata": {},
   "source": [
    "### Давайте теперь искать наилучшую политику или наилучший способ поведения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-closure",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-37.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-38_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-measure",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-39_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-40.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-moisture",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-41.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-42_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-shanghai",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-43.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-45_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-steering",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-44_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-46_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-cooking",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-47_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-48.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-poetry",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-49.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-pennsylvania",
   "metadata": {},
   "source": [
    "## Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-atmosphere",
   "metadata": {},
   "source": [
    "### Policy evaluation\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-56.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-67.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-michigan",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-9.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-10.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-11.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-pleasure",
   "metadata": {},
   "source": [
    "### Policy improvement\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-60.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-13.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-chase",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-72.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-75.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-caution",
   "metadata": {},
   "source": [
    "## Библиотека OpenAI Gym\n",
    "\n",
    "В OpenAI создали готовую библиотеку для моделировавния, визуализирования и обучения, что не только удобно для тестов. Одно из хороших преимуществ это качественный абстрактный интерфейс взаимодействия среды и агента, который позволяет делать совместимые с Gym среды и впоследствии интегрироваться в экосистему, легко подменять типы агентов, тестировать, сравнивать.\n",
    "Среди готовых сред для взаимодействия с ними есть\n",
    "- игры Atari - полноценные компьютерные игры, типа Арканоида, где вместо человека в компьютер играет агент.\n",
    "- классические настольные пошаговые игры: шахматы, go.\n",
    "- физические симуляции, где нужно управлять физическим объектом: маятник, который должен перевернуться и балансировать за счёт раскачивания; тележка, которая должна проехать через холм за счёт раскачивания.\n",
    "- и т.д.### State-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-medication",
   "metadata": {},
   "source": [
    "### Пример окружения Gym\n",
    "В этом примере загружается готовая среда: машинка должна заехать на горку. Функция награды встроенная. А агент - случайное воздействие. Оно ничему не учится, лишь хаотически выдаёт действия, но это показывает где взять все необходимые данные для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "MAX_NUM_EPISODES = 5\n",
    "\n",
    "for episode in range(MAX_NUM_EPISODES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n",
    "    step = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        \n",
    "        action = env.action_space.sample()  # Sample random action.\n",
    "                                            # This will be replaced\n",
    "                                            # by our agent's action\n",
    "                                            # when we # start\n",
    "                                            # developing the agent algorithms\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = \\\n",
    "        env.step(action)  # Send the action to the\n",
    "                          # environment and receive       \n",
    "                          # the next_state, reward and\n",
    "                          # whether done or not\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        obs = next_state\n",
    "    print(\"\\n Episode #{} ended in {} steps. total_reward={}\".format(episode, step+1,\n",
    "total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-chapter",
   "metadata": {},
   "source": [
    "## Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-billy",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-fight",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-14.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-16.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-murder",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-19.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-20.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-frank",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-24.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-25.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-optimum",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-27.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-28.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-dryer",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-31.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-32.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-viewer",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-33.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-34.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-playback",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-36.png\" alt=\"Drawing\" style=\"width: 500px;\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-compatibility",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-37.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-39.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-volunteer",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-40.png\" alt=\"Drawing\" style=\"width: 430px;\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-reasoning",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-44.png\" alt=\"Drawing\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-renaissance",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-48.png\" alt=\"Drawing\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-strike",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-50.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-52.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-controversy",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-53.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-54.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-memory",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-59.png\" alt=\"Drawing\" style=\"width: 450px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-florence",
   "metadata": {},
   "source": [
    "## Аппроксимация Q-функции (Q-таблицы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-litigation",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-6.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-8.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-tablet",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-9.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-10.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-article",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-11.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-12.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-welsh",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/coursera-4-24.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/coursera-4-25.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-monster",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-14.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-15.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-edgar",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-16.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-17.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-dividend",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-18.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-62.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "В большинстве окружений информация, получаемая агентом распределена не независимо. Т.е. последовательные наблюдения агента сильно коррелированы между собой (что понятно из интуитивных соображений, т.к. большинство окружений, в которых применяется RL, предполагают, что все изменения в них последовательны). Корреляция примеров ухудшает сходимость стохастического градиентного спуска. Таким образом нам нужен способ, который позволяет улучшить распределение примеров для обучения (устранить или снизить корреляцию между ними). Обычно используется метод **проигрывания опыта (experience replay)**. Суть этого метода в том, что мы сохраняем некоторое количество примеров (состояние, действия, вознаграждение) в специальном буфере и для обучения выбираем случайные мини-батчи из этого буфера.\n",
    "\n",
    "Так же **experience replay** позволяет агенту эффективнее использовать свой прошлый опыт."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-airport",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-23.png\" alt=\"Drawing\" style=\"width: 500px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-costa",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-24.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-25.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-checkout",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-26.png\" alt=\"Drawing\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-secretary",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-28.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-29.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-proxy",
   "metadata": {},
   "source": [
    "## Пример c CartPole\n",
    "\n",
    "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
    "\n",
    "CartPole is the simplest one. It should take several minutes to solve it.\n",
    "\n",
    "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "potential-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brazilian-adaptation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = make_env()\n",
    "env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-cornwall",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map observations to state q-values.\n",
    "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "disturbed-minority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device\n",
    "print(state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fewer-personal",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88e7fd6557d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        \n",
    "        \n",
    "        # Define NN\n",
    "        ##############################################\n",
    "        hidden_size = 150\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        ##############################################\n",
    "\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        \n",
    "        ##############################################\n",
    "        qvalues = self._nn(state_t)\n",
    "        ##############################################\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2 and \n",
    "            qvalues.shape[0] == state_t.shape[0] and \n",
    "            qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "protective-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-startup",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer and Target Networks\n",
    "\n",
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virgin-artwork",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'replay_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ee6078eca878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreplay_buffer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexp_replay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'replay_buffer'"
     ]
    }
   ],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "\n",
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-meditation",
   "metadata": {},
   "source": [
    "### TD-Loss\n",
    "\n",
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "functioning-repeat",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-66b645af1eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mcheck_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     device=device):\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# shape: [batch_size, *state_shape]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # with torch.no_grad():\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    ##############################################\n",
    "    next_state_values = predicted_next_qvalues.max(axis=-1)[0]\n",
    "    ##############################################\n",
    "\n",
    "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
    "        \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    ###############################################\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * is_not_done\n",
    "    ##############################################\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
    "            \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim() == 1, \\\n",
    "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
    "            \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-action",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "foreign-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "robust-thought",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a7f32f7e56a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# your favourite random seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# your favourite random seed\n",
    "seed = 2\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = make_env(seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "practical-twins",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ReplayBuffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-10373fbc09de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mREPLAY_BUFFER_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexp_replay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREPLAY_BUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_enough_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_available_gb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ReplayBuffer' is not defined"
     ]
    }
   ],
   "source": [
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gorgeous-mattress",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-888783097f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdecay_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minit_epsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "obvious-queensland",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fec580d4d12b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_enough_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'less that 100 Mb RAM available, freezing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print('less that 100 Mb RAM available, freezing')\n",
    "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        # <YOUR CODE: sample batch_size of data from experience replay>\n",
    "        s,a,r,next_s, is_done = exp_replay.sample(batch_size)\n",
    "        # loss = <YOUR CODE: compute TD loss>\n",
    "        loss = compute_td_loss(s, a, r, next_s, is_done, agent, target_network)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            # <YOUR CODE>\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(evaluate(\n",
    "                make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues(\n",
    "                [make_env(seed=step).reset()]\n",
    "            )\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-tsunami",
   "metadata": {},
   "source": [
    "## Дальнейшие идеи для улучшения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-twelve",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-30.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-31.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-pitch",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-32.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-33.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-alarm",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-34.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-35.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-philosophy",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-36.png\" alt=\"Drawing\" style=\"width: 500px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-thailand",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-37.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-38.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "Одной из проблем Q-Networks является неустойчивость. Часто разность ожидаемых вознаграждений для различных действий близка и поскольку выбор действия производится с помощью argmax, то выброс в данных может привести к тому, что выбираемое действие изменится. Для того, чтобы повысить стабильность используется техника **Target Q-Network**. Суть в том, что мы замораживаем веса нашей сети на фиксированное число шагов и затем используем её для вычисления функции ошибки и обучения второй сети. Периодически копируем из веса рабочей сети в Target Q-Network.\n",
    "В следующем примере мы будем делать это каждый эпизод."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-collins",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-39.png\" alt=\"Drawing\" style=\"width: 500px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-picture",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-48.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-49.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-dividend",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-50.png\" alt=\"Drawing\" style=\"width: 500px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-revolution",
   "metadata": {},
   "source": [
    "# Другие улучшения DQN\n",
    "\n",
    "## Prioritized Experience Replay\n",
    "\n",
    "Минибатчи из памяти выбираются не с равномерным распределением, а добавляем туда больше примеров, в которых предсказанные значения Q сильнее всего отличаются от корректных. Т.е. примеры с максимальным **TD error** получают максимальный приоритет.\n",
    "\n",
    "## Dueling networks\n",
    "\n",
    "Основная идея в том, что мы разделяем нашу сеть на две головы, одна из которых предсказывает абсолютное значение состояния \\\\( V(S) \\\\), а вторая - относительное преимущество одний действий над другими \\\\( A(s, a) = Q(s, a) - V(s) \\\\). Это преимущество называется advantage. Далее из этих двух значений мы собираем нашу Q-функцию, как \\\\( Q(s,a) = V(s) + A(a) \\\\)\n",
    "\n",
    "\n",
    "## Multi-step learning/n-step learning\n",
    "\n",
    "Основная идея в том, чтобы считать функцию ценности не по двум соседним примерам, а сразу по n. Это позволяет сети лучше запоминать длинные последовательности действий.\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "Детерминированное значение Q заменяется случайным распределением Z с некоторыми параметрами, которые определяются в ходе обучения.\n",
    "\n",
    "# Rainbow\n",
    "\n",
    "Rainbow - набор перечисленных выше твиков. \n",
    "\n",
    "# R2D2\n",
    "\n",
    "Использование RNN сетей для experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-bernard",
   "metadata": {},
   "source": [
    "## Резюме по типам алгоритмов RL\n",
    "\n",
    "### Model-free / Model-based\n",
    "Model-free не строит модель окружения или функции награды. Это проще. Model-based алгоритм пытается предсказывать, каким будет следующее состояние окружения или вознаграждение. Это позволяет мыслить на несколько шагов вперёд. Например, совершенно не больно прыгать с крыши. Больно потом, когда разбиваешься о землю. Модель среды и награды позволяет принимать решения не прыгать с крыши, хотя и model-free подход позволяет это понять, хоть и более сложно и грубо.\n",
    "### Value-based / policy-based\n",
    "Policy-based  методы оптимизируют напрямую функцию принятия решения агента. Стратегия (policy) обычно представлена распределением вероятности доступных действий. Value-based метод оптимизирует оценку вознаграждения для всех действий и выбирает выбирает то действие, по которому прогнозируется большее значение. Методы, основанные на Policy Gradients лучше работают при большой размерности пространства действий, а Value-based методы, такие, как Deep Q-Learning требуют меньшего количества повторений для сходимости при малой размерности.\n",
    "### On-Policy / Off-Policy\n",
    "Off-policy подход позволяет учиться на исторических данных или на записанных заранее действиях человека. On-policy - только на собственных действиях агента. Это довольно важное деление, так как обучение на собственных действиях просто недоступно для многих задач. Плохо ли автомобилю с автопилотом въехать в стену? Надо попробовать, так будет работать on-policy метод. А лучше взять понимание, что это плохо из готовых данных, синтетических, например и усвоить этот опыт. Даже если мы используем симуляцию среды, а не реальную среду, то количество эпизодов симуляции обычно порядка сотен тысяч и их симуляция съедает много времени. Каждый раз проходить переобучение с нуля очень неудобно, а возможность импортировать накопленный опыт симуляции среды сильно экономит ресурсы. Однако off-policy методы можно применить не всегда.\n",
    "### Deterministic Policy / Stochastic Policy\n",
    "В зависимости от среды, наша стратегия может быть либо детерминированной - выбираем сразу определённое действие с помощью argmax, либо стохастической, когда мы окончательное решение принимается с помощью генератора случайных чисел на основе распределения вероятности, выданного сетью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-myrtle",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "## DQN\n",
    "\n",
    "## C51\n",
    "## Monte-Carlo Tree Search\n",
    "Silver L8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-chance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
