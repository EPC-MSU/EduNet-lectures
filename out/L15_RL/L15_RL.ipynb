{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complimentary-document",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Рекомендованная литература:\n",
    "\n",
    "* Barto Sutton Reinforcement Learning: An Introduction (классика теории)\n",
    "* Lapan Deep Reinforcement Learning Hands-On Second Edition (Книга с большим количеством примеров по RL)\n",
    "* Sudharsan Ravichandiran Hands-On Reinforcement Learning with Python (Тоже много примеров)\n",
    "\n",
    "Прочая дополнительная литература:\n",
    "\n",
    "* Alex Zai and Brandon Brown Deep Reinforcement Learning in Action\n",
    "\n",
    "\n",
    "Полезные ссылки:\n",
    "* https://www.alexirpan.com/2018/02/14/rl-hard.html Deep Reinforcement Learning Doesn't Work Yet\n",
    "* https://openai.com/blog/faulty-reward-functions/  Faulty Reward Functions in the Wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "################################################\n",
    "# For MDP\n",
    "################################################\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
    "    !touch .setup_complete\n",
    "    \n",
    "else:\n",
    "    sys.path.append(\"../../src/L15_RL/lib\")\n",
    "    print('Not Colab')\n",
    "\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !rm .setup_complete\n",
    "    \n",
    "\n",
    "################################################\n",
    "# For CartPole\n",
    "################################################\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "        \n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/atari_wrappers.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/utils.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/replay_buffer.py\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/framebuffer.py\n",
    "\n",
    "    !pip install gym[box2d]\n",
    "\n",
    "    !touch .setup_complete\n",
    "else:\n",
    "    sys.path.append(\"../../src/L15_RL/lib\")\n",
    "    print('Not Colab')  \n",
    "    \n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "# ../../src/L15_RL/lib/\n",
    "\n",
    "# import src.L15_RL.lib.utils\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-wedding",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-sunset",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Обучение с учителем\n",
    "Если у нас есть набор примеров с правильными ответами, то мы используем эту выборку для обучения нашей модели, а после обучения, применяем её к неразмеченным данным. Именно этот подход мы использовали, когда обучали классификатор для MNIST, подавая на вход сети картинки с изображениями рукописных цифр и считая градиент для подстройки весов на основе разницы между известным лэйблом цифры и выходом нейросети.\n",
    "### Обучение без учителя\n",
    "В некоторых случаях у нас нет размеченных данных, на которых мы могли бы заранее обучить модель. Но, при решении некоторых задач, можно обойтись без размеченной выборки. Примером такой задачи является задача кластеризации.\n",
    "### Обучение с подкреплением\n",
    "В некоторых случаях существующие методы обучения без учителя нам не подходят. В то же время у нас нет возможности создать качественную обучающую выборку. При этом мы можем постфактум оценить действия нашей модели и использовать эту оценку подстроить модель так, чтобы она чаще совершала желательные действия и реже - нежелательные. В литературе такую оценку называют вознаграждением (reward), а обучение строится таким образом, чтобы это модель стремилась максимизировать получаемое вознаграждение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-russia",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-7.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-17.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-arbitration",
   "metadata": {},
   "source": [
    "## Терминология: агент, функция награды, состояние среды\n",
    "\n",
    "Агент и среда - ключевые понятия в обучении с подкрелением.\n",
    "\n",
    "**Агент** - программа, принимающая решение о дальнейших действиях на основе информации о состоянии среды.\n",
    "\n",
    "**Среда** - это мир, в котором агент должен \"выживать\", т.е. всё, с чем агент может прямо или косвенно взаимодействовать. Среда обладает состоянием (State), агент может влиять на среду, совершая какие-то действия (Actions), переводя среду при этом из одного состояния в другое и получая какое-то вознаграждение. Среда описывается пространством возможных состояний. Конкретное состояние - вектор в этом пространстве.\n",
    "\n",
    "В зависимости от конкретной задачи, агент может наблюдать либо полное состояние среды, либо только некоторую его часть. Во втором случае, агенту может потребоваться какое-то внутреннее представление полного состояния, которое будет обновляться по мере получения новых данных.\n",
    "\n",
    "**Фукнция награды** - вводимая программистом формула вычисления ценности действия на основе финального результата, ожидания этого результата, промежуточных результатов и любых других параметров, которые будут подсказывать путь к наилучшей последовательности действий агента. Это некоторый аналог функции потерь, без которой непонятно чему учиться. Например, в шахматах истинная награда это победа, но взятая фигура соперника тоже ценна и должна увеличивать награду, если мы хотим подсказать агенту, что брать чужие фигуры полезно. Может ли после этого агент получать мат, позарившись на незащищенную фигуру? Да, ровно как и неопытный шахматный игрок. Попытка передать через дополнительные неосновные награды подсказки к получению основной награды называется **reward shaping**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-cherry",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-8.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-9.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Этот список ожно дополнить несколькими примерамии:\n",
    "* drug discovery: обычно функции отражающие полезность лекарства недифференицруемы и можно искать оптимальные, вводя RL\n",
    "* не так давно одна российская металлургическая компания использовала RL для оптимаизации работы прокатного стана\n",
    "* NAS (Neural Architecture Search): при поиске оптимальных топологий сети можно решать задачу с помощью RL, награждая нашего агента за нахождение ххороших топологий и наказывая за плохие"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-episode",
   "metadata": {},
   "source": [
    "### MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-envelope",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-24_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-4.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-skirt",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-25_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-budapest",
   "metadata": {},
   "source": [
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-12.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-13.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Введние discounting необходимо по целому ряду причин. Одной из основных причин явялется необходимость ограничения максимальной суммы наград (для избегания бесконечных циклов вознаграждения). В самом деле:\n",
    "\n",
    "Пусть $R_{max} = max R_i$ - максимальная награда\n",
    "$G_t = R_{t+1} + \\gamma * R_{t+2} + \\gamma^2 * R_{t+3} + ... \\le R_{max} + \\gamma * R_{max} + \\gamma^2 * R_{max} + ... = R_{max} (1 + \\gamma + \\gamma^2 + ... ) \\le R_{max} * \\frac{1}{1-\\gamma} <= const $\n",
    "\n",
    "Также, это отражает поведения животных и человека, который больше предпочитают моментальную награду, чем отложенную на будущее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-triple",
   "metadata": {},
   "source": [
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-15.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-17.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-dispatch",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-26.png\" alt=\"Drawing\" width=\"500\">\n",
    "</tr></table>\n",
    "\n",
    "Стоит отметить, что политика, как следует из определения не обязательно должна быть жестко зафиксированной. Вполне возможно, что она будет случайной. Это может, к примеру, пригодиться при исследованиях среды."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-election",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-28.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-29_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Зная теперь понятия MDP и политики нам хочется находить такую политику, которая будет максимизировать наши потенциальные выигрыши $G_t$.\n",
    "\n",
    "\n",
    "Для этого нам понадобятся  две важнейшие в RL функции: V и Q. \n",
    "\n",
    "* $V_{\\pi}(s)$ является матожиданием дисконтрованных наград, которые мы получим если будем из состояния s действовать согласно политике $\\pi$\n",
    "\n",
    "* $Q_{\\pi}(s, a)$ является матожиданием дисконтрованных наград, которые мы получим если сделаем из состояния s действие a и **дальше** будем действовать согласно политике $\\pi$\n",
    "\n",
    "То есть в q мы в отличии от v фиксируем наше первое действие (не обязательно соответсвующее политике), а в v мы первое действие выбираем согалсно политике $\\pi$\n",
    "\n",
    "Под $E_\\pi$ мы подразумеваем стохастичность и политки и среды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-wallpaper",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-30.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-31.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "* Вместо $q_\\pi(S_{t+1}, A_{t+1})$ должна быть $v_\\pi(S_{t+1})$\n",
    "\n",
    "Q функция убирает стохастичность политики на первом шаге. С помощью нее мы фиксируем наш первый шаг из этого состояния.\n",
    "\n",
    "На правом слайде мы расписываем v как сумму произведений вероятностей действий (это определяет политика) и q функций "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-arctic",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-33_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-35__.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Распишем $E_\\pi$ глубже. Здесь еще добавляется в явном виде случайность среды, которая в зависимости от текущего s и выбранного a, выдает нам новый s' и r. На диаграмме получается, что верхние ребра дерева символизируют случайность политики, а нижние - случайность среды\n",
    "\n",
    "\n",
    "На правом слайде рассчитаны значени v функции для случайной политик (каждое действие в каждом состоянии равновероятно)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-implementation",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-32_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-34_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Похожее раскрытие матожидания можно проделать и для Q, но в нем сначала будет идти случайность среды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-pleasure",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-36.png\" alt=\"Drawing\" width=\"500\">\n",
    "\n",
    "Если все записать в матричной форме, то можно увидеть, что Bellman Expectaion Equation имеет решение (что нельзя сказать о поиске оптимальной политике, как мы увидим ниже)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-deputy",
   "metadata": {},
   "source": [
    "### Давайте теперь искать наилучшую политику или наилучший способ поведения\n",
    "\n",
    "Умея сравнивать ожидания от политик в кажом состоянии не сложно начать сравнивать политики целиком "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-charity",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-37.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-38_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Для начала определим оптимальные функции v и q\n",
    "\n",
    "Но знание об оптимальных q эквивалентно знанию об оптимальном действии (А почему?)\n",
    "\n",
    "На графе MDP мы расситали оптимальные значения V (алгоритм их получения вы узнаете чуть позже)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-carolina",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-39_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-40.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "В примере подсчитаны q* для кажого конкретного действия и приписаны к ребрам, отвечающим за эти действия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-truth",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-41.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-42_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-planning",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-43.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-45_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Зная оптималную оценку мы должны действовать жадно, если хотим ее достичь. В самом деле, любое другое действие снизит наш выигрыш"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-possibility",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-44_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-46_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "* Эти условия и есть уравнения оптимальности Белммана. Их можно интерпретировать следующим образом: если мы точно знаем оптимальные q функции в будущем (то есть, знаем сколько мы получим наград, если из будущих состояний будем действовать определенным способом) и знаем моментальные награды, то из всех доступных нам сейчас действия нужно выбрать, то которое максимизирует сумму нашей немедленной награды и будущих наград при условии оптимального поведения в будущем. ~ **На каждом шаге следует выбирать оптимальное решение в предположении об оптимальности всех последущих шагов**.\n",
    "\n",
    "* Но какое поведение на следюущем шаге при известных Q функциях может быть оптимальным? Очевидно, то которое максимизирует Q.\n",
    "\n",
    "* А принцип оптимальности Беллмана говорит следующее: оптимальная стратегия имеет свойство, что какими бы ни были начальное состояние и начальное решение, последующие решения должны составлять оптимальный курс действий по отношению к состоянию, полученному в результате первого решения ~ **оптимальная стратегия зависит только от  текущего состояния и цели и не зависит от предыстории**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-kitty",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-47_.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-48.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-jersey",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-49.png\" alt=\"Drawing\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-bikini",
   "metadata": {},
   "source": [
    "## Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-friendship",
   "metadata": {},
   "source": [
    "### Policy evaluation\n",
    "\n",
    "Если на картинках, привденных выше с игрушечным MDP попробовать самому просчитать $v_\\pi$, то можно заметить, что вы со временем приедете к рекурсии и не очень понятно что с этим делать. В теории это можно превратить в СЛАУ и решить, но это нам не очень подойдет. \n",
    "\n",
    "Тут можно воспользоваться идеями динамического программирования, так как благодаря уравнениям Беллмана у нас есть рекурсивное разложение => оптимальная подструктура. А также можно переиспользовать полученные ранее оценки v\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-56.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-67.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Идея bellman evaluation заключается в том, чтобы много раз применять bellman expectation equation. И можно доказать, что этот процесс сойдется в итоге к точной оценке значений $V_\\pi (s)$ для всех s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-translation",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-7.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-9.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-10.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-11.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "На слайдах выше слева представлены, получаемые итеративно оценки $V_\\pi$, а справа жадная плоитика на основе этих оценок. Как мы видим на слайдах справа, со временем мы сходимся к оптимальной политике (почему так происходит подробнее объясняется ниже). Интуитивно это объясняется так: жадная политика гарантированно будет только улучшением по сравнению со случайной политикой, и в конце концов это нас приведет к оптимальной политике"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-album",
   "metadata": {},
   "source": [
    "### Policy improvement\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-60.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-13.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Можно доказать, что такой жадный выбор не ухудшает, а возможно, улучшает политику. Это policy improvement theorem (док-во смотри в BurtoSutton стр 78)\n",
    "\n",
    "Идея Generalized policy iteration\n",
    "* Когда мы выполнили процедуру оценки - наша политика перестала быть жадной относительно новой v функции. \n",
    "* И наоборот, когда мы выбралии жадные действия - наша оценка v перестала ей соответсвовать\n",
    "\n",
    "Выполняя этот цикл гаша политика сходится к оптимальной\n",
    "\n",
    "Также не имеет значения с какой политики вы стартовали, все равно данный метод сойдется. (Это следует из теоремы Банаха)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-organizer",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-72.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-75.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-mirror",
   "metadata": {},
   "source": [
    "### Value iteration\n",
    "\n",
    "Одним из недостатков policy iteration заключается в том, что каждая из ее итераций включает в себя оценку политики, которая сама по себе может быть длительным итеративным вычислением, требующим многократных проходов по набору состояний. Если оценка политики выполняется итеративно, то сходимость точно к $V_\\pi$ происходит только в пределе. Должны ли мы ждать точного схождения, или мы можем не останавливаться на достигнутом? Пример с жадным выбором политик выше показывает, что можно усечь этап policy evaluation. В этом примере итерации policy evaluation за пределами первых трех не влияют на соответствующую жадную политику.\n",
    "\n",
    "Фактически, этап policy evaluation of policy iteration  может быть сокращен различными способами без потери гарантий сходимости policy iteration. Один важный особый случай - это когда оценка политики останавливается сразу после одного цикла (одно обновление каждого состояния). Этот алгоритм называется value iteration. Его можно записать как  простую операцию обновления, которая сочетает в себе этапы улучшения политики и усеченной policy evaluation (один шаг).\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-77.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-78.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "#### Отличие value iteration и policy iteration\n",
    "Де факто value iteration это policy iteration с одним циклом policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-enlargement",
   "metadata": {},
   "source": [
    "## Библиотека OpenAI Gym\n",
    "\n",
    "В OpenAI создали готовую библиотеку для моделировавния, визуализирования и обучения, что не только удобно для тестов. Одно из хороших преимуществ это качественный абстрактный интерфейс взаимодействия среды и агента, который позволяет делать совместимые с Gym среды и впоследствии интегрироваться в экосистему, легко подменять типы агентов, тестировать, сравнивать.\n",
    "Среди готовых сред для взаимодействия с ними есть\n",
    "- игры Atari - полноценные компьютерные игры, типа Арканоида, где вместо человека в компьютер играет агент.\n",
    "- классические настольные пошаговые игры: шахматы, go.\n",
    "- физические симуляции, где нужно управлять физическим объектом: маятник, который должен перевернуться и балансировать за счёт раскачивания; тележка, которая должна проехать через холм за счёт раскачивания.\n",
    "- и т.д.### State-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-basket",
   "metadata": {},
   "source": [
    "### Пример окружения Gym\n",
    "В этом примере загружается готовая среда: машинка должна заехать на горку. Функция награды встроенная. А агент - случайное воздействие. Оно ничему не учится, лишь хаотически выдаёт действия, но это показывает где взять все необходимые данные для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "MAX_NUM_EPISODES = 5\n",
    "\n",
    "for episode in range(MAX_NUM_EPISODES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n",
    "    step = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        \n",
    "        \n",
    "        \n",
    "        action = env.action_space.sample()  # Sample random action.\n",
    "                                            # This will be replaced\n",
    "                                            # by our agent's action\n",
    "                                            # when we # start\n",
    "                                            # developing the agent algorithms\n",
    "        \n",
    "        \n",
    "        next_state, reward, done, info = \\\n",
    "        env.step(action)  # Send the action to the\n",
    "                          # environment and receive       \n",
    "                          # the next_state, reward and\n",
    "                          # whether done or not\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        obs = next_state\n",
    "    print(\"\\n Episode #{} ended in {} steps. total_reward={}\".format(episode, step+1,\n",
    "total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-macedonia",
   "metadata": {},
   "source": [
    "## MDP practice\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" alt=\"Diagram by Waldoalvarez via Wikimedia Commons, CC BY-SA 4.0\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "# We can now use MDP just as any other gym environment:\n",
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP methods\n",
    "\n",
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "\n",
    "# state, action, next_state\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "\n",
    "# get_transition_prob(self, state, action, next_state)\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-situation",
   "metadata": {},
   "source": [
    "### Value iteration\n",
    "\n",
    "Давайте теперь попробуем решить задачу максимизации на этом MDP с помощью __V__alue __I__teration__\n",
    "\n",
    "Псевдокод VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Для начала, реализуем функцию для вычисление state-action value function $Q^{\\pi}$, которая определяется следующей формулой: \n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration\n",
    "\n",
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"    \n",
    "    next_states = mdp.get_next_states(state, action)\n",
    "    \n",
    "    Q = 0.0\n",
    "    \n",
    "    for next_state in next_states:\n",
    "        p = mdp.get_transition_prob(state, action, next_state)\n",
    "        # state, action, next_state\n",
    "        Q += p * (mdp.get_reward(state, action, next_state) + gamma * state_values[next_state])\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-experience",
   "metadata": {},
   "source": [
    "С помощью $Q(s,a)$ мы можем теперь определить уравнение для получения \"следующего\" V(s) в value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    Qs = [get_action_value(mdp, state_values, state, action, gamma) for action in actions]\n",
    "    \n",
    "    return max(Qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-layer",
   "metadata": {},
   "source": [
    "Наконец, объединим все в единый работающий алгоритм value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    # new_state_values = <YOUR CODE>\n",
    "    new_state_values = {state: get_new_state_value(mdp, state_values, state, gamma) for state in mdp.get_all_states()}\n",
    "\n",
    "    assert isinstance(new_state_values, dict)\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s])\n",
    "               for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-insert",
   "metadata": {},
   "source": [
    "Теперь, давайте использовать $V^{*}(s)$ для получения оптимальных действий (optimal actions) для всех состояний:\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "Единственное отличие от поиска V (s) заключается в том, что здесь мы берем не max, а argmax для того, чтобы найти действие с максимальным Q (s, a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    Qs = {action: get_action_value(mdp, state_values, state, action, gamma) for action in actions}\n",
    "    \n",
    "    return max(Qs, key=Qs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-allocation",
   "metadata": {},
   "source": [
    "## HW 1:\n",
    "Протестировать value iteration на Frozen Lake. Сделать policy iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-intro",
   "metadata": {},
   "source": [
    "## Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-tennessee",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-danger",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-14.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-16.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Так-как p(s',r|s,a) нам не дали, то остается только что-то аппроксимировать. Саму p аппроксимировать дорого, поэтому лучше q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-level",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-19.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-20.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Напоминание $G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + ..$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-kitchen",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-24.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-25.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-summit",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-27.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-28.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Пусть у нас есть оценка для текущего состояния $S_t$ и действия $A_t$ $Q(S_t, A_t)$.\n",
    "Сделаем один шаг, получим реальную награду r, посмотрим оценку на следующем шаге $maxQ(S_{t+1}, A)$.\n",
    "Но в идеале $r + maxQ(S_t, A) = Q(S_t, A_t)$, но если есть разница, то ее можно пытаться исправить.\n",
    "\n",
    "Идея TD в том, чтобы смотреть на разницу между нашей прошлой оценкой и уточненной и использовать эту информацию\n",
    "\n",
    "Temporal - так-как есть оценка сейчас и есть уточненная оценка через один шаг\n",
    "\n",
    "В теории можно смотреть через n-шагов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-agreement",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-31.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-32.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-tiffany",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-33.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-34.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Проблема может быть в том, что робот впадет в какой-то локально оптимальный оптимум (аля шаг вперед и упасть и получить +1) и уже оттуда не выберется, так-как ему всегда выгодно брать оптмальной действие."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-fifteen",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-36.png\" alt=\"Drawing\" width=\"500\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-mattress",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-37.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-39.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Иногда нужно выполять случайные, непривычные действия, чтобы познавать мир"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-acceptance",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-40.png\" alt=\"Drawing\" width=\"500\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-wealth",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-44.png\" alt=\"Drawing\" /> \n",
    "\n",
    "Чему будет равняться Q функция в клетках?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-drunk",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-48.png\" alt=\"Drawing\" /> \n",
    "\n",
    "Наша Q функция не учитывает тот факт, что агент иногда делает случайные действия. Это из-за max (так-как агент всегда смотрит на лучший исход).\n",
    "\n",
    "Алгоритм SARSA считает более реалистичные оценки. Так-как учитывает прыжки робота в лаву, то есть считает оценку по $\\epsilon -greedy$ политике."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-association",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-50.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-52.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-weather",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-53.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-54.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "SARSA сходится к оптимальной политике по вероятности , но Q может сойтись к неоптималной, если есть $\\epsilon$-greedy\n",
    "SARSA выигрывает во время обучения, так-как учитывает нашу политику обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-neutral",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-59.png\" alt=\"Drawing\" width=\"500\"> \n",
    "\n",
    "\n",
    "Off-policy подход позволяет учиться на исторических данных или на записанных заранее действиях человека. On-policy - *только на собственных действиях агента*. Это довольно важное деление, так как обучение на собственных действиях просто недоступно для многих задач. Плохо ли автомобилю с автопилотом въехать в стену? Надо попробовать, так будет работать on-policy метод. А лучше взять понимание, что это плохо из готовых данных, синтетических, например и усвоить этот опыт. Даже если мы используем симуляцию среды, а не реальную среду, то количество эпизодов симуляции обычно порядка сотен тысяч и их симуляция съедает много времени. Каждый раз проходить переобучение с нуля очень неудобно, а возможность импортировать накопленный опыт симуляции среды сильно экономит ресурсы. Однако off-policy методы можно применить не всегда."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-snowboard",
   "metadata": {},
   "source": [
    "## Аппроксимация Q-функции (Q-таблицы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-detection",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-6.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-8.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-hardwood",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-9.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-10.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-mystery",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-11.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-12.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "Очень важно, что мы предполагаем $Q(S_{t+1}, A_t)$ как const, хотя на самом деле обучение нашей функции в теории влияет и на него. Поэтому метод и называется semi-gradient. Не забываем в реализации про detach() и no_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-trauma",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/coursera-4-24.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/coursera-4-25.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-indonesia",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-14.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-15.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "$\\epsilon -greedy$ нужна для исследования среды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-apparatus",
   "metadata": {},
   "source": [
    "## Проблемы DQN и их решение\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-16.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-17.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-musician",
   "metadata": {},
   "source": [
    "## Experience replay\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-18.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-62.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "В большинстве окружений информация, получаемая агентом распределена не независимо. Т.е. последовательные наблюдения агента сильно коррелированы между собой (что понятно из интуитивных соображений, т.к. большинство окружений, в которых применяется RL, предполагают, что все изменения в них последовательны). Корреляция примеров ухудшает сходимость стохастического градиентного спуска. Таким образом нам нужен способ, который позволяет улучшить распределение примеров для обучения (устранить или снизить корреляцию между ними). Обычно используется метод **проигрывания опыта (experience replay)**. Суть этого метода в том, что мы сохраняем некоторое количество примеров (состояние, действия, вознаграждение) в специальном буфере и для обучения выбираем случайные мини-батчи из этого буфера.\n",
    "\n",
    "Так же **experience replay** позволяет агенту эффективнее использовать свой прошлый опыт.\n",
    "\n",
    "\n",
    "Стоит отметить, что нашим состоянием в DQN является картинка и хранить много картинок это не дешево"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-brown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-23.png\" alt=\"Drawing\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-garlic",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-24.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-25.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-fantasy",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-26.png\" alt=\"Drawing\" /> \n",
    "\n",
    "Одной из проблем Q-Networks является неустойчивость. Часто разность ожидаемых вознаграждений для различных действий близка и поскольку выбор действия производится с помощью argmax, то выброс в данных может привести к тому, что выбираемое действие изменится. Для того, чтобы повысить стабильность используется техника **Target Q-Network**. Суть в том, что мы замораживаем веса нашей сети на фиксированное число шагов и затем используем её для вычисления функции ошибки и обучения второй сети. Периодически копируем из веса рабочей сети в Target Q-Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-minute",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-28.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-29.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "На слайде справа показаны все компоненты с помощю которых была реализована в 2013 первая DQN, играющая в Atari "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-dollar",
   "metadata": {},
   "source": [
    "## Пример c CartPole\n",
    "\n",
    "\n",
    "### Описание CartPole\n",
    "\n",
    "CartPole это одна из классических задач RL, которая была введене Barto, Sutton, Anderson в 1983 году. В ней шест (Pole) прикреплен к тележке (Cart), которая движется по дорожке влево/вправо без трения. Система управляется приложением к тележке силы +1 или -1. Маятник запускается вертикально, и цель состоит в том, чтобы не допустить его падения. Эпизод заканчивается, когда штанга отклоняется от вертикали более чем на 15 градусов или тележка перемещается более чем на 2.4 единицы от центра. Награда +1 дается за каждый такт времени. Очевидно, что если  эпизод закончится, то мы перестанем получать награды и наш выигрыш составит то количество очков которое мы успели набрать.\n",
    "\n",
    "Каждое состояние описываетcя 4 значениями:\n",
    "\n",
    "\n",
    "|N|Action|min|max|\n",
    "|------|------|------|------|\n",
    "|0|Cart Position|-2.4|2.4|\n",
    "|1|Cart Velocity|-Inf|Inf|\n",
    "|2|Pole Angle|-41.8°| 41.8°|\n",
    "|3|Pole VelocityAt Tip|-Inf|Inf|\n",
    "\n",
    "\n",
    "Ко всем наблюдениям добавляется случайный шум из равномерное распределения U(-0.05,+0.05)\n",
    "\n",
    "### Задача\n",
    "\n",
    "Приведенный ниже код можно использовать без модификации для других env(игр). Единстенное требование, что пространство состояний должно быть единым вектором, действия должны быть дискретными.\n",
    "\n",
    "CartPole - самый простой. На ее решение должно уйти несколько минут.\n",
    "\n",
    "Для LunarLander может потребоваться 1-2 часа, чтобы набрать 200 баллов (хороший результат) на Colab, и прогресс обучения не выглядит информативным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-priority",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "Теперь нам нужно построить нейронную сеть, которая cможет отображать состояние (state) в набор q-значений для этого состония.\n",
    "Модель не обязательно должна быть большой. Возможно, будет достаточно 1-2 скрытых слоев с <200 нейронами и функцией активации ReLU. Batch normalization и dropout могут здесь все испортить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device\n",
    "print(state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        \n",
    "        \n",
    "        # Define NN\n",
    "        ##############################################\n",
    "        hidden_size = 150\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        ##############################################\n",
    "\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        \n",
    "        ##############################################\n",
    "        qvalues = self._nn(state_t)\n",
    "        ##############################################\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2 and \n",
    "            qvalues.shape[0] == state_t.shape[0] and \n",
    "            qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-penny",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer and Target Networks\n",
    "\n",
    "#### Интерфейс взаимодействия с replay buffer очень прост:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "\n",
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-principle",
   "metadata": {},
   "source": [
    "### TD-Loss\n",
    "\n",
    "Вычислим Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "где Q-reference определен следующим образом:\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Где\n",
    "* $Q_{target}(s',a')$ это Q-value для следующего состояния (next state) и следующего действия (next action), которое предсказывается __target_network__\n",
    "* $s, a, r, s'$ - текущее состояние (current state), действие (action), награда (reward) и следующее состояние (next state) соответсвенно\n",
    "* $\\gamma$ - discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # with torch.no_grad():\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    ##############################################\n",
    "    next_state_values = predicted_next_qvalues.max(axis=-1)[0]\n",
    "    ##############################################\n",
    "\n",
    "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
    "        \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    ###############################################\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * is_not_done\n",
    "    ##############################################\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
    "            \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim() == 1, \\\n",
    "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
    "            \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-communist",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your favourite random seed\n",
    "seed = 2\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = make_env(seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \n",
    "    hint: use agent.sample.actions\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    # <YOUR CODE>\n",
    "    for _ in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        \n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        # action = action.argmax(axis=-1)[0]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        sum_rewards += reward\n",
    "        \n",
    "        exp_replay.add(s, action, reward, state, done)\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        \n",
    "        s = state\n",
    "        \n",
    "    \n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print('less that 100 Mb RAM available, freezing')\n",
    "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        # <YOUR CODE: sample batch_size of data from experience replay>\n",
    "        s,a,r,next_s, is_done = exp_replay.sample(batch_size)\n",
    "        # loss = <YOUR CODE: compute TD loss>\n",
    "        loss = compute_td_loss(s, a, r, next_s, is_done, agent, target_network)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            # <YOUR CODE>\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(evaluate(\n",
    "                make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues(\n",
    "                [make_env(seed=step).reset()]\n",
    "            )\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  make_env(),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)\n",
    "assert final_score > 300, 'not good enough for DQN'\n",
    "print('Well done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-thursday",
   "metadata": {},
   "source": [
    "## Дальнейшие идеи для улучшения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-reward",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-30.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-31.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-spanish",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-32.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-33.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-ambassador",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-34.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-35.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>\n",
    "\n",
    "То есть, допустим, что Q(s,a) распределен случайно. То есть, выбор action никак не влияет. Но так-как мы берем max по Q, то будет казаться, что мы можем получать большие значения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-pittsburgh",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-36.png\" alt=\"Drawing\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-binary",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-37.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-38.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-balloon",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-39.png\" alt=\"Drawing\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-kazakhstan",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-48.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-49.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-moisture",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-50.png\" alt=\"Drawing\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-interaction",
   "metadata": {},
   "source": [
    "# Другие улучшения DQN\n",
    "\n",
    "## Prioritized Experience Replay\n",
    "\n",
    "Минибатчи из памяти выбираются не с равномерным распределением, а добавляем туда больше примеров, в которых предсказанные значения Q сильнее всего отличаются от корректных. Т.е. примеры с максимальным **TD error** получают максимальный приоритет.\n",
    "\n",
    "## Dueling networks\n",
    "\n",
    "Основная идея в том, что мы разделяем нашу сеть на две головы, одна из которых предсказывает абсолютное значение состояния \\\\( V(S) \\\\), а вторая - относительное преимущество одний действий над другими \\\\( A(s, a) = Q(s, a) - V(s) \\\\). Это преимущество называется advantage. Далее из этих двух значений мы собираем нашу Q-функцию, как \\\\( Q(s,a) = V(s) + A(a) \\\\)\n",
    "\n",
    "\n",
    "## Multi-step learning/n-step learning\n",
    "\n",
    "Основная идея в том, чтобы считать функцию ценности не по двум соседним примерам, а сразу по n. Это позволяет сети лучше запоминать длинные последовательности действий.\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "Детерминированное значение Q заменяется случайным распределением Z с некоторыми параметрами, которые определяются в ходе обучения.\n",
    "\n",
    "# Rainbow\n",
    "\n",
    "Rainbow - набор перечисленных выше твиков. \n",
    "\n",
    "# R2D2\n",
    "\n",
    "Использование RNN сетей для experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-leader",
   "metadata": {},
   "source": [
    "## Резюме по типам алгоритмов RL\n",
    "\n",
    "### Model-free / Model-based\n",
    "Model-free не строит модель окружения или функции награды. Это проще. Model-based алгоритм пытается предсказывать, каким будет следующее состояние окружения или вознаграждение. Это позволяет мыслить на несколько шагов вперёд. Например, совершенно не больно прыгать с крыши. Больно потом, когда разбиваешься о землю. Модель среды и награды позволяет принимать решения не прыгать с крыши, хотя и model-free подход позволяет это понять, хоть и более сложно и грубо.\n",
    "### Value-based / policy-based\n",
    "Policy-based  методы оптимизируют напрямую функцию принятия решения агента. Стратегия (policy) обычно представлена распределением вероятности доступных действий. Value-based метод оптимизирует оценку вознаграждения для всех действий и выбирает выбирает то действие, по которому прогнозируется большее значение. Методы, основанные на Policy Gradients лучше работают при большой размерности пространства действий, а Value-based методы, такие, как Deep Q-Learning требуют меньшего количества повторений для сходимости при малой размерности.\n",
    "### On-Policy / Off-Policy\n",
    "Off-policy подход позволяет учиться на исторических данных или на записанных заранее действиях человека. On-policy - только на собственных действиях агента. Это довольно важное деление, так как обучение на собственных действиях просто недоступно для многих задач. Плохо ли автомобилю с автопилотом въехать в стену? Надо попробовать, так будет работать on-policy метод. А лучше взять понимание, что это плохо из готовых данных, синтетических, например и усвоить этот опыт. Даже если мы используем симуляцию среды, а не реальную среду, то количество эпизодов симуляции обычно порядка сотен тысяч и их симуляция съедает много времени. Каждый раз проходить переобучение с нуля очень неудобно, а возможность импортировать накопленный опыт симуляции среды сильно экономит ресурсы. Однако off-policy методы можно применить не всегда.\n",
    "### Deterministic Policy / Stochastic Policy\n",
    "В зависимости от среды, наша стратегия может быть либо детерминированной - выбираем сразу определённое действие с помощью argmax, либо стохастической, когда мы окончательное решение принимается с помощью генератора случайных чисел на основе распределения вероятности, выданного сетью."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
