{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "* –í–≤–µ–¥–µ–Ω–∏–µ\n",
    "    * –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º\n",
    "    * –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è\n",
    "    * –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º\n",
    "\n",
    "* RL - —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –∏ –æ—Ç–ª–∏—á–∏–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "    * Agent - –ê–≥–µ–Ω—Ç\n",
    "    * State - –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã\n",
    "    * Reward - –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã\n",
    "    \n",
    "* –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ RL, c–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
    "    * –ù–∏–∑–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "    * –°–ª–æ–∂–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã\n",
    "    * –ù–µ–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "    \n",
    "* –û–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è\n",
    "    * –†–∞—Å—à–∏—Ä–∏—Ç—å —Ä–∞–∑–¥–µ–ª\n",
    "    \n",
    "* –°—Ö–æ–¥—Å—Ç–≤–æ —Å –ú–∞—Ä–∫–æ–≤—Å–∫–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º\n",
    "    * \n",
    "\n",
    "* Gym - —Å—Ä–µ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤\n",
    "    * –û–ø–∏—Å–∞–Ω–∏–µ\n",
    "    * –ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã\n",
    "    \n",
    "    \n",
    "* –û–±—É—á–µ–Ω–∏–µ —Å–µ—Ç–µ–π DQN\n",
    "    * Q-learning\n",
    "    * DQN\n",
    "    * Experience Replay Buffer\n",
    "    * TD-Loss\n",
    "    * –î—Ä—É–≥–∏–µ —É–ª—É—á—à–µ–Ω–∏—è DQN\n",
    "    \n",
    "* –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
    "\n",
    "* –°—Å—ã–ª–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í–≤–µ–¥–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º\n",
    "\n",
    "–ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –Ω–∞–±–æ—Ä –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏, —Ç–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç—É –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏, –∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –ø—Ä–∏–º–µ–Ω—è–µ–º –µ—ë –∫ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º. –ò–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏, –∫–æ–≥–¥–∞ –æ–±—É—á–∞–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è MNIST, –ø–æ–¥–∞–≤–∞—è –Ω–∞ –≤—Ö–æ–¥ —Å–µ—Ç–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä –∏ —Å—á–∏—Ç–∞—è –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–ª—è –ø–æ–¥—Å—Ç—Ä–æ–π–∫–∏ –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É –∏–∑–≤–µ—Å—Ç–Ω—ã–º –ª—ç–π–±–ª–æ–º —Ü–∏—Ñ—Ä—ã –∏ –≤—ã—Ö–æ–¥–æ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.\n",
    "\n",
    "### –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è\n",
    "\n",
    "–í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö —É –Ω–∞—Å –Ω–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –º–æ–≥–ª–∏ –±—ã –∑–∞—Ä–∞–Ω–µ–µ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å. –ù–æ, –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á, –º–æ–∂–Ω–æ –æ–±–æ–π—Ç–∏—Å—å –±–µ–∑ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏. –ü—Ä–∏–º–µ—Ä–æ–º —Ç–∞–∫–æ–π –∑–∞–¥–∞—á–∏ —è–≤–ª—è–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.\n",
    "\n",
    "### –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º\n",
    "\n",
    "–í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ —É—á–∏—Ç–µ–ª—è –Ω–∞–º –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç. –í —Ç–æ –∂–µ –≤—Ä–µ–º—è —É –Ω–∞—Å –Ω–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É. –ü—Ä–∏ —ç—Ç–æ–º –º—ã –º–æ–∂–µ–º –ø–æ—Å—Ç—Ñ–∞–∫—Ç—É–º –æ—Ü–µ–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –æ—Ü–µ–Ω–∫—É –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–∞ —á–∞—â–µ —Å–æ–≤–µ—Ä—à–∞–ª–∞ –∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –∏ —Ä–µ–∂–µ - –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ. –í –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ —Ç–∞–∫—É—é –æ—Ü–µ–Ω–∫—É –Ω–∞–∑—ã–≤–∞—é—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ–º (reward), –∞ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç—Ä–æ–∏—Ç—Å—è —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã —ç—Ç–æ –º–æ–¥–µ–ª—å —Å—Ç—Ä–µ–º–∏–ª–∞—Å—å –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∞–µ–º–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/agend_and_environment.gif\" width=\"600\"/>\n",
    "\n",
    "- –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å: –¥–µ–π—Å—Ç–≤–∏—è –∏ –Ω–∞–≥—Ä–∞–¥—ã —Å–ª—É—á–∞–π–Ω—ã\n",
    "- –û—Ç–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å r_t+1 –º–æ–∂–µ—Ç –Ω–µ –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç a_t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL - —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è –∏ –æ—Ç–ª–∏—á–∏–µ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "    \n",
    "### –ê–≥–µ–Ω—Ç\n",
    "\n",
    "–ê–≥–µ–Ω—Ç - —ç—Ç–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞, –ø—Ä–∏–Ω–∏–º–∞—é—â–∞—è —Ä–µ—à–µ–Ω–∏–µ –æ –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å—Ä–µ–¥—ã. –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –∞–≥–µ–Ω—Ç–æ–º –±—É–¥–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å.\n",
    "\n",
    "### –°—Ä–µ–¥–∞\n",
    "\n",
    "–°—Ä–µ–¥–∞ - —ç—Ç–æ –º–∏—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω \"–≤—ã–∂–∏–≤–∞—Ç—å\", —Ç.–µ. –≤—Å—ë, —Å —á–µ–º –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—Ä—è–º–æ –∏–ª–∏ –∫–æ—Å–≤–µ–Ω–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å. –°—Ä–µ–¥–∞ –æ–±–ª–∞–¥–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º (**state**), –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ —Å—Ä–µ–¥—É, —Å–æ–≤–µ—Ä—à–∞—è –∫–∞–∫–∏–µ-—Ç–æ –¥–µ–π—Å—Ç–≤–∏—è (**Actions**), –ø–µ—Ä–µ–≤–æ–¥—è —Å—Ä–µ–¥—É –ø—Ä–∏ —ç—Ç–æ–º –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –¥—Ä—É–≥–æ–µ –∏ –ø–æ–ª—É—á–∞—è –∫–∞–∫–æ–µ-—Ç–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ. –°—Ä–µ–¥–∞ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π. –ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ - –≤–µ–∫—Ç–æ—Ä –≤ —ç—Ç–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.\n",
    "\n",
    "–í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏, –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –Ω–∞–±–ª—é–¥–∞—Ç—å –ª–∏–±–æ –ø–æ–ª–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã, –ª–∏–±–æ —Ç–æ–ª—å–∫–æ –Ω–µ–∫–æ—Ç–æ—Ä—É—é –µ–≥–æ —á–∞—Å—Ç—å. –í–æ –≤—Ç–æ—Ä–æ–º —Å–ª—É—á–∞–µ, –∞–≥–µ–Ω—Ç—É –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∫–∞–∫–æ–µ-—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –ø–æ –º–µ—Ä–µ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "### –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã\n",
    "\n",
    "–§—É–∫–Ω—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã (**Reward**) - –≤–≤–æ–¥–∏–º–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–º —Ñ–æ—Ä–º—É–ª–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –æ–∂–∏–¥–∞–Ω–∏—è —ç—Ç–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –ø—É—Ç—å –∫ –Ω–∞–∏–ª—É—á—à–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞. –≠—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–æ–≥ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å, –±–µ–∑ –∫–æ—Ç–æ—Ä–æ–π –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ —á–µ–º—É —É—á–∏—Ç—å—Å—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —à–∞—Ö–º–∞—Ç–∞—Ö –∏—Å—Ç–∏–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ —ç—Ç–æ –ø–æ–±–µ–¥–∞, –Ω–æ –≤–∑—è—Ç–∞—è —Ñ–∏–≥—É—Ä–∞ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞ —Ç–æ–∂–µ —Ü–µ–Ω–Ω–∞ –∏ –¥–æ–ª–∂–Ω–∞ —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å –Ω–∞–≥—Ä–∞–¥—É, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø–æ–¥—Å–∫–∞–∑–∞—Ç—å –∞–≥–µ–Ω—Ç—É, —á—Ç–æ –±—Ä–∞—Ç—å —á—É–∂–∏–µ —Ñ–∏–≥—É—Ä—ã –ø–æ–ª–µ–∑–Ω–æ. –ú–æ–∂–µ—Ç –ª–∏ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞—Ç—å –º–∞—Ç, –ø–æ–∑–∞—Ä–∏–≤—à–∏—Å—å –Ω–∞ –Ω–µ–∑–∞—â–∏—â–µ–Ω–Ω—É—é —Ñ–∏–≥—É—Ä—É? –î–∞, —Ä–æ–≤–Ω–æ –∫–∞–∫ –∏ –Ω–µ–æ–ø—ã—Ç–Ω—ã–π —à–∞—Ö–º–∞—Ç–Ω—ã–π –∏–≥—Ä–æ–∫. –ü–æ–ø—ã—Ç–∫–∞ –ø–µ—Ä–µ–¥–∞—Ç—å —á–µ—Ä–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–µ–æ—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –ø–æ–¥—Å–∫–∞–∑–∫–∏ –∫ –ø–æ–ª—É—á–µ–Ω–∏—é –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è **reward shaping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ RL. –°–ª–æ–∂–Ω–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ù–∏–∑–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (sample efficiency)\n",
    "\n",
    "–û–±—â–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –≤—Å–µ—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º - –Ω–∏–∑–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –í —Ç–æ –≤—Ä–µ–º—è, –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫—É –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–¥–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è, —á—Ç–æ–±—ã –≤—ã—É—á–∏—Ç—å –∫–∞–∫–æ–µ-—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ, –∞–≥–µ–Ω—Ç—É RL —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–µ—Å—è—Ç–∫–∏ —Ç—ã—Å—è—á –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –¥–∞–∂–µ –≤ –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –í –∫–∞–∫–æ–π-—Ç–æ —Å—Ç–µ–ø–µ–Ω–∏ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –Ω–æ —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –≤–∫–ª–∞–¥ –¥–∞—ë—Ç —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ —á–µ–ª–æ–≤–µ–∫ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –≤ –ø—Ä–æ—à–ª–æ–º –æ–ø—ã—Ç –∏–∑ –¥—Ä—É–≥–∏—Ö –æ–±–ª–∞—Å—Ç–µ–π. –ò–≥—Ä–∞ Montezuma's Revenge - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –ø–æ–¥–æ–ø—ã—Ç–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è RL –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è. –ò —è—Ä–∫–∏–π –ø—Ä–∏–º–µ—Ä –Ω–∏–∑–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π  —É –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ RL –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —á–µ–ª–æ–≤–µ–∫–æ–º. \n",
    "\n",
    "–ß–µ–ª–æ–∫–µ–∫, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –±—ã—Å—Ç—Ä–æ –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–±–µ–≥–∞—Ç—å —á–µ—Ä–µ–ø–∞ –∏ –∑–∞–±—Ä–∞—Ç—å –∫–ª—é—á, –≥—Ä–∞–≤–∏—Ç–∞—Ü–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤–Ω–∏–∑, –∞ –ø–∞–¥–µ–Ω–∏–µ —Å –±–æ–ª—å—à–æ–π –≤—ã—Å–æ—Ç—ã –æ–ø–∞—Å–Ω–æ. –ê–ª–≥–æ—Ä–∏—Ç–º—É –∂–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç—å—Å—è –æ–±—É—á–∞—Ç—å—Å—è —Å –ø–æ–ª–Ω–æ–≥–æ –Ω—É–ª—è. –ï—Å–ª–∏ –∂–µ –ø–æ–¥–º–µ–Ω–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –Ω–∞ –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞, —Ç–æ –µ–≥–æ sample-efficency —Ç–æ–∂–µ —Å–∏–ª—å–Ω–æ –ø–∞–¥–∞–µ—Ç (—Ö–æ—Ç—è –≤—Å—ë-—Ä–∞–≤–Ω–æ –ª—É—á—à–µ, —á–µ–º RL).\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/game_prior.gif\" width=\"400\">\n",
    "\n",
    "–ê —Ç–µ–ø–µ—Ä—å —Ç–æ –∂–µ —Å–∞–º–æ–µ, –Ω–æ –≤ –Ω–µ—á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–º –≤–∏–¥–µ. –î–ª—è RL —Ä–∞–∑–Ω–∏—Ü—ã –Ω–µ—Ç, –∞ –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞ —Å—Ä–∞–∑—É —Å—Ç–∞–ª–æ —Å–ª–æ–∂–Ω–µ–µ.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/game_no_prior.gif\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–ª–æ–∂–Ω–æ–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã\n",
    "–¢–∞–∫ –∂–µ –≤–∞–∂–Ω—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —è–≤–ª—è—é—Ç—Å—è —Ä–µ–¥–∫–∏–µ –Ω–∞–≥—Ä–∞–¥—ã. –ß–∞—Å—Ç–æ –≤ —Ö–æ–¥–µ –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –∞–ª–≥–æ—Ä–∏—Ç–º –¥–µ–ª–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, –∞ –Ω–∞–≥—Ä–∞–¥—É –ø–æ–ª—É—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –≤–µ—Å–∞ —Å–µ—Ç–∏ –º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ —ç–ø–∏–∑–æ–¥–∞ –∏ –Ω–µ–ª—å–∑—è –ø–æ–æ—â—Ä–∏—Ç—å –∏–ª–∏ –Ω–∞–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≤–Ω—É—Ç—Ä–∏ —ç–ø–∏–∑–æ–¥–∞. –í –∏—Ç–æ–≥–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤.\n",
    "\n",
    "–û–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ —É–ª—É—á—à–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–µ–¥–∫–∏—Ö –Ω–∞–≥—Ä–∞–¥–∞—Ö - reward shaping - –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã —Ç–∞–∫, —á—Ç–æ–±—ã —è–≤–Ω–æ –ø–æ–æ—â—Ä—è–ª–∏—Å—å –∫–∞–∫–∏–µ-—Ç–æ –¥–µ–π—Å—Ç–≤–∏—è –≤–Ω—É—Ç—Ä–∏ —ç–ø–∏–∑–æ–¥–∞. –ù–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ —Å–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é —Ç—è–∂–µ–ª–æ, –∞ –æ—à–∏–±–∫–∏ –≤ –Ω–µ–π –º–æ–≥—É—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–º —ç—Ñ—Ñ–µ–∫—Ç–∞–º.\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä 1** - –í –≥–æ–Ω–∫–µ –ª–æ–¥–æ–∫ –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–ª –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–µ –Ω–µ —Ç–æ–ª—å–∫–æ –∑–∞ –ø–æ–±–µ–¥—É –≤ –≥–æ–Ω–∫–µ, –Ω–æ –∏ –∑–∞ —Å–±–æ—Ä –∏–≥—Ä–æ–≤—ã—Ö –±–æ–Ω—É—Å–æ–≤. –í –∏—Ç–æ–≥–µ –æ–Ω —Ä–µ—à–∏–ª, —á—Ç–æ –≥–æ–Ω–∫–∞ –Ω–µ –æ—á–µ–Ω—å-—Ç–æ –∏ –Ω—É–∂–Ω–∞, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–±–∏—Ä–∞—Ç—å –±–æ–Ω—É—Å—ã.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/coastrunner.gif\" width=\"400\">\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä 2** - –£ –¥–∞–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –º—ã –Ω–∞–±–ª—é–¥–∞–µ–º –ø–æ–ø–æ–¥–∞–Ω–∏–µ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º. –≠—Ç–æ—Ç –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç –ø–æ–æ—â—Ä–µ–Ω–∏–µ –∑–∞ –Ω–∞–±—Ä–∞–Ω–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å. –ù–∞ –Ω–∞—á–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ –≤–æ –≤—Ä–µ–º—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–≥–µ–Ω—Ç –æ–±–Ω–∞—Ä—É–∂–∏–ª, —á—Ç–æ –∫—É–≤—ã—Ä–∫–Ω—É—Ç—å—Å—è –≤–ø–µ—Ä—ë–¥ –¥–∞—ë—Ç —Ö–æ—Ä–æ—à–µ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ. –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ, –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫, –ø–µ—Ä–µ–≤–æ—Ä–∞—á–∏–≤–∞–Ω–∏–µ –Ω–∞ —Å–ø–∏–Ω—É –∑–∞–∫—Ä–µ–ø–∏–ª–æ—Å—å, –∫–∞–∫ —É—Å–ø–µ—à–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è. –ü–æ—Å–ª–µ –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è —Ç–∞–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç –Ω–µ —Å–º–æ–≥ –≤—ã–π—Ç–∏ –∏–∑ —ç—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, —Ç.–∫. –æ–∫–∞–∑–∞–ª–æ—Å—å –ø—Ä–æ—â–µ –Ω–∞—É—á–∏—Ç—å—Å—è –¥–≤–∏–≥–∞—Ç—å—Å—è –≤ —Ç–∞–∫–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏, —á–µ–º –Ω–∞—É—á–∏—Ç—å—Å—è –ø–µ—Ä–µ–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å—Å—è –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –Ω–æ–≥–∏. –ü–æ—Ö–æ–∂–µ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–∂–Ω–æ —Å–ª—É—á–∞–Ω–æ –ø–æ–ª—É—á–∏—Ç—å, –µ—Å–ª–∏ –ø–æ–æ—â—Ä—è—Ç—å –∞–≥–µ–Ω—Ç–∞ –∑–∞ —Ç–æ, —á—Ç–æ –µ–≥–æ –Ω–æ–≥–∏ –æ—Ç–æ—Ä–≤–∞–Ω—ã –æ—Ç –∑–µ–º–ª–∏.\n",
    "\n",
    "<center><video controls src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/upsidedown_half_cheetah.mp4\" width=\"400\"> </video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ù–µ–≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "–û–±—ã—á–Ω–æ –º—ã –≤—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–µ–º —Å–æ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤, —á—Ç–æ –Ω–µ –º–µ—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø–æ–ª—É—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è. –ù–æ —Å RL —ç—Ç–æ –Ω–µ —Ç–∞–∫. –î–∞–∂–µ –≤ –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–µ —Å 3 —Å—Ç–µ–ø–µ–Ω—è–º–∏ —Å–≤–æ–±–æ–¥—ã (state - —Ç—Ä—ë—Ö–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä) –∏ –æ–¥–Ω–æ–π —Å—Ç–µ–ø–µ–Ω—å—é –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è (action - —Å–∫–∞–ª—è—Ä), –æ–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–∞—Ä–æ–¥—ã—à–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –ø—Å–µ–≤–¥–æ—Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª –ø—Ä–∏–≤–µ–ª–æ –≤ 30% —Å–ª—É—á–∞–µ–≤ –∫ —Ñ–∏–∞—Å–∫–æ. –ó–∞—Ä–∞–Ω–µ–µ –ø–æ–Ω—è—Ç—å, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –ø–æ—à–ª–æ –ø–ª–æ—Ö–æ, –Ω–µ–ª—å–∑—è –∏–ª–∏ —Å–ª–æ–∂–Ω–æ.\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img_license/pendulum_results.png\" width=\"400\">\n",
    "<center><em> episode_reward/test <em><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è\n",
    "\n",
    "* –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–±–æ—Ç–∞–º–∏, –ë–ü–õ\n",
    "* –ò–≥—Ä—ã\n",
    "* –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –≤ —É—Å–ª–æ–≤—Ç—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º–∏\n",
    "* drug discovery: –æ–±—ã—á–Ω–æ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç—Ä–∞–∂–∞—é—â–∏–µ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ –Ω–µ–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω–∏—Ü—Ä—É–µ–º—ã –∏ –º–æ–∂–Ω–æ –∏—Å–∫–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ, –≤–≤–æ–¥—è RL\n",
    "* –ù–µ —Ç–∞–∫ –¥–∞–≤–Ω–æ –æ–¥–Ω–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∞—è –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞ RL –¥–ª—è –æ–ø—Ç–∏–º–∞–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ–∫–∞—Ç–Ω–æ–≥–æ —Å—Ç–∞–Ω–∞\n",
    "* NAS (Neural Architecture Search): –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ç–æ–ø–æ–ª–æ–≥–∏–π —Å–µ—Ç–∏ –º–æ–∂–Ω–æ —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É —Å –ø–æ–º–æ—â—å—é RL, –Ω–∞–≥—Ä–∞–∂–¥–∞—è –Ω–∞—à–µ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∑–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Ö—Ö–æ—Ä–æ—à–∏—Ö —Ç–æ–ø–æ–ª–æ–≥–∏–π –∏ –Ω–∞–∫–∞–∑—ã–≤–∞—è –∑–∞ –ø–ª–æ—Ö–∏–µ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ RL\n",
    "### Model-free / Model-based\n",
    "Model-free –Ω–µ —Å—Ç—Ä–æ–∏—Ç –º–æ–¥–µ–ª—å –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã. –≠—Ç–æ –ø—Ä–æ—â–µ. Model-based –∞–ª–≥–æ—Ä–∏—Ç–º –ø—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å, –∫–∞–∫–∏–º –±—É–¥–µ—Ç —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º—ã—Å–ª–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –≤–ø–µ—Ä—ë–¥. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–µ –±–æ–ª—å–Ω–æ –ø—Ä—ã–≥–∞—Ç—å —Å –∫—Ä—ã—à–∏. –ë–æ–ª—å–Ω–æ –ø–æ—Ç–æ–º, –∫–æ–≥–¥–∞ —Ä–∞–∑–±–∏–≤–∞–µ—à—å—Å—è –æ –∑–µ–º–ª—é. –ú–æ–¥–µ–ª—å —Å—Ä–µ–¥—ã –∏ –Ω–∞–≥—Ä–∞–¥—ã –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –Ω–µ –ø—Ä—ã–≥–∞—Ç—å —Å –∫—Ä—ã—à–∏, —Ö–æ—Ç—è –∏ model-free –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ç–æ –ø–æ–Ω—è—Ç—å, —Ö–æ—Ç—å –∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ –∏ –≥—Ä—É–±–æ.\n",
    "### Value-based / policy-based\n",
    "Policy-based  –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç –Ω–∞–ø—Ä—è–º—É—é —Ñ—É–Ω–∫—Ü–∏—é –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞. –°—Ç—Ä–∞—Ç–µ–≥–∏—è (policy) –æ–±—ã—á–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π. Value-based –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ—Ü–µ–Ω–∫—É –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –≤—ã–±–∏—Ä–∞–µ—Ç —Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç—Å—è –±–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –ú–µ—Ç–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ Policy Gradients –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—é—Ç –ø—Ä–∏ –±–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π, –∞ Value-based –º–µ—Ç–æ–¥—ã, —Ç–∞–∫–∏–µ, –∫–∞–∫ Deep Q-Learning —Ç—Ä–µ–±—É—é—Ç –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –¥–ª—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –º–∞–ª–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.\n",
    "### On-Policy / Off-Policy\n",
    "Off-policy –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—á–∏—Ç—å—Å—è –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –Ω–∞ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –∑–∞—Ä–∞–Ω–µ–µ –¥–µ–π—Å—Ç–≤–∏—è—Ö —á–µ–ª–æ–≤–µ–∫–∞. On-policy - —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö –∞–≥–µ–Ω—Ç–∞. –≠—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ –≤–∞–∂–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ, —Ç–∞–∫ –∫–∞–∫ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö –ø—Ä–æ—Å—Ç–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –º–Ω–æ–≥–∏—Ö –∑–∞–¥–∞—á. –ü–ª–æ—Ö–æ –ª–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—é —Å –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–æ–º –≤—ä–µ—Ö–∞—Ç—å –≤ —Å—Ç–µ–Ω—É? –ù–∞–¥–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å, —Ç–∞–∫ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å on-policy –º–µ—Ç–æ–¥. –ê –ª—É—á—à–µ –≤–∑—è—Ç—å –ø–æ–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ —ç—Ç–æ –ø–ª–æ—Ö–æ –∏–∑ –≥–æ—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö, –Ω–∞–ø—Ä–∏–º–µ—Ä –∏ —É—Å–≤–æ–∏—Ç—å —ç—Ç–æ—Ç –æ–ø—ã—Ç. –î–∞–∂–µ –µ—Å–ª–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–º—É–ª—è—Ü–∏—é —Å—Ä–µ–¥—ã, –∞ –Ω–µ —Ä–µ–∞–ª—å–Ω—É—é —Å—Ä–µ–¥—É, —Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ —Å–∏–º—É–ª—è—Ü–∏–∏ –æ–±—ã—á–Ω–æ –ø–æ—Ä—è–¥–∫–∞ —Å–æ—Ç–µ–Ω —Ç—ã—Å—è—á –∏ –∏—Ö —Å–∏–º—É–ª—è—Ü–∏—è —Å—ä–µ–¥–∞–µ—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ö–∞–∂–¥—ã–π —Ä–∞–∑ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è –æ—á–µ–Ω—å –Ω–µ—É–¥–æ–±–Ω–æ, –∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –æ–ø—ã—Ç —Å–∏–º—É–ª—è—Ü–∏–∏ —Å—Ä–µ–¥—ã —Å–∏–ª—å–Ω–æ —ç–∫–æ–Ω–æ–º–∏—Ç —Ä–µ—Å—É—Ä—Å—ã. –û–¥–Ω–∞–∫–æ off-policy –º–µ—Ç–æ–¥—ã –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–µ –≤—Å–µ–≥–¥–∞.\n",
    "### Deterministic Policy / Stochastic Policy\n",
    "–í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å—Ä–µ–¥—ã, –Ω–∞—à–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–∏–±–æ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π - –≤—ã–±–∏—Ä–∞–µ–º —Å—Ä–∞–∑—É –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ —Å –ø–æ–º–æ—â—å—é argmax, –ª–∏–±–æ —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–π, –∫–æ–≥–¥–∞ –º—ã –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –≤—ã–¥–∞–Ω–Ω–æ–≥–æ —Å–µ—Ç—å—é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π (MDP)\n",
    "\n",
    "–ü—Ä–µ–∂–¥–µ —á–µ–º –ø–µ—Ä–µ–π—Ç–∏ –∫ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º —Å–µ—Ç—è–º, —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –º–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º —Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è.\n",
    "\n",
    "**Markov property** - –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ —Ç–µ–∫—É—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è. –≠—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø—Ä–∏ –æ–±—Ö–æ–¥–µ –≥—Ä–∞—Ñ–æ–≤.\n",
    "\n",
    "**–ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π** (–∞–Ω–≥–ª. Markov decision process (MDP)) ‚Äî —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞–±–ª—é–¥–∞–µ–º–æ–π —Å—Ä–µ–¥—ã —Å –º–∞—Ä–∫–æ–≤—Å–∫–æ–π –º–æ–¥–µ–ª—å—é –ø–µ—Ä–µ—Ö–æ–¥–∞ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è–º–∏. –ù–∞–∑–≤–∞–Ω –≤ —á–µ—Å—Ç—å –ê–Ω–¥—Ä–µ—è –ú–∞—Ä–∫–æ–≤–∞, —Å–ª—É–∂–∏—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Å–Ω–æ–≤–æ–π –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –≤ —Å–∏—Ç—É–∞—Ü–∏—è—Ö, –≥–¥–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–∞—Å—Ç–∏—á–Ω–æ —Å–ª—É—á–∞–π–Ω—ã –∏ —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ª–∏—Ü–∞, –ø—Ä–∏–Ω–∏–º–∞—é—â–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –°–µ–≥–æ–¥–Ω—è —ç—Ç–∞ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –æ–±–ª–∞—Å—Ç–µ–π, –≤–∫–ª—é—á–∞—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫—É, –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, —ç–∫–æ–Ω–æ–º–∏–∫—É –∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ—Ä \n",
    "\n",
    "–°–æ—Å—Ç–∞–≤–∏–º –≥—Ä–∞—Ñ:\n",
    "- –≤–µ—Ä—à–∏–Ω–∞ - —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã\n",
    "- —Ä–µ–±—Ä–æ - –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏ ~= –¥–µ–π—Å—Ç–≤–∏–µ\n",
    "\n",
    "–ö—Ä–æ–º–µ —Ç–æ–≥–æ:\n",
    "- –ö–∞–∂–¥—ã–µ –ø–µ—Ä–µ—Ö–æ–¥ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è –Ω–∞–≥—Ä–∞–¥–æ–π (reward)\n",
    "- –ü–µ—Ä–µ—Ö–æ–¥—ã —Å–ª—É—á–∞–π–Ω—ã\n",
    "- –°—É—â–µ—Å—Ç–≤—É—é—Ç —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" />\n",
    "\n",
    "\n",
    "- —á–µ—Ä–Ω—ã–µ –±–µ–∑–∑–Ω–∞–∫–æ–≤—ã–µ —á–∏—Å–ª–∞ - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤\n",
    "- –∑–Ω–∞–∫–æ–≤—ã–µ —ç—Ç–æ –Ω–∞–≥—Ä–∞–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –ø—Ä–∏–º–µ—Ä–æ–º:\n",
    "https://github.com/yandexdataschool/Practical_RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
    "!touch .setup_complete\n",
    "\n",
    "# After this setup we can import mdp package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import MDP \n",
    "\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "\n",
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP methods\n",
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "\n",
    "# state, action, next_state\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "\n",
    "# get_transition_prob(self, state, action, next_state)\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import mdp as mdp_package\n",
    "display(mdp_package.plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ù–∞–≥—Ä–∞–¥–∞ –≤ MDP\n",
    "\n",
    "* –ö–∞–∂–¥—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –∞—Å—Å–æ—Ü–∏–∏—Ä—É–µ—Ç—Å—è —Å –Ω–∞–≥—Ä–∞–¥–æ–π R.\n",
    "* –ù–∞–≥—Ä–∞–¥–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫ –Ω—É–ª–µ–≤–æ–π, —Ç–∞–∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π.\n",
    "\n",
    "* –ù–∞–ø—Ä–∏–º–µ—Ä –≤ MDP –æ–ø–∏—Å—ã–≤–∞—é—â–µ–º –∏–≥—Ä—É –≤ —à–∞—Ö–º–∞—Ç—ã –∏–ª–∏ GO –Ω–∞–≥—Ä–∞–¥–∞ –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ \"–ø–∞—Ä—Ç–∏—è –≤—ã–∏–≥—Ä–∞–Ω–Ω–∞\". –ù–∞ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ö–æ–¥–∞—Ö –Ω–∞–≥—Ä–∞–¥–∞ –±—É–¥–µ—Ç –Ω—É–ª–µ–≤–æ–π. –≠—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–æ–Ω–µ—á–Ω–∞—è —Ü–µ–ª—å –∏–≥—Ä—ã - –ø–æ–±–µ–¥–∞. \n",
    "\n",
    "* –î–ª—è –ø–µ—Ä–µ–º–µ—â–∞—é—â–µ–≥–æ—Å—è —Ä–æ–±–æ—Ç–∞ –Ω–∞–≥—Ä–∞–¥–∞ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π, —Ç–∞–∫ –∫–∞–∫ –Ω–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —ç–Ω–µ—Ä–≥–∏—è. –∏.—Ç.–ø. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (discounting)\n",
    "\n",
    "\n",
    "–ß—Ç–æ –ª—É—á—à–µ: \"–ü–æ–ª—É—á–∏—Ç—å –º–∏–ª–ª–∏–æ–Ω –¥–æ–ª–ª–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ 100 –ª–µ—Ç –∏–ª–∏ $100 —Å–µ–π—á–∞—Å\"\n",
    "\n",
    "–ß–∞—Å—Ç–æ –≤—Ä–µ–º—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞–≥—Ä–∞–¥—ã –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ. –ü–æ—ç—Ç–æ–º—É –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –∫—É–º–º—É–ª—è—Ç–∏–≤–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —à–∞–≥–µ t (Gt) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è $\\gamma$ \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-12_.png\" alt=\"Drawing\" width=\"700px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-15.png\" alt=\"Drawing\" width=\"700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-24__.png\"  width=\"700px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–ª–≥–æ—Ä–∏—Ç–º:\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/mdp.png\"  width=\"600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏\n",
    "\n",
    "–ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∫–æ—Ç–æ—Ä–æ–π –±—É–¥–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ (Gt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### –ü–æ–ª–∏—Ç–∏–∫–∞\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-26_.png\" alt=\"Drawing\" width=\"700\">\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –µ–µ —Ä–µ—à–µ–Ω–∏—è –±—É–¥–µ—Ç \"–ü–æ–ª–∏—Ç–∏–∫–∞\" –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ a –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è s.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img_license/policy1.png\"  width=\"800px;\"/>\n",
    "\n",
    "\n",
    "*–°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –ø–æ–ª–∏—Ç–∏–∫–∞, –∫–∞–∫ —Å–ª–µ–¥—É–µ—Ç –∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∂–µ—Å—Ç–∫–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π. –í–ø–æ–ª–Ω–µ –≤–æ—Ö–º–æ–∂–Ω–æ, —á—Ç–æ –æ–Ω–∞ –±—É–¥–µ—Ç —Å–ª—É—á–∞–π–Ω–æ–π. –≠—Ç–æ –º–æ–∂–µ—Ç, –∫ –ø—Ä–∏–º–µ—Ä—É, –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è –ø—Ä–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö —Å—Ä–µ–¥—ã.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img_license/policy2.png\"  width=\"500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value - function\n",
    "\n",
    "–î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –≤–≤–æ–¥—è—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏–∏ V –∏ Q \n",
    "\n",
    "\n",
    "\n",
    "V(s) - –∫–∞–∫–æ–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –µ—Å–ª–∏ –Ω–∞—á–∞—Ç—å –¥–≤–∏–≥–∞—Ç—å—Å—è –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è s.\n",
    "\n",
    "* –í —Ç–µ—á–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏\n",
    "\n",
    "Q(s,a) -  –∞–Ω–ª–æ–≥–∏—á–Ω–æ, –Ω–æ –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ —á—Ç–æ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ s (s0 = s) –±—ã–ª–æ –≤—ã–±—Ä–∞–Ω–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-28_.png\" alt=\"Drawing\" width=\"700px;\"/>\n",
    "\n",
    "–ü–æ–¥ $E_\\pi$ –º—ã –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–∏—Ç–∫–∏ –∏ —Å—Ä–µ–¥—ã\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img_license/silver-2-29_.png\" alt=\"Drawing\" width=\"450px;\"/>\n",
    "\n",
    "\n",
    "–í –≤–µ—Ä—à–∏–Ω–∞—Ö –∑–Ω–∞—á–µ–Ω–∏—è V –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ $v_\\pi$ (–µ—Ä—Ö—É –∫–∞—Ä—Ç–∏–Ω–∫–∏)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ—à–µ–Ω–∏–µ\n",
    "\n",
    "–ó–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–µ—à–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö V* –∏ Q* - —Ñ—É–Ω–∫—Ü–∏–∏.\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-2-37_.png\" alt=\"Drawing\" width=\"700px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation\n",
    "\n",
    "–î–ª—è –µ–µ —Ä–µ—à–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ [–ë–µ–ª–º–∞–Ω–∞](https://en.wikipedia.org/wiki/Richard_E._Bellman)\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/bellman.png\" width=\"700px;\"/> \n",
    "\n",
    "Q —Ñ—É–Ω–∫—Ü–∏—è —É–±–∏—Ä–∞–µ—Ç —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –ø–µ—Ä–≤–æ–º —à–∞–≥–µ. –° –ø–æ–º–æ—â—å—é –Ω–µ–µ –º—ã —Ñ–∏–∫—Å–∏—Ä—É–µ–º –Ω–∞—à –ø–µ—Ä–≤—ã–π —à–∞–≥ –∏–∑ —ç—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.\n",
    "\n",
    "\n",
    "–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –º—ã –±—É–¥–µ–º —Å—á–∏—Ç–∞—Ç—å Q –∏ V —Ç–∞–∫:\n",
    "\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n",
    "–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è V –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Q \n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img_license/silver-2-39_.png\" alt=\"Drawing\" width=\"800px;\"/>\n",
    "\n",
    "<center><em> Example: Optimal Action-Value Function for Student MDP <em><center> \n",
    "\n",
    "—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ q –∑–∞–¥–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –í–∞—Ä–∏–∞–Ω—Ç—ã —Ä–µ—à–µ–Ω–∏–π\n",
    "\n",
    "- Bellman Optimality Equation\n",
    "- Solving the Bellman Optimality Equation\n",
    "- Bellman Optimality Equation is non-linear No closed form solution (in general) Many iterative solution methods\n",
    "    - Value Iteration \n",
    "\n",
    "    - Policy Iteration \n",
    "\n",
    "    - Q-learning \n",
    "\n",
    "    - Sarsa\n",
    "\n",
    "\n",
    "- –ü—Ä–∏–Ω—Ü–∏–ø –ë–µ–ª–ª–º–∞–Ω–∞ - –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ —Å–ª–µ–¥—É–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Ö –≤ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–∏ –æ–± –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤.\n",
    "- –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø—Ä–µ–¥—ã—Å—Ç–æ—Ä–∏–∏.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "\n",
    "–ê–ª–≥–æ—Ä–∏—Ç–º\n",
    "\n",
    "1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "- –°–æ–∑–¥–∞–µ–º –º–∞—Å—Å–∏–≤ V —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ä–∞–≤–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ—Å—Ç–æ—è–Ω–∏–π.\n",
    "\n",
    "- –ó–∞–ø–æ–ª–Ω—è–µ–º –µ–≥–æ –Ω—É–ª—è–º–∏\n",
    "\n",
    "\n",
    "2. –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ (Policy evaluation)\n",
    "\n",
    "- –î–ª—è –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å—á–∏—Ç–∞–µ–º Q(s,a)\n",
    "\n",
    "- –û–±–Ω–æ–≤–ª—è–µ–º –º–∞—Å—Å–∏–≤ V[s] -> max(Q(s,a))\n",
    "\n",
    "\n",
    "*–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç Policy iteration —Å–∞–º–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ –≤ –ø–∞–º—è—Ç–∏ –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—å—Å—è, –æ–Ω–∞ –≥–µ–Ω–µ–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ Q-—Ñ—É–Ω–∫—Ü–∏–∏.\n",
    "\n",
    "\n",
    "3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ (Policy improvement)\n",
    "\n",
    "- –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—á–∏—Ç–∞–µ–º Q(s,a) –¥–ª—è –≤—Å–µ—Ö a\n",
    "\n",
    "- –í—ã–±–∏—Ä–∞–µ–º a –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ Q(s,a) - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\n",
    "\n",
    "- –ï—Å–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —à–∞–≥—É 2, –∏–Ω–∞—á–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è\n",
    "\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "r(s,a,s') - –Ω–∞–≥—Ä–∞–¥–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –≤—ã–±–æ—Ä—É –¥–µ–π—Å—Ç–≤–∏—è a –≤ s\n",
    "s - –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ \n",
    "s' - –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n",
    "s' - –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" \n",
    "    Computes Q(s,a) as in formula above \n",
    "    mdp : MDP object\n",
    "    state_values : dictionayry of { state_i : V_i }\n",
    "    state: string id of current state\n",
    "    gamma: float discount coeff\n",
    "    \"\"\"\n",
    "\n",
    "    next_states = mdp.get_next_states(state, action)\n",
    "\n",
    "    Q = 0.0\n",
    "\n",
    "    for next_state in next_states.keys():\n",
    "        p = next_states[next_state] # alternatively p = mdp.get_transition_prob(state, action, next_state)\n",
    "        Q += p * (mdp.get_reward(state, action, next_state) + gamma * state_values[next_state])\n",
    "    return Q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0 # Game over\n",
    "\n",
    "    q_max = float('-inf')\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "        q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "        q_max = max(q_max, q)\n",
    "    return q_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdp as MDP\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "display(MDP.plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "\n",
    "    new_state_values = {}\n",
    "    for s in state_values.keys():\n",
    "        new_state_values[s] = get_new_state_value(mdp,state_values,s,gamma)   \n",
    "\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s])\n",
    "               for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–¥–µ—Å—å –Ω–µ—Ç –Ω–∏–∫–∞–∫–æ–≥–æ \"—Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(MDP.plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    \n",
    "    best_action = None\n",
    "    q_max = float('-inf')\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "        q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "        if q > q_max:\n",
    "            best_action = a\n",
    "            q_max = q\n",
    "            \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(MDP.plot_graph_optimal_strategy_and_state_values(mdp, state_values, get_action_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q - Learning\n",
    "\n",
    "–í —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ø–æ–¥—Ö–æ–¥ –æ–ø–∏—Å–∞–Ω–Ω—ã–π –≤—ã—à–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç. –ù–∞–º –∑–∞—Ä–∞–Ω–µ–µ –Ω–µ –∏–∑–≤–µ—Å—Ç–Ω—ã –Ω–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning\n",
    "\n",
    "–º—ã –Ω–µ –∑–Ω–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤. –ú—ã –º–æ–∂–µ–º –ø–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –Ω–∏—Ö.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-19.png\" alt=\"Drawing\" width=\"700px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-31.png\" alt=\"Drawing\" width=\"700px;\"/>\n",
    "\n",
    "\n",
    "Q - —à—Ç—Ä–∏—Ö —Ä–∞—Å—Å—á–µ—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, Q –±–µ–∑ —à—Ç—Ä–∏—Ö–∞ —Ç–æ —á—Ç–æ –±—ã–ª–æ –≤ —Ç–∞–±–ª–∏—Ü–µ, –∞–ª—å—Ñ–∞ —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "**temporal difference**\n",
    "\n",
    "–ü—É—Å—Ç—å —É –Ω–∞—Å –µ—Å—Ç—å –æ—Ü–µ–Ω–∫–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è ùëÜùë° –∏ –¥–µ–π—Å—Ç–≤–∏—è ùê¥ùë° ùëÑ(ùëÜùë°,ùê¥ùë°). –°–¥–µ–ª–∞–µ–º –æ–¥–∏–Ω —à–∞–≥, –ø–æ–ª—É—á–∏–º —Ä–µ–∞–ª—å–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É r, –ø–æ—Å–º–æ—Ç—Ä–∏–º –æ—Ü–µ–Ω–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ ùëöùëéùë•ùëÑ(ùëÜùë°+1,ùê¥). –ù–æ –≤ –∏–¥–µ–∞–ª–µ ùëü+ùëöùëéùë•ùëÑ(ùëÜùë°,ùê¥)=ùëÑ(ùëÜùë°,ùê¥ùë°), –Ω–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑–Ω–∏—Ü–∞, —Ç–æ –µ–µ –º–æ–∂–Ω–æ –ø—ã—Ç–∞—Ç—å—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å.\n",
    "\n",
    "–ò–¥–µ—è TD –≤ —Ç–æ–º, —á—Ç–æ–±—ã —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –Ω–∞—à–µ–π –ø—Ä–æ—à–ª–æ–π –æ—Ü–µ–Ω–∫–æ–π –∏ —É—Ç–æ—á–Ω–µ–Ω–Ω–æ–π –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
    "\n",
    "Temporal - —Ç–∞–∫-–∫–∞–∫ –µ—Å—Ç—å –æ—Ü–µ–Ω–∫–∞ —Å–µ–π—á–∞—Å –∏ –µ—Å—Ç—å —É—Ç–æ—á–Ω–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω —à–∞–≥\n",
    "\n",
    "–í —Ç–µ–æ—Ä–∏–∏ –º–æ–∂–Ω–æ —Å–º–æ—Ç—Ä–µ—Ç—å —á–µ—Ä–µ–∑ n-—à–∞–≥–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning\n",
    "\n",
    "–ï—Å—Ç—å –∑–∞–¥–∞—á–∏ –≥–¥–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –æ–≥—Ä–æ–º–Ω–æ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω—ã–º.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-6.png\" width=\"700px;\"/>\n",
    "\n",
    "–Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –º—ã —É—á–∏–º –∞–ª–≥–æ—Ä–∏—Ç–º –∏–≥—Ä–∞—Ç—å –≤ –ø—Ä–æ—Å—Ç—É—é –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—É—é –∏–≥—Ä—É, —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –±—É–¥–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —ç–∫—Ä–∞–Ω–∞ (210x160).\n",
    "\n",
    "–í —Ç–∞–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö Q- —Ñ—É–Ω–∫—Ü–∏—é –Ω–µ —Å—á–∏—Ç–∞—é—Ç –≤ —è–≤–Ω–æ–º –≤–∏–¥–µ, –∞ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É—é—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible architectures**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img_license/yds-4-10.png\" alt=\"Drawing\" width=\"850px;\"/>\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "–ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–ª–∞ Q -–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –¥–µ–π—Å—Ç–≤–∏–π.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-11_.png\" alt=\"Drawing\" width=\"600px;\"/>\n",
    "\n",
    "<center><em> Approximate Q-learning <em><center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img_license/yds-4-14.png\" alt=\"Drawing\" width=\"700px;\"/>\n",
    "\n",
    "<center><em> Basic deep Q-learning <em><center> \n",
    "\n",
    "$\\epsilon -greedy$ –Ω—É–∂–Ω–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—Ä–µ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–º–∞**:\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-16.png\" alt=\"Drawing\" width=\"700px;\"/>\n",
    "\n",
    "–í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –æ–∫—Ä—É–∂–µ–Ω–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –ø–æ–ª—É—á–∞–µ–º–∞—è –∞–≥–µ–Ω—Ç–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –Ω–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ. –¢.–µ. –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π (—á—Ç–æ –ø–æ–Ω—è—Ç–Ω–æ –∏–∑ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç.–∫. –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –æ–∫—Ä—É–∂–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è RL, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –Ω–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã). –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ —É—Ö—É–¥—à–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –Ω–∞–º –Ω—É–∂–µ–Ω —Å–ø–æ—Å–æ–±, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—É—Å—Ç—Ä–∞–Ω–∏—Ç—å –∏–ª–∏ —Å–Ω–∏–∑–∏—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏). \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-62.png\" alt=\"Drawing\" width=\"700px;\"/>\n",
    "\n",
    "–û–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ **–ø—Ä–æ–∏–≥—Ä—ã–≤–∞–Ω–∏—è –æ–ø—ã—Ç–∞ (experience replay)**. –°—É—Ç—å —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≤ —Ç–æ–º, —á—Ç–æ –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å–æ—Å—Ç–æ—è–Ω–∏–µ, –¥–µ–π—Å—Ç–≤–∏—è, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ) –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–º –±—É—Ñ–µ—Ä–µ –∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –º–∏–Ω–∏-–±–∞—Ç—á–∏ –∏–∑ —ç—Ç–æ–≥–æ –±—É—Ñ–µ—Ä–∞.\n",
    "\n",
    "–¢–∞–∫ –∂–µ **experience replay** –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–π –ø—Ä–æ—à–ª—ã–π –æ–ø—ã—Ç.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î—Ä—É–≥–∏–µ —É–ª—É—á—à–µ–Ω–∏—è DQN\n",
    "\n",
    "## Prioritized Experience Replay\n",
    "\n",
    "–ú–∏–Ω–∏–±–∞—Ç—á–∏ –∏–∑ –ø–∞–º—è—Ç–∏ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –Ω–µ —Å —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º, –∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ç—É–¥–∞ –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è Q —Å–∏–ª—å–Ω–µ–µ –≤—Å–µ–≥–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –æ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö. –¢.–µ. –ø—Ä–∏–º–µ—Ä—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º **TD error** –ø–æ–ª—É—á–∞—é—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç.\n",
    "\n",
    "## Dueling networks\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –≤ —Ç–æ–º, —á—Ç–æ –º—ã —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞—à—É —Å–µ—Ç—å –Ω–∞ –¥–≤–µ –≥–æ–ª–æ–≤—ã, –æ–¥–Ω–∞ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è \\\\( V(S) \\\\), –∞ –≤—Ç–æ—Ä–∞—è - –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –æ–¥–Ω–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞–¥ –¥—Ä—É–≥–∏–º–∏ \\\\( A(s, a) = Q(s, a) - V(s) \\\\). –≠—Ç–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è advantage. –î–∞–ª–µ–µ –∏–∑ —ç—Ç–∏—Ö –¥–≤—É—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º—ã —Å–æ–±–∏—Ä–∞–µ–º –Ω–∞—à—É Q-—Ñ—É–Ω–∫—Ü–∏—é, –∫–∞–∫ \\\\( Q(s,a) = V(s) + A(a) \\\\)\n",
    "\n",
    "## Noisy nets\n",
    "\n",
    "–¢.–∫. –ø–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç –±—É–¥–µ—Ç —Å—Ç—Ä–µ–º–∏—Ç—å—Å—è –≤—ã–±–∏—Ä–∞—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Q, —Å—Ä–µ–¥–∏ —É–∂–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö, —ç—Ç–æ –º–æ–∂–µ—Ç –ø–æ–º–µ—à–∞—Ç—å –µ–º—É –Ω–∞–π—Ç–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –µ–≥–æ –µ—â—ë –Ω–µ –±—ã–ª–æ. –û–¥–Ω–∏–º –∏–∑ —Ä–µ—à–µ–Ω–∏–π —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã —è–≤–ª—è–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏ —Å–ª—É—á–∞–π–Ω–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–æ—Ç–æ—Ä–æ–π —Ç–∞–∫ –∂–µ –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞.\n",
    "\n",
    "## Multi-step learning/n-step learning\n",
    "\n",
    "–û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã —Å—á–∏—Ç–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–µ –ø–æ –¥–≤—É–º —Å–æ—Å–µ–¥–Ω–∏–º –ø—Ä–∏–º–µ—Ä–∞–º, –∞ —Å—Ä–∞–∑—É –ø–æ n. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–µ—Ç–∏ –ª—É—á—à–µ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π.\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Q –∑–∞–º–µ–Ω—è–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º Z —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –≤ —Ö–æ–¥–µ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "# Rainbow\n",
    "\n",
    "State of the art –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ Q-–æ–±—É—á–µ–Ω–∏—è - –Ω–∞–±–æ—Ä –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤—ã—à–µ —Ç–≤–∏–∫–æ–≤. –ù–∞ –≥—Ä–∞—Ñ–∏–∫–µ –Ω–∏–∂–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—á–∫–æ–≤, —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω–æ–µ –ø–æ –∏–≥—Ä–∞–º Atari –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å–æ —Å—Ä–µ–¥–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —á–µ–ª–æ–≤–µ–∫–∞.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/rainbow_dqn.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å—Ä–µ–¥ –¥–ª—è RL\n",
    "\n",
    "\n",
    "–í –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö –∫ –∫–æ–¥—É –≤—ã—à–µ, –≥–æ–≤–æ—Ä–∏–ª–æ—Å—å –æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Gym\n",
    "\n",
    "*\"Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.\"*\n",
    "\n",
    "\n",
    "\n",
    "form [openai](https://openai.com/projects/)\n",
    "https://gym.openai.com/docs/\n",
    "\n",
    "\n",
    "–°–ø–∏—Å–æ–∫ –æ–∫—Ä—É–∂–µ–Ω–∏–π:\n",
    "\n",
    "https://gym.openai.com/envs/#classic_control\n",
    "\n",
    "\n",
    "https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb\n",
    "\n",
    "...\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –æ–∫—Ä—É–∂–µ–Ω–∏—è** \n",
    "–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –≥–æ—Ç–æ–≤–∞—è —Å—Ä–µ–¥–∞: –º–∞—à–∏–Ω–∫–∞ –¥–æ–ª–∂–Ω–∞ –∑–∞–µ—Ö–∞—Ç—å –Ω–∞ –≥–æ—Ä–∫—É. –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è. –ê –∞–≥–µ–Ω—Ç - —Å–ª—É—á–∞–π–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ. –û–Ω–æ –Ω–∏—á–µ–º—É –Ω–µ —É—á–∏—Ç—Å—è, –ª–∏—à—å —Ö–∞–æ—Ç–∏—á–µ—Å–∫–∏ –≤—ã–¥–∞—ë—Ç –¥–µ–π—Å—Ç–≤–∏—è, –Ω–æ —ç—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≥–¥–µ –≤–∑—è—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Num        | Observation           | Min  | Max |\n",
    "| ------------- |:-------------:|:-------------:|-------------:|\n",
    "| 0      | position | -1.2 | 0.6 |\n",
    "| 1      | velocity | -0.07 | 0.07 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Num        | Action |\n",
    "| ------------- |-------------:|\n",
    "| 0\t| push left |\n",
    "| 1\t| no push |\n",
    "| 2\t| push right |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary package for show visualization in Colab\n",
    "# https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab\n",
    "\n",
    "!apt-get install -y xvfb python-opengl\n",
    "!pip install gym pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some helper functions and init virtual display\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def query_environment(name):\n",
    "    env = gym.make(name)\n",
    "    spec = gym.spec(name)\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "    print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "    print(f\"Reward Range: {env.reward_range}\")\n",
    "    print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
    "\n",
    "def draw(env):\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_environment(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gym/wiki/MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "MAX_NUM_EPISODES = 1\n",
    "\n",
    "for episode in range(MAX_NUM_EPISODES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n",
    "    step = 0\n",
    "    while not done:\n",
    "        draw(env)       \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Sample random action.\n",
    "        # This will be replaced by our agent's action\n",
    "        # when we # start developing the agent algorithms\n",
    "\n",
    "        #action = int(input()) 0,1,2\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # Send the action to the environment and receive       \n",
    "        # the next_state, reward and whether done or not\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        obs = next_state\n",
    "        \n",
    "        if step > 50:\n",
    "          # Forse exit in Colab demo\n",
    "          done = True\n",
    "          \n",
    "    print(\"\\n Episode #{} ended in {} steps. total_reward={}\".format(episode, step+1,\n",
    "total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_env = gym.wrappers.Monitor(env, 'test', force = True)\n",
    "wrapped_env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = wrapped_env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wrapped_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ—Ä c CartPole DQN\n",
    "\n",
    "Deep Q Network  = DQN \n",
    "\n",
    "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
    "\n",
    "CartPole is the simplest one. It should take several minutes to solve it.\n",
    "\n",
    "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "################################################\n",
    "# For CartPole\n",
    "################################################\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    \n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/atari_wrappers.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/utils.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/replay_buffer.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/framebuffer.py\n",
    "\n",
    "!pip install gym[box2d]\n",
    "\n",
    "!touch .setup_complete\n",
    "\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env = make_env()\n",
    "env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map observations to state q-values.\n",
    "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device\n",
    "print(state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        \n",
    "        \n",
    "        # Define NN\n",
    "        ##############################################\n",
    "        hidden_size = 150\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        ##############################################\n",
    "\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        \n",
    "        ##############################################\n",
    "        qvalues = self._nn(state_t)\n",
    "        ##############################################\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2 and \n",
    "            qvalues.shape[0] == state_t.shape[0] and \n",
    "            qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer and Target Networks\n",
    "\n",
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "\n",
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Loss\n",
    "\n",
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # with torch.no_grad():\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    ##############################################\n",
    "    next_state_values = predicted_next_qvalues.max(axis=-1)[0]\n",
    "    ##############################################\n",
    "\n",
    "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
    "        \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    ###############################################\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * is_not_done\n",
    "    ##############################################\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
    "            \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim() == 1, \\\n",
    "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
    "            \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# your favourite random seed\n",
    "seed = 2\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = make_env(seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∞–≥–µ–Ω—Ç –¥–ª—è –∏–≥—Ä—ã, –≤—Ç–æ—Ä–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü–µ—Ä–µ–æ–¥–∏—á–µ—Å–∫–∏ –≤–µ—Å–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–∞–µ–Ω—Ç–∞ –∫–æ–ø–∏—Ä—É—é—Ç—Å—è –≤ \"–∏–≥—Ä–æ–∫–∞\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = list()\n",
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \n",
    "    hint: use agent.sample.actions\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for _ in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        \n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        # action = action.argmax(axis=-1)[0]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        sum_rewards += reward\n",
    "        g.append(reward)\n",
    "        exp_replay.add(s, action, reward, state, done)\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        \n",
    "        s = state\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import src.L15_RL.lib.utils\n",
    "import utils\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print('less that 100 Mb RAM available, freezing')\n",
    "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        # <YOUR CODE: sample batch_size of data from experience replay>\n",
    "        s,a,r,next_s, is_done = exp_replay.sample(batch_size)\n",
    "        # loss = <YOUR CODE: compute TD loss>\n",
    "        loss = compute_td_loss(s, a, r, next_s, is_done, agent, target_network)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            # <YOUR CODE>\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(evaluate(\n",
    "                make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues(\n",
    "                [make_env(seed=step).reset()]\n",
    "            )\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  make_env(),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)\n",
    "assert final_score > 300, 'not good enough for DQN'\n",
    "print('Well done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞:\n",
    "\n",
    "* Barto Sutton Reinforcement Learning: An Introduction (–∫–ª–∞—Å—Å–∏–∫–∞ —Ç–µ–æ—Ä–∏–∏)\n",
    "* Lapan Deep Reinforcement Learning Hands-On Second Edition (–ö–Ω–∏–≥–∞ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ RL)\n",
    "* Sudharsan Ravichandiran Hands-On Reinforcement Learning with Python (–¢–æ–∂–µ –º–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤)\n",
    "\n",
    "–ü—Ä–æ—á–∞—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞:\n",
    "\n",
    "* Alex Zai and Brandon Brown Deep Reinforcement Learning in Action\n",
    "\n",
    "\n",
    "–ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏:\n",
    "* https://www.alexirpan.com/2018/02/14/rl-hard.html Deep Reinforcement Learning Doesn't Work Yet\n",
    "* https://openai.com/blog/faulty-reward-functions/  Faulty Reward Functions in the Wild\n",
    "\n",
    "== –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ ==\n",
    "* [BURLAP](http://burlap.cs.brown.edu) (Brown-UMBC Reinforcement Learning and Planning)¬†‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –æ–¥–Ω–æ- –∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —è–∑—ã–∫ Java, –ª–∏—Ü–µ–Ω–∑–∏—è LGPL\n",
    "* [MMLF](http://mmlf.sourceforge.net/) (Maja Machine Learning Framework)¬†‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏ –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å—Ä–µ–¥ –¥–ª—è –∏—Ö –ø—Ä–æ–≤–µ—Ä–∫–∏, —è–∑—ã–∫ Python, –ª–∏—Ü–µ–Ω–∑–∏—è GPL\n",
    "* [OpenAI Gym](https://gym.openai.com)¬†‚Äî –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –æ—Ç [[OpenAI]], —è–∑—ã–∫ Python, –ª–∏—Ü–µ–Ω–∑–∏—è MIT\n",
    "* [PyBrain](http://www.pybrain.org/)¬†‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —è–∑—ã–∫ Python, –ª–∏—Ü–µ–Ω–∑–∏—è BSD\n",
    "* [RLPy](https://rlpy.readthedocs.io/en/latest)‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —è–∑—ã–∫ Python, 3-—Ö –ø—É–Ω–∫—Ç–æ–≤–∞—è –ª–∏—Ü–µ–Ω–∑–∏—è BSD\n",
    "* [Teachingbox](http://servicerobotik.hs-weingarten.de/en/teachingbox.php)¬†‚Äî –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —è–∑—ã–∫ Java, –ª–∏—Ü–µ–Ω–∑–∏—è GPL"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
