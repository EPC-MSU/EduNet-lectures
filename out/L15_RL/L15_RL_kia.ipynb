{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Reinforced learning</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Обучение с учителем\n",
    "Если у нас есть набор примеров с правильными ответами, то мы используем эту выборку для обучения нашей модели, а после обучения, применяем её к неразмеченным данным. Именно этот подход мы использовали, когда обучали классификатор для MNIST, подавая на вход сети картинки с изображениями рукописных цифр и считая градиент для подстройки весов на основе разницы между известным лэйблом цифры и выходом нейросети.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение без учителя\n",
    "В некоторых случаях у нас нет размеченных данных, на которых мы могли бы заранее обучить модель. Но, при решении некоторых задач, можно обойтись без размеченной выборки. Примером такой задачи является задача кластеризации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением\n",
    "В некоторых случаях существующие методы обучения без учителя нам не подходят. В то же время у нас нет возможности создать качественную обучающую выборку. При этом мы можем постфактум оценить действия нашей модели и использовать эту оценку подстроить модель так, чтобы она чаще совершала желательные действия и реже - нежелательные. В литературе такую оценку называют вознаграждением (reward), а обучение строится таким образом, чтобы это модель стремилась максимизировать получаемое вознаграждение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже оставить только Brances of Machine Learning(левую картинку)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-7.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-1-17.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Терминология: агент, функция награды, состояние среды\n",
    "\n",
    "Агент и среда - ключевые понятия в обучении с подкреплением.\n",
    "\n",
    "**Агент** - программа, принимающая решение о дальнейших действиях на основе информации о состоянии среды.\n",
    "\n",
    "**Среда** - это мир, в котором агент должен \"выживать\", т.е. всё, с чем агент может прямо или косвенно взаимодействовать. Среда обладает состоянием (State), агент может влиять на среду, совершая какие-то действия (Actions), переводя среду при этом из одного состояния в другое и получая какое-то вознаграждение. Среда описывается пространством возможных состояний. Конкретное состояние - вектор в этом пространстве.\n",
    "\n",
    "В зависимости от конкретной задачи, агент может наблюдать либо полное состояние среды, либо только некоторую его часть. Во втором случае, агенту может потребоваться какое-то внутреннее представление полного состояния, которое будет обновляться по мере получения новых данных.\n",
    "\n",
    "**Фукнция награды** - вводимая программистом формула вычисления ценности действия на основе финального результата, ожидания этого результата, промежуточных результатов и любых других параметров, которые будут подсказывать путь к наилучшей последовательности действий агента. Это некоторый аналог функции потерь, без которой непонятно чему учиться. Например, в шахматах истинная награда это победа, но взятая фигура соперника тоже ценна и должна увеличивать награду, если мы хотим подсказать агенту, что брать чужие фигуры полезно. Может ли после этого агент получать мат, позарившись на незащищенную фигуру? Да, ровно как и неопытный шахматный игрок. Попытка передать через дополнительные неосновные награды подсказки к получению основной награды называется **reward shaping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Различные подходы в обучении Нейронных сетей\n",
    "\n",
    "![image.png](http://localhost:8889/files/Documents/GitHub/EduNet-lectures/out/L15_RL/%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA%20%D1%8D%D0%BA%D1%80%D0%B0%D0%BD%D0%B0%202021-09-12%20%D0%B2%2023.41.31.png?_xsrf=2%7C25cb4d43%7C760af36e5f63b276724c80cd6232c4b6%7C1630564945)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отличие от supervised learning\n",
    "\n",
    "Обучение с подкреплением\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/rl_idea.png\" width=\"700\"/>\n",
    "\n",
    "Обучение с учителем\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/sv_idea.png\" width=\"700\"/>\n",
    "\n",
    "Разница:\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/rl_rand.png\" width=\"700\"/>\n",
    "\n",
    "- Неопределенность: переходы и награды случайны\n",
    "- Отложенность r_t+1 может не зависеть от a_t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классические примеры задач RL\n",
    "\n",
    "* Научить автопилот водить машину\n",
    "* Управление роботами, БПЛ\n",
    "* Научить нейронную сеть играть в игры/видеоигры\n",
    "* Составление расписаний с помощью нейросети\n",
    "* Рекомендательные системы\n",
    "* Трейдинг, принятие решений в условиях неопределенности, управление инвестициями\n",
    "* drug discovery: есть много свойств химических веществ, которые подсчитываются специальными программами. Выходы которых, очевидно, недиффиренцируемы. Если мы хотим генерировать вещества с заданными свойствами, то один из подходов - RL. \n",
    "* Не так давно одна российская металлургическая компания использовала RL для оптимаизации работы прокатного стана\n",
    "* NAS (Neural Architecture Search): при поиске оптимальных топологий сети можно решать задачу с помощью RL, награждая нашего агента за нахождение ххороших топологий и наказывая за плохие\n",
    "\n",
    "\n",
    "Во всех этих задачах не возможно или не практично собрать и размечать датасет, по этому используется обучение с подкреплением.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Особенности RL. Сложности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Низкая скорость обучения (sample efficiency)\n",
    "\n",
    "Общая проблема всех алгоритмов обучения с подкреплением - низкая скорость обучения. В то время, как человеку может быть достаточно одного повторения, чтобы выучить какое-то действие, агенту RL требуется десятки тысяч повторений даже в простых задачах. В какой-то степени это связано с несовершенством архитектуры, но самый большой вклад даёт тот факт, что человек может использовать накопленный в прошлом опыт из других областей. Игра Montezuma's Revenge - популярная подопытная среда для RL в последнее время. И яркий пример низкой эффективности повторений  у алгоритмов RL по сравнению с человеком. \n",
    "\n",
    "Челокек, как правило, быстро понимает, что нужно избегать черепа и забрать ключ, гравитация направлена вниз, а падение с большой высоты опасно. Алгоритму же приходиться обучаться с полного нуля. Если же подменить элементы интерфейса на неочевидные для человека, то его sample-efficency тоже сильно падает (хотя всё-равно лучше, чем RL).\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/game_prior.gif\" width=\"400\">\n",
    "\n",
    "А теперь то же самое, но в нечеловекочитаемом виде. Для RL разницы нет, а для человека сразу стало сложнее.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/game_no_prior.gif\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сложное проектирование функции награды\n",
    "Так же важным фактором являются редкие награды. Часто в ходе одного эпизода алгоритм делает множество различных действий, а награду получает только в конце. Соответственно, веса сети можно обновить только в конце эпизода и нельзя поощрить или наказать конкретные действия внутри эпизода. В итоге требуется большое количество повторений для достижения оптимальных весов.\n",
    "\n",
    "Один из способов улучшить эффективность при редких наградах - reward shaping - модификация функции награды так, чтобы явно поощрялись какие-то действия внутри эпизода. Но качественно сконструировать такую функцию тяжело, а ошибки в ней могут приводить к неожиданным эффектам.\n",
    "\n",
    "**Пример 1** - В гонке лодок агент получал вознаграждене не только за победу в гонке, но и за сбор игровых бонусов. В итоге он решил, что гонка не очень-то и нужна, достаточно собирать бонусы.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/coastrunner.gif\" width=\"400\">\n",
    "\n",
    "**Пример 2** - У данного агента мы наблюдаем попадание в локальный минимум. Этот агент получает поощрение за набранную скорость. На начальном этапе во время случайного поиска агент обнаружил, что кувыркнуться вперёд даёт хорошее вознаграждение в начале. Постепенно, после нескольких попыток, переворачивание на спину закрепилось, как успешная стратегия. После закрепления такого поведения агент не смог выйти из этого состояния, т.к. оказалось проще научиться двигаться в таком состоянии, чем научиться переворачиваться обратно на ноги. Похожее поведение можно случано получить, если поощрять агента за то, что его ноги оторваны от земли.\n",
    "\n",
    "<center><video controls src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/upsidedown_half_cheetah.mp4\" width=\"400\"> </video></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Невоспроизводимость обучения\n",
    "Обычно мы всегда начинаем со случайного распределения весов, что не мешает стабильно получать результат обучения. Но с RL это не так. Даже в простой задаче с 3 степенями свободы (state - трёхмерный вектор) и одной степенью воздействия (action - скаляр), обучение с разной инициализацией генератора псевдослучайных чисел привело в 30% случаев к фиаско. Заранее понять, что обучение пошло плохо, нельзя или сложно.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/pendulum_results.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасеты\n",
    "\n",
    "Инструментарий для разработки и сравнения алгоритмов обучения с подкреплением\n",
    "\n",
    "OpenAI \n",
    "\n",
    "[Environments](https://gym.openai.com/envs/#classic_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Состояние (State)\n",
    "\n",
    "В процессе взаимодействия со средой агент накапливает историю $H_t=R_1, O_1, A_1,...,R_t,O_t,A_t$. Очевидно, что для принятия решения, хранение всей истории крайне избыточно:\n",
    "\n",
    "*   Games: $O_t$ - скриншот экрана (1200х700х3)\n",
    "*   Markets: оборот NYSE - 474m акций в день\n",
    "\n",
    "Мы хотим иметь такое представление $S_t = f(H_t)$, которое было бы \"достаточной статистикой\" для будущего.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov property\n",
    "\n",
    "Сделаем также важное предположение о природе нашего процесса - он полностью описывается своим текущим состоянием. Все, что произйдет в будущем не зависит от информации из прошлого, кроме той, что мы уже наблюдаем в настоящем. \n",
    "\n",
    "Классические примеры такого:\n",
    "\n",
    "1. Игральный кубик. Мы знаем, что на нем выпадет любая из гранней с некоторой фиксированной вероятностью. На это никак не влияет то, что до этого на кубике выпало 6 шестерок подряд. Это может повлиять на нашу оценку вероятностей выпадения той или иной грани, но не на реальную вероятность. \n",
    "\n",
    "2. Шахматы. \"Текущая позиция на доске + чей ход\" однозначно описывает игру. \n",
    "\n",
    "3. А подходит ли покер? \n",
    "\n",
    "С одной стороны - да. Текущее количество денег у каждого из игроков однозначно описывает игру. Но это если мы не учитываем блеф и прочее, которые могут сработать / не сработать в завимости от предыдущих ситуаций в игре. То есть, все зависит от того, как именно мы будем описывать покер и какие допущения накладываем. \n",
    "\n",
    "И получение хорошего представления S - тоже важная задача. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Определение Markov Property\n",
    "\"The future is independent of the past given the present\"\n",
    "\n",
    "Сосотояние $S_{t}$ является Марковым тогда и только тогда:\n",
    "$$\n",
    "p\\left(r_{t}, s_{t+1} \\mid s_{0}, a_{0}, r_{0}, \\ldots, s_{t}, a_{t}\\right)=p\\left(r_{t}, s_{t+1} \\mid s_{t}, a_{t}\\right)\n",
    "$$\n",
    "\n",
    "**Что происходит потом, зависит только от предыдущего состояния**\n",
    "\n",
    "Вероятность перехода в новое состояние зависит только от текущего состояния и текущего действия.\n",
    "\n",
    "\n",
    "Марковский процесс описывает **(fully observable) полностью наблюдаемые** среды. Можно описать их так:\n",
    "\n",
    "* Информации, которую получает агент в момент времени t достаточно, чтобы принять оптимальное решение для момента времени t\n",
    "* Текущее состояние среды содержит всю релевантную информацию из прошлого\n",
    "* Текущее состояние - это достаточная статистика будущего -> не существует никакой дополнительной информации, которая могла бы улучшить наше описание будущего\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov process \n",
    "\n",
    "Представим себе, что студент живет вот по такой схеме. Заметим, что влиять в такой схеме на свои решения он не может - все решается подкидыванием кубика\n",
    "\n",
    "\n",
    "![alttext](https://kodomo.fbb.msu.ru/FBB/year_20/ml/markov_process.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение \n",
    "Марковский процесс (цепь) это кортеж $(S, P)$, где\n",
    "- $S$ - принимает дискретные (конечные значения)\n",
    "- $P$ - матрица переходов (transition matrix)\n",
    "$$\n",
    "P_{s s^{\\prime}}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s\\right)\n",
    "$$\n",
    "Строго говоря необходимо еще распределение начальных состояний (но мы предполагаем, что оно вырождено, т.е. мы знаем где начинаем с вероятностью 1 ).\n",
    "\n",
    "Марковский процесс - основа для RL. Мы будем постепенно усложнять эту модель, добавляя rewards и actions.\n",
    "$$\n",
    "P_{s s^{\\prime}}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В определении нам встретилась матрица переходов, зададим ее тоже формально"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матрица состояний \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition matrix\n",
    "\n",
    "Пусть $S_{t}$ - последовательность дискретных состояний.\n",
    "\n",
    "(Пример будет очень скоро).\n",
    "\n",
    "Поскольку последовательность задается распределением \n",
    "\n",
    "$\\operatorname{Pr}\\left(S_{t+1} \\mid S_{t}\\right)$ естественно упорядочить его в матрицу. $P_{s s^{\\prime}}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s\\right)$\n",
    "\n",
    "$$\n",
    "\\mathcal{P}=\\text { from }\\left[\\begin{array}{ccc}\n",
    "\\mathcal{P}_{11} & \\ldots & \\mathcal{P}_{1 n} \\\\\n",
    "\\vdots & & \\\\\n",
    "\\mathcal{P}_{n 1} & \\ldots & \\mathcal{P}_{n n}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Какие суммы вероятностей должны равняться единице?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Награда (Reward)\n",
    "\n",
    "Наши состояния, очевидно, неравноценны. Давайте к каждому состоянию привяжем награду $R_s$ \n",
    "Она может быть как нулевой, положительной или отрицательной.\n",
    "\n",
    "Например, в марковском процессе принятия решений(MDP) описывающем игру в шахматы или GO, награда агента может быть положительной только при переходе в состояние \"партия выигранна\"\n",
    "\n",
    "На всех остальных ходах награда будет нулевой.\n",
    "\n",
    "Для перемещающегося робота награда на каждом шаге может быть отрицательной, так как на перемещение требуется энергия. и.т.п. \n",
    "\n",
    "Для покера наградой можно назначить изменение текущей суммы игрока после сыгранной раздачи. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схема для студента в этом случае модифицируется следующим образом. Заметьте, награды тут расставляются с точки зрения актора - студента \n",
    "\n",
    "![alttext](https://kodomo.fbb.msu.ru/FBB/year_20/ml/markov_reward.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Суммарная награда (Return)\n",
    "\n",
    "Для турнира по шахматам, где играется 10 партий, финальный reward будет складываться из reward за каждую партию в отдельности. \n",
    "\n",
    "Если у нас есть тест из 40 вопросов, то опять же, нам не важно, как именно мы получили 30 баллов, главное - мы их получили \n",
    "\n",
    "То есть в таких случаях мы можем считать Return следующим образом:\n",
    "\n",
    "$$Return = \\sum_i {R_i}$$\n",
    "\n",
    "\n",
    "А теперь представим себе, что мы можем работать 2 года бесплатно и получить разом 5 миллионов рублей. А можем работать 2 года и каждый год получать 70 тысяч. \n",
    "\n",
    "\n",
    "Или мы можем 6 лет учиться, получая сторонними активностями 20 тысяч в месяц, а затем сразу начать получать 150 тысяч в месяц, а затем и больше. А можем сразу пойти на работу и начать зарабатывать 50 тысяч в месяц, постепенно дойдя до 100.  \n",
    "\n",
    "Что лучше - опубликовать две статьи в журнале с импакт фактором 5 в этом году или 1, но через 2 года и в журнале с импакт-фактором 20?\n",
    "\n",
    "Что выгоднее? \n",
    "\n",
    "Очевидно, в таких ситуациях лучше учитывать не только Reward, но и то, когда он получен. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дисконтирование (discounting)\n",
    "\n",
    "\n",
    "Поэтому при оценке куммулятивной награды на шаге $t (G_t)$ используется коэффициент дисконтирования $\\gamma$ \n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + ... =\\sum^{\\infty}_{k=0} \\gamma ^ kR_{t+k+1}\n",
    "$$\n",
    "\n",
    "тут $R_{t+1}, R_{t+2}, ...$ - это вознаграждение по ребрам переходов\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Какое значение $\\gamma$ выбрать? \n",
    "\n",
    "\n",
    "**Можно ли его выбрать равным 1?** \n",
    "\n",
    "Да, мы уже это делали ранее. \n",
    "\n",
    "**Можно ли его выбрать равным 0?**\n",
    "\n",
    " Тоже да. В этом случае у нас получится \"жадный\" алгоритм - мы всегда выбираем решение, которое дает максимальную награду сейчас.\n",
    "\n",
    "**Может $\\gamma = 0$ привести в проблемам?**\n",
    "\n",
    "Да, например:\n",
    "\n",
    "* Вариант1: Получить сегодня 1000 рублей.\n",
    "\n",
    "* Вариант2: Получить завтра 100000 рублей\n",
    "\n",
    "Обычно, второй вариант более предпочтителен. Но жадный алгоритм его не увидит.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Может $\\gamma = 1$ привести в проблемам?**\n",
    "\n",
    "Да, увидим чуть позже. \n",
    "\n",
    "\n",
    "Представим себе генератор шуток про медведя. Большая часть людей знает эту шутку, потому хорошо бы посередине вставлять дополнительные детали - \"для ценителей\". Но заканчиваться шутка должна одинаково. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обычно $0 \\le \\gamma \\le 1$** \n",
    "\n",
    "Обычно $\\gamma$ ставят равной чему-то между двумя этими крайностями. \n",
    " \n",
    "Близость $\\gamma$ к 0 отражает нашу \"нетерпеливость\" - насколько важно получить награду именно сейчас. \n",
    "\n",
    "Наличие такого $\\gamma$ позволяет нам не делать различия моделями с ограниченным числом шагов и неограниченным - теперь по-любому return будет конечным числом, т.к выражение для return ограничено сверху суммой бесконечно убывающей геометрической последовательности \n",
    "\n",
    "Пусть $R_{max} = max R_i$ - максимальная награда, которую мы в принципе можем получить в каком-то состоянии. \n",
    "\n",
    "$G_t = R_{t+1} + \\gamma \\cdot R_{t+2} + \\gamma^2 \\cdot R_{t+3} + ... \\le R_{max} + \\gamma \\cdot R_{max} + \\gamma^2 \\cdot R_{max} + ... = R_{max} (1 + \\gamma + \\gamma^2 + ... ) \\le R_{max} \\cdot \\frac{1}{1-\\gamma} = const $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-2-15.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G_{0}=\\sum_{k=0}^{\\infty} \\gamma^{k}=\\frac{1}{1-\\gamma}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно понимать, что выбор $\\gamma$ меняет задачу, которую мы решаем и, соответственно, меняет решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение (value of the state)\n",
    "$$\n",
    "v(s)=\\mathbb{E}\\left[G_{t} \\mid S_{t}=s\\right]\n",
    "$$\n",
    "Ценность состояния - ожидаемая сумма всех полученных rewards, если стартовать из $\\mathrm{s}$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v(s) &=\\mathbb{E}\\left[G_{t} \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}\\left[R_{t+1}+\\gamma\\left(R_{t+2}+\\gamma R_{t+3}+\\ldots\\right) \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}\\left[R_{t+1}+\\gamma v\\left(S_{t+1}\\right) \\mid S_{t}=s\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уравнение Беллмана\n",
    "\n",
    "Задача оптимальной политики решается через поиск оптимальных V* и Q* - функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение Optimal Value Function\n",
    "\n",
    "Оптимальное значение состояния функции  $v_{*}(s)$ это максимальное значение функции для всех политик \n",
    "$$\n",
    "v_{*}(s)=\\max _{\\pi} v_{\\pi}(s)\n",
    "$$\n",
    "Оптимальное значение действия функции $q_{*}(s, a)$ это максимальное значение действия функции для всех политик \n",
    "$$\n",
    "q_{*}(s, a)=\\max _{\\pi} q_{\\pi}(s, a)\n",
    "$$\n",
    "\n",
    "- Оптимальное значения функции определяет наилучшую возможную производительность в MDP\n",
    "- MDP считается решенным, когда мы знаем оптимальное значение $\\mathrm{fn}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation\n",
    "\n",
    "Для ее решения используется рекуррентное уравнение [Белмана](https://en.wikipedia.org/wiki/Richard_E._Bellman)\n",
    "\n",
    "\n",
    "Мы можем предстваить функцию как немедленное получение награды плюс дисконтированное значение предыдущего состояния\n",
    "$$\n",
    "v_{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right]\n",
    "$$\n",
    "\n",
    "так же как и \n",
    "$$\n",
    "q_{\\pi}(s, a)=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma q_{\\pi}\\left(S_{t+1}, A_{t}\\right) \\mid S_{t}=s, A_{t}=a\\right]\n",
    "$$\n",
    "\n",
    "Q функция убирает стохастичность политики на первом шаге. С помощью нее мы фиксируем наш первый шаг из этого состояния.\n",
    "\n",
    "\n",
    "На практике мы будем считать Q и V так:\n",
    "\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n",
    "Для получения V будем использовать Q \n",
    "\n",
    "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$\n",
    "\n",
    "\n",
    "фактически q задает политику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример\n",
    "\n",
    "![alttext](https://kodomo.fbb.msu.ru/FBB/year_20/ml/bel_example.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Markov reward process (MRP)\n",
    "MRP это кортеж $(S, R, P, \\gamma)$, где \n",
    "\n",
    "$S$ - принимает дискретные(конечные значения)\n",
    "\n",
    "$R$ - функция rewards, $R_s = \\mathbb{E}[R_{t+1}|S_t=s]$\n",
    "\n",
    "$P$ - матрица переходов(transition matrix)\n",
    "\n",
    "$P_{ss^`}=Pr(S_{t+1}=s^`|S_t=s)$\n",
    "\n",
    "$\\gamma$ - коэффициент дисконтирования(discount factor)\n",
    "\n",
    "Осталось добавить actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Марковский процесс принятия решений\n",
    "Markov decision process (MDP) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавляем действия, но пока убираем случайность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого у нас получалось малость неадекватное представление процесса - студент не мог ни на что повлиять. \n",
    "\n",
    "На самом же деле студент может решать, куда ему надо - в аудиторию, домой спать или в бар.\n",
    "\n",
    "\n",
    "Представим себе другой пример, выкинув случайность. \n",
    "\n",
    "Мы решили написать программу, которая генерирует шутки про медведя. Шутка должна начинаться с \"Шел медведь по лесу\" и заканчиваться \"Сел в машину и сгорел\". Как бы это описать? \n",
    "\n",
    "![alttext](https://kodomo.fbb.msu.ru/FBB/year_20/ml/burning_bear.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть у нас есть состояния, а модель сама решает, куда ей переходить. Вероятностей мы тут не добавили. Что модель решила - то и будет. \n",
    "\n",
    "Сразу видим и проблему, которую мы получаем при $\\gamma = 1$, пусть и в утрированном виде. Ничто не запрещает нашей модели бесконечно добавлять детали к анекдоту, тем самым увеличивая $G_t$. \n",
    "Работать с такой моделью невозможно. \n",
    "\n",
    "Если в какой-то момент обучения наша модель стала походить на эту - то ничего осмысленного мы и далее не получим. А вот если введением дискаунта дать модели понять, что слушатели анекдота все же хотят вскоре услышать его конец - то проблема уйдет \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возвращаем случайность \n",
    "\n",
    "Наше действие, к сожалению, не всегда определяет состояние, в которое мы перейдем.\n",
    "\n",
    "Хочет наш студент пойти на лекцию, но по пути встречает товарища - и идут они в бар. \n",
    "\n",
    "Хочет он пойти спать, а по пути к выходу из университета встречает лектора с хорошей памятью, и идет на лекцию. \n",
    "\n",
    "И т.д.\n",
    "\n",
    "Как это отразить на схеме так, чтобы это можно было прочитать? \n",
    "\n",
    "Введем промежуточные состояния, куда нас переводят действия студента. А уже из этих промежуточных состояний случайно будем переходить в состояния среды.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alttext](https://kodomo.fbb.msu.ru/FBB/year_20/ml/markov_decision_process.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формальное описание Markov decision process\n",
    "MRP это кортеж $(S, A, R, P, \\gamma)$, где\n",
    "- $S$ - состояния (дискретное пространтсво)\n",
    "- $A$ - действия (дискретное пространтсво)\n",
    "- $R$ - функция rewards, $R_{s}^{a}=\\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s, A_{t}=a\\right]$\n",
    "- $P$ - матрица переходов (transition matrix)\n",
    "$P_{s s^{\\prime}}^{a}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s, A_{t}=a\\right)$\n",
    "- $\\gamma$ - discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" />\n",
    "\n",
    "\n",
    "- черные беззнаковые числа - вероятности переходов\n",
    "- знаковые это награды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся [примером](https://github.com/yandexdataschool/Practical_RL)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
    "!touch .setup_complete\n",
    "\n",
    "# After this setup we can import mdp package"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from mdp import MDP \n",
    "\n",
    "transition_probs = {\n",
    "    's0': {\n",
    "        'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}\n",
    "    },\n",
    "    's1': {\n",
    "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}\n",
    "    },\n",
    "    's2': {\n",
    "        'a0': {'s0': 0.4, 's2': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
    "    }\n",
    "}\n",
    "rewards = {\n",
    "    's1': {'a0': {'s0': +5}},\n",
    "    's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
    "\n",
    "# We can now use MDP just as any other gym environment:\n",
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# MDP methods\n",
    "\n",
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "\n",
    "# state, action, next_state\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "\n",
    "# get_transition_prob(self, state, action, next_state)\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "import mdp as mdp_package\n",
    "display(mdp_package.plot_graph(mdp))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym\n",
    "\n",
    "\n",
    "\n",
    "В комментариях к коду выше, говорилось о совместимости с Gym\n",
    "\n",
    "*\"Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.\"*\n",
    "\n",
    "\n",
    "\n",
    "[OpenAI projects](https://openai.com/projects/)\n",
    "\n",
    "[Документация](https://gym.openai.com/docs/)\n",
    "\n",
    "\n",
    "[Список окружений](https://gym.openai.com/envs/#classic_control)\n",
    "\n",
    "\n",
    "[Colab demonstration](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb)\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "**Пример окружения** \n",
    "В этом примере загружается готовая среда: машинка должна заехать на горку. Функция награды встроенная. А агент - случайное воздействие. Оно ничему не учится, лишь хаотически выдаёт действия, но это показывает где взять все необходимые данные для обучения."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install necessary package for show visualization in Colab\n",
    "# https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab\n",
    "\n",
    "!apt-get install -y xvfb python-opengl\n",
    "!pip install gym pyvirtualdisplay"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define some helper functions and init virtual display\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def query_environment(name):\n",
    "  env = gym.make(name)\n",
    "  spec = gym.spec(name)\n",
    "  print(f\"Action Space: {env.action_space}\")\n",
    "  print(f\"Observation Space: {env.observation_space}\")\n",
    "  print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
    "  print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
    "  print(f\"Reward Range: {env.reward_range}\")\n",
    "  print(f\"Reward Threshold: {spec.reward_threshold}\")\n",
    "\n",
    "def draw(env):\n",
    "  screen = env.render(mode='rgb_array')\n",
    "  plt.imshow(screen)\n",
    "  ipythondisplay.clear_output(wait=True)\n",
    "  ipythondisplay.display(plt.gcf())\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "query_environment(\"MountainCar-v0\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MountainCar ](https://github.com/openai/gym/wiki/MountainCar-v0)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "MAX_NUM_EPISODES = 1\n",
    "\n",
    "for episode in range(MAX_NUM_EPISODES):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0 # To keep track of the total reward obtained in each episode\n",
    "    step = 0\n",
    "    while not done:\n",
    "        draw(env)       \n",
    "        action = env.action_space.sample() \n",
    "\n",
    "        # Sample random action.\n",
    "        # This will be replaced by our agent's action\n",
    "        # when we # start developing the agent algorithms\n",
    "\n",
    "        #action = int(input()) 0,1,2\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # Send the action to the environment and receive       \n",
    "        # the next_state, reward and whether done or not\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        obs = next_state\n",
    "        \n",
    "        if step > 50:\n",
    "          # Forse exit in Colab demo\n",
    "          done = True\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"\\n Episode #{} ended in {} steps. total_reward={}\".format(episode, step+1,\n",
    "total_reward))\n",
    "env.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "wrapped_env = gym.wrappers.Monitor(env, 'test', force = True)\n",
    "wrapped_env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "  action = env.action_space.sample()\n",
    "  observation, reward, done, info = wrapped_env.step(action)\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "wrapped_env.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эпизод\n",
    "\n",
    "\n",
    "Достижение терминального состояния (конец игры)\n",
    "\n",
    "`done`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "Нужно найти последовательность переходов которой будет соответствовать максимальная награда (Gt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение Политики\n",
    "Политика $\\pi$ это распределение действий от полученных состояний среды \n",
    "$$\n",
    "\\pi(a \\mid s)=\\mathbb{P}\\left[A_{t}=a \\mid S_{t}=s\\right]\n",
    "$$\n",
    "- Политики полностью определяет поведение агента\n",
    "- Политики MDP зависят от текущего состояния среды(а не от прошлых состояний) \n",
    "- т. е. политики являются стационарными (не зависящими от времени),\n",
    "$$\n",
    "A_{t} \\sim \\pi\\left(\\cdot \\mid S_{t}\\right), \\forall t>0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нашего примера со студентом \n",
    "\n",
    "![alttext](https://kodomo.fbb.msu.ru/FBB/year_20/ml/markov_policy.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Политика не обязана быть оптимальной\n",
    "\n",
    "Представим себе такую задачу - дойти в одну из звездочек за наименьшее число шагов\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/policy1.png\"  style=\"width: 500px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно предложить разные политики, некоторые из которых вообще не всегда будут решать задачу\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/gan/policy2.png\"  style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы научиться искать хорошую политику, надо научиться сравнивать политики между собой "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value - function\n",
    "\n",
    "Для оценки политики вводятся функции V и Q \n",
    "\n",
    "\n",
    "$v_\\pi(s)$ - какое ожидаемое вознаграждение можно получить, если начать двигаться из состояния $s$ в течении всего оставшегося времени, придерживаясь политики $\\pi$\n",
    "\n",
    "$q_\\pi(s,a)$ -  аналогично, но при условии что в состоянии $s (s_0 = s)$ было выбранно действие $a$ (мы фиксируем действие)\n",
    "\n",
    "То есть в $q$ мы в отличии от $v$ фиксируем наше первое действие (не обязательно соответсвующее политике), а в $v$ мы первое действие выбираем согласно политике $\\pi$\n",
    "\n",
    "Матожидание здесь берется по нашей политике, т.к она стохастична. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение Value Function\n",
    "\n",
    "Функция значения состояния $v_{\\pi}(s)$ для MDP ожидает возвращение начального состояния  $s$, и затем следует политике  $\\pi$\n",
    "$$\n",
    "v_{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right]\n",
    "$$\n",
    "\n",
    "Определение функция значения действия $q_{\\pi}(s, a)$ ожидает возвращение начального состояния $s$ , предпринимая действие $a$, а затем следуя политике $\\pi$\n",
    "\n",
    "$$\n",
    "q_{\\pi}(s, a)=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s, A_{t}=a\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нахождение оптимальной политики Беллмана \n",
    "\n",
    "Решить аналитически даную задачу нельзя. Но есть подходы, позволяющие в разных ситуациях находить решения итеративными методами. \n",
    "\n",
    "Все эти способы предполагают следующее:\n",
    "\n",
    "1. На каждом шаге оптимальное решение выбирается в предположении об оптимальности всех последующих шагов (**принцип Беллмана**)\n",
    "\n",
    "2. Оптимальный выбор действия зависит от текущего состояния и не зависит от предыстории\n",
    "\n",
    "Способы: \n",
    "\n",
    "\n",
    "1. Policy iteration \n",
    "2. Value iteration\n",
    "3. Q-learning\n",
    "4. SARSA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration \n",
    "\n",
    "Оказывается, весь процесс поиска можно разбить на две части\n",
    "\n",
    "Policy evaluation \n",
    "\n",
    "У нас есть некое policy $\\pi$. Если мы много раз посчитаем $v_\\pi$ по формуле, то в итоге мы сойдемся к хорошей оценке $v_\\pi(s)$ каждого состояния. \n",
    "\n",
    "Policy improvement \n",
    "\n",
    "Теперь мы знаем правильно $v$ для нашего policy. Как нам его улучшить? \n",
    "Легко. Будем в каждом состоянии выбирать менять нашу политику таким образом, чтобы мы шли в состояние с лучшим $q$\n",
    "\n",
    " Policy improvement:\n",
    "\n",
    "\n",
    "$$\n",
    "\\pi^{\\prime}(s) \\leftarrow \\underset{a}{\\arg \\max } \\overbrace{\\sum_{r, s^{\\prime}} p\\left(r, s^{\\prime} \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]}^{q_{\\pi}(s, a)}\n",
    "$$\n",
    "\n",
    "Это гарантирует нам получить лучшую политику\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\text { если } \\quad q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) \\geq v_{\\pi}(s) \\quad \\text { для всех состояний } \\\\\n",
    "&\\text { тогда } \\quad v_{\\pi^{\\prime}}(s) \\geq v_{\\pi}(s) \\\\\n",
    "&\\text { означает, что } \\quad \\pi^{\\prime} \\geq \\pi\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/silver-3-13.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод действительно сходится"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/game1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/game2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"https://kodomo.fbb.msu.ru/FBB/year_20/ml/game3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Фактически, этап policy evaluation of policy iteration  может быть сокращен различными способами без потери гарантий сходимости policy iteration. \n",
    "\n",
    "Один важный особый случай - это когда оценка политики останавливается сразу после одного цикла (одно обновление каждого состояния). Этот алгоритм называется value iteration. Его можно записать как  простую операцию обновления, которая сочетает в себе этапы улучшения политики и усеченной policy evaluation (один шаг).\n",
    "\n",
    "Это позволяет, в частности, вообще не записывать policy в явном виде - она однозначно определяется Q-функцией\n",
    "\n",
    "\n",
    "Value iteration (VI) vs. Policy iteration (PI)\n",
    "\n",
    "- VI быстрее за одну итерацию $-\\mathrm{O}\\left(|\\mathrm{A} \\| \\mathrm{S}|^{2}\\right)$\n",
    "- VI требуется много итераций\n",
    "-PI медленнее за одну итерацию $-\\mathrm{O}\\left(|\\mathrm{A} \\| \\mathrm{S}|^{2}+|\\mathrm{S}|^{3}\\right)$\n",
    "- PI требуется мало итераций\n",
    "\n",
    "Проведем эксперимент с каким то количеством шагов для нахождения лучшей политики\n",
    "\n",
    "\n",
    "Алгоритм\n",
    "\n",
    "1. Инициализация\n",
    "\n",
    "- Создаем массив V с количеством элементов равным количеству состояний.\n",
    "\n",
    "- Заполняем его нулями\n",
    "\n",
    "\n",
    "2. Оценка политики (Policy evaluation)\n",
    "\n",
    "- Для всех состояний считаем Q(s,a)\n",
    "\n",
    "- Обновляем массив V[s] -> max(Q(s,a))\n",
    "\n",
    "\n",
    "*В отличие от Policy iteration сама политика в памяти не храниться, она генеируется при помощи Q-функции.\n",
    "\n",
    "\n",
    "3. Обновление политики (Policy improvement)\n",
    "\n",
    "- Для каждого состояния считаем Q(s,a) для всех a\n",
    "\n",
    "- Выбираем a для которого Q(s,a) - максимально\n",
    "\n",
    "- Если политика изменилась, переходим к шагу 2, иначе останавливаемся\n",
    "\n",
    "\n",
    "Запишем решения для этого MDP. Самый простой алгоритм это  __V__alue __I__teration\n",
    "\n",
    "Псевдо код для VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "r(s,a,s') - награда соответствующая выбору действия a в s\n",
    "s - исходное состояние \n",
    "s' - новое состояние"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n",
    "s' - зависит от вероятности перехода\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "  \"\"\" \n",
    "  Computes Q(s,a) as in formula above \n",
    "\n",
    "  mdp : MDP object\n",
    "  state_values : dictionayry of { state_i : V_i }\n",
    "  state: string id of current state\n",
    "  gamma: float discount coeff\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  next_states = mdp.get_next_states(state, action)\n",
    "\n",
    "  Q = 0.0\n",
    "\n",
    "  for next_state in next_states.keys():\n",
    "    p = next_states[next_state] # alternatively p = mdp.get_transition_prob(state, action, next_state)\n",
    "    Q += p * (mdp.get_reward(state, action, next_state) + gamma * state_values[next_state])\n",
    "  return Q \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя $Q(s,a)$  мы можем определить \"следующее\" V(s) для VI.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "      return 0 # Game over\n",
    "    \n",
    "    q_max = float('-inf')\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "      q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "      q_max = max(q_max, q)\n",
    "    return q_max"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объеденим все вместе:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import mdp as MDP\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "display(MDP.plot_graph_with_state_values(mdp, state_values))\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "\n",
    "    new_state_values = {}\n",
    "    for s in state_values.keys():\n",
    "      new_state_values[s] = get_new_state_value(mdp,state_values,s,gamma)   \n",
    "\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s])\n",
    "               for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь нет никакого \"текущего состояния\"!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(MDP.plot_graph_with_state_values(mdp, state_values))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте использовать это  $V^{*}(s)$ для нахождения лучших действия для каждого состояния\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "Единственное отличие от V(s) то что мы используем argmax вместо max: найдем такое действие с максимальным Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "    \n",
    "    best_action = None\n",
    "    q_max = float('-inf')\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "      q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "      if q > q_max:\n",
    "        best_action = a\n",
    "        q_max = q\n",
    "\n",
    "    return best_action"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(MDP.plot_graph_optimal_strategy_and_state_values(mdp, state_values, get_action_value))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.40 < np.mean(rewards) < 0.55)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q - Learning\n",
    "\n",
    "В реальных задачах подход описанный выше не работает. Нам заранее не известны ни состояния ни вероятности переходов.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning\n",
    "\n",
    "мы не знаем вероятности переходов. Мы можем полько генерировать траектории и учиться на них.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-19.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-31.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Q - штрих рассчетное значение, Q без штриха то что было в таблице, альфа шаг обучения.\n",
    "\n",
    "**temporal difference**\n",
    "\n",
    "Пусть у нас есть оценка для текущего состояния $𝑆_𝑡$ и действия $𝐴_𝑡 𝑄(𝑆_𝑡,𝐴_𝑡)$. Сделаем один шаг, получим реальную награду $r$, посмотрим оценку на следующем шаге $𝑚𝑎𝑥𝑄(𝑆_{𝑡+1},𝐴)$. Но в идеале $𝑟+𝑚𝑎𝑥𝑄(𝑆_𝑡,𝐴)=𝑄(𝑆_𝑡,𝐴_𝑡)$, но если есть разница, то ее можно пытаться исправить.\n",
    "\n",
    "Идея TD в том, чтобы смотреть на разницу между нашей прошлой оценкой и уточненной и использовать эту информацию\n",
    "\n",
    "Temporal - так-как есть оценка сейчас и есть уточненная оценка через один шаг\n",
    "\n",
    "В теории можно смотреть через n-шагов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-36.png\" alt=\"Drawing\" width=\"500\"> \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-37.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-39.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration Vs Exploitation\n",
    "\n",
    "Стратегии:\n",
    "\n",
    "- $\\varepsilon-$greedy\n",
    "    - С вероятностью $\\varepsilon$ выбираем случайное действие; в противном случае, выбираем оптимальное.\n",
    "    - Всегда есть вероятность выскочить из локального оптимума;\n",
    "    - Но для \\varepsilon-greedy одинаково интересно зайти в новую область или убиться ещё раз головой об стенку.\n",
    "    \n",
    "- Softmax\n",
    "\n",
    "  Выбираем действие пропорционально значению softmax от нормализованных значений $Q(s,a)$:\n",
    "  \n",
    "$$\n",
    "\\pi(a \\mid s)=\\operatorname{softmax}\\left(\\frac{Q(s, a)}{\\tau}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**Иногда нужно выполять случайные, непривычные действия, чтобы познавать мир**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration over time\n",
    "\n",
    "Вначале, когда Q(s,a) плоха, рандом будет вести себя не хуже.\n",
    "\n",
    "Потом Q станет лучше и стратегии уже лучше сильно не мешать.\n",
    "\n",
    "**Idea**:\n",
    "    Если вы хотите перейти к оптимальной политике, вам нужно постепенно сокращать выбор случайных действий\n",
    "\n",
    "**Example**:\n",
    "\n",
    "\n",
    "Создаем $\\varepsilon$-greedy \n",
    "\n",
    "$\\varepsilon=0.5$, затем постепенно уменьшаем его \n",
    "\n",
    "  - Будьте осторожны с нестационарными средами\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако наш вариант Q-learning не учитывает того, что мы хотим иногда \"рандомить\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-44.png\" alt=\"Drawing\" /> \n",
    "\n",
    "Чему будет равняться Q функция в клетках?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-48.png\" alt=\"Drawing\" /> \n",
    "\n",
    "\n",
    "\n",
    "Алгоритм SARSA считает более реалистичные оценки. Так-как учитывает прыжки робота в лаву, то есть считает оценку по $\\epsilon -greedy$ политике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning\n",
    "\n",
    "Есть задачи где количество состояний и переходов огромно и поддерживать таблицу значений не представляется возможным.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-6.png\" style=\"width: 500\"/>\n",
    "\n",
    "например, если мы учим алгоритм играть в простую компьютерную игру, состоянием будет изображение экрана (210x160).\n",
    "\n",
    "В таких ситуациях Q-функцию не считают в явном виде, а аппроксимируют нейросетью.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-10.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы считать градиент приходится принимать значение Q-функции фиксированным и не зависящим от параметров нашей нейросети. Что, конечно, не так.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-11.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Q-learning\n",
    "Q-values:\n",
    "$$\n",
    "\\hat{Q}\\left(s_{t}, a_{t}\\right)=r+\\gamma \\cdot \\max _{a^{\\prime}} Q\\left(s_{t+1}, a^{\\prime}\\right)\n",
    "$$\n",
    "Objective:\n",
    "$$\n",
    "L=\\left(Q\\left(s_{t}, a_{t}\\right)-\\left[r+\\gamma \\cdot \\max _{a^{\\prime}} Q\\left(s_{t+1}, a^{\\prime}\\right)\\right]\\right)^{2}\n",
    "$$\n",
    "Gradient step:\n",
    "$$\n",
    "w_{t+1}=w_{t}-\\alpha \\cdot \\frac{\\delta L}{\\delta w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм обучения\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-14.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "$\\epsilon -greedy$ нужна для исследования среды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "\n",
    "\n",
    "**Проблемма**:\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-16.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "В большинстве окружений, информация, получаемая агентом распределена не независимо. Т.е. последовательные наблюдения агента сильно коррелированы между собой (что понятно из интуитивных соображений, т.к. большинство окружений, в которых применяется RL, предполагают, что все изменения в них последовательны). Корреляция примеров ухудшает сходимость стохастического градиентного спуска. Таким образом нам нужен способ, который позволяет улучшить распределение примеров для обучения (устранить или снизить корреляцию между ними). \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-3-62.png\" alt=\"Drawing\" />\n",
    "\n",
    "Обычно используется метод **проигрывания опыта (experience replay)**. Суть этого метода в том, что мы сохраняем некоторое количество примеров (состояние, действия, вознаграждение) в специальном буфере и для обучения выбираем случайные мини-батчи из этого буфера.\n",
    "\n",
    "Так же **experience replay** позволяет агенту эффективнее использовать свой прошлый опыт.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример c CartPole DQN\n",
    "\n",
    "Deep Q Network  = DQN \n",
    "\n",
    "Cartpole - известный также как перевернутый маятник с центром тяжести над своей точкой поворота. Он нестабилен, но его можно контролировать, перемещая точку поворота под центром массы. Цель состоит в том, чтобы сохранить равновесие, прикладывая соответствующие усилия к точке поворота.\n",
    "\n",
    "Другой env можно использовать без каких-либо изменений кода. Пространство состояний должно быть единым вектором, действия должны быть дискретными.\n",
    "\n",
    "CartPole - самый простой. На ее решение должно уйти несколько минут.\n",
    "\n",
    "Для LunarLander может потребоваться 1-2 часа, чтобы получить 200 баллов (хороший балл) в Colab, и прогресс в обучении не выглядит информативным.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "################################################\n",
    "# For CartPole\n",
    "################################################\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    \n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/atari_wrappers.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/utils.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/replay_buffer.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/framebuffer.py\n",
    "\n",
    "!pip install gym[box2d]\n",
    "\n",
    "!touch .setup_complete\n",
    "\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gym\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env = make_env()\n",
    "env.reset()\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "Теперь нам нужно построить нейронную сеть, которая может сопоставлять наблюдения с состоянием q-значений. Модель еще не должна быть слишком сложной. 1-2 скрытых слоя с < 200 нейронами и активацией ReLU, вероятно, будет достаточно. Batch normalization и dropout могут все испортить, поэтому их не используем\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device\n",
    "print(state_shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "        \n",
    "        \n",
    "        # Define NN\n",
    "        ##############################################\n",
    "        hidden_size = 150\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        ##############################################\n",
    "\n",
    "        \n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        \n",
    "        ##############################################\n",
    "        qvalues = self._nn(state_t)\n",
    "        ##############################################\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2 and \n",
    "            qvalues.shape[0] == state_t.shape[0] and \n",
    "            qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer and Target Networks\n",
    "\n",
    "#### Интерфейс довольно прост:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - сохраняет (s,a,r,s',done) кортеж в буффер\n",
    "* `exp_replay.sample(batch_size)` - возвращает observations, actions, rewards, next_observations и is_done для `batch_size` random samples.\n",
    "* `len(exp_replay)` - вовзращает количество элементов хранящихся в replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "\n",
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Loss\n",
    "\n",
    "Вычислим ошибку TD Q-learning:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "С Q-reference определенным как\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Где\n",
    "* $Q_{target}(s',a')$ обозначает $Q$-значение следующего предсказанного состояния и следующего действия  __target_network__\n",
    "* $s, a, r, s'$ текущее состояние, действие, вознаграждение и следующее состояние соотвественно\n",
    "* $\\gamma$ является коэффициентом дисконтирования, определенным двумя ячейками выше."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # with torch.no_grad():\n",
    "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    ##############################################\n",
    "    next_state_values = predicted_next_qvalues.max(axis=-1)[0]\n",
    "    ##############################################\n",
    "\n",
    "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
    "        \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    ###############################################\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * is_not_done\n",
    "    ##############################################\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
    "            \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim() == 1, \\\n",
    "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
    "            \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# your favourite random seed\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = make_env(seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один агент для игры, второй для обучения. Переодически веса обученного агента копируются в \"игрока\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \n",
    "    hint: use agent.sample.actions\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    # <YOUR CODE>\n",
    "    for _ in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        \n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        # action = action.argmax(axis=-1)[0]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        sum_rewards += reward\n",
    "        \n",
    "        exp_replay.add(s, action, reward, state, done)\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        \n",
    "        s = state\n",
    "        \n",
    "    \n",
    "\n",
    "    return sum_rewards, s"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# import src.L15_RL.lib.utils\n",
    "import utils\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "state = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print('less that 100 Mb RAM available, freezing')\n",
    "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        # <YOUR CODE: sample batch_size of data from experience replay>\n",
    "        s,a,r,next_s, is_done = exp_replay.sample(batch_size)\n",
    "        # loss = <YOUR CODE: compute TD loss>\n",
    "        loss = compute_td_loss(s, a, r, next_s, is_done, agent, target_network)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm)\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            # <YOUR CODE>\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(evaluate(\n",
    "                make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues(\n",
    "                [make_env(seed=step).reset()]\n",
    "            )\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "                (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(utils.smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "final_score = evaluate(\n",
    "  make_env(),\n",
    "  agent, n_games=30, greedy=True, t_max=1000\n",
    ")\n",
    "print('final score:', final_score)\n",
    "if final_score > 300:\n",
    "  print('Well done')\n",
    "else:\n",
    "  print('not good enough for DQN')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дальнейшие идеи\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-30.png\" alt=\"Drawing\" /> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L15_RL/img/yds-4-31.png\" alt=\"Drawing\" /> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, что $Q(s,a)$ распределен случайно. То есть, выбор action никак не влияет. Но так-как мы берем max по $Q$, то будет казаться, что мы можем получать большие значения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие улучшения DQN\n",
    "\n",
    "**Prioritized Experience Replay**\n",
    "\n",
    "Минибатчи из памяти выбираются не с равномерным распределением, а добавляем туда больше примеров, в которых предсказанные значения $Q$ сильнее всего отличаются от корректных. Т.е. примеры с максимальным **TD error** получают максимальный приоритет.\n",
    "\n",
    "**Dueling networks**\n",
    "\n",
    "Основная идея в том, что мы разделяем нашу сеть на две головы, одна из которых предсказывает абсолютное значение состояния \\\\( V(S) \\\\), а вторая - относительное преимущество одних действий над другими \\\\( A(s, a) = Q(s, a) - V(s) \\\\). Это преимущество называется advantage. Далее из этих двух значений мы собираем нашу Q-функцию, как \\\\( Q(s,a) = V(s) + A(a) \\\\)\n",
    "\n",
    "**Noisy nets**\n",
    "\n",
    "Т.к. по мере обучения агент будет стремиться выбирать состояния с максимальным $Q$, среди уже исследованных, это может помешать ему найти более эффективные состояния, в которых его ещё не было. Одним из решений этой проблемы является использование детерминированной и случайной нейросети, распределение параметров которой так же обучается с помощью градиентного спуска.\n",
    "\n",
    "**Multi-step learning/n-step learning**\n",
    "\n",
    "Основная идея в том, чтобы считать функцию ценности не по двум соседним примерам, а сразу по n. Это позволяет сети лучше запоминать длинные последовательности действий.\n",
    "\n",
    "**Distributional RL**\n",
    "\n",
    "Детерминированное значение $Q$ заменяется случайным распределением $Z$ с некоторыми параметрами, которые определяются в ходе обучения.\n",
    "\n",
    "**Rainbow**\n",
    "\n",
    "State of the art в развитии Q-обучения - набор перечисленных выше твиков. На графике ниже сравнение различных алгоритмов по количеству очков, усреднённое по играм Atari в сравнении со средними результатами человека.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L15_RL/img/zap/rainbow_dqn.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double Q-learning (NIPS 2010)**\n",
    "\n",
    "$y=r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)$\n",
    "- Q-learning target\n",
    "\n",
    "$y=r+\\gamma Q\\left(s^{\\prime}, \\operatorname{argmax}_{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)\\right)$\n",
    "- Rewritten $Q$-learning target\n",
    "\n",
    "**Idea**: используем две оценки q-values: $Q^{A}, Q^{B}$\n",
    "Они должны компенсировать ошибки друг друга, потому что они будут независимыми. Получим argmax от другой оценки\n",
    "\n",
    "$y=r+\\gamma Q^{A}\\left(s^{\\prime}, \\operatorname{argmax}_{a} Q^{B}\\left(s^{\\prime}, a^{\\prime}\\right)\\right)$ - Double $\\mathrm{Q}$-learning target\n",
    "\n",
    "То есть теперь есть две Q функции, которые мы обучаем: $Q^{A}$ и $Q ^{\\text {B}}$ если ошибки независимы, то одна Q функция скажет, где выпал шум (где значения выше), а вторая тогда попытается взять значение, которое больше. И тогда\n",
    "у нее не будет той же ошибки. Значение будет другое. $\\Rightarrow$ Надеемся,\n",
    "что проблема будет частично решена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Альтернативные подходы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor-CriticI**: Обучить актора, который предсказывает действия(policy gradient) и критика, который предсказывает будующие награды от предсказанных действий(Q-Learning) \n",
    "\n",
    "\"Asynchronous Methods for Deep Reinforcement Learning\", ICML 2016\n",
    "\n",
    "**Model-Based**: Обучить модель переходу состояния  $P\\left(s_{t+1} \\mid s_{t}, a_{t}\\right)$  и затем использовать модель для принятия решений \n",
    "\n",
    "**Imitation Learning**: Собрать данные о том, как эксперты работают в окружающей среде, обучить функцию, чтобы имитировать то, что они делают (подход к обучению под наблюдением)\n",
    "\n",
    "\n",
    "**Inverse Reinforcement Learning**:\n",
    "Соберите данные экспертов, работающих в среде, изучите функцию вознаграждения, которую они, по-видимому, оптимизируют, затем используйте RL для этой функции вознаграждения.\n",
    "\n",
    "\"Algorithms for Inverse Reinforcement Learning\", ICML 2000\n",
    "\n",
    "**Adversarial Learning**: Научиться обманывать дискриминатор, который классифицирует действия, как верные/ошибочные\n",
    "\n",
    "Ho and Ermon, \"Genertive Adversaraal limitation Leaming\", NeurlPS 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекомендованная литература:\n",
    "\n",
    "* Barto Sutton Reinforcement Learning: An Introduction (классика теории)\n",
    "* Lapan Deep Reinforcement Learning Hands-On Second Edition (Книга с большим количеством примеров по RL)\n",
    "* Sudharsan Ravichandiran Hands-On Reinforcement Learning with Python (Тоже много примеров)\n",
    "\n",
    "Прочая дополнительная литература:\n",
    "\n",
    "* Alex Zai and Brandon Brown Deep Reinforcement Learning in Action\n",
    "\n",
    "\n",
    "Полезные ссылки:\n",
    "* [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)\n",
    "* [Faulty Reward Functions in the Wild](https://openai.com/blog/faulty-reward-functions/)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {}
}
