{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjUmgGL28VkP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OyOQErIWcC4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rERfaA70j7cc"
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXQVqvNdWcC5",
    "outputId": "4c134b07-1d12-4bb3-be6e-f723d5393e89"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "a9Elfe6OWcC6",
    "outputId": "af436517-d45f-4613-952c-0158676eb3e6"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Немного теории о Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы, возможно, уже знаете, основными двумя частями системы обучения с подкреплением являются:\n",
    "\n",
    "**Среда (Environment)** — это «игровое поле» или в нашем случае рынок, который может подсказать нам, что происходит прямо сейчас и какова будет наша награда в будущем, если мы совершим какое-то действие прямо сейчас.\n",
    "\n",
    "**Агент (Agent)** — «игрок», который взаимодействует с окружающей средой и учится максимизировать долгосрочное вознаграждение, выполняя различные действия в различных ситуациях.\n",
    "\n",
    "\n",
    "**Состояние (State)** - Текущая ситуация агента.\n",
    "\n",
    "**Награда или Вознаграждение (Reward)** -  обратная связь от окружения.\n",
    "\n",
    "**Политика (Policy)** - метод сопоставления состояния агента с действиями.\n",
    "\n",
    "**Ценность (Value)** - будущая награда, которую агент получит, совершив действие в определенном состоянии.\n",
    "\n",
    "Графически это можно представить на следующей схеме:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pic1.png](Pic1.png)\n",
    "<center>Иллюстрация с https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html<center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhO6h_Uc__Wi"
   },
   "source": [
    "# Рабочая среда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если состояния сложны и их трудно представить в виде таблицы действий, их можно аппроксимировать с помощью нейронной сети (это то, что мы будем делать).\n",
    "\n",
    "С помощью нейронной сети мы можем выбрать самое прибыльное действие в каждом состоянии (согласно нашей функции `Q`) и максимизировать награду. \n",
    "\n",
    "Но как построить такую функцию `Q`? Здесь нам поможет алгоритм `Q-Learning`, основой которого будет знаменитое `уравнение Беллмана`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pic2.png](Pic2.png)\n",
    "<center>Иллюстрация с https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**S** = Состояние или наблюдение\n",
    "\n",
    "**A** = Действие, которое предпринимает агент\n",
    "\n",
    "**R** = Награда за действие\n",
    "\n",
    "**t** = Временной шаг\n",
    "\n",
    "**Ɑ** = Скорость обучения\n",
    "\n",
    "**ƛ** = Коэффициент дисконтирования, из-за которого вознаграждения со временем теряют свою ценность, поэтому более немедленные вознаграждения ценятся выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Рассмотрим, как будет проходить процесс обучения нашей нейронной сети с помощью Deep Q-Learning:**\n",
    "\n",
    "![Pic3.png](Pic3.png)\n",
    "<center>Иллюстрация с https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Инициализируем нейронную сеть\n",
    "\n",
    "2. Выберем действие.\n",
    "\n",
    "3. Обновим веса сети, используя уравнение Беллмана.\n",
    "\n",
    "Другими словами, мы будем итеративно, на каждом шаге `t`, обновлять значение, соответствующее состоянию `S` и заданному действию `A`, с двумя весовыми частями:\n",
    " - Текущее значение функции Q для этого состояния и действия\n",
    " - Награда за такое решение + долгосрочная награда от будущих шагов\n",
    " \n",
    "`Альфа` здесь измеряет компромисс между текущим значением и новым вознаграждением (т. е. скоростью обучения), `гамма` дает вес для долгосрочного вознаграждения. \n",
    "\n",
    "Кроме того, во время итераций (обучения) в нашей среде мы иногда будем действовать случайным образом с некоторой вероятностью `эпсилон` — чтобы позволить нашему агенту изучить новые действия и потенциально еще большие награды! \n",
    "\n",
    "Обновление функции `Q` в случае аппроксимации нейронной сети будет означать аппроксимацию нашей нейронной сети `Q` новым значением для данного действия.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN_kNTX-AZ2A"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    '''\n",
    "        Рабочая среда робота, внутри которого будет \n",
    "        происходить дальнейшее обучение \n",
    "    '''\n",
    "    def __init__(self, length = 100, normalize = True, noise = True, data = []):\n",
    "        self.length = length\n",
    "\n",
    "        if len(data) == 0:\n",
    "            # Если данные не поданы, формируем их сами \n",
    "            # на основе синуса размером length\n",
    "            self.data = pd.DataFrame(np.sin(np.arange(length) / 30.0))\n",
    "        else:\n",
    "            # Иначе подугружаем существующие\n",
    "            self.data = data\n",
    "        \n",
    "\n",
    "        if noise:\n",
    "            # Подаем шум для данных от 0.1 до 1\n",
    "            self.data += pd.DataFrame(np.random.normal(0, 0.1, size=(length, )))\n",
    "\n",
    "        if normalize:\n",
    "            # Нормализация после данных после шума\n",
    "            self.data = (self.data - self.data.min()) / (self.data.max() - self.data.min())\n",
    "\n",
    "    def get_state(self, time, lookback, diff = True):\n",
    "        \"\"\"\n",
    "        Возвращаем производные отдельного окна в нашей выборке\n",
    "        и убираем нули в начале\n",
    "        \"\"\"\n",
    "        window = self.data.iloc[time-lookback:time]\n",
    "        if diff: window = window.diff().fillna(0.0)\n",
    "        return window\n",
    "\n",
    "    def get_reward(self, action, action_time, reward_time, coef = 100):\n",
    "        \"\"\"\n",
    "        Основная логика получения награды\n",
    "        0 => long 1 => hold 2 => short\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            action = 1\n",
    "        elif action == 1:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = -1\n",
    "        # print(23, self.data)\n",
    "        # Вытаскиваем текущую цену\n",
    "        price_now = self.data.iloc[action_time]\n",
    "        # Вытаскиваем следующую цену\n",
    "        price_reward = self.data.iloc[reward_time]\n",
    "        # Получаем разницу в проценте\n",
    "        price_diff = (price_reward - price_now) / price_now\n",
    "        # Прибавляем к портфелю следующее число:\n",
    "        # Дельта изменения валюты * покупку/продажу/холд * коэф. закупки\n",
    "        reward = np.sign(price_diff) * action * coef\n",
    "        # print(12121, reward)\n",
    "        return reward.values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "zrbu-JaeKu1V",
    "outputId": "60bef2b1-361c-415d-9ccf-bedf1d814b4e"
   },
   "outputs": [],
   "source": [
    "# Создадим тестовую среду\n",
    "lin_env = Environment(normalize=True, noise=True)\n",
    "# Отобразим все производные отдельного окна с 95 по 100 выборку\n",
    "lin_env.get_state(100, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "T1ielGYotO3-",
    "outputId": "b8bb78cf-d21b-4cd6-cbf6-985fb31250a4"
   },
   "outputs": [],
   "source": [
    "# Отобразим сгенерированные данные внутри среды\n",
    "plt.figure()\n",
    "plt.plot(lin_env.data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4CO7W5HLSb3",
    "outputId": "3bd4c803-bbea-4180-c63a-d3d0bbe9a89f"
   },
   "outputs": [],
   "source": [
    "lin_env.get_reward(0, 50, 51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примечания по реализации:\n",
    "\n",
    "Реализация классов `Environment` и `Agent` относительно проста, но я хотел бы еще раз обрисовать цикл обучения здесь:\n",
    "\n",
    "Итерация происходит в течение `N` эпох, где каждая эпоха — это общая среда итерации.\n",
    "\n",
    "Для каждого образца в среде мы:\n",
    "1. Получаем `текущее состояние` в момент времени `t`\n",
    "2. Получим `функцию значений` для всех действий в этом состоянии (наша нейросеть выдаст нам 3 значения)\n",
    "3. Выполним `действие` в этом состоянии (например, действуйем случайным образом, исследуя)\n",
    "4. Получим `награду` за это действие от окружения (см. класс)\n",
    "5. Получим `следующее состояние` после текущего (для будущих долгосрочных вознаграждений)\n",
    "6. `Сохраним кортеж` текущего состояния, следующего состояния, функции значения и вознаграждения за повтор опыта.\n",
    "7. `Воспроизведем опыт` — подгоним нашу нейронную сеть к некоторым образцам из буфера воспроизведения опыта, чтобы сделать функцию `Q` более адекватной в отношении того, какие награды мы получаем за действия на этом этапе.\n",
    "\n",
    "Рекомендую прочитать подробнее [здесь](https://deeplizard.com/learn/video/Bcuj2fTH4_4) , но в двух словах лучше тренироваться на некоррелированных мини-пакетах данных, чем на очень коррелированных пошаговых наблюдениях — это помогает обобщению и сходимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch-OHZBzAlgH"
   },
   "source": [
    "# Агент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJQnbrkIhISN"
   },
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6JaXr-5WcC_"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Строим простую модель нейронки\"\"\"\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_shape, 10)\n",
    "        self.fc2 = nn.Linear(10, action_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R95PsMPJWcC_",
    "outputId": "985dbfbf-83a0-48bd-cbcb-69eb622a93b5"
   },
   "outputs": [],
   "source": [
    "# Создадим экземпляр модели с 10 входами, 3 выходами\n",
    "# и отобразим ее\n",
    "net = Net(10, 3)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzCQw17UAmRH"
   },
   "outputs": [],
   "source": [
    "class BuyHoldSellAgent:\n",
    "    '''\n",
    "        Агент для покупки продажи\n",
    "    '''\n",
    "    def __init__(self, state_shape = 10, action_shape = 3, experience_size = 100):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.experience_size = experience_size\n",
    "        self.experience = collections.deque(maxlen=self.experience_size)\n",
    "\n",
    "        # Создадим экземпляр модели\n",
    "        self.model = Net(state_shape, action_shape)\n",
    "\n",
    "        # Создадим функцию ошибки\n",
    "        self.criterion = nn.MSELoss()\n",
    "        # Добавим оптимизатор\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.1)\n",
    "\n",
    "    def save_experience(self, state_i, q_value_i, action_i, reward_i, state_i_1):\n",
    "        \"\"\"Метод для сохранения предудыщих данных эксперимента\"\"\"\n",
    "        self.experience.append({\n",
    "            'state_i': state_i,\n",
    "            'q_value_i': q_value_i,\n",
    "            'action_i': action_i,\n",
    "            'reward_i': reward_i,\n",
    "            'state_i_1': state_i_1\n",
    "        })\n",
    "\n",
    "    def replay_experience(self, alpha, gamma, sample_size):\n",
    "        \"\"\"Метод для оптимизации данных тренировки\"\"\"\n",
    "        # Создаем фиксированную выборку из добавленных событий\n",
    "        indices_sampled = np.random.choice(\n",
    "            len(self.experience), \n",
    "            sample_size, \n",
    "            replace=False\n",
    "        )\n",
    "        # Проходимся только по тем элементам, которые были добавлены в выборку\n",
    "        for i, e in enumerate(self.experience):\n",
    "            if i in indices_sampled:\n",
    "                state_i, action_i, reward_i, q_value_i = e['state_i'], e['action_i'], e['reward_i'], e['q_value_i']\n",
    "                state_i_1 = e['state_i_1']\n",
    "\n",
    "                # Получаем прогноз по следующему состоянию\n",
    "                q_value_i_1 = self.get_value_action_value(state_i_1.values.reshape(1, WINDOW_SHAPE))[None, ...]\n",
    "\n",
    "                # Отсоединяем тензор и кидаем в новую переменную \n",
    "                y_i = q_value_i.detach()\n",
    "                \n",
    "                # Уравнение Бэллмена\n",
    "                # Суть в том, что мы берем максимально возможную награду \n",
    "                # из действия из будущего шага (q_value_i_1) , умножаем ее на гамму \n",
    "                # (коэф. значимости будущих наград), прибавляем к текущей награде\n",
    "                # и заносим в Q таблицу для обучения\n",
    "                y_i[action_i] = (1 - alpha) * y_i[action_i] + alpha * (reward_i + gamma * max(q_value_i_1.detach().numpy()[0]))\n",
    "                \n",
    "                outputs = self.model(torch.from_numpy(np.expand_dims(state_i.values.reshape(1, WINDOW_SHAPE), 0)).float())[0]\n",
    "                loss = self.criterion(outputs, torch.Tensor(y_i))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def get_value_action_value(self, state):\n",
    "        \"\"\"Метод для прогноза сигнала\"\"\"\n",
    "        pred = self.model(torch.from_numpy(np.expand_dims(state, 0)).float())\n",
    "        return pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oS3vaStaRceW"
   },
   "outputs": [],
   "source": [
    "# Создадим нового агента\n",
    "agent = BuyHoldSellAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XUeMMlXRgiu",
    "outputId": "9d077b60-d77a-4539-846e-9c28093e693c"
   },
   "outputs": [],
   "source": [
    "# Попробуем предсказать на линейной выборке\n",
    "agent.get_value_action_value(\n",
    "    pd.DataFrame(np.array([range(10)]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OMdkUloYZ1B"
   },
   "source": [
    "# Тренировка сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kru4HB2XYaeg"
   },
   "outputs": [],
   "source": [
    "# Количество эпох обучения\n",
    "epochs = 10\n",
    "# Коэф. значимости награды на шаг вперед\n",
    "gamma = 0.9\n",
    "# Количество эпох обучения\n",
    "epsilon = 0.95\n",
    "# Скорость обучения\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nd9X3rnAami_"
   },
   "outputs": [],
   "source": [
    "# Размер датасета\n",
    "DATASET_LENGTH = 250\n",
    "# Размер окна из которого будут браться предыдущие данные\n",
    "WINDOW_SHAPE = 5\n",
    "# Шаг предыдущих данных\n",
    "REWARD_TIME = 1\n",
    "# Число доступных действий\n",
    "ACTIONS_SHAPE = 3\n",
    "# Размер выборки\n",
    "SAMPLE_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSzO9afGbVxb"
   },
   "outputs": [],
   "source": [
    "# Объявляем новую среду с агентом, данные будут генерироваться автоматически\n",
    "environment = Environment(DATASET_LENGTH, True, False)\n",
    "agent = BuyHoldSellAgent(WINDOW_SHAPE, ACTIONS_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "QKUEjfcY6wDe",
    "outputId": "0b93b15f-ca24-4073-f023-dde0e3a84138"
   },
   "outputs": [],
   "source": [
    "environment.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "dvXyWGNfQPxz",
    "outputId": "c36214ed-316c-46e0-8718-40dc661d6d74"
   },
   "outputs": [],
   "source": [
    "# Отобразим сгенерированные данные внутри среды\n",
    "plt.figure()\n",
    "plt.plot(environment.data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31uD9MugAopP",
    "outputId": "f50288a9-f6ee-4cb3-bc42-416d7caa336f"
   },
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    learning_progress = []\n",
    "    for j in range(WINDOW_SHAPE, DATASET_LENGTH - REWARD_TIME, REWARD_TIME): \n",
    "        print(f\"{i} / {epochs} : {j} {DATASET_LENGTH - REWARD_TIME}\", end='\\r')\n",
    "\n",
    "        # 1. Получаем текущее состояние. Производные в отдельном интервале\n",
    "        # Текущий Y на графике и предыдущие WINDOW_SHAPE шагов\n",
    "        state_j = environment.get_state(j, WINDOW_SHAPE)\n",
    "        # Получаем предсказание по данному состоянию через модель\n",
    "        q_value_j = agent.get_value_action_value(state_j.values.reshape(1, WINDOW_SHAPE))\n",
    "\n",
    "        # 2. Выбираем действие для текущего состояния\n",
    "        # В зависимости от длины эпсилона, выбираем случайное действие \n",
    "        # или реальные по предсказаниям\n",
    "        if (np.random.random() < epsilon):\n",
    "            action = np.random.randint(0, ACTIONS_SHAPE)\n",
    "        else:\n",
    "            action = (np.argmax(q_value_j.detach()))\n",
    "\n",
    "        # 3. Получаем награду за выбранное действие\n",
    "        reward_value_j = environment.get_reward(action, j, j + REWARD_TIME)\n",
    "        learning_progress.append(reward_value_j)\n",
    "\n",
    "        # 4. Получаем будущее состояние. Производные в будущем интервале\n",
    "        # Текущий Y + 1 на графике и предыдущие WINDOW_SHAPE шагов\n",
    "        state_j_1 = environment.get_state(j + 1, WINDOW_SHAPE)\n",
    "\n",
    "        # 5. Сохраняем все полученные данные в агента\n",
    "        agent.save_experience(state_j, q_value_j, action, reward_value_j, state_j_1)\n",
    "        \n",
    "        # 6. Как только число ранее полученных сэмплов перевалило \n",
    "        # за необходимое, начинаем тренировать сеть\n",
    "        if j > SAMPLE_SIZE * 2:\n",
    "            agent.replay_experience(alpha, gamma, SAMPLE_SIZE)\n",
    "\n",
    "    # С каждым шагом уменьшаем эпсилон, чтобы бот начинал действовать \n",
    "    # по собственным прогнозам и переставал изучать среду\n",
    "    if epsilon > 0.1:\n",
    "        epsilon -= 1.0 / epochs\n",
    "\n",
    "    print('Epoch', i, '...', np.mean(learning_progress))\n",
    "    learning_progress = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptLqbYzMDYMd"
   },
   "source": [
    "# Проверка\n",
    "\n",
    "Давайте посмотрим, сможем ли мы получить какую-то прибыль, после обучения сети всего-лишь на 10 эпохах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtgNyS5l0apF"
   },
   "outputs": [],
   "source": [
    "action_to_backtest_action = {\n",
    "    0: 1,  # покупаем\n",
    "    1: 0,  # ничего не делаем\n",
    "    2: -1  # продаем\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ6rwYG6vDaD"
   },
   "source": [
    "### Простой синус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKU03wEYDZaK"
   },
   "outputs": [],
   "source": [
    "actions = []\n",
    "for j in range(WINDOW_SHAPE, DATASET_LENGTH, REWARD_TIME): \n",
    "    # Получаем производные в отдельном интервале\n",
    "    # Текущая цена и предыдущие WINDOW_SHAPE шагов\n",
    "    state_j = environment.get_state(j, WINDOW_SHAPE)\n",
    "    # Получаем предикт бота по данным\n",
    "    q_value_j = agent.get_value_action_value(state_j.values.reshape(1,WINDOW_SHAPE))\n",
    "    # Определяем сигнал и записываем в события\n",
    "    actions.append(action_to_backtest_action[np.argmax(q_value_j.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "ydfzppX4DwT4",
    "outputId": "df8f5957-587b-42e8-c552-7e5efe7c7e3a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(environment.data)\n",
    "for e, a in enumerate(actions):\n",
    "    e += WINDOW_SHAPE\n",
    "    if a == 1:\n",
    "        plt.scatter(e, environment.data.iloc[e], color = 'green')\n",
    "    elif a == -1:\n",
    "        plt.scatter(e, environment.data.iloc[e], color = 'red')\n",
    "    else:\n",
    "        pass\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "hDHWBMBWzX4G",
    "outputId": "a2f1defa-d509-4bad-d575-442695fa5b88"
   },
   "outputs": [],
   "source": [
    "# Составляем график доходов. Сделаем фрейм формата Цена/Сигнал\n",
    "backtest = pd.DataFrame({\n",
    "    'price': environment.data.values.flatten(),\n",
    "    # Первые 5 сигналов отсутствуют, мы должны расширить данные\n",
    "    # и добавить дополнительные 5 элементов в начало с сигналом 0\n",
    "    'signal': [0] * WINDOW_SHAPE + actions\n",
    "})\n",
    "# Получаем производные цен и делаем сдвиг\n",
    "backtest['price_diff'] = backtest['price'].diff().shift(-1)\n",
    "# Суммируем доходы и выводим график\n",
    "(backtest['price_diff'] * backtest['signal']).cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLQV1ShDwjS2"
   },
   "source": [
    "### Синус с разными частотами\n",
    "\n",
    "Давайте усложним жизнь нашему агенту — просуммируем 4 функции косинуса с разными частотными периодами и попробуем торговать по этим объединенным волнам. \n",
    "\n",
    "Результат по-прежнему отличный — наше представление рынка четко отражает тенденции, и даже если наша модель была обучена на данных другого типа, она все равно знает, что делать с другой волной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rL6-wAu0wk5J",
    "outputId": "c047e140-0202-408e-df61-5b7df177c439"
   },
   "outputs": [],
   "source": [
    "data_new = pd.DataFrame(\n",
    "    np.cos(np.arange(DATASET_LENGTH)/5.0) + \n",
    "    np.cos(np.arange(DATASET_LENGTH)/10) + \n",
    "    np.cos(np.arange(DATASET_LENGTH)/20) + \n",
    "    np.cos(np.arange(DATASET_LENGTH)/30)\n",
    "    )\n",
    "print(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz3I53Ek8Y9Q"
   },
   "outputs": [],
   "source": [
    "environment3 = Environment(DATASET_LENGTH, True, True, data_new)\n",
    "actions = []\n",
    "for j in range(WINDOW_SHAPE, DATASET_LENGTH, REWARD_TIME): \n",
    "    # Получаем производные в отдельном интервале\n",
    "    # Текущая цена и предыдущие WINDOW_SHAPE шагов\n",
    "    state_j = environment3.get_state(j, WINDOW_SHAPE)\n",
    "    # Получаем предикт бота по данным\n",
    "    q_value_j = agent.get_value_action_value(state_j.values.reshape(1, WINDOW_SHAPE))\n",
    "    # Определяем сигнал и записываем в события\n",
    "    actions.append(action_to_backtest_action[np.argmax(q_value_j.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "Ji3YzkKJ9CN6",
    "outputId": "5e2964a7-0d2c-4797-910e-9ee010e3ebbe"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(environment3.data)\n",
    "for e, a in enumerate(actions):\n",
    "    e += WINDOW_SHAPE\n",
    "    if a == 1:\n",
    "        plt.scatter(e, environment3.data.iloc[e], color = 'green')\n",
    "    elif a == -1:\n",
    "        plt.scatter(e, environment3.data.iloc[e], color = 'red')\n",
    "    else:\n",
    "        pass\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "pnoKA7iD9CjR",
    "outputId": "273e9fd6-f102-4ce6-a95a-48be5ccd6d3d"
   },
   "outputs": [],
   "source": [
    "# Составляем график доходов. Сделаем фрейм формата Цена/Сигнал\n",
    "backtest = pd.DataFrame({\n",
    "    'price': environment3.data.values.flatten(),\n",
    "    # Первые 5 сигналов отсутствуют, мы должны расширить данные\n",
    "    # и добавить дополнительные 5 элементов в начало с сигналом 0\n",
    "    'signal': [0] * WINDOW_SHAPE + actions\n",
    "})\n",
    "# Получаем производные цен и делаем сдвиг\n",
    "backtest['price_diff'] = backtest['price'].diff().shift(-1)\n",
    "# Суммируем доходы и выводим график\n",
    "(backtest['price_diff'] * backtest['signal']).cumsum().plot()\n",
    "plt.title('Вознаграждение алгоритма')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGydYxUFvFhe"
   },
   "source": [
    "### Косинус с шумом\n",
    "\n",
    "Теперь давайте немного усложним упражнение и `добавим гауссов шум` во временной ряд без переобучения модели.\n",
    "\n",
    "Алгоритм работает хуже, но какую-то прибыль зарабатывает! Теперь есть некоторые запутанные моменты, но в среднем модель все еще узнает, где находятся долгосрочные тренды нашей зашумленной функции косинуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPw2wDT_hcCJ"
   },
   "outputs": [],
   "source": [
    "environment2 = Environment(DATASET_LENGTH, True, True)\n",
    "actions = []\n",
    "for j in range(WINDOW_SHAPE, DATASET_LENGTH, REWARD_TIME): \n",
    "    # Получаем производные в отдельном интервале\n",
    "    # Текущая цена и предыдущие WINDOW_SHAPE шагов\n",
    "    state_j = environment2.get_state(j, WINDOW_SHAPE)\n",
    "    # Получаем предикт бота по данным\n",
    "    q_value_j = agent.get_value_action_value(state_j.values.reshape(1, WINDOW_SHAPE))\n",
    "    # Определяем сигнал и записываем в события\n",
    "    actions.append(action_to_backtest_action[np.argmax(q_value_j.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "xtRiCRcwvL0j",
    "outputId": "6ce0cc2c-0368-41c6-da0a-54f883c09712"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(environment2.data)\n",
    "for e, a in enumerate(actions):\n",
    "    e += WINDOW_SHAPE\n",
    "    if a == 1:\n",
    "        plt.scatter(e, environment2.data.iloc[e], color = 'green')\n",
    "    elif a == -1:\n",
    "        plt.scatter(e, environment2.data.iloc[e], color = 'red')\n",
    "    else:\n",
    "        pass\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "1mmWwuMdvimy",
    "outputId": "3bbbe334-ba50-4aa4-a19a-9de5c4ed9544"
   },
   "outputs": [],
   "source": [
    "# Составляем график доходов. Сделаем фрейм формата Цена/Сигнал\n",
    "backtest = pd.DataFrame({\n",
    "    'price': environment2.data.values.flatten(),\n",
    "    # Первые 5 сигналов отсутствуют, мы должны расширить данные\n",
    "    # и добавить дополнительные 5 элементов в начало с сигналом 0\n",
    "    'signal': [0] * WINDOW_SHAPE + actions\n",
    "})\n",
    "# Получаем производные цен и делаем сдвиг\n",
    "backtest['price_diff'] = backtest['price'].diff().shift(-1)\n",
    "# Суммируем доходы и выводим график\n",
    "(backtest['price_diff'] * backtest['signal']).cumsum().plot()\n",
    "plt.title('Вознаграждение алгоритма')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Op7Gkr2WcDF"
   },
   "source": [
    "### Акции Tesla\n",
    "\n",
    "Осталось проверить, сможем ли мы обогатиться, применив эту же нейронную сеть к реальным котировкам акций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance\n",
    "clear_output()\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yf.download(tickers='TSLA')\n",
    "df = df[-500:]\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "Y-h_PUx4jxE1",
    "outputId": "eb2988d6-2bfc-4bea-8a8a-e64411b0a075"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    y = df['Close'][:],\n",
    "    line =  dict(shape =  'spline' ),\n",
    "    name = 'Курс акции TSLA'\n",
    "            ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Xyp1X48bBIH",
    "outputId": "c3fc502e-975f-43e8-b8e5-5013e9f3c83c"
   },
   "outputs": [],
   "source": [
    "df['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiXtPCHGWcDF"
   },
   "outputs": [],
   "source": [
    "# Прописываем новую среду и передаем данные о закрытии цены\n",
    "environment4 = Environment(len(df['Close']), False, False, df['Close'])\n",
    "actions = []\n",
    "\n",
    "for j in range(WINDOW_SHAPE, len(df['Close']), REWARD_TIME): \n",
    "    # 1. Получаем производные в отдельном интервале\n",
    "    # Текущая цена и предыдущие WINDOW_SHAPE шагов\n",
    "    state_j = environment4.get_state(j, WINDOW_SHAPE)\n",
    "\n",
    "    # Получаем предикт бота по данным\n",
    "    q_value_j = agent.get_value_action_value(state_j.values.reshape(1, WINDOW_SHAPE))\n",
    "    # Определяем сигнал и записываем в события\n",
    "    actions.append(action_to_backtest_action[np.argmax(q_value_j.detach().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "jFvCNwlr8v7B",
    "outputId": "621d7586-5d31-4bd3-e254-3d6d10598464"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (25, 10))\n",
    "plt.plot(environment4.data)\n",
    "for e, a in enumerate(actions):\n",
    "    print(f\"{e} {a}\", end='\\r')\n",
    "    e += WINDOW_SHAPE\n",
    "    if a == 1:\n",
    "        plt.scatter(e, environment4.data.iloc[e], color = 'green')\n",
    "    elif a == -1:\n",
    "        plt.scatter(e, environment4.data.iloc[e], color = 'red')\n",
    "    else:\n",
    "        pass\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "eI04XSTlWcDG",
    "outputId": "a034ecbd-dfb3-4e20-f929-3180b9fa3282"
   },
   "outputs": [],
   "source": [
    " # Составляем график доходов. Сделаем фрейм формата Цена/Сигнал\n",
    "backtest = pd.DataFrame({\n",
    "    'price': environment4.data.values.flatten(),\n",
    "    # Первые 5 сигналов отсутствуют, мы должны расширить данные\n",
    "    # и добавить дополнительные 5 элементов в начало с сигналом 0\n",
    "    'signal': [0] * WINDOW_SHAPE + actions\n",
    "})\n",
    "# Получаем производные цен и делаем сдвиг\n",
    "backtest['price_diff'] = backtest['price'].diff().shift(-1)\n",
    "# Суммируем доходы и выводим график\n",
    "(backtest['price_diff'] * backtest['signal']).cumsum().plot()\n",
    "plt.title('Вознаграждение алгоритма')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как думаете, почему вдруг мы не можем получить стабильный результат?\n",
    "\n",
    "**Ответ:** в нестационарности временного ряда. \n",
    "\n",
    "В реальной жизни, котировки акции носят практически случайный характер: меняется среднее, дисперсия и частота изменений. Поэтому наша нейронная сеть `ловит` только частные случаи `несовершества рынка` и, в такие моменты, алгоритм увеличивает свое вознаграждение. А в остальное время - теряет.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].diff(1).plot.hist(bins=50, alpha=0.5)\n",
    "plt.title('Распределение изменений значения цены акции')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именно потому, что реальная жизнь очень разнообразна, мало пользы в обучении RL на основе рафинированных (упрощенных) моделей, предлагаемых различными искусственными средами).\n",
    "\n",
    "Нужно учиться самостоятельно описывать среду, агентаи вознаграждения. Только тогда Вы поймете, насколько это тонкая задача, требующая понимания взаимосвязи реальных факторов друг с другом и с действиями агента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Некоторые ссылки:\n",
    "\n",
    "1. [Reinforcement Learning: An Introduction. Richard S. Sutton and Andrew G. Barto 2014, 2015](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "2. [Advances in Financial Machine Learning](https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089)\n",
    "3. [Trend following](https://en.wikipedia.org/wiki/Trend_following)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pytorch_Intro_to_RL_for_Trading_harmonic_functions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
