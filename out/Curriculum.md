Программа курса

## Лекция 1 “Введение в машинное обучение”

Задача курса. AI, ML, DL. Области применения. Связь с наукой. Обзор курса. Базовые задачи. Комбинированные задачи. Инструменты. Контейнеры. План исследования. Сбор и подготовка данных. Извлечение закономерностей. Валидация результата. Пример ML задачи. Данные. Связность данных. Загрузка и визуализация данных. Алгоритм k-NN. Описание модели. Нормализация данных. Параметры и гиперпараметры модели. Метрики. Accuracy. Precision, Recall. F-мера. AUC-ROC. Разделение train-validation-test. Примеры ошибок в данных и при разбиении. Подбор гиперпараметров на тестовой выборке. Кросс-валидация. Алгоритм кросс-валидации. Оценка результата кросс-валидации. Типичные ошибки при кросс-валидации. Кросс-валидация для научных исследований: на что обратить внимание. GridSearch. RandomizedSearch

## Лекция 2 “Линейные модели”

Линейная регрессия. Модель и ее параметры. Функция потерь. Поиск локального минимума. Метод наименьших квадратов. Метрики регрессии. Модель линейной регрессии из библиотеки scikit-learn. Классификация. Hinge loss. 1D классификация. Maximum Margin Classifier. 2D классификация. SVM: Hard and Soft Margin Classifier. 3D классификация. Многоклассовая классификация. Метод градиентного спуска. Численный расчет производной. Аналитический расчет производной от функции потерь. Выбор шага обучения. Алгоритм стохастического градиентного спуска. Выбор размера батча. Практические особенности работы с линейными моделями. Единый подход к учету смещения. Нормализация данных. Проблема корреляции признаков  в случае линейных моделей. Регуляризация. Обобщенные линейные модели. Kernel SVM. Вероятностный подход в задаче классификации. Наивный Байесовский классификатор. Кросс-энтропия, как общая функция потерь для задач классификации. Расчет функции потерь. Пример обучения линейного классификатора с hinge loss. MNIST. CIFAR-10

## Лекция 3 “Классическое машинное обучение”

Системы предсказаний. Экспертные системы (Rule-based systems). Классическое машинное обучение. Глубокое машинное обучение. Необходимость методов классического машинного обучения. Деревья решений. Принцип работы дерева решений. Деревья решений (классификация). Деревья решений (Регрессия). Преимущества и недостатки деревьев решений. Bias, Variance, Irreducible error. Бутстрэп. Корреляция и построение доверительного интервала для нее. Построение доверительного интервала для качества метрики. Ансамбли. Корректирующий код. Усреднение предсказания классификаторов. Bagging = Bootstrap aggregation. Метод случайных подпространств (RSM, random subspace method). Комбинация RSM и Bagging. Случайный лес. Boosting. Gradient boosting (градиентный бустинг). Модификации градиентного бустинга. Блендинг и Стэкинг. Некоторые практические рекомендации. Применение нейронных сетей к табличным данным

## Лекция 4 “Генерация и отбор признаков”

Проблемы при работе с реальной задачей машинного обучения. Дисбаланс классов. Обнаружение аномалий. Кластеризация. Алгоритм K-Means. Алгоритм DBSCAN. Кодирование признаков. Типы признаков. Преобразования признаков. Разведочный анализ данных (exploratory data analysis, EDA). Описательные статистики. Взаимодействие признаков. Анализ категориальных признаков. Генерация признаков. Baseline. Генерация признаков, полученных при помощи другой модели. Ручная генерация признаков. Отбор признаков. Зачем отбирать признаки. Полный перебор. Одномерный отбор признаков. Отбор признаков на основе моделей. Randomization/Permutation. Boruta. Жадный отбор признаков. Рекомендации по отбору признаков. Задача понижения размерности. Manifold assumption. PCA (Метод главных компонент). Kernel PCA Ядровой (нелинейный) метод главных компонент. t-SNE (t-distributed Stochastic Neighbor Embedding). UMAP

## Лекция 5 “Нейронные сети”

Ограничения Линейного классификатора. ХОR — проблема. Проблемы классификации более сложных объектов. Многослойные сети. Обучение нейронной сети. Прямое и обратное распространение. Веса сети. Нейронная сеть как универсальный аппроксиматор. Метод обратного распространения ошибки. Функции потерь (loss functions). Функции активации. Углубление в PyTorch. Пример нейронной сети на MNIST. Dataset и DataLoader. Трансформации (Transforms). Создание нейронной сети. Обучение нейронной сети. Сохранение и загрузка весов модели

## Лекция 6 “Сверточные нейронные сети”

Введение в сверточные нейронные сети. Полносвязная нейронная сеть. Нарушение связей между соседними пикселями. Другие 'hand-crafted' фильтры. Свертка с фильтром. Сверточный слой нейросети. Применение свёрточных слоёв. Общая структура свёрточной нейронной сети. Другие виды сверток. 1D. 3D. Визуализация. Визуализация весов. Визуализация карт активаций. Feature extractor. Практические рекомендации. Аугментация. Transfer learning. Порядок действий при transfer learning. Практический пример transfer learning

## Лекция 7 “Улучшение сходимости нейросетей и борьба с переобучением”

Сигмоида затухает и теоретически, и практически. Затухание градиента. Нормализация входов и выходов. Нормализация входных данных. Нормализация целевых значений в задаче регрессии. Инициализация весов. Инициализация Ксавье (Xavier Glorot). Инициализация Каймин Хе (Kaiming He). Важность инициализации весов. Ортогональная инициализация. Инициализация весов в PyTorch. Слои нормализации. Covariate shift (Ковариантный сдвиг). Internal covariate shift. Batch Normalization. Другие Normalization. Регуляризация. L1, L2 регуляризации. Dropout. DropConnect. DropBlock. Batch Normalization до или после Dropout. Оптимизация параметров нейросетей. Обзор популярных оптимизаторов. Ландшафт функции потерь. Сравнение оптимизаторов. Режимы обучения. Ранняя остановка. Понижение шага обучения на каждой эпохе. Neural Network WarmUp. Cyclical learning schedule. Взаимодействие learning schedule и адаптивного изменения learning rate

## Лекция 8 “Архитектуры CNN”

Базовые компоненты свёрточных сетей. ImageNet. Метрики Top 1 and Top5. Baseline (AlexNet 2012). Тюнинг гиперпараметров (ZFnet). Базовый блок (VGGNet 2014). Вычислительные ресурсы. Inception module (GoogLeNet 2014). "Stem network". Global Average Pooling. Затухание градиента. Batchnorm (революция глубины). Skip connection (ResNet 2015). Архитектура ResNet. Stage ratio. BasicBlock в PyTorch. Bottleneck layer. Обучение ResNet. Grouped Convolution (ResNeXt 2016). Grouped Convolution in PyTorch. ResNeXt = Skip connection + Inception + Grouped convolution. Обзор сети MobileNet (2017 г.). Сравнение моделей. Много skip connection (DenseNet 2016). Ширина вместо глубины (WideResNet 2016). Squeeze-and-Excitation (SENet 2017). Поиск хорошей архитектуры. Neural Architecture Search. Обзор сети EfficientNet (2019 г.). Self Attention (ViT 2020). Сравнение со сверткой. Архитектура ViT. Предсказание с помощью ViT. Обучение ViT. DeiT: Data-efficient Image Transformers. ConvNext (2022). Model soups (2022). Обучение без разметки. Дистилляция. Feature extraction. BYOL. CLIP. Практические рекомендации

## Лекция 9 “Рекуррентные нейронные сети (RNN)”

Особенности данных. Примеры задач. Базовый RNN блок. RNNCell. Пример прогнозирования временного ряда. Теория и классические подходы. Разделение данных. Статистические модели предсказания. Нейросетевой подход. Создание и обучение модели. LSTM. LSTMCell. LSTM in PyTorch. Модификации LSTM. Пример посимвольной генерации текста. Подготовка данных. Создание и обучение модели. Embedding. Токенизация. Sequence-to-Sequence with RNNs. Аугментация. NLP метрики. Традиционные метрики. Референсные нейросетевые метрики. Безреференсные метрики

## Лекция 10 “Трансформеры”

Attention. Sequence-to-Sequence with RNNs and Attention mechanism. Модели внимания в машинном переводе. Модели внимания в задаче генерации подписи к изображениям. Проблема attention. Разновидности функций сходства векторов. Key, query, value. Multihead Attention. Image Captioning with RNNs and Attention. Transformer для машинного перевода. Архитектура сети Transformer. Общий пайплайн задачи машинного перевода. Архитектура трансформера-кодировщика. Архитектура трансформера-декодировщика. Небольшая историческая справка. Hugging Face. Языковое моделирование. Как работает GPT. Токенизация. Архитектура GPT. Positional Encoding. Transformer Decoder Block. Методы Генерации текста. Сравнение поколений GPT. Файнтюнинг. Как происходит обучение. Обучающие данные. Training. Результат файнтюнинга. NLP метрики. Примеры применений Transformer. BERT (Bidirectional Encoder Representations from Transformers ). GPT (Generative Pretrained Transformer )

## Лекция 11 “Сегментация и детектирование”

Задачи компьютерного зрения. Dataset COCO — Common Objects in Context. Семантическая сегментация (*Semantic segmentation*). Способы предсказания класса для каждого пикселя. Fully Convolutional Networks. Разжимающий слой. Пирамида признаков. IoU — оценка точности. Loss функции для сегментации. U-Net: Convolutional Networks for Biomedical Image Segmentation. Обзор DeepLabv3+ (2018). Segmentation models PyTorch (SMP). Детектирование (Object detection). Детектирование единственного объекта. Детектирование нескольких объектов. Object proposal. NMS. Backbone для детекторов. YOLO. Instance Segmentation. Panoptic Segmentation. OWL-ViT (2022). SAM (2023). Оценка качества детекции. mAP — mean Average Precision. Практические соображения

## Лекция 12 “Обучение представлений”

Representation learning. Unsuprevised Representation learning. Metric learning. Формирование векторов-признаков (embeddings). Сиамская сеть (Siamese Network). Реализация сиамской сети. Автоэнкодеры (AE). Unsupervised learning. Понижение размерности. Архитектура автоэнкодера. Сжатие информации и потери. Manifold assumption. Метод главных компонент (PCA). Аналогия AE и PCA. Очищение изображения от шумов. PCA для избавления от шума. Реализация автоэнкодера. Обнаружение аномалий. Предобучение на неразмеченных данных. Автоэнкодер как генератор и его ограничения. Плавная интерполяция. Вариационные автоэнкодеры (VAE). Реализация вариационного автоэнкодера. Проблемы  «ванильного» VAE. Автоэнкодеры с условием (CAE). Условные вариационные автоэнкодеры (CVAE). Реализация вариационного автоэнкодера с условиями, CVAE. Разделение (disentangling) стиля и метки

## Лекция 13 “Генеративные модели”

"Классические" генеративные алгоритмы. Задача генерации. Простейший пример: генерация объектов из нормального распределения. Генеративные алгоритмы, основанные на глубоком обучении. Введение  в генеративно-состязательные нейронные сети. Latent space. Наивный подход в решении задачи генерации. Дискриминатор. Generative adversarial network (GAN). DCGAN — Генерация изображений. cGAN — GAN с условием. Тонкости обучения GANов. Диффузионные модели. Прямой диффузный процесс. Обратный диффузный процесс. Denoising U-Net. Реализация прямого и обратного диффузного процесса. Обучение диффузных моделей

## Лекция 14 “Explainability”

Причины использования Explainability. Оценка важности признаков для простых моделей. Оценка важности признака для линейных моделей. Оценка важности признака для дерева. Методы, изучающие отклик модели на изменение входных данных. ICE (Individual Conditional Expectation). LIME (Local Interpretable Model-agnostic Explanations). SHAP (SHapley Additive exPlanations). Градиентные методы. Vanilla Gradient. SmoothGrad. Integrated Gradients. SHAP Deep Explainer. Grad-CAM. Критика градиентных методов. Методы, специфичные для трансформеров. Attention rollout. Attention Flow. Gradient-weighted Attention Rollout. Заключение

## Лекция 15 “Обучение с подкреплением”

Обучение с учителем. Обучение без учителя. Обучение с подкреплением. Терминология: агент, функция награды, состояние среды. Классические примеры задач RL. Особенности и сложности RL. Gym. Markov property. Markov process. Матрица состояний. Награда (Reward). Суммарная награда (Return). Марковский процесс принятия решений. Формальное описание MDP. Нахождение лучшей последовательности переходов. Уравнение Беллмана. Нахождение оптимальной политики Беллмана. Траектории MDP. Q-Learning. Пример c CartPole DQN. Дальнейшие идеи. Альтернативные подходы

