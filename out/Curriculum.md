Программа курса

## Лекция 1 “Введение в машинное обучение”

Два пути. Задача курса. AI, ML, DL. Области применения. Связь с наукой. Обзор курса. Задачи. Базовые. Комбинированные задачи. План исследования. Сбор и подготовка данных. Разведочный анализ. Baseline. Метрики. Построение модели, эксперименты. Проверка гипотез. Анализ работы модели. Инструменты. Данные. Связность данных. Загрузка и визуализация данных. Работа с данными и моделью. Описание модели k-NN. Простейшая метрика. Параметры и гиперпараметры модели. Разделение train-validation-test. Стратификация. Кросс-валидация. Алгоритм кросс-валидации. Оценка результата кросс-валидации. Типичные ошибки при кросс-валидации. GridSearch. RandomizedSearch. Метрики классификации. Accuracy. Confusion matrix. Balanced accuracy. Precision, Recall. F-мера. AUC-ROC. PR-кривая. Multiclass accuracy. Multilabel. Метрики регрессии. MAE (mean absolute error). MSE (mean squared error). RMSE (root mean squared error). R². MSLE (mean squared logarithmic error)

## Лекция 2 “Линейные модели”

Линейная регрессия. Модель и ее параметры. Функция потерь. Поиск локального минимума. Метод наименьших квадратов. Метрики регрессии. Модель линейной регрессии из библиотеки scikit-learn. Метод градиентного спуска. Градиент. Идея градиентного спуска. Выбор скорости обучения. Единый подход к учету смещения. Необходимость нормализации. Cтохастический градиентный спуск. Условия применимости линейной регрессии. Квартет Энскомбе (Anscombe’s quartet). Анализ остатков. Проблема корреляции признаков. Регуляризация. Линейная классификация. Постановка задачи. Переход к вероятностям. Многоклассовая классификация. Cross-Entropy loss. Метод опорных векторов (SVM). 1D классификация. Многомерная классификация. Обобщенные линейные модели. Полиномиальная модель. Kernel SVM. Наивный Байесовский классификатор. Пример на табличных данных. Практические особенности работы с линейными моделями. Нормализация данных. Борьба с переобучением

## Лекция 3 “Классическое машинное обучение”

Необходимость методов классического машинного обучения. Деревья решений. Принцип работы дерева решений. Классификация. Регрессия. Свойства деревьев решений. Bias, Variance, Irreducible error. Бутстрэп. Построение доверительного интервала для качества метрики. Ансамбли. Bagging = **B**ootstrap **agg**regat**ing**. Метод случайных подпространств (RSM, random subspace method). Комбинация RSM и Bagging. Случайный лес. Boosting. Gradient boosting (градиентный бустинг). Модификации градиентного бустинга. Блендинг и Стэкинг. Применение нейронных сетей к табличным данным

## Лекция 4 “Генерация и отбор признаков”

Проблемы при работе с реальными данными. Дисбаланс классов. Обнаружение аномалий. Кластеризация. Алгоритм K-Means. Алгоритм DBSCAN. Кодирование признаков. Типы признаков. Преобразования признаков. Разведочный анализ данных. Описательные статистики. Взаимодействие признаков. Анализ категориальных признаков. Генерация признаков. Baseline. Генерация признаков, полученных при помощи другой модели. Ручная генерация признаков. Отбор признаков. Зачем отбирать признаки. Полный перебор. Одномерный отбор признаков. Отбор признаков на основе моделей. Randomization/Permutation. Boruta. Жадный отбор признаков. Рекомендации по отбору признаков. Задача понижения размерности. Manifold assumption. PCA (Метод главных компонент). Kernel PCA (нелинейный) метод главных компонент. t-SNE (t-distributed Stochastic Neighbor Embedding). UMAP

## Лекция 5 “Нейронные сети”

Ограничения линейных моделей. Проблемы классификации более сложных объектов. Многослойные нейронные сети. Веса и смещения. Нейронная сеть как универсальный аппроксиматор. Обучение нейронной сети. Прямое и обратное распространение. Метод обратного распространения ошибки. Функции потерь (loss functions). Функции активации. Углубление в PyTorch. Пример нейронной сети на MNIST. Dataset и DataLoader. Трансформации (Transforms). Создание нейронной сети. Обучение нейронной сети

## Лекция 6 “Сверточные нейронные сети”

Введение в сверточные нейронные сети. Полносвязная нейронная сеть. Нарушение связей между соседними пикселями. Свертка с фильтром. Сверточный слой нейросети. Применение свёрточных слоёв. Общая структура свёрточной нейронной сети. Другие виды сверток. 1D. 3D. Графовые свертки. Визуализация. Визуализация весов. Визуализация карт признаков. Feature extractor. Аугментации. Lightning. Практические рекомендации

## Лекция 7 “Улучшение сходимости нейросетей и борьба с переобучением”

Трудности при обучении глубоких нейронных сетей. Затухание градиента. Нормализация входов и выходов. Нормализация входных данных. Нормализация целевых значений в задаче регрессии. Инициализация весов. Инициализация Ксавье (Xavier Glorot). Инициализация Каймин Хе (Kaiming He). Важность инициализации весов. Инициализация весов в PyTorch. Слои нормализации. Internal covariate shift. Batch Normalization. Другие Normalization. Регуляризация. L1, L2 регуляризации. Dropout. DropConnect. DropBlock. Batch Normalization до или после Dropout. Оптимизация параметров нейросетей. Обзор популярных оптимизаторов. Ландшафт функции потерь. Сравнение оптимизаторов. Режимы обучения. Ранняя остановка. Уменьшение скорости обучения на плато. Понижение скорости обучения на каждой эпохе. Neural Network WarmUp. Cyclical learning schedule. Model soup. Взаимодействие learning schedule и адаптивного изменения learning rate

## Лекция 8 “Архитектуры CNN”

Базовые компоненты свёрточных сетей. ImageNet. Метрики ImageNet. Baseline (AlexNet 2012). Тюнинг гиперпараметров (ZFnet). Базовый блок (VGGNet 2014). Вычислительные ресурсы. Inception module (GoogLeNet 2014). Stem network. Global Average Pooling. Затухание градиента. BatchNorm (революция глубины). Skip connection (ResNet 2015). Архитектура ResNet. BasicBlock в PyTorch. Bottleneck layer. Stage ratio. Обучение ResNet. Grouped Convolution. Grouped Convolution in PyTorch. ResNeXt. Обзор сети MobileNet (2017 г.). Сравнение моделей. Много skip connection (DenseNet 2016). Ширина вместо глубины (WideResNet 2016). Squeeze-and-Excitation (SENet 2017). Поиск хорошей архитектуры. Обзор сети EfficientNet (2019 г.). Трансформеры. ConvNext (2022). Torch Image Models (timm). Custom feature extractor. Дистилляция. Hard targets. Soft targets

## Лекция 9 “Рекуррентные нейронные сети (RNN)”

Временные ряды. Компоненты временных рядов. Задачи анализа временных рядов. Особенности валидации при анализе временных рядов. Линейные модели временных рядов. Нелинейные модели временных рядов. Рекуррентный слой в нейронных сетях. RNN слой в PyTorch. Пример прогнозирования временного ряда с помощью RNN. Проблемы RNN. LSTM (Long Short-Term Memory). GRU (Gated Recurrent Unit). Обработка естественного языка (NLP). Bidirectional RNN. Представления текстовых данных. Пример посимвольной генерации текста. Подготовка данных. Создание и обучение модели. Задача Sequence-to-Sequence. Задача машинного перевода. Пример реализации машинного перевода. NLP метрики

## Лекция 10 “Трансформеры”

Классический seq2seq. Cross-Attention. RNN + Cross-Attention. Разновидности функций сходства. Self-Attention. Архитектура сети Transformer. Кодировщик. Подготовка данных. HuggingFace. BERT. Декодировщик. GPT. Методы Генерации текста. Файнтюнинг. Большие языковые модели (LLM). LLaMA. LoRa. DeepSpeed. NLP метрики. BertScore. Self Attention (ViT 2020). Архитектура ViT. Предсказание с помощью ViT. DeiT: Data-efficient Image Transformers. Использование ViT с собственным датасетом

## Лекция 11 “Сегментация и детектирование”

Задачи компьютерного зрения. Dataset COCO — Common Objects in Context. Семантическая сегментация (Semantic segmentation). Способы предсказания класса для каждого пикселя. Fully Convolutional Networks. Разжимающий слой. Пирамида признаков. Метрики. Loss функции для сегментации. U-Net: Convolutional Networks for Biomedical Image Segmentation. Обзор DeepLabv3+ (2018). Segmentation models PyTorch (SMP). Albumentations. Особенности применения аугментаций при задаче сегментации. Пример использования Albumentations. Детектирование (Object detection). Детектирование единственного объекта. Детектирование нескольких объектов. Object proposal. NMS. Backbone для детекторов. YOLO. Instance Segmentation. Panoptic Segmentation. OWL-ViT2 (Jul 2023). SAM (2023). Оценка качества детекции. mAP — mean Average Precision. Практические соображения

## Лекция 12 “Обучение представлений”

Глубокие нейронные сети как модели обучения представлений. Понижение размерности и гипотеза о многообразии. Metric learning. Формирование векторов признаков. Сиамская сеть. Реализация сиамской сети. CLIP. Автоэнкодеры (AE). Архитектура автоэнкодера. Функции потерь в автоэнкодерах. Очищение изображения от шумов. Реализация автоэнкодера. Обнаружение аномалий. Предобучение на неразмеченных данных. Автоэнкодер как генератор и его ограничения. Вариационные автоэнкодеры (VAE). Семплирование в латентном пространстве. Регуляризация латентного пространства. Реализация VAE. Плавная интерполяция. Векторная арифметика. Ограничения VAE. Условные вариационные автоэнкодеры (CVAE). Реализация CVAE

## Лекция 13 “Генеративные модели”

Общий взгляд на генеративные алгоритмы. Задача генерации. Простейший пример: генерация объектов из нормального распределения. Генеративные алгоритмы, основанные на глубоком обучении. Введение  в генеративно-состязательные нейронные сети GAN. Вход модели (latent space). Наивный подход в решении задачи генерации. Дискриминатор. Generative adversarial network (GAN). DCGAN — Генерация изображений. Тонкости обучения GAN. Метрики генерации. cGAN — GAN с условием. Модификации GAN. Диффузионные модели. Базовые понятия библиотеки Diffusers. Прямой диффузионный процесс (добавление шума). Обратный диффузионный процесс (удаление шума). Обучение модели. Диффузия в латентном пространстве

## Лекция 14 “Explainability”

Мотивация использования Explainability. Объяснимость моделей классического ML. Оценка важности признаков для линейных моделей. Оценка важности признаков для деревьев решений. Методы, изучающие отклик модели на изменение входных данных. ICE (Individual Conditional Expectation). LIME (Local Interpretable Model-agnostic Explanations). SHAP (SHapley Additive exPlanations). Градиентные методы. Vanilla Gradient. SmoothGrad. Integrated Gradients. Grad-CAM. Критика градиентных методов

## Лекция 15 “Обучение с подкреплением”

Обучение методом проб и ошибок. Мотивация использования обучения с подкреплением. Устоявшаяся терминология. Примеры задач, решаемых с использованием RL. Stateless environment in RL. Задача о многоруких бандитах. Gymnasium framework. Поиск оптимальной стратегии решения. Мета-эвристики в обучении с подкреплением. Cross-entropy method (CEM). Пример CEM в Gym. Марковский процесс принятия решений (Markov decision process, MDP). Markov property. Markov process. Проблема поиска оптимальной политики. Поиск оптимальной политики Беллмана для MDP (решение "MDP"). Temporal difference (TD)-обучение (TD-learning). Q-Learning. Deep Q-Learning

