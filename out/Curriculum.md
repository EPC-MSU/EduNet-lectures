Программа курса

## Лекция 1 “Введение в машинное обучение”

Задача курса. AI, ML, DL. Области применения. Связь с наукой. Обзор курса. Базовые задачи. Комбинированные задачи. Инструменты. Контейнеры. План исследования. Сбор и подготовка данных. Извлечение закономерностей. Валидация результата. Пример ML задачи. Данные. Связность данных. Загрузка и визуализация данных. Алгоритм k-NN. Описание модели. Нормализация данных. Параметры и гиперпараметры модели. Метрики. Accuracy. Precision, Recall. F-мера. AUC-ROC. Разделение train-validation-test. Примеры ошибок в данных и при разбиении. Подбор гиперпараметров на тестовой выборке. Кросс-валидация. Алгоритм кросс-валидации. Оценка результата кросс-валидации. Типичные ошибки при кросс-валидации. Кросс-валидация для научных исследований: на что обратить внимание. GridSearch. RandomizedSearch

## Лекция 2 “L02_Linear_classifier.ipynb”

Линейные модели. Задача регресии. Линейная регрессия. Случай многих переменных и практические особенности работы с линейными моделями. Линейный классификатор с пороговой функций принятия решения. Hard Margin Classifier. Линейный классификатор с Hinge loss. Soft Margin Classifier. Метод градиентного спуска. Численный расчет производной. Аналитический расчет производной от функции потерь. Выбор шага обучения. Алгоритм стохастического градиентного спуска. Выбор размера батча. Регуляризация. Обобщенные линейные модели. Kernel SVM. Вероятностный подход в задаче классификации. Наивный Байесовский классификатор. Кросс-энтропия, как общая функция потерь для задач классификации. Расчет функции потерь. Пример обучения линейного классификатора с hinge loss. MNIST. CIFAR-10

## Лекция 3 “Классическое машинное обучение”

Системы предсказаний. Экспертные системы (Rule-based systems). Классическое машинное обучение. Глубокое машинное обучение. Необходимость методов классического машинного обучения. Деревья решений. Принцип работы дерева решений. Деревья решений (классификация). Деревья решений (Регрессия). Деревья решений и работа с пропущенными значениями. Преимущества и недостатки деревьев решений. Bias, Variance, Irreducible error. Бутстрэп. Корреляция и построение доверительного интервала для нее. Построение доверительного интервала для качества метрики. Ансамбли. Корректирующий код. Усреднение предсказания классификаторов. Bagging = Bootstrap aggregation. Метод случайных подпространств (RSM, random subspace method). Комбинация RSM и Bagging. Случайный лес. Boosting. Gradient boosting (градиентный бустинг). Модификации градиентного бустинга. Блендинг и Стэкинг. Некоторые практические рекомендации. Применение нейронных сетей к табличным данным

## Лекция 4 “Генерация и отбор признаков”

Генерация признаков. Типы признаков. Преобразования признаков. Практический пример работы с признаками. Примеры данных, которые нецелесообразно отправлять в модель в сыром виде. Отбор признаков. Зачем отбирать признаки. Полный перебор. Одномерный отбор признаков. Жадный отбор признаков. Отбор признаков на основе моделей. Randomization/Permutation. Boruta. Dropped variable importance. Отбор признаков — это тоже выбор гиперпарметров. Задача понижения размерности. Manifold assumption. PCA (Метод главных компонент). Kernel PCA Ядровой (нелинейный) метод главных компонент. Методы, основанные на сохранении расстояний. t-SNE (t-distributed stochastic neighbor embedding). UMAP. Кластеризация

## Лекция 5 “Нейронные сети”

Ограничения Линейного классификатора. ХОR — проблема. Проблемы классификации более сложных объектов. Многослойные сети. Обучение нейронной сети. Прямое и обратное распространение. Веса сети. Нейронная сеть как универсальный аппроксиматор. Метод обратного распространения ошибки. Функции потерь (loss functions). Функции активации. Углубление в PyTorch. Пример нейронной сети на MNIST. Dataset и DataLoader. Трансформации (Transforms). Создание нейронной сети. Обучение нейронной сети. Сохранение и загрузка весов модели

## Лекция 6 “Сверточные нейронные сети”

Введение в сверточные нейронные сети. Полносвязная нейронная сеть. Нарушение связей между соседними пикселями. Другие 'hand-crafted' фильтры. Свертка с фильтром. Сверточный слой нейросети. Использование свёрточных слоёв. Пример сверточной сети. Визуализация. Визуализация весов. Визуализация карт активаций. Feature extractor

## Лекция 7 “Улучшение сходимости нейросетей и борьба с переобучением”

Сигмоида затухает и теоретически, и практически. Затухание градиента. Инициализация весов. Инициализация Ксавье (Xavier Glorot). Инициализация Каймин Хе (Kaiming He). Важность инициализации весов. Обобщение инициализаций Ксавье и Каймин Хе. Ортогональная инициализация. Инициализация весов в PyTorch. Нормализация. Нормализация входных данных. Covariate shift (Ковариантный сдвиг). Internal covariate shift. Batch Normalization. Другие Normalization. Регуляризация. L1, L2 регуляризации. Dropout. DropConnect. DropBlock. Batch Normalization до или после Dropout. Оптимизация параметров нейросетей. Обзор популярных оптимизаторов. Ландшафт функции потерь. Сравнение оптимизаторов. Режимы обучения. Ранняя остановка. Понижение шага обучения на каждой эпохе. Cyclical learning schedule. Neural Network WarmUp

## Лекция 8 “Рекуррентные нейронные сети (RNN)”

Особенности рекуррентных нейронных сетей. Примеры задач. Базовый RNN блок. RNNCell. Пример прогнозирования временного ряда. Подготовка данных. Создание и обучение модели. Пример посимвольной генерации текста. Подготовка данных. Создание и обучение модели. LSTM. LSTMCell. LSTM in PyTorch. Модификации LSTM. Embedding. Sequence-to-Sequence with RNNs. Attention. Sequence-to-Sequence with RNNs and Attention mechanism. Проблема attention. Image Captioning with RNNs and Attention. Key, query, value. Attention Layer. Self-Attention Layer. Multihead Self-Attention Layer. Attention is all you need. Decoder. Токенизация. Примеры применений Transformer. BERT (Bidirectional Encoder Representations from Transformers ). GPT (Generative Pretrained Transformer )

## Лекция 9 “Архитектуры CNN”

Базовые компоненты свёрточных сетей. ImageNet Large Scale Visual Recognition Challenge. Обзор сети AlexNet (2012 г.). Архитектура ZFnet (2013 г.). Обзор сети VGGNet (2014 г.). Оценка вычислительных ресурсов. Обзор сети GoogLeNet (2014 г.). Inception module. 1x1 Convolution. "Stem network". Global Average Pooling. Затухание градиента. Появление "глубоких" моделей (deep models). Обзор сети ResNet (2015 г.). Residual connection. Архитектура ResNet. ResNet: bottleneck layer. ResNet реализация в PyTorch. Обучение ResNet. Обзор сети ResNeXt (2016 г.). Groupped Convolution. Grouped Convolution in PyTorch. ResNeXt, Inception, Grouped convolution. Feature extraction. Сравнение моделей. Обзор сети DenseNet (2016 г.). WideResNet (2016 г.). Архитектура SENet (2017 г.). Обзор сети MobileNet (2017 г.). Depthwise separable convolution. Inverted residual block. Swish. Neural Architecture Search. Обзор сети EfficientNet (2019 г.). Обзор Visual Transformers (2020 г.). Архитектура ViT. Предсказание с помощью ViT. Обучение ViT. Использование ViT с собственным датасетом. Обзор сети MLP-Mixer (2020 г.)

## Лекция 10 “Explainability”

Причины использования Explainability. Обнаружение некорректных зависимостей. Доверие к предсказаниям. Публикации в научных журналах. Explainability & Interpretability. Оценка важности признаков в простых случаях. Оценка важности признака для линейных моделей. Оценка важности признака для дерева. Randomization/Permutation. Boruta. Dropped variable importance. Градиентные методы оценки важности признаков. Gradient Ascent. Grad-CAM. Библиотеки для реализации explanation и визуализации. SHAP (SHapley Additive exPlanations). LIME. Другие библиотеки. Заключение

## Лекция 11 “Обучение на реальных данных”

Проблемы при работе с реальной задачей машинного обучения. Общие подходы при работе с реальными данными. Недостаток данных. Баланс классов. Изменение баланса класса сэмплированием. Аугментация. Изображения. Аудио. Текст. Transfer Learning. Структурные компоненты. Практический пример Transfer Learning. Few/One-Shot learning. Формирование векторов-признаков (embedding). Сиамская сеть (Siamese Network). Реализация сиамской сети. Few-shot learning in GPT. Оптимизация гиперпараметров

## Лекция 12 “Сегментация и детектирование”

Задачи компьютерного зрения. Dataset COCO — Common Objects in Context. Семантическая сегментация (*Semantic segmentation*). Способы предсказания класса для каждого пикселя. Автокодировщик. Разжимающий слой. Пирамида признаков. U-Net: Convolutional Networks for Biomedical Image Segmentation. IoU — оценка точности. Loss функции для сегментации. Fully Convolutional Networks. Обзор DeepLabv3+ (2018). Детектирование (Object detection). Детектирование единственного объекта. Детектирование нескольких объектов. Нard Example Mining. Instance Segmentation. Оценка качества детекции. mAP — mean Average Precision. DINO — Self-supervised representation learning (with segmentation capabilities). Сегментация изображений. Сегментация видео. Кластеризация. Swin Transformer

## Лекция 13 “GAN — Генеративно-состязательные нейронные сети”

Введение  в генеративно-состязательные нейронные сети. Latent space. Наивный подход в решении задачи генерации. Дискриминатор. Generative adversarial network (GAN). DCGAN — Генерация изображений. cGAN — GAN с условием. ProGAN, StyleGAN, StyleGAN2, Alias-Free GAN. Тонкости обучения GANов. Частые/простые ошибки. Зачем давать преимущество дискриминатору. Использование оптимизатора ADAM. Top K Training. Краткое описание примечательных моделей GAN. GAN для решения задачи распознавания капчи. Pix2Pix. Семантическая генерация. Text to image. Задача переноса стиля

## Лекция 14 “Автоэнкодеры”

Автоэнкодер (AE). Unsupervised learning. Representation learning. Снижение размерности. Архитектура автоэнкодера. Сжатие информации и потери. Manifold assumption. Метод главных компонент (PCA). Аналогия AE и PCA. Очищение изображения от шумов. PCA для избавления от шума. Реализация автоэнкодера. Разреженный автоэнкодер. Автоэнкодер, как генератор и его ограничения. Плавная интерполяция. Вариационные автоэнкодеры (VAE). Реализация вариационного автоэнкодера. Почему KL(Q||P). Проблемы  «ванильного» VAE. Автоэнкодеры с условием (CAE). Условные вариационные автоэнкодеры (CVAE). Реализация вариационного автоэнкодера с условиями, CVAE. Состязательные автокодировщики (AAE = AE + GAN). Разделение (disentangling) стиля и метки. Semisupervised AAE

## Лекция 15 “Обучение с подкреплением”

Обучение с учителем. Обучение без учителя. Обучение с подкреплением. Терминология: агент, функция награды, состояние среды. Отличие от supervised learning. Классические примеры задач RL. Особенности и сложности RL. Gym. Markov property. Markov process. Матрица состояний. Награда (Reward). Суммарная награда (Return). Марковский процесс принятия решений. Формальное описание MDP. Нахождение лучшей последовательности переходов. Уравнение Беллмана. Нахождение оптимальной политики Беллмана. Temporal difference (TD) learning. Q-Learning. Пример c CartPole DQN. Дальнейшие идеи. Альтернативные подходы

