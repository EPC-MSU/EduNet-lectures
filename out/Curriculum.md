Программа курса

## Лекция 1 “Введение в машинное обучение”

Два пути. Задача курса. AI, ML, DL. Области применения. Связь с наукой. Обзор курса. Задачи. Базовые. Комбинированные задачи. План исследования. Сбор и подготовка данных. Разведочный анализ. Baseline. Метрики. Построение модели, эксперименты. Проверка гипотез. Анализ работы модели. Инструменты. Данные. Связность данных. Загрузка и визуализация данных. Работа с данными и моделью. Описание данных. Описание модели k-NN. Простейшая метрика. Разделение train-validation-test. Параметры и гиперпараметры модели. Стратификация. Нормализация. k-NN в прикладных задачах. Кросс-валидация. Алгоритм кросс-валидации. Оценка результата кросс-валидации. Типичные ошибки при кросс-валидации. GridSearch. RandomizedSearch. Метрики. Accuracy. Precision, Recall. F-мера. AUC-ROC. Multilabel

## Лекция 2 “Линейные модели”

Линейная регрессия. Модель и ее параметры. Функция потерь. Поиск локального минимума. Метод наименьших квадратов. Метрики регрессии. Модель линейной регрессии из библиотеки scikit-learn. Метод градиентного спуска. Градиент. Идея градиентного спуска. Выбор скорости обучения. Единый подход к учету смещения. Необходимость нормализации. Cтохастический градиентный спуск. Классификация. Hinge loss. 1D классификация. Maximum Margin Classifier. 2D классификация. SVM: Hard and Soft Margin Classifier. 3D классификация. Многоклассовая классификация. Обобщенные линейные модели. Полиномиальная модель. Kernel SVM. Практические особенности работы с линейными моделями. Нормализация данных. Проблема корреляции признаков  в случае линейных моделей. Регуляризация. Вероятностный подход в задаче классификации. Наивный Байесовский классификатор. Кросс-энтропия как общая функция потерь для задач классификации. Расчет функции потерь

## Лекция 3 “Классическое машинное обучение”

Системы предсказаний. Экспертные системы (Rule-based systems). Классическое машинное обучение. Глубокое машинное обучение. Необходимость методов классического машинного обучения. Деревья решений. Принцип работы дерева решений. Классификация. Регрессия. Преимущества и недостатки деревьев решений. Bias, Variance, Irreducible error. Бутстрэп. Построение доверительного интервала для качества метрики. Ансамбли. Корректирующий код. Усреднение предсказания классификаторов. Bagging = Bootstrap aggregation. Метод случайных подпространств (RSM, random subspace method). Комбинация RSM и Bagging. Случайный лес. Boosting. Gradient boosting (градиентный бустинг). Модификации градиентного бустинга. Блендинг и Стэкинг. Некоторые практические рекомендации. Применение нейронных сетей к табличным данным

## Лекция 4 “Генерация и отбор признаков”

Проблемы при работе с реальными данными. Дисбаланс классов. Обнаружение аномалий. Кластеризация. Алгоритм K-Means. Алгоритм DBSCAN. Кодирование признаков. Типы признаков. Преобразования признаков. Разведочный анализ данных. Описательные статистики. Взаимодействие признаков. Анализ категориальных признаков. Генерация признаков. Baseline. Генерация признаков, полученных при помощи другой модели. Ручная генерация признаков. Отбор признаков. Зачем отбирать признаки. Полный перебор. Одномерный отбор признаков. Отбор признаков на основе моделей. Randomization/Permutation. Boruta. Жадный отбор признаков. Рекомендации по отбору признаков. Задача понижения размерности. Manifold assumption. PCA (Метод главных компонент). Kernel PCA (нелинейный) метод главных компонент. t-SNE (t-distributed Stochastic Neighbor Embedding). UMAP

## Лекция 5 “Нейронные сети”

Ограничения линейного классификатора. Проблемы классификации более сложных объектов. ХОR — проблема. Многослойные нейронные сети. Веса и смещения. Нейронная сеть как универсальный аппроксиматор. Обучение нейронной сети. Прямое и обратное распространение. Метод обратного распространения ошибки. Функции потерь (loss functions). Функции активации. Углубление в PyTorch. Пример нейронной сети на MNIST. Dataset и DataLoader. Трансформации (Transforms). Создание нейронной сети. Обучение нейронной сети. Сохранение и загрузка весов модели

## Лекция 6 “Сверточные нейронные сети”

Введение в сверточные нейронные сети. Полносвязная нейронная сеть. Нарушение связей между соседними пикселями. Свертка с фильтром. Сверточный слой нейросети. Применение свёрточных слоёв. Общая структура свёрточной нейронной сети. Другие виды сверток. 1D. 3D. Графовые свертки. Визуализация. Визуализация весов. Визуализация карт активаций. Feature extractor. Transfer learning. Аугментация. Lightning. Практические рекомендации

## Лекция 7 “Улучшение сходимости нейросетей и борьба с переобучением”

Трудности при обучении глубоких нейронных сетей. Затухание градиента. Нормализация входов и выходов. Нормализация входных данных. Нормализация целевых значений в задаче регрессии. Инициализация весов. Инициализация Ксавье (Xavier Glorot). Инициализация Каймин Хе (Kaiming He). Важность инициализации весов. Ортогональная инициализация. Инициализация весов в PyTorch. Слои нормализации. Internal covariate shift. Batch Normalization. Другие Normalization. Регуляризация. L1, L2 регуляризации. Dropout. DropConnect. DropBlock. Batch Normalization до или после Dropout. Оптимизация параметров нейросетей. Обзор популярных оптимизаторов. Ландшафт функции потерь. Сравнение оптимизаторов. Режимы обучения. Ранняя остановка. Понижение шага обучения на каждой эпохе. Neural Network WarmUp. Cyclical learning schedule. Model soup. Взаимодействие learning schedule и адаптивного изменения learning rate

## Лекция 8 “Архитектуры CNN”

Базовые компоненты свёрточных сетей. ImageNet. Baseline (AlexNet 2012). Метрики ImageNet. Тюнинг гиперпараметров (ZFnet). Базовый блок (VGGNet 2014). Вычислительные ресурсы. Inception module (GoogLeNet 2014). Stem network. Global Average Pooling. Затухание градиента. Batchnorm (революция глубины). Skip connection (ResNet 2015). Архитектура ResNet. BasicBlock в PyTorch. Bottleneck layer. Обучение ResNet. Grouped Convolution. Grouped Convolution in PyTorch. ResNeXt. Обзор сети MobileNet (2017 г.). Сравнение моделей. Много skip connection (DenseNet 2016). Ширина вместо глубины (WideResNet 2016). Squeeze-and-Excitation (SENet 2017). Поиск хорошей архитектуры. Обзор сети EfficientNet (2019 г.). Трансформеры. ConvNext (2022). Torch Image Models (timm). Custom feature extractor. Обучение без разметки. Дистилляция. CLIP. Практические рекомендации

## Лекция 9 “Рекуррентные нейронные сети (RNN)”

Особенности данных. Теория и классические подходы. Статистические модели предсказания. Разделение данных. Рекуррентные нейронные сети. Области применения. Основная идея. Базовый RNN блок. Пример прогнозирования временного ряда. Нейросетевой подход. Создание и обучение модели. LSTM. LSTMCell. LSTM в PyTorch. Модификации LSTM. Типы задач. Пример посимвольной генерации текста. Подготовка данных. Создание и обучение модели. Представление данных. Токенизация. TF-IDF. Word2Vec. Слой эмбеддингов. Размер словаря. Byte Pair Encoding. Аугментация. NLP метрики. Задача Sequence-to-Sequence. Реализация

## Лекция 10 “Трансформеры”

Классический seq2seq. Attention. RNN + Attention. Вычислительная сложность. Разновидности функций сходства. Архитектура сети Transformer. Кодировщик. Алгоритм. Self-Attention. Multihead Attention. BERT. BertScore. Декодировщик. GPT. Методы Генерации текста. Файнтюнинг. LLaMA. LoRa. NLP метрики. Self Attention (ViT 2020). Архитектура ViT. Предсказание с помощью ViT. DeiT: Data-efficient Image Transformers. Использование ViT с собственным датасетом

## Лекция 11 “Сегментация и детектирование”

Задачи компьютерного зрения. Dataset COCO — Common Objects in Context. Семантическая сегментация (Semantic segmentation). Способы предсказания класса для каждого пикселя. Fully Convolutional Networks. Разжимающий слой. Пирамида признаков. Метрики. Loss функции для сегментации. U-Net: Convolutional Networks for Biomedical Image Segmentation. Обзор DeepLabv3+ (2018). Segmentation models PyTorch (SMP). Детектирование (Object detection). Детектирование единственного объекта. Детектирование нескольких объектов. Object proposal. NMS. Backbone для детекторов. YOLO. Instance Segmentation. Panoptic Segmentation. OWL-ViT2 (Jul 2023). SAM (2023). Оценка качества детекции. mAP — mean Average Precision. Практические соображения

## Лекция 12 “Обучение представлений”

Representation learning. Unsuprevised Representation learning. Metric learning. Формирование векторов-признаков (embeddings). Сиамская сеть (Siamese Network). Реализация сиамской сети. Автоэнкодеры (AE). Unsupervised learning. Понижение размерности. Архитектура автоэнкодера. Сжатие информации и потери. Manifold assumption. Метод главных компонент (PCA). Аналогия AE и PCA. Очищение изображения от шумов. PCA для избавления от шума. Реализация автоэнкодера. Обнаружение аномалий. Предобучение на неразмеченных данных. Автоэнкодер как генератор и его ограничения. Плавная интерполяция. Вариационные автоэнкодеры (VAE). Реализация вариационного автоэнкодера. Проблемы  «ванильного» VAE. Условные вариационные автоэнкодеры (CVAE). Реализация вариационного автоэнкодера с условиями, CVAE. Разделение (disentangling) стиля и метки

## Лекция 13 “Генеративные модели”

"Классические" генеративные алгоритмы. Задача генерации. Простейший пример: генерация объектов из нормального распределения. Генеративные алгоритмы, основанные на глубоком обучении. Введение  в генеративно-состязательные нейронные сети. Latent space. Наивный подход в решении задачи генерации. Дискриминатор. Generative adversarial network (GAN). DCGAN — Генерация изображений. cGAN — GAN с условием. Тонкости обучения GANов. Диффузионные модели. Прямой диффузный процесс. Обратный диффузный процесс. Denoising U-Net. Реализация прямого и обратного диффузного процесса. Обучение диффузных моделей

## Лекция 14 “Explainability”

Причины использования Explainability. Оценка важности признаков для простых моделей. Оценка важности признака для линейных моделей. Оценка важности признака для дерева. Методы, изучающие отклик модели на изменение входных данных. ICE (Individual Conditional Expectation). LIME (Local Interpretable Model-agnostic Explanations). SHAP (SHapley Additive exPlanations). Градиентные методы. Vanilla Gradient. SmoothGrad. Integrated Gradients. SHAP Deep Explainer. Grad-CAM. Критика градиентных методов. Методы, специфичные для трансформеров. Attention rollout. Attention Flow. Gradient-weighted Attention Rollout. Заключение

## Лекция 15 “Обучение с подкреплением”

Как работает обучение с подкреплением. Место RL в машинном обучении. Терминология: агент, функция награды, состояние среды. Классические примеры задач RL. Особенности и сложности RL. Библиотеки. Gym. Markov property. Markov process. Матрица состояний. Награда (Reward). Суммарная награда (Return). Марковский процесс принятия решений. Формальное описание MDP. Нахождение лучшей последовательности переходов. Уравнение Беллмана. Нахождение оптимальной политики Беллмана. Траектории MDP. Q-Learning. Пример c CartPole DQN. Дальнейшие идеи. Альтернативные подходы

