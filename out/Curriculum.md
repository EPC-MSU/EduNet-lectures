Программа курса

## Лекция 1 “Введение в машинное обучение”

ML и DL и AI в Computer Science. Связь DL с наукой. История развития DL. Сферы применения и технологии . Причины успехов технологий на основе DL. ML — Подход к научным проблемам . Базовые задачи ML. Типы данных. Примеры датасетов. Оценка результата. Демонстрация работы с данными. Табличные данные. Аудиоданные. Обзор контейнеров. Предобработка данных . Обзор видов связности данных. Работа с изображениями

## Лекция 2 “Линейный классификатор”

Ограничения алгоритма k-nearest neighbors (kNN). kNN для классификации. Практические аспекты работы с классификаторами. Линейный классификатор. Переход к сравнению с шаблоном. Переход к весам. Support Vector Machine (метод опорных векторов). Функция потерь SVM. Вычисление функции потерь SVM. Обновления весов методом градиентного спуска. Градиент функции потерь. Численный расчет производной. Аналитический расчет производной от функции потерь SVM. Выбор шага обучения. Выбор размера батча. Регуляризация. Функция потерь Кросс-энтропия. Переход к вероятностям. Определение Кросс-энтропии. Градиент функции потерь Кросс-энтропии. Практическое вычисление SoftMax

## Лекция 3 “Классическое машинное обучение”

Системы предсказаний. Экспертные системы (Rule-based systems). Классическое машинное обучение. Глубокое машинное обучение. Необходимость методов классического машинного обучения. Деревья решений. Принцип работы дерева решений. Деревья решений (классификация). Деревья решений (Регрессия). Деревья решений и работа с пропущенными значениями. Преимущества и недостатки деревьев решений. Bias, Variance, Irreducible error. Бутстрэп. Корреляция и построение доверительного интервала для нее. Построение доверительного интервала для качества метрики. Ансамбли. Корректирующий код. Усреднение предсказания классификаторов. Bagging  = Bootstrap aggregation. Метод случайных подпространств (RSM, random subspace method). Комбинация RSM и Bagging. Случайный лес. Boosting. AdaBoost. Gradient boosting (градиентный бустинг). Модификации градиентного бустинга. Блендинг и Стэкинг. Применение нейронных сетей к табличным данным

## Лекция 4 “Генерация и отбор признаков”

Генерация признаков. Типы признаков. Преобразования признаков. Практический пример работы с признаками. Примеры данных, которые нецелесообразно отправлять в модель в сыром виде. Отбор признаков. Зачем отбирать признаки. Полный перебор. Одномерный отбор признаков. Жадный отбор признаков. Отбор признаков на основе моделей. Отбор признаков — это тоже выбор гиперпарметров. Задача понижении размерности. Manifold assumption. PCA (Метод главных компонент). Kernel PCA Ядровой (нелинейный) метод главных компонент. Методы, основанные на сохранении расстояний. tSNE (t-distributed stochastic neighbor embedding). UMAP. tSNE и UMAP на цифрах

## Лекция 5 “Нейронные сети”

Ограничения Линейного классификатора. ХОR — проблема. Проблемы классификации более сложных объектов. Переход от линейного классификатора к перцептрону. Перцептрон — нейросеть с одним скрытым слоем. Многослойные сети. Функции потерь (loss functions). Функции активации. Обучение нейронной сети. Прямое распространение. Веса сети. Метод обратного распространения ошибки. Основная идея метода. Граф вычислений. Преимущества и недостатки метода. Пример простой сети на датасете mnist

## Лекция 6 “Сверточные нейронные сети”

Введение в сверточные нейронные сети. Полносвязная нейронная сеть. Нарушение связей между соседними пикселями. Примеры 'hand-crafted' фильтров. Свертка с фильтром. Сверточный слой нейросети. Основные параметры свёртки. Использование свёрточных слоёв. Пример сверточной сети. Визуализация. Визуализация весов. Визуализация карт активаций. Saliency map. Feature extractor

## Лекция 7 “Улучшение сходимости нейросетей и борьба с переобучением”

Сигмоида затухает и теоретически и практически. Затухание градиента. Инициализация весов. Инициализация Ксавьера (Xavier, Glorot). He-инициализация (Kaiming). Важность инициализации весов. Обобщение инициализаций Ксавьера и He-инициализации. Ортогональная инициализация. Инициализация весов в Pytorch. Регуляризация. L1, L2 регуляризации. Dropout. Dropconnect. DropBlock. Нормализация. Нормализация входных данных. Covariate shift(Ковариантный сдвиг). Internal covariate shift. BatchNormalization. Другие Normalization. Оптимизация весов нейросетей. Обзор популярных оптимизаторов. Сравнение оптимизаторов. Режимы обучения. Ранняя остановка. Понижение шага обучения на каждой эпохе. Cyclical learning schedule. Neural Network WarmUp

## Лекция 8 “Рекуррентные нейронные сети (RNN)”

Особенности рекуррентных нейронных сетей. Примеры задач. Базовый RNN блок. RNNCell. Пример прогнозирования временного ряда. Подготовка данных. Создание и обучение модели. Пример посимвольной генерации текста. Подготовка данных. Создание и обучение модели. LSTM. Gates (Врата). LSTMCell. LSTM in Pytorch. Модификации LSTM. Sequence-to-Sequence with RNNs. Attention. Sequence-to-Sequence with RNNs and Attention mechanism. Проблема attention. Image Captioning with RNNs and Attention. Key, query, value. Attention Layer. Self-Attention Layer. Multihead Self-Attention Layer. Attention is all you need. Decoder. Примеры применений Transformer. BERT (Bidirectional Encoder Representations from Transformers ). GPT (Generative Pretrained Transformer )

## Лекция 9 “Архитектуры CNN”

Базовые компоненты сверточных сетей. ImageNet Large Scale Visual Recognition Challenge. Обзор сети AlexNet(2012 г.). Архитектура ZFnet(2013 г.). Обзор сети VGGNet(2014 г.). Оценка памяти занимаемой моделью. Обзор сети GoogleNet(2014 г.). Inception module. 1x1 Convolution. "Stem network". Global Average Pooling. Затухание градиента. Появление "глубоких" моделей (deep models). Обзор сети ResNet(2015 г.). Resudial connection. Архитектура ResNet. ResNet: bottleneck layer. ResNet реализация в Pytorch. Обучение ResNet. Обзор сети ResNeXt(2016 г.). Groupped Convolution. Grouped convolution in Pytorch. ResNext, Inception, grouped conv. Feature extraction. Сравнение моделей. Обзор сети DenseNet(2016 г.). 2016 WideResNet. Архитектура SENet(2017 г.). Обзор сети MobileNet(2017 г.). Swish. Depthwise separable convolution. Shuffled Grouped Convolution. Neural Architecture Search. Обзор сети EfficientNet(2019 г.). Обзор Visual Transformers(2020 г.). Архитектура ViT. Предсказание с помощью ViT. Обучение ViT. Использование ViT с собственным датасетом. Обзор сети MLP-Mixer(2020 г.)

## Лекция 10 “Explainability”

Причины использования Explainability. Обнаружение некорректных зависимостей. Доверие к предсказаниям. Публикации в научных журналах. Explainability & Interpretability. Оценка важности признаков в простых случаях. Оценка важности признака для регрессии. Оценка важности признака для дерева. Randomization/Permutation. Dropped variable importance. Библиотеки для реализации explanation. SHAP (SHapley Additive exPlanations). LIME. Boruta. Другие библиотеки. Примеры explanations для разных видов данных. Табличные данные. NLP: Пример абстрактного обобщения текста. Изображения

## Лекция 11 “Обучение на реальных данных”

Проблемы при работе с реальной задачей машинного обучения. Общие подходы при работе с реальными данными. Большее количество данных. Изменение баланса класса сэмплированием. Аугментация. Изображения. Аудио. Текст. Transfer Learning. Структурные компоненты. Практический пример Transfer Learning. Few/One-Shot learning. Формирование векторов-признаков (embedding). Сиамская сеть (Siamese Network). Реализация сиамской сети. Few-shots learning in GPT. Оптимизация гиперпараметров

## Лекция 12 “Сегментация и детектирование”

Задачи компьютерного зрения. Dataset COCO — Common Objects in COntext. Семантическая сегментация (*Semantic segmentation*). Способы предсказания класса для каждого пикселя. Автокодировщик. U-Net: Convolutional Networks for Biomedical Image Segmentation. Мультиклассовая сегментация. Обзор Fully Convolutional Network(2014). Обзор DeepLabv3+(2018). Детектирование (Object detection). Детектирование нескольких объектов. Нard Example Mining. Instance Segmentation. ROI Align. Оценка качества детекции. mAP - mean Average Precision. DINO — Self-supervised representation learning (with segmentation capabilities). Результаты DINO. Принцип работы. Сегментация изображений. Сегментация видео. Кластеризация

## Лекция 13 “GAN — Генеративно-состязательные нейронные сети”

Введение  в генеративно-состязательные нейронные сети. Latent space. Наивный подход в решении задачи генерации. Дискриминатор. Generative adversarial network (GAN). DCGAN — Генерация изображений. cGAN — GAN с условием. Wasserstein GAN. ProGAN, StyleGAN, StyleGAN2, Alias-Free GAN. Тонкости обучения GANов. Частые/простые ошибки. Зачем давать преимущество дискриминатору. Использование оптимизатора ADAM. Top K Training. Краткое описание примечательных моделей GAN. GAN для решения задачи распознавания капчи. BigGAN. Domain transfer network. SRGAN и StackGANs. Pix2Pix. Семантическая генерация. Text to image. Задача переноса стиля

## Лекция 14 “Автоэнкодеры”

Автоэнкодер (AE). Unsupervised learning. Representation learning. Снижение размерности. Архитектура автоэнкодера. Сжатие информации и потери. Manifold assumption. Метод главных компонент (PCA). Аналогия AE и PCA. Очищение изображения от шумов. PCA для избавления от шума. Реализация автоэнкодера. Разреженный автоэнкодер. Автоэнкодер как генератор и его ограничения. Плавная интерполяция. Вариационные автоэнкодеры (VAE). Реализация вариационного автоэнкодера. Почему KL(Q||P). Проблемы  «ванильного» VAE. Автоэнкодеры с условием(CAE). Условные вариационные автоэнкодеры (CVAE). Реализация вариационного автоэнкодера с условиями, CVAE. Состязательные автокодировщики (AAE = AE + GAN). Разделение (disentangling) стиля и метки. Semisupervised AAE

## Лекция 15 “Обучение с подкреплением”

Обучение с учителем. Обучение без учителя. Обучение с подкреплением. Терминология: агент, функция награды, состояние среды. Отличие от supervised learning. Классические примеры задач RL. Особенности и сложности RL.. Состояние среды (State). Markov property. Markov process. Матрица состояний. Награда (Reward). Суммарная награда (Return). Уравнение Беллмана. Марковский процесс принятия решений. Формальное описание MDP. Нахождение лучшей последовательности переходов. Нахождение оптимальной политики Беллмана. Q - Learning. Exploration vs exploitation. Пример c CartPole DQN. Дальнейшие идеи. Альтернативные подходы

