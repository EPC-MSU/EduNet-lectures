{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsmL4eVM-EEB"
   },
   "source": [
    "# Архитектуры CNN\n",
    "\n",
    "Мы рассмотрели базовые компоненты, из которых состоят современные сверточные нейронные сети, а также техники их обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0bNOuQr-EEN"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_1.png\"  width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/l09.png\" >\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/l09_2.png\" width=\"760\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuvAbIqq_KQC"
   },
   "source": [
    "На этом занятии рассмортим, какие именно модели можно построить на основе этих компонент. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg0zzVpq-EEP"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_2.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvMN3RH-uVU3"
   },
   "source": [
    "Условия: http://image-net.org/challenges/LSVRC/2017/\n",
    "\n",
    "Загрузка с [официального сайта](http://image-net.org/download) недоступна.\n",
    "\n",
    "\n",
    "Можно загрузить данные с [Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/data)\n",
    "\n",
    "Однако архив занимает порядка 156Gb и не поместится на диск Colab. \n",
    "Поэтому воспользуемся другим репозиторием, в котором находится 1000 изображений из оригинального датасета.\n",
    "\n",
    "P.S. Для загрузки данных, которые стали недоступны на официальных сайтах, можно использовать  https://academictorrents.com\n",
    "\n",
    "В частности для ImageNet:\n",
    "\n",
    "https://academictorrents.com/browse.php?search=imagenet\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-2SdIiElTIy"
   },
   "outputs": [],
   "source": [
    "# Full list of labels \n",
    "#! wget \"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\"\n",
    "#! wget 'https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'\n",
    "! wget 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "\n",
    "# Tiny Image-Net\n",
    "#урезанная версия датасета подготовленная в стенфордском университете.\n",
    "#!wget 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "#!unzip tiny-imagenet-200.zip\n",
    "\n",
    "! git clone https://github.com/ajschumacher/imagen.git\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DQvmGVOEUcd"
   },
   "source": [
    "Загрузили категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vs3UAeuipEI"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "with open('imagenet_class_index.json') as f:\n",
    "      imagenet_labels = json.load(f)\n",
    "\n",
    "print(imagenet_labels)\n",
    "\n",
    "armadillo = Image.open('imagen/imagen/n02454379_10511_armadillo.jpg')\n",
    "sunglasses = Image.open('imagen/imagen/n04356056_8960_sunglasses.jpg')\n",
    "volleyball = Image.open('imagen/imagen/n04540053_14903_volleyball.jpg')\n",
    "microwave = Image.open('imagen/imagen/n03761084_32799_microwave.jpg')\n",
    "strawberry = Image.open('imagen/imagen/n07745940_1671_strawberry.jpg')\n",
    "goldfish = Image.open('imagen/imagen/n01443537_2625_goldfish.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UYUvXrzmTy8"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MicroImageNet(Dataset):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Load labels\n",
    "    self.num2id = {}\n",
    "    with open('imagenet_class_index.json') as f:\n",
    "      imagenet_labels = json.load(f)\n",
    "    w_net = {}\n",
    "    # Because not world net all image codes from imagen exists in imagenet_labels \n",
    "    # we need to filter this image\n",
    "    for key in imagenet_labels.keys():\n",
    "      wn_id = imagenet_labels[key][0]\n",
    "      w_net[wn_id] = {'num': int(key), 'name': imagenet_labels[key][1] }\n",
    "    self.labels = []\n",
    "    self.paths = []\n",
    "\n",
    "    # Load data\n",
    "    images = glob('imagen/imagen/*.jpg')\n",
    "    for i, path in enumerate(images):\n",
    "      name = path.split(\"_\")[2] # Class name\n",
    "      id = path.split(\"_\")[0][14:] # WorldNet based ID\n",
    "      if w_net.get(id,None): \n",
    "        self.labels.append([w_net[id]['num'], w_net[id]['name'], id])\n",
    "        self.paths.append(path)\n",
    "    \n",
    "  def __getitem__(self,idx):\n",
    "    im = Image.open(self.paths[idx])\n",
    "    class_num = self.labels[idx][0]\n",
    "    return im, class_num\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.paths)\n",
    "\n",
    "\n",
    "microImgNet = MicroImageNet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OENo6KyWEXUS"
   },
   "source": [
    "Посмотрим на картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guXvmPqspgPr"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (25,15)\n",
    "plt.tight_layout()\n",
    "\n",
    "def show(img,label1,num,label2=\"\"):\n",
    "    ax = plt.subplot(2, 3,num+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(label1)\n",
    "    ax.set_xlabel(label2)\n",
    "  \n",
    "\n",
    "for i in range(6):\n",
    "  im, cls = microImgNet[i]\n",
    "  name = microImgNet.labels[i][1]\n",
    "  show(im,name,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqIioZCI_yaT"
   },
   "source": [
    "Как упомяналось на первой лекции, современный бум нейростевых технологий начался в 2012 году, когда AlexNet с большим отрывом от конкурентов победила в ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhYmlwCm-EER"
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "\n",
    "создатели: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n",
    "**University of Toronto**\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_3.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Особенности:\n",
    "- Большие фильтры на первых слоях\n",
    "- Обучалась на двух видеокартах  GTX580 ( ... )\n",
    "- Уменьшение размерности\n",
    "- 2 Полносвязанных слоя в конце\n",
    "\n",
    "https://neurohive.io/ru/vidy-nejrosetej/alexnet-svjortochnaja-nejronnaja-set-dlja-raspoznavanija-izobrazhenij/\n",
    "\n",
    "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "\n",
    "ImageNet Classification with Deep ConvolutionalNeural Networks\n",
    "\n",
    "\n",
    "В AlexNet есть все компоненты, которые мы рассматривали ранее. Её архитектура состоит из пяти свёрточных слоёв, между которыми располагаются pooling-слои и слои нормализации, а завершают нейросеть три полносвязных слоя.\n",
    "\n",
    "\n",
    "На схеме архитектуры все выходные изображения делятся на два одинаковых участка — это связано с тем, что нейросеть обучалась на старых GPU GTX580, у которых было всего 3 ГБ видеопамяти. Для обработки использовались две видеокарты, чтобы параллельно выполнять операции над двумя частями изображения.\n",
    "Изначально на вход подаётся фотография размером 227×227×3, и размер свёрточных фильтров первого слоя — 11×11. Всего применяется 96 фильтров с шагом 4.\n",
    "\n",
    "Пространственные размеры изображения сначала довольно сильно сжимаются, затем постепенно увеличивается число фильтров. В результате ширина и высота этого уже не изображения, а набора активаций признаков, сильно уменьшаются, после чего оно поступает на два полносвязных своя, где количество весов уже довольно большое.\n",
    "\n",
    "\n",
    "AlexNet не получится использовать для классификации cifar10, потому что если начать так агрессивно уменьшать изображение размером 32Х32 px, то в определенный момент в него просто не поместится следующий фильтр, который нужно применить, и изображение просто исчезнет.\n",
    "\n",
    "Структура некоторых (особенно старых) сетей, заточенных под ImageNet, напрямую зависит от размера изображений: если соотношение сторон позволит фильтрам поместиться, проблем не возникнет,  если же оно меньше и в какой-то момент размер уменьшится до 2Х2 или 1Х1, то фильтр 3Х3 просто не сработает. В современных сетях есть слой, который позволяет решить эту проблему.\n",
    "\n",
    "\n",
    "\n",
    "На тот момент такая архитектура показала прорывную точность (ошибка упала с 20% до 15.4%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R5-zshX-EES"
   },
   "source": [
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_4_1.png\"  width=\"750\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_4_2.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Также стоит упомянуть о нескольких важных параметрах нейросети, необычных на тот момент:\n",
    "\n",
    "\n",
    "* AlexNet — практически первая архитектура, в которой применяется нелинейность ReLU. Ранее использовались сигмоидальные функции, которые работали медленнее;\n",
    "\n",
    "* Используется собственная нормализация (не столь универсальная, как Batch-нормализацию) с отдельными слоями. Как выяснилось позднее, они не дают значительного улучшающего эффекта и поэтому не распространены в современных архитектурах;\n",
    "\n",
    "* На этапе предварительной обработки используется очень большое дополнение данных (аугментация);\n",
    "\n",
    "* Dropout 0.5 (то есть при регуляризации отсеивается половина нейронов);\n",
    "\n",
    "* Размер пакета — 128;\n",
    "\n",
    "* SGD Momentum 0.9 (как показывают сегодняшние эксперименты, это не плохой вариант, но чтобы обучение сходилось, им требовалась с помощью эвристики ниже периодически обучать Learning rate);\n",
    "\n",
    "* Скорость обучения — 1e−2, снижается в 10 раз вручную, если точность в какой-то момент перестаёт расти;\n",
    "\n",
    "\n",
    "* Затухание весов L2 — 5e−4;\n",
    "\n",
    "* В архитектуре используется ансамбль из 7 CNN — это позволило снизить процент ошибок с 18,2% до 15,4%. (Ансамбль моделей — это когда обучается несколько моделей, а результат считается по среднему значению. Здесь используются 7 сетей, результат усредняется. Таким образом достаточно сильно снижается ошибка).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exzX8LDCDi8J"
   },
   "source": [
    "Сравним Pytorch реализацию с оригинальной. В чем отличия?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AVBRR95CfV7"
   },
   "outputs": [],
   "source": [
    "from torchvision import models,utils\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jenu7MTnDdyF"
   },
   "source": [
    "Проверим как работает. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOrdx6Buk8EE"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "\n",
    "def img2tensor(img):\n",
    "  t = F.to_tensor(img)\n",
    "  #t = F.normalize(t, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "  return t\n",
    "\n",
    "def catId2names(nums):\n",
    "  titles = []\n",
    "  for num in nums:\n",
    "    titles.append(imagenet_labels[str(num.item())][1])\n",
    "    titles.reverse()\n",
    "  return \", \".join(titles)\n",
    "  \n",
    "for i in range(6,12):\n",
    "  im, cls = microImgNet[i]\n",
    "  tensor = img2tensor(im)\n",
    "  out = alexnet(tensor.unsqueeze(0)) # Add batch dimension\n",
    "  labels_num = torch.argsort(out[0]) # Ascending order\n",
    "  predicted = catId2names(labels_num[-5:]) # Top 5\n",
    "  titles = []\n",
    "  name =  microImgNet.labels[i][1]\n",
    "  show(im,name,i-6,predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYp2G8zcADcx"
   },
   "source": [
    "Почему так плохо?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cdmvHwx-EEU"
   },
   "source": [
    "## ZFnet\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_5.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Тюнингованный AlexNet\n",
    "\n",
    "Matthew D. Zeilerzeiler@cs.nyu.eduDept. of Computer Science, Courant Institute, Rob Fergusfergus@cs.nyu.eduDept. of Computer Science, **Courant Institute, New York Universit**\n",
    "\n",
    "\n",
    "В 2013 году выиграла соревнования созданная учеными из Йорского университета нейросеть ZFnet, достигнув результата 11.7% — в ней AlexNet использовалась в качестве основы, но с изменёнными параметрами и слоями.\n",
    "\n",
    "Отличия от AlexNet небольшие:\n",
    "* Немного поменялись размеры фильтров (было 11, стало 7);\n",
    "* Увеличилось общее количество фильтров;\n",
    "\n",
    "В целом и количество слоев, и общая структура сети, когда слои свертки и пулинга перемежаются друг с другом, а затем идут два полносвязных слоя, сохранились.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAVT-30n-EEW"
   },
   "source": [
    "## VGGNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_6.png\"  width=\"850\">\n",
    "\n",
    "Karen Simonyan and Andrew Zisserman\n",
    "\n",
    "Visual Geometry Group - **Oxford**\n",
    "\n",
    "https://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "\n",
    "* Появление \"стандартных\" блоков внутри модели\n",
    "* Свертки 3x3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVs2mn0D4XPW"
   },
   "source": [
    "#### доп. инфо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHXloOzJk2T7"
   },
   "source": [
    "\n",
    "В 2014-ом году в Оксфорде была разработана модель VGGNet. Сеть получилась более точной и более глубокой.\n",
    "\n",
    "На слайде выше изображены сети AlexNet и две версии VGG-16 и VGG-19 с 16 и 19 слоями соответственно. На соревнованиях победила более глубокая VGG19, достигнув более чем в два раза лучшего результата по сравнению с AlexNet.\n",
    "\n",
    "Несколько фактов об архитектуре VGGNet:\n",
    "\n",
    "* Нейросеть заняла 2 место в задаче классификации и 1 место в локализации на соревновании ImageNet (при локализации необходимо не только классифицировать объект, но и обвести его в ограничивающие рамки);\n",
    "\n",
    "* Процедура обучения такая же, как у AlexNet;\n",
    "\n",
    "* Слои нормализации отсутствуют.\n",
    "\n",
    "В прикладных задачах обычно используются архитектуры VGG16 или VGG19 (VGG19 работает лучше, но расходует больше памяти).\n",
    "Особенности данной архитектуры: все сверточные слои имеют фильтры с рецептивным полем размера 3×3, они объединены в блоки, состоящие из некоторого количества сверток с разным (постепенно увеличивающимся) количеством фильтров. Затем идут слои пулинга.\n",
    "Появление \"стандартных\" блоков внутри модели — одно из важных нововведений. Идея базового блока внутри сети будет достаточно широко использоваться дальше.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYQ0kicj4d6d"
   },
   "source": [
    "## Ресурсы\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSLsJjsrCkkh"
   },
   "source": [
    "Чтобы понять, в чем смысл этих усовершенствований, нужно оценить количество памяти для хранения весов и количество операций, которые требуется выполнить при прямом прохождении сигнала через сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oe0Sq__-EEW"
   },
   "source": [
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_7.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD7iiCXvDmmj"
   },
   "source": [
    "*Пример оценки (на AlexNet)*\n",
    "\n",
    "Чем больше особенностей попадает в рецептивное поле свертки, тем к большим обобщениям она должна быть способна. Несмотря на это, авторы сети решили отказаться от использования сверток с большим рецептивным полем в пользу скорости потому, что оказалось, что размер можно заменить количеством, что 2 маленьких фильтра работают как один большой или даже лучше.\n",
    "\n",
    "Основная причина, по которой они начали об этом думать — это увеличение сложности. **Большее количество слоев требует больше памяти, весов и вычислительных мощностей**.\n",
    "\n",
    "На слайде изображен один слой. Тензор красного цвета — это вход, его мы не считаем.\n",
    "\n",
    "Первый параметр — **количество элементов на выходе слоя**. Синий тензор — это активации, для которых требуется память во время работы модели. Чтобы понять, сколько нужно памяти, чтобы хранить активации, которые мы получаем, нужно умножить количество фильтров на пространственные размеры изображения и на количество байт, которые требуются для хранения одного элемента.\n",
    "\n",
    "**Количество весов (параметров)** — это пространственные размеры одного фильтра, умноженные на количество слоев и на количество фильтров. В то время, как правая часть занимает память только во время работы, обученные веса являются ценностью и должны сохраняться на диск.\n",
    "\n",
    "**Количество операций, которые нам потребуются** — это количество элементов на выходе, которые мы хотим получить, умноженное на количество операций. Для получения каждого элемента нужно свернуть фильтр, который имеет размер К×К с каждым слоем входа.\n",
    "Все это будет учитываться при построении сетей большего размера.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyUbV-Ys4rtN"
   },
   "source": [
    "Оценка памяти для хранения параметров слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVAD59bn3Esm"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "conv_layer = nn.Conv2d(3,64,5,stride = 1, padding = 1)\n",
    "for tag, p in conv_layer.named_parameters():\n",
    "  print(tag, p.shape) \n",
    "\n",
    "# memory = (64 * 5 * 5 + 64 (bias) ) * 4 (byte in float32) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siih9oIYAtt5"
   },
   "source": [
    "## Базовый блок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6gGgvlI-EEY"
   },
   "source": [
    "## Размер рецептивного поля\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_8.png\"  width=\"850\">\n",
    "\n",
    "Авторы стремились уменьшить количество параметров, используя следующую идею:\n",
    "\n",
    "Допустим, мы свернули часть изображения с фильтром 5×5 (на слайде выше — квадрат синего цвета), получили одну активацию (одна свертка — одна активация).\n",
    "\n",
    "Если же начать сворачивать ту же область с фильтром 3×3, то мы получим блок активаций (изображен красным). Но если второй раз применить такой же фильтр (размером 3×3) уже к блоку А2, он его закроет полностью и мы получим единичную активацию.\n",
    "\n",
    "Для того же рецептивного поля свертка с двумя последовательными фильтрами 3×3 будет эквивалентна свертке с одним фильтром 5×5. При этом потребуется меньшее количество параметров, потому что в данном случае мы складываем две свертки. В случае же с одним фильтром 5×5 это зависит от размера фильтра, который возводится в квадрат. Соответственно, $2⋅(3^2⋅𝐶^2)$ меньше, чем $5^2⋅𝐶^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHkWL8fL-EEZ"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_9.png\"  width=\"850\">\n",
    "\n",
    "Если же размер фильтра будет ещё больше, то будет больше и выгода.\n",
    "\n",
    "Получается, что можно заменять фильтры, не теряя при этом их обобщающую способность. К тому же в процессе применения этих нескольких фильтров мы добавляем дополнительные нелинейности, которые нам обеспечивают находящиеся между свертками функции активации. То есть операция получается достаточно выигрышной.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb1lBBrb-EEb"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_10.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Отчасти благодаря такой экономии получилось сделать большую по тем временам сеть (16 слоев), но, несмотря на способы уменьшить вычислительную сложность и количество параметров, сеть все равно получилась огромной (для 16-тислойной сети одно изображение в памяти занимало 96 МБ).\n",
    "\n",
    "Надо понимать, что при обучении мы используем батч, размеры этого батча ограничены этим <font color=red >каким?</font> числом.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLL66UuZ-EEf"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_11.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "* Основная часть памяти расходуется на большие свертки в начальных слоях, где пространственные размеры (ширина и высота) велики\n",
    "\n",
    "* Больше всего весов в полносвязанных слоях\n",
    "\n",
    "* Вычислительные ресурсы нужны в первую очередь для сверток\n",
    "\n",
    "\n",
    "VGG-16 получилась существенно больше по сравнению с и так довольно объемной AlexNet, и тем более по сравнению с современными моделями.\n",
    "\n",
    "В значительной степени с этим связано дальнейшее направление развития моделей. В следующем году ImageNet выиграла сеть под названием GoogleNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtQgRP_DJ8K_"
   },
   "source": [
    "## Оценка памяти занимаемой моделью\n",
    "\n",
    "Установим библиотеку для мониторинга ресурсов GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPa_CGeLFtLF"
   },
   "outputs": [],
   "source": [
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSRBf8ZhF15w"
   },
   "outputs": [],
   "source": [
    "import humanize\n",
    "import GPUtil as GPU\n",
    "import psutil\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def gpu_usage():\n",
    "    GPUs = GPU.getGPUs()\n",
    "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
    "    if len(GPUs) == 0:\n",
    "        return False\n",
    "    gpu = GPUs[0]\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1bJQdsSKTi1"
   },
   "source": [
    "Посмотрим сколько памяти потребуется VGG19 и какого размера batch можно использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd-r5PncF6dn"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "vgg19 = torchvision.models.vgg19(pretrained = True, progress = True)\n",
    "vgg19.requires_grad=True\n",
    "vgg19.cuda()\n",
    "\n",
    "!nvidia-smi # Common GPU info\n",
    "\n",
    "vgg19.train()\n",
    "\n",
    "\n",
    "for bs in [1,8,16,32,64]:\n",
    "  dummy_input = torch.rand(bs,3,224,224,device = 'cuda:0')\n",
    "  out = vgg19(dummy_input)\n",
    "  print(\"Batch size\", bs)\n",
    "  gpu_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYL1YjkyNSlS"
   },
   "source": [
    "Очистка памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHBs2lhKKgFc"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvkybwCJLFnk"
   },
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()\n",
    " !nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc1NTxDtLPu-"
   },
   "outputs": [],
   "source": [
    "dummy_input = None #del dummy_input \n",
    "out = None #del out\n",
    "\n",
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAiQb0BJ-EEg"
   },
   "source": [
    "## GooleNet\n",
    "В отличие от предшествующих моделей разработана в коммерческой компании с целью реального применения. \n",
    "\n",
    "Поэтому основной упор был сделан на эффективности.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_12.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHMO5qmik2UB"
   },
   "source": [
    "GoogleNet — ещё более глубокая архитектура с 22 слоями. В ней нет полносвязных слоёв, и она содержит всего 5 миллионов параметров — в 12 раз меньше, чем у AlexNet. При этом сеть оказалась немного более точной (ошибка снизилась с 7.3% до 6.7%).\n",
    "\n",
    "Рассмотрим, за счет чего удалось достичь такого огромного выигрыша в ресурсах, так как многие идеи, которые впервые были использованы для GoogleNet, активно применяются до сих пор.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjRODyby-EEg"
   },
   "source": [
    " ### Inception module\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_13.png\"  width=\"850\">\n",
    "\n",
    "Для того, чтобы разработать с наибольшей вычислительной эффективностью, был придуман так называемый модуль Inception, основная идея которого заключается в том, что он сам по себе является небольшой локальной сетью. Его архитектура состоит из множества таких модулей, следующих друг за другом.\n",
    "\n",
    "Вместо того, чтобы добавлять новые слои фильтров один над другим, инженеры из Google решили делать свертки параллельно. Они попробовали поместить разные варианты сверток в один блок и объединить их результаты (конкатенировать, а не сложить) между собой.\n",
    "\n",
    "То есть фильтры применяются параллельно. Затем результаты объединяются, и создаётся выходной сигнал, который переходит на следующий слой.\n",
    "\n",
    "Но здесь же кроется проблема: при таком подходе \"в лоб\" количество фильтров все равно растет довольно быстро.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sjsBXgGCSza"
   },
   "outputs": [],
   "source": [
    "# Вспомогательный метод для запуска Tensorboard в Colab\n",
    "\n",
    "# Fix https://stackoverflow.com/questions/60730544/tensorboard-colab-tensorflow-api-v1-io-gfile-has-no-attribute-get-filesystem\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "import os\n",
    "# Запуск Tensorboard в Colab\n",
    "def reinit_tensorboard(clear_log = True):\n",
    "  # Лог-файлы читаются из этого каталога: \n",
    "  logs_base_dir = \"runs\"\n",
    "  if clear_log:\n",
    "    # Очистка логов\n",
    "    !rm -rfv {logs_base_dir}/*\n",
    "    os.makedirs(logs_base_dir, exist_ok=True)\n",
    "  # Магия Colab\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO9LIcHJKz47"
   },
   "outputs": [],
   "source": [
    "googlenet = models.googlenet()\n",
    "# https://pytorch.org/vision/stable/_modules/torchvision/models/googlenet.html#googlenet\n",
    "#https://hackmd.io/@bouteille/Bk-61Fo8U\n",
    "#print(googlenet)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "reinit_tensorboard(clear_log = True)\n",
    "writer = SummaryWriter(comment = tag)\n",
    "dummy_input = torch.rand((1,3,224,224))\n",
    "writer.add_graph(googlenet,dummy_input)\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55pUVeNT-EEh"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_14.png\"  width=\"850\">\n",
    "\n",
    "Чтобы этого избежать, введены так называемые «узкие места» — слои с фильтром 1×1, уменьшающие глубину изображения. Благодаря им удалось достичь того, чтобы количество каналов на входе и на выходе либо не менялось, либо менялось только в моменты, когда это необходимо.\n",
    "\n",
    "Как видно на изображении выше, для сверток с неединичным фильтром перед ними добавляется свертка размером 1×1, а max pooling ненастоящий, его шаг 1, он не меняет пространственные размеры, а просто выбирает свертку с максимальным значением из поля 3×3.\n",
    "Такой слой работает более эффективно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of74SOSB-EEi"
   },
   "source": [
    "### 1x1 Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_15.png\"  width=\"850\">\n",
    "\n",
    "*Уменьшение глубины изображения с помощью 32 фильтров 1×1*\n",
    "\n",
    "Сверку 1х1 можно сравнить с полносвязанными слоями, когда мы берем столбик из этого тензора и сворачиваем его с некоторым количеством фильтров, получаем на выходе вектор (то есть получаем из одного вектора другой).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKMpC5Xu-EEj"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_16.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Количество параметров уменьшается в два с лишним раза по сравнению с лобовой реализацией. Сеть получается значительно экономичнее.\n",
    "\n",
    "Использование таких модулей и отсутствие полносвязных слоёв делают GoogleNet очень эффективной и достаточно точной сетью. Но это далеко не все нововведения, которые появились в этой модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJcegWEj-EEk"
   },
   "source": [
    "### \"Stem network\"\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_17.png\"  width=\"850\">\n",
    "\n",
    "\n",
    " - в первых слоях быстро уменьшаются пространственные размеры. \n",
    " \n",
    " В составе GoogleNet есть небольшая подсеть — Stem Network. Она состоит из трёх свёрточных слоёв с двумя pooling-слоями и располагается в самом начале архитектуры.\n",
    " \n",
    "На входе располагаются свертки с большим фильтром. Они служат для того, чтобы достаточно быстро и сильно уменьшить пространственные размеры (по сути сжать изображение перед параллельной обработкой), чтобы минимизировать количество элементов в слоях.\n",
    "\n",
    "Отдельно стоит обратить внимание на завершающую часть сети: Global Average Pooling. Несложно заметить, что больше всего параметров появляется в полносвязных слоях, где каждый элемент связан с каждым. Сверточный слой очень экономичный по сравнению с полносвязным. В предыдущих моделях в конце было два полносвязных слоя, в которых находилась большая часть весов.\n",
    "\n",
    "Была придумана достаточно интересная вещь: слой глобального пулинга (при котором берется среднее значение).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuJsEx2t-EEk"
   },
   "source": [
    "### Global Average Pooling\n",
    "\n",
    "Полносвязанные слои замененны на GAP.\n",
    "\n",
    "* Меньше весов\n",
    "* Независимость от размера входа\n",
    "* Регуляризация\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_18.png\"  width=\"850\">\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1312.4400.pdf\">2014 Network In Network</a>\n",
    "\n",
    "https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/\n",
    "\n",
    "Идея в том, что все пространственные размеры, какими бы они не были (например, 6х6), сворачиваются в единицу.\n",
    "Мы накладываем фильтр размером 6х6 и берем среднее значение. То есть делаем пулинг с фильтром такого размера. Это происходит независимо по каждому слою.\n",
    "\n",
    "Таким образом, мы избавляемся от пространственного измерения и получаем на выходе вектор. При этом нам не нужно запоминать кучу параметров, как в полносвязном слое, и таким образом, это выполняет некоторую регуляризацию, которая позволяет, в том числе, бороться с переобучением, потому что мы избавляемся от части менее важных активаций.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtgQnwXqN55x"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "def file2tensor(filename):\n",
    "  img = Image.open(filename)\n",
    "  #print(\"Image size\",img.size)\n",
    "  t = F.to_tensor(img)\n",
    "  t = F.normalize(t, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "  return t\n",
    "\n",
    "class CNNfromHW(nn.Module):\n",
    "\n",
    "    def __init__(self,conv_module = None):\n",
    "        super().__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(3,16,5,padding=2) # 16xHxW\n",
    "        self.pool = nn.MaxPool2d(2,2) # 16 x H/2 x W/2 \n",
    "        self.conv2 = nn.Conv2d(16,32,3,padding=1) # 32 x H/2 x W/2\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1,1)) # Any spatial size -> 32x1x1\n",
    "        self.fc = nn.Linear(32,10)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape\",x.shape)\n",
    "        x = self.conv1(x) # 16xHxW\n",
    "        x = self.pool(x) # 16 x H/2 x W/2 \n",
    "        x = self.conv2(x) # 32 x H/2 x W/2\n",
    "        x = self.activation(x) # Any spatial size -> 32x1x1\n",
    "        x = self.gap(x)\n",
    "        scores = self.fc(x.flatten(1))\n",
    "        print(\"Output shape\",scores.shape)\n",
    "        return scores\n",
    "\n",
    "print(\"CIFAR10 like\")\n",
    "dummy_input = torch.rand(1,3,32,32)\n",
    "net_with_gap = CNNfromHW()\n",
    "out = net_with_gap(dummy_input)\n",
    "\n",
    "\n",
    "print(\"Arbitrary size\")\n",
    "# Different sizes work too!\n",
    "aramdillo_t = file2tensor('imagen/imagen/n02454379_10511_armadillo.jpg')\n",
    "out = net_with_gap(aramdillo_t.unsqueeze(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TXH1Bga-EEk"
   },
   "source": [
    "### Затухание градиента\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_19.png\"  width=\"850\">\n",
    "\n",
    "Помимо основного классификатора на выходе сети, два дополнительных классификатора, встроенных в промежуточные слои. Они понадобились для того, чтобы улучшить обратное распространение градиента, потому что без батч-нормализации в таких глубоких сетях градиент очень быстро затухал, и обучить сеть такого размера было серьезной проблемой.\n",
    "\n",
    "Обучение VGG осуществлялось непростым способом: сначала обучали 7 слоев, затем добавляли туда следующие и обучали это вручную. Без использования батч-нормализации вряд ли получится повторить результат.\n",
    "\n",
    "Google подошел более системно, он добавил дополнительные выходы, которые способствовали тому, чтобы градиент меньше затухал. Благодаря этому удалось решить серьезную на тот момент проблему, которая ограничивала возможность обучения глубоких моделей.\n",
    "Статья про батч-нормализацию появилась как раз в 15ом году, видимо уже после выхода этой модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATU7YtWd-EEq"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_20.png\"  width=\"850\">\n",
    "\n",
    "В 15ом году появились существенно более глубокие модели. Но одной батч-нормализации было недостаточно, потому что появилась возможность благодаря ее использованию тренировать сети с большим количеством слоев (на слайде выше — пример 56-слойной сети Microsoft). Но, как видно из графиков, у нее ошибка и на тренировочном, и на тестовом датасете больше, чем у 20-тислойной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaDINObE-EE3"
   },
   "source": [
    "## ResNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_21.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1512.03385.pdf\">2015 Deep Residual Learning for Image Recognition</a>\n",
    "\n",
    "В 2015 году соревнования выиграла сеть ResNet, произведя настоящую революцию глубины нейросетей. Она состояла из 152 слоёв и снизила процент ошибок до 3,57%. Это сделало её почти в два раза эффективнее GoogleNet.\n",
    "\n",
    "Возникло предположение, что сеть, состоящая из большего количества слоев должна работать как минимум не хуже, чем сеть меньшего размера, потому что в ней есть те же 20 слоев.\n",
    "\n",
    "Что же происходит с нейросетью, когда мы увеличиваем число слоёв? Можно ли, взяв обычную архитектуру вроде VGG, просто складывать всё больше и больше слоёв друг на друга и достигать лучшей точности?\n",
    "\n",
    "Нет, нельзя. Скорее всего, более глубокая нейросеть покажет даже худшие результаты как при обучении, так и при тестировании. И переобучение здесь не при чём, поскольку тогда тренировочная ошибка была бы низкой.\n",
    "\n",
    "Можно провести подобную аналогию: если мы видим изображение какое-то короткое время и нам нужно быстро его описать, мы скорее всего заметим какие-то крупные объекты (сцена: либо это помещение, либо лес, улица, возможно, мы отметим фигуры людей на переднем плане или же автомобили, здания. Если же требуется более подробно описать рассмотреть и описать картинку, мы уже к этому же имеющемуся описанию, добавим детали: во что одеты люди, какого цвета машины, погода и так далее.\n",
    "\n",
    "Возможно, руководствуясь подобными соображениями, был придуман остаточный слой (residual).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpLZoi9-EE3"
   },
   "source": [
    "### Resudial connection\n",
    "\n",
    "* сумма, а не конкатенация\n",
    "* Batch normalization присутствует\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_22.png\"  width=\"850\">\n",
    "\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "Его идея состоит в том, что мы к имеющемуся уже набору признаков добавляем значения некоторых новых: то есть не перезаписываем то, что было на предыдущем слое, а копируем признаки, сохраняем их и через несколько слоев мы суммируем (а не конкатенируем) их с результатами сверток вот на нескольких слоях, которые здесь присутствуют (Residual block).\n",
    "\n",
    "Все это вместе называется остаточным слоем или residual block.\n",
    "\n",
    "Это дало потрясающий эффект: с одной стороны, по этому каналу стал хорошо распространяться градиент без дополнительных хаков, с другой стороны, перестали теряться важные свойства, которые удалось выделить на предыдущих блоках.\n",
    "\n",
    "Если сеть из 20ти слоев способна распознавать с точностью более 90%, то там уже есть большая часть информации, а детали не должны ее затирать. Такая архитектура показала очень хороший результат.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zp6ogrj-EE4"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_23.png\"  width=\"850\">\n",
    "\n",
    "identity =  Тождественное отображение\n",
    "\n",
    "Из таких блоков удалось построить очень глубокую сеть (были эксперименты из 1000 слоев). Для решения конкретной задачи победы на ImageNet хватило 150 слоев (добавление большего количества блоков уже не давало прироста точности для данной задачи).\n",
    "\n",
    "Здесь используются многие идеи, которые присутствовали в предыдущих моделях: вначале изображение резко уменьшается, дальше используются блоки 3х3, как в VGG, далее применяется глобальный пулинг вместо несколько полносвязных слоев. Блоки состоят из конструкций, изображенных выше: две свертки 3Х3 и прибавление результата предыдущего слоя.\n",
    "\n",
    "Также, на каждом слое перед активацией используется батч-нормализация.\n",
    "\n",
    "Периодически, в местах, где используется деление на 2, здесь в два раза уменьшаются пространственные размеры и также в 2 раза увеличивается количество фильтров.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzjItME9-EE4"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_24.png\"  width=\"850\">\n",
    "\n",
    "Если посмотреть на реализацию ImageNet, то вот что происходит с размерами изображения: сначала его уменьшают обычной сверткой с большим фильтром, а потом, в зависимости от глубины сети, идет некоторое количество блоков без уменьшения, затем происходит даунсэмплинг. При этом здесь используется не pulling, а просто свёртка с шагом 2.\n",
    "\n",
    "Таким образом, через некоторый набор этих блоков, вес изображений уменьшается в два раза. На выходе average pooling. Даже такая огромная 152-хслойная сеть с точки зрения потребления ресурсов получилось более эффективной, чем VGG-19.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-VF8yi_-EE5"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_25.png\"  width=\"850\">\n",
    "\n",
    "Это еще было достигнуто также за счет того, что в более глубоких сетях вместо двух блоков 3х3 применялся вот такой более эффективный блок, где сначала происходит свёртка 1х1 и мы уменьшаем количество фильтров. С этим маленьким количеством фильтров делаем свертку 3х3, а потом восстанавливаем количество фильтров до начальных значений, чтобы можно было прибавить к ним вход, иначе у нас не совпадут размерности. Засчет этого достигается эффективность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-KE9j79-EE5"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_26.png\"  width=\"850\">\n",
    "\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html\n",
    "\n",
    "Так выглядит в коде этот базовый блок для сетей со слоями до 50 слоев. То есть  свёртка, батч-нормализация, активация, свертка, батч-нормализация. Если некоторый параметр (даунсемпл) задан, то вызывается несколько слоев для даунсемплинга: то есть это конволюция (свертка) и ReLu с батч-нормализацией. Это реализуется простым прибавлением значения выходов к входам. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnWnwOZd-EE6"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_27.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1901.09321\">2019 Fixup Initialization: Residual Learning Without Normalization</a>\n",
    "\n",
    "https://towardsdatascience.com/understanding-fixup-initialization-6bf08d41b427\n",
    "\n",
    "В дополнение можно заметить, что не так давно вышла статья, где авторы обратили внимание, что правила, которые мы рассматривали для инициализации (что дисперсия входов должна равняться дисперсии выходов) нарушается при таком подходе, потому что мы здесь к выходам добавляем некоторые значения. Авторы  предлагают решить эту проблему довольно странным на первый взгляд способом - обнулить веса 2ого слоя в блоке при инициализации. На первый взгляд это звучит как ошибка. Но  поскольку значение cо входа будут сложенны с выходами блока, то при обратном распространении градиент не будет нулевым.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzB6_pEY-EE6"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_28.png\"  width=\"850\">\n",
    "\n",
    "Обучение ResNet.\n",
    "\n",
    "Когда точность выходила на плато, шаг обучения понижали вручную. Это давало хороший эффект: ошибка падала.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rrm3AwC-EE7"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_29.png\"  width=\"850\">\n",
    "\n",
    "Помимо того, что эта модель c огромным отрывом выиграла ImageNet от моделей прошлого года и, как уже видно по некоторым исследованиям, ее результаты лучше, чем у человека, решения на базе этой архитектуры также стали победителями на соревнованиях по детектированию и сегментации. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd7UxT2M-EE8"
   },
   "source": [
    "## Feature extraction\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_30.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Дело в том, что в процессе выполнения задачи классификации картинок у нас появляются веса, которые отвечают за распознавание разных особенностей, разных паттернов на изображениях. Эти веса в сверточных слоях представляют большую ценность, потому что мы можем их использовать в данном случае для классификации.\n",
    "\n",
    "\n",
    "та же картинка + Х\n",
    "\n",
    "\n",
    "Но для решения другой задачи мы можем часть сети просто отрезать и заменить ее чем-то еще:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIJly3d8-EE8"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_31.png\"  width=\"850\">\n",
    "\n",
    "Например, мы можем на основании карт активаций, которые мы получили (которые уже намного меньше, чем были начальные изображения) проводить детектирование в этих картах, проводить сегментацию, можем генерировать вектора признаков (embedding) для разных задач: для сравнения изображений, для распознавания лиц, трекинга и многого другого.\n",
    "\n",
    "Часть сети (слева) является достаточно универсальной и получить эти веса можно, обучая сеть распознавать изображения для классификации, а потом использовать ещё  каким-то образом, возможно даже еще не придуманным в настоящий момент. \n",
    "\n",
    "Эта часть сети называется backbone (скелет, основа), на которые уже строятся какие-то дополнительные алгоритмы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEFjAevj-EE8"
   },
   "source": [
    "## Обзор моделей\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_32.png\"  width=\"850\">\n",
    "\n",
    "https://arxiv.org/pdf/1810.00736.pdf\n",
    "\n",
    "Данная картинка из статьи 18 года, то есть не последняя, но тем не менее, на них довольно наглядно собрано много информации: здесь есть информация и о размере модели, и о скорости, и о точности. Здесь можно увидеть, что VGG — огромные по объему модели, но по нынешним меркам они обладают средний точностью.\n",
    "\n",
    "Они требуют больших вычислительных ресурсов, поэтому ну сейчас их имеет смысл использовать разве что в учебных целях, а модели на базе ResNet (ResNet-50, ResNet-152)  довольно хороши: в плане точности какого-то большого отрыва от них здесь не видно. Но, тем не менее, есть модели, которые работают несколько лучше. Рассмотрим их кратко, чтобы было понимание того, куда двигалась мысль в этой области.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmkKdv4r-EE8"
   },
   "source": [
    "\n",
    "## DensNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_33.png\"  width=\"850\">\n",
    "\n",
    "Один из вариантов, что можно сделать — это добавить еще дополнительных связей в обход блоков, чтобы градиент тёк ещё лучше.\n",
    "\n",
    "Можно заменить сумму на конкатенацию. Это тоже работает, но надо понимать, что конкатенация увеличивает количество признаков. Видимо, с этим можно бороться засчет сверток 1Х1. Это так называемый DenceNet. С точки зрения ресурсов он, как правило, чуть более требовательный, чем базовый ResNet и немного более точный. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZiU8FmG-EE-"
   },
   "source": [
    "## ResNeXt\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_34.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1611.05431\">2016 Aggregated Residual Transformations for Deep Neural Networks</a>\n",
    "\n",
    "Еще одна идея — это ResNeXt. Эта сеть выиграла ImageNet в следующем году. Здесь налицо заимствование идеи от Inception модуля. Можно обрабатывать не сразу все каналы, а распараллелить обработку и обрабатывать по несколько каналов. \n",
    "\n",
    "Эта идея уже была в AlexNet, но она там присутствовала вынужденно, потому что AlexNet обучали на двух видеокартах параллельно, просто в силу того, что модель не помещалась в память одной видеокарты на тот момент (видеокарты были по 3 Гб) и уже в конце на полносвязных слоях объединяли результаты двух карт, то есть там обработка происходила параллельно вынужденным образом. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5NQkB-h-EE-"
   },
   "source": [
    "### Groupped Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_35.png\"  width=\"850\">\n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215\n",
    "\n",
    "Это не так плохо, потому что это увеличивает скорость, уменьшает число параметров и в некотором смысле работает как регуляризация.\n",
    "\n",
    "На изображении выше на AlexNet видно, что сеть у них разделена на 2 части. \n",
    "\n",
    "Это демонстрация того, как это работает в обычном слое: можно каким-то образом разделить тензор так, что часть каналов начнет обрабатываться одной сверткой, а часть другой, а потом их просто конкатенировать. Глубина фильтров, которые потребуются для этой свертки, будет меньше. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkEudbdP-EE_"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_36.png\"  width=\"850\">\n",
    "\n",
    "Не нужно дожидаться результатов предыдущих операций, можем считать их параллельно. Этот механизм уже заложен в свёртку, которой пользовались в pytorch. Параметр group = 1, по умолчанию. Это означает, что групп нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJ0opALuUAXr"
   },
   "outputs": [],
   "source": [
    "# CPU test\n",
    "from torch import nn\n",
    "import time \n",
    "import torch\n",
    "\n",
    "def time_synchronized():\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    return time.time()\n",
    "\n",
    "dummy_input = torch.rand(8,512,112,112)\n",
    "start = time_synchronized() \n",
    "normal_conv = nn.Conv2d(512,1024,3,groups = 1)\n",
    "out = normal_conv(dummy_input)\n",
    "tm = time_synchronized() -start\n",
    "print(f\"Normal convolution take  {tm} sec.\")\n",
    "\n",
    "start = time_synchronized() \n",
    "groupped_conv = nn.Conv2d(512,1024,3,groups = 64)\n",
    "out = groupped_conv(dummy_input)\n",
    "tm = time_synchronized() -start\n",
    "print(f\"Groupped convolution take  {tm} sec.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQcQYTNe-KHo"
   },
   "outputs": [],
   "source": [
    "# GPU test\n",
    "start = time_synchronized() \n",
    "normal_conv = nn.Conv2d(512,1024,3,groups = 1).cuda()\n",
    "out = normal_conv(dummy_input.cuda())\n",
    "tm = time_synchronized() -start\n",
    "print(f\"Normal convolution take  {tm} sec.\")\n",
    "\n",
    "start = time_synchronized() \n",
    "groupped_conv = nn.Conv2d(512,1024,3,groups = 64).cuda()\n",
    "out = groupped_conv(dummy_input.cuda())\n",
    "tm = time_synchronized() -start\n",
    "print(f\"Groupped convolution take  {tm} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozwzh2f79sxD"
   },
   "source": [
    "Очистим память\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m89-8RWB91ZE"
   },
   "outputs": [],
   "source": [
    "dummy_input = None\n",
    "out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz7ATAEb-EE_"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_37.png\"  width=\"850\">\n",
    "\n",
    "https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\n",
    "\n",
    "Это используется и в inception сетях, но там результаты конкатинируются и в ResNeXt, и в Inception сетях, в (??Xt??), где они суммируются.\n",
    "\n",
    "То есть эта операция групп convolutional оказалась достаточно эффективной. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U6rw6yU-EFA"
   },
   "source": [
    "## WideResNet\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_38.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1605.07146\">2016 Wide Residual Networks</a>\n",
    "\n",
    "dropout внутри блоков\n",
    "\n",
    "Еще одна идея, связанная уже не с распараллеливанием, а с увеличением количества фильтров. Мы можем увеличивать количество фильтров и уменьшить количество слоев. В такого рода моделях первая цифра — это количество слоев, вторая — коэффициент, с которым мы увеличиваем количество наших фильтров. Поскольку эта операция тоже неплохо распараллеливается, 50-тислойная сеть с коэффициентом (??2 или 4??) обгоняет 150тислойную оригинальную ResNet. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56XiFQSX-EFA"
   },
   "source": [
    "## SENet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_39.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1709.01507\">2017 Squeeze-and-Excitation Networks</a>\n",
    "\n",
    "https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8zrhhQZ-EFB"
   },
   "source": [
    "## MobileNet\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_40.png\"  width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPoXjOUW-EFB"
   },
   "source": [
    "### Depthwise separable convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_41.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWIiGgeJ-EFC"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_42.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps9DFALC-EFC"
   },
   "source": [
    "### Shuffled Grouped Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_43.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt2ggPn5-EFD"
   },
   "source": [
    "## Neural Architecture Search\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_44.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sIsfSby-EFD"
   },
   "source": [
    "## Visual Transformers\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_45.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2006.03677\">2020 Visual Transformers: Token-based Image Representation and Processing for Computer Vision</a>\n",
    "\n",
    "\n",
    "old version: https://syncedreview.com/2020/06/12/facebook-and-uc-berkeley-boost-cv-performance-and-lower-compute-cost-with-visual-transformers/\n",
    "\n",
    "MAC (multiply and accumulate operations) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpJ5x_35-EFD"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_46.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv5r9u9f-EFE"
   },
   "source": [
    "Реализация\n",
    "https://github.com/lucidrains/vit-pytorch\n",
    "\n",
    "Разбор:\n",
    "https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYoUixCQsOLq"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEozb7D1pkPd"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],[0.2470, 0.2434, 0.2615])\n",
    "                                            ]) \n",
    "\n",
    "trainset = CIFAR10(root='./CIFAR10', train=True, download=True, transform=transform)\n",
    "testset = CIFAR10(root='./CIFAR10', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset_mini, _ = torch.utils.data.random_split(trainset, [20000, 30000])\n",
    "testset_mini, _ = torch.utils.data.random_split(testset, [2000, 8000])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 1024\n",
    "\n",
    "trainloader = DataLoader(trainset_mini, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset_mini, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSPUZA5esSbq"
   },
   "source": [
    "# Split image into pathches\n",
    "\n",
    "В примере ниже это происходит внутри модели. Данный код приведен в качестве иллюстрации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_ppSvofsXuf"
   },
   "outputs": [],
   "source": [
    "from torchvision import models,utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "im = testset[0][0]\n",
    "patches = []\n",
    "sz = 8\n",
    "for r in range(0,im.shape[1] , sz):\n",
    "    for c in range(0,im.shape[2] , sz):\n",
    "        patches.append(im[:,r:r+sz,c:c+sz])\n",
    "\n",
    "patches = torch.stack(patches).type(torch.float)\n",
    "\n",
    "img_grid = utils.make_grid(patches,pad_value=10,normalize=True,nrow=4)\n",
    "plt.imshow(np.transpose(img_grid , (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AythVzzXEmlf"
   },
   "source": [
    "Создание transformer - слоя в Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQ_gTbhDUKxD"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZUWCZ8SAsqk"
   },
   "source": [
    "# Install Visual transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqU26kEM9pCv"
   },
   "outputs": [],
   "source": [
    "!pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRkcF71AynMx"
   },
   "outputs": [],
   "source": [
    "# Validation function. Don't change this code\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def validate(model,testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNCdpyUF9y7P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from vit_pytorch import ViT\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "model  = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 10,\n",
    "    dim = 256,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 1024,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "reinit_tensorboard()\n",
    "def train(model,loss_function,optimizer , epochs = 50, tag = \"cifar10\"):\n",
    "  \n",
    "  writer = SummaryWriter(comment = tag)\n",
    "  #writer.add_graph(model,torch.rand(1,3,32,32))\n",
    "  for epoch in range(epochs):\n",
    "    hist_loss = 0\n",
    "    correct = 0\n",
    "    model.to(device)\n",
    "    for images, labels in trainloader: # get bacth         \n",
    "        optimizer.zero_grad() # sets the gradients of all optimized tensors to zero.\n",
    "        outputs = model(images.to(device)) # call forward inside \n",
    "        loss = loss_function(outputs, labels.to(device)) # calculate loss\n",
    "        loss.backward() # calculate gradients\n",
    "        optimizer.step() # performs a single optimization step (parameter update).\n",
    "        # stat collection\n",
    "        _, predicted = torch.max(outputs.data, 1)  \n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "        hist_loss += loss.item()\n",
    "   \n",
    "    accuracy_v  = validate(model.cpu(), testloader)\n",
    "    print(accuracy_v)\n",
    "   \n",
    "    writer.add_scalars(\"Accuracy\", { \n",
    "          'Val': accuracy_v,\n",
    "          'Train' : correct / len(trainloader.dataset)\n",
    "      }, epoch)\n",
    "\n",
    "    writer.add_scalars(\"Loss\", { \n",
    "          \n",
    "          'Train': hist_loss /  len(trainloader),\n",
    "      }, epoch)\n",
    "    writer.close()\n",
    "\n",
    "  return accuracy_v\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
    "#reinit_tensorboard(clear_log =False)\n",
    "train(model, nn.CrossEntropyLoss(), optimizer, epochs =10, tag = 'ViT' )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "L09_CNN_Architectures.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
