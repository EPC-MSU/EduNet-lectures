{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsmL4eVM-EEB"
   },
   "source": [
    "# Архитектуры CNN\n",
    "\n",
    "Мы рассмотрели базовые компоненты из которых состоят современные сверточные нейронные сети, а так же техники их обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0bNOuQr-EEN"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_1.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuvAbIqq_KQC"
   },
   "source": [
    "На этом занятии рассмортим какие именно модели можно построить на основе этих компонент. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg0zzVpq-EEP"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_2.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqIioZCI_yaT"
   },
   "source": [
    "Как упомяналось на первой лекции современный бум нейростевых технологий начался в 2012 году когда AlexNet большим отрывом от конкурентов победила в ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhYmlwCm-EER"
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "\n",
    "создатели: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n",
    "**University of Toronto**\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_3.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Особенности\n",
    "- Большие фильтры на первых слоях\n",
    "- Обучалась на двух видеокартах  GTX580 ( ... )\n",
    "- Уменьшение размерности\n",
    "- 2 Полносвязанных слоя в конце\n",
    "\n",
    "https://neurohive.io/ru/vidy-nejrosetej/alexnet-svjortochnaja-nejronnaja-set-dlja-raspoznavanija-izobrazhenij/\n",
    "\n",
    "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "ImageNet Classification with Deep ConvolutionalNeural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R5-zshX-EES"
   },
   "source": [
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_4.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cdmvHwx-EEU"
   },
   "source": [
    "## ZFnet\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_5.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Тюнингованный AlexNet\n",
    "\n",
    "Matthew D. Zeilerzeiler@cs.nyu.eduDept. of Computer Science, Courant Institute, Rob Fergusfergus@cs.nyu.eduDept. of Computer Science, **Courant Institute, New York Universit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAVT-30n-EEW"
   },
   "source": [
    "##VGGNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_6.png\"  width=\"850\">\n",
    "\n",
    "Karen Simonyan and Andrew Zisserman\n",
    "\n",
    "Visual Geometry Group - **Oxford**\n",
    "\n",
    "https://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "\n",
    "* Появление \"стандартных\" блоков внутри модели\n",
    "* Свертки 3x3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSLsJjsrCkkh"
   },
   "source": [
    " Что бы понять в чем смысл этих усовершенствований, нужно оценить количество памяти для хранения весов и количество операций которые требуется выполнить при прямом прохождении сигнала через сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oe0Sq__-EEW"
   },
   "source": [
    "### Ресурсы\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_7.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD7iiCXvDmmj"
   },
   "source": [
    "Пример оценки (на AlexNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6gGgvlI-EEY"
   },
   "source": [
    "### Размер рецептивного поля\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_8.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHkWL8fL-EEZ"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_9.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb1lBBrb-EEb"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_10.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLL66UuZ-EEf"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_11.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "* Основная часть памяти расходуется на большие свертки в начальных слоях, где пространственные размеры(ширина и высота) велики\n",
    "\n",
    "* Больше всего весов в полносвязанных слоях\n",
    "\n",
    "* Вычислительные ресурсы нужны в первую очередь для сверток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAiQb0BJ-EEg"
   },
   "source": [
    "## GooleNet\n",
    "В отличие от предшествующих моделей разработанна в коммерческой компании, с целью реального применения. \n",
    "\n",
    "Поэтому оснвной упор бул сделан на эффективности.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_12.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjRODyby-EEg"
   },
   "source": [
    " ### Inception module\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_13.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55pUVeNT-EEh"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_14.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of74SOSB-EEi"
   },
   "source": [
    "### 1x1 Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_15.png\"  width=\"850\">\n",
    "\n",
    "Сверка 1х1 может рассматриваться как альтернатива полносвязанному слою.\n",
    "\n",
    "Alternatively, interpret it as applying the same FC layer on each input pixel preserves spatial dimensions, reduces depth!\n",
    "Projects depth to lower dimension (combination of feature maps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKMpC5Xu-EEj"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_16.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJcegWEj-EEk"
   },
   "source": [
    "### \"Stem network\"\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_17.png\"  width=\"850\">\n",
    "\n",
    "\n",
    " - в первых слоях быстро уменьшаются пространственные размеры. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuJsEx2t-EEk"
   },
   "source": [
    "### Global Average Pooling\n",
    "\n",
    "Полносвязанные слои замененны на GAP.\n",
    "\n",
    "* Меньше весов\n",
    "* Не зависимость от размера входа\n",
    "* Регуляризация\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_18.png\"  width=\"850\">\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1312.4400.pdf\">2014 Network In Network</a>\n",
    "\n",
    "https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TXH1Bga-EEk"
   },
   "source": [
    "### Затухание градиента\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_19.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATU7YtWd-EEq"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_20.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaDINObE-EE3"
   },
   "source": [
    "## ResNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_21.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1512.03385.pdf\">2015 Deep Residual Learning for Image Recognition</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpLZoi9-EE3"
   },
   "source": [
    "Resudial connection\n",
    "\n",
    "* сумма а не конкатенация\n",
    "* Batch normalization присутствует\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_22.png\"  width=\"850\">\n",
    "\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zp6ogrj-EE4"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_23.png\"  width=\"850\">\n",
    "\n",
    "identity =  Тождественное отображение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzjItME9-EE4"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_24.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-VF8yi_-EE5"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_25.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-KE9j79-EE5"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_26.png\"  width=\"850\">\n",
    "\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnWnwOZd-EE6"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_27.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1901.09321\">2019 Fixup Initialization: Residual Learning Without Normalization</a>\n",
    "\n",
    "https://towardsdatascience.com/understanding-fixup-initialization-6bf08d41b427"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzB6_pEY-EE6"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_28.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rrm3AwC-EE7"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_29.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd7UxT2M-EE8"
   },
   "source": [
    "## Feature extraction\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_30.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIJly3d8-EE8"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_31.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEFjAevj-EE8"
   },
   "source": [
    "## Обзор моделей\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_32.png\"  width=\"850\">\n",
    "\n",
    "https://arxiv.org/pdf/1810.00736.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmkKdv4r-EE8"
   },
   "source": [
    "\n",
    "## DensNet\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_33.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZiU8FmG-EE-"
   },
   "source": [
    "## ResNeXt\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_34.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1611.05431\">2016 Aggregated Residual Transformations for Deep Neural Networks</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5NQkB-h-EE-"
   },
   "source": [
    "### Groupped Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_35.png\"  width=\"850\">\n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkEudbdP-EE_"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_36.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz7ATAEb-EE_"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_37.png\"  width=\"850\">\n",
    "\n",
    "https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U6rw6yU-EFA"
   },
   "source": [
    "## WideResNet\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_38.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1605.07146\">2016 Wide Residual Networks</a>\n",
    "\n",
    "dropout внутри блоков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56XiFQSX-EFA"
   },
   "source": [
    "## SENet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_39.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1709.01507\">2017 Squeeze-and-Excitation Networks</a>\n",
    "\n",
    "https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8zrhhQZ-EFB"
   },
   "source": [
    "MobileNet\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_40.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPoXjOUW-EFB"
   },
   "source": [
    "### Depthwise separable convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_41.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWIiGgeJ-EFC"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_42.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps9DFALC-EFC"
   },
   "source": [
    "### Shuffled Grouped Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_43.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt2ggPn5-EFD"
   },
   "source": [
    "## Neural Architecture Search\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_44.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sIsfSby-EFD"
   },
   "source": [
    "## Visual Transformers\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_45.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2006.03677\">2020 Visual Transformers: Token-based Image Representation and Processing for Computer Vision</a>\n",
    "\n",
    "\n",
    "old version: https://syncedreview.com/2020/06/12/facebook-and-uc-berkeley-boost-cv-performance-and-lower-compute-cost-with-visual-transformers/\n",
    "\n",
    "MAC (multiply and accumulate operations) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpJ5x_35-EFD"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_46.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv5r9u9f-EFE"
   },
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "colab": {
   "name": "L09_CNN Architectures.ipynb",
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
