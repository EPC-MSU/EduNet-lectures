{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsmL4eVM-EEB"
   },
   "source": [
    "# Архитектуры CNN\n",
    "\n",
    "Мы рассмотрели базовые компоненты из которых состоят современные сверточные нейронные сети, а так же техники их обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0bNOuQr-EEN"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_1.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuvAbIqq_KQC"
   },
   "source": [
    "На этом занятии рассмортим какие именно модели можно построить на основе этих компонент. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg0zzVpq-EEP"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_2.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqIioZCI_yaT"
   },
   "source": [
    "Как упомяналось на первой лекции современный бум нейростевых технологий начался в 2012 году когда AlexNet большим отрывом от конкурентов победила в ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhYmlwCm-EER"
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "\n",
    "создатели: Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n",
    "**University of Toronto**\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_3.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Особенности\n",
    "- Большие фильтры на первых слоях\n",
    "- Обучалась на двух видеокартах  GTX580 ( ... )\n",
    "- Уменьшение размерности\n",
    "- 2 Полносвязанных слоя в конце\n",
    "\n",
    "https://neurohive.io/ru/vidy-nejrosetej/alexnet-svjortochnaja-nejronnaja-set-dlja-raspoznavanija-izobrazhenij/\n",
    "\n",
    "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "ImageNet Classification with Deep ConvolutionalNeural Networks\n",
    "\n",
    "\n",
    "\n",
    "В AlexNet есть все компоненты, которые мы рассматривали ранее. Её архитектура состоит из пяти свёрточных слоёв, между которыми располагаются pooling-слои и слои нормализации, а завершают нейросеть три полносвязных слоя.\n",
    "\n",
    "\n",
    "На схеме архитектуры все выходные изображения делятся на два одинаковых участка — это связано с тем, что нейросеть обучалась на старых GPU GTX580, у которых было всего 3 ГБ видеопамяти. Для обработки использовались две видеокарты, чтобы параллельно выполнять операции над двумя частями изображения.\n",
    "Изначально на вход подаётся фотография размером 227×227×3, и размер свёрточных фильтров первого слоя — 11×11. Всего применяется 96 фильтров с шагом 4.\n",
    "\n",
    "Пространственные размеры изображения сначала довольно сильно сжимаются, затем постепенно увеличивается число фильтров. В результате ширина и высота этого уже не изображения, а набора активаций признаков, сильно уменьшаются, после чего оно поступает на два полносвязных своя, где количество весов уже довольно большое.\n",
    "\n",
    "\n",
    "AlexNet не получится использовать для классификации cifar10, потому что если начать так агрессивно уменьшать изображение размером 32Х32 px, то в определенный момент в него просто не поместится следующий фильтр, который нужно применить, и изображение просто исчезнет.\n",
    "\n",
    "Структура некоторых (особенно старых) сетей, заточенных под ImageNet, напрямую зависит от размера изображений: если соотношение сторон позволит фильтрам поместиться, проблем не возникнет,  если же оно меньше и в какой-то момент размер уменьшится до 2Х2 или 1Х1, то фильтр 3Х3 просто не сработает. В современных сетях есть слой, который позволяет решить эту проблему.\n",
    "\n",
    "\n",
    "\n",
    "На тот момент такая архитектура показала прорывную точность (ошибка упала с 20% до 15.4%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R5-zshX-EES"
   },
   "source": [
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_4_1.png\"  width=\"750\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_4_2.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Также стоит упомянуть о нескольких важных параметрах нейросети, необычных на тот момент:\n",
    "\n",
    "\n",
    "* AlexNet — практически первая архитектура, в которой применяется нелинейность ReLU. Ранее использовались сигмоидальные функции, которые работали медленнее;\n",
    "\n",
    "* Используется собственная нормализация (не столь универсальная, как Batch-нормализацию) с отдельными слоями. Как выяснилось позднее, они не дают значительного улучшающего эффекта и поэтому не распространены в современных архитектурах;\n",
    "\n",
    "* На этапе предварительной обработки используется очень большое дополнение данных (аугментация);\n",
    "\n",
    "* Dropout 0.5 (то есть при регуляризации отсеивается половина нейронов);\n",
    "\n",
    "* Размер пакета — 128;\n",
    "\n",
    "* SGD Momentum 0.9 (как показывают сегодняшние эксперименты, это не плохой вариант, но чтобы обучение сходилось, им требовалась с помощью эвристики ниже периодически обучать Learning rate);\n",
    "\n",
    "* Скорость обучения — 1e−2, снижается в 10 раз вручную, если точность в какой-то момент перестаёт расти;\n",
    "\n",
    "* Встречаются следующие рекомендации — изначально обучить сеть <font color=red >N(?)</font>, а затем добиться дополнительной точности в 2%, то можно вернуться к SGD Momentum и аккуратно поработать с ним с маленьким шагом;\n",
    "\n",
    "* Затухание весов L2 — 5e−4;\n",
    "\n",
    "* В архитектуре используется ансамбль из 7 CNN — это позволило снизить процент ошибок с 18,2% до 15,4%. (Ансамбль моделей — это когда обучается несколько моделей, а результат считается по среднему значению. Здесь используются 7 сетей, результат усредняется. Таким образом достаточно сильно снижается ошибка).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cdmvHwx-EEU"
   },
   "source": [
    "## ZFnet\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_5.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "Тюнингованный AlexNet\n",
    "\n",
    "Matthew D. Zeilerzeiler@cs.nyu.eduDept. of Computer Science, Courant Institute, Rob Fergusfergus@cs.nyu.eduDept. of Computer Science, **Courant Institute, New York Universit**\n",
    "\n",
    "\n",
    "В 2013 году выиграла соревнования созданная учеными из Йорского университета нейросеть ZFnet, достигнув результата 11.7% — в ней AlexNet использовалась в качестве основы, но с изменёнными параметрами и слоями. \n",
    "\n",
    "Отличия от AlexNet небольшие: \n",
    "* Немного поменялись размеры фильтров (было 11, стало 7); \n",
    "* Увеличилось общее количество фильтров;\n",
    "\n",
    "В целом и количество слоев, и общая структура сети, когда слои свертки и пулинга перемежаются друг с другом, а затем идут два полносвязных слоя, сохранились.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAVT-30n-EEW"
   },
   "source": [
    "## VGGNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_6.png\"  width=\"850\">\n",
    "\n",
    "Karen Simonyan and Andrew Zisserman\n",
    "\n",
    "Visual Geometry Group - **Oxford**\n",
    "\n",
    "https://www.robots.ox.ac.uk/~vgg/research/very_deep/\n",
    "\n",
    "\n",
    "* Появление \"стандартных\" блоков внутри модели\n",
    "* Свертки 3x3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 2014-ом году в Оксфорде была разработана модель VGGNet. Сеть получилась более точной и более глубокой.\n",
    "\n",
    "На слайде выше изображены сети AlexNet и две версии VGG-16 и VGG-19 с 16 и 19 слоями соответственно. На соревнованиях победила более глубокая VGG19, достигнув более чем в два раза лучшего результата по сравнению с AlexNet.\n",
    "\n",
    "Особенности: все сверточные слои имеют фильтры с активным полем размера 3Х3, они объединены в блоки, состоящие из некоторого количества сверток с разным (постепенно увеличивающимся) количеством фильтров. Затем идут слои пулинга. Идея базового блока внутри сети будет достаточно широко использоваться дальше. \n",
    "\n",
    "В 2014 году VGGNet достигла более чем в два раза лучшего результата по сравнению с AlexNet. \n",
    "\n",
    "С маленькими фильтрами мы получим не так много параметров, но при этом сможем гораздо эффективнее обрабатывать их.\n",
    "\n",
    "Попробуем посчитать размер выходных изображений и число параметров точно так же, как мы это делали для AlexNet. Если вычислить объём памяти, необходимый для обработки одного изображения, то он будет равен примерно 100 МБ (только для прямого прохода). Это означает, что видеокарта с 5 ГБ памяти сможет обрабатывать всего 50 фотографий за раз.\n",
    "\n",
    "\n",
    "Несколько фактов об архитектуре VGGNet:\n",
    "\n",
    "Нейросеть заняла 2 место в задаче классификации и 1 место в локализации на соревновании ImageNet (при локализации необходимо не только классифицировать объект, но и обвести его в ограничивающие рамки).\n",
    "Процедура обучения такая же, как у AlexNet.\n",
    "Слои нормализации отсутствуют.\n",
    "В прикладных задачах обычно используются архитектуры VGG16 или VGG19 (VGG19 работает лучше, но расходует больше памяти)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSLsJjsrCkkh"
   },
   "source": [
    "Чтобы понять, в чем смысл этих усовершенствований, нужно оценить количество памяти для хранения весов и количество операций, которые требуется выполнить при прямом прохождении сигнала через сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oe0Sq__-EEW"
   },
   "source": [
    "### Ресурсы\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_7.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD7iiCXvDmmj"
   },
   "source": [
    "Пример оценки (на AlexNet)\n",
    "\n",
    "Большее количество слоев требует больше памяти, весов и вычислительных мощностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6gGgvlI-EEY"
   },
   "source": [
    "### Размер рецептивного поля\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_8.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHkWL8fL-EEZ"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_9.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bb1lBBrb-EEb"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_10.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLL66UuZ-EEf"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_11.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "* Основная часть памяти расходуется на большие свертки в начальных слоях, где пространственные размеры(ширина и высота) велики\n",
    "\n",
    "* Больше всего весов в полносвязанных слоях\n",
    "\n",
    "* Вычислительные ресурсы нужны в первую очередь для сверток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAiQb0BJ-EEg"
   },
   "source": [
    "## GooleNet\n",
    "В отличие от предшествующих моделей разработанна в коммерческой компании, с целью реального применения. \n",
    "\n",
    "Поэтому оснвной упор бул сделан на эффективности.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_12.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogleNet — ещё более глубокая архитектура с 22 слоями. Целью Google было разработать нейросеть с наибольшей вычислительной эффективностью. Для этого они придумали так называемый модуль Inception — вся архитектура состоит из множества таких модулей, следующих друг за другом.\n",
    "\n",
    "Также в GoogleNet нет полносвязных слоёв, и она содержит всего 5 миллионов параметров — в 12 раз меньше, чем у AlexNet.\n",
    "\n",
    "В составе GoogleNet есть небольшая подсеть — Stem Network. Она состоит из трёх свёрточных слоёв с двумя pooling-слоями и располагается в самом начале архитектуры.\n",
    "\n",
    "На схеме нейросети можно увидеть небольшие промежуточные «отростки» — это вспомогательные классификационные выходы для введения дополнительного градиента на начальных слоях.\n",
    "\n",
    "Идея основного модуля Inception заключается в том, что он сам по себе является небольшой локальной сетью. Вся его работа состоит в параллельном применении нескольких фильтров на исходное изображение. Данные фильтров объединяются, и создаётся выходной сигнал, который переходит на следующий слой.\n",
    "\n",
    "Но здесь же кроется проблема. Рассмотрим пример: возьмём упрощённую версию модуля Inception и попробуем посчитать размеры выходных изображений после применения каждого фильтра. При исходной фотографии 28×28×256 получим следующее:\n",
    "\n",
    "(залить отдельное изображение)\n",
    "\n",
    "\n",
    "В результате вычислений мы обнаружили, что наш небольшой блок выполняет 854 миллиона операций! При этом pooling-слои не уменьшают размер объекта — это означает, что после каждого объединения фильтров он будет только расти, и вычисления станут очень дорогостоящими.\n",
    "\n",
    "Чтобы этого избежать, введены так называемые «узкие места» — слои с фильтром 1×1, уменьшающие глубину изображения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjRODyby-EEg"
   },
   "source": [
    " ### Inception module\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_13.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55pUVeNT-EEh"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_14.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Of74SOSB-EEi"
   },
   "source": [
    "### 1x1 Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_15.png\"  width=\"850\">\n",
    "\n",
    "Сверка 1х1 может рассматриваться как альтернатива полносвязанному слою.\n",
    "\n",
    "Alternatively, interpret it as applying the same FC layer on each input pixel preserves spatial dimensions, reduces depth!\n",
    "Projects depth to lower dimension (combination of feature maps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKMpC5Xu-EEj"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_16.png\"  width=\"850\">\n",
    "\n",
    "Использование таких модулей и отсутствие полносвязных слоёв делают GoogleNet очень эффективной и достаточно точной сетью. Но на этом развитие свёрточных архитектур не остановилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJcegWEj-EEk"
   },
   "source": [
    "### \"Stem network\"\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_17.png\"  width=\"850\">\n",
    "\n",
    "\n",
    " - в первых слоях быстро уменьшаются пространственные размеры. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuJsEx2t-EEk"
   },
   "source": [
    "### Global Average Pooling\n",
    "\n",
    "Полносвязанные слои замененны на GAP.\n",
    "\n",
    "* Меньше весов\n",
    "* Не зависимость от размера входа\n",
    "* Регуляризация\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_18.png\"  width=\"850\">\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1312.4400.pdf\">2014 Network In Network</a>\n",
    "\n",
    "https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TXH1Bga-EEk"
   },
   "source": [
    "### Затухание градиента\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_19.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATU7YtWd-EEq"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_20.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaDINObE-EE3"
   },
   "source": [
    "## ResNet\n",
    "\n",
    "В 2015 году ResNet произвела настоящую революцию глубины нейросетей. Она состояла из 152 слоёв и снизила процент ошибок до 3,57% в соревновании классификации ImageNet. Это сделало её почти в два раза эффективнее GoogleNet.\n",
    "\n",
    "Что же происходит с нейросетью, когда мы увеличиваем число слоёв? Можно ли, взяв обычную архитектуру вроде VGG, просто складывать всё больше и больше слоёв друг на друга и достигать лучшей точности? \n",
    "\n",
    "Нет, нельзя. Скорее всего, более глубокая нейросеть покажет даже худшие результаты как при обучении, так и при тестировании. И переобучение здесь не при чём, поскольку тогда тренировочная ошибка была бы низкой.\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_21.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1512.03385.pdf\">2015 Deep Residual Learning for Image Recognition</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpLZoi9-EE3"
   },
   "source": [
    "Resudial connection\n",
    "\n",
    "* сумма а не конкатенация\n",
    "* Batch normalization присутствует\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_22.png\"  width=\"850\">\n",
    "\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zp6ogrj-EE4"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_23.png\"  width=\"850\">\n",
    "\n",
    "identity =  Тождественное отображение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzjItME9-EE4"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_24.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-VF8yi_-EE5"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_25.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-KE9j79-EE5"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_26.png\"  width=\"850\">\n",
    "\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/models/resnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnWnwOZd-EE6"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_27.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1901.09321\">2019 Fixup Initialization: Residual Learning Without Normalization</a>\n",
    "\n",
    "https://towardsdatascience.com/understanding-fixup-initialization-6bf08d41b427"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzB6_pEY-EE6"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_28.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rrm3AwC-EE7"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_29.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd7UxT2M-EE8"
   },
   "source": [
    "## Feature extraction\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_30.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIJly3d8-EE8"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_31.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEFjAevj-EE8"
   },
   "source": [
    "## Обзор моделей\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_32.png\"  width=\"850\">\n",
    "\n",
    "https://arxiv.org/pdf/1810.00736.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmkKdv4r-EE8"
   },
   "source": [
    "\n",
    "## DensNet\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_33.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZiU8FmG-EE-"
   },
   "source": [
    "## ResNeXt\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_34.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1611.05431\">2016 Aggregated Residual Transformations for Deep Neural Networks</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5NQkB-h-EE-"
   },
   "source": [
    "### Groupped Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_35.png\"  width=\"850\">\n",
    "\n",
    "https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkEudbdP-EE_"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_36.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz7ATAEb-EE_"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_37.png\"  width=\"850\">\n",
    "\n",
    "https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U6rw6yU-EFA"
   },
   "source": [
    "## WideResNet\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_38.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1605.07146\">2016 Wide Residual Networks</a>\n",
    "\n",
    "dropout внутри блоков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56XiFQSX-EFA"
   },
   "source": [
    "## SENet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_39.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1709.01507\">2017 Squeeze-and-Excitation Networks</a>\n",
    "\n",
    "https://towardsdatascience.com/squeeze-and-excitation-networks-9ef5e71eacd7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8zrhhQZ-EFB"
   },
   "source": [
    "MobileNet\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_40.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPoXjOUW-EFB"
   },
   "source": [
    "### Depthwise separable convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_41.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWIiGgeJ-EFC"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_42.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps9DFALC-EFC"
   },
   "source": [
    "### Shuffled Grouped Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_43.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt2ggPn5-EFD"
   },
   "source": [
    "## Neural Architecture Search\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_44.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sIsfSby-EFD"
   },
   "source": [
    "## Visual Transformers\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_45.png\"  width=\"850\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2006.03677\">2020 Visual Transformers: Token-based Image Representation and Processing for Computer Vision</a>\n",
    "\n",
    "\n",
    "old version: https://syncedreview.com/2020/06/12/facebook-and-uc-berkeley-boost-cv-performance-and-lower-compute-cost-with-visual-transformers/\n",
    "\n",
    "MAC (multiply and accumulate operations) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpJ5x_35-EFD"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_46.png\"  width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv5r9u9f-EFE"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "L09_CNN Architectures.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
