{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Трансформеры</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порождение выходной последовательности по входной находит применение во многих областях машинного обучения: от обработки естественного языка до генерации описаний объектов на фотографиях.\n",
    "До недавнего времени наиболее эффективные seq2seq-модели основались на сложных рекуррентных или сверточных нейронных сетях, беря за основу подход encoder-decoder и механизм внимания. В 2017 году в статье [«Attention Is All You Need»](https://arxiv.org/abs/1706.03762), была предложена новая архитектура, основанная исключительно на механизмах внимания, названная\n",
    "Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классический seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В простейшем варианте модель seq2seq представляет собой две последовательно соединенные рекурентные сети: **Encoder** и **Decoder**. Encoder принимает на вход последовательность векторных представлений токенов и генерирует **hidden state**, который подается на вход Decoder’а. Decoder, в свою очередь, служит для построения целевой последовательности по внутреннему состоянию.\n",
    "\n",
    "На примере задачи перевода: на вход кодировщику подается текст на исходном языке. Тогда hidden state можно интерпритировать как смысл этого текста, по которому затем декодировщик восстанавливает текст на целевом языке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/seq_to_seq_with_rnn.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот было бы здорово так-то так кодировать данные, что сжатые преставления на английском языке были бы близки к сжатым представлениями на русском.\n",
    "\n",
    "А ещё хорошо бы добиться того, чтобы сжатые представления перестали быть равнозначными. Для слова \"мы\" гораздо важнее \"we\", нежели \"bread\".\n",
    "\n",
    "\n",
    "\n",
    "<center><img src =\"https://blog.floydhub.com/content/images/2019/09/Slide36.JPG\" width=\"700\"></center>\n",
    "\n",
    "Кроме того, контекст фиксированного размера $h_N$ — бутылочное горлышко — вектор фиксированного размера $h_N$.\n",
    "\n",
    "**Идея:** поставить нейросетку поверх $h_1 ... h_4$, чтобы вектор в декодер шёл как взвешенная комбинация векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/seq2seq_encoder_decoder_with_attention.png\" width=\"600\"></center>\n",
    "\n",
    "[К.В. Воронцов, Машинное обучение: Обработка последовательностей и модели внимания](http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концепция **attention** состоит в предположении, что между токенами существуют некоторые взаимосвязи. При таком подходе кодировщик передает в декодировщик не одно состояние, кодирующее всю последовательность целиком, а набор состояний всех токенов. Для этого применяется мы можем умножать вектора на матрицы весов. И учить эти веса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN + Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлой лекции мы работали с RNN без внимания. Теперь посмотрим, как это будет со слоем Attention.\n",
    "\n",
    "Материал на основе [официальной документации PyTorch](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам понадобится уникальный индекс для каждого слова, чтобы позже использовать его в качестве входных данных и таргетов. Сделаем вспомогательный класс Lang, из словарей **слово → индекс** (word2index) **и индекс → слово** (index2word), а также счетчик каждого слова word2count, который будет использоваться для замены редких слов позже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все файлы представлены в формате Unicode, поэтому для упрощения мы преобразуем символы Unicode в ASCII, сделаем все строчными и уберём большую часть знаков препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим файл на строки, а строки на пары."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -q https://raw.githubusercontent.com/L1aoXingyu/seq2seq-translation/master/data/eng-rus.txt\n",
    "#!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/eng-rus.txt\n",
    "!wget -O eng-rus.txt -q https://raw.githubusercontent.com/bond005/seq2seq/master/data/eng_rus_for_training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = (\n",
    "        open(\"%s-%s.txt\" % (lang1, lang2), encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    )\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для скорости сократим датасет до предложений не длинее 10 слов, и отфильтруем апострофы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \",\n",
    "    \"i m \",\n",
    "    \"he is\",\n",
    "    \"he s \",\n",
    "    \"she is\",\n",
    "    \"she s \",\n",
    "    \"you are\",\n",
    "    \"you re \",\n",
    "    \"we are\",\n",
    "    \"we re \",\n",
    "    \"they are\",\n",
    "    \"they re \",\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return (\n",
    "        len(p[0].split(\" \")) < MAX_LENGTH\n",
    "        and len(p[1].split(\" \")) < MAX_LENGTH\n",
    "        and p[1].startswith(eng_prefixes)\n",
    "    )\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData(\"eng\", \"rus\", True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодировщик-декодировщик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждом этапе декодирования декодеру предоставляется входной токен и скрытое состояние. Начальный входной токен — токен начала строки <SOS>, первое скрытое состояние — вектор контекста (последнее скрытое состояние кодировщика)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(\n",
    "            batch_size, 1, dtype=torch.long, device=device\n",
    "        ).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(\n",
    "                decoder_input, decoder_hidden\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(\n",
    "                    -1\n",
    "                ).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return (\n",
    "            decoder_outputs,\n",
    "            decoder_hidden,\n",
    "            None,\n",
    "        )  # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слой Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы вычисляем **набор весов Attention**. Они будут умножены на выходные векторы кодера для создания взвешенной комбинации. Результат **`attn_applied`** должен содержать информацию об этой конкретной части входной последовательности и, таким образом, помогать декодеру выбирать правильные выходные слова.\n",
    "\n",
    "Вычисление весов Attention выполняется с помощью линейного слоя **`attn`**, В данных присутствуют предложения всех размеров, для фактического создания и обучения этого слоя нужно выбрать максимальную длину предложения. В предложениях максимальной длины будут использоваться все веса внимания, в то время как в более коротких предложениях будут использоваться только первые несколько.\n",
    "\n",
    "Здесь реализован механизм **аддитивного внимания**, [Bandanau et al.](https://arxiv.org/abs/1409.0473). Этот механизм использует изученную модель выравнивания для вычисления оценок внимания между скрытыми состояниями кодера и декодера и полносвязную сеть для расчета весов Attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large a(h, h') = \\color{red}{w}^Tth(\\color{red}{U}h + \\color{red}{V}h')$ — аддитивное внимание с $\\color{red}{w, U, V}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(\n",
    "            batch_size, 1, dtype=torch.long, device=device\n",
    "        ).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(\n",
    "                    -1\n",
    "                ).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подготовка данных**\n",
    "\n",
    "Для обучения для каждой пары нам понадобится входной тензор (индексы слов во входном предложении) и целевой тензор (индексы слов в целевом предложении). При создании этих векторов мы добавим токен EOS к обеим последовательностям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData(\"eng\", \"rus\", True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, : len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, : len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(\n",
    "        torch.LongTensor(input_ids).to(device), torch.LongTensor(target_ids).to(device)\n",
    "    )\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, sampler=train_sampler, batch_size=batch_size\n",
    "    )\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение модели**\n",
    "\n",
    "Для обучения мы пропускаем предложение через кодировщик и отслеживаем каждый выход и последнее скрытое состояние. Затем декодер получает токен <SOS> в качестве первого входа и последнее скрытое состояние кодировщика в качестве первого скрытого состояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion\n",
    "):\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)), target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (- %s)\" % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Процесс обучения**:\n",
    "\n",
    "* Запустить таймер\n",
    "* Инициализировать оптимизаторы и loss\n",
    "* Создайтnm набор обучающих пар"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_dataloader,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    n_epochs,\n",
    "    learning_rate=0.001,\n",
    "    print_every=100,\n",
    "    plot_every=100,\n",
    "):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(\n",
    "            train_dataloader,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            criterion,\n",
    "        )\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\n",
    "                \"%s (%d %d%%) %.4f\"\n",
    "                % (\n",
    "                    timeSince(start, epoch / n_epochs),\n",
    "                    epoch,\n",
    "                    epoch / n_epochs * 100,\n",
    "                    print_loss_avg,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для построения графиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "%matplotlib inline\n",
    "plt.switch_backend(\"agg\")\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тестирование**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(\n",
    "            encoder_outputs, encoder_hidden\n",
    "        )\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем оценить случайные предложения из обучающего набора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(\">\", pair[0])\n",
    "        print(\"=\", pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = \" \".join(output_words)\n",
    "        print(\"<\", output_sentence)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение и тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap=\"bone\")\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    # prepare number of positions on the axes\n",
    "    x_ticks = []\n",
    "    y_ticks = []\n",
    "    for i in range(0, len(input_sentence.split(\" \")) + 2):\n",
    "        x_ticks.append(i)\n",
    "    for i in range(0, len(output_words) + 1):\n",
    "        y_ticks.append(i)\n",
    "\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels([\"\"] + input_sentence.split(\" \") + [\"<EOS>\"], rotation=90)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_yticklabels([\"\"] + output_words)\n",
    "\n",
    "    # # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder, decoder, input_sentence, input_lang, output_lang\n",
    "    )\n",
    "    print(\"input =\", input_sentence)\n",
    "    print(\"output =\", \" \".join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions[0, : len(output_words), :])\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"Whose beer is this?\")\n",
    "#evaluateAndShowAttention(\"il n est pas aussi grand que son pere\")\n",
    "# evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
    "# evaluateAndShowAttention('je suis desole si c est une question idiote')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычислительная сложность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention решает проблему \"забывания\" при работе с последовательностями. Но цена этого решения — квадратичное возрастание вычислительной сложности с ростом длины последовательности.\n",
    "\n",
    "Вычислительная сложность **одного слоя RNN** составляет **$O(bn d^2)$**, где $b$ — длина батча, $n$ — число токенов и $d$ — размерность входа. Часть $d^2$ обусловлена матричным перемножением внутри блока RNN.\n",
    "\n",
    "Вычислительная сложность **одного слоя attention** в простейшей реализации составляет $O(bn^2 d)$, то есть растет квадратично при росте длины последовательности $n$. Это объясняется тем, что длина выходной последовательности приблизительно равна длине входной последовательности $n$, и необходимо для каждого выходного токена рассчитать коэффициенты attention со всеми входными токенами. Сложность расчета одного коэффициента в простейшем случае составляет $O(d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ни рекуррентные сети, ни attention не могут эффективно работать с очень длинными последовательностями.** RNN/LSTM \"забывают\" начало последовательности, а attention просто не может выполнить расчет за разумное время."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разновидности функций сходства векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large a(h, h') = h^Th'$ — скалярное произведение;\n",
    "\n",
    "$\\large a(h, h') = exp(h^Th')$ — тогда norm превращается в SoftMax;\n",
    "\n",
    "$\\large a(h, h') = h^T\\color{red}{W}h'$ — c матрицей обучаемых параметров $\\color{red}{W}$;\n",
    "\n",
    "$\\large a(h, h') = \\color{red}{w}^Tth(\\color{red}{U}h + \\color{red}{V}h')$ — аддитивное внимание с $\\color{red}{w, U, V}$.\n",
    "\n",
    "$$ \\sum_{i=1}^{N}a_{ti} = 1,$$\n",
    "\n",
    "$$  0\\leqslant a_{ti} \\leqslant 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводя внимание, мы говорили о некоторой **функции сходства** между скрытым состоянием декодировщика $h'$ и скрытым состоянием кодировщика $h$. Обобщением механизма внимания является введение в функцию сходства  **обучаемых параметров**.\n",
    "\n",
    "Какие вообще бывают функции сходства?\n",
    "\n",
    "\n",
    "1.   Первое, что приходит голову — просто считать скалярное произведение $h$ и $h'$.\n",
    "2.   Также можно брать от него экспоненту, тогда оператор нормировки превращается в **SoftMax**.\n",
    "\n",
    "Первые два способа возможны, только если потребовать, чтобы $h$ и $h'$ имели одинаковую размерность.\n",
    "\n",
    "3.   Можно вводить матрицу обучаемых параметров $W$.\n",
    "4.   Можно вводить небольшую двухслойную нейронную сеть с несколькими весовыми матрицами. Такое введение функции сходства называется аддитивным вниманием.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Идея:** $J$ разных моделей внимания совместно обучаются выделять различные аспекты входной информации (например, части речи, синтаксим, фразеологизмы):\n",
    "\n",
    "$\\large c_j = Attn(\\color{red}{W^j_q}q, \\color{red}{W^j_k}H,\\color{red}{W^j_v}H, \\ j = 1, \\dots, j)$\n",
    "\n",
    "**Варианты** агрегирования выходного вектор:\n",
    "\n",
    "$\\large \\displaystyle c = {1 \\over j} \\sum\\limits^J_{j=1}c^j$ — усреднение;\n",
    "\n",
    "$\\large \\displaystyle c = [c^1 \\dots c^J]$ — конкатенация;\n",
    "\n",
    "$\\large \\displaystyle c = [c^1 \\dots c^J]\\color{red}{W}$ — возвращение к нужной размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на то, к каким словам предложения gave может иметь отношение. В общем случае глагол может иметь связку со многими частями предложения. Как, например, с подлежащим, так и с причастиями.\n",
    "\n",
    "В идеале, нам бы хотелось обратить внимание функции (attention) на все эти взаимосвязи. Для этого нам просто надо поставить несколько attention слоев параллельно. Тогда каждый из них будет учить что-нибудь свое по аналогии со сверточными слоями.\n",
    "\n",
    "* Чтобы осуществить задуманное, вместо одного набора query будем использовать несколько независимых наборов.\n",
    "\n",
    "* Причем каждый набор будет считаться уникальной матрицей.\n",
    "\n",
    "* Аналогично сделаем для keys и values. Количество таких наборов внутри keys, queries, values должно быть **одинаковым**.\n",
    "\n",
    "* Обозначим это число как $J$, далее производим аналогичные манипуляции, при этом введем в параллель h таких функций attention.\n",
    "\n",
    "* На последнем шаге мы их соединяем (конкатинируем).\n",
    "\n",
    "* При этом можно заметить, что при таком подходе на каждом шаге размерность токена будет увеличиваться (если, например, в качестве и key, и value, и query мы подаем одно и тоже предсталение токена). Если хотим сохранять управление размерностью токена, то придется получать по меньшей мере value путем домножения на матрицу, размерность которой по второй оси меньше — **выполнять проекцию наших токенов в пространство меньшей размерности**.\n",
    "\n",
    "* В частности, можно подобрать размерность этого пространства таким образом, чтобы при конкатенации размерность полученного токена равнялась исходной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/multihead_self_attention_layer.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё раз посмотрим на получившуюся архитектуру.  **RNN справа нам больше не требуется!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/seq2seq_encoder_decoder_with_attention.png\" width=\"600\"></center>\n",
    "\n",
    "[К.В. Воронцов, Машинное обучение: Обработка последовательностей и модели внимания](http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура сети Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/transformer_architecture.png\" width=\"450\">\n",
    "\n",
    "<em>Архитектура трансформера</em>\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1706.03762.pdf\"> Attention Is All You Need</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention – ключевая часть модели. Он позволяет находить связи между словами в предложении и выбирать нужные для будущей генерации.\n",
    "\n",
    "Self-Attention смотрит на представителей одной и той же сущности: например, всеми состояниями кодера на каком-то уровне.\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/decenc_vs_self-min.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"300\" src=\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/encoder_self_attention.mp4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого токена (слова или подслова) из словаря строится набор ключей K, V, Q, где:\n",
    "\n",
    "* Q (query) – запрашивает информацию;\n",
    "\n",
    "* K (key) – сообщает, что в нем есть какая-то информация;\n",
    "\n",
    "* V (value) – выданная информация.\n",
    "\n",
    "**Query** используется, когда токен смотрит на других — он ищет информацию, чтобы лучше понять себя. **Key** отвечает на запрос запроса: он используется для вычисления **весов внимания**. **Value** используется для вычисления вывода внимания: оно предоставляет информацию токенам, которые «говорят», что им это нужно (т.е. этому токену присвоены большие веса)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src =\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/qkv_explained-min.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center><img src =\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/qkv_attention_formula-min.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key, query, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейные преобразования векторов** **Query**, **Key** и **Value**.\n",
    "\n",
    "Наиболее часто используемым подходом является введение трех типов векторов, которые называют **Query**, **Key** и **Value**.\n",
    "\n",
    "Для каждого типа вектора вводится свое линейное преобразование, которое из исходного вектора делает вектор в каком-то другом пространстве. Все три они\n",
    "обычно приводятся к одной и той же размерности, обозначенной $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large a(h_i, h^\\prime_{t-1}) = (\\color{red}{W_k}h_i)^T(\\color{red}{W_q}h^\\prime_{t-1}) / \\sqrt d$\n",
    "\n",
    "$\\large \\alpha_{ti} = SoftMax_i \\space a(h_i, h^\\prime_{t-1})$\n",
    "\n",
    "$\\large c_t = \\Sigma_i \\alpha_{ti} \\color{red}{W_v} h_i$\n",
    "\n",
    "$ \\large \\color{red}{W_q}_{d \\times dim(h^\\prime)}, \\color{red}{W_k}_{d \\times dim(h)}, \\color{red}{W_v}_{d \\times dim(h)}$ — матрицы весов линейных нейронов (обучаемые линейные преобразования в пространство размерности $\\large d$).\n",
    "\n",
    "Возможно упрощение модели: $\\large \\color{red}{W_k} \\equiv \\color{red}{W_v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/query_key_value.png\" width=\"250\">\n",
    "\n",
    "<em>Source: <a href=\"http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf\">Обработка последовательностей: модели внимания и трансформеры</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Self-Attention Layer\n",
    "\n",
    "\n",
    "Допустим,  у нас стоит проблема, что мы не должны видеть часть слов в предложении — например, при генерации текста (по текущим словам предсказать следующее). Например, хотим сгенерировать фразу \"robot must obey orders\" на основе только первого слова.\n",
    "\n",
    "Если мы подадим в нейронную сеть во время обучения сразу все предложение, которое хотим генерировать, то у нас в первых словах будет \"протекать\" информация о предыдущих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/queries_keys_scores_before_softmax.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае мы можем просто на соответствующих местах матрицы $E$ поставить минус бесконечности. Тогда в эмбеддингах слов, которые не должны знать о каких-то словах, информации об этих словах не будет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/scores_before_softmax_apply_attention_mask_masked_scores_before_softmax.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате после SoftMax \"лишняя\" информация не будет использоваться при генерации ответа на query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/masked_scores_softmax_along_rows_scores.png\" width=\"800\">\n",
    "\n",
    "<em>Source: <a href=\"https://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря этому трюку у нас получается обучать transfomer по-прежнему как простую single-pass нейросеть, а не \"скатываться\" в RNN, где у нас возникнут проблемы с градиентами и временем работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порядок вычислений трансформера-кодировщика:\n",
    "\n",
    "1. Добавляются позиционные векторы $p_i$:\n",
    "\n",
    "$\\qquad \\large h_i = x_i + p_i;$\n",
    "\n",
    "$\\qquad \\large H = (h_1, \\dots, h_n).$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ x_i, \\ p_i, \\ h_i = 512, \\ dim \\ H = 512 \\times n$\n",
    "\n",
    "2. Многомерное самовнимание:\n",
    "\n",
    "$\\qquad \\large h^j_i = Attn(\\color{red}{W^j_q}h_i, \\color{red}{W^j_k}H, \\color{red}{W^j_v}H).$\n",
    "\n",
    "$\\qquad$ Размерность: $j = 1, \\dots, J=8, \\ dim \\ h^j_i = 64, \\ dim \\ W^j_q, \\ W^j_k, \\ W^j_k = 64 \\times 512 $\n",
    "\n",
    "3. Конкатенация:\n",
    "\n",
    "$\\qquad \\large h'_i =  MH_j (h^j_i) \\equiv [h^1_i, \\dots, h^J_i].$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ h'_i = 512$\n",
    "\n",
    "4. Сквозная связка + нормировка уровня:\n",
    "\n",
    "$\\qquad \\large h''_i =  LN(h'_i + h_i; \\color{red}{\\mu_1, \\sigma_1}).$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ h''_i, \\ \\mu_1, \\ \\sigma_1 = 512$\n",
    "\n",
    "5. Полносвязная 2-хслойная сеть FFN:\n",
    "\n",
    "$\\qquad \\large h'''_i = \\color{red}{W_2}ReLU(\\color{red}{W_1}h''_i + \\color{red}{b_1}) + \\color{red}{b_2}.$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ W_1 = 2048\\times512, \\ dim \\ W_2 = 512\\times2048$\n",
    "\n",
    "6. Сквозная связь + нормировка уровня:\n",
    "\n",
    "$\\qquad \\large z_i = LN(h'''_i + h''_i; \\color{red}{\\mu_2, \\sigma_2}).$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ z_i, \\ \\mu_2, \\ \\sigma_2 = 512$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/transformer_encoder.png\" width=\"200\">\n",
    "\n",
    "<em>Архитектура трансформера-кодировщика</em>\n",
    "\n",
    "<em>Source: <a href=\"http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf\"> К.В. Воронцов, Машинное обучение: Обработка последовательностей и модели внимания</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Layer Normalization\n",
    "\n",
    "$\\qquad  \\large x_i, \\ \\color{red}{\\mu}, \\ \\color{red}{\\sigma} \\in \\mathbb{R};$\n",
    "\n",
    "$\\qquad  \\large \\displaystyle LN_s(x; \\color{red}{\\mu}, \\ \\color{red}{\\sigma}) = \\color{red}{\\sigma_s} {{x_s - \\overline x} \\over \\sigma_x} + \\color{red}{\\mu_s}, \\ s = 1, \\dots, d;$\n",
    "\n",
    "$\\qquad \\displaystyle \\overline x = {1 \\over d} \\sum\\limits_{s}x_s$ и $\\displaystyle \\sigma^2_x = {1 \\over d} \\sum\\limits_{s}(x_s - \\overline x)^2$ — среднее и дисперсия $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positional encoding**\n",
    "\n",
    "Единственный возможный минус — нейросеть не учитывает порядок слов в предложении при составлении embedding. Это может нам мешать. Например, если в предложении два it, то они часто относятся к разным словам. Поэтому хотелось бы уметь учитывать информацию о позиции. Для этого к $X$ при составлении $Q$ добавляется информация о позиции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делается это хитрым образом: мы добавляем к каждому значению исходного вектора токенов некую комбинацию $sin$ и $cos$ с разными параметрами. **Значения суммируются, а не конкатенируются.**\n",
    "\n",
    "Вектор $PE$, который мы будем добавлять к $X$, будет определяться по следующей формуле:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{pos, 2i} = \\sin \\left({\\dfrac {pos} {10000^{2i/d}}}\\right)$$\n",
    "\n",
    "$$p_{pos, 2i+1} = \\cos \\left({\\dfrac {pos} {10000^{2i/d}}}\\right)$$\n",
    "\n",
    "$pos$ &mdash; это позиция токена\n",
    "\n",
    "$d$ &mdash; количество размерностей токена\n",
    "\n",
    "$i$ &mdash; $i$-тая размерность токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].detach()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(20)\n",
    "y = pe(\n",
    "    torch.zeros(1, 100, 20)\n",
    ")  # sequence of shape 100, every token of sequence has shape 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.arange(100), y[0, :, 0:4].data.numpy())\n",
    "plt.legend([\"dim %d\" % p for p in [1, 2, 3, 4]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате каждая позиция кодируется уникальным представлением. При этом представление позволяет легко находить слова на заданном расстоянии от исходного (у них будет одинаково значение сигнала по какой-то оси)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это помогает трансформеру достаточно уникальным образом определять каждую позицию и понимать относительное расстояние между разными токенами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Почему не используется одно число, например значение индекса?**\n",
    "\n",
    "Для длинных последовательностей индексы могут сильно увеличиваться по величине. Если вы нормализуете значение индекса так, чтобы оно лежало между $0$ и $1$, это может создать проблемы для последовательностей переменной длины, поскольку они будут нормализованы по-разному.\n",
    "\n",
    "Поэтому используется кодирование позиции в виде вектора, который прибавляется к эмбеддингу токена. Эти позиционные эмбеддинги можно как зафиксировать заранее (так делается в оригинальном трансформере, см. пример на картинке), так и обучать, как в случае GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"https://imageup.ru/img7/4644185/2023-11-30_08-19.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря процедуре маскирования, BERT-подобные модели могут обучаться без учителя. И мы можем их дообучать под наши задачи.\n",
    "\n",
    "Для начала посмотрим на инференс модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers sentencepiece\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "# model.cuda()  # uncomment it if you have a GPU\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embed_bert_cls('Привет мир', model, tokenizer).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или же через альтернативный запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('cointegrated/rubert-tiny2')\n",
    "sentences = [\"привет мир\", \"hello world\", \"здравствуй вселенная\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот так можно задать количество классов, прямо во время импорта модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('sberbank-ai/ruBert-large')\n",
    "model = BertForSequenceClassification.from_pretrained('sberbank-ai/ruBert-large', num_labels=6).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('sberbank-ai/ruBert-large')\n",
    "model = BertForSequenceClassification.from_pretrained('sberbank-ai/ruBert-large').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"stevhliu/my_awesome_model\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass your inputs to the model and return the `logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BertScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERTScore**\n",
    "\n",
    "Одна из самых популярных метрик, [предложенная Zhang et al.](https://arxiv.org/pdf/1904.09675.pdf) в 2019 для оценки качества генерируемого текста. Основана на оценке близости контекстных эмбеддингов, полученных из предобученной нейросетевой модели BERT. Частично **решает проблему синонимов и опечаток** метрик BLEU и ROUGE.\n",
    "\n",
    "Для расчета BERTScore близость двух предложений – сгенерированного моделью и эталонного – оценивается как сумма косинусных подобий между эмбеддингами слов, составляющих эти предложения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/architecture_bertscore.png\" width=\"1000\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://wiki.math.uwaterloo.ca/statwiki/index.php?title=BERTScore:_Evaluating_Text_Generation_with_BERT\">The Illustrated GPT-2 (BERTScore: Evaluating Text Generation with BERT)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе обоих токенов рассчитывается **Recall**, **Precision** и **F1**:\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/bertscore_equations.png\" width=\"1000\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://wiki.math.uwaterloo.ca/statwiki/index.php?title=BERTScore:_Evaluating_Text_Generation_with_BERT\">The Illustrated GPT-2 (BERTScore: Evaluating Text Generation with BERT)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научимся это делать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "\n",
    "def getpreferredencoding(do_setlocale=True):\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "\n",
    "!pip install -q evaluate\n",
    "!pip install -q bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из важных параметров стоит [упомянуть](https://huggingface.co/spaces/evaluate-metric/bertscore) выбор языка и тип модели. По-умолчанию используется `roberta-large`, и, чтобы не загружать 1.4G Гб, мы выставляем более мелкую модель менее чем в 300 Мб."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = bertscore.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    lang=\"en\",\n",
    "    nthreads=-1,\n",
    "    batch_size=128,\n",
    "    model_type=\"distilbert-base-uncased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что результат зависит от того, что мы принимаем за референс. Поэтому нам и выдают пару чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Декодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторегрессионный синтез последовательности:\n",
    "\n",
    "$\\large y_0 = \\langle {BOS} \\rangle$ — эмбеддинг символа начала.\n",
    "\n",
    "Для всех $t = 1, 2, \\dots$ выполняется следующая последовательность вычислений:\n",
    "\n",
    "1. Маскирование \"данных из будущего\":\n",
    "\n",
    "$\\qquad \\large h_t = y_{t-1} + p_t;$\n",
    "\n",
    "$\\qquad \\large H_t = (h_1, \\dots, h_t).$\n",
    "\n",
    "2. Многомерное самовнимание:\n",
    "\n",
    "$\\qquad \\large h'_t = LN \\circ MH_j \\circ Attn(\\color{red}{W^j_q}h_t, \\color{red}{W^j_k}H_t, \\color{red}{W^j_v}H_t).$\n",
    "\n",
    "3. Многомерное внимание на кодировку $Z$:\n",
    "\n",
    "$\\qquad \\large h''_t = LN \\circ MH_j \\circ Attn(\\color{red}{W^j_q}h'_t, \\color{red}{W^j_k}Z, \\color{red}{W^j_v}Z).$\n",
    "\n",
    "4. Двухслойная полносвязная сеть:\n",
    "\n",
    "$\\qquad \\large y_t = LN \\circ FFN(h''_t).$\n",
    "\n",
    "5. Линейный предсказывающий слой:\n",
    "\n",
    "$\\qquad \\large p(\\tilde w | t) SoftMax_{\\tilde w}(\\color{red}{W_y}y_t + b_y).$\n",
    "\n",
    "Генерация $\\tilde w_t = argmax(p(\\tilde w | t))$ продолжается пока $\\tilde w_t \\neq \\langle {EOS} \\rangle$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/transformer_decoder.png\" width=\"350\">\n",
    "\n",
    "<em>Архитектура трансформера-декодировщика</em>\n",
    "\n",
    "<em>Source: <a href=\"http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf\"> К.В. Воронцов, Машинное обучение: Обработка последовательностей и модели внимания</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее часть текста основана на статье [GPT для чайников: от токенизации до файнтюнинга](https://habr.com/ru/articles/599673/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Так что же такое GPT?**\n",
    "* Это нейронная сеть для генерации (продолжения) текста.\n",
    "\n",
    "* Более строго — языковая модель, основанная на архитектуре трансформер и обученная в self-supervised режиме на огромном [корпусе](https://philology.by/about/yaskevich/corpus-linguistics-yaskevich) текстовых данных.\n",
    "\n",
    "**Оригинальные статьи про поколения GPT:**\n",
    "* [Improving Language Understanding by Generative Pre-Training (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "* [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "* [Language Models are Few-Shot Learners (2020)](https://arxiv.org/pdf/2005.14165.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с GPT будем использовать предобученную модель. Лучший выбор для работы с трансформерами — библиотеки от **Hugging Face**: `transformers`, `tokenizers`, `datasets`.\n",
    "\n",
    "Hugging Face занимается стандартизацией применения трансформеров, а также хранит наборы весов и датасеты для различных NLP-задач. Воспользуемся русскоязычной моделью ruGPT3 и дообучим её.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим библиотеку Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем необходимую модель. API для различных моделей одинаковый, для подмены модели достаточно изменить название модели `model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading and initialization of model and tokenizer\n",
    "model_name = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Языковое моделирование** — предсказание следующего слова (или части слова) с учётом предыдущего контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/yandex_search.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/599673/\">GPT для чайников: от токенизации до файнтюнинга</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы сгенерированное моделью продолжение текста было верным не только грамматически, но и семантически, модель должна хорошо понимать смысл изначального текста и, желательно, даже иметь знания о реальном мире.\n",
    "\n",
    "Эти внутренние знания позволяют модели отвечать на вопросы, суммаризировать текст, создавать диалоговые системы и многое другое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, если мы хотим при помощи языковой модели ответить на вопрос: **«Сколько будет 2+2?»**, то можем подать на вход модели следующий текст:\\\n",
    "`«Вопрос: Сколько будет 2+2? Ответ: … »`\\\n",
    "и естественным продолжением такого текста будет ответ на вопрос, поэтому модель допишет `«4»`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Вопрос: 'Сколько будет 2+2?'\\nОтвет:\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False, max_length=20, pad_token_id=20)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похожим способом можно кратко пересказывать тексты, если в конце дописывать `«TL:DR»`, т.к. модель во время обучения запомнила, что после этих символов идёт краткое содержание. Подбор модификаций текста называется **«Prompt Engineering»**. Такая простая идея позволяет решать практически неограниченное количество задач. Именно поэтому многие считают GPT-3 подобием сильного искусственного интеллекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из ключевых этапов в обработке текста — **токенизация**. На этом этапе происходит разделение текста на отдельные единицы — предложения и слова. Затем создается словарь, в который заносятся уникальные лексемы, встретившиеся в корпусе или тексте. На этих этапах можно столкнуться с несколькими проблемами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема 1. Размер словаря**\n",
    "\n",
    "Самый простой способ токенизации — назначить каждому уникальному слову своё число. Но есть проблема: слов и их форм миллионы, и поэтому словарь таких слов получится чересчур большим, а это будет затруднять обучение модели.\n",
    "\n",
    "Можно разбивать текст не на слова, а на отдельные буквы (char-level tokenization), тогда в словаре будет всего несколько десятков токенов, НО в таком случае уже сам текст после токенизации будет слишком длинным, а это тоже затрудняет обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема 2. Богатая морфология**\n",
    "\n",
    "\"Нейросеть\", \"сетка\", \"сеть\" являются разными словами, но имеют схожий смысл. Эту проблему классически всегда решал этап **стемминга** (удаление суффикса, приставки, окончания) или **лемматизации** (приведение слова к канонической форме)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема 3. Сложные слова**\n",
    "\n",
    "Но все проблемы эти этапы не решают. В германских языках (в английском, немецком, шведском и т.д.) очень продуктивно образуются новые сложные слова. Значения таких слов выводятся из значения их элементов. Их можно создавать бесконечно долго, и большинство из них не зафиксировано в «бумажном» словаре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/swedish_word_example.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Пример шведского названия гаечного ключа для колеса мотоцикла</a></em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://sysblok.ru/nlp/7250/\">Как работает алгоритм токенизации текстов для нейросетей</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с этими языками сложность также возникает на этапе составления словаря. При составлении словаря модели ориентируются на частотность (например, сохраняем слово, если оно встретилось чаще пяти раз), поэтому не будут запоминать такое длинное и сложное слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема 4: Границы слова**\n",
    "\n",
    "Для нас, привыкших к языкам европейского типа, слово — это набор букв между пробелами и знаками препинания. Но в английском языке многие сложные слова пишутся раздельно, а в японском, наоборот, между словами вообще нет пробелов. Поэтому универсальный токенизатор создать было нелегко."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение — Byte Pair Encoding**\n",
    "\n",
    "Изначально алгоритм компрессии BPE позволяет моделям узнавать как можно больше слов при ограниченном объеме словаря.\n",
    "\n",
    "1.   Слово = последовательность токенов\n",
    "2.   Словарь = все токены\n",
    "3.   Повторять, пока не достигли ограничения на размер словаря:\n",
    "\n",
    "     Назначаем новым токеном объединение двух существующих токенов, которое\n",
    "встречается чаще других пар в корпусе (встречаются вместе)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В применении BPE возможны разные варианты. Один из естественных – идём по всем токенам по убыванию частоты, находим соответствующую последовательность символов в корпусе, заменяем на токен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/subword_tokenization.png\" width = \"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://alexanderdyakonov.wordpress.com/2019/11/29/токенизация-на-подслова-subword-tokenization/\">Токенизация на подслова (Subword Tokenization)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот же способ помогает решить **проблему** **OOV (out of vocabulary)**. В обучающей выборке может не быть слова *Unfriendly*, но поскольку **Unfriendly** = **Un** + **friend** + **ly**, мы можем рассчитывать, что сеть будет правильно обрабатывать / генерировать и слово целиком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/token_unfriendly.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.thoughtvector.io/blog/subword-tokenization/\">Subword Tokenization — Handling Misspellings and Multilingual Data</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но даже это иногда не самый оптимальный выбор. Чтобы сжать словарь ещё сильнее, для обучения GPT OpenAI использовали **byte-level BPE** токенизацию. Эта модификация BPE работает не с текстом, а напрямую с его байтовым представлением. Использование такого трюка позволило сжать словарь до всего-лишь ~50k токенов при том, что с его помощью всё ещё можно выразить любое слово на любом языке мира (и даже эмодзи)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -q transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading and initialization of model and tokenizer\n",
    "model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример токенизации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Нейронные сети - это очень просто и увлекательно\"\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens: \", tokens)\n",
    "print(\"Decoded tokens: \", decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-level токенизатор **не гарантирует**, что для любого токена найдется **соответствующий** символ или слово. Некоторые **токены** **существуют** только **в комбинациях**. Так, представленные токены не декодируются по отдельности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode([167]))\n",
    "print(tokenizer.decode([245]))\n",
    "print(tokenizer.decode([256]))\n",
    "\n",
    "print(tokenizer.decode([167, 245, 256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При генерации продолжения текста с помощью GPT происходит следующее:\n",
    "\n",
    "1. Входной текст токенизируется в последовательность чисел (токенов).\n",
    "2. Список токенов проходит через Embedding layer (линейный слой) и преобразуется в список эмбеддингов.\n",
    "3. К каждому эмбеддингу прибавляется **positional embedding**.\n",
    "4. Список эмбеддингов проходит через несколько одинаковых блоков (Transformer Decoder Block).\n",
    "5. После того, как список эмбеддингов пройдёт через последний блок, эмбеддинг, соответствующий последнему токену, матрично умножается на всё тот же входной, но уже транспонированный Embedding Layer, и после применения SoftMax получается распределение вероятностей следующего токена.\n",
    "6. Из этого распределения выбирается следующий токен (например, с помощью argmax)\n",
    "7. Полученный токен добавляется к входному списку токенов, шаги 1-6 повторяются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/gpt3.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/how-gpt3-works-visualizations-animations/\">How GPT3 Works — Visualizations and Animations</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от рекуррентных сетей, архитектура трансформера не чувствительна к порядку входных токенов, то есть при перемешевании слов местами выход будет получаться одинаковым (permutation invarience).\n",
    "\n",
    "Позиционное кодирование описывает позицию объекта в последовательности так,  что каждой позиции соответствует уникальное представление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Почему не используется одно число, например значение индекса?**\n",
    "\n",
    "Для длинных последовательностей индексы могут сильно увеличиваться по величине. Если вы нормализуете значение индекса так, чтобы оно лежало между $0$ и $1$, это может создать проблемы для последовательностей переменной длины, поскольку они будут нормализованы по-разному.\n",
    "\n",
    "Поэтому в GPT используется кодирование позиции в виде вектора, который прибавляется к эмбеддингу токена. Эти позиционные эмбеддинги можно как зафиксировать заранее (так делается в оригинальном трансформере, см. пример на картинке), так и обучать, как в случае GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/pos_encoding_visual.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Позиционные эмбеддинги оригинального трансформера</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/599673/\">GPT для чайников: от токенизации до файнтюнинга</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у вас есть входная последовательность длины $L$, и требуется задать положение $k$-того объекта в этой последовательности. Позиционное кодирование задается функциями синуса и косинуса различной частоты:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(k, 2i) = sin (\\frac{k}{2^{2i/d}})$$\n",
    "\n",
    "$$P(k, 2i+1) = cos (\\frac{k}{2^{2i/d}})$$\n",
    "\n",
    "где $k$ — позиция объекта в последовательности, $\\displaystyle 0\\leq k< \\frac{L}{2}$,\n",
    "\n",
    "$d$ — размерность выходного пространства эмбеддингов,\n",
    "\n",
    "$P(k,j)$ — функция, которая переводит позицию $k$ в индекс $(k,j)$ позиционной матрицы,\n",
    "\n",
    "$n$ — константа, обычно равно $10 000$ согласно статье *Attention is all You Need*,\n",
    "\n",
    "$i$ — индекс колонки, $0 \\leq i < d/2$, одинаково для синуса и для косинуса.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной блок GPT состоит из слоёв self-attention, нормализации, feed-forward и residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/decoder_block.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://ai-news.ru/2019/06/obobshennye_yazykovye_modeli.html\">Обобщенные Языковые Модели</a></em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы Генерации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковая модель генерирует распределение вероятностей следующего токена. Однако способы генерации текста могут отличаться. Далее разберём, какие варианты бывают.\n",
    "\n",
    "Для наглядности применим основные методы для продолжения следующего текста  \\\n",
    "`'Определение: \"Нейронная сеть\" — это'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Определение: \"Нейронная сеть\" - это'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидный вариант — ArgMax-генерация (жадный поиск). Выбирается максимально вероятный токен.\n",
    "\n",
    "При таком способе мы не получим разнообразного текста на один и тот же запрос, и, что ещё хуже, генерация может застревать в локальных минимумах и выдавать повторяющиеся фрагменты, например `the the the the ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArgMax is defaulf behaviour\n",
    "out = model.generate(input_ids, do_sample=False, max_length=30, pad_token_id=30)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Несколько более сложный и качественный способ сэмплирования — **beam search**. Каждый раз мы выбираем не один самый вероятный токен, а сразу несколько (`beam-size`), и дальше продолжаем поиск для каждого из выбранных токенов.\n",
    "\n",
    "Таким образом создаётся **граф** со сгенерированными **вариантами предложений**. Далее выбирается предложение с наибольшей **perplexity** (уверенностью модели в реалистичности текста).\n",
    "\n",
    "Обычно это приводит к высокой связности (когерентности) текста, но при этом к сухости и скучности текста. Также это не решает полностью проблему с повторениями кусочков текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/beam_search.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/599673/\">GPT для чайников: от токенизации до файнтюнинга</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation with beam-search\n",
    "out = model.generate(\n",
    "    input_ids, do_sample=False, num_beams=5, max_length=30, pad_token_id=30\n",
    ")\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сэмплирование с Температурой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы добавить тексту непредсказуемости и человечности, можно использовать вероятностное сэмплирование с температурой. Будет использоваться не самый вероятный токен, а случайный, с учётом распределения вероятностей.\n",
    "\n",
    "Параметр температуры позволяет контролировать степень случайности. При нулевой температуре метод совпадает с жадным сэмплированием, при  большой температуре токены будут выбираться полностью случайно. Обычно хорошо работает температура в диапазоне `0.8–2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула модификации распределения вероятностей очень похожа на формулу распределения Больцмана: чем выше температура системы, тем больше \"размазывается\" распределение вероятностей её возможных состояний, отсюда слово \"температура\".\n",
    "\n",
    "$$p=softmax(log(p)/t)$$\n",
    "\n",
    "Стоит отметить, что случайная природа генерации будет иногда приводить к полностью некорректным результатам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(\n",
    "    input_ids, do_sample=True, temperature=1.3, max_length=30, pad_token_id=30\n",
    ")\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сэмплирование с Ограничением Маловероятных Токенов (Nucleus sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ввести запрет на семплирование наименее вероятных токенов:\n",
    "\n",
    "* `top-k` зануляет все вероятности, кроме $k$ наибольших;\n",
    "\n",
    "* `top-p` оставляет минимальный набор токенов, причём сумма их вероятностей будет не больше $p$.\n",
    "\n",
    "`top-p` ограничение называют **Nucleus Sampling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=1.3,\n",
    "    top_k=20,\n",
    "    top_p=0.8,\n",
    "    max_length=30,\n",
    "    pad_token_id=30,\n",
    ")\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Файнтюнинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся моделью меньшего размера, чтобы она поместилась на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import transformers\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -q transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Процесс обучения**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучающий текст нарезается на случайные блоки, которые составляются в последовательности из 1024 (2048 у GPT-3) токенов, разделяясь специальным `<|endoftext|>` символом. Во время обучения модель учится предсказывать (классифицировать) каждый токен в последовательности один за другим при помощи Cross-Entropy Loss.\n",
    "\n",
    "Так как входная последовательность всегда заполнена до конца, padding не используется. Но во время инференса длина входного текста может быть произвольной, поэтому надо явно указывать, чем паддить оставшиеся позиции. По дефолту использутеся тот же `<|endoftext|>`.\n",
    "\n",
    "В отдельных версиях GPT вышесказанное может модифицироваться. Например, в ruGPT3 гораздо больше специальных токенов: `<s\\>`, `<s>`, `<pad>`, `<unk>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучающие данные\n",
    "Будем учить GPT генерировать стихи Маяковского. В качестве обучающих данных возьмём всего лишь один стих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Дым табачный воздух выел.\n",
    "Комната —\n",
    "глава в крученыховском аде.\n",
    "Вспомни —\n",
    "за этим окном\n",
    "впервые\n",
    "руки твои, исступленный, гладил.\n",
    "Сегодня сидишь вот,\n",
    "сердце в железе.\n",
    "День еще —\n",
    "выгонишь,\n",
    "может быть, изругав.\n",
    "В мутной передней долго не влезет\n",
    "сломанная дрожью рука в рукав.\n",
    "Выбегу,\n",
    "тело в улицу брошу я.\n",
    "Дикий,\n",
    "обезумлюсь,\n",
    "отчаяньем иссеча́сь.\n",
    "Не надо этого,\n",
    "дорогая,\n",
    "хорошая,\n",
    "дай простимся сейчас.\n",
    "Все равно\n",
    "любовь моя —\n",
    "тяжкая гиря ведь —\n",
    "висит на тебе,\n",
    "куда ни бежала б.\n",
    "Дай в последнем крике выреветь\n",
    "горечь обиженных жалоб.\n",
    "Если быка трудом уморят —\n",
    "он уйдет,\n",
    "разляжется в холодных водах.\n",
    "Кроме любви твоей,\n",
    "мне\n",
    "нету моря,\n",
    "а у любви твоей и плачем не вымолишь отдых.\n",
    "Захочет покоя уставший слон —\n",
    "царственный ляжет в опожаренном песке.\n",
    "Кроме любви твоей,\n",
    "мне\n",
    "нету солнца,\n",
    "а я и не знаю, где ты и с кем.\n",
    "Если б так поэта измучила,\n",
    "он\n",
    "любимую на деньги б и славу выменял,\n",
    "а мне\n",
    "ни один не радостен звон,\n",
    "кроме звона твоего любимого имени.\n",
    "И в пролет не брошусь,\n",
    "и не выпью яда,\n",
    "и курок не смогу над виском нажать.\n",
    "Надо мною,\n",
    "кроме твоего взгляда,\n",
    "не властно лезвие ни одного ножа.\n",
    "Завтра забудешь,\n",
    "что тебя короновал,\n",
    "что душу цветущую любовью выжег,\n",
    "и су́етных дней взметенный карнавал\n",
    "растреплет страницы моих книжек…\n",
    "Слов моих сухие листья ли\n",
    "заставят остановиться,\n",
    "жадно дыша?\n",
    "Дай хоть\n",
    "последней нежностью выстелить\n",
    "твой уходящий шаг..\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке transformers есть готовые инструменты для подготовки датасета и даталодера. На вход нужен всего лишь один `.txt` файл с обучающим текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save text train data as .txt file\n",
    "train_path = \"train_dataset.txt\"\n",
    "with open(train_path, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Creating Dataset\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=64)\n",
    "\n",
    "# Сreating DataLoader (crop the text into optimal length pieces)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Для файнтюнинга нам необходим объект класса Trainer, который сделает всю работу за нас. Далее нужно будет всего лишь запустить `trainer.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned\",  # The output directory\n",
    "    overwrite_output_dir=True,  # overwrite the content of the output directory\n",
    "    num_train_epochs=200,  # number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    warmup_steps=10,  # number of warmup steps for learning rate scheduler\n",
    "    gradient_accumulation_steps=16,  # to make \"virtual\" batch size larger\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    optimizers=(\n",
    "        torch.optim.AdamW(model.parameters(), lr=1e-5),\n",
    "        None,\n",
    "    ),  # Optimizer and learnig rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат файнтюнинга\n",
    "Готово! Теперь давайте посмотрим, что же сочинит GPT в стиле Маяковского, если на вход подать такую строчку:\n",
    "\n",
    "\"Учим нейросеть за нейросетью!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability sampling with limit example\n",
    "text = \"Как же сложно учить матанализ!\\n\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        num_beams=2,\n",
    "        temperature=1.5,\n",
    "        top_p=0.9,\n",
    "        max_length=100,\n",
    "        pad_token_id=512,\n",
    "    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полезные ссылки**\n",
    "1. [GPT в картинках](https://habr.com/ru/post/490842/) — очень подробный разбор внутренней архитектуры GPT-2 с акцентом на иллюстрации.\n",
    "2. [Трансформер в картинках](https://habr.com/ru/post/486358/) — очень подробный разбор архитектуры Transformer с акцентом на иллюстрации.\n",
    "3. [Tokenizers tutorial](https://huggingface.co/docs/transformers/tokenizer_summary) — краткий разбор всех типов токенизаторов от Huggingface с примерами.\n",
    "4. [Как генерировать текст](https://huggingface.co/blog/how-to-generate) — обзор способов сэмплирования текста с помощью языковых моделей (бимсёрч и тд).\n",
    "5. [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) — оригинальная статья про первый трансформер.\n",
    "6. [GPT-1](https://openai.com/blog/language-unsupervised/) — статья в блоге OpenAI про GPT-1.\n",
    "7. [GPT-2](https://openai.com/blog/better-language-models/) — статья в блоге OpenAI про GPT-2.\n",
    "8. [GPT-3](https://openai.com/blog/gpt-3-apps/) — статья в блоге OpenAI про GPT-3.\n",
    "9. [WebGPT](https://openai.com/blog/improving-factual-accuracy/) — статья в блоге OpenAI про GPT-3, обученную гуглить.\n",
    "10. [Codex](https://openai.com/blog/openai-codex/) — статья в блоге OpenAI про GPT-3, обученную писать код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NLP метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Выбирайте метрику под свою конкретную задачу!</font> Или даже конструируйте её самостоятельно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помните эту картинку с предыдущей лекции? Сегодня мы разберём нейросетевые метрики.\n",
    "\n",
    "**BLEURT, Prism** - нейросети, последние слои принимают на вход **эмбеддинги машинного и эталонного переводов**, а на выходе дают оценку качества перевода.\n",
    "\n",
    "**COMET, UniTE** - нейросети, **эмбеддинги машинного и эталонного переводов**, **оригинал** переводимого текста.\n",
    "\n",
    "**Безреференсные метрики** - модели, сравнивающие **напрямую машинный перевод и первоисточник** (reference-free metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/nlp_metrics.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Перечень метрик и их объяснений](https://habr.com/ru/articles/745642/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\">Хорошие источники</font>\n",
    "\n",
    "[Про трансформеры](https://arig23498.notion.site/Transformers-969f4b27c48147778c1e2dbda0c83ce0)\n",
    "\n",
    "[Аннотированный трансформер](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "[Код множества моделей с красивыми комментариями](https://nn.labml.ai/)\n",
    "\n",
    "[Зоопарк Трансформеров: большой обзор моделей от BERT до Alpaca](https://habr.com/ru/companies/just_ai/articles/733110/)\n",
    "\n",
    "[Transformers in computer vision: ViT architectures, tips, tricks and improvements](https://theaisummer.com/transformers-computer-vision/)\n",
    "\n",
    "[Illustrated transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "[Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)\n",
    "\n",
    "[Open-source реализация GPT-3](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)\n",
    "\n",
    "[Transformer для русского языка](https://github.com/vlarine/transformers-ru)\n",
    "\n",
    "[NLP Course for you](https://lena-voita.github.io/nlp_course.html)\n",
    "\n",
    "[Курс по NLP от ШАД](https://github.com/yandexdataschool/nlp_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели внимания в машинном переводе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как такой подход  работает на примере перевода с английского на французский.\n",
    "\n",
    "На каждом шаге генерируется набор весов, которые отвечают за фокусировку на том или ином месте входной последовательности. Как мы видим, английское предложение имеет иной порядок слов относительно французского. Например, в английском варианте словосочетание **European Economic Area**, в то время как во французском **zone économique européenne**.\n",
    "\n",
    "В английском прилагательные идут перед существительным, в то время как во французском языке наоборот.\n",
    "\n",
    "Таким образом, благодаря гибкости модели мы можем обрабатывать и учитывать разный порядок слов в разных языках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/visualize_attention_weights.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1409.0473.pdf\">Neural machine translation by jointly learning to align and translate</a></em>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Механизм внимания не обязательно должен принимать на вход последовательность.\n",
    "\n",
    "* Мы можем применять его в том числе для генерации подписей для картинок. Входом в данном случае будет являться матрица признаков, которая была получена при применении сверточной сети к картинке.\n",
    "\n",
    "* Далее по этой матрице мы считаем веса внимания и делаем аналогично первому примеру."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом будет также заметить, что для достаточно длинного предложения наша модель может забыть и то, что она генерирует. Потому мы можем сделать два attention: один — на представление исходного предложения, а второй — на представление того, что уже сгенерировано (что еще не сгенерировано заменяем нулями)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning with RNNs and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, основанные на внимании (attention), намного более продвинутые, нежели обычные нейросети. Они могут концентрироваться на отдельных частях данных, что позволяет избежать зашумления представлений.\n",
    "\n",
    "Идея состоит в том, что на каждом этапе генерации описания нейронная сеть в разной степени обращает внимание на те или иные фрагменты изображения, соответствующие следующему слову в описании.\n",
    "\n",
    "После обучения модели можно увидеть, что она как бы переносит своё внимание по изображению для каждого генерируемого слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/image_captioning_with_rnn_and_attention_example_step_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/image_captioning_with_rnn_and_attention_example_step_2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/image_captioning_with_rnn_and_attention_example_step_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/image_captioning_with_rnn_and_attention_example_step_4.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"http://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf\">Stanford University CS231n: lectures</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**А если картинки?**\n",
    "\n",
    "К примеру, у нас есть картинка. На этой картинки у нас есть области, которые можно описать одним словом — **key**. Например, фонарь/девушка/...\n",
    "\n",
    "Сами эти области — это **value**, которые введенным **key** соответствуют.\n",
    "\n",
    "Далее нам приходит **query**, например, running. Мы можем посчитать похожесть каждого из ключей, которые у нас есть, на query.\n",
    "\n",
    "И далее выдать информацию только по **value**, похожим на наш **query**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/key_query_value_example.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути мы описали то, как будем делать при помощи нейронных сетей питоновский словарь. С той разницей, что питоновский словарь может выдавать значения только для тех ключей, что в нем есть, а наш словарь выдает ответ для любого ключа-запроса, основываясь на его похожести на ключи словаря."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели внимания в задаче генерации подписи к изображениям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель с вниманием также может быть применена в задаче, когда от нейронной сети требуется по изображению сгенерировать подпись.\n",
    "\n",
    "Имеем набор пар \"картинка : подпись\"\n",
    "\n",
    "Вместо рекуррентного кодировщика используем сверточную нейронную сеть. Веса внимания применяем к признакам на карте активации после нескольких сверточных слоев. Получается \"маска\" внимания.\n",
    "\n",
    "Таким образом декодер имеет возможность обращать внимание на разные участки входного изображения при генерации очередного слова.\n",
    "\n",
    "На иллюстрации приведены входные изображения и наложенные на них маски внимания, которые возникли в сети при генерации подчеркнутого слова.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/visulize_attention_map_examples.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1502.03044.pdf\">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что “привлекает внимание” нейронной сети при написании текстового описания картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/visulize_attention_map.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1502.03044.pdf\">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети, использующие механизм внимания (attention), активно применяются для решения задачи [Visual Question Answering](https://paperswithcode.com/paper/vqa-visual-question-answering). В данной задаче нейросеть должна научиться давать развернутые ответы на вопросы по изображению. Модель должна не только решать задачу классификации, но и распознавать признаки (цвет, форма, размер, количество и т.д.) предметов на изображении, различать, в какой части изображения находится предмет и его положение относительно других предметов. Решение этой задачи может помочь людям с проблемами со зрением лучше ориентироваться в пространстве.\n",
    "\n",
    "Подробнее:\n",
    "1. [Нейросеть описывает мир незрячим людям](https://www.reg.ru/blog/nejroset-opisyvaet-mir-nezryachim-lyudyam/)\n",
    "2. [Учим нейросети рассуждать о том, что они видят](https://www.reg.ru/blog/uchim-nejroseti-rassuzhdat-o-tom-chto-oni-vidyat/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention (ViT 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Visual Transformers: Token-based Image Representation and Processing for Computer Vision (Wu et al., 2020)](https://arxiv.org/abs/2006.03677)\n",
    "\n",
    "[Реализация](https://github.com/lucidrains/vit-pytorch)\n",
    "\n",
    "[Разбор ViT](https://viso.ai/deep-learning/vision-transformer-vit/)\n",
    "\n",
    "\n",
    "**Vision Transformer** — это модель для классификации изображений, которая использует архитектуру трансформера. Попробуем разобраться, как она работает.\n",
    "\n",
    "В 2020 году стали появляться работы, где модели на базе архитектур трансформер смогли показать результаты лучше, чем у **CNN** моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/cited_vit_accuracy.png\"  width=\"650\"></center>\n",
    "\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020</a></em></center>\n",
    "\n",
    "BiT — это baseline модель на базе **ResNet**, ViT — **Visual Transformer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Недостатки сверточного слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы практически полностью отказались от использования сверток,  заменив их слоями **self-attention**.  Попробуем понять, почему это сработало.\n",
    "\n",
    "Добавляя в модель свёрточный слой, мы руководствуемся резонным предположением: чем ближе пиксели на изображении, тем больше будет их взаимное влияние.\n",
    "\n",
    "В большинстве случаев это работает:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/cnn_ok.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - На слое n (красный) активируются нейроны, которые реагируют на морду и на хвост кота.\n",
    "\n",
    " - В карте активаций их выходы оказываются рядом, и в слое n + 1 (синий) они попадают в одну свертку, которая активируется на объектах типа \"кот\".\n",
    "\n",
    "Так случается часто, но не всегда:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/cnn_fail.jpg\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом изображении активации нейронов, реагирующих на морду и хвост, не попадут в одну свертку на следующем слое. Это может привести к тому, что нейрон, обучившийся реагировать на кошек, не активируется.\n",
    "\n",
    "Причиной этого является допущение ([Inductive bias](https://en.wikipedia.org/wiki/Inductive_bias)) о взаимном влиянии соседних пикселей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/global_attention.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-attention** слой лишен этого недостатка. Он обучается оценивать взаимное влияние входов друг на друга. Но как применить его к изображениям?\n",
    "\n",
    "В статье [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)](https://arxiv.org/pdf/2010.11929.pdf) предлагается разбивать картинки на кусочки (patches) размером 16x16 пикселей и подавать их на вход модели.\n",
    "\n",
    "Проделаем это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/cat.jpeg\"\n",
    "!wget -q $URL -O image.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем изображение в тензор, порежем на фрагменты и отобразим их, используя image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"image.jpg\")\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "img = transform(img)\n",
    "patches = []\n",
    "sz = 64\n",
    "for r in range(0, img.shape[1], sz):\n",
    "    for c in range(0, img.shape[2], sz):\n",
    "        patches.append(img[:, r : r + sz, c : c + sz])\n",
    "\n",
    "patches = torch.stack(patches).type(torch.float)\n",
    "\n",
    "img_grid = utils.make_grid(patches, pad_value=10, normalize=True, nrow=4)\n",
    "plt.imshow(transforms.ToPILImage()(img_grid).convert(\"RGB\"))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход модели они поступят в виде вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "img_grid = utils.make_grid(patches, pad_value=10, normalize=True, nrow=256 // 16)\n",
    "plt.imshow(transforms.ToPILImage()(img_grid).convert(\"RGB\"))\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем последовательность из фрагментов изображения передается в модель, где после ряда преобразований попадает на вход слоя **self-attention**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/self_attention.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинки приведены исключительно для наглядности, в действительности слой работает с векторами признаков, которые не визуализируются столь очевидно. Однако кэффициенты, с которыми складываются вектора-признаков, отражают важность каждого с учетом всех остальных входов.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение со сверткой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/conv_vs_self_attention1.png\" width=\"400\">\n",
    "\n",
    "При свертке каждый признак умножается на свой вес, и затем они суммируются. Важно что вклад взвешенных признаков в сумму не зависит от контекста.\n",
    "\n",
    "То есть ягода клубники, лежащая на столе (где рядом с ней может быть все, что угодно), даст такой же вклад в сумму, как и ягода с клубничного куста.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/conv_vs_self_attention2.png\"  width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слой self-attention выполняет ту же задачу, что и свертка: получает на вход вектор признаков и возвращает другой, более информативный.  Но делает это более умно:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/conv_vs_self_attention3.png\"  width=\"900\"></center>\n",
    "\n",
    "*Вместо чисел здесь вектора, но принципильно это ничего не меняет, можно применить self-attention и к отдельным признакам (яркостям, пикселям), просто для это потребуется очень много ресурсов.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Каждый признак** участвует в каждой сумме, а не только те, что попали в рецептивное поле фильтра.\n",
    "Кроме этого, суммируются они с коэффициентами $a$, которые **зависят от входов** и различны для каждой суммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения этих коэффициентов и нужна большая часть слоя self-attention. На рисунке выделено красным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/conv_vs_self_attention5.png\"  width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как получить веса внимания?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim**0.5)\n",
    "        attention = scores.softmax(dim=2)\n",
    "        print(\"Scores shape\", scores.shape)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "self_attention_layer = SelfAttention(embed_dim)\n",
    "dummy_x = torch.randn(1, 4 * 4, embed_dim)  # Batch_size x Sequence_len x Embedding_size\n",
    "out = self_attention_layer(dummy_x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Соображения относительно размера patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформеры работают с последовательностями за счёт механизма внимания (**self-attention**). И чтобы подать на вход изображение, требуется превратить его в последовательность.\n",
    "\n",
    "Сделать это можно разными способами, например, составить последовательность из всех пикселей изображения. Её длина $n =  H*W$ (высота на ширину)\n",
    "\n",
    "[Сложность вычисления](https://www.researchgate.net/figure/Compare-the-computational-complexity-for-self-attention-where-n-is-the-length-of-input_tbl7_347999026) одноголового слоя **self-attention** $O(n^2 d )$,  где $n$ — число токенов и $d$ — размерность входа (embedding)  (для любознательных расчеты [тут](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model)).\n",
    "\n",
    "То есть для квадратных изображений $(H==W)$ получим $O(H^3 d )$\n",
    "\n",
    "1. Такой подход будет очень вычислительно сложен.\n",
    "\n",
    "2. Интуитивно понятно, что кодировать каждый пиксель относительно большим embedding-ом не очень осмысленно.\n",
    "\n",
    "\n",
    "*Для тех, кто забыл, напомним что $O()$ — это Big O notation, которая отражает ресурсы, требуемые для вычисления. Так для $O(1)$ — время вычисления будет постоянным, вне зависимости от количества данных, а для $O(N)$ — расти пропорционально количеству данных.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберём на примере: Допустим, мы используем трансформер для предложения длиной в 4 слова — \"Мама мылом мыла раму\" => у нас есть `4 токена`. Закодируем их в *embeddings* с размерностью `256`. Потребуется порядка $4^2*256 = 4096$ операций.\n",
    "\n",
    "А теперь попробуем провернуть то же самое для картинки размерами 256 на 256.\n",
    "Количество токенов\n",
    "\n",
    " $256^3*256  = 256^4 =  4 294 967 296 $. Упс... Кажется, нам так никаких ресурсов не хватит — трансформеры с картинками использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Посчитаем сложность для картинки размером 256x256, разбитой на кусочки по 16px. при том же размере токена (256) $n = 16$.\n",
    "$16^2*256 = 256^2 = 65536 $. И впрямь! ~65000 раз меньше ресурсов требуется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Как устроен  self-attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)\n",
    "\n",
    "[Self-attention слой в PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не теряем ли мы важной информации, разбивая изображение на фрагменты? На первый взгляд кажется, что модель сможет научиться восстанавливать порядок, в котором фрагменты шли в исходном изображении.\n",
    "\n",
    "Всегда ли?\n",
    "\n",
    "Рассмотрим пример изображения, где нет ярко выраженной текстуры:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/positional_transformer_explanation.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунке а) наковальня падает на ребенка, на рисунке б) ребенок прыгает на наковальне.\n",
    "\n",
    "Суть принципиально отличается, но что будет, если составить из фрагментов любого изображения набор патчей:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/positional_vec_transformer_explanation.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Восстановить по нему можно будет любой из вариантов!\n",
    "\n",
    "Так как **self-attention** блок никак не кодирует позицию элемента на входе, то важная информация потеряется.\n",
    "\n",
    "Чтобы избежать таких потерь, информацию, кодирующую позицию фрагмента (patch),  добавляют к входным данным **self-attention** слоя в явном виде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/linear_projection_of_flattened_patches.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Методы для кодирования позиции](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем грузить наши изображения в **Vi**sual **T**ransformer.\n",
    "\n",
    "**Self-attention** блок мы разобрали, остальные блоки модели нам знакомы:\n",
    "\n",
    "> **MLP** (Multi layer perceptron) — Блок из одного или нескольких линейных слоев\n",
    "\n",
    "> **Norm** — Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/visual_transformer_architecture.png\" width=\"1000\"></center>\n",
    "<center><em>Архитектура Visual Transformer </em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Изображение режется на фрагменты (patch).\n",
    "2.   Фрагменты (patch) подвергаются линейной проекции с помощью **MLP**.\n",
    "3.   С полученными на выходе **MLP** векторами конкатенируются **positional embeddings** (кодирующие информацию о позиции path, как и в обычном трансформере для текста).\n",
    "4. К полученным векторам добавляют еще один **0***, который называют **class embedding**.\n",
    "\n",
    "Любопытно, что для предсказания класса используется только выход. Он соответствует дополнительному **class embedding**.  Остальные выходы (а для каждего токена в трансформере есть свой выход) отбрасываются за ненадобностью.\n",
    "\n",
    "В финале этот специальный токен **0*** прогоняют через **MLP** и предсказывают классы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем провести аналогию со свертками. Если рассмотреть сверточную сеть без слоев пулинга и других способов изменения пространственных размеров карт признаков, то можно считать такую сеть механизмом постепенно улучшаюшем качество признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L10/out/selfattention_feature_flow.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание с помощью ViT\n",
    "\n",
    "\n",
    "Используем пакет [ViT PyTorch](https://pypi.org/project/pytorch-pretrained-vit/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pytorch_pretrained_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В пакете доступны несколько [предобученных моделей](https://github.com/lukemelas/PyTorch-Pretrained-ViT#loading-pretrained-models):\n",
    "\n",
    "B_16, B_32, B_16_imagenet1k, ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_vit import ViT\n",
    "\n",
    "model = ViT(\"B_16_imagenet1k\", pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим классы для небольшой части датасета ImageNet и посмотрим на них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full list of labels\n",
    "#'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)\n",
    "\n",
    "with open(\"imagenet_class_index.json\") as f:\n",
    "    imagenet_labels = json.load(f)\n",
    "\n",
    "classes = np.array(list(imagenet_labels.values()))[:, 1]\n",
    "\n",
    "pp.pprint(\n",
    "    dict(list(imagenet_labels.items())[:10])\n",
    ")  # Use Pretty Print to display long dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И загрузим изображение, с которым будем работать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/capybara.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capybara_in_pil = Image.open(\"capybara.jpg\")\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((384, 384)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "capybara_in_tensor = transforms(capybara_in_pil)\n",
    "print(capybara_in_tensor.shape)  # torch.Size([1, 3, 384, 384])\n",
    "\n",
    "# Classify\n",
    "with torch.no_grad():\n",
    "    outputs = model(capybara_in_tensor.unsqueeze(0))\n",
    "print(outputs.shape)  # (1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что нам предсказывает ViT. Для этого подгрузим dict с переводом индексов в человеческие названия:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, собственно, переведем индекс в название:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = outputs[0].topk(3).indices\n",
    "top3 = top3.tolist()\n",
    "\n",
    "\n",
    "print(\"Top 3 predictions:\")\n",
    "for class_num in top3:\n",
    "    print(class_num, classes[class_num])\n",
    "display(capybara_in_pil.resize((384, 384)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну что ж, почти (капибар в классах ImageNet 1k, как вы могли догадаться, просто нет)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объем данных и ресурсов\n",
    "\n",
    "Как следует из текста [статьи](https://arxiv.org/abs/2010.11929), **ViT**, обученный на **ImageNet**, уступал baseline CNN-модели\n",
    "на базе сверточной сети (**ResNet**). И только при увеличении датасетов больше, чем **ImageNet**, преимущество стало заметным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/cited_vit_accuracy.png\"  width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020)</a></em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вряд ли в вашем распоряжении окажется датасет, сравнимый с [JFT-300M](https://paperswithcode.com/dataset/jft-300m) (300 миллионов изображений),\n",
    "и GPU/TPU ресурсы, необходимые для обучения с нуля (*it could be trained using a standard cloud TPUv3 with 8 cores in approximately 30 days*)\n",
    "\n",
    "Поэтому для работы с пользовательскими данными используется техника дообучения ранее обученной модели на пользовательских данных (**fine-tuning**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeiT: Data-efficient Image Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для практических задач рекомендуем использовать эту реализацию. Авторы предлагают подход, благодаря которому становится возможным обучить модель на стандартном **ImageNet** (ImageNet1k) на одной рабочей станции за 3 дня.\n",
    "\n",
    "*We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/cited_deit_vit.png\"  width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/2012.12877\">Training data-efficient image transformers & distillation through attention</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбор этого материала уже не входит в наш курс и рекомендуется к самостоятельному изучению.\n",
    "\n",
    "Дополнительно:\n",
    "\n",
    "[[arxiv] 🎓Training data-efficient image transformers\n",
    "& distillation through attention](https://arxiv.org/pdf/2012.12877v2.pdf)\n",
    "\n",
    "Статьи, предшествовавшие появлению **ViT**:\n",
    "\n",
    "[Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n",
    "\n",
    "[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование ViT с собственным датасетом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для использования **ViT** с собственными данными рекомендуем не обучать собственную модель с нуля, а использовать уже предобученную.\n",
    "\n",
    "Рассмотрим этот процесс на примере. Есть предобученный на **ImageNet** **Visual Transformer**, например: [deit_tiny_patch16_224](https://github.com/facebookresearch/deit)\n",
    "\n",
    "И мы хотим использовать ее со своим датасетом, который может сильно отличаться от **ImageNet**.\n",
    "\n",
    "Для примера возьмем **CIFAR-10**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель. Как указано на [github](https://github.com/facebookresearch/deit), модель зависит от библиотеки [timm](https://fastai.github.io/timmdocs/), которую нужно установить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загружаем модель с [pytorch-hub](https://pytorch.org/hub/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load(\n",
    "    \"facebookresearch/deit:main\", \"deit_tiny_patch16_224\", pretrained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что модель запускается.\n",
    "Загрузим изображение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L10/capybara.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И подадим его на вход трансформеру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "pil = Image.open(\"capybara.jpg\")\n",
    "\n",
    "# create the data transform that DeiT expects\n",
    "imagenet_transform = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "out = model(imagenet_transform(pil).unsqueeze(0))\n",
    "print(out.shape)\n",
    "pil.resize((224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать модель с **CIFAR-10**, нужно поменять количество выходов слоя, отвечающих за классификацию. Так как в **CIFAR-10** десять классов, а в **ImageNet** — тысяча.\n",
    "\n",
    "Чтобы понять, как получить доступ к последнему слою, выведем структуру модели:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что последний слой называется head и, судя по количеству параметров на выходе (1000), которое совпадает с количеством классов **ImageNet**, именно он отвечает за классификацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим его слоем с 10-ю выходами по количеству классов в CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head = torch.nn.Linear(192, 10, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что модель не сломалась."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(imagenet_transform(pil).unsqueeze(0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузим **CIFAR-10** и проверим, как дообучится модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "cifar10 = CIFAR10(root=\"./\", train=True, download=True, transform=imagenet_transform)\n",
    "\n",
    "# We use only part of CIFAR10 to reduce training time\n",
    "trainset, _ = torch.utils.data.random_split(cifar10, [10000, 40000])\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = CIFAR10(root=\"./\", train=False, download=True, transform=imagenet_transform)\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Проведем стандартный цикл обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, num_epochs=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tqdm_notebook(train_loader):\n",
    "            inputs, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дообучаем (**fine tune**) только последний слой модели, который мы изменили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.head.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим точность, на всей тестовой подвыборке **CIFAR-10**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def accuracy(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in testloader:\n",
    "        images, labels = batch\n",
    "        outputs = model(images.to(device))\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of fine-tuned network : {accuracy(model, test_loader):.2f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дообучив последний слой на одной эпохе с использованием 20% данных, мы получили точность ~0.75\n",
    "\n",
    "Если дообучить все слои на 2-х эпохах, можно получить точность порядка 0.95.\n",
    "\n",
    "Это результат намного лучше чем тот, что мы получали на семинарах.\n",
    "\n",
    "Для этого потребуется порядка 10 мин (на GPU). Сейчас мы этого делать не будем.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И одной из причин того, что обучение идет относительно медленно, является увеличение изображений размером 32x32 до 224x224.\n",
    "\n",
    "Если бы мы использовали изображения **CIFAR-10** в их родном размере, мы бы не потеряли никакой информации, но могли бы в разы ускорить обучение.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение размеров входа ViT\n",
    "\n",
    "На первый взгляд, ничего не мешает это сделать: **self-attention** слой работает с произвольным количеством входов.\n",
    "\n",
    "Давайте посмотрим, что будет, если подать на вход модели изображение, отличное по размерам от 224x224.\n",
    "\n",
    "Для этого перезагрузим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = torch.hub.load(\n",
    "        \"facebookresearch/deit:main\", \"deit_tiny_patch16_224\", pretrained=True\n",
    "    )\n",
    "    model.head = torch.nn.Linear(192, 10, bias=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И уберем из трансформаций Resize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_transform = T.Compose(\n",
    "    [\n",
    "        # T.Resize((224, 224)),    don't remove this line\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Change transformation in base dataset\n",
    "cifar10.transform = cifar_transform\n",
    "first_img = trainset[0][0]\n",
    "\n",
    "model.to(torch.device(\"cpu\"))\n",
    "try:\n",
    "    out = model(first_img.unsqueeze(0))\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем ошибку.\n",
    "\n",
    "Ошибка возникает в объекте [PatchEmbed](https://huggingface.co/spaces/Andy1621/uniformer_image_demo/blob/main/uniformer.py#L169), который превращает изображение в набор эмбеддингов.\n",
    "\n",
    "У объекта есть свойство `img_size`, попробуем просто поменять его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.patch_embed.img_size = (32, 32)\n",
    "try:\n",
    "    out = model(first_img.unsqueeze(0))\n",
    "except Exception as e:\n",
    "    print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем новую ошибку.\n",
    "\n",
    "И возникает она в строке\n",
    "`x = self.pos_drop(x + self.pos_embed)`\n",
    "\n",
    "x — это наши новые эмбеддинги для CIFAR-10 картинок\n",
    "\n",
    "Откуда взялось число 5?\n",
    "\n",
    "4 — это закодированные фрагменты (patch) для картинки 32х32, их всего 4 (16x16) + один embedding для предсказываемого класса(class embedding).\n",
    "\n",
    "А 197 — это positional encoding — эмбеддинги, кодирующие позицию элемента. Они остались от **ImageNet**.\n",
    "\n",
    "Так как в ImageNet картинки размера 224x224, то в каждой помещалось 14x14 = 196 фрагментов и еще embedding для класса, итого 197 позиций.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги для позиций доступны через свойство:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_embed.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам надо изменить количество pos embeddings так, чтобы оно было равно 5  (количество patch + 1).\n",
    "Возьмем 5 первых:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_embed.data = model.pos_embed.data[:, :5, :]\n",
    "out = model(first_img.unsqueeze(0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заработало!\n",
    "\n",
    "Теперь обучим модель. Так как изображения стали намного меньше, то мы можем увеличить размер batch и использовать весь датасет. Также будем обучать все слои, а не только последний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10.transform = cifar_transform\n",
    "train_loader = DataLoader(cifar10, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "# Now we train all parameters because model altered\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(model, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильно быстрее.\n",
    "Посмотрим на результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.transform = cifar_transform\n",
    "print(f\"Accuracy of altered network : {accuracy(model,test_loader):.2f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильно хуже.\n",
    "\n",
    "Это можно объяснить тем, что  маленькие patch  ImageNet(1/196) семантически сильно отличаются от четвертинок картинок из CIFAR-10 (1/4).\n",
    "\n",
    "Но есть и другая причина: мы взяли лишь первые 4 pos_embedding а остальные отбросили. В итоге модель вынуждена практически заново обучаться работать с малым pos_embedding, и двух эпох для этого мало.\n",
    "\n",
    "Зато теперь мы можем использовать модель с изображениями любого размера."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
