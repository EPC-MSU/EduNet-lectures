{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Трансформеры</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence with RNNs and Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель внимания — сходство входного и выходного состояния.**\n",
    "\n",
    "$\\large a(h, h^{'})$ — функция сходства состояний входа $h$ и выхода $h^{'}$\n",
    "\n",
    "$\\large a_{ti}$  — важность входа $i$ для выхода $t$ (attention score), $\\large \\sum_{i=1}a_{ti} = 1$\n",
    "\n",
    "$\\large c_t$ — вектор входного контекста для выхода t (context vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large h_i = f_{in}(x_i, h_{i-1});$\n",
    "\n",
    "$\\large \\color{red}{\\alpha_{ti} = norm_i \\ a(h_i, h'_{t-1});}$\n",
    "\n",
    "$\\large \\color{red}{c_t = \\sum\\limits_i \\alpha_{t_i} h_i;}$\n",
    "\n",
    "$\\large h'_t = f_{out}(h'_{t-1}, y_{t-1}, \\color{red}{c_t});$\n",
    "\n",
    "$\\large y_t = f_y(h'_t, y_{t-1}, \\color{red}{c_t}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примечание: $\\large \\displaystyle \\color{red}{norm_i(p_i)} = {p_i \\over \\sum\\limits_k p_k} .$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/seq2seq_encoder_decoder_with_attention.png\" width=\"700\">\n",
    "\n",
    "[К.В. Воронцов, Машинное обучение: Обработка последовательностей и модели внимания](http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности:\n",
    "\n",
    "* можно отказаться от рекуррентности по $h_i$;\n",
    "\n",
    "* можно вводить обучаемые параметры в $a$ и $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как решить проблему “бутылочного горлышка”?\n",
    "* формировать свой контекст $c_t$ для каждого элемента **выходной последовательности** $y_t$,\n",
    "* использовать для формирования контекста $c_t$ все **скрытые состояния** кодировщика $h_i$.\n",
    "\n",
    "Для формирования **векторов контекста** $(c_1, ..., c_T)$ возьмем линейную комбинацию **скрытых состояний** кодировщика $h_i$ с весами $a_{ti}$:\n",
    "$$ c_t=\\sum_{i=1}^{N}a_{ti}h_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha_{ti}$ называются **весами внимания**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса $a_{ti}$ указывают, какие **скрытые состояния** кодировщика $h_i$ важны для формирования элемента **выходной последовательности** $y_t$.  Они “показывают” декодировщику куда “смотреть” при генерации данного элемента. Такой механизм в нейросетях получил название **attention** (внимание).\n",
    "\n",
    "Веса $a_{ti}$ предсказывает сама модель. Для удобства веса подбираются таким образом, чтобы их сумма для каждого **вектора контекста** $c_t$ была равна 1 (нормализация):\n",
    "$$ \\sum_{i=1}^{N}a_{ti} = 1,$$\n",
    "\n",
    "$$  0\\leqslant a_{ti} \\leqslant 1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого на выходе предсказывающего веса слоя ставят **SoftMax**.\n",
    "\n",
    "Чтобы **вектор контекста** $c_t$ содержал информацию об уже сгенеренных элементах **выходной последовательности**, значение веса до нормализации  $e_{ti}$ зависит не только от скрытого состояния кодировщика $h_i$, но и от предыдущего скрытого состояния декодировщика $s_{t-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели внимания в машинном переводе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как такой подход  работает на примере перевода с английского на французский.\n",
    "\n",
    "На каждом шаге генерируется набор весов, которые отвечают за фокусировку на том или ином месте входной последовательности. Как мы видим, английское предложение имеет иной порядок слов относительно французского. Например, в английском варианте словосочетание **European Economic Area**, в то время как во французском **zone économique européenne**.\n",
    "\n",
    "В английском прилагательные идут перед существительным, в то время как во французском языке наоборот.\n",
    "\n",
    "Таким образом, благодаря гибкости модели мы можем обрабатывать и учитывать разный порядок слов в разных языках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/visualize_attention_weights.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1409.0473.pdf\">Neural machine translation by jointly learning to align and translate</a></em>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Механизм внимания не обязательно должен принимать на вход последовательность.\n",
    "\n",
    "* Мы можем применять его в том числе для генерации подписей для картинок. Входом в данном случае будет являться матрица признаков, которая была получена при применении сверточной сети к картинке.\n",
    "\n",
    "* Далее по этой матрице мы считаем веса внимания и делаем аналогично первому примеру."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом будет также заметить, что для достаточно длинного предложения наша модель может забыть и то, что она генерирует. Потому мы можем сделать два attention: один — на представление исходного предложения, а второй — на представление того, что уже сгенерировано (что еще не сгенерировано заменяем нулями)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели внимания в задаче генерации подписи к изображениям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель с вниманием также может быть применена в задаче, когда от нейронной сети требуется по изображению сгенерировать подпись.\n",
    "\n",
    "Имеем набор пар \"картинка : подпись\"\n",
    "\n",
    "Вместо рекуррентного кодировщика используем сверточную нейронную сеть. Веса внимания применяем к признакам на карте активации после нескольких сверточных слоев. Получается \"маска\" внимания.\n",
    "\n",
    "Таким образом декодер имеет возможность обращать внимание на разные участки входного изображения при генерации очередного слова.\n",
    "\n",
    "На иллюстрации приведены входные изображения и наложенные на них маски внимания, которые возникли в сети при генерации подчеркнутого слова.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/visulize_attention_map_examples.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1502.03044.pdf\">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что “привлекает внимание” нейронной сети при написании текстового описания картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/visulize_attention_map.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1502.03044.pdf\">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети, использующие механизм внимания (attention), активно применяются для решения задачи [Visual Question Answering](https://paperswithcode.com/paper/vqa-visual-question-answering). В данной задаче нейросеть должна научиться давать развернутые ответы на вопросы по изображению. Модель должна не только решать задачу классификации, но и распознавать признаки (цвет, форма, размер, количество и т.д.) предметов на изображении, различать, в какой части изображения находится предмет и его положение относительно других предметов. Решение этой задачи может помочь людям с проблемами со зрением лучше ориентироваться в пространстве.\n",
    "\n",
    "Подробнее:\n",
    "1. [Нейросеть описывает мир незрячим людям](https://www.reg.ru/blog/nejroset-opisyvaet-mir-nezryachim-lyudyam/)\n",
    "2. [Учим нейросети рассуждать о том, что они видят](https://www.reg.ru/blog/uchim-nejroseti-rassuzhdat-o-tom-chto-oni-vidyat/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention решает проблему \"забывания\" при работе с последовательностями. Но цена этого решения — квадратичное возрастание вычислительной сложности с ростом длины последовательности.\n",
    "\n",
    "Вычислительная сложность **одного слоя RNN** составляет $O(bn d^2)$, где $b$ — длина батча, $n$ — число токенов и $d$ — размерность входа. Часть $d^2$ обусловлена матричным перемножением внутри блока RNN.\n",
    "\n",
    "Вычислительная сложность **одного слоя attention** в простейшей реализации составляет $O(bn^2 d)$, то есть растет квадратично при росте длины последовательности $n$. Это объясняется тем, что длина выходной последовательности приблизительно равна длине входной последовательности $n$, и необходимо для каждого выходного токена рассчитать коэффициенты attention со всеми входными токенами. Сложность расчета одного коэффициента в простейшем случае составляет $O(d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ни рекуррентные сети, ни attention не могут эффективно работать с очень длинными последовательностями. RNN/LSTM \"забывают\" начало последовательности, а attention просто не может выполнить расчет за разумное время.\n",
    "\n",
    "На практике attention предпочтительнее, потому что удобнее иметь модель, которая или работает адекватно, или не работает вообще, чем модель, которая работает неадекватно (\"забывает\" контекст) без предупреждения.\n",
    "\n",
    "Обычно для attention используют достаточно большую длину последовательности, чтобы в нее могло поместиться практически любое предложение или даже несколько предложений, например, 512 токенов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разновидности функций сходства векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large a(h, h') = h^Th'$ — скалярное произведение;\n",
    "\n",
    "$\\large a(h, h') = exp(h^Th')$ — тогда norm превращается в SoftMax;\n",
    "\n",
    "$\\large a(h, h') = h^T\\color{red}{W}h'$ — c матрицей обучаемых параметров $\\color{red}{W}$;\n",
    "\n",
    "$\\large a(h, h') = \\color{red}{w}^Tth(\\color{red}{U}h + \\color{red}{V}h')$ — аддитивное внимание с $\\color{red}{w, U, V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводя внимание, мы говорили о некоторой **функции сходства** между скрытым состоянием декодировщика $h'$ и скрытым состоянием кодировщика $h$. Обобщением механизма внимания является введение в функцию сходства  **обучаемых параметров**.\n",
    "\n",
    "Какие вообще бывают функции сходства?\n",
    "\n",
    "\n",
    "1.   Первое, что приходит голову — просто считать скалярное произведение $h$ и $h'$.\n",
    "2.   Также можно брать от него экспоненту, тогда оператор нормировки превращается в **SoftMax**.\n",
    "\n",
    "Первые два способа возможны, только если потребовать, чтобы $h$ и $h'$ имели одинаковую размерность.\n",
    "\n",
    "3.   Можно вводить матрицу обучаемых параметров $W$.\n",
    "4.   Можно вводить небольшую двухслойную нейронную сеть с несколькими весовыми матрицами. Такое введение функции сходства называется аддитивным вниманием.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key, query, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейные преобразования векторов** **Query**, **Key** и **Value**.\n",
    "\n",
    "Наиболее часто используемым подходом является введение трех типов векторов, которые называют **Query**, **Key** и **Value**.\n",
    "\n",
    "Для каждого типа вектора вводится свое линейное преобразование, которое из исходного вектора делает вектор в каком-то другом пространстве. Все три они\n",
    "обычно приводятся к одной и той же размерности, обозначенной $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large a(h_i, h^\\prime_{t-1}) = (\\color{red}{W_k}h_i)^T(\\color{red}{W_q}h^\\prime_{t-1}) / \\sqrt d$\n",
    "\n",
    "$\\large \\alpha_{ti} = SoftMax_i \\space a(h_i, h^\\prime_{t-1})$\n",
    "\n",
    "$\\large c_t = \\Sigma_i \\alpha_{ti} \\color{red}{W_v} h_i$\n",
    "\n",
    "$ \\large \\color{red}{W_q}_{d \\times dim(h^\\prime)}, \\color{red}{W_k}_{d \\times dim(h)}, \\color{red}{W_v}_{d \\times dim(h)}$ — матрицы весов линейных нейронов (обучаемые линейные преобразования в пространство размерности $\\large d$).\n",
    "\n",
    "Возможно упрощение модели: $\\large \\color{red}{W_k} \\equiv \\color{red}{W_v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция сходства $a$ — это скалярное произведение, но перед тем,\n",
    "как сделать скалярное произведение, каждый из двух векторов $h$ и $h'$ мы переводим с помощью линейного преобразования в новое пространство. Тем самым мы даем возможность модели обучить эти параметры матриц весовых коэффициентов $W_k$ и $W_q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/query_key_value.png\" width=\"250\">\n",
    "\n",
    "<em>Source: <a href=\"http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf\">Обработка последовательностей: модели внимания и трансформеры</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два преобразованных вектора они скалярное произведение, которое для нормировки делится на корень из размерности.\n",
    "\n",
    "Нормировка на корень из размерности позволяет сделать SoftMax более сглаженным. Беда может заключаться в том, что в результате скалярного произведения могут образовываться слишком большие или слишком маленькие значения, соответственно, будет слишком большой разброс, и когда мы будем считать экспоненту, получим вектор, который стремится к нулям или к единицам.\n",
    "\n",
    "Чтобы этого не происходило, производится деление на корень из размерности. Идея в том, что скалярное произведение двух векторов в пространстве размерности $d$ — это сумма $d$ компонент. Закон больших чисел говорит о том, что когда мы складываем много одинаково распределенных случайных\n",
    "величин, то их дисперсия растет пропорционально $d$ и, соответственно,\n",
    "среднеквадратическое отклонение — это $\\sqrt d$.\n",
    "\n",
    "Далее мы к результату нормированного скалярного произведения применяем SoftMax.\n",
    "\n",
    "Еще одно обобщение: когда мы считаем вектор контекста, то мы складываем не сами входные векторы $h_i$, а преобразуем их в векторы значений (Value).\n",
    "\n",
    "У нас есть три места, где используются векторы, и в каждом из этих мест выполняется свое преобразование вектора в пространство размерности $d$.\n",
    "\n",
    "Матрицы весов этих преобразований обучаются вместе со всей остальной моделью.\n",
    "\n",
    "Иногда вводят упрощение и считают преобразования Key и Value одним преобразованием. Это позволяет сократить количество параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax normalization\n",
    "\n",
    "Вы могли заметить, что в формуле для вычисления сходства между Key и Query мы делим на $\\sqrt d$. Давайте убедимся, что стандартное отклонение скалярного произведения двух величин хорошо оценивается корнем из размерности.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сгенерируем вектор из многомерного нормального распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.random.normal(0, 100, size=(10000))\n",
    "\n",
    "plt.title(\"Normal distribution, std = 100\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xlabel(\"Sample value\")\n",
    "\n",
    "plt.hist(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, что будет с распределением значений этого вектора, если к нему применить SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "plt.title(\"Softmax on N(0, 100)\")\n",
    "plt.ylabel(\"Softmax value\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "\n",
    "plt.plot(softmax(a))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почти все значения 0 и одно (наибольшее), стало 1.\n",
    "Мы получили так называемое вырожденное распределение — у него вся плотность сосредоточена в одной точке.\n",
    "\n",
    "И получили на абсолютно случайных данных.\n",
    "\n",
    "Это приведет к затуханию градиента: мы будем распространять ошибку только для 1 значения из 10000. Учиться сеть будет плохо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но этого можно избежать — давайте просто стандартизируем наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.random.normal(0, 100, size=(10000))\n",
    "\n",
    "unit_std = std / 100\n",
    "\n",
    "plt.title(\"Normal distribution, std = 100\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xlabel(\"Sample value\")\n",
    "plt.hist(std)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Normal distribution, std = 1\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xlabel(\"Sample value\")\n",
    "plt.hist(unit_std)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути, в распределении ничего не поменялось, только масштаб. Но теперь SoftMax работает нормально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Softmax on N(0, 1)\")\n",
    "plt.ylabel(\"Softmax value\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "\n",
    "plt.plot(softmax(unit_std))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остается только понять, как нормировать наши данные в нашем слое. Считать налету, наверное, не лучшая идея.\n",
    "\n",
    "Наш слой делает скалярное произведение между двумя векторами, предположим, нормально распределенными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(dimensionality, experiments=int(10e4)):\n",
    "    c = []\n",
    "    for i in range(experiments):\n",
    "        a = torch.normal(0, 1, size=(int(dimensionality),))\n",
    "        b = torch.normal(0, 1, size=(int(dimensionality),))\n",
    "        c.append(torch.dot(a, b))\n",
    "\n",
    "    c = torch.Tensor(c)\n",
    "    return float(c.mean()), float(c.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "means, stds = {}, {}\n",
    "dims = torch.linspace(0, 100, 20)\n",
    "\n",
    "for dim in dims:\n",
    "    dim = float(dim)\n",
    "    t_mean, t_std = statistics(dim)\n",
    "    means[dim] = t_mean\n",
    "    stds[dim] = t_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(means.keys())\n",
    "y = list(means.values())\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.axhline(y=0, c=\"r\", linestyle=\"--\")\n",
    "plt.legend([\"Mean value\", \"Mean = 0\"])\n",
    "plt.title(\"Mean value of dot products\")\n",
    "plt.ylabel(\"Mean value\")\n",
    "plt.xlabel(\"Vector dimensionality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что среднее не сильно отличается от 0 (можно показать, что в среднем оно равно 0 для произведения нормально распределенных величин).\n",
    "\n",
    "А вот стандартное отклонение растет. И можно предположить, что растет оно как корень из размерности вектора. Так и есть. Потому и появляется именно такой нормировочный множитель в attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(stds.keys())\n",
    "y = list(stds.values())\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Std values and square distance\")\n",
    "plt.xlabel(\"Vector dimensionality\")\n",
    "\n",
    "x = np.linspace(0, 100, 10000)\n",
    "plt.plot(x, x**0.5, color=\"r\")\n",
    "plt.legend([\"sqrt(x)\", \"empirical std\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Идея:** $J$ разных моделей внимания совместно обучаются выделять различные аспекты входной информации (например, части речи, синтаксим, фразеологизмы):\n",
    "\n",
    "$\\large c_j = Attn(\\color{red}{W^j_q}q, \\color{red}{W^j_k}H,\\color{red}{W^j_v}H, \\ j = 1, \\dots, j)$\n",
    "\n",
    "**Варианты** агрегирования выходного вектор:\n",
    "\n",
    "$\\large \\displaystyle c = {1 \\over j} \\sum\\limits^J_{j=1}c^j$ — усреднение;\n",
    "\n",
    "$\\large \\displaystyle c = [c^1 \\dots c^J]$ — конкатенация;\n",
    "\n",
    "$\\large \\displaystyle c = [c^1 \\dots c^J]\\color{red}{W}$ — возвращение к нужной размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на то, к каким словам предложения gave может иметь отношение. В общем случае глагол может иметь связку со многими частями предложения. Как, например, с подлежащим, так и с причастиями.\n",
    "\n",
    "В идеале, нам бы хотелось обратить внимание функции (attention) на все эти взаимосвязи. Для этого нам просто надо поставить несколько attention слоев параллельно. Тогда каждый из них будет учить что-нибудь свое по аналогии со сверточными слоями.\n",
    "\n",
    "* Чтобы осуществить задуманное, вместо одного набора query будем использовать несколько независимых наборов.\n",
    "\n",
    "* Причем каждый набор будет считаться уникальной матрицей.\n",
    "\n",
    "* Аналогично сделаем для keys и values. Количество таких наборов внутри keys, queries, values должно быть **одинаковым**.\n",
    "\n",
    "* Обозначим это число как $J$, далее производим аналогичные манипуляции, при этом введем в параллель h таких функций attention.\n",
    "\n",
    "* На последнем шаге мы их соединяем (конкатинируем).\n",
    "\n",
    "* При этом можно заметить, что при таком подходе на каждом шаге размерность токена будет увеличиваться (если, например, в качестве и key, и value, и query мы подаем одно и тоже предсталение токена). Если хотим сохранять управление размерностью токена, то придется получать по меньшей мере value путем домножения на матрицу, размерность которой по второй оси меньше — **выполнять проекцию наших токенов в пространство меньшей размерности**.\n",
    "\n",
    "* В частности, можно подобрать размерность этого пространства таким образом, чтобы при конкатенации размерность полученного токена равнялась исходной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/multihead_self_attention_layer.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning with RNNs and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, основанные на внимании (attention), намного более продвинутые, нежели обычные нейросети. Они могут концентрироваться на отдельных частях данных, что позволяет избежать зашумления представлений.\n",
    "\n",
    "Идея состоит в том, что на каждом этапе генерации описания нейронная сеть в разной степени обращает внимание на те или иные фрагменты изображения, соответствующие следующему слову в описании.\n",
    "\n",
    "После обучения модели можно увидеть, что она как бы переносит своё внимание по изображению для каждого генерируемого слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/image_captioning_with_rnn_and_attention_example_step_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/image_captioning_with_rnn_and_attention_example_step_2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/image_captioning_with_rnn_and_attention_example_step_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/image_captioning_with_rnn_and_attention_example_step_4.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"http://cs231n.stanford.edu/slides/2022/lecture_11_ruohan.pdf\">Stanford University CS231n: lectures</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**А если картинки?**\n",
    "\n",
    "К примеру, у нас есть картинка. На этой картинки у нас есть области, которые можно описать одним словом — **key**. Например, фонарь/девушка/...\n",
    "\n",
    "Сами эти области — это **value**, которые введенным **key** соответствуют.\n",
    "\n",
    "Далее нам приходит **query**, например, running. Мы можем посчитать похожесть каждого из ключей, которые у нас есть, на query.\n",
    "\n",
    "И далее выдать информацию только по **value**, похожим на наш **query**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/key_query_value_example.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути мы описали то, как будем делать при помощи нейронных сетей питоновский словарь. С той разницей, что питоновский словарь может выдавать значения только для тех ключей, что в нем есть, а наш словарь выдает ответ для любого ключа-запроса, основываясь на его похожести на ключи словаря."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer для машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура сети Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/transformer_architecture.png\" width=\"450\">\n",
    "\n",
    "<em>Архитектура трансформера</em>\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1706.03762.pdf\"> Attention Is All You Need</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий пайплайн задачи машинного перевода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформер (англ. transformer) — это нейросетевая архитектура на основе моделей внимания и полносвязных слоёв, без RNN.\n",
    "\n",
    "**Схема преобразований данных в машинном переводе:**\n",
    "\n",
    "$S = (w_1, \\dots , w_n)$ — слова предложения на входном языке\n",
    "\n",
    "$\\color{blue}{\\downarrow \\quad \\text{обучаемая или предобученная векторизация слов}}$\n",
    "\n",
    "$X = (x_1, \\dots , x_n)$ — эмбеддинги слов входного предложения\n",
    "\n",
    "$\\color{blue}{\\downarrow \\quad \\text{трансформер-кодировщик}}$\n",
    "\n",
    "$Z = (z_1, \\dots , z_n)$ — контекстные эмбеддинги слов\n",
    "\n",
    "$\\color{blue}{\\downarrow \\quad \\text{трансформер-декодировщик, похож на кодировщика}}$\n",
    "\n",
    "$Y = (y_1, \\dots , y_m)$ — эмбеддинги слов выходного предложения\n",
    "\n",
    "$\\color{blue}{\\downarrow \\quad \\text{генерация слов из построенной языковой модели}}$\n",
    "\n",
    "$\\tilde S = (\\tilde w_1, \\dots , \\tilde w_m)$ — слова предложения на выходном языке\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура трансформера-кодировщика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порядок вычислений трансформера-кодировщика:\n",
    "\n",
    "1. Добавляются позиционные векторы $p_i$:\n",
    "\n",
    "$\\qquad \\large h_i = x_i + p_i;$\n",
    "\n",
    "$\\qquad \\large H = (h_1, \\dots, h_n).$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ x_i, \\ p_i, \\ h_i = 512, \\ dim \\ H = 512 \\times n$\n",
    "\n",
    "2. Многомерное самовнимание:\n",
    "\n",
    "$\\qquad \\large h^j_i = Attn(\\color{red}{W^j_q}h_i, \\color{red}{W^j_k}H, \\color{red}{W^j_v}H).$\n",
    "\n",
    "$\\qquad$ Размерность: $j = 1, \\dots, J=8, \\ dim \\ h^j_i = 64, \\ dim \\ W^j_q, \\ W^j_k, \\ W^j_k = 64 \\times 512 $\n",
    "\n",
    "3. Конкатенация:\n",
    "\n",
    "$\\qquad \\large h'_i =  MH_j (h^j_i) \\equiv [h^1_i, \\dots, h^J_i].$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ h'_i = 512$\n",
    "\n",
    "4. Сквозная связка + нормировка уровня:\n",
    "\n",
    "$\\qquad \\large h''_i =  LN(h'_i + h_i; \\color{red}{\\mu_1, \\sigma_1}).$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ h''_i, \\ \\mu_1, \\ \\sigma_1 = 512$\n",
    "\n",
    "5. Полносвязная 2-хслойная сеть FFN:\n",
    "\n",
    "$\\qquad \\large h'''_i = \\color{red}{W_2}ReLU(\\color{red}{W_1}h''_i + \\color{red}{b_1}) + \\color{red}{b_2}.$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ W_1 = 2048\\times512, \\ dim \\ W_2 = 512\\times2048$\n",
    "\n",
    "6. Сквозная связь + нормировка уровня:\n",
    "\n",
    "$\\qquad \\large z_i = LN(h'''_i + h''_i; \\color{red}{\\mu_2, \\sigma_2}).$\n",
    "\n",
    "$\\qquad$ Размерность: $dim \\ z_i, \\ \\mu_2, \\ \\sigma_2 = 512$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/transformer_encoder.png\" width=\"200\">\n",
    "\n",
    "<em>Архитектура трансформера-кодировщика</em>\n",
    "\n",
    "<em>Source: <a href=\"http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf\"> К.В. Воронцов, Машинное обучение: Обработка последовательностей и модели внимания</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности архитектуры трансформера-кодировщика:\n",
    "\n",
    "* вычисления параллельны по элементам последовательности $(x_1, \\dots, x_n) \\rightarrow (z_1,\\dots, z_n)$, что было бы невозможным в RNN;\n",
    "\n",
    "* N = 6 блоков $h_i \\rightarrow \\Box \\rightarrow z_i$ соединяются последовательно;\n",
    "\n",
    "* возможно использование предварительно обученных ембеддингов $x_i$;\n",
    "\n",
    "* возможно обучение эмбеддингов $x_i \\in \\mathbb{R}^d$ слов $w_i \\in V$:\n",
    "\n",
    "$\\qquad \\large x_i = \\color{red}{u_{w_i}}$ или в матричной записи $X_{d \\times n} = \\color{red}{U_{d \\times V}} \\cdot B_{V \\times n}$, где:\n",
    "\n",
    "$\\qquad V$ — словарь слов входных последовательностей,\n",
    "\n",
    "$\\qquad \\color{red}{U}$ — матрица обучаемых векторных представлений слов,\n",
    "\n",
    "$\\qquad b_{vi} = [w_i = v]$ — матрица бинарного (one-hot) кодирования;\n",
    "\n",
    "* нормировка уровня (Layer Normalization):\n",
    "\n",
    "$\\qquad  \\large x_i, \\ \\color{red}{\\mu}, \\ \\color{red}{\\sigma} \\in \\mathbb{R};$\n",
    "\n",
    "$\\qquad  \\large \\displaystyle LN_s(x; \\color{red}{\\mu}, \\ \\color{red}{\\sigma}) = \\color{red}{\\sigma_s} {{x_s - \\overline x} \\over \\sigma_x} + \\color{red}{\\mu_s}, \\ s = 1, \\dots, d;$\n",
    "\n",
    "$\\qquad \\displaystyle \\overline x = {1 \\over d} \\sum\\limits_{s}x_s$ и $\\displaystyle \\sigma^2_x = {1 \\over d} \\sum\\limits_{s}(x_s - \\overline x)^2$ — среднее и дисперсия $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "\n",
    "Единственный возможный минус — нейросеть не учитывает порядок слов в предложении при составлении embedding. Это может нам мешать. Например, если в предложении два it, то они часто относятся к разным словам. Поэтому хотелось бы уметь учитывать информацию о позиции. Для этого к $X$ при составлении $Q$ добавляется информация о позиции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делается это хитрым образом: мы добавляем к каждому значению исходного вектора токенов некую комбинацию $sin$ и $cos$ с разными параметрами. **Значения суммируются, а не конкатенируются.**\n",
    "\n",
    "Вектор $PE$, который мы будем добавлять к $X$, будет определяться по следующей формуле:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{pos, 2i} = \\sin \\left({\\dfrac {pos} {10000^{2i/d}}}\\right)$$\n",
    "\n",
    "$$p_{pos, 2i+1} = \\cos \\left({\\dfrac {pos} {10000^{2i/d}}}\\right)$$\n",
    "\n",
    "$pos$ &mdash; это позиция токена\n",
    "\n",
    "$d$ &mdash; количество размерностей токена\n",
    "\n",
    "$i$ &mdash; $i$-тая размерность токена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].detach()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(20)\n",
    "y = pe(\n",
    "    torch.zeros(1, 100, 20)\n",
    ")  # sequence of shape 100, every token of sequence has shape 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.arange(100), y[0, :, 0:4].data.numpy())\n",
    "plt.legend([\"dim %d\" % p for p in [1, 2, 3, 4]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате каждая позиция кодируется уникальным представлением. При этом представление позволяет легко находить слова на заданном расстоянии от исходного (у них будет одинаково значение сигнала по какой-то оси)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это помогает трансформеру достаточно уникальным образом определять каждую позицию и понимать относительное расстояние между разными токенами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура трансформера-декодировщика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторегрессионный синтез последовательности:\n",
    "\n",
    "$\\large y_0 = \\langle {BOS} \\rangle$ — эмбеддинг символа начала.\n",
    "\n",
    "Для всех $t = 1, 2, \\dots$ выполняется следующая последовательность вычислений:\n",
    "\n",
    "1. Маскирование \"данных из будущего\":\n",
    "\n",
    "$\\qquad \\large h_t = y_{t-1} + p_t;$\n",
    "\n",
    "$\\qquad \\large H_t = (h_1, \\dots, h_t).$\n",
    "\n",
    "2. Многомерное самовнимание:\n",
    "\n",
    "$\\qquad \\large h'_t = LN \\circ MH_j \\circ Attn(\\color{red}{W^j_q}h_t, \\color{red}{W^j_k}H_t, \\color{red}{W^j_v}H_t).$\n",
    "\n",
    "3. Многомерное внимание на кодировку $Z$:\n",
    "\n",
    "$\\qquad \\large h''_t = LN \\circ MH_j \\circ Attn(\\color{red}{W^j_q}h'_t, \\color{red}{W^j_k}Z, \\color{red}{W^j_v}Z).$\n",
    "\n",
    "4. Двухслойная полносвязная сеть:\n",
    "\n",
    "$\\qquad \\large y_t = LN \\circ FFN(h''_t).$\n",
    "\n",
    "5. Линейный предсказывающий слой:\n",
    "\n",
    "$\\qquad \\large p(\\tilde w | t) SoftMax_{\\tilde w}(\\color{red}{W_y}y_t + b_y).$\n",
    "\n",
    "Генерация $\\tilde w_t = argmax(p(\\tilde w | t))$ продолжается пока $\\tilde w_t \\neq \\langle {EOS} \\rangle$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/transformer_decoder.png\" width=\"350\">\n",
    "\n",
    "<em>Архитектура трансформера-декодировщика</em>\n",
    "\n",
    "<em>Source: <a href=\"http://www.machinelearning.ru/wiki/images/1/19/Voron-ML-Attention-slides.pdf\"> К.В. Воронцов, Машинное обучение: Обработка последовательностей и модели внимания</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Self-Attention Layer\n",
    "\n",
    "\n",
    "Допустим,  у нас стоит проблема, что мы не должны видеть часть слов в предложении — например, при генерации текста (по текущим словам предсказать следующее). Например, хотим сгенерировать фразу \"robot must obey orders\" на основе только первого слова.\n",
    "\n",
    "Если мы подадим в нейронную сеть во время обучения сразу все предложение, которое хотим генерировать, то у нас в первых словах будет \"протекать\" информация о предыдущих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/queries_keys_scores_before_softmax.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае мы можем просто на соответствующих местах матрицы $E$ поставить минус бесконечности. Тогда в эмбеддингах слов, которые не должны знать о каких-то словах, информации об этих словах не будет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/scores_before_softmax_apply_attention_mask_masked_scores_before_softmax.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате после SoftMax \"лишняя\" информация не будет использоваться при генерации ответа на query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/masked_scores_softmax_along_rows_scores.png\" width=\"800\">\n",
    "\n",
    "<em>Source: <a href=\"https://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря этому трюку у нас получается обучать transfomer по-прежнему как простую single-pass нейросеть, а не \"скатываться\" в RNN, где у нас возникнут проблемы с градиентами и временем работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Небольшая историческая справка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее часть текста основана на статье [GPT для чайников: от токенизации до файнтюнинга](https://habr.com/ru/articles/599673/).\n",
    "\n",
    "Качественный скачок в решении NLP-задачи произошёл благодаря рекуррентным сетям. Затем появился механизм внимания — **attention**, который применялся в RNN и давал огромный прирост качества.\n",
    "\n",
    "Далее статья _Attention Is All You Need_ показала, что attention отлично работет вовсе без RNN. В этой статье трансформер из кодировщика и декодировщика был обучен переводить текст, и делал это великолепно.\n",
    "\n",
    "Затем произошёл раскол: в OpenAI решили сконцентироваться на декодерах, а в Google — на энкодерах. Так появились первые **GPT** и **BERT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Так что же такое GPT?**\n",
    "* Это нейронная сеть для генерации (продолжения) текста.\n",
    "\n",
    "* Более строго — языковая модель, основанная на архитектуре трансформер и обученная в self-supervised режиме на огромном [корпусе](https://philology.by/about/yaskevich/corpus-linguistics-yaskevich) текстовых данных.\n",
    "\n",
    "**Оригинальные статьи про поколения GPT:**\n",
    "* [Improving Language Understanding by Generative Pre-Training (2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "* [Language Models are Unsupervised Multitask Learners (2019)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "* [Language Models are Few-Shot Learners (2020)](https://arxiv.org/pdf/2005.14165.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с GPT будем использовать предобученную модель. Лучший выбор для работы с трансформерами — библиотеки от **Hugging Face**: `transformers`, `tokenizers`, `datasets`.\n",
    "\n",
    "Hugging Face занимается стандартизацией применения трансформеров, а также хранит наборы весов и датасеты для различных NLP-задач. Воспользуемся русскоязычной моделью ruGPT3 и дообучим её.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим библиотеку Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем необходимую модель. API для различных моделей одинаковый, для подмены модели достаточно изменить название модели `model_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading and initialization of model and tokenizer\n",
    "model_name = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Языковое моделирование** — предсказание следующего слова (или части слова) с учётом предыдущего контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/yandex_search.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/599673/\">GPT для чайников: от токенизации до файнтюнинга</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы сгенерированное моделью продолжение текста было верным не только грамматически, но и семантически, модель должна хорошо понимать смысл изначального текста и, желательно, даже иметь знания о реальном мире.\n",
    "\n",
    "Эти внутренние знания позволяют модели отвечать на вопросы, суммаризировать текст, создавать диалоговые системы и многое другое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, если мы хотим при помощи языковой модели ответить на вопрос: **«Сколько будет 2+2?»**, то можем подать на вход модели следующий текст:\\\n",
    "`«Вопрос: Сколько будет 2+2? Ответ: … »`\\\n",
    "и естественным продолжением такого текста будет ответ на вопрос, поэтому модель допишет `«4»`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Вопрос: 'Сколько будет 2+2?'\\nОтвет:\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "out = model.generate(input_ids, do_sample=False, max_length=20, pad_token_id=20)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похожим способом можно кратко пересказывать тексты, если в конце дописывать `«TL:DR»`, т.к. модель во время обучения запомнила, что после этих символов идёт краткое содержание. Подбор модификаций текста называется **«Prompt Engineering»**. Такая простая идея позволяет решать практически неограниченное количество задач. Именно поэтому многие считают GPT-3 подобием сильного искусственного интеллекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как работает GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из ключевых этапов в обработке текста — **токенизация**. На этом этапе происходит разделение текста на отдельные единицы — предложения и слова. Затем создается словарь, в который заносятся уникальные лексемы, встретившиеся в корпусе или тексте. На этих этапах можно столкнуться с несколькими проблемами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема 1. Размер словаря**\n",
    "\n",
    "Самый простой способ токенизации — назначить каждому уникальному слову своё число. Но есть проблема: слов и их форм миллионы, и поэтому словарь таких слов получится чересчур большим, а это будет затруднять обучение модели.\n",
    "\n",
    "Можно разбивать текст не на слова, а на отдельные буквы (char-level tokenization), тогда в словаре будет всего несколько десятков токенов, НО в таком случае уже сам текст после токенизации будет слишком длинным, а это тоже затрудняет обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема 2. Богатая морфология**\n",
    "\n",
    "\"Нейросеть\", \"сетка\", \"сеть\" являются разными словами, но имеют схожий смысл. Эту проблему классически всегда решал этап **стемминга** (удаление суффикса, приставки, окончания) или **лемматизации** (приведение слова к канонической форме)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема 3. Сложные слова**\n",
    "\n",
    "Но все проблемы эти этапы не решают. В германских языках (в английском, немецком, шведском и т.д.) очень продуктивно образуются новые сложные слова. Значения таких слов выводятся из значения их элементов. Их можно создавать бесконечно долго, и большинство из них не зафиксировано в «бумажном» словаре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/swedish_word_example.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Пример шведского названия гаечного ключа для колеса мотоцикла</a></em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://sysblok.ru/nlp/7250/\">Как работает алгоритм токенизации текстов для нейросетей</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с этими языками сложность также возникает на этапе составления словаря. При составлении словаря модели ориентируются на частотность (например, сохраняем слово, если оно встретилось чаще пяти раз), поэтому не будут запоминать такое длинное и сложное слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема 4: Границы слова**\n",
    "\n",
    "Для нас, привыкших к языкам европейского типа, слово — это набор букв между пробелами и знаками препинания. Но в английском языке многие сложные слова пишутся раздельно, а в японском, наоборот, между словами вообще нет пробелов. Поэтому универсальный токенизатор создать было нелегко."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение — Byte Pair Encoding**\n",
    "\n",
    "Изначально алгоритм компрессии BPE позволяет моделям узнавать как можно больше слов при ограниченном объеме словаря.\n",
    "\n",
    "1.   Слово = последовательность токенов\n",
    "2.   Словарь = все токены\n",
    "3.   Повторять, пока не достигли ограничения на размер словаря:\n",
    "\n",
    "     Назначаем новым токеном объединение двух существующих токенов, которое\n",
    "встречается чаще других пар в корпусе (встречаются вместе)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В применении BPE возможны разные варианты. Один из естественных – идём по всем токенам по убыванию частоты, находим соответствующую последовательность символов в корпусе, заменяем на токен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L08/subword_tokenization.png\" width = \"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://alexanderdyakonov.wordpress.com/2019/11/29/токенизация-на-подслова-subword-tokenization/\">Токенизация на подслова (Subword Tokenization)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот же способ помогает решить **проблему** **OOV (out of vocabulary)**. В обучающей выборке может не быть слова *Unfriendly*, но поскольку **Unfriendly** = **Un** + **friend** + **ly**, мы можем рассчитывать, что сеть будет правильно обрабатывать / генерировать и слово целиком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/token_unfriendly.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.thoughtvector.io/blog/subword-tokenization/\">Subword Tokenization — Handling Misspellings and Multilingual Data</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но даже это иногда не самый оптимальный выбор. Чтобы сжать словарь ещё сильнее, для обучения GPT OpenAI использовали **byte-level BPE** токенизацию. Эта модификация BPE работает не с текстом, а напрямую с его байтовым представлением. Использование такого трюка позволило сжать словарь до всего-лишь ~50k токенов при том, что с его помощью всё ещё можно выразить любое слово на любом языке мира (и даже эмодзи)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading and initialization of model and tokenizer\n",
    "model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(device)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример токенизации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Нейронные сети - это очень просто и увлекательно\"\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "decoded_tokens = [tokenizer.decode([token]) for token in tokens]\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens: \", tokens)\n",
    "print(\"Decoded tokens: \", decoded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-level токенизатор **не гарантирует**, что для любого токена найдется **соответствующий** символ или слово. Некоторые **токены** **существуют** только **в комбинациях**. Так, представленные токены не декодируются по отдельности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode([167]))\n",
    "print(tokenizer.decode([245]))\n",
    "print(tokenizer.decode([256]))\n",
    "\n",
    "print(tokenizer.decode([167, 245, 256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При генерации продолжения текста с помощью GPT происходит следующее:\n",
    "\n",
    "1. Входной текст токенизируется в последовательность чисел (токенов).\n",
    "2. Список токенов проходит через Embedding layer (линейный слой) и преобразуется в список эмбеддингов.\n",
    "3. К каждому эмбеддингу прибавляется **positional embedding**.\n",
    "4. Список эмбеддингов проходит через несколько одинаковых блоков (Transformer Decoder Block).\n",
    "5. После того, как список эмбеддингов пройдёт через последний блок, эмбеддинг, соответствующий последнему токену, матрично умножается на всё тот же входной, но уже транспонированный Embedding Layer, и после применения SoftMax получается распределение вероятностей следующего токена.\n",
    "6. Из этого распределения выбирается следующий токен (например, с помощью argmax)\n",
    "7. Полученный токен добавляется к входному списку токенов, шаги 1-6 повторяются"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/gpt3.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/how-gpt3-works-visualizations-animations/\">How GPT3 Works — Visualizations and Animations</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от рекуррентных сетей, архитектура трансформера не чувствительна к порядку входных токенов, то есть при перемешевании слов местами выход будет получаться одинаковым (permutation invarience).\n",
    "\n",
    "Позиционное кодирование описывает позицию объекта в последовательности так,  что каждой позиции соответствует уникальное представление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Почему не используется одно число, например значение индекса?**\n",
    "\n",
    "Для длинных последовательностей индексы могут сильно увеличиваться по величине. Если вы нормализуете значение индекса так, чтобы оно лежало между $0$ и $1$, это может создать проблемы для последовательностей переменной длины, поскольку они будут нормализованы по-разному.\n",
    "\n",
    "Поэтому в GPT используется кодирование позиции в виде вектора, который прибавляется к эмбеддингу токена. Эти позиционные эмбеддинги можно как зафиксировать заранее (так делается в оригинальном трансформере, см. пример на картинке), так и обучать, как в случае GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/pos_encoding_visual.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Позиционные эмбеддинги оригинального трансформера</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/599673/\">GPT для чайников: от токенизации до файнтюнинга</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у вас есть входная последовательность длины $L$, и требуется задать положение $k$-того объекта в этой последовательности. Позиционное кодирование задается функциями синуса и косинуса различной частоты:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(k, 2i) = sin (\\frac{k}{2^{2i/d}})$$\n",
    "\n",
    "$$P(k, 2i+1) = cos (\\frac{k}{2^{2i/d}})$$\n",
    "\n",
    "где $k$ — позиция объекта в последовательности, $\\displaystyle 0\\leq k< \\frac{L}{2}$,\n",
    "\n",
    "$d$ — размерность выходного пространства эмбеддингов,\n",
    "\n",
    "$P(k,j)$ — функция, которая переводит позицию $k$ в индекс $(k,j)$ позиционной матрицы,\n",
    "\n",
    "$n$ — константа, обычно равно $10 000$ согласно статье *Attention is all You Need*,\n",
    "\n",
    "$i$ — индекс колонки, $0 \\leq i < d/2$, одинаково для синуса и для косинуса.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной блок GPT состоит из слоёв self-attention, нормализации, feed-forward и residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/decoder_block.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://ai-news.ru/2019/06/obobshennye_yazykovye_modeli.html\">Обобщенные Языковые Модели</a></em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Методы Генерации текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Языковая модель генерирует распределение вероятностей следующего токена. Однако способы генерации текста могут отличаться. Далее разберём, какие варианты бывают.\n",
    "\n",
    "Для наглядности применим основные методы для продолжения следующего текста  \\\n",
    "`'Определение: \"Нейронная сеть\" — это'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Определение: \"Нейронная сеть\" - это'\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидный вариант — ArgMax-генерация (жадный поиск). Выбирается максимально вероятный токен.\n",
    "\n",
    "При таком способе мы не получим разнообразного текста на один и тот же запрос, и, что ещё хуже, генерация может застревать в локальных минимумах и выдавать повторяющиеся фрагменты, например `the the the the ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArgMax is defaulf behaviour\n",
    "out = model.generate(input_ids, do_sample=False, max_length=30, pad_token_id=30)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Несколько более сложный и качественный способ сэмплирования — **beam search**. Каждый раз мы выбираем не один самый вероятный токен, а сразу несколько (`beam-size`), и дальше продолжаем поиск для каждого из выбранных токенов.\n",
    "\n",
    "Таким образом создаётся **граф** со сгенерированными **вариантами предложений**. Далее выбирается предложение с наибольшей **perplexity** (уверенностью модели в реалистичности текста).\n",
    "\n",
    "Обычно это приводит к высокой связности (когерентности) текста, но при этом к сухости и скучности текста. Также это не решает полностью проблему с повторениями кусочков текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/beam_search.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/599673/\">GPT для чайников: от токенизации до файнтюнинга</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation with beam-search\n",
    "out = model.generate(input_ids, do_sample=False, num_beams=5, max_length=30, pad_token_id=30)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сэмплирование с Температурой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы добавить тексту непредсказуемости и человечности, можно использовать вероятностное сэмплирование с температурой. Будет использоваться не самый вероятный токен, а случайный, с учётом распределения вероятностей.\n",
    "\n",
    "Параметр температуры позволяет контролировать степень случайности. При нулевой температуре метод совпадает с жадным сэмплированием, при  большой температуре токены будут выбираться полностью случайно. Обычно хорошо работает температура в диапазоне `0.8–2.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула модификации распределения вероятностей очень похожа на формулу распределения Больцмана: чем выше температура системы, тем больше \"размазывается\" распределение вероятностей её возможных состояний, отсюда слово \"температура\".\n",
    "\n",
    "$$p=softmax(log(p)/t)$$\n",
    "\n",
    "Стоит отметить, что случайная природа генерации будет иногда приводить к полностью некорректным результатам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(input_ids, do_sample=True, temperature=1.3, max_length=30, pad_token_id=30)\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сэмплирование с Ограничением Маловероятных Токенов (Nucleus sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ввести запрет на семплирование наименее вероятных токенов:\n",
    "\n",
    "* `top-k` зануляет все вероятности, кроме $k$ наибольших;\n",
    "\n",
    "* `top-p` оставляет минимальный набор токенов, причём сумма их вероятностей будет не больше $p$.\n",
    "\n",
    "`top-p` ограничение называют **Nucleus Sampling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=1.3,\n",
    "    top_k=20,\n",
    "    top_p=0.8,\n",
    "    max_length=30,\n",
    "    pad_token_id=30\n",
    ")\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение поколений GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT — Generative Pretraining of Transformers**, состоящая всего из 12 слоёв и обученная на 7000 книгах. Хотя длинные тексты генерировались неважно, при файнтюнинге на узкоспециализированные задачи отрабатывала лучше SOTA.\n",
    "\n",
    "Основным новшеством было доказательство того, что предобучение без учителя языковой модели позволяет добиться отличных результатов после дальшейшего файнтюнинга.\n",
    "\n",
    "Максимальный размер контекста — 512 токенов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вскоре после GPT-1 появился BERT, побивший её результаты. После чего OpenAI **увеличили количество слоёв в 10 раз** и довели **количество парамеров** до **1.5B**. Модель была дообучена на **8 миллионов сайтов**, суммарно на 40 Гб текста. Архитектрурно это та же модель с перемещёнными слоями нормализации.\n",
    "\n",
    "GPT-2 научилась писать длинные связные тексты и даже решать при помощи prompt engineering множество новых задач.\n",
    "\n",
    "Максимальный размер контекста — 1024 токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель **увеличилась ещё в 10 раз (175B параметров)**, объём датасета — 570 Гб текста. Архитектурно та же, изменения коснулись оптимизации attention.\n",
    "\n",
    "Теперь модель может писать рабочий программный код [(CODEX)](https://openai.com/blog/openai-codex/) и решать много других почти сверхъестественных задач ([воскрешать мёртвых ](https://futurism.com/openai-dead-fiancee)).\n",
    "\n",
    "Максимальный размер контекста — 2048 токенов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Файнтюнинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся моделью меньшего размера, чтобы она поместилась на GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from IPython.display import clear_output\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как происходит обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучающий текст нарезается на случайные блоки, которые составляются в последовательности из 1024 (2048 у GPT-3) токенов, разделяясь специальным `<|endoftext|>` символом. Во время обучения модель учится предсказывать (классифицировать) каждый токен в последовательности один за другим при помощи Cross-Entropy Loss.\n",
    "\n",
    "Так как входная последовательность всегда заполнена до конца, padding не используется. Но во время инференса длина входного текста может быть произвольной, поэтому надо явно указывать, чем паддить оставшиеся позиции. По дефолту использутеся тот же `<|endoftext|>`.\n",
    "\n",
    "В отдельных версиях GPT вышесказанное может модифицироваться. Например, в ruGPT3 гораздо больше специальных токенов: `<s\\>`, `<s>`, `<pad>`, `<unk>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучающие данные\n",
    "Будем учить GPT генерировать стихи Маяковского. В качестве обучающих данных возьмём всего лишь один стих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Дым табачный воздух выел.\n",
    "Комната —\n",
    "глава в крученыховском аде.\n",
    "Вспомни —\n",
    "за этим окном\n",
    "впервые\n",
    "руки твои, исступленный, гладил.\n",
    "Сегодня сидишь вот,\n",
    "сердце в железе.\n",
    "День еще —\n",
    "выгонишь,\n",
    "может быть, изругав.\n",
    "В мутной передней долго не влезет\n",
    "сломанная дрожью рука в рукав.\n",
    "Выбегу,\n",
    "тело в улицу брошу я.\n",
    "Дикий,\n",
    "обезумлюсь,\n",
    "отчаяньем иссеча́сь.\n",
    "Не надо этого,\n",
    "дорогая,\n",
    "хорошая,\n",
    "дай простимся сейчас.\n",
    "Все равно\n",
    "любовь моя —\n",
    "тяжкая гиря ведь —\n",
    "висит на тебе,\n",
    "куда ни бежала б.\n",
    "Дай в последнем крике выреветь\n",
    "горечь обиженных жалоб.\n",
    "Если быка трудом уморят —\n",
    "он уйдет,\n",
    "разляжется в холодных водах.\n",
    "Кроме любви твоей,\n",
    "мне\n",
    "нету моря,\n",
    "а у любви твоей и плачем не вымолишь отдых.\n",
    "Захочет покоя уставший слон —\n",
    "царственный ляжет в опожаренном песке.\n",
    "Кроме любви твоей,\n",
    "мне\n",
    "нету солнца,\n",
    "а я и не знаю, где ты и с кем.\n",
    "Если б так поэта измучила,\n",
    "он\n",
    "любимую на деньги б и славу выменял,\n",
    "а мне\n",
    "ни один не радостен звон,\n",
    "кроме звона твоего любимого имени.\n",
    "И в пролет не брошусь,\n",
    "и не выпью яда,\n",
    "и курок не смогу над виском нажать.\n",
    "Надо мною,\n",
    "кроме твоего взгляда,\n",
    "не властно лезвие ни одного ножа.\n",
    "Завтра забудешь,\n",
    "что тебя короновал,\n",
    "что душу цветущую любовью выжег,\n",
    "и су́етных дней взметенный карнавал\n",
    "растреплет страницы моих книжек…\n",
    "Слов моих сухие листья ли\n",
    "заставят остановиться,\n",
    "жадно дыша?\n",
    "Дай хоть\n",
    "последней нежностью выстелить\n",
    "твой уходящий шаг..\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке transformers есть готовые инструменты для подготовки датасета и даталодера. На вход нужен всего лишь один `.txt` файл с обучающим текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save text train data as .txt file\n",
    "train_path = \"train_dataset.txt\"\n",
    "with open(train_path, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Creating Dataset\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=64)\n",
    "\n",
    "# Сreating DataLoader (crop the text into optimal length pieces)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Для файнтюнинга нам необходим объект класса Trainer, который сделает всю работу за нас. Далее нужно будет всего лишь запустить `trainer.train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned\",  # The output directory\n",
    "    overwrite_output_dir=True,  # overwrite the content of the output directory\n",
    "    num_train_epochs=200,  # number of training epochs\n",
    "    per_device_train_batch_size=32,  # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    warmup_steps=10,  # number of warmup steps for learning rate scheduler\n",
    "    gradient_accumulation_steps=16,  # to make \"virtual\" batch size larger\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    optimizers=(\n",
    "        torch.optim.AdamW(model.parameters(), lr=1e-5),\n",
    "        None,\n",
    "    ),  # Optimizer and learnig rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат файнтюнинга\n",
    "Готово! Теперь давайте посмотрим, что же сочинит GPT в стиле Маяковского, если на вход подать такую строчку:\n",
    "\n",
    "\"Учим нейросеть за нейросетью!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability sampling with limit example\n",
    "text = \"Как же сложно учить матанализ!\\n\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        num_beams=2,\n",
    "        temperature=1.5,\n",
    "        top_p=0.9,\n",
    "        max_length=100,\n",
    "        pad_token_id=512\n",
    "    )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "print()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полезные ссылки**\n",
    "1. [GPT в картинках](https://habr.com/ru/post/490842/) — очень подробный разбор внутренней архитектуры GPT-2 с акцентом на иллюстрации.\n",
    "2. [Трансформер в картинках](https://habr.com/ru/post/486358/) — очень подробный разбор архитектуры Transformer с акцентом на иллюстрации.\n",
    "3. [Tokenizers tutorial](https://huggingface.co/docs/transformers/tokenizer_summary) — краткий разбор всех типов токенизаторов от Huggingface с примерами.\n",
    "4. [Как генерировать текст](https://huggingface.co/blog/how-to-generate) — обзор способов сэмплирования текста с помощью языковых моделей (бимсёрч и тд).\n",
    "5. [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) — оригинальная статья про первый трансформер.\n",
    "6. [GPT-1](https://openai.com/blog/language-unsupervised/) — статья в блоге OpenAI про GPT-1.\n",
    "7. [GPT-2](https://openai.com/blog/better-language-models/) — статья в блоге OpenAI про GPT-2.\n",
    "8. [GPT-3](https://openai.com/blog/gpt-3-apps/) — статья в блоге OpenAI про GPT-3.\n",
    "9. [WebGPT](https://openai.com/blog/improving-factual-accuracy/) — статья в блоге OpenAI про GPT-3, обученную гуглить.\n",
    "10. [Codex](https://openai.com/blog/openai-codex/) — статья в блоге OpenAI про GPT-3, обученную писать код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры применений Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Непосредственное применение разобранной архитектуры Encoder-Decoder для перевода текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/transformer_text_translation_example.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representations from Transformers )\n",
    "\n",
    "В случае BERT используется только Encoder часть\n",
    "\n",
    "Это нейросеть, предобученная на огромном корпусе английского текста.\n",
    "\n",
    "Перед ней ставили следующие задачи:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 1**\n",
    "\n",
    "На вход дается предложение. В нем выбрано $15\\%$ токенов, из которых:\n",
    "1. $80\\%$ замаскированы;\n",
    "2. $10\\%$ заменены случайным;\n",
    "3. $10\\%$ оставлены без изменений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 2**\n",
    "\n",
    "На вход даются два предложения. Необходимо определить, идет ли второе непосредственно за первым в тексте или нет (просто случайное предложение из корпуса).\n",
    "\n",
    "В результате на вход подается все в таком виде:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/bert.jpg\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1810.04805.pdf\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$CLS$ токен нужен для того, чтобы нейросети было, куда класть информацию обо всем предложении в целом.\n",
    "\n",
    "$SEP$ нужен просто для того, чтобы разделять два предложения (если мы подаем их два) и для того, чтобы отмечать окончание. Использование $SEP$ с двумя целями позволяет подавать в уже обученную нейросеть только одно предложение.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Language Model\n",
    "\n",
    "Для первой задачи используем Encoder-Decoder\n",
    "\n",
    "То есть у нас есть **Encoder**, который получает богатые представления, и добавленный только на время обучения **Decoder** (не attention, просто MLP).\n",
    "\n",
    "Именно Decoder отвечает за то, чтобы предсказывать пропущенные/замененные токены. Ошибка считается только по тем $15\\%$ токенов, для которых могли произойти изменения, а не по всему предложению.\n",
    "\n",
    "Как гарантируется, что модель не заменяет имевшиеся в предложение слова на другие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/masked_language_model.png\" width=\"800\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Sentence Prediction\n",
    "\n",
    "Для второй задачи — Classifier.\n",
    "\n",
    "При этом на вход ему подается только сам CLS токен\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/next_sentence_prediction.png\" width=\"800\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель на обеих задачах одновременно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning с BERT\n",
    "\n",
    "Обученную таким образом модель (оставляем только encoder), можно использовать для огромного числа других задач\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/transfer_learning_with_bert.jpg\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1810.04805.pdf\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot learning\n",
    "\n",
    "Более того, есть BERT, тренированная на большом числе разных языков.\n",
    "\n",
    "Это позволяет, например, сравнивать предложения из разных языков, хотя мы этому даже не учились. Поиграть можно [здесь](https://colab.research.google.com/github/deepmipt/dp_tutorials/blob/master/Tutorial_2_DeepPavlov_BERT_transfer_learning.ipynb#scrollTo=S1iqGcxUINyU). Веса будут грузиться ДОЛГО"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/zero_shot_learning_bert.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1810.04805.pdf\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Специализированные аналоги BERT\n",
    "\n",
    "Более того, можно тренировать BERT под строго определенные задачи — например, анализ текстов научных статей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/bert_specialized_analogs.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT (Generative Pretrained Transformer )\n",
    "\n",
    "В случае GPT используется только Decoder часть. Но теперь во всех частях используются masked attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/generative_pretrained_transformer_gpt.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это нейросеть обширно используется для задачи генерации текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает она следующим образом.\n",
    "\n",
    "На вход подается \"затравка\" — какой-то текст (набор токенов). Можно подать просто SOS (Start of sentence) токен, обозначающий начало предложения и больше не несущий никакой дополнительной информации.\n",
    "\n",
    "Сеть генерирует следующий токен. Добавляем его к входной последовательности и подаем этот удлиненный текст нейросети как вход.\n",
    "\n",
    "Делаем так до тех пор, пока не надоест/не достигнем максимальной длины предложения/не встретим символ окончания генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/gpt2_autoregression.gif\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как ее обучали? На самом деле, тоже unsupervised learning. Но теперь перед моделью ставится задача предсказывать по предыдущим словам в предложении текущее.\n",
    "\n",
    "Понятно, что такую модель можно сразу же идти и использовать для генерации, для которой она и училась.\n",
    "\n",
    "Однако затем ее можно применять для многих других задач (опять же, путем transfer learning):\n",
    "\n",
    "1. Классификации — подаем сразу все предложение, полученное представление используем для предсказания.\n",
    "\n",
    "2. Entailment (Определение логического следования) — даем изначальные данные, гипотезу, надо оценить, следует ли гипотеза из данных.\n",
    "\n",
    "3. Similarity — можем оценивать похожесть предложений. Так как это мы определяем порядок предложений, то, чтобы избежать неявной зависимости от порядка пропускания предложений, будем использовать результат пропускания через нейросеть обоих возможных порядков.\n",
    "\n",
    "4. Выбор варианта ответа на вопросы — может обучить нейросеть отвечать на вопросы с множественным выбором."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L08/out/gpt_classification_entailment_similarity.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно научить нейросеть отвечать на вопросы и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы можем даже текст переводить с помощью GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/decoder_only_transformer_translation.png\" width=\"800\">\n",
    "\n",
    "<em>Source: <a href=\"https://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто подаем предложение с токеном в конце, определяющим, на какой язык переводим"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогичной схеме можем научить нашу сеть [делать summary текста](https://arxiv.org/abs/2109.10862)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L08/decoder_only_summarization.png\" width=\"800\">\n",
    "\n",
    "<em>Source: <a href=\"https://jalammar.github.io/illustrated-gpt2/\">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Здесь](https://russiannlp.github.io/rugpt-demo/) можно поиграть с open-source русскоязычным аналогом GPT-3 от Сбера.\n",
    "\n",
    "А [здесь](https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb#scrollTo=e-NKauYvgTNG) — поиграть с GPT-J-6B, но уже в Collab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\">Хорошие источники</font>\n",
    "\n",
    "[Про трансформеры](https://www.notion.so/Transformers-969f4b27c48147778c1e2dbda0c83ce0)\n",
    "\n",
    "[Аннотированный трансформер](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "\n",
    "[Код множества моделей с красивыми комментариями](https://nn.labml.ai/)\n",
    "\n",
    "[Зоопарк BERT](https://ai.plainenglish.io/so-how-is-bert-different-ad43a42cab48)\n",
    "\n",
    "[Transformers in computer vision: ViT architectures, tips, tricks and improvements](https://theaisummer.com/transformers-computer-vision/)\n",
    "\n",
    "[Illustrated transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "[Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)\n",
    "\n",
    "[Open-source реализация GPT-3](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)\n",
    "\n",
    "[Transformer для русского языка](https://github.com/vlarine/transformers-ru)\n",
    "\n",
    "[NLP Course for you](https://lena-voita.github.io/nlp_course.html)\n",
    "\n",
    "[Курс по NLP от ШАД](https://github.com/yandexdataschool/nlp_course)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
