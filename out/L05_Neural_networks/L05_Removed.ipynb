{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L05_Removed.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L32hP9PlBxOf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JZPNtLR77XQ"
      },
      "source": [
        "##  Веса сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhy2rwoD77XR"
      },
      "source": [
        "В нейронной сети используются **веса**. (Иногда используют название синапсы по аналогии с человеческим мозгом). Веса сети - это вещественные числа (чаще от -1 до 1), которых характеризуют влияние входа на выход.\n",
        "\n",
        "**Нейрон** – базовая единица нейронной сети. У каждого нейрона есть определённое количество входов, куда поступают сигналы, которые суммируются с учётом значимости (веса) каждого входа. Далее сигналы поступают на входы других нейронов. Вес каждого такого «узла» может быть как положительным, так и отрицательным. Например, если у нейрона есть два 'входа', то у него есть и два  весовых значения, которые можно регулировать независимо друг от друга."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doFK6VRG77XR"
      },
      "source": [
        "###  Как вычислить результат работы нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xWTRp-I77XR"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-content/L05/img_license/nn__xor_example.png\"  width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adq78V7n77XS"
      },
      "source": [
        "Рассмотрим задачу классификации XOR, то есть на вход подадим 1 и 0, и будем ожидать 1 на выходе. Веса сети определим случайным образом:\n",
        "\n",
        "$I1=1\\quad I2=0$\n",
        "\n",
        "$w_1=0.45\\quad  w_2=0.78\\quad \n",
        "w_3=-0.12\\quad  w_4=0.13$\n",
        "\n",
        "\n",
        "$w_5=1.5\\quad  w_6=-2.3$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he0qCc--77XS"
      },
      "source": [
        "```python\n",
        "H1 = I1*W1+I2*W3 = 1*0.45+0*-0.12 = 0.45\n",
        "H2 = I1*W2+I2*W4 = 1*0.78+0*0.13 = 0.78\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4w9bn377XS"
      },
      "source": [
        "Для того чтобы значения H1 и H2 не выходили за предельные значения, используется функция активации "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLuUB4Bo77XT"
      },
      "source": [
        "```python\n",
        "H1_out = sigmoid(H1) = sigmoid(0.45) = 0.61\n",
        "H2_out = sigmoid(H2) = sigmoid(0.78) = 0.69\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Or3e4u-77XT"
      },
      "source": [
        "```python\n",
        "O1_in = 0.61*1.5+0.69*-2.3=-0.672\n",
        "O1_out = sigmoid(-0.672)=0.33\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Vi_Pqo77XT"
      },
      "source": [
        "Ответ нейронной сети O1_out = 0.33, а мы ожидали на выходе 1. О том, как скорректировать веса нужным образом будет рассказано в разделе о методе обратного распространения ошибки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeiY3F3_77XT"
      },
      "source": [
        "###  Смещение (bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN52K0Ad77XU"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-content/L05/img_license/why_add_bias_example.png\"  width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWI02AWy77XU"
      },
      "source": [
        "Рассмотрим простой пример. На вход нейрона подаётся вес, умноженный на входное значение. После применения функции активации, в зависимости от веса, при всевозможных значениях входа мы можем получить следующие графики:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cFDQ2BB77XU"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L05/non_bias_problem_plot.png\"  width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKVcR3Kn77XV"
      },
      "source": [
        "[A Biased Graph Neural Network Sampler](https://arxiv.org/pdf/2103.01089.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Vbl1bJ77XV"
      },
      "source": [
        "Но что если мы захотим чтобы при ```x=2``` чтобы сеть выводила ```0```, тогда без веса смещения эту задачу не решить.\n",
        "\n",
        "Просто изменить крутизну сигмоиды на самом деле не получится - вы хотите иметь возможность сдвинуть всю кривую вправо."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r569JRX77XV"
      },
      "source": [
        "**Смещение** (англ. bias) - это дополнительный коэффициент прибавляющийся к сумме входов, наличие смещения позволяет сдвинуть функцию активации влево или вправо, что может иметь решающее значение для успешного обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_2FaRr-77XW"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-content/L05/img_license/add_bias_example.png\"  width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c19P1VOl77XW"
      },
      "source": [
        "Тогда, при разных смещениях мы можем получить сдвинутые функции активации, что способствует лучшему обучению нейронной сети:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPJR2MnA77XW"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L05/after_add_bias_plot.png\"  width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVUHOer477Xs"
      },
      "source": [
        "###  Анимация работы метода обратного распространения ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUA9Zcib77Xt"
      },
      "source": [
        "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L05/backprop_animation.gif\"  width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOXXX3vn77Xy"
      },
      "source": [
        "##  Преимущества и недостатки метода"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfc8z4CD77Xy"
      },
      "source": [
        "Несмотря на многочисленные успешные применения метода обратного распространения, он не является универсальным решением. Больше всего неприятностей приносит неопределённо долгий процесс обучения. В сложных задачах для обучения сети могут потребоваться дни или даже недели, она может и вообще не обучиться. Причиной может быть одна из описанных ниже."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCyf52kD77Xy"
      },
      "source": [
        "* Паралич сети\n",
        "\n",
        "    В процессе обучения сети значения весов могут в результате коррекции стать очень большими величинами. Это может привести к тому, что все или большинство нейронов будут функционировать при очень больших значениях, в области, где производная функции очень мала. Так как посылаемая обратно в процессе обучения ошибка пропорциональна этой производной, то процесс обучения может практически замереть. \n",
        "    \n",
        "    В теоретическом отношении эта проблема плохо изучена. Обычно этого избегают уменьшением размера шага, но это увеличивает время обучения. Различные эвристики использовались для предохранения от паралича или для восстановления после него, но пока что они могут рассматриваться лишь как экспериментальные."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqcK8OCH77Xz"
      },
      "source": [
        "\n",
        "* Локальные минимумы\n",
        "\n",
        "    Метод градиентного спуска может застрять в локальном минимуме, так и не попав в глобальный минимум.\n",
        "\n",
        "    Обратное распространение использует разновидность градиентного спуска, то есть осуществляет спуск вниз по поверхности ошибки, непрерывно подстраивая веса в направлении к минимуму. \n",
        "    \n",
        "    Поверхность ошибки сложной сети сильно изрезана и состоит из холмов, долин, складок и оврагов в пространстве высокой размерности. Сеть может попасть в локальный минимум (неглубокую долину), когда рядом имеется гораздо более глубокий минимум. В точке локального минимума все направления ведут вверх, и сеть не способна из него выбраться. Основную трудность при обучении нейронных сетей составляют как раз методы выхода из локальных минимумов: каждый раз, выходя из локального минимума, снова ищется следующий локальный минимум тем же методом обратного распространения ошибки до тех пор, пока найти из него выход уже не удаётся."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXSolx7r77Xz"
      },
      "source": [
        "* Размер шага\n",
        "\n",
        "    Если размер шага фиксирован и очень мал, то сходимость слишком медленная, если же он фиксирован и слишком велик, то может возникнуть паралич или постоянная неустойчивость. Эффективно увеличивать шаг до тех пор, пока не прекратится улучшение оценки в данном направлении антиградиента и уменьшать, если такого улучшения не происходит. П. Д. Вассерман описал адаптивный алгоритм выбора шага, автоматически корректирующий размер шага в процессе обучения. В книге А. Н. Горбаня предложена разветвлённая технология оптимизации обучения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWIwLMkW77Xz"
      },
      "source": [
        "* Переобучение\n",
        "\n",
        "    Следует также отметить возможность переобучения сети (overfitting), что является скорее результатом ошибочного проектирования её топологии и/или неправильным выбором критерия остановки обучения. При переобучении теряется свойство сети обобщать информацию. Весь набор образов, предоставленных к обучению, будет выучен сетью, но любые другие образы, даже очень похожие, могут быть распознаны неверно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2J_lRfz77Wy"
      },
      "source": [
        "В математическом смысле функция активации преобразует результат работы нейрона в известный диапазон значений, например $(0;1)$.\n",
        "\n",
        "Историческим примером функции активации является пороговая функция активации, вдохновленная активацией нейронов, использовавшаяся в перцептронах &mdash; первых нейронных сетях.\n",
        "\n",
        "В биологических нейронных сетях функция активации определяется пороговым потенциалом, при достижении которого происходит возбуждение потенциала действия в клетке. В наиболее простой форме эта функция является двоичной — то есть нейрон либо возбуждается, либо нет. \n",
        "\n",
        "Таким же образом ведет себя пороговая функция активации, использовавшаяся в перцептронах &mdash; первых нейронных сетях:\n",
        "\n",
        "$$f(x) =\n",
        "\\begin{cases}\n",
        "0, &\\text{$x<b$} \\\\ \n",
        "1, &\\text{$x\\geq b$}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "<img src =\"https://edunet.kea.su/repo/EduNet-content/L05/img_license/threshold_function_plot.png\"  width=\"300\">\n",
        "\n",
        "Производная пороговой функции активации:\n",
        "\n",
        "$$f'(x) =\n",
        "\\begin{cases}\n",
        "0, &\\text{$x\\neq b$} \\\\ \n",
        "DNE, &\\text{$x= b$}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Главным недостатком пороговой функции активации является то, что поскольку производная пороговой функции неопределена при $x=b$, а во всех остальных случаях равна 0, не может быть использована для оптимизации параметров нейронной сети методом градиентного спуска, использующимися при обучении современных нейронных сетей. \n",
        "\n",
        "В настоящее время пороговая функция активации не используется, поскольку не удовлетворяет требованиям, которые предъявляются к современным нейронным сетям."
      ]
    }
  ]
}