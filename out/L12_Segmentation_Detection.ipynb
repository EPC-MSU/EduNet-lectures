{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Сегментация и детектирование</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи компьютерного зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе предыдущих занятий, мы рассмотрели задачу классификации изображений и различные модели, используемые при решении этой задачи, а также поняли, как применять их к реальным данным. Однако, компьютерное зрение решает не только задачу классификации, но и такие задачи как сегментация, детектирование, создание подписей к изображением и многие другие. \n",
    "\n",
    "Сегодня мы поговорим о первых двух упомянутых задачах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/0.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем говорить о способах решения задач компьютерного зрения, разберемся с форматами входных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset COCO — Common Objects in COntext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из наиболее популярных датасатов содержащий данные для сегментации и детектирования. Он содержит более трёхсот тысяч изображений, большая часть из которых размечена и содержит следующую информацию:\n",
    "- Категории\n",
    "- Маски\n",
    "- Ограничивающие боксы (*bounding boxes*)\n",
    "- Описания (*captions*)\n",
    "- Ключевые точки (*keypoints*)\n",
    "- И многое другое\n",
    "\n",
    "Формат разметки изображений, использованный в этом датасете, нередко используется и в других наборах данных. Как правило, он упоминается просто как \"COCO format\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с датасетом используется пакет `pycocotools`.\n",
    "\n",
    "[Подробнее о том как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим формат аннотаций на примере одной записи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"cat\"])  # cats IDs\n",
    "print('class ID(cat) = %i' % catIds[0])\n",
    "\n",
    "imgIds = coco.getImgIds(catIds=catIds)  # Filtering dataset by tag\n",
    "print(\"All images: %i\" % len(imgIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим метаданные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = coco.loadImgs(imgIds[0])  # 1 example\n",
    "img = img_list[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на изображение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконвертируем в PIL формат для удобства дальнейшей работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(pil_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категории в COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на примеры категорий в нашем датасете. Отобразим каждую 10ую категорию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = coco.loadCats(coco.getCatIds())  # loading categories\n",
    "num2cat = {}  \n",
    "print(\"COCO categories: \")\n",
    "for cat in cats:\n",
    "    num2cat[cat[\"id\"]] = cat[\"name\"]\n",
    "    if cat[\"id\"] in range(0, 80, 10):\n",
    "        print(cat[\"id\"], \":\", cat[\"name\"], end=\"   \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете также есть категория **0**. Ее используют для обозначения класса фона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также существуют надкатегории. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'cats[2]: {cats[2]}')\n",
    "print(f'cats[3]: {cats[3]}')\n",
    "\n",
    "nms = set([cat[\"supercategory\"] for cat in cats])\n",
    "print(\"COCO supercategories: \\n{}\".format(\"\\n\".join(nms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вернемся к метаданным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо метаданных нам доступна разметка ([подробнее о разметке](https://cocodataset.org/#format-data)), давайте её загрузим и отобразим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "plt.imshow(I)\n",
    "plt.axis(\"off\")\n",
    "coco.showAnns(anns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На изображении можно увидеть разметку пикселей изображения по классам. То есть, пиксели из объектов, относящихся к интересующим классам, приписываются к классу этого объекта. К примеру, можно увидеть объекты двух классов: \"cat\" и \"keyboard\". \n",
    "\n",
    "Давайте теперь посмотрим, из чего состоит разметка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_anns(anns):\n",
    "    for i, a in enumerate(anns):\n",
    "        print(f\"\\n#{i}\")\n",
    "        for k in a.keys():\n",
    "            if k == \"category_id\" and num2cat.get(a[k], None):\n",
    "                print(k, \": \", a[k], num2cat[a[k]])  # Show cat. name\n",
    "            else:\n",
    "                print(k, \": \", a[k])\n",
    "\n",
    "dump_anns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что аннотация изображения может состоять из описаний нескольких объектов, каждое из которых содержит следующую информацию:\n",
    "* `segmentation` - последовательность пар чисел ($x$, $y$), координат каждой из вершин \"оболочки\" объекта;\n",
    "* `area` - площадь объекта;\n",
    "* `iscrowd` - информация о том, находится в оболочке один объект или же несколько, но слишком много для пообъектной разметки (толпа людей, к примеру);\n",
    "* `image_id` - идентификатор изображения, к которому принаделжит описываемый объект;\n",
    "* `bbox` - *будет рассмотрен далее в ходе лекции*;\n",
    "* `category_id` - идентификатор категории, к которой относится данный объект;\n",
    "* `id` - идентификатор самого объекта.\n",
    "\n",
    "Попробуем посмотреть на пример, в котором `iscrowd = True` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (120, 60)\n",
    "\n",
    "catIds = coco.getCatIds(catNms=[\"people\"])\n",
    "annIds = coco.getAnnIds(catIds=catIds, iscrowd=True)\n",
    "anns = coco.loadAnns(annIds[0:1])\n",
    "\n",
    "dump_anns(anns)\n",
    "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(I)\n",
    "coco.showAnns(anns)  # People in the stands\n",
    "seg = anns[0][\"segmentation\"]\n",
    "print(\"Counts\", len(seg[\"counts\"]))\n",
    "print(\"Size\", seg[\"size\"])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы из `pycocotools`, можно с лёгкостью преобразовать набор вершин \"оболочки\" сегментируемого объекта в более удобный для отображения вид - в маску объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "msk = np.zeros(seg[\"size\"])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        msk = coco.annToMask(anns[i])\n",
    "        ax[row, col].imshow(msk, cmap = 'gray')\n",
    "        ax[row, col].set_title(num2cat[anns[i][\"category_id\"]])\n",
    "        ax[row, col].axis(\"off\")\n",
    "        i += 1\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых случаях, попиксельная разметка изображения может быть избыточной - к примеру, в случае если необхдимо посчитать количество человек на изображении, достаточно просто каким-то образом промаркировать каждого из них, после чего посчитать количество наших \"отметок\". Одним из вариантов маркировки является \"обведение\" объекта рамкой (`bounding boxes`), внутри которой он находится. Такая информация об объектах также сохранена в аннотациях формата COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "RGB_img = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "for i in range(len(anns)):\n",
    "    x, y, width, heigth = anns[i][\"bbox\"]\n",
    "    x, y, width, heigth = int(x), int(y), int(width), int(heigth)\n",
    "    if anns[i][\"category_id\"] == 1: # person\n",
    "        color = (255, 255, 255)\n",
    "    if anns[i][\"category_id\"] == 40: # glove\n",
    "        color = (0, 255, 0)\n",
    "    RGB_img = cv2.rectangle(RGB_img, (x, y), (x + width, y + heigth), color, 2)\n",
    "cv2_imshow(RGB_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{brown}{\\text{Дополнительная информация}}$ \n",
    "### Еще более глубокое понимание разметки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое [run-length encoding - RLE](https://en.wikipedia.org/wiki/Run-length_encoding)\n",
    "\n",
    "[Видео-разбор](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семантическая сегментация (*Semantic segmentation*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи:\n",
    "\n",
    "Предсказать класс для каждого пикселя.\n",
    "\n",
    "Входные данные маска: \n",
    "\n",
    "[ x,y - > class_num ] \n",
    "\n",
    "Выходные данные маска:\n",
    "\n",
    "[ x,y - > class_num ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способы предсказания класса для каждого пикселя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Наивный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейшим вариантом решения является использование так называемого \"скользящего окна\" - последовательное рассмотрение фрагментов изображения. В данном случае, интересующими фрагментами будут небольшие зоны, окружающие каждый из пикселей изображения. К каждому из данных фрагментов применяется свёрточная нейронная сеть, предсказывающая, к какому классу относится центральный пиксель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### б) Разумный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменить архитектуру используемой модели и убрать линейный слой в конце сети, сделав сеть полносвёрточной. При этом, важно сохранить пространственный размер (ширину и высоту) промежуточных и конечного представлений. Сделав в конечном представлении количество каналов равным количеству классов, можно использовать значения внутри каждой из карт активаций, как ненормированное значение вероятности отношения каждого из пикселей к тому или иному классу. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/4.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/5.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лекции про сверточные сети мы говорили о том, что свертку 1x1 можно рассматривать как полносвязанный слой. Именно так она и будет использоваться при этом способе сегментации, чтобы достичь необходимого количества каналов в конечном представлении.\n",
    "\n",
    "Проблемы:\n",
    "- нужно большое рецептивное поле, следовательно много слоев ($L$ раз свёртка $3\\times3$ $\\to$ рецептивное поле $(1+2L)\\times(1+2L)$);\n",
    "- очень медленно работает на полноразмерных картах активации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### в) Эффективный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/6.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В традиционной структуре свёрточной сети присутствует сжатие пространственных размеров, что позволяет стремительнее увеличивать рецептивные поля нейронов на более глубоких уровнях. Благодаря этому, нейросети может извлекать информацию о всё более сложных зрительных образах, встречающихся на изображениях. Данные признаки могут быть полезны при предсказнии принадлежности пикселей к классам, однако проблема заключается в том, что карты активаций меньше размера исходного изображения. Чтобы решить данную проблему, нейросеть завершают разжимающим блоком, позволяющим постепенно увеличивать карты активаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автокодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/7.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая архитектура довольно популярна и применяется не только для сегментации: \n",
    "\n",
    "- сглаживание шума;\n",
    "- снижение размерности $\\to$ вектор-признак;\n",
    "- генерация данных.\n",
    "\n",
    "Этому будет посвящена одна из следующих лекций, сейчас же мы детально рассмотрим разжимающий блок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение размеров изображений "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# upsample layers\n",
    "upsample_nn = torch.nn.Upsample(mode='nearest', scale_factor=2) # nearest\n",
    "upsample_bl = torch.nn.Upsample(mode='bilinear', scale_factor=2) # bilinear\n",
    "\n",
    "# define dummy tensor\n",
    "a = torch.tensor([[1,3,0,1],[3,3,3,7],[8,1,8,7],[6,1,1,1]]).float() \n",
    "a = a[None, None,:] # add axis\n",
    "\n",
    "interp_nn = upsample_nn(a)\n",
    "interp_bl = upsample_bl(a)\n",
    "\n",
    "# plot result\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15,5), sharex=True, sharey=True);\n",
    "sns.heatmap(a[0][0], annot=True, ax=ax[0], lw=1, cbar=False);\n",
    "sns.heatmap(interp_nn[0][0], annot=True, ax=ax[1], lw=1, cbar=False);\n",
    "sns.heatmap(interp_bl[0][0], annot=True, ax=ax[2], lw=1, cbar=False);\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "ax[0].set_title('Raster dataset')\n",
    "ax[1].set_title('Nearest neighbor interpolation')\n",
    "ax[2].set_title('Bilinear interpolation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/8-1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Билинейная интерполяция рассматривает квадрат $2\\times2$ известных пикселя, окружающих неизвестный. В качестве интерполированного значения используется взвешенное среднее этих четырёх пикселей. \n",
    "\n",
    "Отметим, что чем ближе интерполируемое значение находится к одному из известных пикселей, тем больший вес будет у значения этого пикселя при подсчёте взвешенной суммы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/8.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Периодически, столь небольшой контекст ($2\\times2$ известных пикселя) может быть не достаточен для сохранения важной информации о распределении цветов в окружающих пикселях (пример будет приведён далее). В таких случаях, можно попробовать использовать большее количество окружающих пикселей для интерполяции - к примеру, $4\\times4$. Такая интерполяция называется бикубической и имеет значительно более сложную формулу, чем билинейная, однако основной приницп остаётся тем же - чем ближе пиксель к пикселю с известным значением, тем больше \"вес\" последнего при интерполяции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/9.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample в Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для увеличения разрешения изображения, в PyTorch используется класс `Upsample`. В нём доступны все упомянутые методы интерполяции, а также трилинейная интерполяция - аналог билинейной интерполяции, используемый для работы с трёхмерными пространственными данными (к примеру, видео). Отметим, что методы интерполяции можно использовать как для самих изображений или видео, так и для карт активаций, появляющихся в процессе работы нейросети. Таким образом, мы можем использовать `Upsample` внутри нашего разжимающего блока."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/10.png\" width=\"800\">\n",
    "\n",
    "[[doc] nn.Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html)\n",
    "\n",
    "[[doc] nn.functional.interpolate](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html?highlight=interp#torch.nn.functional.interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "def upsample(pil, ax, mode=\"nearest\"):\n",
    "    tensor = TF.to_tensor(pil)\n",
    "    upsampler = nn.Upsample(scale_factor=2, mode=mode)\n",
    "    tensor_128 = upsampler(tensor.unsqueeze(0))\n",
    "    img_128 = TF.to_pil_image(tensor_128.squeeze()).convert(\"RGB\")\n",
    "    ax.imshow(img_128)\n",
    "    ax.set_title(mode)\n",
    "    ax.set_xlim(0, 20 * 2)\n",
    "    ax.set_ylim(20 * 2, 0)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "pic = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "pil_64 = pic.resize((64, 64))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(15, 5))\n",
    "ax[0].imshow(pil_64)\n",
    "ax[0].set_title(\"Resized image\")\n",
    "ax[0].set_xlim(0, 20)\n",
    "ax[0].set_ylim(20, 0)\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "\n",
    "upsample(pil_64, mode=\"nearest\", ax=ax[1])\n",
    "upsample(pil_64, mode=\"bilinear\", ax=ax[2])\n",
    "upsample(pil_64, mode=\"bicubic\", ax=ax[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в данном случае каждое из пространственных измерений изображения увеличилось в 2 раза, но при необходимости возможно использовать увеличение в иное количество раз!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxUnpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо свёртки, на этапе снижения размерности также используются слои pooling'а. Наиболее популярным вариантом является maxpooling, сохраняющий информацию о значении наибольшего элемента внутри сегментов. Для того чтобы обратить данную операцию субдискретизации, был предложен MaxUnpooling слой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/11-1.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный слой требует от max pooling'а сохранения индексов максимальных элементов внутри сегментов - при обратной операции, максимальное значение приписывается тому же элементу сегмента, в котором был максимальный элемент сегмента до соответствующей субдискретизации. Соответственно, каждому слою MaxUnpooling должен соответствовать слой субдискретизации, что визуально можно представить следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/11-2.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация к MaxPool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "[Документация к MaxUnpool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_show(tensor, title=\"\", ax=ax):\n",
    "    img = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
    "    ax.set_title(title + str(img.size))\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=2, return_indices=True)  # False by default(get indexes to upsample)\n",
    "unpool = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "pil = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].set_title(\"original \" + str(pil.size))\n",
    "ax[0].imshow(pil)\n",
    "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
    "print(\"Orginal shape\", tensor.shape)\n",
    "\n",
    "# Downsample\n",
    "tensor_half_res, indexes1 = pool(tensor)\n",
    "tensor_show(tensor_half_res, \"1/2 down \", ax=ax[1])\n",
    "\n",
    "tensor_q_res, indexes2 = pool(tensor_half_res)\n",
    "tensor_show(tensor_q_res, \"1/4 down \", ax=ax[2])\n",
    "print(\"Downsample shape\", indexes2.shape)\n",
    "\n",
    "# Upsample\n",
    "tensor_half_res1 = unpool(tensor_q_res, indexes2)\n",
    "tensor_show(tensor_half_res1, \"1/2 up \", ax=ax[3])\n",
    "\n",
    "\n",
    "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
    "tensor_show(tensor_recovered, \"full size up \", ax=ax[4])\n",
    "print(\"Upsample shape\", tensor_recovered.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем нужен pad?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "array = torch.ones((24, 24), dtype=int)\n",
    "sns.heatmap(array, annot=True, fmt=\"d\", ax=ax[0], cbar=False, vmin=0, vmax=1)\n",
    "print(\"Array size:\", array.size())\n",
    "\n",
    "array_padded = F.pad(array, pad=[4, 4])\n",
    "sns.heatmap(array_padded, annot=True, fmt=\"d\", ax=ax[1], cbar=False)\n",
    "print(\"Array size with padding:\", array.size())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose convolution\n",
    "\n",
    "Способы восстановления пространственных размерностей, которые мы рассмотрели, не содержали обучаемых параметров. Для повышения пространственного разрешения можно использовать аналог операции свёртки - транспонированную свёртку. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/13.png\" width=\"700\"></center>\n",
    "<center><em>Обычная свертка.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличие этого метода от свёртки заключается в том, что вместо перемножения сегмента изображения и фильтра с целью получить скаляр, используется домножение скаляра на фильтр с целью получения некого сегмента создаваемой карты активаций. Причём в зонах, где полученные значения \"накладываются\" друг на друга, будет найдена их сумма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/14.png\" width=\"700\">\n",
    "</center>\n",
    "<center><em>Upsample/transpose convolution.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/16.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про 2d свертки с помощью перемножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)\n",
    "\n",
    "[Документация к ConvTranspose2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/15.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяет 2D-транспонированный оператор свертки к входному изображению, состоящему из нескольких входных плоскостей.\n",
    "Этот модуль можно рассматривать как градиент Conv2d по отношению к его входу. Это также известно, как свертка с частичным шагом или деконволюция (последнее - это не правильный термин, что-то вроде \"Силиконовая долина\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "\n",
    "input = torch.randn(1, 16, 16, 16) # define dummy input\n",
    "print('Original size', input.shape)\n",
    "\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) # define downsample layer\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) # define upsample layer\n",
    "\n",
    "# let`s downsample and upsample input\n",
    "with torch.no_grad(): \n",
    "  output_1 = downsample(input)\n",
    "  print(\"Downsampled size\", output_1.size())\n",
    "\n",
    "  output_2 = upsample(output_1, output_size = input.size())\n",
    "  print(\"Upsampled size\", output_2.size())\n",
    "\n",
    "# plot results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "sns.heatmap(input[0, 0, :, :], ax=ax[0], cbar=False, vmin=-2, vmax=2)\n",
    "ax[0].set_title(\"Input\")\n",
    "sns.heatmap(output_1[0, 0, :, :], ax=ax[1], cbar=False, vmin=-2, vmax=2)\n",
    "ax[1].set_title(\"Downsampled\")\n",
    "sns.heatmap(output_2[0, 0, :, :], ax=ax[2], cbar=False, vmin=-2, vmax=2)\n",
    "ax[2].set_title(\"Upsampled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "\n",
    "Популярная архитектура для сегментации. Изначально была предложена в статье [U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597) для анализа  медицинских изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-20.png\" width=\"700\"></center>\n",
    "<center><em>Архитектура U-Net (Ronneberger et al., 2015).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Реализация на PyTorch](https://github.com/milesial/Pytorch-UNet)\n",
    "\n",
    "[U-Net на PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)\n",
    "\n",
    "[Блог-пост разбор](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит обратить особое внимание на серые стрелки на схеме: они соответствуют операции конкатенации копий ранее полученных карт активаций, по аналогии с DenseNet. Чтобы это было возможно, необходимо поддерживать соответствие между размерами карт активаций в процессах снижения и повышения пространственных размерностей. Для этой цели, изменения размеров происходят только при операциях `MaxPool` и `MaxUnpool` - в обоих случаях, в два раза. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/18.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/17.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После upsample блоков ReLU не используется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мультиклассовая сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net используется для одноклассовой сегментации, однако в некоторых случаях может понадобиться предсказать принадлежность пикселей к одному из нескольких классов. В таких случаях, как правило, используются иные модели, архитектуры некоторых из которых мы сейчас рассмотрим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обзор Fully Convolutional Network(2014)  \n",
    "\n",
    "Fully Convolutional Network\n",
    "для того что бы не было путаницы с Fully Connected Network\n",
    "последние именуют MLP (Multi Layer Perceptron)\n",
    "\n",
    "Примеры реализации: [1](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/) и [2](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
    "\n",
    "\n",
    "Предобученная модель была обучена на части датасета COCO train2017 (на 20 категориях, представленных так же в датасете  Pascal VOC). Использовались следующие классы:\n",
    "\n",
    "`['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/l12out.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FCN — Fully Convolutional Network (Semantic Segmentation)](https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает довольно просто. Берем любой *back-bone* (например `ResNet50` или `VGG16`) и прикручиваем слой `upsample` до нужных нам размеров в конце. Посмотрим, как оно работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "# fix seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "# load resnet50\n",
    "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
    "    pretrained=True, num_classes=21\n",
    ")\n",
    "\n",
    "classes = [\"__background__\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\",\n",
    "           ]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # ImageNet\n",
    "    ]\n",
    ")\n",
    "\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "input_tensor = transform(pil_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = fcn_model(input_tensor.unsqueeze(0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращаются 2 массива\n",
    "\n",
    "* out - каждый пиксель отражает ненормированную вероятность, соответствующую предсказанию каждого класса.\n",
    "\n",
    "* aux - содержит значения *auxillary loss* на пиксель. На инференсе `output['aux']` бесполезный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"output keys: \", output.keys())  # Ordered dictionary\n",
    "print(\"out: \", output[\"out\"].shape, \"Batch, class_num, h, w\")\n",
    "print(\"aux: \", output[\"aux\"].shape, \"Batch, class_num, h, w\")\n",
    "\n",
    "output_predictions = output[\"out\"][0].argmax(0)  # for first element of batch\n",
    "print(f'output_predictions: {output_predictions.shape}')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "indexes = output_predictions\n",
    "\n",
    "# plot all classes predictions\n",
    "fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
    "i = 0 # counter\n",
    "for row in range(4):\n",
    "    for col in range(5):\n",
    "        mask = torch.zeros(indexes.shape)\n",
    "        mask[indexes == i] = 255\n",
    "        ax[row, col].set_title(classes[i])\n",
    "        ax[row, col].imshow(mask)\n",
    "        ax[row, col].axis(\"off\")\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обзор DeepLabv3+(2018)\n",
    "\n",
    "DeepLab - семейство моделей для сегментации, значительно развивавшееся в течение четырёх лет. Основой данного рода моделей является использование **atros (dilated) convolutions** и, начиная со второй модели, **atros spatial pyramid pooling**, опирающейся на **spatial pyramid pooling**.\n",
    "\n",
    "[Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
    "\n",
    "[Реализация на PyTorch](https://pytorch.org/vision/stable/models.html#deeplabv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/19.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial pyramid pooling (SPP) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (He et al., 2014)](https://arxiv.org/abs/1406.4729)\n",
    "\n",
    "**Spatial Pyramid Pooling (SPP)** - это *pooling* слой, который устраняет ограничение фиксированного размера сети, т.е. CNN не требует входного изображения фиксированного размера. В частности, мы добавляем слой SPP поверх последнего сверточного слоя. \n",
    "\n",
    "Слой SPP объединяет признаки и генерирует выходные данные фиксированной длины, которые затем поступают в MLP (или другие классификаторы). Другими словами, мы выполняем некоторую агрегацию информации на более глубоком этапе иерархии сети (между сверточными слоями и полностью связанными слоями), чтобы избежать необходимости обрезать или деформировать изображение в начале."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-23.png\" width=\"700\"></center>\n",
    "<center><em>Схема SPP (He et al., 2014).</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В pytorch нет отдельного модуля, но результат может быть получен за счет применения нескольких AdaptiveMaxPool2d с разными размерами выходов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atros (Dilated) Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/16.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dilated convolution** (расширенные свертки) - это тип свертки, который \"раздувает\" ядро, вставляя отверстия между элементами ядра. Дополнительный параметр (скорость расширения, **dilation**) указывает, насколько сильно расширяется ядро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-26-1.png\" width=\"800\">\n",
    "\n",
    "[[doc] nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Atros example\n",
    "with torch.no_grad():\n",
    "    # define dummy input\n",
    "    input = torch.tensor([[[[1, 1, 1],\n",
    "                            [1, 1, 1],\n",
    "                            [1, 1, 1]]]], dtype=torch.float)\n",
    "    \n",
    "    # define conv layer, dilation = 1\n",
    "    conv = nn.Conv2d(1, 1, kernel_size=2, dilation=1, bias=False)\n",
    "    # define kernel weights\n",
    "    conv.weight = nn.Parameter(torch.tensor([[[[2, 2],\n",
    "                                               [2, 2]]]], dtype=torch.float))\n",
    "    output = conv(input)\n",
    "\n",
    "# plot results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0][0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    output[0, 0, :, :],\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input \\nshape: \"+str(input.shape))\n",
    "ax[1].set_title(\"Kernel \\nshape: \"+str(conv.weight.shape))\n",
    "ax[2].set_title(\"Output \\nshape: \"+str(output.shape))\n",
    "fig.suptitle(\"Dilation = 1\", y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dilation to 2\n",
    "conv = nn.Conv2d(1, 1, kernel_size=2, dilation=2, bias=False)  # Fell free to change dilation\n",
    "conv.weight = nn.Parameter(torch.tensor([[[[2, 2],\n",
    "                                           [2, 2]]]], dtype=torch.float))\n",
    "\n",
    "output = conv(input)\n",
    "\n",
    "# plot results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0][0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    output[0, 0, :, :].detach(),\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input \\nshape: \"+str(input.shape))\n",
    "ax[1].set_title(\"Kernel \\nshape: \"+str(conv.weight.shape))\n",
    "ax[2].set_title(\"Output \\nshape: \"+str(output.shape))\n",
    "fig.suptitle(\"Dilation = 2\", y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change input tensor\n",
    "input = torch.tensor([[[[0, 1, 0],\n",
    "                        [1, 1, 1],\n",
    "                        [0, 1, 0]]]], dtype=torch.float)\n",
    "output = conv(input)\n",
    "\n",
    "#plot results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0][0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    output[0, 0, :, :].detach(),\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input \\nshape: \"+str(input.shape))\n",
    "ax[1].set_title(\"Kernel \\nshape: \"+str(conv.weight.shape))\n",
    "ax[2].set_title(\"Output \\nshape: \"+str(output.shape))\n",
    "fig.suptitle(\"Dilation = 2\", y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{brown}{\\text{Дополнительная информация}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IoU - оценка точности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-52.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Intersection over Union](http://datahacker.rs/deep-learning-intersection-over-union/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-wise cross entropy loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/24.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "one_class_out = torch.randn(1, 1, 32, 32)\n",
    "one_class_target = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "loss = bce_loss(one_class_out, one_class_target)\n",
    "print(\"BCE\", loss)\n",
    "\n",
    "\n",
    "two_class_out = torch.randn(1, 2, 32, 32)\n",
    "two_class_target = torch.randint(1, (1, 32, 32))\n",
    "\n",
    "print(two_class_out.shape)\n",
    "print(two_class_target.shape)\n",
    "\n",
    "loss = cross_entropy(two_class_out, two_class_target)\n",
    "\n",
    "print(\"Cross entropy loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DiceLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/dice.jpeg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про семантическую сегментацию](https://www.jeremyjordan.me/semantic-segmentation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Soft Dice loss of binary class\n",
    "    Args:\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "       Returns:\n",
    "        Loss tensor\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=2, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = p  # pow degree\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        predict = predict.flatten(1)\n",
    "        target = target.flatten(1)\n",
    "\n",
    "        # https://pytorch.org/docs/stable/generated/torch.mul.html\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.epsilon\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.epsilon\n",
    "        loss = 1 - 2 * num / den\n",
    "\n",
    "        return loss.mean()  # over batch\n",
    "\n",
    "\n",
    "criterion = BinaryDiceLoss()\n",
    "output = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "\n",
    "target = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "soft_loss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n",
    "print(\"Loss\", soft_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детектирование (Object detection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/0.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектирование - задача компьютерного зрения, в которой модели требуется восстановить информацию об ограничивающих прямоугольниках объектов, относящихся к различным классам. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-05.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачу семантической сегментации мы решали через классификацию. Для детектирования разумно использовать регрессию.\n",
    "\n",
    "В случае для одного объекта можно обучить модель предсказывать числа:\n",
    "\n",
    "* координаты центра + ширину и высоту\n",
    "* координаты правого верхнего и левого нижнего углов\n",
    "* координаты вершин многоугольника ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "\n",
    "# load pretrained model\n",
    "resnet_detector = resnet18(pretrained=True)\n",
    "clear_output()\n",
    "# Change \"head\" to predict coordinates (x1,y1 x2,y2)\n",
    "resnet_detector.fc = nn.Linear(resnet_detector.fc.in_features, 4)  # x1,y1 x2,y2\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# This is a random example. Don't expect good results\n",
    "input = torch.rand((1, 3, 224, 224))\n",
    "target = torch.tensor([[0.1, 0.1, 0.5, 0.5]])  # x1,y1 x2,y2 or x,y w,h\n",
    "print(f'Target: {target}')\n",
    "output = resnet_detector(input)\n",
    "loss = criterion(output, target)\n",
    "print(f'Output: {output}')\n",
    "print(f'Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Recent Progress in Appearance-based Action Recognition (Humphreys et al., 2020)](https://arxiv.org/abs/2011.12619)\n",
    "\n",
    "На прошлом семинаре мы упоминали про модели, которые ищут ключевые точки на лице человека (MTCNN). Можно использовать тот же подход для поиска любых точек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-07.png\" width=\"700\"></center>\n",
    "<center><em>Примеры предсказывания точек (Humphreys et al., 2020)</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с ситуации, когда объект один."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/26.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводные:\n",
    "- изображение\n",
    "- координаты границ прямоугольника boundind box\n",
    "(x,y,w,h)\n",
    "- класс объекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/27.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобно задаче сегментации, мы можем попробовать создать решение на основе предобученного классификатора, на основе которого мы попробуем создать модель, решающую задачу детекции. Для этой цели модели придётся одновремено улучшать свои предсказания по определению расположения объекта (координат ограничивающего прямоугольника) и по классу найденного объекта. Чтобы при обучении модели скомбинировать две функции потерь, перед поиском градиента находится сумма данных ошибок и веса обновляются уже с учётом неё."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/29.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{brown}{\\text{Дополнительная информация}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression loss\n",
    "\n",
    "$\\displaystyle\\mathrm{MSE} = \\frac{\\sum^n_{i=1}\\left(y_i-y_i^p\\right)^2}{n}$ - L2/ MSE/ Mean Squared Error/ Среднеквадратичная ошибка \n",
    "\n",
    "\n",
    "$\\displaystyle\\mathrm{MAE} = \\frac{\\sum^n_{i=1}\\left|y_i-y_i^p\\right|}{n}$ - L1/ MAE/ Mean Absolute Error/ Средняя ошибка\n",
    "\n",
    "\n",
    "$\\displaystyle\\\n",
    "    L_{\\delta}(y, f(x))=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  \\frac{1}{2}\\left(y-f\\left(x\\right)\\right)^2 \\qquad &\\mathrm{for}\\  |y-f(x)| \\leq \\delta\\\\\n",
    "                  \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 \\qquad &\\mathrm{otherwise}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "  $ - Huber Loss/ Smooth Mean ABsolute Error/ Функция потерь Хьюбера\n",
    "\n",
    "\n",
    "$\\displaystyle L\\left(y,y^p\\right)=\\sum_{i=1}^n log\\left(cosh\\left(y^p_i-y_i\\right)\\right)$ - Log-Cosh Loss/ Логарифм гиперболического косинуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Regression Loss Functions All Machine Learners Should Know](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE vs MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber vs Log-cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multitask loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-14.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics](https://arxiv.org/pdf/1705.07115.pdf)\n",
    "\n",
    "[Пример реализации MultiTask learning](https://github.com/Hui-Li/multi-task-learning-example-PyTorch/blob/master/multi-task-learning-example-PyTorch.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектирование нескольких объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике, приходится чаще сталкиваться с одновременным детектированием нескольких объектов, нежели с детектированием одного объекта. Это может показаться проблемой, поскольку архитектура нейросети фиксирована, в связи с чем фиксировано и количество предсказываемых ответов, однако если для одного объекта мы предсказываем $k$ значений (к примеру, для предсказания `(x, y, w, h)`, $k = 4$), то для изображения с $N$ объектами будет необходимо предсказать $N \\cdot k$ значений, причём значение $N$ может меняться от изображения к изображению, что вызывает дополнительные трудности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-042.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stanford University CS231n: Detection and Segmentation](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Наивный способ решения: скользящее окно\n",
    "\n",
    "Одним из вариантов решения данной задачи является применение классификатора ко всем возможным местоположениям объектов. В данном случае, классификатор решает, есть ли на выбранном фрагменте изображения один из интересующих нас объектов или же нет - тогда фрагмент относится к классу \"фон\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-044.gif\" width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемой данного подхода является то, что приходится применять классификатор к огромному количеству различных фрагментов, что крайне дорого с точки зрения вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Эвристика\n",
    "\n",
    "Вместо того чтобы применять классификатор \"наобум\", можно для начала выбрать те области изображения, в которых вероятность нахождения объекта наиболее высока и запускать классификатор лишь для них. Данные области называются **regions of interest**, сокращённо - **ROI**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-049.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Найдите области “больших” (blobby) изображений, которые, вероятно, будут содержать объекты\n",
    "- Относительно быстрый запуск; Selective Search дает 2000 предложений за несколько секунд на CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective search\n",
    "\n",
    "Selective search - известный алгоритмический метод поиска **ROI**, целью которого было быстрое обнаружение объектов разного размера. Для этого использовалось последовательное объединение потенциальных **ROI** на основе комбинации четырёх метрик сходств предполагаемых областей интереса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-18.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "Возвращает порядка 2000 прямоугольников для изображения. С таким количеством уже можно работать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN - Region CNN \n",
    "Построена по такому принципу:\n",
    "\n",
    "- на изображении ищутся ROI \n",
    "- для каждого делается resize \n",
    "- каждый ROI обрабатывается сверточной сетью, которая предсказывает класс объекта, который в него попал"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-055.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме класса модель предсказывает смещения для каждого bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS\n",
    "\n",
    "Теперь возникает другая проблема: в районе объекта алгоритм генерирует множество ограничивающих прямоугольников (bounding box), которые частично перекрывают друг друга.\n",
    "\n",
    "Чтобы избавиться от них используется другой алгоритм\n",
    "Non maxima suppression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/31.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Его задача избавиться от bbox, которые накладаваются на истинный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/32.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки схожести обычно используется метрика IoU, а значение IoU, при котором bbox считаются принадлежащими одному объекту, является гиперпараметром (часто 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft NMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой способ решения данной задачи - Soft NMS. В процессе работы, он не удаляет наименее вероятные из перекрывающихся результатов детекции. Вместо этого, он, как и NMS, оставляет неизменным наиболее вероятный ограничивающий прямоугольник $\\mathcal{M}$, уменьшая вероятность обнаружения объекта внутри некого прямоугольника $b_i$ при увеличении значения $\\text{iou}$ прямоугольников $\\mathcal{M}$ и $b_i$. Простым вариантом является домножение вероятности $s_i$ на $(1 - \\text{iou}(\\mathcal{M}, b_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/33.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/34-1.png\" width=\"800\">\n",
    "\n",
    "[[doc] torchvision.ops.nms](https://pytorch.org/vision/stable/ops.html#torchvision.ops.nms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast R-CNN\n",
    "\n",
    "Проблемой описанного выше подхода является скорость.\n",
    "Так как мы вынуждены применять CNN порядка 2000 раз (в зависимости от эвристики, которая генерирует ROI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-26.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И решением является поиск ROI не на самом изображении, а на карте признаков, полученной после обработки всего изображения CNN. В таком случае большая часть сверток выполняется только один раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-27.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это радикально ускоряет процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Pooling\n",
    "\n",
    "Появляется новая задача - 'resize' ROI на карте признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/35.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация Roi Pooling](https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/36-1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-077.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорость работы CNN снизилась и теперь узким местом становится эвристика для поиска ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим Region Proposal Сеть (RPN) для прогнозирования предложения от функций\n",
    "\n",
    "\n",
    "\n",
    "Обучается с 4 лоссами:\n",
    "1. RPN классифицирует объект/не объект\n",
    "2. Координаты блока регрессии RPN\n",
    "3. Итоговая классификационная оценка (классы объекта)\n",
    "4. Окончательные координаты бокса "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/37.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Идея: пусть сеть сама предсказывает ROI по карте признаков**\n",
    "\n",
    "Для обучения требуется посчитать 4 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region proposal network\n",
    "\n",
    "Карта признаков имеет фиксированные и относительно небольшие пространственные размеры (например 20x15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/35.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому можно вернуться к идее скользящего окна которая была отвергнута в самом начале.\n",
    "\n",
    "При этом можно использовать окна нескольких форм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываются два значения:\n",
    "\n",
    "* вероятность того что в ROI находится объект\n",
    "* смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама сеть при этом может быть очень простой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/41.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате скорость увеличивается почти в 10 раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-086.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Модель на PyTorch](https://pytorch.org/vision/stable/models.html#faster-r-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "# load model\n",
    "fr_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    pretrained=True, progress=True, num_classes=91, pretrained_backbone=True\n",
    ")\n",
    "fr_rcnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# load data\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"]) # get category IDs\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(imgIds[12])  # http://images.cocodataset.org/val2017/000000370208.jpg\n",
    "img = img_list[0]\n",
    "\n",
    "# plot image\n",
    "plt.figure(figsize=(10, 10))\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "# plot boundy boxes\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('Image data: ')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from PIL import ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# lets predict objects by resnet50\n",
    "with torch.no_grad():\n",
    "    tensor = TF.pil_to_tensor(pil_img) / 255 # Normalize\n",
    "    output = fr_rcnn(tensor.unsqueeze(0))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    \n",
    "    # plot rectangles\n",
    "    for i, bbox in enumerate(output[0][\"boxes\"]):\n",
    "        if output[0][\"scores\"][i] > 0.5: \n",
    "            draw.rectangle((tuple(bbox[:2].numpy()), tuple(bbox[2:].numpy())), width=2)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two stage detector\n",
    "\n",
    "Faster RCNN = Two stage detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/42.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предложения по объединению ролей\n",
    "\n",
    "Первый этап: \n",
    "\n",
    "Запуск один раз для \n",
    "каждого изображения\n",
    "- Backbone network\n",
    "- Region Proposal Network\n",
    "- Region proposal network feature map\n",
    "\n",
    "Второй этап: \n",
    "\n",
    "Запуск один раз для каждого региона\n",
    "- Особенности обрезки: Rol pool / align\n",
    "- Предсказать класс объектов CNN\n",
    "- Prediction box offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На среднем и верхнем слое выполняются очень похожие операции. Разница в том, что на последнем слое предсказывается класс объекта, а промежуточном только вероятность его присутствия (objectness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Stage detector\n",
    "\n",
    "Если сразу предсказывать класс, то можно избавиться от второй стадии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/43.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детекторы работающие \"за один проход\":\n",
    "\n",
    "YOLO, SSD, RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/44.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Сравнение скорости моделей](https://pytorch.org/vision/stable/models.html#runtime-characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSD: Single Shot MultiBox Detector\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector (Liu et al., 2015)](https://arxiv.org/abs/1512.02325)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/45.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* модель VGG-16, предобученная на ImageNet\n",
    "* вручную определяем набор соотношений сторон, используемых для B ограничивающих прямоугольников в каждой ячейке сетки, а также смещения (x,y,w,h)\n",
    "* напрямую предсказывает вероятность того, что класс присутствует в bounding box.\n",
    "* есть класс для \"background\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retina Net - [Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/L9-43.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост: Что такое Focal Loss и когда его использовать](https://amaarora.github.io/2020/06/29/FocalLoss.html)\n",
    "\n",
    "Фокальные потери разработаны для решения одноэтапного сценария обнаружения объектов, когда во время обучения наблюдается крайний дисбаланс между классами переднего и заднего плана (например, 1:1000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature pyramid network\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection (Sergelius et al., 2016)](https://arxiv.org/abs/1612.0314)\n",
    "\n",
    "Это feature extractor для детекторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/18.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, как и в случае с сегментацией, точность повышается если делать предсказания на картах признаков разных масштабов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/46.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но первые слои содержат мало семантической информации (только низкоуровневые признаки). Из-за этого детектирование (и сегментация) мелких объектов удается хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/47.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея состоит в том, что бы делать предсказание с учетом семантической информации полученной на более глубоких слоях. \n",
    "\n",
    "\n",
    "При этом признаки суммируются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/48.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем к новым картам признаков может применяться дополнительная свертка.\n",
    "\n",
    "\n",
    "На выходе получаем карты признаков P2 - P5 на которых уже предсказываются bounding box.\n",
    "\n",
    "\n",
    "В случае 2-stage детектора (RCNN) карты подаются на вход RPN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/49.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А признаки для предсказаний используются из backbone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/12.jpg\" width=\"1300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetinaNet использует выходы FPN и для предсказаний класса и bbox. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-42.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO\n",
    "\n",
    "* [You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640) \n",
    "* [YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "* [YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "* [YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "* Июнь 2020. [YOLOv5 (Glenn Jocher)](https://github.com/ultralytics/yolov5)\n",
    "* Июль 2021. [YOLOX: Exceeding YOLO Series in 2021 (Ge et al., 2021)](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "Первые версии проигрывали конкурентам. Но проект развивался. В настоящий момент это, пожалуй, оптимальный детектор по соотношению качество распознавания/скорость.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительная информация\n",
    "\n",
    "Старые версии YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### YOLOv3\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/yolov3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOLO v3: Better, not Faster, Stronger](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b#:~:text=YOLO%20v2%20used%20a%20custom,more%20layers%20for%20object%20detection.&text=First%2C%20YOLO%20v3%20uses%20a,layer%20network%20trained%20on%20Imagenet.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### YOLOv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/yolov4.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOLO v4](https://medium.com/visionwizard/yolov4-version-3-proposed-workflow-e4fa175b902)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- SPP block\n",
    "- Dense Block\n",
    "- Больше слоев\n",
    "- online Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-47.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статья не публиковалась.\n",
    "Точность сравнима  с v4, но модель определенно лучше упакована."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"])\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(imgIds[5])\n",
    "img = img_list[0]\n",
    "\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Image data:\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка модели с Torch Hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load model from torch\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из коробки работает с изображениями в разных форматах и даже url, автоматически меняет размер входного изображения, возвращает объект с результатами ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Apply yolov5 model\n",
    "results = model(pil_img)\n",
    "results.print() # print predicted results\n",
    "results.save()  # image on disk\n",
    "\n",
    "print(f'\\nresults.xyxy type: {type(results.xyxy)}\\\n",
    "\\nlen(results.xyxy): {len(results.xyxy)}\\\n",
    "\\nresults.xyxy[0].shape: {results.xyxy[0].shape}')\n",
    "\n",
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import numpy as np\n",
    "\n",
    "# plot predicted results\n",
    "cv_img = np.array(pil_img)\n",
    "RGB_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "annos = results.pandas().xyxy[0]\n",
    "\n",
    "for i in range(len(annos)):\n",
    "    x_min, y_min, x_max, y_max = (\n",
    "        int(annos[\"xmin\"].iloc[i]),\n",
    "        int(annos[\"ymin\"].iloc[i]),\n",
    "        int(annos[\"xmax\"].iloc[i]),\n",
    "        int(annos[\"ymax\"].iloc[i]),\n",
    "    )\n",
    "\n",
    "    if annos[\"name\"].iloc[i] == \"person\":\n",
    "        color = (255, 255, 255)\n",
    "    if annos[\"name\"].iloc[i] == \"bicycle\":\n",
    "        color = (0, 0, 255)\n",
    "    if annos[\"name\"].iloc[i] == \"backpack\":\n",
    "        color = (0, 255, 0)\n",
    "    RGB_img = cv2.rectangle(RGB_img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "cv2_imshow(RGB_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако если подать на вход модели тензор, выходы радикально меняются. YOLO переходит в режим обучения, что довольно не очевидно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((1, 3, 416, 416))\n",
    "results = model(input)\n",
    "print(f'type(results): {type(results)}\\nlen(results): {len(results)}\\n')\n",
    "print(f'results[0].shape: {results[0].shape}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нard Example Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "В общем случае, при обучении с учителем набор данных представляет собой набор упорядоченных пар $(\\mathbf{x}_i, \\mathbf{y}_i)$, целью обучения является аппроксимация функции $f(\\mathbf{x}_i) = \\mathbf{y}_i$. \n",
    "\n",
    "Аппроксимация функции происходит путём уменьшения ошибки предсказания модели. Ожидаемо, модель может показывать \"хорошие\" результаты на **большей** части данных (изображения с одного и того же ракурса, сделанные при одном освещении), однако \"плохо\" справляться с нетипичными данными (изображения с другого ракурса, иное освещение). При этом, нетипичные данные не обязательно являются выбросами.\n",
    "\n",
    "К примеру, рассмотрим задачу обнаружения автомобилей на потоках данных с камер наружного видеонаблюдения. Если в обучающем наборе большая часть данных - снимки, сделанные днём, то качество работы модели ночью будет низким. В данном случае, \"нетипичными\" данными будут ночные снимки. Но, на самом-то деле, \"нетипичных\" случаев может быть довольно много, и некоторые из них могут происходить даже днём: \n",
    "* ночь (изменение освещения)\n",
    "* дождь, туман (изменение резкости, помехи на изображении)\n",
    "* смена сезона (снег либо листья могут покрыть дорогу - изменение фона)\n",
    "* и другие..."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Довольно простым и эффективным решением проблемы является сбор \"сложных\" случаев (**hard example mining**) и дообучение модели на них. При этом, поскольку модель уже довольно хорошо работает на большей части данных, можно дополнительно удалить часть данных из обучающей выборки - таким образом, мы сосредатачиваем модель на обучении на сложных примерах. \n",
    "\n",
    "К примеру, **SVM** - прекрасная модель для обучения на hard example: сложные примеры просто становятся опорными векторами, тогда как \"простые примеры\" по сути не используются при обучении. Однако давайте заметим, что модель придётся каждый раз переобучать на новом наборе сложных примеров, что довольно затратно.\n",
    "\n",
    "В различных реальных приложениях компьютерного зрения, довольно часто приходится обращаться к дообучению на сложных примерах (как раз из-за высокой вариативности данных). Однако, существуют и некоторые другие интересные способы борьбы с падением качества работы модели из-за вариативности данных."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Одним из интересных способов является уже упомянутый **focal loss**.  \n",
    "Суть его заключается в том, что при использовании кросс-энтропии, loss от большого количества \"хорошо распознанных\" примеров может быть более значительным, чем loss от малого количества \"плохо распознанных\" примеров. Итого, при обучении модель стремится снизить свою ошибку на большинстве и так уже неплохо предсказываемых примеров, не стремясь исправлять высокую ошибку на редких примерах (что может привести к её дальнейшему увеличению). \n",
    "\n",
    "Идея **focal loss** заключается в том, что можно попробовать снизить значения **cross-entropy loss** на итак уже неплохо предсказываемых примерах, чтобы дать модели возможность лучше обучиться на \"сложных\" для неё примерах. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для начала, давайте вспомним формулу кросс-энтропии:\n",
    "\n",
    "$$p_t = \n",
    "\\begin{cases}\n",
    "p &\\text{if }y=1\\\\\n",
    "1-p &\\text{otherwise,}\n",
    "\\end{cases}$$  \n",
    "$$\\text{CE}(p_t) = -\\log(p_t).$$\n",
    "\n",
    "Теперь давайте посмотрим на focal loss:\n",
    "\n",
    "$$\\text{FL}(p_t) = -(1 - p_t)^{\\gamma}\\log(p_t) = \\text{CE}(p_t) \\cdot (1 - p_t)^{\\gamma}.$$\n",
    "\n",
    "Отметим, что $(1 - p_t)^{\\gamma} = 1$ при $\\gamma = 0$, то есть focal loss в какой-то мере является обобщением cross-entropy loss.  \n",
    "Также, при $\\gamma > 0$, множитель является убывающей функцией, принимающей значения ниже единицы. Однако, значение множителя убывает достаточно быстро, чтобы влияние сложных примеров на loss повысилось относительно влияния простых примеров.\n",
    "\n",
    "Чтобы убедиться в этом, давайте построим графики focal loss при различных значениях $\\gamma$ в промежутке $(0; 1)$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(percent_correct):\n",
    "    return -np.log(percent_correct)\n",
    "\n",
    "def focal_loss(percent_correct, gamma = 5):\n",
    "    return cross_entropy(percent_correct) * (1 - percent_correct)**gamma "
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "eps = 10e-3 # small constant\n",
    "random_x = np.linspace(eps, 1-eps, 1000)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "gammas = [0] + [i for i in range(1, 20, 4)]\n",
    "for gamma in gammas:\n",
    "  fig.add_trace(go.Scatter(x=random_x, y=focal_loss(random_x, gamma),\n",
    "                      mode='lines',\n",
    "                      name=str(gamma)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Focal loss\",\n",
    "    xaxis_title=\"Correct percent\",\n",
    "    yaxis_title=\"Loss value\",\n",
    "    legend_title=\"Value of gamma\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как уже обсуждалось, проблема cross-entropy заключается в том, что большое количество примеров с небольшой ошибкой вносит такой же loss, как и малое количество примеров с большой ошибкой. Давайте попробуем в этом убедиться."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "p1 = 0.8 # вероятность для почти правильных предсказаний\n",
    "p2 = 0.4 # вероятность для проблемных предсказаний\n",
    "\n",
    "print(f\"При вероятности почти правильных предсказаний {p1} и вероятности проблемных предсказаний {p2}\\n\")\n",
    "\n",
    "for gamma in gammas:\n",
    "    fl1 = focal_loss(p1, gamma)\n",
    "    fl2 = focal_loss(p2, gamma)\n",
    "    print(f\"Для gamma = {gamma},\".ljust(15), f\"для равного loss с проблемным предсказанием, почти правильных требуется {int(fl2 / fl1)}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как видно, при увеличении значения $\\gamma$, в реальности можно достичь значительного роста \"важности\" примеров с высокой ошибкой, что по сути позволяет модели обращать внимание на \"hard examples\". "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online hard example mining"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "В некоторых случаях, hard exapmle mining можно выполнять прямо во время формирования батча, \"на лету\". В таких случаях, говорят про **online hard example mining**. \n",
    "\n",
    "Один из вариантов может быть реализован в two-stage детекторах.  \n",
    "Напоминаю: первая часть детектора вытупает за обнаружение regions of interest, затем выполняется (как правило, сравнительно вычислительно дешёвая) классификация. Одним из вариантов реализации идеи может быть выполнение forward pass классификатора по всем предложенным RoI, и затем формированию батча, в котором будет выделено определённое количество \"мест\" под RoI, предсказания на которых выполняются наихудшим образом."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-49.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог пост про Hard Mining Example](https://erogol.com/online-hard-example-mining-pytorch/)\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining (Shrivastava et al., 2016)](https://arxiv.org/abs/1604.03540)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors (Yu et al., 2018)](https://arxiv.org/abs/1804.04606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/0.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[COCO panoptic](https://cocodataset.org/#panoptic-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-03.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mask R-CNN** (Detectron) - концептуально простая, гибкая и общая схема сегментации объектов. Подход эффективно обнаруживает объекты на изображении и одновременно генерирует высококачественную маску сегментации для каждого объекта. \n",
    "\n",
    "Метод, названный Mask R-CNN, расширяет Faster R-CNN (который мы обсуждали ранее), добавляя ветвь для предсказания маски объекта параллельно с существующей ветвью для распознавания *bounding boxes*. \n",
    "\n",
    "Код доступен [по ссылке](https://github.com/facebookresearch/Detectron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/53.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Модель Mask R-CNN](https://pytorch.org/vision/stable/models.html#mask-r-cnn)\n",
    "\n",
    "[Пример запуска Mask R-CNN есть в документации Pytorch](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROI Align\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выравнивание области интереса, или **RoI Align**, - это операция извлечения небольшой карты признаков из каждого RoI (Region of Interest) в задачах обнаружения и сегментации. Она устраняет жесткое квантование *RoI pool*, правильно выравнивая извлеченные признаки с входными данными. Чтобы избежать квантования границ или бинов RoI, RoIAlign использует билинейную интерполяцию для вычисления точных значений входных признаков в четырех регулярно дискретизированных местах в каждом бине RoI, а затем результат агрегируется (используя max или average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/54.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/55-1.png\" width=\"800\">\n",
    "\n",
    "[[doc] torchvision.ops.roi_align](https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_align)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества детекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP - mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP (*Average Precision* - средняя точность) - это популярная метрика для измерения качества детекторов объектов, таких как Faster R-CNN, SSD и др. Средняя точность вычисляет среднее значение *precision* (точности) для значения *recall* от 0 до 1. Звучит сложно, но на самом деле довольно просто. Давайте разберем на конкретных примерах. Но перед этим давайте вкратце разберем что такое *precision*, *recall* и IoU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision & recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** измеряет, насколько точны предсказания сети (т.е. процент правильных предсказаний)\n",
    "\n",
    "**Recall** измеряет, насколько хорошо сеть находит все положительные срабатывания (*positives*). Например, мы можем найти 80% возможных положительных срабатываний в наших K лучших предсказаниях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот их математические определения:\n",
    "\n",
    "$\\displaystyle\\mathrm{Precision} = \\frac{TP}{TP+FP}$\n",
    "\n",
    "$\\displaystyle\\mathrm{Recall} = \\frac{TP}{TP+FN}$\n",
    "\n",
    "где $TP$ - True Positive, $TN$ - True Negative, $FP$ - False Positive, $FN$ - False Negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы пытаемся детектировать яблоки на фотографиях. Предположим, мы обработали 20 фотографий (на 10 фотографиях по одному яблоку и на 10 фотографиях яблок нет) и обнаружили что:\n",
    "\n",
    "* в 7 случаях наша нейросеть обнаружила яблоко там, где оно было на самом деле (True Positive),\n",
    "* в 3 случаях не обнаружила яблоко там, где оно было (False Negative),\n",
    "* в 4 случаях обнаружила яблоко там, где его не было (False Positive)\n",
    "* в 6 случаях правильно определила, что на фотографии яблок нет (True Negative),\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем precision и recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(TP, FP):\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "pres = precision(TP=7, FP=4)\n",
    "rec  = recall(TP=7, FN=3)\n",
    "\n",
    "print('Precision = %.2f' % pres)\n",
    "print('Recall = %.2f' % rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU (Intersection over union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU измеряет перекрытие между двумя границами. Мы используем его для измерения того, насколько сильно наша предсказанная граница совпадает с истиной (границей реального объекта). В некоторых наборах данных мы заранее определяем порог IoU (например, 0.5) для классификации того, является ли предсказание True Positive или False Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, рассмотрим предсказание сети для фотографии яблока:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L12/red_apple.jpg -O img.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "img = Image.open('img.jpg')\n",
    "array = np.array(img)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "x0 = 75\n",
    "y0 = 80\n",
    "w0 = 450\n",
    "h0 = 440\n",
    "\n",
    "ground_truth = Rectangle((x0, y0), w0, h0, linewidth=2, edgecolor='g', linestyle='--', facecolor='none')\n",
    "\n",
    "x1 = 90\n",
    "y1 = 120\n",
    "w1 = 500\n",
    "h1 = 310\n",
    "predicted = Rectangle((x1, y1), w1, h1, linewidth=2, edgecolor='b', facecolor='none')\n",
    "\n",
    "ax.add_patch(ground_truth)\n",
    "ax.add_patch(predicted)\n",
    "\n",
    "ax.imshow(array)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU определена как\n",
    "\n",
    "$\\displaystyle\\mathrm{IoU} = \\frac{\\text{area of overlap}}{\\text{area of union}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем *area of overlap* и *area of union*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1):\n",
    "    x0_max = x0 + w0\n",
    "    y0_max = y0 + h0\n",
    "    x1_max = x1 + w1\n",
    "    y1_max = y1 + h1\n",
    "    dx = min(x0_max, x1_max) - max(x0, x1)\n",
    "    dy = min(y0_max, y1_max) - max(y0, y1)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy\n",
    "\n",
    "def area_of_rectangle(w, h):\n",
    "    return w*h\n",
    "# Compute intersection areas of the predictions\n",
    "a_of_overlap = area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1)\n",
    "\n",
    "# Compute individual areas of the rectangles\n",
    "a_0 = area_of_rectangle(w0, h0)\n",
    "a_1 = area_of_rectangle(w1, h1)\n",
    "\n",
    "# Compute area of their union\n",
    "a_of_union = a_0+a_1-a_of_overlap\n",
    "\n",
    "print('Area of overlap = %i' % a_of_overlap)\n",
    "print('Area of union = %i' % a_of_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoU = a_of_overlap/a_of_union\n",
    "print('IoU = %.2f' % IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как будет меняться IoU в зависимости от качества предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plot_predictions_and_calculate_IoU(x1, y1, w1, h1):\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "    x0 = 75\n",
    "    y0 = 80\n",
    "    w0 = 450\n",
    "    h0 = 440\n",
    "\n",
    "    ground_truth = Rectangle((x0, y0), w0, h0, linewidth=2, edgecolor='g', linestyle='--', facecolor='none')\n",
    "    predicted = Rectangle((x1, y1), w1, h1, linewidth=2, edgecolor='b', facecolor='none')\n",
    "\n",
    "    ax.add_patch(ground_truth)\n",
    "    ax.add_patch(predicted)\n",
    "\n",
    "    ax.imshow(array)\n",
    "    ax.axis('off');\n",
    "\n",
    "    a_of_overlap = area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1)\n",
    "    a_0 = area_of_rectangle(w0, h0)\n",
    "    a_1 = area_of_rectangle(w1, h1)\n",
    "    a_of_union = a_0+a_1-a_of_overlap\n",
    "    IoU = a_of_overlap/a_of_union\n",
    "    print('IoU = %.2f' % IoU)\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_predictions_and_calculate_IoU, \n",
    "         x1 = widgets.IntSlider(min=0, max=array.shape[0], step=10, value=90), \n",
    "         y1 = widgets.IntSlider(min=0, max=array.shape[1], step=10, value=120), \n",
    "         w1 = widgets.IntSlider(min=0, max=array.shape[0], step=10, value=500),\n",
    "         h1 = widgets.IntSlider(min=0, max=array.shape[1], step=10, value=310)\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что чем лучше предсказание совпадает с реальностью - тем выше у нас значение метрики IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжая тему яблок, возьмем все те же 20 фотографий на 10 из которых яблоки были, а на 10 яблок не было. И представим, что у нас есть некая нейросеть, которая выдает следующие предсказания по всему датасету (или например по батчу):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nn_preds = pd.DataFrame({'IoU' : [0.6, 0.98, 0.4, 0.3, 0.1, 0.96, 0.7, 0.3, 0.2, 0.8],  \n",
    "                        'precision' : [1,1,0.67,0.5,0.4,0.5,0.57,0.5,0.44,0.5],\n",
    "                        'recall' : [0.2,0.4,0.4,0.4,0.4,0.6,0.8,0.8,0.8,1]})\n",
    "nn_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем считать, что если $\\mathrm{IoU} \\geq 0.5$ - то предсказание правильное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_preds['correct'] = False\n",
    "nn_preds['correct'][nn_preds['IoU'] >= 0.5] = True\n",
    "\n",
    "nn_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что precision имеет зигзагообразный характер - она снижается при ложных срабатываниях и снова повышается при истинных срабатываниях. \n",
    "\n",
    "Давайте построим график precision от recall и убедимся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision)\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По определению, что бы найти AP, нужно найти площадь под кривой recall-precision:\n",
    "\n",
    "$\\displaystyle AP = \\int_0^1p(r)dr$\n",
    "\n",
    "Господи, интеграл! Какая жуть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision и recall всегда находятся в пределах от 0 до 1. Поэтому $AP$ также находится в пределах от 0 до 1. Перед расчетом $AP$ для обнаружения объекта мы часто сначала сглаживаем зигзагообразный рисунок (на каждом уровне recall мы заменяем каждое значение precision максимальным значением точности справа от этого уровня отзыва)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_precision(nn_preds):\n",
    "    smooth_prec = []\n",
    "    for i in range(len(nn_preds.precision.values)):\n",
    "        max = nn_preds.precision.values[i:].max()\n",
    "        smooth_prec.append(max)\n",
    "    nn_preds['smooth_precision'] = smooth_prec\n",
    "    return nn_preds\n",
    "\n",
    "nn_preds = smooth_precision(nn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим как это выглядит на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision, label='precision', color='blue')\n",
    "ax.plot(nn_preds.recall, nn_preds.smooth_precision, label='smooth precision', color='red')\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем нам нужно сглаживание? Что бы снизить влияние случайных выбросов и \"прыжков\" в предсказаниях модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посчитаем-таки $AP$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision, label='precision', color='blue')\n",
    "ax.plot(nn_preds.recall, nn_preds.smooth_precision, label='smooth precision', color='red')\n",
    "ax.fill_between(nn_preds.recall, nn_preds.smooth_precision, \n",
    "                np.zeros_like(nn_preds.smooth_precision), \n",
    "                color='red', alpha=0.1,\n",
    "                label='Area under the curve')\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_ylim(0.35,1.05)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "AP = auc(nn_preds.recall, nn_preds.smooth_precision)\n",
    "\n",
    "print('AP = %.2f' % AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последних исследовательских работах, как правило, приводятся результаты только для набора данных COCO. Для COCO AP - это среднее значение (*mean*) по нескольким IoU (минимальный IoU, который следует считать положительным совпадением). AP@[.5:.95] соответствует среднему AP для IoU от 0.5 до 0.95 с шагом 0.05. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем посчитать mAP. Для этого посчитаем AP для каждого уровня IoU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nn_prediction_at_iou = []\n",
    "APs = []\n",
    "for iou in np.arange(0.5,1,0.05):\n",
    "    nn_preds_limited = nn_preds[nn_preds['IoU'] >= iou]\n",
    "    nn_preds_limited = smooth_precision(nn_preds_limited)\n",
    "    AP = auc(nn_preds_limited.recall, nn_preds_limited.smooth_precision)\n",
    "    APs.append(AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(10, 5), dpi=80)\n",
    "\n",
    "plt.plot(np.arange(0.5,1,0.05), APs, color='black')\n",
    "plt.axhline(np.mean(APs), color='red', ls='--', label='mAP')\n",
    "plt.xlabel('IoU')\n",
    "plt.ylabel('AP')\n",
    "plt.grid('on')\n",
    "plt.legend();\n",
    "\n",
    "print('mAP@[0.5:0.95] = %.2f' % np.mean(APs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть несколько различных определений mAP, которые разняться от соревнования к соревнованию (суть одинаковая, но разница в деталях подхода), поэтому для каждого соревнования лучше использовать их собственные библиотеки для расчета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO — Self-supervised representation learning (with segmentation capabilities)\n",
    "[Emerging Properties in Self-Supervised Vision Transformers (Caron et al., 2021)](https://arxiv.org/abs/2104.14294)\n",
    "\n",
    "[Отличное видео объяснение статьи](https://www.youtube.com/watch?v=h3ij3F3cPIk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала подгрузим модель DINO (self-**DI**stillation with **NO** labels)\n",
    "\n",
    "*Перед запуском необходимо сбросить среду выполнения!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/dino.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузим случайную картинку (можно выбрать любую, просто замените ссылку на свою)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "URL = 'https://edunet.kea.su/repo/EduNet-web_dependencies/L12/capybara_image.jpg'\n",
    "!wget $URL -qO test.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты DINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, что с ней может сделать DINO, а затем обсудим как это работает и что вообще происходит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!python /content/dino/visualize_attention.py --image_path /content/test.jpg \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока не вдаваясь в детали, посмотрим на картинки которые генерирует DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def img_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "    fig,ax = plt.subplots(nrows=rows, ncols=cols, figsize=(28,8))\n",
    "    for num, img in enumerate(imgs):\n",
    "        img_PIL = Image.open(img)\n",
    "        ax[num].imshow(img_PIL)\n",
    "        ax[num].set_xticks([])\n",
    "        ax[num].set_yticks([])\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "img_grid(imgs=sorted(glob('*.png'))[::-1], rows=1, cols=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Принцип работы "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы понять, что мы видим, давайте разберем архитектуру DINO и посмотрим, как ее обучали. На самом деле, DINO не столько архитектура, сколько метод - то есть в качестве backbone можно использовать любую нейросеть (например, ResNet или ViT). Самые лучшие результаты показали DINO на основе VIT, соответственно в этом блокноте будем разбирать именно эту конфигурацию. Также для понимания принципа работы DINO, кратко разберем, что такое дистилляция знаний ([knowledge distillation](https://arxiv.org/abs/2006.05525)) и как применяется. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала, давайте вспомним как работает ViT. **Vi**sual **T**ransformer получает на вход картинку, разбитую на кусочки (*patches*) размером 8x8 или 16x16 пикселей. Затем эти кусочки расправляются в вектор (*flatten*) и пропускаются через энкодер трансформера. Поверх трансформера прикручена MLP голова, которая собирает информацию с голов трансформера и предсказывает класс для картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/l12out2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[vision_transformer](https://github.com/google-research/vision_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся с понятием дистилляция знаний нейронных сетей. \n",
    "\n",
    "Представим, что у нас есть обученная сеть с большим количеством параметров (весов) и мы хотели бы использовать ее с меньшим количеством параметров (получить более легковесную версию сети, например для использования на мобильном устройстве). Один из подходов - обучить маленькую модель имитировать предсказания большой модели. Т.е., мы можем попробовать на предсказаниях большой модели (*teacher*), обучить легковесную модель (*student*).\n",
    "\n",
    "Идея в том, что большие модели содержат больше информации (обладают бОльшим знанием), но не вся эта информация используется, поэтому с помощью дистилляции мы сможем вытащить наиболее важную информацию для предсказаний.\n",
    "\n",
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/L12/img/distillation.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DINO - это self-supervised метод, а значит классы ему недоступны. Что же делает эта сеть?\n",
    "\n",
    "Разберем по шагам. DINO на вход получает изображение $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Image.open('/content/img.png')\n",
    "input_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше к этому изображению применяются две различные аугментации - $x_1$ и $x_2$"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_x1 = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(size=(128, 128)),\n",
    "        transforms.ColorJitter(\n",
    "            brightness = np.random.uniform(0.5,1),\n",
    "            contrast = np.random.uniform(0.5,1),\n",
    "            saturation = np.random.uniform(0.5,1)\n",
    "        ),\n",
    "        transforms.GaussianBlur((1,5),(0.1,2))\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_x2 = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(size=(128, 128)),\n",
    "        transforms.ColorJitter(\n",
    "            brightness = np.random.uniform(0.5,1),\n",
    "            contrast = np.random.uniform(0.5,1),\n",
    "            saturation = np.random.uniform(0.5,1)\n",
    "        ),\n",
    "        transforms.GaussianBlur((1,3),(0.1,1))     \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "aug_image_x1 = transform_x1(input_img)\n",
    "aug_image_x2 = transform_x2(input_img)\n",
    "\n",
    "fig,ax = plt.subplots(ncols=2,figsize=(10,20))\n",
    "ax[0].imshow(aug_image_x1)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(aug_image_x2)\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее каждая из этих аугментаций проходит через свою собственную версию **ViT** - *student* и *teacher*. Однако, в DINO, teacher и student имеют похожую архитектуру (учитель не является большой сетью, а студент маленькой). \n",
    "\n",
    "Обучается именно студент, а веса учителя обновляются как **E**xponential **M**oving **A**verage (ema) весов студента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img_licence/65.png\" width=\"450\">\n",
    "\n",
    "На схеме можно заменить, что используется *centering* при прогоне через учителя, фактически это позволяет держать значения в диапозоне (похоже на нормализацию), чтобы избежать коллапса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конечном итоге обе ветки сети выдают какое-то представление (*representation*) данных, которое, как мы надеемся, будет близко для похожих изображений и далеко для непохожих. Давайте на него посмотрим"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import sys\n",
    "mod_name = 'utils'\n",
    "if mod_name in sys.modules:\n",
    "    del sys.modules[mod_name]\n",
    "    \n",
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', force_reload=True, skip_validation=True)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Transform each augmented image into a tensor\n",
    "x1_tensor = transform(aug_image_x1).unsqueeze(0) \n",
    "x2_tensor = transform(aug_image_x2).unsqueeze(0)\n",
    "\n",
    "# Get representation from DINO\n",
    "x1_representation = vits16(x1_tensor)\n",
    "x2_representation = vits16(x2_tensor)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "ax[0].imshow(x1_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[1].imshow(x2_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[0].set_title('$x_1$');\n",
    "ax[1].set_title('$x_2$');\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит и впрямь довольно похоже. А что если мы засунем туда что-то совершенно непохожее?"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "URL = 'https://edunet.kea.su/repo/EduNet-content/L12/img_license/robots.jpg'\n",
    "!wget $URL -qO robots.jpg\n",
    "\n",
    "input_img = Image.open('/content/robots.jpg')\n",
    "input_img"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transform_x3 = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomCrop(size=(128, 128)),\n",
    "     transforms.RandomSolarize(0.5, p=1)\n",
    "     ]\n",
    "    )\n",
    "\n",
    "aug_image_x3 = transform_x3(input_img)\n",
    "\n",
    "aug_image_x3"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x3_tensor = transform(aug_image_x3).unsqueeze(0) \n",
    "\n",
    "x3_representation = vits16(x3_tensor[:,:3,:,:])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(10,5))\n",
    "ax[0].imshow(x1_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[1].imshow(x2_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[2].imshow(x3_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[0].set_title('$x_1$');\n",
    "ax[1].set_title('$x_2$');\n",
    "ax[2].set_title('$x_3$');\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, выглядит не очень похоже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss для такой сети можно записать следующим образом: \n",
    "\n",
    "$\\text{loss} = H(t_1, s_2)/2 + H(t_2, s_1)/2$,\n",
    "\n",
    "где, $ H(a, b) = −a \\space log(b)$\n",
    "\n",
    " $t_i$ - выученное представление i-ой аугментации teacher network и $s_i$ -  выученное представление i-ой аугментации student network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте вновь посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid(imgs=sorted(glob('*.png'))[::-1], rows=1, cols=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим 6 карт внимания (*self-attention maps* - веса слоя self-attention) на 6 головах Visual Transformer. В результате self-supervised обучения по методике DINO, трансформер **САМОСТОЯТЕЛЬНО** придумал обращать внимание на различные части изображения, таким образом, производя семантическую сегментацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем все эти карты внимания объединить в одно изображение, и просто назначить каждой карте свой цвет, а в качестве прозрачности использовать интенсивность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "def overlay(img, segmentations):\n",
    "    img_PIL = Image.open(img)\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10,10))\n",
    "    ax[0].imshow(img_PIL)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    \n",
    "    ax[1].imshow(img_PIL.convert('LA'), alpha=0.5)\n",
    "    for num, img in enumerate(segmentations):\n",
    "        segment_PIL = Image.open(img).convert('LA')\n",
    "        segment_arr = np.array(segment_PIL)\n",
    "        colors = [(*cm.tab10(num)[:-1], c) for c in np.linspace(0,0.75,100)]\n",
    "        cmap = mcolors.LinearSegmentedColormap.from_list('mycmap', colors, N=5)\n",
    "        ax[1].imshow(segment_arr[:,:,0], cmap=cmap)\n",
    "    ax[1].set_facecolor('black')\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "overlay('/content/img.png', sorted(glob('*.png'))[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что DINO сегментирует разные части нашей картинки на разные семантические группы. В случае с капибарой - это голова, лицо (нос, глаза) и тело."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация видео"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы думали что на этом все? Нет, DINO может еще удивить. Например она умеет сегментировать видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install --upgrade youtube_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем видео (можно любое)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://edunet.kea.su/repo/EduNet-web_dependencies/L12/cheetah_video.mp4'\n",
    "!youtube-dl -o video.mp4 $URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сгенерируем сегментированное видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!python /content/dino/video_generation.py --input_path /content/video.mp4 --output_path  /content/video_segmented --resize 360 640\n",
    "!ffmpeg -i /content/video.mp4 -vf scale=640:360 /content/video_scaled.mp4\n",
    "!ffmpeg \\\n",
    "  -i /content/video_scaled.mp4 \\\n",
    "  -i /content/video_segmented/video.mp4 \\\n",
    "  -filter_complex '[0:v]pad=iw*2:ih[int];[int][1:v]overlay=W/2:0[vid]' \\\n",
    "  -map '[vid]' \\\n",
    "  -c:v libx264 \\\n",
    "  -crf 23 \\\n",
    "  -preset veryfast \\\n",
    "  output.mp4\n",
    "\n",
    "clear_output()\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось. Обратите внимание, эта сеть обучалась в режиме self-supervision!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open('/content/output.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=800 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще DINO умеет кластеризовать изображения. Выполнять не будем, так как процесс не быстрый, но можем посмотреть на результаты из их статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/1.gif\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Advancing the state of the art in computer vision with self-supervised Transformers and 10x more efficient training](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"6\">Список использованной литературы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Подробнее о том как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch).\n",
    "\n",
    "[Подробнее о разметке COCO](https://cocodataset.org/#format-data)\n",
    "\n",
    "[Что такое  run-length encoding - RLE](https://en.wikipedia.org/wiki/Run-length_encoding)?\n",
    "\n",
    "[Видео-разбор Run-length encoding](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Семантическая сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FCN Semantic segmenation](https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1)\n",
    "\n",
    "[Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
    "\n",
    "[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (He et al., 2014)](https://arxiv.org/abs/1406.4729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Детектирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Recent Progress in Appearance-based Action Recognition (Humphreys et al., 2020)](https://arxiv.org/abs/2011.12619)\n",
    "\n",
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector (Liu et al., 2015)](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "[Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "[Блог-пост: Что такое Focal Loss и когда его использовать](https://amaarora.github.io/2020/06/29/FocalLoss.html)\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection (Sergelius et al., 2016)](https://arxiv.org/abs/1612.0314)\n",
    "\n",
    "[Understanding feature pyramid networks for object detection - FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)\n",
    "\n",
    "[You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640) \n",
    "\n",
    "[YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "\n",
    "[YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "\n",
    "[YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "\n",
    "[YOLOv5 (Glenn Jocher)](https://github.com/ultralytics/yolov5)\n",
    "\n",
    "[YOLOX: Exceeding YOLO Series in 2021 (Ge et al., 2021)](https://arxiv.org/abs/2107.08430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hard example mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог пост про Hard Mining Example](https://erogol.com/online-hard-example-mining-pytorch/)\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining (Shrivastava et al., 2016)](https://arxiv.org/abs/1604.03540)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors (Yu et al., 2018)](https://arxiv.org/abs/1804.04606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " DINO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Emerging Properties in Self-Supervised Vision Transformers (Caron et al., 2021)](https://arxiv.org/abs/2104.14294)\n",
    "\n",
    "[Отличное видео объяснение статьи DINO](https://www.youtube.com/watch?v=h3ij3F3cPIk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "[Блог-пост про 2d свертки с помощью перемножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
