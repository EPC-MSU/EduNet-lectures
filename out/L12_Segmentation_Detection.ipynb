{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Сегментация и детектирование</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи компьютерного зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе предыдущих занятий мы подробно рассмотрели задачу классификации изображений.\n",
    "\n",
    "Но порой недостаточно знать, что на изображении есть объект определенного класса. Важно, где именно расположен объект. В ряде случаев нужно знать еще и точные границы объекта. Например, если речь идет о рентгеновском снимке или изображении клеток ткани, полученном с микроскопа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/classification_semantic_segmentation.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение того, какие фрагменты изображения принадлежат объектам определенных классов — это задача **сегментации** (segmentation).\n",
    "\n",
    "Если нас интересуют не индивидуальные объекты, а только тип (класс) объекта, которым занят конкретный пиксель (как в случае с клетками под микроскопом), то говорят о **семантической сегментации** (semantic segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нас интересуют конкретные объекты, и при этом достаточно знать только область, в которой объект локализован, то это задача **детектирования** (Detection)\n",
    "\n",
    "В качестве примера такой задачи можно рассмотреть подсчет количество китов на спутниковом снимке.\n",
    "\n",
    "\n",
    "Если же важны и индивидуальные объекты, и их точные границы, то это уже задача **Instance segmentation** Например для  автопилота важно не просто что перед ним несколько автомобилей, а важно отличить где именно находится ближний а где дальний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset COCO — Common Objects in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем говорить о способах решения этих задач, надо разобраться с форматами входных данных. Сделаем это на примере датасета [COCO](https://cocodataset.org/).\n",
    "\n",
    "Один из наиболее популярных датасатов, содержащий данные для сегментации и детектирования. Он содержит более трёхсот тысяч изображений, большая часть из которых размечена и содержит следующую информацию:\n",
    "- Категории\n",
    "- Маски\n",
    "- Ограничивающие боксы (*bounding boxes*)\n",
    "- Описания (*captions*)\n",
    "- Ключевые точки (*keypoints*)\n",
    "- И многое другое\n",
    "\n",
    "Формат разметки изображений, использованный в этом датасете, нередко используется и в других наборах данных. Как правило, он упоминается просто как \"COCO format\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# !wget -N \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "!wget -N \"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/annotations_trainval2017.zip\"\n",
    "!unzip -n annotations_trainval2017.zip\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с датасетом используется пакет `pycocotools`.\n",
    "\n",
    "[Подробнее о том, как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим формат аннотаций на примере одной записи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"cat\"])  # cat's IDs\n",
    "print(\"class ID(cat) =\", catIds)\n",
    "\n",
    "imgIds = coco.getImgIds(catIds=catIds)  # Filtering dataset by tag\n",
    "print(\"All images: %i\" % len(imgIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим метаданные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = coco.loadImgs(imgIds[0])  # 1 example\n",
    "img_metadata = img_list[0]\n",
    "img_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на изображение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "I = coco2pil(img_metadata[\"coco_url\"])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категории в COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на категории в датасете. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = coco.loadCats(coco.getCatIds())  # loading categories\n",
    "num2cat = {}\n",
    "print(\"COCO categories: \")\n",
    "\n",
    "iterator = iter(cats)\n",
    "cat = next(iterator)\n",
    "for i in range(0, 91):\n",
    "    if i == cat[\"id\"]:\n",
    "        num2cat[cat[\"id\"]] = cat[\"name\"]\n",
    "        name = cat[\"name\"]\n",
    "        if i < 90:\n",
    "            cat = next(iterator)\n",
    "    else:\n",
    "        name = \"---\"\n",
    "\n",
    "    print(f\"{i:2}. {name:20}\", end=\"\")\n",
    "\n",
    "    if not i % 6:\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Категория **0** используется для обозначения класса фона. Некоторые номера категорий не заняты ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также существуют надкатегории. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"cats[2]: {cats[2]}\")\n",
    "print(f\"cats[3]: {cats[3]}\")\n",
    "\n",
    "nms = set([cat[\"supercategory\"] for cat in cats])\n",
    "print(\"COCO supercategories: \\n{}\".format(\"\\t\".join(nms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разметка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо метаданных, нам доступна разметка ([подробнее о разметке](https://cocodataset.org/#format-data)). Давайте её загрузим и отобразим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annIds = coco.getAnnIds(imgIds=img_metadata[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "plt.imshow(I)\n",
    "plt.axis(\"off\")\n",
    "coco.showAnns(anns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На изображении можно увидеть разметку пикселей изображения по классам. То есть, пиксели из объектов, относящихся к интересующим классам, приписываются к классу этого объекта. К примеру, можно увидеть объекты двух классов: \"cat\" и \"keyboard\". \n",
    "\n",
    "Давайте теперь посмотрим, из чего состоит разметка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_anns(anns):\n",
    "    for i, a in enumerate(anns):\n",
    "        print(f\"\\n#{i}\")\n",
    "        for k in a.keys():\n",
    "            if k == \"category_id\" and num2cat.get(a[k], None):\n",
    "                print(k, \": \", a[k], num2cat[a[k]])  # Show cat. name\n",
    "            else:\n",
    "                print(k, \": \", a[k])\n",
    "\n",
    "\n",
    "dump_anns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что аннотация изображения может состоять из описаний нескольких объектов, каждое из которых содержит следующую информацию:\n",
    "* `segmentation` — последовательность пар чисел ($x$, $y$), координат каждой из вершин \"оболочки\" объекта;\n",
    "* `area` — площадь объекта;\n",
    "* `iscrowd` — несколько объектов, например толпа людей, в этом случае информация о границах объекта (маска) храниться в [RLE](https://en.wikipedia.org/wiki/Run-length_encoding) формате;\n",
    "* `image_id` — идентификатор изображения, к которому принадлежит описываемый объект;\n",
    "* `bbox` — *будет рассмотрен далее в ходе лекции*;\n",
    "* `category_id` — идентификатор категории, к которой относится данный объект;\n",
    "* `id` — идентификатор самого объекта.\n",
    "\n",
    "Попробуем посмотреть на пример, в котором `iscrowd = True` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"people\"])\n",
    "annIds = coco.getAnnIds(catIds=catIds, iscrowd=True)\n",
    "anns = coco.loadAnns(annIds[0:1])\n",
    "\n",
    "dump_anns(anns)\n",
    "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
    "I = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(I)\n",
    "coco.showAnns(anns)  # People in the stands\n",
    "seg = anns[0][\"segmentation\"]\n",
    "print(\"Counts\", len(seg[\"counts\"]))\n",
    "print(\"Size\", seg[\"size\"])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое [run-length encoding (RLE)](https://en.wikipedia.org/wiki/Run-length_encoding)\n",
    "\n",
    "[Видео-разбор](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы из `pycocotools`, можно  преобразовать набор вершин \"оболочки\" сегментируемого объекта в более удобный, но менее компактный вид — маску объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "msk = np.zeros(seg[\"size\"])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        ann = anns[i]\n",
    "        msk = coco.annToMask(ann)\n",
    "        ax[row, col].imshow(msk, cmap=\"gray\")\n",
    "        ax[row, col].set_title(num2cat[anns[i][\"category_id\"]])\n",
    "        ax[row, col].axis(\"off\")\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых случаях попиксельная разметка изображения может быть избыточной. К примеру, если необходимо посчитать количество человек на изображении, то достаточно просто каким-то образом промаркировать каждого из них, после чего посчитать количество наших \"отметок\". Одним из вариантов маркировки является \"обведение\" объекта рамкой (`bounding box`), внутри которой он находится. Такая информация об объектах также сохранена в аннотациях формата COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "draw = ImageDraw.Draw(I)\n",
    "\n",
    "colors = {1: \"white\", 40: \"lime\"}  # person - white, glove - lime\n",
    "for ann in anns:\n",
    "    x, y, width, heigth = ann[\"bbox\"]  # bounding box here\n",
    "    color = colors.get(ann[\"category_id\"], None)\n",
    "    if color:\n",
    "        draw.rectangle((x, y, x + width, y + heigth), outline=color, width=2)\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семантическая сегментация (*Semantic segmentation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Сегментация изображения — задача поиска групп пикселей, каждая из которых характеризует один смысловой объект.*\n",
    "\n",
    "Технически это выглядит так. Есть набор изображений:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого изображения есть маска W x H:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маска задает класс объекта для каждого пикселя:\n",
    "[ x,y - > class_num ]\n",
    "\n",
    "Набор таких изображений с масками — это и есть наш датасет, на нем мы учимся.\n",
    "\n",
    "На вход модель получает новое изображение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "и должна предсказать метку класса для каждого пикселя (маску).\n",
    "\n",
    "[ x,y - > class_num ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим такую маску из COCO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# !wget -N \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "!wget -N \"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/annotations_trainval2017.zip\"\n",
    "!unzip -n annotations_trainval2017.zip\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")\n",
    "clear_output()\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
    "I = coco2pil(img[\"coco_url\"])\n",
    "\n",
    "semantic_seg_person_mask = np.zeros(I.size[::-1], dtype=bool)  # WxH -> HxW\n",
    "\n",
    "for ann in anns:\n",
    "    msk = coco.annToMask(ann)  # HxW\n",
    "    if ann[\"category_id\"] == 1 and not ann[\"iscrowd\"]:  # single person:\n",
    "        # semantic_seg_person_mask = msk | semantic_seg_person_mask  # union\n",
    "        semantic_seg_person_mask += msk.astype(bool)\n",
    "\n",
    "semantic_seg_person_mask = semantic_seg_person_mask > 0  # binarize\n",
    "plt.imshow(semantic_seg_person_mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способы предсказания класса для каждого пикселя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте подумаем о том, как такую задачу можно решить.\n",
    "\n",
    "Из самой постановки задачи видно, что это задача классификации. Только не \n",
    "всего изображения, а каждого пикселя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Наивный**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Простейшим вариантом решения является использование так называемого \"скользящего окна\" — последовательное рассмотрение фрагментов изображения. В данном случае интересующими фрагментами будут небольшие зоны, окружающие каждый из пикселей изображения. К каждому из таких фрагментов применяется свёрточная нейронная сеть, предсказывающая, к какому классу относится центральный пиксель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/naive_way_predict_pixel_class.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**б) Разумный**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятно, что запускать классификатор для каждого пикселя абсолютно неэффективно, так как для одного изображения потребуется $H*W$ запусков.\n",
    "\n",
    "Можно пойти другим путем: получить карту признаков и по ней делать предсказание для всех пикселей разом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого потребуется поменять привычную нам архитектуру сверточной сети следующим образом:\n",
    "\n",
    "* убрать слои, уменьшающие пространственные размеры;\n",
    "\n",
    "* убрать линейный слой в конце, заменив его сверточным.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/reasonable_way_predict_pixel_class.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пространственные размеры выхода (W,H) будут равны ширине и высоте исходного изображения.\n",
    "\n",
    "Количество выходных каналов будет равно количеству классов, которые мы учимся предсказывать. \n",
    "\n",
    "Тогда можно использовать значения каждой из карт активаций на выходе последнего слоя сети как ненормированное значение вероятности принадлежности (core) каждого из пикселей к тому или иному классу. \n",
    "\n",
    "То есть номер канала с наибольшим значением будет соответствовать классу объекта, который изображает данный пиксель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "last_layer_output = torch.randn((3, 32, 32))  # class_num, W,H\n",
    "print(\"Output of last layer shape\", last_layer_output.shape)  # activation slice\n",
    "mask = torch.argmax(last_layer_output, dim=0)  # class_nums prediction\n",
    "print(\"One class mask shape\", mask.shape)\n",
    "print(\"Predictions for all classes \\n\", mask[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы на выходе сети получить количество каналов равное количеству классов, используется свертка 1x1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/1x1_kernel_size_fully_connected_layer.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лекции про сверточные сети мы говорили о том, что свертку 1x1 можно рассматривать как аналог полносвязного слоя. Именно так она тут и работает.\n",
    "\n",
    "**Проблемы:**\n",
    "- Что рецептивное поле нейронов на последних слоях было сопоставимо с размером изображения, требуется много сверточных слоев ($L$ раз свёртка $3\\times3$ $\\to$ рецептивное поле $(1+2L)$);\n",
    "- Свертки медленно работают на полноразмерных картах активации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**в) Эффективный**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем стандартную сверточную сеть, но полносвязанные слои заменим на сверточные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)\n",
    "\n",
    "\n",
    "Сокращенно FCN. Для того, чтобы не было путаницы с Fully Connected Network, последние именуют MLP (Multi Layer Perceptron).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За основу берется обычная сверточная сеть для калассификации:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/fcn_backbone.png\" width=\"500\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1\"> Review: FCN — Fully Convolutional Network (Semantic Segmentation) </a></p> </em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такую сеть можно построить, взяв за основу другую сверточную архитектуру (*backbone*) , например `ResNet50` или `VGG16`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/fcn_changes.png\" width=\"500\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1\"> Review: FCN — Fully Convolutional Network (Semantic Segmentation) </a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И затем заменить полносвязные слои на свертки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/fully_convolution_network_scheme.png\" width=\"500\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1\"> Review: FCN — Fully Convolutional Network (Semantic Segmentation) </a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце добавить `upsample` слой до нужных нам размеров. \n",
    "\n",
    "На вход такая модель может получать изображение произвольного размера. \n",
    "Так как для задач сегментации изменение размеров входного изображения приводит к потере важной информации о границах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разжимающий слой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как реализовать декодировщик?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерполяция при увеличении разрешения "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, как повышают разрешение для обычных изображений, а уже затем перейдем к картам признаков.\n",
    "\n",
    "Допустим, требуется увеличить изображение размером 2x2 до размера 4x4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/upsample.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Comparison_of_1D_and_2D_interpolation.svg/1920px-Comparison_of_1D_and_2D_interpolation.svg.png\" width=\"800\">\n",
    "\n",
    "[Bilinear_interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если для интерполяции используются значения четырех соседних пикселей, то такая интерполяция называется билинейной. В качестве интерполированного значения используется взвешенное среднее этих четырёх пикселей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def img_to_heatmap(img, ax, title):  # Magik method to show img as heatmap\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(title)\n",
    "    array = np.array(img)\n",
    "    array = array[None, None, :]\n",
    "    sns.heatmap(array[0][0], annot=True, ax=ax, lw=1, cbar=False)\n",
    "\n",
    "\n",
    "# Fake image\n",
    "raw = np.array([[1, 3, 0, 1], [3, 3, 3, 7], [8, 1, 8, 7], [6, 1, 1, 1]], dtype=np.uint8)\n",
    "pil = Image.fromarray(raw)\n",
    "\n",
    "interp_nn = pil.resize((8, 8), resample=Image.NEAREST)\n",
    "interp_bl = pil.resize((8, 8), resample=Image.BILINEAR)\n",
    "\n",
    "# plot result\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "img_to_heatmap(raw, ax[0], \"Raster dataset\")\n",
    "img_to_heatmap(interp_nn, ax[1], \"Nearest neighbor interpolation\")\n",
    "img_to_heatmap(interp_bl, ax[2], \"Bilinear interpolation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Билинейная интерполяция](https://en.wikipedia.org/wiki/Bilinear_interpolation) позволяет избавиться от резких границ, которые возникают при увеличении методом ближайшего соседа.  Существуют и другие виды интерполяции, использующие большее количество соседних пикселей.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample в PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К чему был этот разговор об увеличении картинок?\n",
    "\n",
    "Оказывается, для увеличения пространственного разрешения карт признаков (feature maps) можно применять те же методы, что и для изображений.\n",
    "\n",
    "Для увеличения пространственного разрешения карт признаков (карт активаций), в PyTorch используется класс `Upsample`. В нём доступны все упомянутые методы интерполяции, а также трилинейная интерполяция — аналог билинейной интерполяции, используемый для работы с трёхмерными пространственными данными (к примеру, видео). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/upsample_pytorch.png\" width=\"1000\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html\"> PyTorch nn.Upsample </a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[doc] nn.functional.interpolate](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html?highlight=interp#torch.nn.functional.interpolate)\n",
    "\n",
    "Таким образом, мы можем использовать `Upsample` внутри нашего разжимающего блока.\n",
    "\n",
    "Загрузим изображение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_1.png -O cat.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "def upsample(pil, ax, mode=\"nearest\"):\n",
    "    tensor = TF.to_tensor(pil)\n",
    "    # Create upsample instance\n",
    "\n",
    "    if mode == \"nearest\":\n",
    "        upsampler = nn.Upsample(scale_factor=2, mode=mode)\n",
    "    else:\n",
    "        upsampler = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
    "\n",
    "    tensor_128 = upsampler(tensor.unsqueeze(0))  # add batch dimension\n",
    "    # Convert tensor to pillow\n",
    "    img_128 = tensor_128.squeeze()\n",
    "    img_128_pil = TF.to_pil_image(img_128.clamp(min=0, max=1))\n",
    "    ax.imshow(img_128_pil)\n",
    "    ax.set_title(mode)\n",
    "\n",
    "\n",
    "# Load and show image in Pillow format\n",
    "pic = Image.open(\"cat.png\")\n",
    "pil_64 = pic.resize((64, 64))\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(15, 5))\n",
    "ax[0].imshow(pil_64)\n",
    "ax[0].set_title(\"Raw\")\n",
    "\n",
    "# Upsample with Pytorch\n",
    "upsample(pil_64, mode=\"nearest\", ax=ax[1])\n",
    "upsample(pil_64, mode=\"bilinear\", ax=ax[2])\n",
    "upsample(pil_64, mode=\"bicubic\", ax=ax[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в данном случае каждое из пространственных измерений изображения увеличилось в 2 раза, но при необходимости возможно использовать увеличение в иное, в том числе не целое, количество раз, используя параметр `scale_factor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слои Upsample обычно комбинируют вместе со сверточными, это рекомендованный способ увеличения пространственных размеров карт признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Upsample(scale_factor=2), nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.ReLU()\n",
    ")\n",
    "\n",
    "dummy_input = torch.randn((0, 3, 32, 32))\n",
    "out = model(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Другие способы \"разжать\" карту признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxUnpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо свёртки, на этапе снижения размерности также используются слои pooling'а. Наиболее популярным вариантом является MaxPooling, сохраняющий значение только наибольшего элемента внутри сегмента. Для того, чтобы обратить данную операцию субдискретизации, был предложен MaxUnpooling слой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/maxunpooling.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный слой требует сохранения индексов максимальных элементов внутри сегментов — при обратной операции максимальное значение помещается на место, в котором был максимальный элемент сегмента до соответствующей субдискретизации. Соответственно, каждому слою MaxUnpooling должен соответствовать слой MaxPooling, что визуально можно представить следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/downsample_and_upsample_layers.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация к MaxPool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "[Документация к MaxUnpool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "torch.use_deterministic_algorithms(False, warn_only=False)\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "def tensor_show(tensor, title=\"\", ax=ax):\n",
    "    img = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
    "    ax.set_title(title + str(img.size))\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "pool = nn.MaxPool2d(\n",
    "    kernel_size=2, return_indices=True\n",
    ")  # False by default(get indexes to upsample)\n",
    "unpool = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "pil = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].set_title(\"original \" + str(pil.size))\n",
    "ax[0].imshow(pil)\n",
    "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
    "print(\"Orginal shape\", tensor.shape)\n",
    "\n",
    "# Downsample\n",
    "tensor_half_res, indexes1 = pool(tensor)\n",
    "tensor_show(tensor_half_res, \"1/2 down \", ax=ax[1])\n",
    "\n",
    "tensor_q_res, indexes2 = pool(tensor_half_res)\n",
    "tensor_show(tensor_q_res, \"1/4 down \", ax=ax[2])\n",
    "print(\"Downsample shape\", indexes2.shape)\n",
    "\n",
    "# Upsample\n",
    "tensor_half_res1 = unpool(tensor_q_res, indexes2)\n",
    "tensor_show(tensor_half_res1, \"1/2 up \", ax=ax[3])\n",
    "\n",
    "\n",
    "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
    "tensor_show(tensor_recovered, \"full size up \", ax=ax[4])\n",
    "print(\"Upsample shape\", tensor_recovered.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transposed convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Способы восстановления пространственных размерностей, которые мы рассмотрели, не содержали обучаемых параметров. \n",
    "\n",
    "Для повышения пространственного разрешения карты признаков можно использовать операцию *Transposed convolution*, в которой, как в обычной свертке, есть **обучаемые параметры**. Альтернативное название: *Fractionally strided convolution*.\n",
    "\n",
    "Иногда **некорректно** называется *обратной сверткой* или *Deconvolution*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/simple_convolution.png\" width=\"700\"></center>\n",
    "<center><em>Обычная свертка</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция обычной свертки накладывает фильтр-ядро на фрагмент карты, выполняет поэлементное умножение, а затем сложение, превращая **один фрагмент** входа в **один пиксель** выхода.\n",
    "\n",
    "Transposed convolution, наоборот, проходит по всем пикселям входа и умножает их на **обучаемое ядро** свертки. При этом каждый **одиночный пиксель** превращается в **фрагмент**. Там, где фрагменты накладываются друг на друга, значения попиксельно суммируются.\n",
    "\n",
    "Если вход имеет несколько каналов, то Transposed convolution применяет отдельный обучаемый фильтр к каждому каналу, а результат суммирует.\n",
    "\n",
    "Параметр `stride` отвечает за дополнительный сдвиг каждого фрагмента на выходе. Используя Transposed convolution с параметром `stride = 2`, можно повышать размер карты признаков приблизительно в два раза, добавляя на нее мелкие детали."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/transposed_convolution_explained.png\" width=\"1024\"></center>\n",
    "\n",
    "<center><em>Transposed convolution</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от обычной свертки, параметр `padding` Transposed convolution отвечает не за увеличение исходной карты признаков, а, наоборот, за \"срезание\" внешнего края карты-выхода. Это может быть полезно, потому что карта строится с перекрытием фрагментов, полученных из соседних пикселей, но по периметру результат формируется без перекрытия и может иметь более низкое качество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правило, размер ядра `kernel_size` выбирают кратным `stride`, чтобы избавиться от артефактов при частичном наложении фрагментов, например:\n",
    "```\n",
    "kernel_size = 4\n",
    "stride = 2\n",
    "```\n",
    "При таких значениях имеет смысл установить `padding=2`, чтобы убрать внешние два пикселя со всех сторон выходной карты признаков, полученные без перекрытия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про 2d свертки с помощью перемножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)\n",
    "\n",
    "[Откуда слово Transposed в названии (раздел 4.1)](https://arxiv.org/pdf/1603.07285v1.pdf)\n",
    "\n",
    "[Документация к ConvTranspose2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, \n",
    "                         stride=1, padding=0, ...)\n",
    "```\n",
    "где:\n",
    "* `in_channels`, `out_channels` — количество каналов в входной и выходной карте признаков,\n",
    "* `kernel_size` — размер ядра свертки Transpose convolution, \n",
    "* `stride` — шаг свертки Transpose convolution,\n",
    "* `padding`— размер отступов, устанавливаемых по краям входной карты признаков.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример использования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 16, 16, 16)  # define dummy input\n",
    "print(\"Original size\", input.shape)\n",
    "\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)  # define downsample layer\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)  # define upsample layer\n",
    "\n",
    "# let`s downsample and upsample input\n",
    "with torch.no_grad():\n",
    "    output_1 = downsample(input)\n",
    "    print(\"Downsampled size\", output_1.size())\n",
    "\n",
    "    output_2 = upsample(output_1, output_size=input.size())\n",
    "    print(\"Upsampled size\", output_2.size())\n",
    "\n",
    "# plot results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "sns.heatmap(input[0, 0, :, :], ax=ax[0], cbar=False, vmin=-2, vmax=2)\n",
    "ax[0].set_title(\"Input\")\n",
    "sns.heatmap(output_1[0, 0, :, :], ax=ax[1], cbar=False, vmin=-2, vmax=2)\n",
    "ax[1].set_title(\"Downsampled\")\n",
    "sns.heatmap(output_2[0, 0, :, :], ax=ax[2], cbar=False, vmin=-2, vmax=2)\n",
    "ax[2].set_title(\"Upsampled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пирамида признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возникает вопрос: не потеряется ли информация о мелких деталях изображения при передаче через центральный блок сети, где пространственное разрешение минимально? Такая проблема существует. \n",
    "\n",
    "Те, кто изучал классические методы машинного зрения, помнят, что  при извлечении дескрипторов особых точек([SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform)) использовалась так называемая пирамида изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/pyramid_of_features.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея состоит в последовательном уменьшении (масштабировании) изображения и последовательном извлечении признаков в разных разрешениях.\n",
    "\n",
    "При уменьшении пространственных размеров мы естественным образом получаем карты признаков с разным пространственным разрешением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_information.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Их можно использовать одновременно как вход для новых сверток так и для получения предсказаний.\n",
    "\n",
    "На этой модели построены FPN - сети: \n",
    "[Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/fcn_1.png\" width=\"1000\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1\"> Review: FCN — Fully Convolutional Network (Semantic Segmentation) </a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как все карты признаком будут увеличены до одного размера они суммируются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры использования: [1](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/) и [2](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
    "\n",
    "\n",
    "Предобученная модель была обучена на части датасета COCO train2017 (на 20 категориях, представленных так же в датасете  Pascal VOC). Использовались следующие классы:\n",
    "\n",
    "`['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "# load resnet50\n",
    "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
    "    weights=\"FCN_ResNet50_Weights.DEFAULT\", num_classes=21\n",
    ")\n",
    "\n",
    "classes = [\n",
    "    \"__background__\",\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # ImageNet\n",
    "    ]\n",
    ")\n",
    "\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "input_tensor = transform(pil_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = fcn_model(input_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращаются 2 массива\n",
    "\n",
    "* out — каждый пиксель отражает ненормированную вероятность, соответствующую предсказанию каждого класса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"output keys: \", output.keys())  # Ordered dictionary\n",
    "print(\"out: \", output[\"out\"].shape, \"Batch, class_num, h, w\")\n",
    "\n",
    "output_predictions = output[\"out\"][0].argmax(0)  # for first element of batch\n",
    "print(f\"output_predictions: {output_predictions.shape}\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "indexes = output_predictions\n",
    "semantic_seg_person_predict = np.zeros(pil_img.size).astype(bool)\n",
    "\n",
    "# plot all classes predictions\n",
    "fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
    "i = 0  # counter\n",
    "for row in range(4):\n",
    "    for col in range(5):\n",
    "        mask = torch.zeros(indexes.shape)\n",
    "        mask[indexes == i] = 255\n",
    "\n",
    "        ax[row, col].set_title(classes[i])\n",
    "        ax[row, col].imshow(mask)\n",
    "        ax[row, col].axis(\"off\")\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_seg_person_predict = torch.zeros(indexes.shape)\n",
    "semantic_seg_person_predict[indexes == 15] = 1  # to obtain binary mask\n",
    "semantic_seg_person_predict = (\n",
    "    semantic_seg_person_predict.numpy()\n",
    ")  # for skliarn compability\n",
    "\n",
    "plt.imshow(semantic_seg_person_predict, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IoU — оценка точности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как оценить качество предсказаний, полученных от модели?\n",
    "\n",
    "Базовой метрикой является Intersection over Union (IoU), она же коэффициент Жаккара([Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index))\n",
    "\n",
    "Имеется предсказание модели (фиолетовая маска) и целевая разметка,  сделанная человеком (красная маска)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/iou_sample.png\" width=\"400\">\n",
    "\n",
    "<left><p><em>Source: <a href=\"https://datahacker.rs/deep-learning-intersection-over-union/\">Intersection over Union</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо оценить качество предсказания.\n",
    "\n",
    "**Для простоты в примере маски прямоугольные, но та же логика будет работать  для масок произвольной формы.*\n",
    "\n",
    "Метрика считается как отношение площади пересечения к площади объединения двух масок: \n",
    "\n",
    "$$ IoU = \\frac{|T \\cap P|}{|T \\cup P|} $$\n",
    "\n",
    "T — True mask, P — predicted mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/iou_formula.png\" width=\"400\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\"> Non-maximum Suppression (NMS)</a></p></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если маски совпадут на 100%, то значение метрики будет равно 1 и это наилучший результат. При пустом пересечении IoU будет нулевым. Значения метрики лежат в интервале [0..1].\n",
    "\n",
    "В терминах ошибок первого/второго рода IoU можно записать как:\n",
    "\n",
    "$$ IoU = \\frac{TP}{TP + FP + FN} $$\n",
    "\n",
    "\n",
    "TP — True positive — пересечение (обозначено желтым),\n",
    "\n",
    "FP — False Positive (остаток фиолетового прямоугольника),\n",
    "\n",
    "FN — False Negative (остаток красного прямоугольника)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На базе этой метрики строится ряд производных от нее метрик, таких как Mean Average Precision, которую мы рассмотрим в разделе Детектирование.\n",
    "\n",
    "Дополнительная информация: [Intersection over Union](http://datahacker.rs/deep-learning-intersection-over-union/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"GT mask\")\n",
    "plt.imshow(semantic_seg_person_mask)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Predicted mask\")\n",
    "plt.imshow(semantic_seg_person_predict)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"GT & Predict overlap\")\n",
    "plt.axis(\"off\")\n",
    "tmp = semantic_seg_person_predict * 2 + semantic_seg_person_mask\n",
    "plt.imshow(tmp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\n",
    "\n",
    "SMP (func): https://smp.readthedocs.io/en/latest/metrics.html#segmentation_models_pytorch.metrics.functional.iou_score\n",
    "\n",
    "Torchmetrics(obj): https://torchmetrics.readthedocs.io/en/stable/classification/jaccard_index.html\n",
    "\n",
    "Torchvision (bbox): https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# wait vectors, so we flatten the data\n",
    "y_true = semantic_seg_person_mask.flatten()\n",
    "y_pred = semantic_seg_person_predict.flatten()\n",
    "iou = jaccard_score(y_true, y_pred)\n",
    "\n",
    "print(f\"IoU = {iou:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "# or use:  smp.metrics.get_stats\n",
    "# https://smp.readthedocs.io/en/latest/metrics.html#segmentation_models_pytorch.metrics.functional.get_stats\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "iou = tp / (tp + fp + fn)\n",
    "print(f\"IoU = {iou:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss функции для сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/loss_overview.png\" width=\"900\"></center>\n",
    "\n",
    "<center><a href=\"https://github.com/JunMa11/SegLoss\">Loss functions for image segmentation</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution - based loss\n",
    "\n",
    "Так как задача сегментации свобится к задаче классификации, то, можно использовать Cross-Entropy Loss, BCE, или Focal Loss которыми мы знакомы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Cross-Entropy (BCE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если предсказывается маска для объектов единственного класса(target.shape = 1xHxW), то задача сводится к бинарной классификации. Так как каждый канал на выходе последнего слоя выдает предсказание для единственного класса.\n",
    "\n",
    "Это позволяет заменить Softmax в Cross-Entropy Loss на сигмоиду, а функцию потерь — на бинарную кросс-энтропию (BCE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mask_class_1 = torch.randint(0, 2, (1, 64, 64))  # [0 , 1]\n",
    "one_class_out = torch.randn(1, 1, 64, 64)\n",
    "print(mask_class_1.shape)\n",
    "print(one_class_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "bce_loss_wl = nn.BCEWithLogitsLoss()  # Sigmoid inside\n",
    "loss = bce_loss_wl(\n",
    "    one_class_out, mask_class_1.float().unsqueeze(0)\n",
    ")  # both params must have equal size\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Если последний слой модели это Сигмоида, то можем использовать [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_one_class_out = one_class_out.sigmoid()\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "loss = bce_loss(\n",
    "    norm_one_class_out, mask_class_1.float().unsqueeze(0)\n",
    ")  # both params must have equal size\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) для одного класса работать не будет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "loss = cross_entropy(one_class_out, mask_class_1.float().unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как softmax от единственного фхода всегда 1-ца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_class_out.softmax(dim=1).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multilabel**\n",
    "Если предсказывается несколько классов и target имеет форму: NxWxH (multilabel)\n",
    "То есть маска каждого храниться в отдельном канале:\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/semantic_mask2.png\" width=\"700\">\n",
    "\n",
    "[An overview of semantic image segmentation](https://www.jeremyjordan.me/semantic-segmentation/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mask_class1 = torch.randint(0, 2, (1, 64, 64))  # [0 , 1]\n",
    "mask_class2 = torch.randint(0, 2, (1, 64, 64))\n",
    "\n",
    "target = torch.cat((mask_class1, mask_class2))\n",
    "\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То форма выхода модели совпадает с формой тензора масок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_class_out = torch.randn(1, 2, 64, 64)\n",
    "print(two_class_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать [BCE](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) поэлементно , предварительно преобразовав target во float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()  # Sigmoid inside\n",
    "float_target = target.float()  # add batch and convert ot float\n",
    "loss = bce_loss(\n",
    "    two_class_out, float_target.unsqueeze(0)\n",
    ")  # both params must have equal size\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "# If containing class probabilities, same shape as the input and each value should be between [0,1][0,1].\n",
    "loss = cross_entropy(two_class_out, float_target.unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты не совпадут так как после Sigmoid и Softmax получаться разные вероятности:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если маска задана одним каналом, в котором классы пронумерованны целыми числами (multiclass) \n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/semantic_mask1.png\" width=\"900\">\n",
    "\n",
    "[An overview of semantic image segmentation](https://www.jeremyjordan.me/semantic-segmentation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_target = target.argmax(0)\n",
    "sq_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То логично использовать [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "\n",
    "```\n",
    "Input: Shape (C), (N,C) or (N,C,d1​,d2​,...,dK​) with K≥1 in the case of K-dimensional loss.\n",
    "\n",
    "Target: If containing class indices, shape (), (N) or (N,d1,d2,...,dK)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = cross_entropy(two_class_out, sq_target.unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region based loss\n",
    "\n",
    "Лосс функции основанны на оценки площади пересечения масок.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Loss\n",
    "\n",
    "В отличие от accuracy, рассчет IoU (Jaccard index):\n",
    "$IoU = JaccardIndex = \\dfrac{  TP  }{TP + FP + FN} \\in [0,1]$\n",
    "\n",
    "можно произвести [дифференцируемым образом](https://arxiv.org/abs/1908.03851).\n",
    "\n",
    "И тогда метрику можно превратить в лосс- функцию инвертировав ее:\n",
    "\n",
    "$Jaccard Loss = 1 - IoU$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Pytorch эта лосс функция не реализована, поэтому для ее использования установим библиотеку [SMP](https://github.com/qubvel/segmentation_models.pytorch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "iou_loss = smp.losses.JaccardLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "print(\"IoU Loss\", iou_loss(two_class_out, float_target.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*при наличии на выходе модели сигмоиды или другой функции преобразующей логиты в вероятности, параметр `from_logits` следует уствновить равным `False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dice loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой популярной метрикой для оценки качества сегментации является Dice коэффициент:\n",
    "\n",
    "$ Dice =  \\dfrac{  2 | AB | }{ |A|+|B| } $\n",
    "\n",
    "Концептульно он похож на IoU но при выржении через ошибки первого и второго рода то будет видно что он совпадет F1 - мерой:\n",
    "\n",
    "$Dice =  \\dfrac{  2AB  } {A+B } = \\dfrac{  2TP  }{2TP + FP + FN} = F1\\_score ∈ [0,1]$\n",
    "\n",
    "[F1 Score = Dice Coefficient](https://chenriang.me/f1-equal-dice-coefficient.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Его вычисление не сложно произвести дифференцируемым образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/dice_loss.jpeg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Dice = \\dfrac{2\\sum (y_{true}y_{pred})}{ \\sum y_{true} + \\sum y_{pred}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И затем превратить в лосс функцию: \n",
    "\n",
    "$ DiceLoss = 1 - Dice $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch эти лосс функции не реализованы, поэтому установим библиотеку [SMP](https://github.com/qubvel/segmentation_models.pytorch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = smp.losses.DiceLoss(smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "\n",
    "print(two_class_out.shape, target.shape)\n",
    "print(\"DICE Loss\", dice_loss(two_class_out, float_target.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Концептуально DiceLoss vs JaccardLoss похожи. Но JaccardLoss [сильнее штрафует можель на выбросах](https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хороший обзор loss функции для семантической сегментации можно найти в репозитории: [Loss functions for image segmentation](https://github.com/JunMa11/SegLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net: Convolutional Networks for Biomedical Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним большую часть традиционной структуры сети, включая слои downsampling (снижение пространственных размеров), извлечем основные признаки, а затем восстановим пространственные размеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/efficient_way_predict_pixel_class.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автокодировщик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта архитектура повторяет архитектуру автокодировщика (autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/encoder_loss.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая архитектура довольно популярна и применяется не только для сегментации, но и в следующих задачах: \n",
    "\n",
    "- сглаживание шума;\n",
    "- снижение размерности $\\to$ вектор-признак;\n",
    "- генерация данных.\n",
    "\n",
    "Этому будет посвящена одна из следующих лекций, сейчас же мы детально рассмотрим разжимающий блок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки полученные при сжатии скопируем и передадим в разжимающие слои, где карты признаков будут иметь соответствующее пространственное разрешение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/add_skip_connection.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, как и ResNet, этот механизм носит название skip connection, но  признаки  не суммируются, а конкатенируются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотренная нами схема используется в U-Net. Эта популярная модель для сегментации медицинских изображений изначально была предложена в статье [U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597) для анализа  медицинских изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/unet_scheme.png\" width=\"700\"></center>\n",
    "<center><em>Архитектура U-Net (Ronneberger et al., 2015).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Реализация на PyTorch](https://github.com/milesial/Pytorch-UNet)\n",
    "\n",
    "[U-Net на PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)\n",
    "\n",
    "[Блог-пост разбор](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит обратить особое внимание на серые стрелки на схеме: они соответствуют операции конкатенации копий ранее полученных карт активаций по аналогии с DenseNet. Чтобы это было возможно, необходимо поддерживать соответствие между размерами карт активаций в процессах снижения и повышения пространственных размерностей. Для этой цели изменения размеров происходят только при операциях `MaxPool` и `MaxUnpool` — в обоих случаях в два раза. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде прямой проход может быть реализован, например, вот так:\n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "    out1 = self.block1(x) #  ------------------------------>\n",
    "    out_pool1 = self.pool1(out1)\n",
    "\n",
    "    out2 = self.block2(out_pool1)\n",
    "    out_pool2 = self.pool2(out2)\n",
    "\n",
    "    out3 = self.block3(out_pool2)\n",
    "    out_pool3 = self.pool2(out3)\n",
    "\n",
    "    out4 = self.block4(out_pool3)\n",
    "    # return up\n",
    "    out_up1 = self.up1(out4)\n",
    "\n",
    "    out_cat1 = torch.cat((out_up1, out3), dim=1)\n",
    "    out5 = self.block5(out_cat1)\n",
    "    out_up2 = self.up2(out5)\n",
    "\n",
    "    out_cat2 = torch.cat((out_up2, out2), dim=1)\n",
    "    out6 = self.block6(out_cat2)\n",
    "    out_up3 = self.up3(out6)\n",
    "\n",
    "    out_cat3 = torch.cat((out_up3, out1), dim=1) # <-------\n",
    "    out = self.block7(out_cat3)\n",
    "\n",
    "    return out\n",
    "\n",
    "```\n",
    "После upsample блоков, ReLU не используется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обзор DeepLabv3+ (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepLab — семейство моделей для сегментации, значительно развивавшееся в течение четырёх лет. Основой данного рода моделей является использование **atrous (dilated) convolutions** и, начиная со второй модели, **atrous spatial pyramid pooling**, опирающейся на **spatial pyramid pooling**.\n",
    "\n",
    "[Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
    "\n",
    "[Реализация на PyTorch](https://pytorch.org/vision/stable/models.html#deeplabv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/deeplabv3_scheme.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atrous (Dilated) Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/dilated_convolution.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dilated convolution** (расширенная свертка) — это тип свертки, который \"раздувает\" ядро, как бы вставляя отверстия между элементами ядра. Дополнительный параметр (скорость расширения, **dilation**) указывает, насколько сильно расширяется ядро.\n",
    "\n",
    "\n",
    "Фактически в такой свертке входные пиксели (признаки) участвуют через один (два, три ...).\n",
    "\n",
    "Расширенные свертки позволяют значительно увеличить рецептивное поле и хорошо показывают себя при [решении задач семантической сегментации изображений](https://arxiv.org/pdf/1511.07122.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "                padding=0, dilation=1, ...)\n",
    "```\n",
    "где:\n",
    "* `in_channels`, `out_channels` — количество каналов в входной и выходной карте признаков,\n",
    "* `kernel_size` — размер ядра свертки,\n",
    "* `stride` — шаг свертки,\n",
    "* `padding` — размер отступов, устанавливаемых по краям входной карты признаков,\n",
    "* `dilation` — скорость расширения свертки.\n",
    "\n",
    "[[doc] nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color =\"orange\">доп инфо. про Atros convolution</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plot\n",
    "def plot_conv2d(input, conv, output, dilation=1):\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "    # input\n",
    "    sns.heatmap(\n",
    "        input[0][0],\n",
    "        ax=ax[0],\n",
    "        annot=True,\n",
    "        fmt=\".0f\",\n",
    "        cbar=False,\n",
    "        vmin=0,\n",
    "        vmax=20,\n",
    "        linewidths=1,\n",
    "    )\n",
    "    # kernel\n",
    "    sns.heatmap(\n",
    "        conv.weight.detach()[0, 0, :, :],\n",
    "        ax=ax[1],\n",
    "        annot=True,\n",
    "        fmt=\".0f\",\n",
    "        cbar=False,\n",
    "        vmin=0,\n",
    "        vmax=20,\n",
    "        linewidths=1,\n",
    "    )\n",
    "    # output\n",
    "    sns.heatmap(\n",
    "        output[0, 0, :, :],\n",
    "        ax=ax[2],\n",
    "        annot=True,\n",
    "        fmt=\".0f\",\n",
    "        cbar=False,\n",
    "        vmin=0,\n",
    "        vmax=20,\n",
    "        linewidths=1,\n",
    "    )\n",
    "    # titles\n",
    "    ax[0].set_title(\"Input \\nshape: \" + str(input.shape))\n",
    "    ax[1].set_title(\"Kernel \\nshape: \" + str(conv.weight.shape))\n",
    "    ax[2].set_title(\"Output \\nshape: \" + str(output.shape))\n",
    "    fig.suptitle(\"Dilation = \" + str(dilation), y=1.05)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atrous example\n",
    "with torch.no_grad():\n",
    "    # define dummy input\n",
    "    # https://stackoverflow.com/questions/58584413/black-formatter-ignore-specific-multi-line-code\n",
    "    # fmt: off\n",
    "    input = torch.tensor([[[[1, 2, 3],\n",
    "                            [1, 2, 3],\n",
    "                            [1, 2, 3]]]], dtype=torch.float)\n",
    "    # fmt: on\n",
    "    # define conv layer, dilation = 1\n",
    "    conv = nn.Conv2d(1, 1, kernel_size=2, dilation=1, bias=False)\n",
    "    # define kernel weights\n",
    "    conv.weight = nn.Parameter(torch.tensor([[[[1, 2], [2, 1]]]], dtype=torch.float))\n",
    "    output = conv(input)\n",
    "    plot_conv2d(input, conv, output, dilation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dilation to 2\n",
    "with torch.no_grad():\n",
    "    conv = nn.Conv2d(\n",
    "        1, 1, kernel_size=2, dilation=2, bias=False\n",
    "    )  # Fell free to change dilation\n",
    "    conv.weight = nn.Parameter(torch.tensor([[[[1, 2], [2, 1]]]], dtype=torch.float))\n",
    "    output = conv(input)\n",
    "    plot_conv2d(input, conv, output, dilation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input = torch.tensor([[[[0, 1, 0], [1, 1, 1], [0, 1, 0]]]], dtype=torch.float)\n",
    "    output = conv(input)\n",
    "    plot_conv2d(input, conv, output, dilation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation models PyTorch (SMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Библиотека](https://github.com/qubvel/segmentation_models.pytorch#architectures-) на базе PyTorch  для сегментации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/smp.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "!pip install segmentation-models-pytorch\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем комбинировать декодер с разными энкодерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "import torch\n",
    "\n",
    "# 'mit_b0' Mix Vision Transformer Backbone from SegFormer pretrained on Imagenet\n",
    "preprocess_input = get_preprocessing_fn(\"mit_b0\", pretrained=\"imagenet\")\n",
    "\n",
    "# MixVisionTransformer encoder does not support in_channels setting other than 3\n",
    "# supported by FPN only for encoder depth = 5\n",
    "model = smp.FPN(\"mit_b0\", in_channels=3, classes=10, encoder_depth=5)\n",
    "\n",
    "# ... Train model on your dataset\n",
    "\n",
    "dummy_input = torch.randn([1, 3, 64, 64])\n",
    "\n",
    "mask = model(dummy_input)\n",
    "print(mask.shape)  # torch.Size([1, 1, 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совместимость с timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует библиотека [pytorch-image-models](https://github.com/huggingface/pytorch-image-models) (timm = Torch IMage Models)\n",
    "\n",
    "\n",
    "В которой собранно большое количество моделей для работы с изображениями\n",
    "\n",
    "https://huggingface.co/docs/timm/index\n",
    "\n",
    "https://huggingface.co/docs/hub/timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model_names = timm.list_models(pretrained=True)\n",
    "print(\"Total pretrained models: \", len(model_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно искать модели по шаблону"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = timm.list_models(\"*mobilenet*small*\")\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_mobilenet = timm.create_model(\"mobilenetv3_small_050\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = timm_mobilenet(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совместимость с SMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать большинство моделей из timm в качестве энкодеров.\n",
    "\n",
    "[Список совместимых моделей](https://smp.readthedocs.io/en/latest/encoders_timm.html).\n",
    "\n",
    "При этом к названии модели которое передается в конструктор класса SMP нужно добавить перфикс `tu-`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_timm_model = smp.DeepLabV3(\"tu-mobilenetv3_small_050\", in_channels=3, classes=80)\n",
    "smp_timm_model.eval()\n",
    "print(\"Created DeepLab with mobileNet encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = smp_timm_model(dummy_input)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детектирование (Object detection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/detection\n",
    ".png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектирование — задача компьютерного зрения, в которой требуется\n",
    "определить местоположение конкретных объектов на изображении.\n",
    "\n",
    "При этом вычислять точные границы объектов не требуется, а достаточно определить только ограничивающие прямоугольники (bounding boxes), в которых находятся объекты.\n",
    "\n",
    "В общем случае объекты могут принадлежать к различным классам, и объектов одного класса на изображении может быть несколько. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектирование единственного объекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём с простой ситуации.\n",
    "\n",
    "Пусть нас интересуют объекты только одного класса, и мы знаем, что такой объект на изображении есть и он один. \n",
    "\n",
    "К примеру, мы разрабатываем систему по распознаванию документов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/predict_bounded_box_example.png\" width=\"700\"></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/ftp/arxiv/papers/1807/1807.05786.pdf\"> MIDV-500: a dataset for identity document analysis and recognition\n",
    "on mobile devices in video stream</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход модели подаётся изображение, и предсказать требуется область, в которой объект локализован.\n",
    "Область (bounding box) определяется набором координат вершин*. Собственно эти координаты и должна предсказать модель.\n",
    "\n",
    "\\* *Если наложить условие, что стороны многоугольника должны быть параллельны сторонам изображения, то можно ограничиться предсказанием 2-х координат.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказание координат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если задачу семантической сегментации получилось свести к классификации, то здесь будем использовать регрессию, поскольку предсказывать нужно не номер класса, а набор чисел.\n",
    "\n",
    "В зависимости от требований эти числа могут нести разный смысл, например:\n",
    "\n",
    "* координаты центра + ширина и высота,\n",
    "* координаты правого верхнего и левого нижнего углов,\n",
    "* координаты вершин многоугольника ...\n",
    "\n",
    "Но в любом случае задача остается регрессионной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зафиксируем seeds для воспроизводимости результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fix_seeds():\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "fix_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решается она так:\n",
    "\n",
    "Берем сверточную сеть и меняем последний полносвязный слой таким образом, чтобы количество выходов совпадало с количеством координат, которые нам нужно предсказать.\n",
    "\n",
    "Так для предсказания двух точек потребуется четыре выхода ( x1 , y1 , x2, y2 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "\n",
    "# load pretrained model\n",
    "resnet_detector = resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "\n",
    "# Change \"head\" to predict coordinates (x1,y1 x2,y2)\n",
    "resnet_detector.fc = nn.Linear(resnet_detector.fc.in_features, 4)  # x1,y1 x2,y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения такой модели придется заменить функцию потерь на регрессионную, например, MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# This is a random example. Don't expect good results\n",
    "input = torch.rand((1, 3, 224, 224))\n",
    "target = torch.tensor([[0.1, 0.1, 0.5, 0.5]])  # x1,y1 x2,y2 or x,y w,h\n",
    "print(f\"Target: {target}\")\n",
    "output = resnet_detector(input)\n",
    "loss = criterion(output, target)\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Координаты обычно предсказываются в процентах от длины и ширины изображения.\n",
    "Таким образом, если bounding box целиком помещается на изображении, обе координаты будут находиться в интервале $[0 .. 1]$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/predict_key_points.png\" width=\"700\"></center>\n",
    "<center><em>Примеры предсказывания точек (Humphreys et al., 2020)</em></center>\n",
    "\n",
    "[Recent Progress in Appearance-based Action Recognition (Humphreys et al., 2020)](https://arxiv.org/abs/2011.12619)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По такому принципу работают многие модели для поиска различных ключевых точек.\n",
    "Например: на лице (facial landmarks) или теле человека."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Координаты прямоугольников мы предсказывать научились.\n",
    "\n",
    "Теперь усложним задачу: объект остается один, но может принадлежать к различным классам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/one_object.png\" width=\"650\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть к задаче локализации добавляется классификация.\n",
    "\n",
    "Задачу классификации мы умеем решать:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/class_prediction.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остается объединить классификацию с регрессией: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/multitask_loss_0.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого нужно одновременно предсказывать:\n",
    "\n",
    "*   вероятность принадлежности к классам,\n",
    "*   координаты ограничивающего прямогульника (bounding box).\n",
    "\n",
    "Тогда выход последнего слоя будет иметь размер: \n",
    "$$N + 4$$\n",
    "\n",
    "где N — количество классов (1000 для ImageNet), \n",
    "\n",
    "а 4 числа — это координаты одного boundig box (x1,y1,x2,y2 или x,y,w,h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Как описать функцию потерь для такой модели?**\n",
    "\n",
    "Можно суммировать loss для классификации и loss для регрессии.\n",
    "\n",
    "$ \\large L_{total} = L_{crossentropy}+L_{mse}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/multitask_loss.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако значения разных loss могут иметь разные порядки, поэтому приходится подбирать весовые коэффициенты для каждого слагаемого. \n",
    "В общем случае функция потерь будет иметь вид:\n",
    "\n",
    "$$\\large L_{total} = \\sum _iw_iL_i$$\n",
    "где $w_i$ — весовые коэффициенты каждой из функций потерь. \n",
    "\n",
    "Они являются гиперпараметрами модели и требуют подбора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор весов для каждой компоненты loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно подбирать веса компонентов loss в процессе обучения. Для этого к модели добавляется дополнительный слой:\n",
    "\n",
    "[Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics(Alex Kendall et al., 2018)](https://arxiv.org/abs/1705.07115)\n",
    "\n",
    "\n",
    "[Пример реализации MultiTask learning](https://github.com/Hui-Li/multi-task-learning-example-PyTorch/blob/master/multi-task-learning-example-PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектирование нескольких объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как быть, если объектов несколько?\n",
    "\n",
    "\n",
    "Для каждого объекта нужно вернуть координаты(x1,y1,x2,y2) и класс (0 .. N). \n",
    "Соответственно, количество выходов модели надо увеличивать.\n",
    "\n",
    "Но нам неизвестно заранее, сколько объектов будет на изображении:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/object_detection_multiple_object.png\" width=\"1000\"> \n",
    "\n",
    "[Stanford University CS231n: Detection and Segmentation](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cкользящее окно**\n",
    "Одним из вариантов решения этой проблемы является применение классификатора ко всем возможным местоположениям объектов. Классификатор предсказывает, есть ли на выбранном фрагменте изображения один из интересующих нас объектов. Если нет, то фрагмент классифицируется как \"фон\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/naive_way_object_detection_multiple_object.gif\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\">Detection and Segmentation</a></em></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемой данного подхода является необходимость применять классификатор к огромному количеству различных фрагментов, что крайне дорого с точки зрения вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сократить количество возможных окон для детекции, применяются разные способы:\n",
    "\n",
    "1. На исходном изображении эвристический алгоритм предсказывает области, где скорее всего находятся объекты (~2K) (R-CNN).\n",
    "2. Эти области предсказывает нейросеть (подсеть) (~2K) (Two stage detectors:  [Faster R-CNN](https://arxiv.org/abs/1506.01497)).\n",
    "3. Эвристически задаются центры и размеры окон, которые плотно покрывают все изображение(~ 10K окон), (One stage detector: [SSD](https://arxiv.org/abs/1512.02325), YOLO).\n",
    "4. Нейросеть предсказывает вероятность нахождения объекта и его размеры для каждой точки на карте признаков, так как на последних слоях карты признаков небольшие. (Anchor free: [FCOS](https://arxiv.org/abs/1904.01355))\n",
    "5. Если в качестве кодировщика используется сеть на базе рхитектуры ViT, то предсказание делается для каждого patch (1K <) (DETR [link text](https://arxiv.org/abs/2005.12872)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/detector_types.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая схема работы детектора**:\n",
    "\n",
    "* Изображение единожды пропускается через базовую часть сети (encoder),  дальнейшие предсказания делаются на карте признаков.\n",
    "* Во всех случаях предсказываются смещение и масштабирование для финальных bounding box.\n",
    "* Для каждого bounding box предсказывается класс.\n",
    "* Предсказывается заведомо больше bounding box, чем может быть объектов на изображении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/detector_scheme.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss\n",
    "\n",
    "Лосс складывается из лосс для классификации $L_{conf}$ и лосс для детекции $L_{loc}$\n",
    "\n",
    "$$\\large L_{det} = L_{conf} + \\alpha L_{loc}$$\n",
    "\n",
    "При этом в $L_{loc}$ учитываются **не все** предсказанные bounding box, а только те которые наилучшем образом пересекаются с GT (bbox из разметки). Фильтрация может проходить по порогу или при помощи алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь возникает другая проблема: в районе объекта алгоритм генерирует множество ограничивающих прямоугольников (bounding box), которые частично перекрывают друг друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/non_max_suppression.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы избавиться от них, используется алгоритм\n",
    "NMS (Non maxima suppression). Его задача — избавиться от bounding boxes, которые накладаваются на истинный:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/non_max_suppression_pseudo_code.png\" width=\"1000\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c\"> Non-maximum Suppression (NMS)</a></p></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "здесь $B$ — это массив всех bounding box,  $C$ — массив предсказаний модели относительно наличия объекта в соответствующем bounding box\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки схожести обычно используется метрика IoU(same == IoU), а значение IoU ($\\lambda_{nms}$), при котором bounding boxes считаются принадлежащими одному объекту, является гиперпараметром (часто 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В PyTorch алгоритм NMS доступен в модуле torchvision.ops\n",
    "\n",
    "`torchvision.ops.nms(boxes, scores, iou_threshold)`,\n",
    "где:\n",
    "* `boxes` — массив bounding box,\n",
    "* `scores` — предсказанная оценка,\n",
    "* `iou_threshold` — порог IoU, NMS отбрасывает все перекрывающиеся поля с $IoU> iou\\_threshold$\n",
    "\n",
    "[[doc] torchvision.ops.nms](https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Soft NMS](https://github.com/Gan4x4/ml_snippets/blob/main/CV/SoftNMS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone для детекторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Aggregation Network Module(PAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В backbone использующимися в детекторах на базе сверточных сетей применяются подходы развивающие идеи FPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/pan.png\" width=\"600\"></center>\n",
    "\n",
    "<center><p>\n",
    "<a href =\"https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Path_Aggregation_Network_CVPR_2018_paper.pdf\" >[Path Aggregation Network for Instance Segmentation]</a>\n",
    "</center></p>\n",
    "\n",
    "\n",
    "а) Строится пирамида признаков, при этом некоторые карты признаков дополняются  признакими с более ранних слоев (красный пунктир)\n",
    "\n",
    "b) Затем на основе последнего слоя FPN строится еще одна, (Bottom-up) и опять новые  карты признаков дополняются признакими полученными на первом уровне  (зеленый пунктир)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swin Transformer (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Aug 2021 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n",
    "\n",
    "\n",
    "Применять ViT напрямую для задач сегментации и детектирования не слишком эффективно, так как при больших размерах patch (16x16) не получится получить точные границы объектов.\n",
    "\n",
    "А при уменьшении размеров patch будет требоваться все больше ресурсов, так как сложность self-attention $O(n^{2})$ пропорциональна квадрату количества элементов на входе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/swin_vs_vit.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/2103.14030.pdf\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы решают проблему при помощи двух усовершенствований.\n",
    "\n",
    "Self-attention применяется не ко всему изображению сразу, а к его большим фрагментам, окнами.\n",
    "\n",
    "На первый взгляд это возвращает проблему сверток, про которую мы говорили:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/cnn_fail.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Указываем сети, куда смотреть, и это помешает оценить взаимное влияние признаков расположенных на разных углах карты.\n",
    "\n",
    "Чтобы не допустить этой проблемы, на каждом следующем transformer-слое окно сдвигается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/swin_window_shift.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/2103.14030.pdf\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p> </em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом сеть может выучить влияние любого патча на любой. При этом не требуется увеличивать количество входов self-attention блока, и количество вычислений не растет. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее пространственные размеры карт признаков уменьшаются аналогично тому, как это происходит в сверточных сетях. Для сегментирования и детектирования используется принцип FPN: признаки с разных пространственных карт агрегируются для предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/swin_architecture.png\" width=\"1200\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/2103.14030.pdf\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch merging здесь — это конкатенация эмбеддингов с последующей подачей на вход линейного слоя.\n",
    "\n",
    "Фрагмент из 4-х эмбеддингов 2x2xC конкатенируются. Получаем один тензор 1x1x4C.\n",
    "\n",
    "Затем подаем его на вход линейному слою,  уменьшающему число каналов в 2 раза,\n",
    "получаем новый эмбеддинг размерностью 1x1x2C.\n",
    "\n",
    "Таким образом, в отличие от традиционных трансформер-архитектур, размер embedding здесь меняется.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/swin_result.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/2103.14030.pdf\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход позволил достичь SOTA результатов как в задаче классификации, так и в задачах детектирования и сегментации. \n",
    "Авторы статьи позиционируют Swin Transformer как backbon решения широкого круга задач CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640) \n",
    "* [YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "* [YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "* [YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "* [YOLOv5 (Glenn Jocher Ultralytics,  June 2020)](https://github.com/ultralytics/yolov5)\n",
    "* [YOLOX: Exceeding YOLO Series in 2021 (Ge et al., June 2021)](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "* [YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors(Chien-Yao Wang et. al. July 2022)](https://arxiv.org/abs/2207.02696)\n",
    "\n",
    "*  [YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications(Chuyi Li et. al., Sept 2022)](https://arxiv.org/abs/2209.02976)\n",
    "\n",
    "*  \t[YOLOv8,\tUltralytics, Dec. 2022](\n",
    "https://github.com/ultralytics/ultralytics)\n",
    "[(documentation !)](https://docs.ultralytics.com/)\n",
    "\n",
    "* [YOLO6v3\tYOLOv6 v3.0: A Full-Scale Reloading Chuyi Li et. al., Dec. 2023](https://arxiv.org/abs/2301.05586)\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov6_performance.png\" width=\"1000\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/abs/2301.05586\">YOLOv6 v3.0: A Full-Scale Reloading\n",
    "</a></p> </em></center>\n",
    "\n",
    "\n",
    "Первая версия YOLO вышла в том же году что и SSD. На тот момент детектор несколько проигрывал SSD в точности.\n",
    "\n",
    "Однако благодаря усилиям Joseph Redmon проект поддерживался и развивался в течение нескольких лет.\n",
    "\n",
    " 3-я версия детектора оказалась настолько удачной, что даже в 2021 можно было прочесть:  \"YOLOv3, one of the most widely used detectors in industry\" [2021](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "Последующие версии создавались разными авторами. Полагаю что правильно считать их разными форками YOLOv3 а не новыми версиями. Даже нумерация условна, например статья про v7 датируется более ранней датой чем v6.\n",
    "\n",
    "В прикладных задачах я бы рекомендовал использовать YOLOv8 так как авторы выложили на свой сайт [документацию](https://docs.ultralytics.com/), а точность при скорости порядка 100 fps у всех современных моделей почти одинакова.\n",
    "\n",
    "В настоящий момент можно сказать, что YOLO — это оптимальный детектор по соотношению качества распознавания к скорости.\n",
    "\n",
    "Подробнее здесь: https://github.com/Gan4x4/ml_snippets/blob/main/Detectors_history.ipynb\n",
    "\n",
    "Мы же запустим одну из последних моделей:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOLOv8**\n",
    "\n",
    "По этой модели не публиковалась статья, зато есть [документация](https://docs.ultralytics.com/quickstart/)\n",
    "\n",
    "Попробуем попробуем запустить ее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "!pip install ultralytics\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инстанцируем модель по названию. Полный [список предобученных моделей](https://github.com/ultralytics/ultralytics#models) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "yolo8 = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектируем объекты на изображении из COCO.\n",
    "\n",
    "Из коробки работает с изображениями в разных форматах и даже url, автоматически меняет размер входного изображения, возвращает объект с результатами...\n",
    "\n",
    "https://docs.ultralytics.com/modes/predict/#arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = yolo8.predict(\n",
    "    \"http://images.cocodataset.org/val2017/000000448263.jpg\",  # baseball\n",
    "    conf=0.25,  # for NMS\n",
    "    iou=0.7,\n",
    ")  # for NMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве результата возвращается список объектов которые содержат информацию полную информацию о детектировании.\n",
    "\n",
    "https://docs.ultralytics.com/modes/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))  # contains detections for one image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У него есть методы для получения списка координат предсказанных bounding box после NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.ultralytics.com/reference/results/\n",
    "print(results[0].boxes.data)  # x1,y2,x2,y2,conf,class_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может даже создать картинку с нарисоваными bounding box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "pil_with_bbox = results[0].plot()\n",
    "plt.imshow(pil_with_bbox)  # BGR?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Похоже объект results хранит картинку в BGR формате, переведем в RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pil_with_bbox[..., ::-1])  # BGR->RGB\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/detection_instance_segmentation.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mask R-CNN** (Detectron) — концептуально простая, гибкая и общая схема сегментации объектов. Подход эффективно обнаруживает объекты на изображении и одновременно генерирует высококачественную маску сегментации для каждого объекта. \n",
    "\n",
    "Метод, названный Mask R-CNN, расширяет Faster R-CNN (который мы обсуждали ранее), добавляя ветвь для предсказания маски объекта параллельно с существующей ветвью для распознавания *bounding boxes*. \n",
    "\n",
    "Код доступен [по ссылке](https://github.com/facebookresearch/Detectron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/mask_r_cnn.png\" width=\"750\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\"> Detection and Segmentation </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Модель Mask R-CNN](https://pytorch.org/vision/stable/models.html#mask-r-cnn)\n",
    "\n",
    "[Пример запуска Mask R-CNN есть в документации PyTorch](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим картинку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "!wget \"http://images.cocodataset.org/val2017/000000448263.jpg\"\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "I = Image.open(\"000000448263.jpg\")\n",
    "t = ToTensor()(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import (\n",
    "    maskrcnn_resnet50_fpn,\n",
    "    MaskRCNN_ResNet50_FPN_Weights,\n",
    ")\n",
    "\n",
    "mask_rcnn = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "clear_output()\n",
    "mask_rcnn.eval()\n",
    "predictions = mask_rcnn(t.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого изображения возвращается словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый ключ содержит тензор. Количество элементов в котором равно количеству детектированных объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0][\"masks\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_masks(masks, classes=None):\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=4, ncols=8, figsize=(20, 10))\n",
    "    i = 0\n",
    "    for row in range(4):\n",
    "        for col in range(8):\n",
    "            if classes is not None:\n",
    "                ax[row, col].set_title(int(classes[i]))\n",
    "            ax[row, col].imshow(masks[i])\n",
    "            ax[row, col].axis(\"off\")\n",
    "            i += 1\n",
    "            if i >= len(masks):\n",
    "                plt.show()\n",
    "                return\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_masks(\n",
    "    predictions[0][\"masks\"].detach().squeeze(1), predictions[0][\"labels\"].detach()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маски не бинарные, при необходимости можем их бинаризовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = predictions[0][\"masks\"][2].detach().squeeze(0)\n",
    "mask = mask > 0.5\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У YOLOv8 заявлена поддержка сегментации:\n",
    "\n",
    "https://docs.ultralytics.com/tasks/segment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "yolo_seg = YOLO(\"yolov8m-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_results = yolo_seg(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_results[0].masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_results[0].masks.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_results[0].boxes.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "pil_with_bbox = seg_results[0].plot(boxes=True, masks=True)\n",
    "plt.imshow(pil_with_bbox[..., ::-1])  # BGR?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panoptic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[COCO panoptic](https://cocodataset.org/#panoptic-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/panoptic_segmentation.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://kharshit.github.io/blog/2019/10/18/introduction-to-panoptic-segmentation-tutorial\">Introduction to Panoptic Segmentation: A Tutorial</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#OWL-ViT (2022)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer for Open-World Localization\n",
    "\n",
    "[Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230)\n",
    "\n",
    "Код: https://huggingface.co/docs/transformers/model_doc/owlvit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектирование по произвольному текстовому запросу.\n",
    "\n",
    "Модели на вход подается изображение и набор текстов. Модель предсказывает bbox , и сравнивает их эмбеддинги с эмбеддингами текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/owl-vit.png\" width=\"1000\">\n",
    "\n",
    "[Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Лосс для bbox такой же как в [DETR](https://arxiv.org/abs/2005.12872) (`bipartite matching`)\n",
    "*  Эмбеддинги учаться аналогично [CLIP](https://arxiv.org/abs/2103.00020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "!pip install transformers\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем модель и `processor`.\n",
    "\n",
    "`processor` — это класс, отвечающий за предобработку и постобработку данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\n",
    "    \"google/owlvit-base-patch32\",\n",
    ")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\n",
    "    \"google/owlvit-base-patch32\",\n",
    ")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"loaded\")  # suppress huge output of model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N \"http://images.cocodataset.org/val2017/000000448263.jpg\"\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "img = Image.open(\"000000448263.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим набор текстовых описаний для поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"cap\", \"botle\", \"text\", \"boy\", \"player\"]  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [img]\n",
    "batch_size = len(batch)\n",
    "with torch.inference_mode():\n",
    "    inputs = processor(\n",
    "        text=texts * batch_size,  # copy the same text for all images\n",
    "        images=[img],  # batch of images in PIL format\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)  # return object of type OwlViTObjectDetectionOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bboxex\", outputs[\"pred_boxes\"].shape)  # batch * total_boxes * 4 coords per box\n",
    "print(\"logits\", outputs[\"logits\"].shape)  # batch * total_boxes * text_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постобработка\n",
    "\n",
    "При предобработке изображения могут масштабироваться (resize), что бы получить корректные bbox а выходе, в метод можно подать список исходных размеров изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = img.size[::-1]  # WH -> HW\n",
    "target_sizes = torch.tensor([size])\n",
    "print(target_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = processor.post_process_object_detection(\n",
    "    threshold=0.1, outputs=outputs, target_sizes=target_sizes.to(device)  #\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.ops import box_convert\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def draw_bbox(img, bb, color, labels=None, xywh=True):\n",
    "    t_img = torch.tensor(img).permute(2, 0, 1)  # to tensor CHW\n",
    "    bb = bb[:, :4].clone().detach()  # take only coords\n",
    "    if xywh:\n",
    "        bb = box_convert(bb, \"xywh\", \"xyxy\")  # convert from COCO format\n",
    "    img_with_bb = draw_bounding_boxes(t_img, bb, colors=color, width=2, labels=labels)\n",
    "    return img_with_bb.permute(1, 2, 0).numpy()  # back to numpy HWC\n",
    "\n",
    "\n",
    "def show_owl_results(results):\n",
    "    labels = []\n",
    "    for i, text_idx in enumerate(results[0][\"labels\"]):\n",
    "        labels.append(texts[text_idx] + f\" {results[0]['scores'][i]:.2f}\")\n",
    "    image_with_bb = draw_bbox(\n",
    "        np.array(img), results[0][\"boxes\"], \"lime\", labels=labels, xywh=False\n",
    "    )\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_with_bb)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_owl_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы получть больше результатов меняем порог"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_results = processor.post_process_object_detection(\n",
    "    threshold=0.07,  # change threshold\n",
    "    outputs=outputs,\n",
    "    target_sizes=target_sizes.to(device),\n",
    ")\n",
    "\n",
    "show_owl_results(coarse_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ёще пример. ](https://colab.research.google.com/github/google-research/scenic/blob/main/scenic/projects/owl_vit/notebooks/OWL_ViT_minimal_example.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SAM (2023)\n",
    "\n",
    "[Segment Anything](https://arxiv.org/abs/2304.02643)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель возвращает набор масок соответствующих входу. Классы объектов не используются.\n",
    "\n",
    "В качестве входа может подаваться: \n",
    "\n",
    "*  набор точек\n",
    "*  набор bounding box\n",
    "*  маски\n",
    "*  текст (подержка в коде пока не реализована)\n",
    "*  изображение \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/sam_overview.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучалась на огромном датасете, частично размеченном в unsupervise режиме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/sam_architecture.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим пакет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка весов\n",
    "\n",
    "https://github.com/facebookresearch/segment-anything#model-checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT-H\n",
    "!wget -N https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry  # SamPredictor, sam_model_registry\n",
    "\n",
    "# model_type = 'vit_h'\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sam.to(device=device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_1.png -O cat.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"http://images.cocodataset.org/val2017/000000448263.jpg\"\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "img = Image.open(\"000000448263.jpg\")\n",
    "np_im = np.array(img)  # HWC format\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем все маски\n",
    "\n",
    "https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим эмбеддинг (на CPU выполняется долго) и предскажем все маски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "masks = mask_generator.generate(np_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получаем список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0][\"segmentation\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones(\n",
    "        (\n",
    "            sorted_anns[0][\"segmentation\"].shape[0],\n",
    "            sorted_anns[0][\"segmentation\"].shape[1],\n",
    "            4,\n",
    "        )\n",
    "    )\n",
    "    img[:, :, 3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann[\"segmentation\"]\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(img)\n",
    "show_anns(masks)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем по точкам.\n",
    "https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
    "Сначала создаем эмбеддинг. Он хранится внутри модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from segment_anything import SamPredictor\n",
    "\n",
    "\n",
    "predictor = SamPredictor(sam)\n",
    "predictor.set_image(np_im)  # create embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь получаем предсказания, указав точки, которые относятся к объекту и фону:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=np.array([[200, 200], [1, 1]]),  # point coords\n",
    "    point_labels=np.array([1, 0]),  # 1 - object(foreground), 0 - background\n",
    "    # box\n",
    "    # mask_input\n",
    "    multimask_output=True,  # return 1 or 3 masks because of the ambiguous input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Masks count\", len(masks))\n",
    "print(\"Scores\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(masks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "show_mask(masks[2], plt.gca())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества детекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP — mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP (Average Precision — средняя точность) — это площадь под сглаженной PR-кривой.\n",
    "\n",
    "m (mean) —  усредненная для разных порогов IoU.\n",
    "\n",
    "Разберемся, что это значит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision & recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** измеряет, насколько точны предсказания сети (т.е. процент правильных предсказаний)\n",
    "\n",
    "**Recall** измеряет, насколько хорошо сеть находит все положительные срабатывания (*positives*). Например, мы можем найти 80% возможных положительных срабатываний в наших K лучших предсказаниях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот их математические определения:\n",
    "\n",
    "$\\displaystyle\\mathrm{Precision} = \\frac{TP}{TP+FP}$\n",
    "\n",
    "$\\displaystyle\\mathrm{Recall} = \\frac{TP}{TP+FN}$\n",
    "\n",
    "где $TP$ — True Positive, $TN$ — True Negative, $FP$ — False Positive, $FN$ — False Negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы пытаемся детектировать яблоки на фотографиях. Предположим, мы обработали 20 фотографий (на 10 фотографиях по одному яблоку и на 10 фотографиях яблок нет) и обнаружили что:\n",
    "\n",
    "* в 7 случаях наша нейросеть обнаружила яблоко там, где оно было на самом деле (True Positive);\n",
    "* в 3 случаях не обнаружила яблоко там, где оно было (False Negative);\n",
    "* в 4 случаях обнаружила яблоко там, где его не было (False Positive);\n",
    "* в 6 случаях правильно определила, что на фотографии яблок нет (True Negative).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем precision и recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(TP, FP):\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "\n",
    "pres = precision(TP=7, FP=4)\n",
    "rec = recall(TP=7, FN=3)\n",
    "\n",
    "print(\"Precision = %.2f\" % pres)\n",
    "print(\"Recall = %.2f\" % rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRCurve\n",
    "\n",
    "Зная Precision и Recall для разных порогов, можно построить так называемую [PR-кривую](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/).\n",
    "\n",
    "[Как построить PR curve](https://github.com/Gan4x4/ml_snippets/blob/main/Training/PR_curve.ipynb).\n",
    "\n",
    "При помощи таких кривых часто оценивают модели в задачах классификации, когда данные не сбалансированы.\n",
    "\n",
    "Для получения значений Precision и Recall требуются:\n",
    "\n",
    "*  Ground True метки;\n",
    "*  уверенность модели в каждом предсказании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 1, 0])  # Labels\n",
    "y_pred = np.array([0.8, 0.1, 0.2])  # Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этого достаточно для того, чтобы построить PR-кривую для предсказаний одного класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def pr_curve(y_true, y_pred):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    plt.plot(recall, precision, marker=\"o\")\n",
    "    for i, t in enumerate(thresholds):\n",
    "        plt.annotate(str(t), (recall[i], precision[i]))\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xlabel(\"recall\")\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.title(\"PR curve\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pr_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU вместо метки класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "При детектировании у нас  **нет меток класса** для предсказанных bounding \n",
    "box.\n",
    "\n",
    "Есть предсказанные детектором bbox в виде вида:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "detections = np.array(\n",
    "    [\n",
    "        [290, 50, 170, 160, 0.7, 53],  # x, y, w, h, confidence, class_num\n",
    "        [10, 200, 190, 180, 0.8, 53],\n",
    "        [310, 250, 120, 130, 0.75, 53],\n",
    "    ]\n",
    ")\n",
    "pd.DataFrame(\n",
    "    data=detections, columns=[\"x\", \"y\", \"width\", \"height\", \"confidence\", \"class_num\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "И есть Ground True bounding box из разметки:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# !wget -N \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "!wget -N \"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/annotations_trainval2017.zip\"\n",
    "!unzip -n annotations_trainval2017.zip\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")\n",
    "clear_output()\n",
    "\n",
    "apples_img_id = 60855  # if of some image with apples\n",
    "apple_cat_id = 53  # apple\n",
    "\n",
    "ann_id = coco.getAnnIds(imgIds=[apples_img_id])\n",
    "anns = coco.loadAnns(ann_id)\n",
    "\n",
    "gt_bbox = []\n",
    "for ann in anns:\n",
    "    if ann[\"category_id\"] == apple_cat_id:\n",
    "        gt_bbox.append(ann[\"bbox\"] + [ann[\"category_id\"]])\n",
    "\n",
    "\n",
    "pd.DataFrame(data=gt_bbox, columns=[\"x\", \"y\", \"width\", \"height\", \"class_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея состоит в том, чтобы посчитать максимальный IoU между этими bounding box и использовать его вместо метки класса (True/False).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import skimage\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "\n",
    "def draw_bbox(img, bb_xywh, color):\n",
    "    t_img = torch.tensor(img).permute(2, 0, 1)  # to tensor CHW\n",
    "    xywh = torch.tensor(bb_xywh)[:, :4]  # take only coords\n",
    "    bb = box_convert(xywh, \"xywh\", \"xyxy\")  # convert from COCO format\n",
    "    img_with_bb = draw_bounding_boxes(t_img, bb, colors=color, width=4)\n",
    "    return img_with_bb.permute(1, 2, 0).numpy()  # back to numpy HWC\n",
    "\n",
    "\n",
    "img_info = coco.loadImgs(apples_img_id)\n",
    "\n",
    "img = skimage.io.imread(img_info[0][\"coco_url\"])\n",
    "img = draw_bbox(img, detections, \"blue\")\n",
    "img = draw_bbox(img, gt_bbox, \"lime\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить максимальное значение IoU для каждого предсказанного bbox,  воспользуемся функцией [torchvision.ops.box_iou](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html).\n",
    "\n",
    "Перед этим преобразуем формат координат к `(x1,y1,x2,y2)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_convert\n",
    "\n",
    "gt = box_convert(torch.tensor(gt_bbox)[:, :4], \"xywh\", \"xyxy\")\n",
    "pred = box_convert(torch.tensor(detections)[:, :4], \"xywh\", \"xyxy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И получим IoU для всех возможных пар bbox:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "pairwise_iou = box_iou(gt, pred)\n",
    "print(pairwise_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас интересует только максимальный:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou, _ = pairwise_iou.max(dim=0)\n",
    "print(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем эти значения в таблицу.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions = torch.tensor(detections)[:, 4]\n",
    "predictions = torch.vstack((predictions, iou)).T\n",
    "\n",
    "pd.DataFrame(data=predictions, columns=[\"confidence\", \"iou\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тперь будем использовать IoU как GT метку класса: если он больше некоторго порога, то предсказание верное.\n",
    "\n",
    "В нашем случае будем считать, что если IoU≥0.5, то предсказание правильное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = iou > 0.5\n",
    "\n",
    "predictions = torch.vstack((predictions[:, 0], gt)).T\n",
    "detection_results = pd.DataFrame(data=predictions, columns=[\"confidence\", \"gt\"])\n",
    "detection_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть все необходимое для того, чтобы построить PR-кривую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "precision, recall, thresholds = precision_recall_curve(gt, predictions[:, 0])\n",
    "plt.plot(recall, precision, marker=\"o\")\n",
    "plt.ylim([0, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP это площадь под PR кривой.\n",
    "\n",
    "$\\large \\displaystyle AP = \\int_0^1p(r)dr$\n",
    "\n",
    "Имея значения Precision и Recall, можно посчитать AP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap = average_precision_score(gt, predictions[:, 0])\n",
    "print(\"AP\", ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним это значение с площадью под кривой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "auc = metrics.auc(recall, precision)\n",
    "print(\"AUC\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, они не совпали. Дело в том, что при подсчете AP кривую сглаживают:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def smooth_precision(precision):\n",
    "    smooth_prec = []\n",
    "    for i in range(1, len(precision) + 1):\n",
    "        max = precision[:i].max()\n",
    "        smooth_prec.append(max)\n",
    "    return smooth_prec\n",
    "\n",
    "\n",
    "detection_results = detection_results.sort_values(\"confidence\", ascending=False)\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    detection_results[\"gt\"], detection_results[\"confidence\"]\n",
    ")\n",
    "\n",
    "smoothed_precision = smooth_precision(precision)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall, precision, marker=\"o\")\n",
    "plt.plot(recall, smoothed_precision, marker=\"o\")\n",
    "plt.ylim([0, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_auc = metrics.auc(recall, smoothed_precision)\n",
    "print(\"smooth AUC \", smooth_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы посчитали AP для одного класса, но в датасете их много.\n",
    "Логично усреднить значения AP для разных классов. В некоторых бенчмарках после такого усреднения к названию метрики добавляют приставку m(mean)\n",
    "\n",
    "*AP is averaged over all categories. Traditionally, this is called “mean average precision” (mAP). We make no distinction between AP and mAP*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP для разных порогов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы выбрали для IoU порог 0.5.\n",
    "\n",
    "Но можно посчитать AP для разных порогов, и затем тоже усреднить. В этом случае в названии метрики указывается минимальный и максимальный пороги. \n",
    "Например AP@[.5:.95] соответствует среднему AP для IoU от 0.5 до 0.95 с шагом 0.05. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "t_gt_bbox = torch.tensor(gt_bbox)\n",
    "t_detections = torch.tensor(detections)\n",
    "\n",
    "map_obj = MeanAveragePrecision(\"xywh\", iou_thresholds=[0.5, 0.55, 0.6, 0.65])\n",
    "results = map_obj(\n",
    "    preds=[\n",
    "        {\n",
    "            \"boxes\": t_detections[:, :4],  # xywh\n",
    "            \"scores\": t_detections[:, 4],  # confidence\n",
    "            \"labels\": t_detections[:, 5],  # class num\n",
    "        }\n",
    "    ],\n",
    "    target=[\n",
    "        {\"boxes\": t_gt_bbox[:, :4], \"labels\": t_gt_bbox[:, 4]}  # xywh  # class num\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"mAP@[0.5:0.65] = \", results[\"map\"].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практические соображения\n",
    "\n",
    "* Сегментация: если данных мало и они специфические, используйте UNet. \n",
    "* Если данных достаточно, используйте DeepLab в качестве baseline.\n",
    "* Используйте пакеты SMP и torchmetrics.\n",
    "* Для детектирования используйте YOLOv8 или YOLOv6.\n",
    "* Если ваша задача связана с трекингом, обратитесь к преподавателю.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"6\">Список использованной литературы</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">COCO</font>\n",
    "\n",
    "[Подробнее о том, как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch).\n",
    "\n",
    "[Подробнее о разметке COCO](https://cocodataset.org/#format-data)\n",
    "\n",
    "[Что такое  run-length encoding - RLE](https://en.wikipedia.org/wiki/Run-length_encoding)?\n",
    "\n",
    "[Видео-разбор Run-length encoding](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">Семантическая сегментация</font>\n",
    "\n",
    "[FCN Semantic segmenation](https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1)\n",
    "\n",
    "[Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
    "\n",
    "[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (He et al., 2014)](https://arxiv.org/abs/1406.4729)\n",
    "\n",
    "[Блог-пост про семантическую сегментацию](https://www.jeremyjordan.me/semantic-segmentation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">Детектирование</font>\n",
    "\n",
    "[Recent Progress in Appearance-based Action Recognition (Humphreys et al., 2020)](https://arxiv.org/abs/2011.12619)\n",
    "\n",
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector (Liu et al., 2015)](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "[Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "[Статья про Focal Loss от Facebook AI Research](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection (Sergelius et al., 2016)](https://arxiv.org/abs/1612.0314)\n",
    "\n",
    "[Understanding feature pyramid networks for object detection - FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)\n",
    "\n",
    "[You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640) \n",
    "\n",
    "[YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "\n",
    "[YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "\n",
    "[YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "\n",
    "[YOLOv5 (Glenn Jocher)](https://github.com/ultralytics/yolov5)\n",
    "\n",
    "[YOLOX: Exceeding YOLO Series in 2021 (Ge et al., 2021)](https://arxiv.org/abs/2107.08430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">Hard Example Mining</font>\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining (Shrivastava et al., 2016)](https://arxiv.org/abs/1604.03540)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors (Yu et al., 2018)](https://arxiv.org/abs/1804.04606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">DINO</font>\n",
    "\n",
    "[Emerging Properties in Self-Supervised Vision Transformers (Caron et al., 2021)](https://arxiv.org/abs/2104.14294)\n",
    "\n",
    "[Отличное видео объяснение статьи DINO](https://www.youtube.com/watch?v=h3ij3F3cPIk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">Другое</font>\n",
    "\n",
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "[Блог-пост про 2d свертки с помощью перемножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
