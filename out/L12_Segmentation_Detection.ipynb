{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Сегментация и детектирование</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи компьютерного зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе предыдущих занятий, мы подробно рассмотрели задачу классификации изображений.\n",
    "\n",
    "\n",
    "Но порой недостаточно знать, что на изображении есть объект определенного класса. Важно, где именно расположен объект. В ряде случаев нужно знать еще и точные границы объекта. Например, если речь идет о рентгеновском снимке или изображении клеток ткани, полученном с микроскопа.\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/classification_semantic_segmentation.png\" width=\"600\">\n",
    "\n",
    "\n",
    "Определение того, какие фрагменты изображения принадлежат объектам определенных классов - это задача **сегментации** (segmentation).\n",
    "\n",
    "\n",
    "Если нас интересуют не индивидуальные объекты, а только тип(класс) объекта которым занят конкретный пиксел (как в случае с клетками под микроскопом) то говорят о **семантической сегментации** (semantic segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нас интересуют конкретные объекты, и при этом достаточно знать только область, в которой объект локализован, то это задача **детектирования** (Detection)\n",
    "\n",
    "В качестве примера такой задачи можно рассмотреть подсчет количество китов на спутниковом снимке.\n",
    "\n",
    "\n",
    "Если же важны и индивидуальные объекты и их точные границы, то это уже задача **Instance segmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset COCO — Common Objects in Context\n",
    "\n",
    "Прежде чем говорить о способах решения этих задач, надо разобраться с форматами входных данных. Сделаем это на примере COCO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из наиболее популярных датасатов, содержащий данные для сегментации и детектирования. Он содержит более трёхсот тысяч изображений, большая часть из которых размечена и содержит следующую информацию:\n",
    "- Категории\n",
    "- Маски\n",
    "- Ограничивающие боксы (*bounding boxes*)\n",
    "- Описания (*captions*)\n",
    "- Ключевые точки (*keypoints*)\n",
    "- И многое другое\n",
    "\n",
    "Формат разметки изображений, использованный в этом датасете, нередко используется и в других наборах данных. Как правило, он упоминается просто как \"COCO format\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с датасетом используется пакет `pycocotools`.\n",
    "\n",
    "[Подробнее о том, как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим формат аннотаций на примере одной записи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"cat\"])  # cats IDs\n",
    "print('class ID(cat) = %i' % catIds[0])\n",
    "\n",
    "imgIds = coco.getImgIds(catIds=catIds)  # Filtering dataset by tag\n",
    "print(\"All images: %i\" % len(imgIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим метаданные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = coco.loadImgs(imgIds[0])  # 1 example\n",
    "img = img_list[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на изображение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконвертируем в PIL формат для удобства дальнейшей работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(pil_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категории в COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на примеры категорий в нашем датасете. Отобразим каждую 10ую категорию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = coco.loadCats(coco.getCatIds())  # loading categories\n",
    "num2cat = {}  \n",
    "print(\"COCO categories: \")\n",
    "for cat in cats:\n",
    "    num2cat[cat[\"id\"]] = cat[\"name\"]\n",
    "    if cat[\"id\"] in range(0, 80, 10):\n",
    "        print(cat[\"id\"], \":\", cat[\"name\"], end=\"   \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете также есть категория **0**. Ее используют для обозначения класса фона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также существуют надкатегории. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'cats[2]: {cats[2]}')\n",
    "print(f'cats[3]: {cats[3]}')\n",
    "\n",
    "nms = set([cat[\"supercategory\"] for cat in cats])\n",
    "print(\"COCO supercategories: \\n{}\".format(\"\\n\".join(nms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разметка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо метаданных нам доступна разметка ([подробнее о разметке](https://cocodataset.org/#format-data)), давайте её загрузим и отобразим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "plt.imshow(I)\n",
    "plt.axis(\"off\")\n",
    "coco.showAnns(anns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На изображении можно увидеть разметку пикселей изображения по классам. То есть, пиксели из объектов, относящихся к интересующим классам, приписываются к классу этого объекта. К примеру, можно увидеть объекты двух классов: \"cat\" и \"keyboard\". \n",
    "\n",
    "Давайте теперь посмотрим, из чего состоит разметка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_anns(anns):\n",
    "    for i, a in enumerate(anns):\n",
    "        print(f\"\\n#{i}\")\n",
    "        for k in a.keys():\n",
    "            if k == \"category_id\" and num2cat.get(a[k], None):\n",
    "                print(k, \": \", a[k], num2cat[a[k]])  # Show cat. name\n",
    "            else:\n",
    "                print(k, \": \", a[k])\n",
    "\n",
    "dump_anns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что аннотация изображения может состоять из описаний нескольких объектов, каждое из которых содержит следующую информацию:\n",
    "* `segmentation` - последовательность пар чисел ($x$, $y$), координат каждой из вершин \"оболочки\" объекта;\n",
    "* `area` - площадь объекта;\n",
    "* `iscrowd` - информация о том, находится в оболочке один объект или же несколько, но слишком много для пообъектной разметки (толпа людей, к примеру);\n",
    "* `image_id` - идентификатор изображения, к которому принадлежит описываемый объект;\n",
    "* `bbox` - *будет рассмотрен далее в ходе лекции*;\n",
    "* `category_id` - идентификатор категории, к которой относится данный объект;\n",
    "* `id` - идентификатор самого объекта.\n",
    "\n",
    "Попробуем посмотреть на пример, в котором `iscrowd = True` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (120, 60)\n",
    "\n",
    "catIds = coco.getCatIds(catNms=[\"people\"])\n",
    "annIds = coco.getAnnIds(catIds=catIds, iscrowd=True)\n",
    "anns = coco.loadAnns(annIds[0:1])\n",
    "\n",
    "dump_anns(anns)\n",
    "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(I)\n",
    "coco.showAnns(anns)  # People in the stands\n",
    "seg = anns[0][\"segmentation\"]\n",
    "print(\"Counts\", len(seg[\"counts\"]))\n",
    "print(\"Size\", seg[\"size\"])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя методы из `pycocotools`, можно с лёгкостью преобразовать набор вершин \"оболочки\" сегментируемого объекта в более удобный для отображения вид - в маску объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "msk = np.zeros(seg[\"size\"])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        msk = coco.annToMask(anns[i])\n",
    "        ax[row, col].imshow(msk, cmap = 'gray')\n",
    "        ax[row, col].set_title(num2cat[anns[i][\"category_id\"]])\n",
    "        ax[row, col].axis(\"off\")\n",
    "        i += 1\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых случаях, попиксельная разметка изображения может быть избыточной - к примеру, в случае, если необходимо посчитать количество человек на изображении, достаточно просто каким-то образом промаркировать каждого из них, после чего посчитать количество наших \"отметок\". Одним из вариантов маркировки является \"обведение\" объекта рамкой (`bounding boxes`), внутри которой он находится. Такая информация об объектах также сохранена в аннотациях формата COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "RGB_img = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "for i in range(len(anns)):\n",
    "    x, y, width, heigth = anns[i][\"bbox\"]\n",
    "    x, y, width, heigth = int(x), int(y), int(width), int(heigth)\n",
    "    if anns[i][\"category_id\"] == 1: # person\n",
    "        color = (255, 255, 255)\n",
    "    if anns[i][\"category_id\"] == 40: # glove\n",
    "        color = (0, 255, 0)\n",
    "    RGB_img = cv2.rectangle(RGB_img, (x, y), (x + width, y + heigth), color, 2)\n",
    "cv2_imshow(RGB_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Еще более глубокое понимание разметки:\n",
    "\n",
    "Что такое [run-length encoding - RLE](https://en.wikipedia.org/wiki/Run-length_encoding)\n",
    "\n",
    "[Видео-разбор](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семантическая сегментация (*Semantic segmentation*)\n",
    "\n",
    "*Сегментация изображения — задача поиска групп пикселей, каждая из которых характеризует один смысловой объект.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Технически это выглядит так.\n",
    "\n",
    "Есть набор изображений:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_1.png\" width=\"300\">\n",
    "\n",
    "Для каждого изображения есть маска W x H:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_2.png\" width=\"300\">\n",
    "\n",
    "Маска задает класс объекта для каждого пикселя:\n",
    "[ x,y - > class_num ]\n",
    "\n",
    "\n",
    "Набор таких изображений с масками это и есть наш датасет, на нем мы учимся.\n",
    "\n",
    "\n",
    "На вход модель получает новое изображение:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_3.png\" width=\"600\">\n",
    "\n",
    "и должна предсказать метку класса для каждого пикселя (маску).\n",
    "\n",
    "[ x,y - > class_num ] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Способы предсказания класса для каждого пикселя\n",
    "\n",
    "Давайте подумаем о том, как такую задачу можно решить.\n",
    "\n",
    "Из самой постановки задачи видно, что это задача классификации. Только не \n",
    "всего изображения, а каждого пикселя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Наивный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Простейшим вариантом решения является использование так называемого \"скользящего окна\" - последовательное рассмотрение фрагментов изображения. В данном случае, интересующими фрагментами будут небольшие зоны, окружающие каждый из пикселей изображения. К каждому из таких фрагментов применяется свёрточная нейронная сеть, предсказывающая, к какому классу относится центральный пиксель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/naive_way_predict_pixel_class.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### б) Разумный\n",
    "\n",
    "Понятно, что запускать классификатор для каждого пикселя абсолютно неэффективно, так как для одного изображения потребуется $H*W$ запусков.\n",
    "\n",
    "Можно пойти другим путем: получить карту признаков и по ней делать предсказание для всех пикселей разом.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого потребуется поменять привычную нам архитектуру сверточной сети следующим образом:\n",
    "\n",
    "* убрать слои уменьшающие пространственные размеры\n",
    "\n",
    "* убрать линейный слой в конце, заменив его сверточным\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/reasonable_way_predict_pixel_class.png\" width=\"700\">\n",
    "\n",
    "Теперь пространственные размеры выхода (W,H) будут равны ширине и высоте исходного изображения.\n",
    "\n",
    " \n",
    "Количество выходных каналов будет равно количеству классов, которые мы учимся предсказывать. \n",
    "\n",
    "\n",
    "Тогда можно использовать значения каждой из карт активаций, на выходе последнего слоя сети, как ненормированное значение вероятности принадлежности(core) каждого из пикселей к тому или иному классу. \n",
    "\n",
    "То есть номер канала с наибольшим значением будет соответствовать классу объекта, который изображает данный пиксел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    " \n",
    "last_layer_output = torch.randn((3, 32, 32)) # class_num, W,H\n",
    "print(last_layer_output.shape, last_layer_output[:, :3, :3]) # activation slice\n",
    "mask = torch.argmax(last_layer_output, dim=0) # class_nums prediction\n",
    "print(mask.shape)\n",
    "print(mask[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы на выходе сети получить нужное количество каналов, используется свертка 1x1.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/1x1_kernel_size_fully_connected_layer.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В лекции про сверточные сети мы говорили о том, что свертку 1x1 можно рассматривать как аналог полносвязанного слоя. Именно так она тут и работает.\n",
    "\n",
    "**Проблемы:**\n",
    "- так как нужно сохранить большое рецептивное поле, требуется много сверточных слоев ($L$ раз свёртка $3\\times3$ $\\to$ рецептивное поле $(1+2L)\\times(1+2L)$);\n",
    "- свертки медленно работают на полноразмерных картах активации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### в) Эффективный\n",
    "\n",
    "Сохраним большую часть традиционной структуры сети, включая слои downsampling (снижение пространственных размеров), извлечем основные признаки, а затем восстановим пространственные размеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/efficient_way_predict_pixel_class.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именно такой подход с некоторыми дополнениями используется на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автокодировщик\n",
    "\n",
    "Эта архитектура повторяет архитектуру автокодировщика (autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/encoder_loss.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая архитектура довольно популярна и применяется не только для сегментации: \n",
    "\n",
    "- сглаживание шума;\n",
    "- снижение размерности $\\to$ вектор-признак;\n",
    "- генерация данных.\n",
    "\n",
    "Этому будет посвящена одна из следующих лекций, сейчас же мы детально рассмотрим разжимающий блок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разжимающий слой\n",
    "\n",
    "Как реализовать декодировщик?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерполяция при увеличении разрешения\n",
    "\n",
    "Вспомним, как повышают разрешение для обычных изображений, а уже затем перейдем к картам признаков.\n",
    "\n",
    "Допустим, требуется увеличить изображение размером 2x2 до размера 4x4. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/upsample.png\" width=\"300\">\n",
    "\n",
    "\n",
    "Самый простой способ это сделать:\n",
    "- создать пустое изображение нужного размера\n",
    "- поделить его на 4 части (по числу пикселей исходного изображения)\n",
    "- каждую часть закрасить цветом соответствующего пикселя маленького изображения\n",
    "\n",
    "Это интерполяция методом ближайшего соседа(Nearest neighbor interpolation).\n",
    "\n",
    "Естественно она реализована в пакете [Pillow Image](https://pillow.readthedocs.io/en/stable/reference/Image.html):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_segmentation_1.png -O cat.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"cat.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "new_size = np.array(img.size) * 2\n",
    "resized = img.resize(new_size, resample=PIL.Image.NEAREST)\n",
    "resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это не самый лучший вид интерполяции: на изображении появились артефакты (квадраты).\n",
    "\n",
    "Чтобы избавиться от таких артефактов можно, значениям новых пикселей присваивать интерполированное значение яркости исходных.\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/change_size_of_image.png\" width=\"800\">\n",
    "\n",
    "Если для интерполяции используются значения четырех соседних пикселей это биленейная интерполяция. В качестве интерполированного значения используется взвешенное среднее этих четырёх пикселей. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def img_to_heatmap(img, ax, title): #Magik method to show img as heatmap\n",
    "  ax.axis('off')\n",
    "  ax.set_title(title)\n",
    "  array = np.array(img)\n",
    "  array = array[None, None, :]\n",
    "  sns.heatmap(array[0][0], annot=True, ax=ax, lw=1, cbar=False)\n",
    "\n",
    "\n",
    "# Fake image\n",
    "raw = np.array([[1, 3, 0, 1], [3, 3, 3, 7], [8, 1, 8, 7], [6, 1, 1, 1]], dtype=np.uint8)\n",
    "pil = Image.fromarray(raw)\n",
    "\n",
    "interp_nn = pil.resize((8, 8), resample=PIL.Image.NEAREST) \n",
    "interp_bl = pil.resize((8, 8), resample=PIL.Image.BILINEAR)\n",
    "\n",
    "# plot result\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "img_to_heatmap(raw,ax[0], 'Raster dataset')\n",
    "img_to_heatmap(interp_nn,ax[1], 'Nearest neighbor interpolation')\n",
    "img_to_heatmap(interp_bl,ax[2], 'Bilinear interpolation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Билинейная Интерполяция позволяет избавиться от резких границ, которые возникают при увеличении методом ближайшего соседа.  Существуют и другие виды интерполяции, использующие большее количество соседних пикселей.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подробнее про интерполяцию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Билинейная интерполяция рассматривает квадрат соседних пикселей. В качестве интерполированного значения используется взвешенное среднее этих четырёх пикселей. \n",
    "\n",
    "Отметим, что чем ближе интерполируемое значение находится к одному из известных пикселей, тем больший вес будет у значения этого пикселя при подсчёте взвешенной суммы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/pixel_iterpolation_2x2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда окрестность $2\\times2$ может быть не достаточна для сохранения важной информации о распределении цветов в окружающих пикселях (пример будет приведён далее). В таких случаях, можно попробовать использовать большую окрестность для интерполяции - к примеру, $4\\times4$. Такая интерполяция называется бикубической и имеет более сложную формулу, чем билинейная, однако основной принцип остаётся тем же - чем ближе пиксель к пикселю с известным значением, тем больше \"вес\" последнего при интерполяции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/pixel_iterpolation_4x4.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample в Pytorch\n",
    "\n",
    "К чему был этот разговор об увеличении картинок?\n",
    "\n",
    "Оказывается для увеличения пространственного разрешения карт признаков (feature maps) можно применять те же методы что и для изображений.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для увеличения пространственного разрешения карт признаков(карт активаций), в PyTorch используется класс `Upsample`. В нём доступны все упомянутые методы интерполяции, а также трилинейная интерполяция - аналог билинейной интерполяции, используемый для работы с трёхмерными пространственными данными (к примеру, видео). \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/upsample_pytorch.png\" width=\"1000\">\n",
    "\n",
    "[[doc] nn.Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html)\n",
    "\n",
    "[[doc] nn.functional.interpolate](https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html?highlight=interp#torch.nn.functional.interpolate)\n",
    "\n",
    "\n",
    "Таким образом, мы можем использовать `Upsample` внутри нашего разжимающего блока."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def upsample(pil, ax, mode=\"nearest\"):\n",
    "    tensor = TF.to_tensor(pil)\n",
    "    # Create upsample instance\n",
    "\n",
    "    if mode == 'nearest':\n",
    "        upsampler = nn.Upsample(scale_factor=2, mode=mode)\n",
    "    else:\n",
    "        upsampler = nn.Upsample(scale_factor=2, mode=mode, align_corners=True)\n",
    "        \n",
    "    tensor_128 = upsampler(tensor.unsqueeze(0)) # add batch dimension\n",
    "    # Convert tensor to pillow\n",
    "    img_128 = tensor_128.squeeze()\n",
    "    img_128_pil = TF.to_pil_image(img_128.clamp(min=0, max=1))\n",
    "    ax.imshow(img_128_pil)\n",
    "    ax.set_title(mode)\n",
    "    \n",
    "# Load and show image in Pillow format\n",
    "pic = Image.open(\"cat.jpg\") \n",
    "pil_64 = pic.resize((64, 64))\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(15, 5))\n",
    "ax[0].imshow(pil_64)\n",
    "ax[0].set_title(\"Raw\")\n",
    "\n",
    "# Upsample with Pytorch\n",
    "upsample(pil_64, mode=\"nearest\", ax=ax[1])\n",
    "upsample(pil_64, mode=\"bilinear\", ax=ax[2])\n",
    "upsample(pil_64, mode=\"bicubic\", ax=ax[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в данном случае каждое из пространственных измерений изображения увеличилось в 2 раза, но при необходимости возможно использовать увеличение в иное в том числе не целое количество раз, используя параметр `scale_factor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слои Upsample обычно комбинируют вместе со сверточными, это рекомендованный способ увеличения пространственных размеров карт признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Upsample(scale_factor=2), \n",
    "  nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "  nn.ReLU()\n",
    ")\n",
    "\n",
    "dummy_input = torch.randn((0, 3, 32, 32))\n",
    "out = model(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Другие способы \"разжать\" карту признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MaxUnpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо свёртки, на этапе снижения размерности также используются слои pooling'а. Наиболее популярным вариантом является maxpooling, сохраняющий значение только наибольшего элемента внутри сегмента. Для того чтобы обратить данную операцию субдискретизации, был предложен MaxUnpooling слой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/maxunpooling.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный слой требует сохранения индексов максимальных элементов внутри сегментов - при обратной операции, максимальное значение помещается на место, в котором был максимальный элемент сегмента до соответствующей субдискретизации. Соответственно, каждому слою MaxUnpooling должен соответствовать слой MaxPooling, что визуально можно представить следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/downsample_and_upsample_layers.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация к MaxPool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "[Документация к MaxUnpool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "def tensor_show(tensor, title=\"\", ax=ax):\n",
    "    img = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
    "    ax.set_title(title + str(img.size))\n",
    "    ax.imshow(img)\n",
    "\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=2, return_indices=True)  # False by default(get indexes to upsample)\n",
    "unpool = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "pil = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].set_title(\"original \" + str(pil.size))\n",
    "ax[0].imshow(pil)\n",
    "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
    "print(\"Orginal shape\", tensor.shape)\n",
    "\n",
    "# Downsample\n",
    "tensor_half_res, indexes1 = pool(tensor)\n",
    "tensor_show(tensor_half_res, \"1/2 down \", ax=ax[1])\n",
    "\n",
    "tensor_q_res, indexes2 = pool(tensor_half_res)\n",
    "tensor_show(tensor_q_res, \"1/4 down \", ax=ax[2])\n",
    "print(\"Downsample shape\", indexes2.shape)\n",
    "\n",
    "# Upsample\n",
    "tensor_half_res1 = unpool(tensor_q_res, indexes2)\n",
    "tensor_show(tensor_half_res1, \"1/2 up \", ax=ax[3])\n",
    "\n",
    "\n",
    "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
    "tensor_show(tensor_recovered, \"full size up \", ax=ax[4])\n",
    "print(\"Upsample shape\", tensor_recovered.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transpose convolution\n",
    "\n",
    "Способы восстановления пространственных размерностей, которые мы рассмотрели, не содержали обучаемых параметров. \n",
    "\n",
    "\n",
    "Для повышения пространственного разрешения карты признаков можно использовать операцию Transpose convolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/simple_convolution.png\" width=\"700\"></center>\n",
    "<center><em>Обычная свертка.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличие этого метода от свёртки заключается в том, что вместо перемножения каждого входного значения на соответствующий элемент фильтра, все элементы фильтра домножаются на единственное входное значение. \n",
    "\n",
    "В результате получается матрица, размеры которой совпадают с размерами фильтра.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center><img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/transposed_convolutions.png\" width=\"1200\"></center>\n",
    "\n",
    "<center><em>Upsample/transpose convolution.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из них формируется выход слоя. В зонах, где полученные значения \"накладываются\" друг на друга, они суммируются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про 2d свертки с помощью перемножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)\n",
    "\n",
    "[Документация к ConvTranspose2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, \n",
    "                         stride=1, padding=0, ...)\n",
    "```\n",
    "где:\n",
    "* `in_channels`, `out_channels` - количество каналов в входной и выходной карте признаков,\n",
    "* `kernel_size` - размер ядра свертки Transpose convolution, \n",
    "* `stride` - шаг свертки Transpose convolution,\n",
    "* `padding` - размер отступов, устанавливаемых по краям входной карты признаков.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 16, 16, 16) # define dummy input\n",
    "print('Original size', input.shape)\n",
    "\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) # define downsample layer\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) # define upsample layer\n",
    "\n",
    "# let`s downsample and upsample input\n",
    "with torch.no_grad(): \n",
    "  output_1 = downsample(input)\n",
    "  print(\"Downsampled size\", output_1.size())\n",
    "\n",
    "  output_2 = upsample(output_1, output_size = input.size())\n",
    "  print(\"Upsampled size\", output_2.size())\n",
    "\n",
    "# plot results\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "sns.heatmap(input[0, 0, :, :], ax=ax[0], cbar=False, vmin=-2, vmax=2)\n",
    "ax[0].set_title(\"Input\")\n",
    "sns.heatmap(output_1[0, 0, :, :], ax=ax[1], cbar=False, vmin=-2, vmax=2)\n",
    "ax[1].set_title(\"Downsampled\")\n",
    "sns.heatmap(output_2[0, 0, :, :], ax=ax[2], cbar=False, vmin=-2, vmax=2)\n",
    "ax[2].set_title(\"Upsampled\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пирамида признаков\n",
    "\n",
    "При помощи Upsample мы можем сконструировать модель с нужной нам архитектурой:\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/efficient_way_predict_pixel_class.png\" width=\"800\">\n",
    "\n",
    "И она будет как-то работать, но возникает вопрос: не потеряется ли информация о мелких деталях изображения при передаче через центральный блок сети, где пространственное разрешение минимально. Такая проблема существует. \n",
    "\n",
    "\n",
    "Те, кто изучал классические методы машинного зрения, помнят, что  при извлечении дескрипторов особых точек([SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform)) использовалась так называемая пирамида изображений.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/pyramid_of_images.png\" width=\"650\">\n",
    "\n",
    "Идея состоит в последовательном уменьшении (масштабировании) изображения и последовательного извлечения признаков в разных разрешениях.\n",
    "\n",
    "\n",
    "Эту идею можно использовать и при работе с картами признаков. При уменьшении пространственных размеров мы естественным образом получаем карты признаков с разным пространственным разрешением.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/pyramid_of_features.png\" width=\"650\">\n",
    "\n",
    "Эти признаки можно сохранить и передать в разжимающие слои, где карты признаков будут иметь соответствующее пространственное разрешение:\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/add_skip_connection.png\" width=\"650\">\n",
    "\n",
    "\n",
    "Так же как и ResNet этот механизм носит название skip connection, но  признаки  не суммируются, а конкатенируются.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "\n",
    "Рассмотренная нами схема используется в U-Net. Эта популярная модель для сегментации медицинских изображений изначально была предложена в статье [U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597) для анализа  медицинских изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/unet_scheme.png\" width=\"700\"></center>\n",
    "<center><em>Архитектура U-Net (Ronneberger et al., 2015).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Реализация на PyTorch](https://github.com/milesial/Pytorch-UNet)\n",
    "\n",
    "[U-Net на PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)\n",
    "\n",
    "[Блог-пост разбор](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит обратить особое внимание на серые стрелки на схеме: они соответствуют операции конкатенации копий ранее полученных карт активаций, по аналогии с DenseNet. Чтобы это было возможно, необходимо поддерживать соответствие между размерами карт активаций в процессах снижения и повышения пространственных размерностей. Для этой цели, изменения размеров происходят только при операциях `MaxPool` и `MaxUnpool` - в обоих случаях, в два раза. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде прямой проход может быть реализован, например, вот так:\n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "    out1 = self.block1(x) #  ------------------------------>\n",
    "    out_pool1 = self.pool1(out1)\n",
    "\n",
    "    out2 = self.block2(out_pool1)\n",
    "    out_pool2 = self.pool2(out2)\n",
    "\n",
    "    out3 = self.block3(out_pool2)\n",
    "    out_pool3 = self.pool2(out3)\n",
    "\n",
    "    out4 = self.block4(out_pool3)\n",
    "    # return up\n",
    "    out_up1 = self.up1(out4)\n",
    "\n",
    "    out_cat1 = torch.cat((out_up1, out3), dim=1)\n",
    "    out5 = self.block5(out_cat1)\n",
    "    out_up2 = self.up2(out5)\n",
    "\n",
    "    out_cat2 = torch.cat((out_up2, out2), dim=1)\n",
    "    out6 = self.block6(out_cat2)\n",
    "    out_up3 = self.up3(out6)\n",
    "\n",
    "    out_cat3 = torch.cat((out_up3, out1), dim=1) # <-------\n",
    "    out = self.block7(out_cat3)\n",
    "\n",
    "    return out\n",
    "\n",
    "```\n",
    "После upsample блоков ReLU не используется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IoU - оценка точности\n",
    "\n",
    "Как оценить качество предсказаний полученных от модели?\n",
    "\n",
    "Базовой метрикой является Intersection over Union (IoU) она же коэффициент Жаккара([Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется предсказание модели (фиолетовая маска) и целевая разметка  сделанная человеком (красная маска)*.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/iou_sample.png\" width=\"400\">\n",
    "\n",
    "Необходимо оценить качество предсказания.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Для простоты в примере маски прямоугольные, но та же логика будет работать  для масок произвольной формы.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Метрика считается как отношение площади пересечения, к площади объединения двух масок: \n",
    "\n",
    "$$ IoU = \\frac{|T \\cap P|}{|T \\cup P|} $$\n",
    "\n",
    "T - True mask, P- predicted mask\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/iou_formula.png\" width=\"400\">\n",
    "\n",
    "Если маски совпадут на 100%, то значение метрики будет равно 1 и это наилучший результат. При пустом пересечении IoU будет нулевым. Значения метрики лежат в интервале от [0..1].\n",
    "\n",
    "В терминах ошибок первого/второго рода IoU можно записать как:\n",
    "\n",
    "$$ IoU = \\frac{TP}{TP + FP + FN} $$\n",
    "\n",
    "\n",
    "TP - True positive == Пересечение (обозначено желтым)\n",
    "\n",
    "FP - False Positive (остаток фиолетового прямоугольника)\n",
    "\n",
    "FN - False Negative (остаток красного прямоугольника)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На базе этой метрики строится ряд производных от нее метрик, таких как Mean Average Precision, которую мы рассмотрим в разделе Детектирование.\n",
    "\n",
    "Дополнительная информация: [Intersection over Union](http://datahacker.rs/deep-learning-intersection-over-union/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss функции для сегментации\n",
    "\n",
    "Так как мы свели задачу сегментации к задаче классификации, то, казалось бы, надо использовать CrossEntropyLoss, с которой мы хорошо знакомы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy (BCE)\n",
    "Но если предсказывается маска для объектов единственного класса, то задача сводится к бинарной классификации. Так как каждый канал на выходе последнего слоя выдает предсказание для единственного класса.\n",
    "\n",
    "Это позволяет заменить softmax в CrossEntropyLoss на сигмоиду, а лосс функцию на бинарную кросс-энтропию (BCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "\n",
    "\n",
    "one_class_out = torch.randn(1, 1, 32, 32)\n",
    "one_class_target = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "loss = bce_loss(one_class_out, one_class_target)\n",
    "print(\"BCE\", loss)\n",
    "\n",
    "\n",
    "two_class_out = torch.randn(1, 2, 32, 32)\n",
    "two_class_target = torch.randint(1, (1, 32, 32))\n",
    "\n",
    "print(\"Model out for two class\", two_class_out.shape)\n",
    "print(\"Target for two class\", two_class_target.shape)\n",
    "\n",
    "loss = cross_entropy(two_class_out, two_class_target)\n",
    "\n",
    "#bce_loss(two_class_out, two_class_target) throwing exception\n",
    "\n",
    "\n",
    "print(\"Cross entropy loss\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Другие loss - функции для сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-wise cross entropy loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/pixel_wise_cross_entropy_loss.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DiceLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/dice_loss.jpeg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про семантическую сегментацию](https://www.jeremyjordan.me/semantic-segmentation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Soft Dice loss of binary class\n",
    "    Args:\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "       Returns:\n",
    "        Loss tensor\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=2, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = p  # pow degree\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        predict = predict.flatten(1)\n",
    "        target = target.flatten(1)\n",
    "\n",
    "        # https://pytorch.org/docs/stable/generated/torch.mul.html\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.epsilon\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.epsilon\n",
    "        loss = 1 - 2 * num / den\n",
    "\n",
    "        return loss.mean()  # over batch\n",
    "\n",
    "\n",
    "criterion = BinaryDiceLoss()\n",
    "output = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "\n",
    "target = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "soft_loss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n",
    "print(\"Loss\", soft_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Networks\n",
    "\n",
    "Сокращенно FCN для того, чтобы не было путаницы с Fully Connected Network, последние именуют MLP (Multi Layer Perceptron)\n",
    "\n",
    "Примеры реализации: [1](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/) и [2](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
    "\n",
    "\n",
    "Предобученная модель была обучена на части датасета COCO train2017 (на 20 категориях, представленных так же в датасете  Pascal VOC). Использовались следующие классы:\n",
    "\n",
    "`['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://arxiv.org/abs/1605.06211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FCN — Fully Convolutional Network (Semantic Segmentation)](https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура похожа на Unet за тем исключением, что сеть не обязана быть симметричной.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/fcn_backbone.png\" width=\"500\">\n",
    "\n",
    "Такую сеть можно построить, взяв за основу другую сверточную архитектуру (*backbone*) , например `ResNet50` или `VGG16`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/fcn_changes.png\" width=\"500\">\n",
    "\n",
    "И затем заменить полносвязанные  на свертки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/fully_convolution_network_scheme.png\" width=\"500\">\n",
    "\n",
    "\n",
    "В конце добавить `upsample` слой до нужных нам размеров. \n",
    "\n",
    "\n",
    "На вход такая модель может получать изображение произвольного размера. \n",
    "Так как для задач сегментации изменение размеров входного изображения приводит к потере важной информации о границах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/fcn_1.png\" width=\"1000\">\n",
    "\n",
    "сохранение информации с верхних слоев достигается не за счет конкатенации карт признаков как в UNet, а за счет суммы.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# fix seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "# load resnet50\n",
    "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
    "    weights='FCN_ResNet50_Weights.DEFAULT', num_classes=21\n",
    ")\n",
    "\n",
    "classes = [\"__background__\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\",\n",
    "           ]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # ImageNet\n",
    "    ]\n",
    ")\n",
    "\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "input_tensor = transform(pil_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = fcn_model(input_tensor.unsqueeze(0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращаются 2 массива\n",
    "\n",
    "* out - каждый пиксель отражает ненормированную вероятность, соответствующую предсказанию каждого класса.\n",
    "\n",
    "* aux - содержит значения *auxillary loss* на пиксель. На инференсе `output['aux']` бесполезный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"output keys: \", output.keys())  # Ordered dictionary\n",
    "print(\"out: \", output[\"out\"].shape, \"Batch, class_num, h, w\")\n",
    "print(\"aux: \", output[\"aux\"].shape, \"Batch, class_num, h, w\")\n",
    "\n",
    "output_predictions = output[\"out\"][0].argmax(0)  # for first element of batch\n",
    "print(f'output_predictions: {output_predictions.shape}')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "indexes = output_predictions\n",
    "\n",
    "# plot all classes predictions\n",
    "fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
    "i = 0 # counter\n",
    "for row in range(4):\n",
    "    for col in range(5):\n",
    "        mask = torch.zeros(indexes.shape)\n",
    "        mask[indexes == i] = 255\n",
    "        ax[row, col].set_title(classes[i])\n",
    "        ax[row, col].imshow(mask)\n",
    "        ax[row, col].axis(\"off\")\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обзор DeepLabv3+(2018)\n",
    "\n",
    "DeepLab - семейство моделей для сегментации, значительно развивавшееся в течение четырёх лет. Основой данного рода моделей является использование **atros (dilated) convolutions** и, начиная со второй модели, **atros spatial pyramid pooling**, опирающейся на **spatial pyramid pooling**.\n",
    "\n",
    "[Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
    "\n",
    "[Реализация на PyTorch](https://pytorch.org/vision/stable/models.html#deeplabv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/deeplabv3_scheme.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial pyramid pooling (SPP) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (He et al., 2014)](https://arxiv.org/abs/1406.4729)\n",
    "\n",
    "**Spatial Pyramid Pooling (SPP)** - это *pooling* слой, который устраняет ограничение фиксированного размера сети, т.е. CNN не требует входного изображения фиксированного размера. В частности, мы добавляем слой SPP поверх последнего сверточного слоя. \n",
    "\n",
    "Слой SPP объединяет признаки и генерирует выходные данные фиксированной длины, которые затем поступают в MLP (или другие классификаторы). Другими словами, мы выполняем некоторую агрегацию информации на более глубоком этапе иерархии сети (между сверточными слоями и полностью связанными слоями), чтобы избежать необходимости обрезать или деформировать изображение в начале."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/spatial_pyramid_pooling_layer.png\" width=\"700\"></center>\n",
    "<center><em>Схема SPP (He et al., 2014).</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В pytorch нет отдельного модуля, но результат может быть получен за счет применения нескольких AdaptiveMaxPool2d с разными размерами выходов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atros (Dilated) Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/dilated_convolution.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dilated convolution** (расширенные свертки) - это тип свертки, который \"раздувает\" ядро, как-бы вставляя отверстия между элементами ядра. Дополнительный параметр (скорость расширения, **dilation**) указывает, насколько сильно расширяется ядро.\n",
    "\n",
    "\n",
    "Фактически в такой свертке входные пикселы (признаки) участвуют через один (два, три ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "                padding=0, dilation=1, ...)\n",
    "```\n",
    "где:\n",
    "* `in_channels`, `out_channels` - количество каналов в входной и выходной карте признаков,\n",
    "* `kernel_size` - размер ядра свертки,\n",
    "* `stride` - шаг свертки,\n",
    "* `padding` - размер отступов, устанавливаемых по краям входной карты признаков,\n",
    "* `dilation` - скорость расширения свертки.\n",
    "\n",
    "[[doc] nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plot\n",
    "def plot_conv2d(input, conv, output, dilation=1):\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "    # input  \n",
    "    sns.heatmap(input[0][0], ax=ax[0], annot=True, \n",
    "                fmt=\".0f\", cbar=False, vmin=0, vmax=20, linewidths=1)\n",
    "    # kernel\n",
    "    sns.heatmap(conv.weight.detach()[0, 0, :, :], ax=ax[1], annot=True, \n",
    "                fmt=\".0f\", cbar=False, vmin=0, vmax=20, linewidths=1)\n",
    "    # output\n",
    "    sns.heatmap(output[0, 0, :, :], ax=ax[2], annot=True, \n",
    "                fmt=\".0f\", cbar=False, vmin=0, vmax=20, linewidths=1)\n",
    "    # titles\n",
    "    ax[0].set_title(\"Input \\nshape: \" + str(input.shape))\n",
    "    ax[1].set_title(\"Kernel \\nshape: \" + str(conv.weight.shape))\n",
    "    ax[2].set_title(\"Output \\nshape: \" + str(output.shape))\n",
    "    fig.suptitle(\"Dilation = \" + str(dilation), y=1.05)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atros example\n",
    "with torch.no_grad():\n",
    "    # define dummy input\n",
    "    input = torch.tensor([[[[1, 2, 3],\n",
    "                            [1, 2, 3],\n",
    "                            [1, 2, 3]]]], dtype=torch.float)\n",
    "    \n",
    "    # define conv layer, dilation = 1\n",
    "    conv = nn.Conv2d(1, 1, kernel_size=2, dilation=1, bias=False)\n",
    "    # define kernel weights\n",
    "    conv.weight = nn.Parameter(torch.tensor([[[[1, 2],\n",
    "                                               [2, 1]]]], dtype=torch.float))\n",
    "    output = conv(input)\n",
    "    plot_conv2d(input, conv, output, dilation=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dilation to 2\n",
    "with torch.no_grad():\n",
    "    conv = nn.Conv2d(1, 1, kernel_size=2, dilation=2, bias=False)  # Fell free to change dilation\n",
    "    conv.weight = nn.Parameter(torch.tensor([[[[1, 2],\n",
    "                                               [2, 1]]]], dtype=torch.float))\n",
    "    output = conv(input)\n",
    "    plot_conv2d(input, conv, output, dilation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change input tensor\n",
    "with torch.no_grad():\n",
    "    input = torch.tensor([[[[0, 1, 0],\n",
    "                            [1, 1, 1],\n",
    "                            [0, 1, 0]]]], dtype=torch.float)\n",
    "    output = conv(input)\n",
    "    plot_conv2d(input, conv, output, dilation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детектирование (Object detection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/detection\n",
    ".png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектирование - задача компьютерного зрения, в которой требуется\n",
    "определить местоположение конкретных объектов на изображении.\n",
    "\n",
    "При этом вычислять точные границы объектов не требуется, а достаточно определить только ограничивающие прямоугольники (bounding boxes), в которых находятся объекты.\n",
    "\n",
    "В общем случае объекты могут принадлежать к различным классам, и объектов одного класса на изображении может быть несколько. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектирование единственного объекта\n",
    "\n",
    "Но начнём с простой ситуации.\n",
    "\n",
    "Пусть нас интересуют объекты только одного класса и мы знаем, что такой объект на изображении есть и он один. \n",
    "\n",
    "К примеру, мы разрабатываем систему по распознаванию документов:\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/predict_bounded_box_example.png\" width=\"700\">\n",
    "\n",
    "На вход модели подаётся изображение, и предсказать требуется область, в которой объект локализован.\n",
    "Область (bounding box) определяется набором координат вершин*. Собственно эти координаты и должна предсказать модель.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\* *Если наложить условие, что стороны многоугольника должны быть параллельны сторонам изображения, то можно ограничиться предсказанием 2-х координат.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказание координат\n",
    "\n",
    "Если задачу семантической сегментации получилось свести к классификации, то здесь будем использовать регрессию, поскольку предсказывать нужно не номер класса, а набор чисел.\n",
    "\n",
    "В зависимости от требований эти числа могут нести разный смысл, например:\n",
    "\n",
    "* координаты центра + ширина и высоту\n",
    "* координаты правого верхнего и левого нижнего углов\n",
    "* координаты вершин многоугольника ...\n",
    "\n",
    "Но в любом случае задача остается регрессионной.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решается она так:\n",
    "\n",
    "Берем сверточную сеть, и меняем последний полносвязанный слой таким образом, чтобы количество  выходов совпадало с количеством координат, которые нам нужно предсказать.\n",
    "\n",
    "Так для предсказания двух точек потребуется четыре выхода ( x1 , y1 , x2, y2 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "\n",
    "# load pretrained model\n",
    "resnet_detector = resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "\n",
    "# Change \"head\" to predict coordinates (x1,y1 x2,y2)\n",
    "resnet_detector.fc = nn.Linear(resnet_detector.fc.in_features, 4)  # x1,y1 x2,y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения такой модели придется заменить функцию потерь на регрессионную, например MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# This is a random example. Don't expect good results\n",
    "input = torch.rand((1, 3, 224, 224))\n",
    "target = torch.tensor([[0.1, 0.1, 0.5, 0.5]])  # x1,y1 x2,y2 or x,y w,h\n",
    "print(f'Target: {target}')\n",
    "output = resnet_detector(input)\n",
    "loss = criterion(output, target)\n",
    "print(f'Output: {output}')\n",
    "print(f'Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Координаты обычно предсказываются в процентах от длины и ширины изображения.\n",
    "Таким образом, если bounding box целиком помещается на изображении, обе координаты будут находиться в интервале от $[0 .. 1]$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/predict_key_points.png\" width=\"700\"></center>\n",
    "<center><em>Примеры предсказывания точек (Humphreys et al., 2020)</em></center>\n",
    "\n",
    "[Recent Progress in Appearance-based Action Recognition (Humphreys et al., 2020)](https://arxiv.org/abs/2011.12619)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По такому принципу работают многие модели для поиска различных ключевых точек.\n",
    "Например: на лице(facial landmarks) или теле человека."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask loss\n",
    "\n",
    "Координаты прямоугольников мы предсказывать научились.\n",
    "\n",
    "Теперь усложним задачу: объект остается один, но может принадлежать к различным классам. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/one_object.png\" width=\"650\">\n",
    "\n",
    "То есть к задаче локализации добавляется классификация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачу классификации мы умеем решать:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/class_prediction.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остается объединить классификацию с регрессией: \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/multitask_loss_0.png\" width=\"650\">\n",
    "\n",
    "Для этого нужно одновременно предсказывать:\n",
    "\n",
    "\n",
    "*   вероятность принадлежности к классам\n",
    "*   координаты ограничивающего прямогульника (bounding box)\n",
    "\n",
    "Тогда выход последнего слоя будет иметь размер: \n",
    "$$N + 4$$\n",
    "\n",
    "\n",
    "где N - количество классов (1000 для ImageNet), \n",
    "\n",
    "а 4 числа это координаты одного boundig box (x1,y1,x2,y2 или x,y,w,h)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Как описать функцию потерь для такой модели?**\n",
    "\n",
    "Можно суммировать loss для классификации и loss для регрессии.\n",
    "\n",
    "$ L_{total} = L_{crossentropy}+L_{mse}$\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/multitask_loss.png\" width=\"650\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако значения разных loss могут иметь разные порядки, поэтому приходится подбирать весовые коэффициенты для каждого слагаемого. \n",
    "В общем случае функция потерь будет иметь вид:\n",
    "\n",
    "$$L_{total} = \\sum _iw_iL_i$$\n",
    "где $w_i$ - весовые коэффициент каждой из лосс функций. \n",
    "\n",
    "Они являются гиперпараметрами модели и требуют подбора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор весов для каждой компоненты loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно подбирать веса компонентов loss в процессе обучения. Для этого к модели добавляется дополнительный слой:\n",
    "\n",
    "[Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics(Alex Kendall et al., 2018)](https://arxiv.org/abs/1705.07115)\n",
    "\n",
    "\n",
    "[Пример реализации MultiTask learning](https://github.com/Hui-Li/multi-task-learning-example-PyTorch/blob/master/multi-task-learning-example-PyTorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции потерь для задач регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MAE vs MSE**\n",
    "\n",
    " <img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/regression_loss.png\" width=\"700\">\n",
    "\n",
    " $\\displaystyle\\mathrm{MSE} = \\frac{\\sum^n_{i=1}\\left(y_i-y_i^p\\right)^2}{n}$ - L2/ MSE/ Mean Squared Error/ Среднеквадратичная ошибка \n",
    "\n",
    "\n",
    "$\\displaystyle\\mathrm{MAE} = \\frac{\\sum^n_{i=1}\\left|y_i-y_i^p\\right|}{n}$ - L1/ MAE/ Mean Absolute Error/ Средняя ошибка\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber vs Log-cos\n",
    " <img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/mae_vs_mse.png\" width=\"700\">\n",
    "\n",
    " $\\displaystyle\\\n",
    "    L_{\\delta}(y, f(x))=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  \\frac{1}{2}\\left(y-f\\left(x\\right)\\right)^2 \\qquad &\\mathrm{for}\\  |y-f(x)| \\leq \\delta\\\\\n",
    "                  \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 \\qquad &\\mathrm{otherwise}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "  $ - Huber Loss/ Smooth Mean ABsolute Error/ Функция потерь Хьюбера\n",
    "\n",
    "\n",
    "$\\displaystyle L\\left(y,y^p\\right)=\\sum_{i=1}^n log\\left(cosh\\left(y^p_i-y_i\\right)\\right)$ - Log-Cosh Loss/ Логарифм гиперболического косинуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Regression Loss Functions All Machine Learners Should Know](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектирование нескольких объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как быть если объектов несколько?\n",
    "\n",
    "\n",
    "Для каждого объекта нужно вернуть координаты(x1,y1,x2,y2) и класс (0 .. N). \n",
    "Соответственно количество выходов модели надо увеличивать...\n",
    "\n",
    "Но нам неизвестно заранее, сколько объектов будет на изображении:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/object_detection_multiple_object.gif\" width=\"700\">\n",
    "\n",
    "[Stanford University CS231n: Detection and Segmentation](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наивный способ решения: скользящее окно\n",
    "\n",
    "Одним из вариантов решения этой проблемы является применение классификатора ко всем возможным местоположениям объектов. Классификатор предсказывает, есть ли на выбранном фрагменте изображения один из интересующих нас объектов. Если нет - то фрагмент классифицируется как \"фон\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/naive_way_object_detection_multiple_object.gif\" width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемой данного подхода является необходимость применять классификатор к огромному количеству различных фрагментов, что крайне дорого с точки зрения вычислений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эвристика для поиска ROI\n",
    "\n",
    "Вместо того чтобы применять классификатор \"наобум\", можно для начала выбрать те области изображения, в которых вероятность нахождения объекта наиболее высока и запускать классификатор лишь для них.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/heuristics_way_object_detection_multiple_object.png\" width=\"700\">\n",
    "\n",
    "Такие области называются **regions of interest**, сокращённо - **ROI**. \n",
    "\n",
    "Для поиска таких областей можно использовать какой-либо эвристический алгоритм, например [Selective search](https://ivi.fnwi.uva.nl/isis/publications/bibtexbrowser.php?key=UijlingsIJCV2013&bib=all.bib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective search\n",
    "\n",
    "\n",
    "\n",
    "Selective search - известный алгоритмический метод поиска **ROI**.\n",
    "\n",
    "Идея алгоритма состоит в разбиении изображения на небольшие области и последующего их итеративного объединения.\n",
    "\n",
    "\n",
    "Объединение происходит на основании сходства, которое вычисляется как сумма 4-х метрик (см. иллюстрацию)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/selective_search.png\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "Возвращает порядка 2000 прямоугольников для изображения, отрабатывает за несколько секунд на CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN (Region CNN)\n",
    "Первая известная модель, построенная по описанному принципу:\n",
    "\n",
    "- на изображении ищутся ROI \n",
    "- для каждого делается resize \n",
    "- каждый ROI обрабатывается сверточной сетью, которая предсказывает класс объекта, который в него попал\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/region_of_interest_cnn.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме класса модель предсказывает смещения для каждого bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/r_cnn_predict_bounded_box_shift.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS\n",
    "\n",
    "Теперь возникает другая проблема: в районе объекта алгоритм генерирует множество ограничивающих прямоугольников (bounding box), которые частично перекрывают друг друга.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/non_max_suppression.png\" width=\"650\">\n",
    "\n",
    "Чтобы избавиться от них используется алгоритм\n",
    "NMS (Non maxima suppression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Его задача избавиться от bbox, которые накладаваются на истинный:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/non_max_suppression_pseudo_code.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "здесь $B$ - это массив всех bounding box,  $C$ - массив предсказаний модели относительно наличия объекта в соответствующем bounding box\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки схожести обычно используется метрика IoU(same == IoU), а значение IoU ($\\lambda_{nms}$), при котором bbox считаются принадлежащими одному объекту, является гиперпараметром (часто 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В Pytorch алгоритм NMS доступен в модуле torchvision.ops\n",
    "\n",
    "`torchvision.ops.nms(boxes, scores, iou_threshold)`\n",
    "где:\n",
    "* `boxes` - массив bounding box,\n",
    "* `scores` - предсказанная оценка,\n",
    "* `iou_threshold` - порог IoU, NMS отбрасывает все перекрывающиеся поля с $IoU> iou\\_threshold$\n",
    "\n",
    "[[doc] torchvision.ops.nms](https://pytorch.org/vision/stable/generated/torchvision.ops.nms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft NMS\n",
    "\n",
    "Как видно из примера выше, NMS может удалять и истинные bounding box. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft NMS не удаляет сомнительные bounding box а корректирует (понижает) для них score - вероятность (уверенность) в том, что в прямоугольнике действительно содержится объект.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/soft_non_max_suppression_pseudo_code.png\" width=\"1000\">\n",
    "\n",
    "\n",
    "Результат:\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/soft_nms.png\" width=\"500\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast R-CNN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемой R-CNN являлась скорость.\n",
    "Так как мы вынуждены применять CNN порядка 2000 раз (в зависимости от эвристики, которая генерирует ROI)\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/r_cnn_scheme.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И решением, которое предложили авторы Fast R-CNN является поиск ROI не на самом изображении, а на карте признаков. В таком случае большая часть сверток выполняется только один раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/fast_r_cnn_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это радикально ускоряет процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Pooling\n",
    "\n",
    "Теперь появляется новая задача - получить карты признаков одинакового размера для всех ROI.\n",
    "\n",
    "Для этого границы ROI проецируются на карту признаков.\n",
    "\n",
    "Затем к полученным фрагментам карты признаков применяется операция max pooling и выходы получаются фиксированного размера. Теперь их можно подать на вход полносвязанного слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/roi_pooling.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROI pooling в Pytorch\n",
    "\n",
    "\n",
    "```\n",
    "torchvision.ops.roi_pool(input, boxes, output_size,...)\n",
    "```\n",
    "где:\n",
    "* `input` -  тензор с входными картами признаков,\n",
    "* `boxes` -  массив bounding box,\n",
    "* `output_size` - размер вывода после ROI pooling.\n",
    "\n",
    "\n",
    "[Документация Roi Pooling](https://pytorch.org/vision/stable/generated/torchvision.ops.roi_pool.html)\n",
    "\n",
    "Статья: [Region of Interest Pooling](https://towardsdatascience.com/region-of-interest-pooling-f7c637f409af)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Align\n",
    "\n",
    "Операция ROI pooling применялась в оригинальной модели Fast-RCNN. В дальнейшем она была заменена на Roi Align. Здесь признаки не отбрасываются, как это происходит при  max pooling, а их значения интерполируются на новую карту признаков.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/roi_align.png\" width=\"750\">\n",
    "\n",
    "\n",
    "Чтобы избежать квантования границ, RoIAlign использует билинейную интерполяцию для вычисления  значений входных признаков.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torchvision.ops.roi_align(input, boxes, output_size, ...)\n",
    "```\n",
    "где:\n",
    "* `input` -  тензор с входными картами признаков,\n",
    "* `boxes` -  массив bounding box,\n",
    "* `output_size` - размер вывода после Roi Align.\n",
    "\n",
    "[[doc] torchvision.ops.roi_align](https://pytorch.org/vision/stable/generated/torchvision.ops.roi_align.html)\n",
    "\n",
    "[Understanding Region of Interest](https://towardsdatascience.com/understanding-region-of-interest-part-2-roi-align-and-roi-warp-f795196fc193)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN\n",
    "\n",
    "**Идея: пусть сеть сама предсказывает ROI по карте признаков**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "После того как в Fast-RCNN большая часть сверток стала запускаться единожды,\n",
    "скорость работы нейросетевой части существенно возросла.\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/compare_training_time_r_cnn_and_fast_r_cnn.png\" width=\"900\">\n",
    "\n",
    "Теперь \"узким местом\" стала эвристика для поиска ROI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому в следующей версии детектора (Faster R-CNN) от эвристики решено было избавиться, а  ROI искать при помощи дополнительной нейросети Region Proposal Network (RPN).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/faster_r_cnn_scheme.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения такой модели требуется посчитать четыре loss.\n",
    "\n",
    "\n",
    "1. RPN классифицирует объект/не объект (классификация)\n",
    "2. Координаты ROI предсказанные RPN (регрессия)\n",
    "3. Класс объекта для каждого bounding box (классификация)\n",
    "4. Координаты bounding boxes (регрессия)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/faster_r_cnn_train_time.png\" width=\"700\">\n",
    "\n",
    "В результате скорость увеличивается ещё почти в 10 раз, но для задач реального времени все равно остаётся неприемлемо низкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Faster R-CNN на PyTorch](https://pytorch.org/vision/stable/models.html#faster-r-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "# load model\n",
    "fr_rcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT',\n",
    "    progress=True,\n",
    "    num_classes=91,\n",
    "    weights_backbone='ResNet50_Weights.DEFAULT'\n",
    ")\n",
    "fr_rcnn_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# load data\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"]) # get category IDs\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(imgIds[12])  # http://images.cocodataset.org/val2017/000000370208.jpg\n",
    "img = img_list[0]\n",
    "\n",
    "# plot image\n",
    "plt.figure(figsize=(10, 10))\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "# plot boundy boxes\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('Image data: ')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# lets predict objects by resnet50\n",
    "with torch.no_grad():\n",
    "    tensor = TF.pil_to_tensor(pil_img) / 255 # Normalize\n",
    "    output = fr_rcnn_model(tensor.unsqueeze(0))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    \n",
    "    # plot rectangles\n",
    "    for i, bbox in enumerate(output[0][\"boxes\"]):\n",
    "        if output[0][\"scores\"][i] > 0.5: \n",
    "            draw.rectangle((tuple(bbox[:2].numpy()), tuple(bbox[2:].numpy())), width=2)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region proposal network (RPN)\n",
    "\n",
    "Как устроена сеть предсказывающая ROI?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/roi_pooling.png\" width=\"800\">\n",
    "\n",
    "Карта признаков имеет фиксированные и относительно небольшие пространственные размеры (например 20x15)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Поэтому можно вернуться к идее скользящего окна, которая была отвергнута в самом начале, из-за большого размера изображения.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/rpn_base.png\" width=\"700\">\n",
    "\n",
    "Для карты признаков размером 20x15 количество ROI получится равным 3000, что сравнимо с количеством предсказаний производимых SelectiveSearch.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не всегда объект хорошо вписывается в квадрат:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/rpn_aspect_ratio.png\" width=\"900\">\n",
    "\n",
    "Поэтому для каждой точки на карте признаков (anchor) можно использовать окна нескольких форм:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/rpn_base_anchor.png\" width=\"700\">\n",
    "\n",
    "Это позволит минимизировать корректировку* и лучше предсказывать ROI  для вытянутых объектов.\n",
    "\n",
    "\n",
    "\\* *помним, что для каждого прямоугольника предсказываются 4 числа обозначающих за сдвиг его вершин относительно начального положения.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого окна предсказываются два значения:\n",
    "\n",
    "* вероятность того, что в ROI находится объект (одно число)\n",
    "* смещения (4 числа)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама сеть при этом может быть очень простой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/simple_nn_predict_objectness_and_boundary_box.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two stage detector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если присмотреться к схеме, можно заметить, что на среднем и верхнем уровнях выполняются схожие операции.\n",
    "\n",
    "\n",
    "Разница в том, что на последнем слое предсказывается класс объекта, а на промежуточном только вероятность его присутствия (objectness). \n",
    "\n",
    "Корректировки для вершин bounding box предсказываются и в обоих случаях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/two_stage_detector.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сказать, что детектирование происходит в две стадии. \n",
    "Соответственно Faster RCNN == Two stage detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Stage detector\n",
    "\n",
    "Если сразу предсказывать класс, то можно избавиться от второй стадии.\n",
    "В этом случае к списку классов нужно добавить еще один элемент, который заменит objectness, либо будет предсказанием класса \"фон\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/one_stage_detector.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детекторы, которые работают \"за один проход\" быстрее, но потенциально менее точные.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolo_ssd_retinanet.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько моделей, построенных по этому принципу:\n",
    "YOLO, SSD, RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Сравнение скорости моделей](https://pytorch.org/vision/stable/models.html#runtime-characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSD: Single Shot MultiBox Detector\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector (Liu et al., 2015)](https://arxiv.org/abs/1512.02325)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/ssd_default_boxes.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Кандидаты в ROI (default box) выбираются на нескольких слоях (4,7, 8, 9,11)\n",
    "* Количество форм окон (default box) на картах признаков зависит от слоя: от 4 до 6 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/single_shot_multibox_detector_scheme.png\" width=\"1500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В качестве backbone используется VGG-16, предобученная на ImageNet\n",
    "* Добавлен класс для \"background\"\n",
    "\n",
    "\n",
    "В общей сложности делается 8732 предсказаний, каждое содержит 4 + (N + 1) чисел. \n",
    "\n",
    "N - это количество классов без фона.\n",
    "4 - смещения\n",
    "\n",
    "\n",
    "[Подробнее.](https://towardsdatascience.com/review-ssd-single-shot-detector-object-detection-851a94607d11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss для детектора\n",
    "\n",
    "Как подсчитать loss для детектора. Теоретически понятно, что она должна включать в себя две части: ошибку локализации и ошибку классификации.\n",
    "\n",
    "И для SSD лосс так и выглядит:\n",
    "\n",
    "$$L(x,c,l,g) = \\frac{1}{N}(L_{conf}(x,c) + \\alpha L_{loc}(x,l,g))$$\n",
    "\n",
    "\n",
    "Однако если мы будем считать ошибку локализации для всех default box, то она никогда не будет нулевой.\n",
    "Default box очень плотно перекрывает все изображение и в большинство из них объект не попадет, особенно если объект один и небольшой.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/default_boxes.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому часть default box при подсчете лосс игнорируются. Используются только те, у которых  большая площадь пересечения с одним из истинным bounding box больше порога (IoU > 0.5)\n",
    "\n",
    "\n",
    "\n",
    "$$L(x,l,g)_{loc} = \\sum_{i \\in Pos}^{N} x_{i,j}^{k}smooth_{L1}(l_i, g_j)$$\n",
    "\n",
    "\n",
    "Здесь:\n",
    "\n",
    "$l$ - финальные координаты, предсказанного bounding box, с учетом смещений\n",
    "\n",
    "$g$ - координаты истинного bounding box\n",
    "\n",
    "$M$ - количество истинных (ground true) bounding box - ов\n",
    "\n",
    "$Pos$ - список отобранных default box пересекающихся с истинными\n",
    "\n",
    "$x_{i,j}^{k} = \\{1,0\\}$ - индикатор того что комбинация default и box - валидна.\n",
    "\n",
    "\n",
    "\n",
    "> i - индекс default box\n",
    "> j - индекс истинного (ground true) bounding box\n",
    "> p - номер класса, к которому относится ground true bounding box (не степень)\n",
    "\n",
    "$smooth_{L1}$ - [Комбинация L1 и L2](https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html) \n",
    "\n",
    "Компонент, отвечающий за локализацию:\n",
    "\n",
    "$$L(x,l,g)_{conf} = -\\sum_{i \\in Pos} x_{i,j}^{k} log(softmax(c_{i}^{p})) -\\sum_{i \\in Neg} log(softmax(c_{i}^{0}))$$\n",
    "\n",
    "$c_{i}^{p}$ - вектор score для i-того default box, p - номер истинного класса, соответствующего bounding box из разметки\n",
    "\n",
    "$Pos$ - список отобранных default box, не пересекающихся с истинными (IoU < treshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\* *Формулы лоя лосс осознанно упрощены. Например, мы опустили расчет L1 для смещений, что является технической деталью.*\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий заслуживающий внимания one-stage детектор это Retina Net - [Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "Собственно авторы придумали новую лосс - функцию (Focal Loss) и опубликовали модель, чтобы продемонстрировать её эффективность.\n",
    "\n",
    "Чтобы понять какую проблему решает FocalLoss, давайте посмотрим на второй компонент Loss классификации для SSD:\n",
    "\n",
    "$$L_{conf} =  \\ ...\\  -\\sum_{i \\in Neg} log(softmax(c_{i}^{0}))$$\n",
    "\n",
    "Это кросс - энтропия для bounding box, содержащих фон. Тут нет ошибки: когда модель обучится правильно предсказывать класс фона (background) каждая из этих компонент будет небольшой.\n",
    "\n",
    "Проблема в том, что таких компонент очень много. Детектор предсказывает несколько тысяч, или десятков тысяч bounding box. Подавляющая часть из них приходится на фон. Cумма большого количества этих небольших лосс становится заметным числом и мешает учиться классифицировать реальные объекты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как решается эта проблема в Focal loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/focal_loss_vs_ce.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Фактически лосс для уверенно классифицированных объектов дополнительно занижается. Это похоже на взвешивание при дисбалансе классов.\n",
    "\n",
    "Достигается этот эффект путем домножения на коэффициент: $ (1-p_{t})^\\gamma$\n",
    "\n",
    "Здесь:\n",
    "\n",
    "$ p_{t} $ - вероятность истинного класса\n",
    "\n",
    "$ \\gamma $ - число большее 1 и являющееся гиперпараметром\n",
    "\n",
    "\n",
    "Пока модель ошибается, $p_{t}$ - мало, и выражение в скобках соответственно близко к 1-це \n",
    "\n",
    "Когда модель обучилась, $p_{t}$ становится близким к 1-це, разность в скобках становится маленьким числом, которое при возведении в степень > 1. Таким образом, домножение на это небольшое число нивелирует вклад верно классифицированных объектов.\n",
    "\n",
    "Это позволяет модели сосредоточиться на изучении сложных объектов (hard example )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост: Что такое Focal Loss и когда его использовать](https://amaarora.github.io/2020/06/29/FocalLoss.html)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature pyramid network\n",
    "\n",
    "\n",
    "Вторым полезным нововведением  в retina-net стало использование пирамиды признаков.\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection (Sergelius et al., 2016)](https://arxiv.org/abs/1612.0314)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/retinanet_use_outputs_fpn.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetinaNet использует выходы FPN и для предсказаний класса и bbox. Мы уже обсуждали пирамиды признаков применительно к сетям для сегментации, в частности FCN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/pyramid_of_features.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждом сверточном слое извлекаются карты признаков. \n",
    "\n",
    "Их пространственное разрешение постепенно уменьшается, а глубина (количество каналов) увеличивается.\n",
    "\n",
    "\n",
    "Но первые слои содержат мало семантической информации (только низкоуровневые признаки).  А карты признаков с глубоких слоев имеют низкое пространственное разрешение, что не позволяет качественно определить границы объектов.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/semantic_information.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, как и в случае с сегментацией, точность повышается, если делать предсказания на картах содержащих признаки для разных масштабов.\n",
    "\n",
    "\n",
    "При этом можно получать карты с большим пространственным разрешением не просто сохраняя их в памяти, а еще и прибавляя к ним значения признаков с более глубоких слоев, предварительно интерполировав их (Upsample)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея состоит в том, чтобы делать предсказание с учетом семантической информации, полученной на более глубоких слоях. Здесь признаки   суммируются, а не конкатенируются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/resnet_scheme.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем к новым картам признаков может применяться дополнительная свертка.\n",
    "\n",
    "На выходе получаем карты признаков P2 - P5 на которых уже предсказываются bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/resnet_prediction_head_scheme.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае 2-stage детектора (RCNN) новые карты признаков подаются на вход RPN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-web_dependencies/L12/features_from_blackbone.jpeg\" width=\"1100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А признаки для предсказаний используются из backbone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно: [Блог-пост про FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO\n",
    "\n",
    "* [You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640) \n",
    "* [YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "* [YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "* [YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "* Июнь 2020. [YOLOv5 (Glenn Jocher)](https://github.com/ultralytics/yolov5)\n",
    "* Июль 2021. [YOLOX: Exceeding YOLO Series in 2021 (Ge et al., 2021)](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "Первая версия YOLO вышла в том же году что и SSD. На тот момет, детектор несколько проигрывал SSD в точности.\n",
    "\n",
    "Однако благодаря усилиям Joseph Redmon проект поддерживался и развивался в течение нескольких лет.\n",
    "\n",
    "\n",
    " 3-я версия детектора оказалась настолько удачной, что до сих пор о ней можно прочесть:  \"YOLOv3, one of the most widely used detectors in industry\" [2021](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "В настоящий момент можно сказать, что YOLO - это  оптимальный детектор по соотношению качество распознавания/скорость.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv3\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov3.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В качестве backbone используется оригинальная сверточная сеть Darknet53, задействующая слои Batch Norm и Skip connection. Но есть реализации с ResNet в качестве backbone.\n",
    "\n",
    "Детектор использует большинство техник, которые мы обсудили:\n",
    "\n",
    "* Default boxes извлекаются на трех слоях различной глубины. Для каждой ячейки  предсказывается З окна\n",
    "* FPN: признаки конкатенируются, а не складываются\n",
    "* Resolution augmentation: При обучении разрешение входных изображений менялось (10 input resolution steps between 384x384 and 672x672)\n",
    "* В качестве лосс для классификации используется бинарная кросс-энтропия, позволяющая предсказывать несколько объектов в одном bounding box. Что позволяет использовать детектор с multilabel датасетами, где один объект может иметь несколько меток (person & woman)\n",
    "* Предсказывается дополнительный параметр objectness score. Он не связан с классификацией. Его задача предсказать насколько вероятно, что в предсказанном default boх действительно есть объект и он будет учитываться при подсчете loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov3_prediction.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, для каждого anchor box предсказывается вот такой вектор значений:\n",
    "\n",
    "* смещения\n",
    "* objectness вероятность наличия объекта \n",
    "* scores - уверенность того, что bbox содержит объект определенного класса. Для моделей тренированных на СOCO классов 80\n",
    "\n",
    "Для 80-ти классов получается 85 значений на один default box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предсказание смещений\n",
    "\n",
    "Эксперименты показывают (см. текст статьи по YOLOv3), что предсказывать абсолютные значения смещений неэффективно. \n",
    "\n",
    "Для примера можно взглянуть, как преобразуются предсказания YOLOv3 для получения финальных координат bounding box\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov3_coordinates_prediction.png\" width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "$c_{x} , c_{y}$ - это координаты центра default box\n",
    "$c_{w} , c_{h}$ - это ширина и высота default box\n",
    "$t_{x} , t_{y}$ - предсказанные смещения  для центра\n",
    "$t_{w} , t_{h}$ - предсказанные корректировки  для ширины и высоты\n",
    "\n",
    "$b_{x} , b_{y}, b_{w}, b_{h}$ - координаты центра, ширина и высота финального предсказанного bouning box. Значения в процентах от ширины и длины исходного изображения\n",
    "\n",
    "$σ(x) $ - сигмоида\n",
    "\n",
    "$e$ - число Эйлера\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOLO v3: Better, not Faster, Stronger](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b#:~:text=YOLO%20v2%20used%20a%20custom,more%20layers%20for%20object%20detection.&text=First%2C%20YOLO%20v3%20uses%20a,layer%20network%20trained%20on%20Imagenet.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv3 на момент выхода стал одним из самых быстрых детекторов и последней версией за авторством Джозефа Редмона.\n",
    "\n",
    "YOLOv4 это детище других авторов, модель не стала быстрее, но стала намного более точной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov4.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[YOLO v4](https://medium.com/visionwizard/yolov4-version-3-proposed-workflow-e4fa175b902)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что же добавили авторы:\n",
    "\n",
    "1. Поменялся Backbone:\n",
    "* Увеличилось количество слоев\n",
    "* Добавился SPP block\n",
    "* Добавились Dense в блоки ...\n",
    "\n",
    "2. Жесткая аугментация\n",
    "\n",
    "* В том числе Mosaic,  и online Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mosaic augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/mosaic_augmentation.jpg\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображение, которое подается на вход сети, склеивается из нескольких (4) фрагментов разных изображений. При этом статистика для Batch norm считается по 4-м полным изображениям.\n",
    "Такая стратегия позволяет уменьшить размер batch, что важно при работе с изображениями, имеющими большое разрешение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Path Aggregation Network Module(PAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Adversarial Training (SAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolov5.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статья не публиковалась.\n",
    "Точность сравнима  с v4, но модель определенно лучше упакована."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"])\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(imgIds[5])\n",
    "img = img_list[0]\n",
    "\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"Image data:\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка модели с Torch Hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# Load model from torch\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True, trust_repo=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из коробки работает с изображениями в разных форматах и даже url, автоматически меняет размер входного изображения, возвращает объект с результатами ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply yolov5 model\n",
    "results = model(pil_img)\n",
    "results.print() # print predicted results\n",
    "results.save()  # image on disk\n",
    "\n",
    "print(f'\\nresults.xyxy type: {type(results.xyxy)}\\\n",
    "\\nlen(results.xyxy): {len(results.xyxy)}\\\n",
    "\\nresults.xyxy[0].shape: {results.xyxy[0].shape}')\n",
    "\n",
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# plot predicted results\n",
    "cv_img = np.array(pil_img)\n",
    "RGB_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "annos = results.pandas().xyxy[0]\n",
    "\n",
    "for i in range(len(annos)):\n",
    "    x_min, y_min, x_max, y_max = (\n",
    "        int(annos[\"xmin\"].iloc[i]),\n",
    "        int(annos[\"ymin\"].iloc[i]),\n",
    "        int(annos[\"xmax\"].iloc[i]),\n",
    "        int(annos[\"ymax\"].iloc[i]),\n",
    "    )\n",
    "\n",
    "    if annos[\"name\"].iloc[i] == \"person\":\n",
    "        color = (255, 255, 255)\n",
    "    if annos[\"name\"].iloc[i] == \"bicycle\":\n",
    "        color = (0, 0, 255)\n",
    "    if annos[\"name\"].iloc[i] == \"backpack\":\n",
    "        color = (0, 255, 0)\n",
    "    RGB_img = cv2.rectangle(RGB_img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "cv2_imshow(RGB_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако если подать на вход модели тензор, выходы радикально меняются. YOLO переходит в режим обучения, что довольно не очевидно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand((1, 3, 416, 416))\n",
    "results = model(input)\n",
    "\n",
    "print(f'type(results): {type(results)}\\nlen(results): {len(results)}\\n')\n",
    "print(f'results[0].shape: {results[0].shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolox_speed.png\" width=\"500\">\n",
    "\n",
    "\n",
    "[YOLOX: Exceeding YOLO Series in 2021](https://arxiv.org/abs/2107.08430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы использовали в качестве baseline модели YOLOv3, и, убедившись на ней в эффективности усовершенствований, применили некоторые из них к YOLOv4 и YOLOv5.\n",
    "\n",
    "Далее рассмотрим список нововведений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoupled head\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolox_decoupled_head.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же как в RetinaNet для регрессии и классификации используются различные головы (подсети)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor-free\n",
    "\n",
    "Все детекторы, которые рассматривались ранее, использовали несколько предопределенных default box (anchor) для каждой точки на карте признаков.\n",
    "Количество и размер этих якорных окон являются гиперпараметрами модели.\n",
    "\n",
    "\n",
    "В 2019 г вышла статья [FCOS: Fully Convolutional One-Stage](https://towardsdatascience.com/forget-the-hassles-of-anchor-boxes-with-fcos-fully-convolutional-one-stage-object-detection-fc0e25622e1c) где авторы отказываются от такого подхода.  Для каждой точки на карте признаков сразу предсказывают один bounding box.\n",
    "\n",
    "Если пиксель соответствующий центру предсказанного bounding box попадает в истинный (ground true) bounding box, то он используется при подсчете loss.\n",
    "\n",
    "\n",
    "Это подход был применен и в YOLOX.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/yolox_techniques.png\" width=\"1000\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* decoupled head\n",
    "* anchor-free, \n",
    "* and advanced label assigning strategy (SimOTA)\n",
    "* MultiPositives\n",
    "* OTA/SimOTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нard Example Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении модели, мы можем обнаружить, что средняя ошибка на всех данных достаточно маленькая, однако, ошибка на редких нетипичных данных довольно высока. При этом, нетипичные данные необязательно являются выбросами.\n",
    "\n",
    "Разберемся, почему так происходит:\n",
    "К примеру, рассмотрим задачу обнаружения автомобилей на потоках данных с камер наружного видеонаблюдения. Если в обучающем наборе большая часть данных - снимки, сделанные днём, то качество работы модели ночью будет низким. В данном случае, \"нетипичными\" данными будут ночные снимки. Но, на самом-то деле, \"нетипичных\" случаев может быть довольно много, и некоторые из них могут происходить даже днём: \n",
    "* изменение погоды (изменение яркости, резкости, помехи на изображении)\n",
    "* смена сезона (снег либо листья могут покрыть дорогу - изменение фона)\n",
    "* машины с экзотическими узорами на кузове"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольно простым и эффективным решением проблемы является сбор \"сложных\" случаев (**hard example mining**) и дообучение модели на них. При этом, поскольку модель уже довольно хорошо работает на большей части данных, можно дополнительно удалить часть данных из обучающей выборки - таким образом, мы сосредотачиваем модель на обучении на сложных примерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/hard_example_mining.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online hard example mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых случаях, hard exapmle mining можно выполнять прямо во время формирования батча, \"на лету\". В таких случаях, говорят про **online hard example mining**.\n",
    "\n",
    "Один из вариантов может быть реализован в two-stage детекторах.\n",
    "Напоминаю: первая часть детектора отвечает за обнаружение regions of interest (RoI), затем выполняется (как правило, сравнительно вычислительно дешёвая) классификация. Одним из вариантов реализации идеи может быть выполнение forward pass классификатора по всем предложенным RoI, и затем формирование батча, в котором будет выделено определённое количество \"мест\" под RoI, предсказания на которых выполняются наихудшим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/online_hard_example_mining.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из интересных способов борьбы с падением качества работы модели на нестандартных данных, является уже упомянутый **focal loss**.\n",
    "При использовании кросс-энтропии, loss от большого количества \"хорошо распознанных\" примеров суммарно может быть более значительным, чем loss от малого количества \"плохо распознанных\" примеров. Итого, при обучении модель стремится снизить свою ошибку на большинстве и так уже неплохо предсказываемых примеров, не стремясь исправлять высокую ошибку на редких примерах.\n",
    "\n",
    "Идея **focal loss** заключается в том, что можно попробовать снизить значения **cross-entropy loss** на и так уже неплохо предсказываемых примерах, чтобы дать модели возможность лучше обучиться на \"сложных\" для неё примерах.\n",
    "\n",
    "Давайте посчитаем для различных значений $γ$, сколько понадобится примеров с небольшой ошибкой, чтобы получить суммарный **focal loss** примерно такой же, как у одного примера с большой ошибкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(percent_correct):\n",
    "    return -np.log(percent_correct)\n",
    "\n",
    "def focal_loss(percent_correct, gamma=5):\n",
    "    return cross_entropy(percent_correct) * (1 - percent_correct) ** gamma\n",
    "\n",
    "p1 = 0.8  # probability for correct examples predictions\n",
    "p2 = 0.4  # probability for hard examples predictions\n",
    "gammas = [0, 1, 5, 9, 13, 17]\n",
    "\n",
    "print(f\"For probability for correct examples predictions {p1} and probability for hard examples predictions {p2}\\n\")\n",
    "\n",
    "for gamma in gammas:\n",
    "    fl1 = focal_loss(p1, gamma)\n",
    "    fl2 = focal_loss(p2, gamma)\n",
    "\n",
    "    print(f\"gamma = {gamma},\".ljust(15), \\\n",
    "          f\"for an equal loss with a problematic prediction, almost correct ones are required {int(fl2 / fl1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, при увеличении значения $\\gamma$, можно достичь значительного роста \"важности\" примеров с высокой ошибкой, что, по сути, позволяет модели обращать внимание на \"hard examples\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог пост про Hard Mining Example](https://erogol.com/online-hard-example-mining-pytorch/)\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining (Shrivastava et al., 2016)](https://arxiv.org/abs/1604.03540)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors (Yu et al., 2018)](https://arxiv.org/abs/1804.04606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L12/out/detection_instance_segmentation.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[COCO panoptic](https://cocodataset.org/#panoptic-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/panoptic_segmentation.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mask R-CNN** (Detectron) - концептуально простая, гибкая и общая схема сегментации объектов. Подход эффективно обнаруживает объекты на изображении и одновременно генерирует высококачественную маску сегментации для каждого объекта. \n",
    "\n",
    "Метод, названный Mask R-CNN, расширяет Faster R-CNN (который мы обсуждали ранее), добавляя ветвь для предсказания маски объекта параллельно с существующей ветвью для распознавания *bounding boxes*. \n",
    "\n",
    "Код доступен [по ссылке](https://github.com/facebookresearch/Detectron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/mask_r_cnn.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Модель Mask R-CNN](https://pytorch.org/vision/stable/models.html#mask-r-cnn)\n",
    "\n",
    "[Пример запуска Mask R-CNN есть в документации Pytorch](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества детекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP - mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP (*Average Precision* - средняя точность) - это популярная метрика для измерения качества детекторов объектов, таких как Faster R-CNN, SSD и др. Средняя точность вычисляет среднее значение *precision* (точности) для значения *recall* от 0 до 1. Звучит сложно, но на самом деле довольно просто. Давайте разберем на конкретных примерах. Но перед этим давайте вкратце разберем, что такое *precision*, *recall* и IoU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision & recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** измеряет, насколько точны предсказания сети (т.е. процент правильных предсказаний)\n",
    "\n",
    "**Recall** измеряет, насколько хорошо сеть находит все положительные срабатывания (*positives*). Например, мы можем найти 80% возможных положительных срабатываний в наших K лучших предсказаниях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот их математические определения:\n",
    "\n",
    "$\\displaystyle\\mathrm{Precision} = \\frac{TP}{TP+FP}$\n",
    "\n",
    "$\\displaystyle\\mathrm{Recall} = \\frac{TP}{TP+FN}$\n",
    "\n",
    "где $TP$ - True Positive, $TN$ - True Negative, $FP$ - False Positive, $FN$ - False Negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы пытаемся детектировать яблоки на фотографиях. Предположим, мы обработали 20 фотографий (на 10 фотографиях по одному яблоку и на 10 фотографиях яблок нет) и обнаружили что:\n",
    "\n",
    "* в 7 случаях наша нейросеть обнаружила яблоко там, где оно было на самом деле (True Positive)\n",
    "* в 3 случаях не обнаружила яблоко там, где оно было (False Negative)\n",
    "* в 4 случаях обнаружила яблоко там, где его не было (False Positive)\n",
    "* в 6 случаях правильно определила, что на фотографии яблок нет (True Negative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем precision и recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(TP, FP):\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "pres = precision(TP=7, FP=4)\n",
    "rec  = recall(TP=7, FN=3)\n",
    "\n",
    "print('Precision = %.2f' % pres)\n",
    "print('Recall = %.2f' % rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU (Intersection over union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU измеряет перекрытие между двумя границами. Мы используем его для измерения того, насколько сильно наша предсказанная граница совпадает с истиной (границей реального объекта). В некоторых наборах данных мы заранее определяем порог IoU (например, 0.5) для классификации того, является ли предсказание True Positive или False Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, рассмотрим предсказание сети для фотографии яблока:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/L12/red_apple.jpg -O img.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "img = Image.open('img.jpg')\n",
    "array = np.array(img)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "x0 = 75\n",
    "y0 = 80\n",
    "w0 = 450\n",
    "h0 = 440\n",
    "\n",
    "ground_truth = Rectangle((x0, y0), w0, h0, linewidth=2, edgecolor='g', linestyle='--', facecolor='none')\n",
    "\n",
    "x1 = 90\n",
    "y1 = 120\n",
    "w1 = 500\n",
    "h1 = 310\n",
    "predicted = Rectangle((x1, y1), w1, h1, linewidth=2, edgecolor='b', facecolor='none')\n",
    "\n",
    "ax.add_patch(ground_truth)\n",
    "ax.add_patch(predicted)\n",
    "\n",
    "ax.imshow(array)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU определена как\n",
    "\n",
    "$\\displaystyle\\mathrm{IoU} = \\frac{\\text{area of overlap}}{\\text{area of union}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем *area of overlap* и *area of union*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1):\n",
    "    x0_max = x0 + w0\n",
    "    y0_max = y0 + h0\n",
    "    x1_max = x1 + w1\n",
    "    y1_max = y1 + h1\n",
    "    dx = min(x0_max, x1_max) - max(x0, x1)\n",
    "    dy = min(y0_max, y1_max) - max(y0, y1)\n",
    "    if (dx >= 0) and (dy >= 0):\n",
    "        return dx * dy\n",
    "\n",
    "def area_of_rectangle(w, h):\n",
    "    return w * h\n",
    "# Compute intersection areas of the predictions\n",
    "a_of_overlap = area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1)\n",
    "\n",
    "# Compute individual areas of the rectangles\n",
    "a_0 = area_of_rectangle(w0, h0)\n",
    "a_1 = area_of_rectangle(w1, h1)\n",
    "\n",
    "# Compute area of their union\n",
    "a_of_union = a_0 + a_1 - a_of_overlap\n",
    "\n",
    "print('Area of overlap = %i' % a_of_overlap)\n",
    "print('Area of union = %i' % a_of_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoU = a_of_overlap / a_of_union\n",
    "print('IoU = %.2f' % IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как будет меняться IoU в зависимости от качества предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plot_predictions_and_calculate_IoU(x1, y1, w1, h1):\n",
    "    fig,ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    x0 = 75\n",
    "    y0 = 80\n",
    "    w0 = 450\n",
    "    h0 = 440\n",
    "\n",
    "    ground_truth = Rectangle((x0, y0), w0, h0, linewidth=2, edgecolor='g', linestyle='--', facecolor='none')\n",
    "    predicted = Rectangle((x1, y1), w1, h1, linewidth=2, edgecolor='b', facecolor='none')\n",
    "\n",
    "    ax.add_patch(ground_truth)\n",
    "    ax.add_patch(predicted)\n",
    "\n",
    "    ax.imshow(array)\n",
    "    ax.axis('off');\n",
    "\n",
    "    a_of_overlap = area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1)\n",
    "    a_0 = area_of_rectangle(w0, h0)\n",
    "    a_1 = area_of_rectangle(w1, h1)\n",
    "    a_of_union = a_0 + a_1 - a_of_overlap\n",
    "    IoU = a_of_overlap / a_of_union\n",
    "    print('IoU = %.2f' % IoU)\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_predictions_and_calculate_IoU, \n",
    "         x1 = widgets.IntSlider(min=0, max=array.shape[0], step=10, value=90), \n",
    "         y1 = widgets.IntSlider(min=0, max=array.shape[1], step=10, value=120), \n",
    "         w1 = widgets.IntSlider(min=0, max=array.shape[0], step=10, value=500),\n",
    "         h1 = widgets.IntSlider(min=0, max=array.shape[1], step=10, value=310)\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что чем лучше предсказание совпадает с реальностью - тем выше у нас значение метрики IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжая тему яблок, возьмем все те же 20 фотографий на 10 из которых яблоки были, а на 10 яблок не было. И представим, что у нас есть некая нейросеть, которая выдает следующие предсказания по всему датасету (или например по батчу):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nn_preds = pd.DataFrame({'IoU' : [0.6, 0.98, 0.4, 0.3, 0.1, 0.96, 0.7, 0.3, 0.2, 0.8],  \n",
    "                        'precision' : [1,1,0.67,0.5,0.4,0.5,0.57,0.5,0.44,0.5],\n",
    "                        'recall' : [0.2,0.4,0.4,0.4,0.4,0.6,0.8,0.8,0.8,1]})\n",
    "nn_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем считать, что если $\\mathrm{IoU} \\geq 0.5$ - то предсказание правильное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_preds['correct'] = False\n",
    "nn_preds.loc[nn_preds['IoU'] >= 0.5, 'correct'] = True\n",
    "\n",
    "nn_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что precision имеет зигзагообразный характер - она снижается при ложных срабатываниях и снова повышается при истинных срабатываниях. \n",
    "\n",
    "Давайте построим график precision от recall и убедимся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10, 3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision)\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По определению, чтобы найти AP, нужно найти площадь под кривой recall-precision:\n",
    "\n",
    "$\\displaystyle AP = \\int_0^1p(r)dr$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision и recall всегда находятся в пределах от 0 до 1. Поэтому $AP$ также находится в пределах от 0 до 1. Перед расчетом $AP$ для обнаружения объекта мы часто сначала сглаживаем зигзагообразный рисунок (на каждом уровне recall мы заменяем каждое значение precision максимальным значением точности справа от этого уровня отзыва)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_precision(nn_preds):\n",
    "    smooth_prec = []\n",
    "    for i in range(len(nn_preds.precision.values)):\n",
    "        max = nn_preds.precision.values[i:].max()\n",
    "        smooth_prec.append(max)\n",
    "    nn_preds['smooth_precision'] = smooth_prec\n",
    "    return nn_preds\n",
    "\n",
    "nn_preds = smooth_precision(nn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как это выглядит на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10, 3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision, label='precision', color='blue')\n",
    "ax.plot(nn_preds.recall, nn_preds.smooth_precision, label='smooth precision', color='red')\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем нам нужно сглаживание? Чтобы снизить влияние случайных выбросов и \"прыжков\" в предсказаниях модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посчитаем-таки $AP$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10, 3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision, label='precision', color='blue')\n",
    "ax.plot(nn_preds.recall, nn_preds.smooth_precision, label='smooth precision', color='red')\n",
    "ax.fill_between(nn_preds.recall, nn_preds.smooth_precision, \n",
    "                np.zeros_like(nn_preds.smooth_precision), \n",
    "                color='red', alpha=0.1,\n",
    "                label='Area under the curve')\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_ylim(0.35, 1.05)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "AP = auc(nn_preds.recall, nn_preds.smooth_precision)\n",
    "\n",
    "print('AP = %.2f' % AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последних исследовательских работах, как правило, приводятся результаты только для набора данных COCO. Для COCO AP - это среднее значение (*mean*) по нескольким IoU (минимальный IoU, который следует считать положительным совпадением). AP@[.5:.95] соответствует среднему AP для IoU от 0.5 до 0.95 с шагом 0.05. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем посчитать mAP. Для этого посчитаем AP для каждого уровня IoU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_prediction_at_iou = []\n",
    "APs = []\n",
    "for iou in np.arange(0.5, 1, 0.05):\n",
    "    nn_preds_limited = nn_preds[nn_preds['IoU'] >= iou].copy()\n",
    "    nn_preds_limited = smooth_precision(nn_preds_limited)\n",
    "    AP = auc(nn_preds_limited.recall, nn_preds_limited.smooth_precision)\n",
    "    APs.append(AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5), dpi=80)\n",
    "\n",
    "plt.plot(np.arange(0.5, 1, 0.05), APs, color='black')\n",
    "plt.axhline(np.mean(APs), color='red', ls='--', label='mAP')\n",
    "plt.xlabel('IoU')\n",
    "plt.ylabel('AP')\n",
    "plt.grid('on')\n",
    "plt.legend();\n",
    "\n",
    "print('mAP@[0.5:0.95] = %.2f' % np.mean(APs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть несколько различных определений mAP, которые разняться от соревнования к соревнованию (суть одинаковая, но разница в деталях подхода), поэтому для каждого соревнования лучше использовать их собственные библиотеки для расчета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO — Self-supervised representation learning (with segmentation capabilities)\n",
    "[Emerging Properties in Self-Supervised Vision Transformers (Caron et al., 2021)](https://arxiv.org/abs/2104.14294)\n",
    "\n",
    "[Отличное видео объяснение статьи](https://www.youtube.com/watch?v=h3ij3F3cPIk)\n",
    "\n",
    "[vision_transformer](https://github.com/google-research/vision_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала подгрузим модель DINO (self-**DI**stillation with **NO** labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/dino.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузим случайную картинку (можно выбрать любую, просто замените ссылку на свою)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://edunet.kea.su/repo/EduNet-web_dependencies/L12/capybara_image.jpg'\n",
    "!wget $URL -qO test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "input_img = Image.open('/content/test.jpg')\n",
    "input_img.resize((400, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим результат с помощью DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!python /content/dino/visualize_attention.py --image_path /content/test.jpg \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на картинки, которые генерирует DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def img_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "    fig,ax = plt.subplots(nrows=rows, ncols=cols, figsize=(28, 8))\n",
    "    for num, img in enumerate(imgs):\n",
    "        img_PIL = Image.open(img)\n",
    "        ax[num].imshow(img_PIL)\n",
    "        ax[num].set_xticks([])\n",
    "        ax[num].set_yticks([])\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "img_grid(imgs=sorted(glob('*.png'))[::-1], rows=1, cols=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим 6 карт внимания (self-attention maps - веса слоя self-attention) на 6 головах Visual Transformer. В результате self-supervised обучения по методике DINO, трансформер САМОСТОЯТЕЛЬНО придумал обращать внимание на различные части изображения, таким образом, производя семантическую сегментацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем все эти карты внимания объединить в одно изображение, и просто назначить каждой карте свой цвет, а в качестве прозрачности использовать интенсивность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm\n",
    "\n",
    "def overlay(img, segmentations):\n",
    "    img_PIL = Image.open(img)\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10, 10))\n",
    "    ax[0].imshow(img_PIL)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    \n",
    "    ax[1].imshow(img_PIL.convert('LA'), alpha=0.5)\n",
    "    for num, img in enumerate(segmentations):\n",
    "        segment_PIL = Image.open(img).convert('LA')\n",
    "        segment_arr = np.array(segment_PIL)\n",
    "        colors = [(*cm.tab10(num)[:-1], c) for c in np.linspace(0, 0.75, 100)]\n",
    "        cmap = mcolors.LinearSegmentedColormap.from_list('mycmap', colors, N=5)\n",
    "        ax[1].imshow(segment_arr[:, :, 0], cmap=cmap)\n",
    "    ax[1].set_facecolor('black')\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "overlay('/content/img.png', sorted(glob('*.png'))[:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что DINO сегментирует разные части нашей картинки на разные семантические группы. В случае с капибарой - это голова, лицо (нос, глаза) и тело."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация видео"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы думали, что на этом все? Нет, DINO может еще удивить. Например, она умеет сегментировать видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install --upgrade youtube_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем видео (можно любое)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://edunet.kea.su/repo/EduNet-web_dependencies/L12/cheetah_video.mp4'\n",
    "!youtube-dl -o video.mp4 $URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сгенерируем сегментированное видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!python /content/dino/video_generation.py --input_path /content/video.mp4 --output_path  /content/video_segmented --resize 360 640\n",
    "!ffmpeg -i /content/video.mp4 -vf scale=640:360 /content/video_scaled.mp4\n",
    "!ffmpeg \\\n",
    "  -i /content/video_scaled.mp4 \\\n",
    "  -i /content/video_segmented/video.mp4 \\\n",
    "  -filter_complex '[0:v]pad=iw*2:ih[int];[int][1:v]overlay=W/2:0[vid]' \\\n",
    "  -map '[vid]' \\\n",
    "  -c:v libx264 \\\n",
    "  -crf 23 \\\n",
    "  -preset veryfast \\\n",
    "  output.mp4\n",
    "\n",
    "clear_output()\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось. Обратите внимание, эта сеть обучалась в режиме self-supervision!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open('/content/output.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=800 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще DINO умеет кластеризовать изображения. Выполнять не будем, так как процесс не быстрый, но можем посмотреть на результаты из их статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/clustering_dino.gif\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Aug 2021 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n",
    "\n",
    "Текущий SOTA результат принадлежит модели на базе трансформера.\n",
    "\n",
    "Применять ViT напрямую для задач сегментации и детектирования - не слишком эффективно, так как при больших размерах patch (16x16) не получится получить точные границы объектов.\n",
    "\n",
    "А при уменьшении размеров patch будет требоваться все больше ресурсов так как сложность self-attention $O(n^{2})$ пропорциональна квадрату количества элементов на входе.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/swin_vs_vit.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы решают проблему при помощи двух усовершенствований.\n",
    "\n",
    "Self - attention применяется не ко всему изображению сразу, а к его большим фрагментам окнам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первый взгляд это возвращает проблему сверток, про которую мы говорили:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/cnn_fail.png\" width=\"600\">\n",
    "\n",
    "\n",
    "Указываем сети куда смотреть, и это помешает оценить взаимное влияние признаков расположенных на разных углах карты.\n",
    "\n",
    "Чтобы не допустить этой проблемы на каждом следующем transformer -  слое окно сдвигается.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/swin_window_shift.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, сеть может выучить влияние любого патча на любой. При этом не требуется увеличивать количество входов self-attention блока и количество вычислений не растет. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, пространственные размеры карт признаков, уменьшаются аналогично тому, как это происходит в сверточных сетях. Для сегментирования и детектирования используется принцип FPN - признака с разных пространственных карт агрегируются для предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/swin_architecture.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch merging здесь это конкатенация эмбеддингов, с последующей подачей на вход линейного слоя:\n",
    "\n",
    "Фрагмент из 4-х эмбеддингов 2x2xC конкатенируются. Получаем один тензор 1x1x4C\n",
    "\n",
    "Затем подаем его на вход линейному слою,  уменьшающему число каналов в 2 раза,\n",
    "получаем новый эмбеддинг размерностью: 1x1x2C\n",
    "\n",
    "Таким образом, в отличие от традиционных трансформер архитектур размер embedding здесь меняется.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L12/swin_result.png\" width=\"500\">\n",
    "\n",
    "Такой подход позволил достичь SOTA результатов как в задаче классификации, так и в задачах детектирования и сегментации. \n",
    "Авторы статьи позиционируют Swin - трансформер как backbon решения широкого круга задач CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"6\">Список использованной литературы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Подробнее о том, как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch).\n",
    "\n",
    "[Подробнее о разметке COCO](https://cocodataset.org/#format-data)\n",
    "\n",
    "[Что такое  run-length encoding - RLE](https://en.wikipedia.org/wiki/Run-length_encoding)?\n",
    "\n",
    "[Видео-разбор Run-length encoding](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Семантическая сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FCN Semantic segmenation](https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1)\n",
    "\n",
    "[Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Chen et al., 2018)](https://arxiv.org/abs/1802.02611v3)\n",
    "\n",
    "[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (He et al., 2014)](https://arxiv.org/abs/1406.4729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Детектирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Recent Progress in Appearance-based Action Recognition (Humphreys et al., 2020)](https://arxiv.org/abs/2011.12619)\n",
    "\n",
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector (Liu et al., 2015)](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "[Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "[Блог-пост: Что такое Focal Loss и когда его использовать](https://amaarora.github.io/2020/06/29/FocalLoss.html)\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection (Sergelius et al., 2016)](https://arxiv.org/abs/1612.0314)\n",
    "\n",
    "[Understanding feature pyramid networks for object detection - FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)\n",
    "\n",
    "[You Only Look Once: Unified, Real-Time Object Detection (Redmon et. al., 2015)](https://arxiv.org/abs/1506.02640) \n",
    "\n",
    "[YOLO9000: Better, Faster, Stronger (Redmon et. al., 2015)](https://arxiv.org/abs/1612.08242)\n",
    "\n",
    "[YOLOv3: An Incremental Improvement (Redmon et. al., 2018)](https://arxiv.org/abs/1804.02767)\n",
    "\n",
    "[YOLOv4: Optimal Speed and Accuracy of Object Detection (Bochkovskiy et al., 2020)](https://arxiv.org/abs/2004.10934)\n",
    "\n",
    "[YOLOv5 (Glenn Jocher)](https://github.com/ultralytics/yolov5)\n",
    "\n",
    "[YOLOX: Exceeding YOLO Series in 2021 (Ge et al., 2021)](https://arxiv.org/abs/2107.08430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hard example mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог пост про Hard Mining Example](https://erogol.com/online-hard-example-mining-pytorch/)\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining (Shrivastava et al., 2016)](https://arxiv.org/abs/1604.03540)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors (Yu et al., 2018)](https://arxiv.org/abs/1804.04606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " DINO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Emerging Properties in Self-Supervised Vision Transformers (Caron et al., 2021)](https://arxiv.org/abs/2104.14294)\n",
    "\n",
    "[Отличное видео объяснение статьи DINO](https://www.youtube.com/watch?v=h3ij3F3cPIk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "[Блог-пост про 2d свертки с помощью перемножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
