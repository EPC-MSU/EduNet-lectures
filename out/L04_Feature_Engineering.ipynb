{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Генерация и отбор признаков</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проблемы при работе с реальной задачей машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальных задачах вы столкнетесь с широким спектром трудностей, и одним из первых этапов будет подготовка данных. Уже на этом этапе есть ряд возможных проблем, например:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Нехватка данных** — у вас может быть мало данных. Более того, понять, какой объем данных необходим — тоже отдельная задача. Другая вариация похожей проблемы — это большой объем данных, но без разметки.\n",
    "\n",
    "- **Некачественная разметка** —  даже в широко известных MNIST, CIFAR-10 и ImageNet есть ошибки в разметке ([примеры](https://labelerrors.com/)). В вашем датасете они тоже будут. Важно понимать, что разметка может не являться бесспорным эталоном.\n",
    "\n",
    "- **Шум в данных** — полезно подумать о потенциальной зашумленности данных. Например, когда люди заполняют таблицы, они могут допускать ошибки. Хорошо понимать, какого рода ошибки могут содержаться в ваших данных.\n",
    "\n",
    "- **Несбалансированность датасета** — какие-то классы могут быть плохо представлены (**минорные классы**). Например, если в вашем датасете будет всего 10 фотографий одного класса и 10 000 другого, то модели будет очень заманчиво вообще не пытаться определять минорный класс (всего 0.1% ошибок).\n",
    "\n",
    "\n",
    "- **Ковариантный сдвиг** — явление, когда признаки тренировочной и тестовой выборок **распределены по-разному**. Из за этого мы можем получать плохие предсказания на тестовой выборке, когда модель просто не знает, что признаки могут находиться в другом распределении.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/covariate_shift.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Практический совет: для быстрого обнаружения ковариантного сдвига можно **обучить модель**, которая будет предсказывать, относится ли объект к **train** или **test** выборке. Если модель легко делит данные по такому принципу, то имеет смысл визуализировать значения признаков, по которым она это делает.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Малое число источников данных** — проблема, родственная предыдущей. В вашем датасете могут быть данные только от одного прибора или одной модели прибора. Могут быть данные, снятые только одним специалистом, или в одной больнице, или только у взрослых. Это также может влиять на способность вашего алгоритма обобщать полученное решение и требует пристального внимания.\n",
    "\n",
    "\n",
    "- **Грязные данные** — в данных могут быть полные/неполные дубликаты, пропуски, ошибки форматов, перемешавшиеся столбцы и многое другое. Такие проблемы могут быть как естественной характеристикой (нет данных о каких-то объектах, потому и пропуски), так и ошибкой при аггрегации данных из разных источников. Важно подумать о проверках и тестах, чтобы быть уверенным в качестве.\n",
    "\n",
    "Все это приводит к целому спектру сложностей, из которых самой типичной будет переобучение модели — какую бы простую модель вы не взяли, она все равно будет выучивать искажения вашего датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде всего надо убедиться, что датасет сбалансирован:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "\n",
    "def show_class_balance(y, classes):\n",
    "    _, counts = torch.unique(torch.tensor(y), return_counts=True)\n",
    "    plt.bar(classes, counts)\n",
    "    plt.ylabel(\"n_samples\")\n",
    "    plt.ylim([0, 75])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "wine = load_wine()\n",
    "classes = wine.target_names\n",
    "\n",
    "show_class_balance(wine.target, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница в 10–20% будет незначительна, поэтому для наглядности мы искусственно разбалансируем наш датасет при помощи метода [`make_imbalance`](https://imbalanced-learn.org/stable/references/generated/imblearn.datasets.make_imbalance.html) из библиотеки [imbalanced-learn](https://imbalanced-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "\n",
    "x, y = make_imbalance(\n",
    "    wine.data, wine.target, sampling_strategy={0: 10, 1: 70, 2: 40}, random_state=42\n",
    ")\n",
    "show_class_balance(y, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение баланса класса сэмплированием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в данных нехватка именно конкретного класса, то можно бороться с этим при помощи разных способов сэмплирования.\n",
    "\n",
    "Важно понимать, что в большинстве случаев данные, полученные таким способом, должны использоваться в качестве **обучающего набора**, но ни в коем случае не в качестве **валидации** или **теста**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дублирование примеров меньшего класса (oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем увеличить число объектов меньшего класса за счет дублирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/oversampling_scheme.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Дублирование примеров меньшего класса</em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой oversampling может быть выполнен с помощью класса [`RandomOverSampler`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html) из imbalanced-learn, как показано ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x_ros, y_ros = ros.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_ros, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Уменьшение числа примеров большего класса (undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично, можно взять для обучения не всех представителей большего класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/undersampling_scheme.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Удаление примеров преобладающего класса</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минус подхода: мы можем выбросить важных представителей большего класса, ответственных за существенное улучшение генерализации, и из-за этого качество модели существенно ухудшится.\n",
    "\n",
    "Выбрасывать объекты большего класса  можно разными способами. Например, кластеризовать объекты большего класса и брать по заданному количеству объектов из каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_res, y_res = rus.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_res, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ансамбли + undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать ансамбли вместе с undersampling. В этом случае мы можем, к примеру, делать сэмплирование только большего класса, а объекты минорного класса оставлять как есть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/ensembles_and_undersampling.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или просто сэмплировать объекты и того, и другого класса в равном количестве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация синтетических данных\n",
    "\n",
    "Другой подход к решению этой проблемы — создание синтетических данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE\n",
    "\n",
    "**Synthetic Minority Over-sampling Technique (SMOTE)** позволяет генерировать синтетические данные за счет реальных объектов из минорного класса.\n",
    "\n",
    "Алгоритм работает следующим образом:\n",
    "\n",
    "1. Для случайной точки из минорного класса выбираем $k$ ближайших соседей из того же класса.\n",
    "2. Для первого соседа проводим отрезок, соединяющий его и выбранную точку. На этом отрезке случайно выбираем точку.\n",
    "3. Эта точка — новый **синтетический** объект минорного класса.\n",
    "4. Повторяем процедуру для оставшихся соседей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/generate_synthetic_data.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число соседей, как и число раз, которое мы запускаем описанную выше процедуру, можно регулировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "x_smote, y_smote = oversample.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_smote, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество объектов каждого класса, которое должно получиться после генерации, можно задать явно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy={0: 20, 1: 70, 2: 70})\n",
    "x_smote, y_smote = over.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_smote, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее про использование пакета можно прочесть в статье: [SMOTE for Imbalanced Classification with Python](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики качества на несбалансированных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращайте внимание на то, какие метрики вы используете. При решении задачи классификации часто используется accuracy (точность), равная доле правильно классифицированных объектов. Эта метрика позволяет адекватно оценить результат классификации в случае сбалансированных классов. В случае дисбаланса классов данная метрика может выдать обманчиво хороший результат.\n",
    "\n",
    "Пример: датасет, в котором 95% объектов относятся к классу 0 и 5% к классу 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "x, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.95],\n",
    "    flip_y=0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "counter = Counter(y)\n",
    "print(\"Class distribution \", Counter(y))\n",
    "\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(y == label)[0]\n",
    "    plt.scatter(x[row_ix, 0], x[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И модель, которая для всех данных выдает класс 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    def predict(self, x):\n",
    "        return np.zeros(x.shape[0])  # always predict class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая модель будет иметь $accuracy = 0.95$, хотя не выдает никакой полезной информации:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_model = DummyModel()\n",
    "y_pred = dummy_model.predict(x)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для несбалансированных данных лучше выбирать [F1 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score),  [MCC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html?highlight=matthews%20correlation%20coefficient#sklearn.metrics.matthews_corrcoef) (Matthews correlation coefficient, коэффициент корреляции Мэтьюса) или [balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) (среднее между recall разных классов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef, balanced_accuracy_score\n",
    "\n",
    "print(\"F1\", f1_score(y, y_pred))\n",
    "print(\"MCC\", matthews_corrcoef(y, y_pred))\n",
    "print(\"Balanced accuracy\", balanced_accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти метрики более адекватно оценивают качество модели в случае наличия дисбаланса классов, и в данном случае отражают отсутствие связи предсказаний с данными на входе модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно: [Your Dataset Is Imbalanced? Do Nothing!](https://towardsdatascience.com/your-dataset-is-imbalanced-do-nothing-abf6a0049813)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обнаружение аномалий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае сильно несбалансированных наборов данных стоит задуматься, могут ли такие примеры рассматриваться как аномалия (выброс) или нет. Если такое событие и впрямь может считаться аномальным, мы можем использовать такие модели, как `OneClassSVM`, методы кластеризации или методы обнаружения гауссовских аномалий.\n",
    "\n",
    "Эти методы требуют изменения взгляда на задачу: мы будем рассматривать аномалии как отдельный класс выбросов. Это может помочь нам найти новые способы разделения и классификации.\n",
    "\n",
    "Пусть мы хотим отличать фрукты по каким-то признакам, скажем, по форме и цвету, и мы имеем выборку, где много яблок и груш. Задача обнаружения аномалий возникает, если мы предполагаем, что среди яблок и груш может вдруг возникнуть клубника и нам нужно отнести ее не к одному из известных классов, а выделить в отдельный, отличающийся класс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/anomaly_fruits.png' width='800'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемой при работе с аномалиями является то, что аномальных значений может быть очень мало или вообще не быть. В таком случае алгоритм учит паттерны нормального поведения и реагирует на отличия от паттернов.\n",
    "\n",
    "Разберем примеры обнаружения аномалий с помощью трех алгоритмов из библиотеки Scikit-Learn (там можно найти еще много различных алгоритмов).\n",
    "\n",
    "Создадим датасет из двух кластеров и случайных значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Train\n",
    "x = 0.3 * rng.randn(100, 2)  # 100 2D points\n",
    "x_train = np.r_[x + 2, x - 2]  # split into two clusters\n",
    "\n",
    "# Test norlmal\n",
    "x = 0.3 * rng.randn(20, 2)  # 20 2D points\n",
    "x_test_normal = np.r_[x + 2, x - 2]  # split into two clusters\n",
    "\n",
    "# Test outliers\n",
    "x_test_outliers = rng.uniform(low=-4, high=4, size=(20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию визуализации, которая будет изображать созданный датасет на рисунке слева, а результат поиска аномалий — на рисунке справа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(x_train, x_test_normal, x_test_outliers, model=None):\n",
    "    fig, (plt_data, plt_model) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    plt_data.set_title(\"Created Dataset (real labels)\")\n",
    "    plot_train = plt_data.scatter(\n",
    "        x_train[:, 0], x_train[:, 1], c=\"white\", s=40, edgecolor=\"k\"\n",
    "    )\n",
    "    plot_test_normal = plt_data.scatter(\n",
    "        x_test_normal[:, 0], x_test_normal[:, 1], c=\"green\", s=40, edgecolor=\"k\"\n",
    "    )\n",
    "    plot_test_outliers = plt_data.scatter(\n",
    "        x_test_outliers[:, 0], x_test_outliers[:, 1], c=\"red\", s=40, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    plt_data.set_xlim((-5, 5))\n",
    "    plt_data.set_ylim((-5, 5))\n",
    "\n",
    "    plt_data.legend(\n",
    "        [plot_train, plot_test_normal, plot_test_outliers],\n",
    "        [\"train\", \"test normal\", \"test outliers\"],\n",
    "        loc=\"lower right\",\n",
    "    )\n",
    "\n",
    "    if model:\n",
    "        plt_model.set_title(\"Model Results\")\n",
    "        # plot decision function\n",
    "        xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "        Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        plt_model.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "        # plot prediction\n",
    "        full_data = np.concatenate((x_train, x_test_normal, x_test_outliers), axis=0)\n",
    "        predicted = model.predict(full_data)\n",
    "\n",
    "        anom_index = np.where(predicted == -1)\n",
    "        anom_values = full_data[anom_index]\n",
    "\n",
    "        plot_all_data = plt_model.scatter(\n",
    "            full_data[:, 0], full_data[:, 1], c=\"white\", s=40, edgecolor=\"k\"\n",
    "        )\n",
    "\n",
    "        plot_anom_data = plt_model.scatter(\n",
    "            anom_values[:, 0], anom_values[:, 1], c=\"red\", s=40, marker=\"x\"\n",
    "        )\n",
    "        plt_model.legend(\n",
    "            [plot_all_data, plot_anom_data],\n",
    "            [\"normal\", \"outliers\"],\n",
    "            loc=\"lower right\",\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как работает на этих данных алгоритм [OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html).\n",
    "\n",
    "Идея алгоритма состоит в поиске функции, которая положительна для областей с высокой плотностью и отрицательна для областей с малой плотностью. Подробнее об алгоритме можно прочитать в оригинальной [статье](https://proceedings.neurips.cc/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Paper.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "gamma = 2.0 # Kernel coefficient\n",
    "contamination = 0.05 # threshold\n",
    "\n",
    "model = OneClassSVM(gamma=gamma, kernel=\"rbf\", nu=contamination)\n",
    "model.fit(x_train)\n",
    "\n",
    "plot_outliers(x_train, x_test_normal, x_test_outliers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как на этих же данных работает алгоритм [IsolationForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html).\n",
    "\n",
    "IsolationForest состоит из деревьев, которые «изолируют» (пытаются отделить от остальной выборки) наблюдения, случайным образом выбирая признак и случайное значение порога для этого признака (между max и min значениями признака). Такой алгоритм чаще и проще отделяет значения аномалии. Если построить по такому принципу множество деревьев, то значения, которые чаще других отделяются раньше, будут аномалиями.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "n_estimators = 200\n",
    "contamination = 0.05 # threshold\n",
    "\n",
    "model = IsolationForest(\n",
    "    n_estimators=n_estimators, contamination=contamination, random_state=rng\n",
    ")\n",
    "model.fit(x_train)\n",
    "\n",
    "plot_outliers(x_train, x_test_normal, x_test_outliers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последним алгоритмом, на который мы посмотрим, будет [LocalOutlierFactor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html).\n",
    "\n",
    "В нем используется метод k-NN. Расстояние до ближайших соседей используется для оценки расположения точек. Если соседи далеко, то точка с большой вероятностью является аномалией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "n_neighbors = 10\n",
    "contamination = 0.05 # threshold\n",
    "\n",
    "model = LocalOutlierFactor(\n",
    "    n_neighbors=n_neighbors, novelty=True, contamination=contamination\n",
    ")\n",
    "model.fit(x_train)\n",
    "\n",
    "plot_outliers(x_train, x_test_normal, x_test_outliers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества в задаче обнаружения аномалий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отдельное время стоит посвятить подбору порога для алгоритма.\n",
    "\n",
    "Порог является граничным значением, выше которого данные считаются аномальными. Выбор порога это компромиссное решение: слишком низкий порог может привести к ложным срабатываниям, тогда как слишком высокий порог может пропустить аномалии. Поэтому выбор порога напрямую связан с задачей и требованиям к детекции.\n",
    "\n",
    "Построим матрицы ошибок (Confusion matrix) для разных порогов:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.concatenate((x_train, x_test_normal, x_test_outliers), axis=0)\n",
    "predicted = model.predict(full_data)\n",
    "\n",
    "# change threshold to 0.005\n",
    "model_2 = LocalOutlierFactor(\n",
    "    n_neighbors=10, novelty=True, contamination=0.005\n",
    ")\n",
    "model_2.fit(x_train)\n",
    "predicted_2 = model_2.predict(full_data)\n",
    "\n",
    "y_true = np.ones(full_data.shape[0]) # create true labels\n",
    "y_true[-x_test_outliers.shape[0]:] = -1 # last 20 is anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, predicted, ax=ax1, colorbar=False)\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, predicted_2, ax=ax2, colorbar=False)\n",
    "ax1.set_title('threshold = 0.05')\n",
    "ax2.set_title('threshold = 0.005')\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одна идея улучшения качества детекции аномалий это аггрегация результатов разных алгоритмов т.е создание ансамбля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация признаков\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Общая схема классического машинного обучения выглядит так. Даже в случае нейросетей некая предобработка исходных данных все равно не бывает лишней."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/data_preparation.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Генерация признаков** &mdash; процесс создания числовых представлений данных.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально объекты в нашем датасете могут быть представлены в виде описаний, которые не являются признаковыми или требуют некоторой предобработки. Например:\n",
    "*   веб-страницы;\n",
    "*   тексты;\n",
    "*   ссылки на участников группы;\n",
    "*   измерения в разных единицах (см, м, дм) и т.д.\n",
    "\n",
    "Большая часть моделей не способна работать с такими представлениями в сыром виде и просто не запустится или будет выдавать неадекватные результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс создания признаков зависит от модели, которую мы собираемся использовать. Для одних моделей полезно добавить признаки, полученные делением/перемножением исходных. Другие модели могут провести эти операции сами, причем, экономнее/менее переобучаясь. Добавление признаков, явно зависящих друг от друга, может даже мешать некоторым моделям.\n",
    "\n",
    "Например, плохая идея добавлять в обычную линейную модель как признаки X1 и X2, так и их сумму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/process_of_generation_features.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно понимать, что процесс feature engineering является критическим местом (bottleneck), в машинном обучении. Все, что модель будет знать о данных, решается на этом этапе. Мы можем как решить задачу составлением правильных признаков, так и допустить критические ошибки, которые буду вводить в заблуждение.\n",
    "\n",
    "\n",
    "Если в данных предоставить модели явную подсказку об ответе, то она будет использовать эту подсказку, а реальные закономерности может и не выучить.\n",
    "\n",
    "\n",
    "К примеру, признак id покупателя. Если есть покупатель, который каждую неделю покупает одно и то же, то модель выучит, что надо предсказывать все по признаку id. Тогда если покупатель изменит свое поведение или придет новый покупатель то модель может выдавать неадекватные предсказания.\n",
    "\n",
    "\n",
    "Точно такую же роль может сыграть информация о номере эксперимента, лаборатории, в которой его проводили, аспиранте, который его проводил и т.д.\n",
    "\n",
    "Такая ситуация будет называться **data leakage**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример на Titanic**\n",
    "\n",
    "Для иллюстрации будут использованы примеры из книги \"[Real-World Machine Learning](https://www.manning.com/books/real-world-machine-learning)\" из открытого [репозитория](https://github.com/brinkar/real-world-machine-learning) и датасет [Titanic](https://www.kaggle.com/datasets/brendan45774/test-file\n",
    ").\n",
    "\n",
    "Данный датасет представляет собой список пассажиров судна.\n",
    "В отличие от датасетов, с которыми мы работали раньше, данные в нем не предобработаны и в сыром виде не могут быть использованы для обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Download the data and save it in a variable called data\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/titanic.csv\"\n",
    ")  # Load the data using pandas\n",
    "dataset[:5]  # Show the first 5 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  SibSp  == Number of Siblings/Spouses Aboard — количество братьев/сестер/супругов на борту Титаника.\n",
    "*  Parch == Number of Parents/Children Aboard — количество родителей/детей на борту.\n",
    "*  Embarked == Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) — порт посадки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы попытаемся обучить модель на таких данных, то у нас ничего не выйдет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "x = dataset.drop(\"Survived\", axis=1)  # drop target\n",
    "y = dataset[\"Survived\"]  # target\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "try:\n",
    "    rf.fit(x, y)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на информацию о признаках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Традиционно признаки делятся на:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вещественные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вещественные признаки бывают:\n",
    "\n",
    " * дискретные. Например, число лайков от пользователей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/discrete_features_social_media_likes.jpg\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете Титаник таким параметром будет SibSp — количество братьев/сестер/супругов на борту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.SibSp[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или Parch — количество родителей/детей на борту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"Parch\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * непрерывные. Например, температура"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/continuous_features_thermometer.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделение условное. Тот же возраст можно посчитать и дискретной переменной (пользователь всегда нам сообщает свои полные года), и непрерывной (возраст можно считать с любой точностью)\n",
    "\n",
    "Иногда вещественные признаки делят на относительные (считаются относительно чего-то, уже нормированные и т.д.)  и интервальные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[[\"Age\", \"Fare\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить модель на вещественных признаках. PassengerId удалим, т.к. этот признак может приводить к утечке в данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.drop(columns=[\"Survived\", \"PassengerId\"])  # drop target and id\n",
    "y = dataset[\"Survived\"]  # target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# drop categorical\n",
    "x_train_working = x_train.drop(\n",
    "    columns=[\"Pclass\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\n",
    ")\n",
    "x_test_working = x_test.drop(\n",
    "    columns=[\"Pclass\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "try:\n",
    "    rf.fit(x_train_working, y_train)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "x_train_working.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема, что Age указан только для 714 пассажиров из 891. Его мы пока тоже удалим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train_working = x_train_working.drop(columns=[\"Age\"])\n",
    "x_test_working = x_test_working.drop(columns=[\"Age\"])\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf.fit(x_train_working, y_train)\n",
    "y_pred = rf.predict(x_test_working)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность предсказания 67.6%. Для улучшения качества попробуем добавить другие признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категориальные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение —  принадлежность к какой-то из категорий. Традиционно делятся на сильно отличающиеся по свойствам:\n",
    " * упорядоченные (ординальные) — имееют порядок (иерархию), для каждой пары категорий можем сказать, какая больше, а какая меньше. Например, класс места или размер одежды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/categorical_ordered_features.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете Титаник таким признаком будет класс. Мы можем сказать, что первый класс лучше третьего, но не можем  сказать, что сумма первого и второго даст третий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"Pclass\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * неупорядоченные (номинальные) &mdash; категории между собой несравнимы. Обычно нельзя сказать, что желтая футболка больше синей или зеленая лучше чем фиолетовая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/categorical_unordered_features.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Однако, важно понимать, что могут быть ситуации, когда один и тот же признак может быть номинальным или ординарным, в зависимости от задачи и наших знаний о ней. Например, цвет в световом спектре опредяелся длиной волны, и уже имеет понятное сравнение.\n",
    "\n",
    "\n",
    "В датасете Титаник таким признаком является Embarked (порт посадки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"Embarked\"].unique())\n",
    "# C = Cherbourg; Q = Queenstown; S = Southampton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто мы сталкиваемся с бинарными категориальными признаками, для которых известно только две возможных категории (например, биологический пол человека)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"Sex\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразования признаков\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует много подходов кодирования признакового пространства. Каждый подход имеет свои особенности. Способ кодирования зависит от используемой модели.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категориальных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Label encoding каждой категории признака однозначно сопоставляется число. Данный подход хорошо работает для упорядоченных (ординальных) признаков.\n",
    "\n",
    "Если признак неупорядоченный (номинальный), то могут возникнуть проблемы.\n",
    "Например, если мы обозначим:\n",
    "$$Уж = 1$$\n",
    "$$Ёж = 2$$\n",
    "$$Белка = 3$$\n",
    "То в некоторых моделях (logreg, svm и линейных), может возникнуть ложная зависимость:\n",
    "\n",
    "$$Уж+Ёж = Белка$$\n",
    "что не является свойством данных.\n",
    "\n",
    "\n",
    "Кроме того, мы не можем сказать, что уж “больше” ежа и сравнить его с белкой, но обучаемая модель про это не знает и будет пытаться их сравнить. Это может привести к низкому качеству модели и выучиванию неправильной информации.\n",
    "\n",
    "Например, дерево решений для выделения одной категории должно будет проделать несколько сравнений, что может не произойти в силу жадности алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые модели (например, [LightGBM](https://lightgbm.readthedocs.io/en/latest/)) могут автоматически подбирать кодировку для категориальных признаков, если предоставить им информацию, что признак категориальный, для других моделей это нужно делать вручную.\n",
    "\n",
    "У нас есть упорядоченный категориальный признак — класс, которым ехал пассажир. Добавим его к данным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_working[\"Pclass\"] = x_train[\"Pclass\"]\n",
    "x_test_working[\"Pclass\"] = x_test[\"Pclass\"]\n",
    "\n",
    "x_train_working[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике часто используется one-hot encoding. Вместо одного категориального признака $X$ создается набор бинарных категориальных признаков, которые отвечают на вопрос $X == C$, где $C$ перебирает все возможные значения категориального признака.\n",
    "\n",
    "Теперь, чтобы выбрать конкретное значение категориального признака, дереву решений достаточно задать один вопрос."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/one_hot_encoding.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У такой схемы есть ряд недостатков:\n",
    "\n",
    "1. Мы получаем линейно зависимые признаки. Это может плохо влиять на некоторые модели. Например, в линейных моделях линейная зависимость признаков приводит к тому, что решение оптимизационной задачи (результат подбора весов) может быть не уникальным и сколь угодно большим по модулю, что негативно сказывается на работе модели. Подробнее об этом можно почитать по [ссылке](https://inmachineswetrust.com/posts/drop-first-columns/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/problem_of_ohe.png\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому одну из категорий могут исключить при кодировании. Например, в примере выше можно исключить **Рыбу**, ведь если все три других признака-категории равны 0, то точно верно, что категория — **Рыба**.\n",
    "\n",
    "2. При использовании one-hot encoding один категориальный признак может преобразовываться в десятки бинарных признаков. При использовании случайного леса выбирается случайное подмножество признаков. Преобразованные во множество бинарных, категориальные признаки будут встречаться чаще, чем вещественные, что может привести к тому, что значимость категориальных признаков будет завышена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть два признака с ограниченным количеством значений: Sex и Embarked.\n",
    "\n",
    "Пол закодируем male = 1 female = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex = {\"male\": 1, \"female\": 0}\n",
    "\n",
    "x_train_working[\"Sex\"] = x_train[\"Sex\"].map(sex)\n",
    "x_test_working[\"Sex\"] = x_test[\"Sex\"].map(sex)\n",
    "\n",
    "x_train_working[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один способ, который показал свою эффективность — это кодирование категориальной переменной по количеству встречаемости в данных.\n",
    "\n",
    "\n",
    "|  category | new_feature |\n",
    "|:---------:|:-----------:|\n",
    "|    food   |      3     |\n",
    "| equipment |      2     |\n",
    "|    food   |      3     |\n",
    "|  food     |      3     |\n",
    "| equipment |      2     |\n",
    "|  clothes  |      1     |\n",
    "\n",
    "Его просто и быстро реализовать. Здесь также присутствует логика порядка, который возникает естественным образом. Чем больше число, тем чаще встречается категория. Также здесь отсутствует ложная линейная зависимость, в нашем случае `equipment + clothes = food` и для нас это действительно так, потому что наше кодирование отражает связь между категориями через их частоту.\n",
    "\n",
    "Дополнительно стоит добавить шум к данным, чтобы избежать ситуации, когда разные категории кодируются одинаковым числом.\n",
    "\n",
    "|  category | new_feature |\n",
    "|:---------:|:-----------:|\n",
    "|    food   |      2.3     |\n",
    "| equipment |      2.04  |\n",
    "|    food   |      2.3     |\n",
    "|  clothes  |      2.6     |\n",
    "| equipment |      2.04     |\n",
    "|  clothes  |      2.6     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование по вещественному признаку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем кодировать категориальный признак по какой-то статистике вещественного.\n",
    "Например, при прогнозировании покупок в интернет магазине может оказаться разумным кодировать категории товаров их средней ценой.\n",
    "\n",
    "|  category |  product | price | new_feature |\n",
    "|:---------:|:--------:|:-----:|:-----------:|\n",
    "|    food   |   pizza  |   30  |      17     |\n",
    "| equipment |  hammer  |  140  |     170     |\n",
    "|    food   | cucumber |   4   |      17     |\n",
    "|  clothes  |   boots  |  100  |      60     |\n",
    "| equipment |  helmet  |  200  |     170     |\n",
    "|  clothes  |  gloves  |   20  |      60     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Target encoding каждая категория кодируется численным параметром, характеризующим то, что мы предсказываем. Например, можно каждую категорию категориального признака заменять на среднее целевого значения (mean target).\n",
    "\n",
    "При этом может возникнуть проблема переобучения: для редких классов модель может научиться копировать значение mean target категориального признака в ответ, игнорируя другие признаки. Как с этим борются можно посмотреть [здесь](https://github.com/Dyakonov/PZAD/blob/master/2020/PZAD2020_042featureengineering_07.pdf) или [здесь](https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как у нас в качестве модели используется случайный лес, для Embarked будем использовать Label encoding. Мы не будем считать среднее. Используем Target, чтобы упорядочить метки. Посмотрим, какой процент выживших для каждого порта. Смотреть будем только на train выборке. Информация из test выборки не должна попасть в модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "train_df = x_train.copy()\n",
    "train_df[\"Survived\"] = y_train\n",
    "sns.barplot(x=\"Embarked\", y=\"Survived\", data=train_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили количество выживших $S<Q<C$. Упорядочим метки соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "emb = {np.nan: 0, \"S\": 0, \"Q\": 1, \"C\": 2}\n",
    "\n",
    "x_train_working[\"Embarked\"] = x_train[\"Embarked\"].map(emb)\n",
    "x_test_working[\"Embarked\"] = x_test[\"Embarked\"].map(emb)\n",
    "\n",
    "x_train_working[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас осталось еще 4 признака, с которыми непонятно, что делать: Name, Ticket, Cabin, Age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно научить вашу модель саму сопоставлять каждой категории некий вектор определенной размерности. Для этого вначале сопоставляем каждой категории случайный вектор заданной длины. А далее изменяем этот вектор как обычные веса. Подробнее о формирование векторов-признаков (embedding) будет рассказано в других лекциях курса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/embedding.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим поле Name. Это поле может нести информацию о поле, социальном статусе, происхождении, национальности, возрасте и т.д. Можно построить модели, которые будут это оценивать и отражать. Мы будем работать с числовыми представлениями текста в нашем курсе. Но пока мы можем использовать метод \"пристального вглядывания\" в данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.Name[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В именах есть информация о социальном статусе:\n",
    "- Miss — незамужняя женщина,\n",
    "- Mrs — замужняя женщина,\n",
    "- Master — несовершеннолетний мужчина,\n",
    "- Mr — совершеннолетний мужчина,\n",
    "- Dr — доктор,\n",
    "- Rev — преподобный,\n",
    "- Capt — капитан,\n",
    "\n",
    "и т.д.\n",
    "\n",
    "Первые 4 встречаются чаще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечем все возможные титулы, они начинаются с пробела и заканчиваются точкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = dataset.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False).unique()\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгруппируем некоторые с похожими значениями. Редкие запишем в нулевой класс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = {\n",
    "    None: 0,\n",
    "    \"Sir\": 0,\n",
    "    \"Countess\": 0,\n",
    "    \"Don\": 0,\n",
    "    \"Jonkheer\": 0,\n",
    "    \"Lady\": 0,\n",
    "    \"Capt\": 0,\n",
    "    \"Ms\": 0,\n",
    "    \"Mme\": 0,\n",
    "    \"Mlle\": 0,\n",
    "    \"Col\": 0,\n",
    "    \"Major\": 0,\n",
    "    \"Rev\": 0,\n",
    "    \"Dr\": 0,\n",
    "    \"Master\": 1,\n",
    "    \"Mrs\": 2,\n",
    "    \"Miss\": 3,\n",
    "    \"Mr\": 4,\n",
    "}\n",
    "\n",
    "x_train_working[\"Title\"] = x_train.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False).map(\n",
    "    titles\n",
    ")\n",
    "x_test_working[\"Title\"] = x_test.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False).map(\n",
    "    titles\n",
    ")\n",
    "\n",
    "x_train_working[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование циклических категориальных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При работе с датой и временем мы можем представить дату и время в виде числа. Один из способов такого представления [Unix Timestamp](https://www.unixtimestamp.com/) (количество секунд, прошедших с 1 января 1970-го года).\n",
    "Для ряда задач важна цикличность времени. Например, загруженность линии метро будет зависеть от времени дня (цикл 24 часа), дня недели (цикл 7 дней) и нерабочих праздничных дней (цикл год). Для прогнозирования количества электроэнергии, выработанной солнечной батареей, важно будет время дня (цикл 24 часа) и время года (цикл год).\n",
    "\n",
    "В случае с такими признаками, как день недели или время суток, мы сталкиваемся с проблемой того, что нам нужно предложить кодирование, которое будет учитывать, что понедельник близок к воскресенью так же, как и ко вторнику, и т.д.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Давайте нанесем наши категории, например, дни недели, на окружность. Как это сделать?\n",
    "Пусть понедельнику соответствует 1, а воскресенью — 7. Далее посчитаем два таких вспомогательных признака по следующим формулам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekdays = np.arange(1, 8)  # create an array of weekdays\n",
    "print(weekdays)\n",
    "sina = np.sin(weekdays * np.pi * 2 / np.max(weekdays))  # feature 1\n",
    "cosa = np.cos(weekdays * np.pi * 2 / np.max(weekdays))  # feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))  # Decide figure size\n",
    "plt.scatter(sina, cosa)  # Plot scatter of feature 1 vs feature 2\n",
    "for i, z in enumerate(\n",
    "    (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n",
    "):  # for each day in a week\n",
    "    plt.text(sina[i], cosa[i], s=z)  # add text labels to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь расстояния между понедельником и вторником и воскресеньем и понедеьником одинаковые:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mon_tue = (sina[1] - sina[0]) ** 2 + (\n",
    "    cosa[1] - cosa[0]\n",
    ") ** 2  # distance between Monday and Tuesday\n",
    "dist_sun_mon = (sina[6] - sina[0]) ** 2 + (\n",
    "    cosa[6] - cosa[0]\n",
    ") ** 2  # distance between Sunday and Monday\n",
    "print(\"Distance between Mon-Tue = %.2f\" % dist_mon_tue)\n",
    "print(\"Distance between Sun-Mon = %.2f\" % dist_sun_mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое верно и для любых отстоящих друг от друга на одинаковое число дней\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mon_wed = (sina[2] - sina[0]) ** 2 + (\n",
    "    cosa[2] - cosa[0]\n",
    ") ** 2  # distance between Monday and Wednesday\n",
    "dist_fri_sun = (sina[4] - sina[6]) ** 2 + (\n",
    "    cosa[4] - cosa[6]\n",
    ") ** 2  # distance between Friday and Sunday\n",
    "print(\"Distance between Mon-Wed = %.2f\" % dist_mon_wed)\n",
    "print(\"Distance between Fri-Sun = %.2f\" % dist_fri_sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, циклические признаки можно кодировать парой признаков (sin и cos), полученных по схеме, описанной выше.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемы подхода:\n",
    "\n",
    "1. Такое кодирование может создать сложности для деревьев решений т.к. они работают с одним признаком за раз.\n",
    "\n",
    "2. Надо понимать, что важность исходной категориальной фичи неочевидным образом делится между двумя полученными из нее таким образом фичами.\n",
    "\n",
    "3. В некоторых задачах one-hot работает лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вещественных признаков\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ряда задач может быть неважно конкретное значение признака. Важнее может оказаться факт превышения порога или наличия значения.\n",
    "\n",
    "Например, уровень сахара крови выше $11.1$ ммоль/л может говорить о наличии у пациента сахарного диабета, что повлияет на результат лечения. А наличие высшего образования больше влияет на платежеспособность, чем средний балл диплома.\n",
    "\n",
    "Для таких признаков можно попробовать использовать бинаризацию: превращение вещественного признака в бинарный по принципу “есть ли значение” или “больше ли значение определенного порога”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинаризируем уровень сахара в крови."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "# fmt: off\n",
    "x = np.array([[1, 12],\n",
    "              [2, 7.6],\n",
    "              [3, 8.4],\n",
    "              [4, 13.5],\n",
    "              [5, 6.3]])\n",
    "# fmt: on\n",
    "\n",
    "transformer = Binarizer(threshold=11.1)\n",
    "binarized = transformer.transform(np.expand_dims(x[:, 1], axis=1))\n",
    "\n",
    "x_binarized = np.concatenate((x, binarized), axis=1)\n",
    "\n",
    "print(x_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning (Бинирование)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам могут быть не интересны точные значения (например, что видео набрало 1000 лайков, а не 1001).\n",
    "\n",
    "К тому же, число просмотров/лайков некоторых видео может быть очень большим в сравнении с остальными.\n",
    "В итоге часть значений у нас встречается часто, а часть — очень редко, что\n",
    "негативно скажется на результате работы модели.\n",
    "\n",
    "Бинирование — это метод группировки вещественных признаков в несколько категорий, определяемых диапазонами значений. При этом категория может кодироваться средним или медианным значением признака в диапазоне данной категории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed-width binning\n",
    "\n",
    "Делим наши значения по диапазонам фиксированной длины. Так часто поступают с возрастом.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/fixed_width_binning.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binning by Instinct\n",
    "Длина диапазона не всегда обязана быть кратна определенному значению, например 10 годам. В социальных исследованиях может быть полезным разделение на возрастные группы, которые определяются занятостью: школьники, студенты, выпускники, пенсионеры и т.д. Бинирование на основе личного понимания данных называют Binning by Instinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaptive Binning\n",
    "\n",
    "Binning с фиксированной длиной бина или с использованием личного понимания данных не всегда работает хорошо. Полезно визуализировать результат разбиения.\n",
    "Например, рассмотрим распределение дохода разработчиков. Оно сильно скошено вправо.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/adaptive_binning.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинирование с фиксированной длиной бина не поможет справиться с редкими значениями.\n",
    "\n",
    "В этой ситуации помогает бинирование, например, по квартилям — когда границы бина расставляются таким образом, чтобы между ними помещалась $1/4$ выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/quantiles_binning.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логарифмирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С ситуацией, когда распределение скошено вправо, работает и другой подход: прологаримфировать величину.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/log_binning.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обобщением этого подхода является [Box-Cox Transform](https://www.statisticshowto.com/box-cox-transformation/#:~:text=A%20Box%20Cox%20transformation%20is,a%20broader%20number%20of%20tests.), общей целью которой является придать данным вид, более похожий на нормальное распределение, с которым работает бoльшее число моделей и сходимость которого лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с пропущенными значениями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L03/out/decision_tree_and_nan_values_in_data.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Со всеми признаками производятся математические операции, поэтому для большинства моделей (деревья решений могут работать с пропущенными значениями, [пример](https://stats.stackexchange.com/questions/171574/meaning-of-surrogate-split)), отсутствие значений просто не дает производить эти операции.\n",
    "\n",
    "Есть два основных подхода для работы с пропущенными значениями:\n",
    "\n",
    " 1. Удалить все объекты с пропущенными значениями.\n",
    "\n",
    " 2. На место пропущенных значений записать какие-то числа (заполнить пропущенные значения)\n",
    "\n",
    "Иногда мы можем просто удалить объекты с пропущенными значениями, но если у нас мало данных и получение большего количества данных дорого/долго/невозможно, то нам нужно заполнить пропуски какими-то значениями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует много способов заполнения пропусков значениями.\n",
    "\n",
    "Рассмотрим наиболее популярные:\n",
    "\n",
    "* **Заполнить константой.** Можно предположить, что данные несложные, а признаки не зависят друг от друга, и заполнить пропущенные значения средними значениями соответствующего признака.\n",
    "Например, если средний возраст людей 30 лет, то для людей, чей возраст неизвестен, будем писать это же значение. В некоторых ситуациях, такое заполнение может только навредить\n",
    "\n",
    "* **Предсказываем признаки моделью.** Используем признак с пропущенными значениями как целевую переменную и на основе других заполненных признаков предсказываем пропущенные значения. Можем использовать алгоритм, который умеет справляться с пропусками. Один из таких алгоритмов — k-NN. Можно брать ближайших соседей по известным признакам и на место неизвестного признака поставить среднее значение соседей.\n",
    "\n",
    "* **Сами придумываем логику заполнения.** Если мы имеем экспертное знание о природе пропусков, можем сами написать логику заполнения какими-то значениями.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете Titanic отстался важный признак Age, который имеет пропуски. Посмотрим, как он связан с выживаемостью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "train_df = x_train.copy()\n",
    "train_df[\"Survived\"] = y_train\n",
    "\n",
    "women = train_df[train_df[\"Sex\"] == \"female\"]\n",
    "men = train_df[train_df[\"Sex\"] == \"male\"]\n",
    "\n",
    "ax = sns.histplot(\n",
    "    women[women[\"Survived\"] == 1].Age.dropna(),\n",
    "    bins=18,\n",
    "    label=\"survived\",\n",
    "    ax=axes[0],\n",
    "    kde=False,\n",
    "    color=\"#27a9e1\",\n",
    "    linewidth=0,\n",
    ")\n",
    "ax = sns.histplot(\n",
    "    women[women[\"Survived\"] == 0].Age.dropna(),\n",
    "    bins=40,\n",
    "    label=\"not survived\",\n",
    "    ax=axes[0],\n",
    "    kde=False,\n",
    "    color=\"#ffab40\",\n",
    "    linewidth=0,\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title(\"Female\")\n",
    "\n",
    "ax = sns.histplot(\n",
    "    men[men[\"Survived\"] == 1].Age.dropna(),\n",
    "    bins=18,\n",
    "    label=\"survived\",\n",
    "    ax=axes[1],\n",
    "    kde=False,\n",
    "    color=\"#27a9e1\",\n",
    "    linewidth=0,\n",
    ")\n",
    "ax = sns.histplot(\n",
    "    men[men[\"Survived\"] == 0].Age.dropna(),\n",
    "    bins=40,\n",
    "    label=\"not survived\",\n",
    "    ax=axes[1],\n",
    "    kde=False,\n",
    "    color=\"#ffab40\",\n",
    "    linewidth=0,\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title(\"Male\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что некоторая связь есть. Обучим модель без этого признака и посмотрим на результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_working.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf.fit(x_train_working, y_train)\n",
    "y_pred = rf.predict(x_test_working)\n",
    "\n",
    "results['no feature'] = accuracy_score(y_test, y_pred).round(3)\n",
    "\n",
    "print(f\"Accuracy without feature 'Age': {results['no feature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним пропуски своей эвристикой. Вспомним, что в поле Title зашита общая информация о возрасте, посчитаем матожидание и дисперсию возраста для Title и сгенерируем недостающие значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_df = x_train.copy()\n",
    "train_df[\"Title\"] = x_train_working[\"Title\"]\n",
    "\n",
    "mean = {}\n",
    "std = {}\n",
    "for title in range(5):\n",
    "    data = train_df.loc[train_df[\"Title\"] == title]\n",
    "    mean[title] = data[\"Age\"].mean()\n",
    "    std[title] = data[\"Age\"].std()\n",
    "\n",
    "\n",
    "def add_age_val(data, mean, std):\n",
    "    for i, row in data.iterrows():\n",
    "        if np.isnan(row[\"Age\"]):\n",
    "            title = int(row[\"Title\"])\n",
    "            data.loc[i, \"Age\"] = round(\n",
    "                np.random.uniform(\n",
    "                    low=int(mean[title] - std[title]),\n",
    "                    high=int(mean[title] + std[title]),\n",
    "                ),\n",
    "                1,\n",
    "            )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_working[\"Age\"] = x_train[\"Age\"]\n",
    "x_test_working[\"Age\"] = x_test[\"Age\"]\n",
    "\n",
    "x_train_working = add_age_val(x_train_working, mean, std)\n",
    "x_test_working = add_age_val(x_test_working, mean, std)\n",
    "\n",
    "x_train_working[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось на обработанных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf.fit(x_train_working, y_train)\n",
    "y_pred = rf.predict(x_test_working)\n",
    "\n",
    "results['by title'] = accuracy_score(y_test, y_pred).round(3)\n",
    "\n",
    "print(f\"Accuracy with hand-filled feature 'Age': {results['by title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало лучше. Воспользуемся [`IterativeImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) из [`sklearn.impute`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute).\n",
    "\n",
    "`IterativeImputer` автоматически заполняет пропущенные значения признаков, оценивая их при помощи модели. По умолчанию используется модель [`BayesianRidge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_working['Age'] = x_train['Age']\n",
    "x_test_working[\"Age\"] = x_test[\"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "imputer.fit(x_train_working)\n",
    "x_train_working = imputer.transform(x_train_working)\n",
    "x_test_working = imputer.transform(x_test_working)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf.fit(x_train_working, y_train)\n",
    "y_pred = rf.predict(x_test_working)\n",
    "\n",
    "results['imputer'] = accuracy_score(y_test, y_pred).round(3)\n",
    "\n",
    "print(f\"Accuracy with feature 'Age' filled with IterativeImputer: {results['imputer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним результаты при разном заполнении пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([results], index=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае [`IterativeImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) дал наилучший результат. Хорошая практика использовать данный метод как быстрый бейзлайн по заполнению пропусков.\n",
    "\n",
    "Важно помнить, что пропуски в данных не всегда являются отсутствием значений, иногда пропуски могут быть обозначены совершенно разными способами: `0`, `-`, `(пусто)`, `'отсутствует'`, `None`, `nan`, и другие. Поэтому нужно приводить пропуски к единому формату, с которым удобно работать в дальнейшем, например `np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование взаимодействия признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки могут по-разному взаимодейстовать, и некоторые модели не могут моделировать это взаимодействие.\n",
    "\n",
    "Посмотрим это на игрушечном примере. Сгенерируем данные и попробуем решить задачу классификации с помощью линейной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x, y = make_circles(n_samples=400, factor=0.3, noise=0.05, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "violet = y == 0\n",
    "yellow = y == 1\n",
    "\n",
    "plt.scatter(x[violet, 0], x[violet, 1], c=\"blueviolet\", s=20, edgecolor=\"k\")\n",
    "plt.scatter(x[yellow, 0], x[yellow, 1], c=\"yellow\", s=20, edgecolor=\"k\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=20)\n",
    "plt.ylabel(\"$x_2$\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что тут нельзя провести плоскость, которая хорошо разделит наши классы.\n",
    "\n",
    "Обучим модель и оценим точность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# We make a 80/20% train/test split of the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Accuracy of the model = %.2f\" % model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понимание взаимодействия признаков может упростить решаемую задачу. Например, введение нового признака $z= x_1^2+x_2^2$ для наших данных позволит линейной модели решить задачу классификации. Добавим этот признак:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(x, columns=[\"x_1\", \"x_2\"])\n",
    "df[\"z\"] = x[:, 0] ** 2 + x[:, 1] ** 2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель с новым признаком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a 80/20% train/test split of the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.values, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Accuracy of the model = %.2f\" % model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, наша линейная модель теперь полностью решает задачу. Визуализируем данные с новым признаком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y, total_len=400, s=50, threshold=21.5):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.scatter(xs=x[:, 0], ys=x[:, 1], zs=x[:, 2], c=y, s=s)\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    ax.plot_surface(XX, YY, XX * YY * 0.2, alpha=0.2)\n",
    "    ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\", zlabel=\"$z$\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "total_len = 400\n",
    "\n",
    "ax = plot_data(df.values, y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Могут быть и более высокоуровневые взаимодействия, когда взаимодействуют много разных признаков.\n",
    "\n",
    "Взаимодействия могут быть самые разные — много способов кодировать. Например, добавлять в число признаков их произведение, возводить в степень, брать логарифм и множество других вариаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практический пример работы с признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обработать данные по-другому."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/titanic.csv\"\n",
    ")  # Load the data using pandas\n",
    "dataset[:5]  # Show the first 5 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Часть полей можно исключить (имя);\n",
    "\n",
    "2. Часть можно преобразовать в числа (пол, порт посадки ...);\n",
    "\n",
    "3. Непрерывные данные можно нормировать (здесь вместо этого берется квадратный корень из цены);\n",
    "\n",
    "4. На основании некоторых можно создать новые, более полезные для модели (номер кабины).\n",
    "\n",
    "\n",
    "cabin_data = array([\"C65\", \"\", \"E36\", \"C54\", \"B57 B59 B63 B66\"])\n",
    "->\n",
    "[['C', 65, 1], ['X', -1, 0], ['E', 36, 1], ['C', 54, 1], ['B', 57, 4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The categorical-to-numerical function\n",
    "# Changed to automatically add column names\n",
    "def cat_to_num(data):  # one-hot encoding\n",
    "    categories = set(data)\n",
    "    features = {}\n",
    "    for cat in categories:\n",
    "        binary = data == cat\n",
    "        if len(set(binary)) == 1:\n",
    "            # Ignore features where all values equal\n",
    "            continue\n",
    "        new_key = f\"{data.name}={cat}\"\n",
    "\n",
    "        features[new_key] = binary.astype(\"int\")\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def cabin_features(data):\n",
    "    features = []\n",
    "    for cabin in data:\n",
    "        cabins = str(cabin).split(\" \")\n",
    "        n_cabins = len(cabins)\n",
    "        # First char is the cabin_char\n",
    "        try:\n",
    "            cabin_char = cabins[0][0]\n",
    "        except IndexError:\n",
    "            cabin_char = \"X\"\n",
    "            n_cabins = 0\n",
    "        # The rest is the cabin number\n",
    "        try:\n",
    "            cabin_num = int(cabins[0][1:])\n",
    "        except:\n",
    "            cabin_num = -1\n",
    "        # Add 3 features for each passanger\n",
    "        features.append([cabin_char, cabin_num, n_cabins])\n",
    "    features = np.array(features)\n",
    "    dic_of_features = {\n",
    "        \"Cabin_num\": features[:, 1].astype(\"int\"),\n",
    "        \"N_cabins\": features[:, 2].astype(\"int\"),\n",
    "    }\n",
    "    out = pd.DataFrame(dic_of_features)\n",
    "    char_column = pd.DataFrame({\"Cabin_char\": features[:, 0]})\n",
    "    cabin_ch = cat_to_num(char_column[\"Cabin_char\"])\n",
    "    return out.join(cabin_ch)\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"Takes a dataframe of raw data and returns ML model features\"\"\"\n",
    "\n",
    "    # Initially, we build a model only on the available numerical values\n",
    "    features = data.drop(\n",
    "        [\n",
    "            \"PassengerId\",\n",
    "            \"Survived\",\n",
    "            \"Fare\",\n",
    "            \"Name\",\n",
    "            \"Sex\",\n",
    "            \"Ticket\",\n",
    "            \"Cabin\",\n",
    "            \"Embarked\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Setting missing age values to -1\n",
    "    features[\"Age\"] = data[\"Age\"].fillna(-1)\n",
    "\n",
    "    # Adding the sqrt of the fare feature\n",
    "    features[\"sqrt_Fare\"] = np.sqrt(data[\"Fare\"])\n",
    "\n",
    "    # Adding gender categorical value\n",
    "    features = features.join(cat_to_num(data[\"Sex\"]))\n",
    "\n",
    "    # Adding Embarked categorical value\n",
    "    features = features.join(cat_to_num(data[\"Embarked\"]))\n",
    "\n",
    "    # Split cabin\n",
    "    features = features.join(cabin_features(data[\"Cabin\"]))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "features = prepare_data(dataset)  # Create variable features\n",
    "features[:5]  # Display first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь модель можно обучать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a 80/20% train/test split of the data\n",
    "features = prepare_data(dataset)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, dataset[\"Survived\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Accuracy of the model = %.2f\" % model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первый раз мы обработали признаки лучше. Вероятно, это связанно с оценкой Age через Title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для целей предварительной обработки признаков существует множество инструментов, в том числе модуль `preprocessing` в пакете `sklearn`.\n",
    "\n",
    "Аналогичные подмодули или целые библиотеки есть и для разных задач, связанных с нейронными сетями (`torchvision`, `torchaudio` и прочее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Make one shot encoded representation\n",
    "one_hot_encoder = make_column_transformer(\n",
    "    (\n",
    "        OneHotEncoder(\n",
    "            sparse_output=False,  # if False return array, if True return sparse matrix\n",
    "            handle_unknown=\"ignore\",\n",
    "        ),  #  ignore if an unknown categorical feature is present during transform\n",
    "        make_column_selector(dtype_include=\"category\"),\n",
    "    ),  # selection of dtypes to include\n",
    "    remainder=\"passthrough\",\n",
    ")  # all columns that were not specified in transformers will be  passed through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing features using sklearn.preprocessing\n",
    "features = dataset.drop(\n",
    "    [\"PassengerId\", \"Survived\", \"Fare\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"],\n",
    "    axis=1,\n",
    ")\n",
    "# make Cabin features, examples: None -> 'X', C85 -> 'C', B42 -> 'B'\n",
    "features[\"Cabin\"] = (\n",
    "    dataset[\"Cabin\"].fillna(\"X\").apply(lambda x: x[0]).astype(\"category\")\n",
    ")\n",
    "\n",
    "\n",
    "def get_cab_num(cab):\n",
    "    try:\n",
    "        return int(cab.split()[0][1:])  # get cabin num (C85 -> 85)\n",
    "    except:\n",
    "        return -1  # if dont know num, return -1 (X -> -1)\n",
    "\n",
    "\n",
    "features[\"Cabin_num\"] = (\n",
    "    dataset[\"Cabin\"].fillna(\"X\").apply(lambda x: get_cab_num(x))\n",
    ")  # get cabin num\n",
    "\n",
    "features[\"N_cabins\"] = (\n",
    "    dataset[\"Cabin\"].fillna(\"X\").str.split(\" \").apply(lambda x: len(x))\n",
    ")  # num of cabins (C23 C25 C27 -> 3)\n",
    "\n",
    "features[\"Sex\"] = dataset[\"Sex\"].astype(\"category\")  # male/female\n",
    "\n",
    "features[\"Embarked\"] = (\n",
    "    dataset[\"Embarked\"].fillna(\"X\").astype(\"category\")\n",
    ")  # Categories: ['C', 'Q', 'S', 'X']\n",
    "features[\"sqrt_Fare\"] = np.sqrt(dataset[\"Fare\"])  # normalize by sqrt\n",
    "features[\"Age\"] = dataset[\"Age\"].fillna(-1)  # NaN -> -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20% train/test split of the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, dataset[\"Survived\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder.fit(x_train)  # fit one-hot encoder to x_train\n",
    "x_train_ohe = one_hot_encoder.transform(\n",
    "    x_train\n",
    ")  # transform x_train with the one-hot encoder\n",
    "x_test_ohe = one_hot_encoder.transform(\n",
    "    x_test\n",
    ")  # transform x_test with the one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)  # specify maximum iterations\n",
    "model.fit(x_train_ohe, y_train)  # fit model to the training data\n",
    "\n",
    "# Make predictions\n",
    "print(\n",
    "    \"Accuracy of the model = %.2f\" % model.score(x_test_ohe, y_test)\n",
    ")  # calculate the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление в модель признаков, полученных на основе другой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/generating_features_using_model.png\" width=\"700\">\n",
    "\n",
    "<em>**Генерация бинарного признакового пространства с помощью RandomForest**</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример, в котором для кодирования значений признаков используется результат работы ансамбля деревьев, а затем на закодированных данных обучается линейная модель.\n",
    "\n",
    "В этом примере сначала на отдельной выборке обучается ансамбль деревьев (случайный лес и градиентный бустинг), и каждому листу каждого дерева в ансамбле присваивается уникальный индекс.\n",
    "\n",
    "Для кодирования с помощью обученного ансамбля каждый объект проходит по всем деревьям. В каждом дереве он оказывается в одном из листьев и получает в качестве нового признака индекс этого листа. Таким образом создается новое пространство признаков. После этого новые признаки (индексы листьев) кодируются по принципу one-hot ecnoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала создадим датасет и разобьем его на три отдельные части:\n",
    "* часть для обучения ансамбля деревьев;\n",
    "* часть для обучения линейной модели;\n",
    "* часть для тестирования линейной модели.\n",
    "\n",
    "Важно обучать ансамбль деревьев на ином подмножестве обучающих данных, чем модель линейной регрессии, чтобы избежать переобучения, в частности, если общее количество листьев окажется равно количеству обучающих образцов или близко к нему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# define dummy dataset\n",
    "x, y = make_classification(n_samples=80000, random_state=42)\n",
    "\n",
    "# split dataset into subsets for training ensemble and linear model and final testing of the linear model\n",
    "x_full_train, x_test, y_full_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# split training subset into parts for ensemble training and for linear model training\n",
    "x_train_ensemble, x_train_linear, y_train_ensemble, y_train_linear = train_test_split(\n",
    "    x_full_train, y_full_train, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем обучить ансабли на специально отложенных для этого данных, а затем применить их для кодирования обучающей и тестовой выборок для линейной модели. На закодированных данных мы обучаем модель логистической регрессии, а затем строим ROC-кривые, чтобы сравнить качество разных подходов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "n_estimator = 10\n",
    "\n",
    "# Supervised transformation based on random forests\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator, random_state=42)\n",
    "rf_enc = OneHotEncoder()\n",
    "rf_lm = LogisticRegression(max_iter=1000)\n",
    "\n",
    "rf.fit(x_train_ensemble, y_train_ensemble)\n",
    "rf_enc.fit(rf.apply(x_train_ensemble))  # apply method return leaf indices\n",
    "rf_lm.fit(rf_enc.transform(rf.apply(x_train_linear)), y_train_linear)\n",
    "\n",
    "y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(x_test)))[:, 1]\n",
    "fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n",
    "\n",
    "\n",
    "# Supervised transformation based on gradient boosted trees\n",
    "grd = GradientBoostingClassifier(n_estimators=n_estimator, random_state=42)\n",
    "grd_enc = OneHotEncoder()\n",
    "grd_lm = LogisticRegression(max_iter=1000)\n",
    "\n",
    "grd.fit(x_train_ensemble, y_train_ensemble)\n",
    "grd_enc.fit(grd.apply(x_train_ensemble)[:, :, 0])  # apply method return leaf indices\n",
    "grd_lm.fit(grd_enc.transform(grd.apply(x_train_linear)[:, :, 0]), y_train_linear)\n",
    "\n",
    "y_pred_grd_lm = grd_lm.predict_proba(grd_enc.transform(grd.apply(x_test)[:, :, 0]))[\n",
    "    :, 1\n",
    "]\n",
    "fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также для сравнения протестируем обученные ансамбли на той же тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random forest model by itself\n",
    "y_pred_rf = rf.predict_proba(x_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "\n",
    "# The gradient boosted model by itself\n",
    "y_pred_grd = grd.predict_proba(x_test)[:, 1]\n",
    "fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим ROC-кривые для четырех моделей:\n",
    "* случайный лес, обученный на исходных данных;\n",
    "* логистическая регрессия, обученная на данных, закодированных с помощью случайного леса;\n",
    "* градиентный бустинг, обученный на исходных данных;\n",
    "* логистическая регрессия, обученная на данных, закодированных с помощью градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figure 1 and figure 2 with subplots\n",
    "fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k--\")\n",
    "ax1.plot(fpr_rf, tpr_rf, label=\"RF\")\n",
    "ax1.plot(fpr_rf_lm, tpr_rf_lm, label=\"RF + LR\")\n",
    "ax1.plot(fpr_grd, tpr_grd, label=\"GBT\")\n",
    "ax1.plot(fpr_grd_lm, tpr_grd_lm, label=\"GBT + LR\")\n",
    "ax1.set_xlabel(\"False positive rate\")\n",
    "ax1.set_ylabel(\"True positive rate\")\n",
    "ax1.set_title(\"ROC curve\")\n",
    "ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.set_xlim(0, 0.2)\n",
    "ax2.set_ylim(0.8, 1)\n",
    "ax2.plot([0, 1], [0, 1], \"k--\")\n",
    "ax2.plot(fpr_rf, tpr_rf, label=\"RF\")\n",
    "ax2.plot(fpr_rf_lm, tpr_rf_lm, label=\"RF + LR\")\n",
    "ax2.plot(fpr_grd, tpr_grd, label=\"GBT\")\n",
    "ax2.plot(fpr_grd_lm, tpr_grd_lm, label=\"GBT + LR\")\n",
    "ax2.set_xlabel(\"False positive rate\")\n",
    "ax2.set_ylabel(\"True positive rate\")\n",
    "ax2.set_title(\"ROC curve (zoomed in at top left)\")\n",
    "ax2.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом модельном примере видим, что применение комбинации моделей (кодирование данных с помощью ансамбля деревьев и классификация с помощью линейной модели) позволяет достичь более высокого качества классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разведочный анализ данных (exploratory data analysis, EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mstz/wine\")[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset.hist(figsize=(20, 10), layout=(-1, 5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "heatmap = sns.heatmap(\n",
    "    dataset.corr().round(2),\n",
    "    annot=True,\n",
    "    square=True,\n",
    "\n",
    "    cmap=\"Blues\",  #\n",
    "    cbar_kws={\"fraction\": 0.01},  # shrink colour bar\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "heatmap.set_xticklabels(\n",
    "    heatmap.get_xticklabels(), rotation=45, horizontalalignment=\"right\"\n",
    ")\n",
    "heatmap.set_title(\"Correalation heatmap\", fontdict={\"fontsize\": 18}, pad=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    dataset.sample(100),\n",
    "    vars=[\"total_sulfur_dioxide\", \"alcohol\", \"chlorides\"],\n",
    "    #corner=True,\n",
    "    hue=\"is_red\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=dataset,\n",
    "            x='quality',\n",
    "            y='total_sulfur_dioxide',\n",
    "            hue='is_red',\n",
    "            aspect=2,\n",
    "            alpha=0.5,\n",
    "            orient='v'\n",
    "           )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "crst = pd.crosstab(\n",
    "    dataset[\"is_red\"],\n",
    "    dataset[\"quality\"],\n",
    "    normalize=True,\n",
    ").round(4)\n",
    "\n",
    "\n",
    "heatmap = sns.heatmap(\n",
    "    crst,\n",
    "    annot=True,\n",
    "    square=True,\n",
    "    cmap=\"Blues\",\n",
    "    cbar_kws={\"fraction\": 0.01},\n",
    "    linewidth=1,\n",
    ")\n",
    "\n",
    "heatmap.set_title(\"Crosstab heatmap\", fontdict={\"fontsize\": 18}, pad=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(\n",
    "    data=dataset,\n",
    "    x=\"alcohol\",\n",
    "    y=\"pH\",\n",
    "    hue=\"is_red\",\n",
    "    kind=\"line\",  # or scatter\n",
    "    aspect=2,\n",
    ")\n",
    "\n",
    "g.set_xticklabels(horizontalalignment=\"right\", step=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[пример EDA](https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/Machine%20Learning%20Project%20Part%201.ipynb)\n",
    "\n",
    "\n",
    "[Примеры графиков matplotlib](https://habr.com/ru/articles/468295/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим способы и подходы, чтобы отделять полезные признаки от бесполезных, а также понижать размерность пространства признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем отбирать признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Количество признаков в данных может оказаться избыточным**:\n",
    "- могут существовать группы признаков, которые лишь при совместном рассмотрении могут оказаться значимыми для решаемой задачи;\n",
    "- некоторые признаки могут оказаться в наборе данных исключительно из-за желания использовать всю имеющуюся информацию и на самом деле быть никак не связны с целевой переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*В первом случае* логично предложить замену группы зависимых признаков через объединение их в один новый композитный признак. Замена группы старых признаков новым может позволить сохранить всю значимую информацию, одновременно избавляясь от проблемы избыточного описания данных.\n",
    "\n",
    "*Во втором случае* можно ожидать улучшение качества предсказания модели, если удастся отфильтровать действительно важные признаки. Включение в данные *не важных* признаков, очевидно, не может улучшить качество обученной на таких данных модели, но, как ни странно, может значительно ухудшить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Некоторые признаки могут оказаться шумом**\n",
    "\n",
    "Предположим, мы добавили в набор данных для обучения некоторой модели регрессии несколько тысяч вещественных признаков, которые представляют собой случайные числа из стандартного нормального распределения — белый шум. Случайные числа, очевидно, никак не могут быть связаны с нашей целевой переменной. Тем не менее, в силу большого числа таких шумовых признаков, значения некоторых из них могут оказаться случайно скоррелированы со значениям нашей целевой переменной в рамках обучающей выборки. Обученная на таких данных модель будет стараться предсказывать целевую переменную, явно учитывая значения признаков, которые на самом деле не имеют никакого смысла. При попытке построения предсказания данной моделью на тестовой выборке мы неизбежно заметим значительное снижение качества предсказания. Полученная модель обладает плохой обобщающей способностью.\n",
    "\n",
    "В более общем случае можно говорить, что в многомерном пространстве почти всегда можно найти корреляции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/fake_correlations.png\" alt=\"alttext\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Больше подобных примеров](https://www.wnycstudios.org/podcasts/otm/articles/spurious-correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Скорость работы модели часто имеет значение**\n",
    "\n",
    "Кроме того, в практически важных задачах часто приходится искать компромисс между точностью предсказания модели и необходимым для его получения временем. Спектр таких задач достаточно широк: от проблем построения быстрых систем ранжирования рекламных объявлений в интернет-маркетинге до построения быстрых систем распознавания сложных событий на ускорителях заряженных частиц. Вычислительная сложность модели, растёт с увеличением числа входных признаков, поэтому работающие с меньшим числом признаков модели являются для таких задач предпочтительными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полный перебор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попытаться перебрать все возможные комбинации признаков. Даже для 100  признаков такой подход будет считаться до конца Вселенной.\n",
    "\n",
    "Потому прибегают к эвристикам, которые могут пропускать оптимальное решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Одномерный отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к отбору признаков — это одномерный подход. В нём оценивается связь каждого признака с целевой переменной, например, измеряется корреляция.\n",
    "\n",
    "Такой подход довольно простой, но он не учитывает сложные закономерности, в нём все признаки считаются независимыми, тогда как в машинном обучении модели учитывают взаимное влияние признаков, их пар или даже более сложные действия на целевую переменную.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/titanic.csv\"\n",
    ")  # Load the data using pandas\n",
    "dataset.head(5)  # Show the first 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# The categorical-to-numerical function\n",
    "# Changed to automatically add column names\n",
    "def cat_to_num(data):  # one-hot encoding\n",
    "    categories = set(data)\n",
    "    features = {}\n",
    "    for cat in categories:\n",
    "        binary = data == cat\n",
    "        if len(set(binary)) == 1:\n",
    "            # Ignore features where all values equal\n",
    "            continue\n",
    "        new_key = f\"{data.name}={cat}\"\n",
    "\n",
    "        features[new_key] = binary.astype(\"int\")\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def cabin_features(data):\n",
    "    features = []\n",
    "    for cabin in data:\n",
    "        cabins = str(cabin).split(\" \")\n",
    "        n_cabins = len(cabins)\n",
    "        # First char is the cabin_char\n",
    "        try:\n",
    "            cabin_char = cabins[0][0]\n",
    "        except IndexError:\n",
    "            cabin_char = \"X\"\n",
    "            n_cabins = 0\n",
    "        # The rest is the cabin number\n",
    "        try:\n",
    "            cabin_num = int(cabins[0][1:])\n",
    "        except:\n",
    "            cabin_num = -1\n",
    "        # Add 3 features for each passanger\n",
    "        features.append([cabin_char, cabin_num, n_cabins])\n",
    "    features = np.array(features)\n",
    "    dic_of_features = {\n",
    "        \"Cabin_num\": features[:, 1].astype(\"int\"),\n",
    "        \"N_cabins\": features[:, 2].astype(\"int\"),\n",
    "    }\n",
    "    out = pd.DataFrame(dic_of_features)\n",
    "    char_column = pd.DataFrame({\"Cabin_char\": features[:, 0]})\n",
    "    cabin_ch = cat_to_num(char_column[\"Cabin_char\"])\n",
    "    return out.join(cabin_ch)\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"Takes a dataframe of raw data and returns ML model features\"\"\"\n",
    "\n",
    "    # Initially, we build a model only on the available numerical values\n",
    "    features = data.drop(\n",
    "        [\n",
    "            \"PassengerId\",\n",
    "            \"Survived\",\n",
    "            \"Fare\",\n",
    "            \"Name\",\n",
    "            \"Sex\",\n",
    "            \"Ticket\",\n",
    "            \"Cabin\",\n",
    "            \"Embarked\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Setting missing age values to -1\n",
    "    features[\"Age\"] = data[\"Age\"].fillna(-1)\n",
    "\n",
    "    # Adding the sqrt of the fare feature\n",
    "    features[\"sqrt_Fare\"] = np.sqrt(data[\"Fare\"])\n",
    "\n",
    "    # Adding gender categorical value\n",
    "    features = features.join(cat_to_num(data[\"Sex\"]))\n",
    "\n",
    "    # Adding Embarked categorical value\n",
    "    features = features.join(cat_to_num(data[\"Embarked\"]))\n",
    "\n",
    "    # Split cabin\n",
    "    features = features.join(cabin_features(data[\"Cabin\"]))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "features = prepare_data(dataset)  # Create variable features\n",
    "features.head(5)  # Display first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем корреляцию каждого признака с целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = prepare_data(dataset)  # produce feature\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, dataset[\"Survived\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "correlations = []  # create a storage for correlations\n",
    "for column in features:\n",
    "    r, p_value = stats.pearsonr(x_train[column], y_train)  # compute Pearson and R\n",
    "    correlations.append((column, abs(r)))  # add to storage\n",
    "\n",
    "df = pd.DataFrame(correlations, columns=[\"Column\", \"Correlation\"]).sort_values(\n",
    "    \"Correlation\", ascending=False\n",
    ")\n",
    "df.head(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая идея такая: давайте посчитаем ROC-AUC по признаку, учитывая его как предсказание модели. Если ROC-AUC высокий (нас интересуют только абсолютные значения), то признак важный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "features = prepare_data(dataset)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, dataset[\"Survived\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "rocs = []  # create a storage for ROCs\n",
    "for column in features:\n",
    "    # use feature as score directly\n",
    "    r1 = roc_auc_score(y_score=x_train[column], y_true=y_train)\n",
    "    # use feature as score in reversed manner\n",
    "    r2 = roc_auc_score(y_score=-x_train[column], y_true=y_train)\n",
    "    r = max(r1, r2)\n",
    "    rocs.append((column, r))\n",
    "\n",
    "df = pd.DataFrame(rocs, columns=[\"Column\", \"Rocs\"]).sort_values(\n",
    "    \"Rocs\", ascending=False\n",
    ")  # sort from highest to lowest\n",
    "df.head(df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы одномерного отбора признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/problems_of_one_dimensional_feature_selection.png\" alt=\"alttext\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У подхода, при котором важности всех признаков оцениваются по отдельности, есть свои недостатки. На левом рисунке изображена двумерная выборка, для которой необходимо решить задачу классификации. Если спроецировать данную выборку на ось абсцисс, то она будет разделима, хотя и будут присутствовать ошибки. Если же спроецировать данную выборку на ось ординат, то все объекты разных классов перемешаются, и выборка будет неразделима. В этом случае при использовании любого метода одномерного оценивания информативности первый признак будет информативен, а второй — совершенно неинформативен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тем не менее, видно, что если использовать эти признаки одновременно, то классы будут разделимы идеально. Второй признак важен, но он важен только в совокупности с первым, и методы одномерного оценивания информативности не способны это определить.\n",
    "\n",
    "На рисунке справа показана выборка, на которой одномерные методы оценки информативности работают ещё хуже. В этом случае, если спроецировать выборку на ось абсцисс или ординат, то объекты классов перемешаются, и в обоих случаях данные будут совершенно неразделимы. И, согласно любому из описанных методов, оба признака неинформативны. Тем не менее, если использовать их одновременно, то, например, решающее дерево может идеально решить данную задачу классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример: влияние роста и веса при предсказании вероятности сердечного заболевания. Избыточный вес может являться важным фактором, но оценить, является ли он избыточным или нормальным, можно только зная рост пациента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Жадный отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жадные методы отбора признаков по сути являются надстройками над методами обучения моделей. Они перебирают различные подмножества признаков и выбирают то из них, которое дает наилучшее качество определённой модели машинного обучения.\n",
    "\n",
    "Данный процесс устроен следующим образом:\n",
    "\n",
    "Обучение модели считается черным ящиком, который на вход принимает информацию о том, какие из его признаков можно использовать при обучении модели, обучает модель, и дальше каким-то методом оценивается качество такой модели, например, по отложенной выборке или кросс-валидации. Таким образом, задача, которую необходимо решить — это оптимизация функционала качества модели по подмножеству признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полный перебор**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/full_sorting.png\" alt=\"alttext\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ решения данной задачи — это полный перебор всех подмножеств признаков и оценивание качества на каждом подмножестве. Итоговое подмножество — то, на котором качество модели наилучшее. Этот перебор можно структурировать и перебирать подмножества последовательно: сначала те, которые имеют мощность 1 (наборы из 1 признака), потом наборы мощности 2, и так далее. Это подход очень хороший, он найдет оптимальное подмножество признаков, но при этом очень сложный, поскольку всего таких подмножеств $2^d -1$, где $d$ — число признаков. Если признаков много — сотни или тысячи, то такой перебор невозможен: он займет слишком много времени, возможно, сотни лет или больше. Поэтому, такой метод подходит либо при небольшом количестве признаков, либо если известно, что информативных признаков очень мало, единицы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Жадное добавление**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/greedy_addition.png\" alt=\"alttext\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Жадная стратегия используется всегда, когда полный перебор не подходит для решения задачи. Например, может оказаться неплохой стратегия жадного наращивания (жадного добавления). Сначала находится один признак, который дает наилучшее качество модели (наименьшую ошибку $Q$):\n",
    "\n",
    "$i_1 = argmin Q(i)$.\n",
    "\n",
    "Тогда множество, состоящее из этого признака:\n",
    "\n",
    "$J_1 = {i_1}$\n",
    "\n",
    "Дальше к этому множеству добавляется еще один признак так, чтобы как можно сильнее уменьшить ошибку модели:\n",
    "\n",
    "$i_2 =argminQ(i_1,i)$, $J_2 ={i_1,i_2}$.\n",
    "\n",
    "Далее каждый раз добавляется по одному признаку, образуются множества $J_3 , J_4 , . . . .$ Если в какой-то момент невозможно добавить новый признак так, чтобы уменьшить ошибку, процедура останавливается. Жадность процедуры заключается в том, что как только какой-то признак попадает в оптимальное множество, его нельзя оттуда удалить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Feature selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "\n",
    "[Model-based and sequential feature selection](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py)\n",
    "\n",
    "[Sequential Feature Selection for Regression](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/#example-5-sequential-feature-selection-for-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sys\n",
    "\n",
    "sys.modules[\"sklearn.externals.joblib\"] = joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "sfs = SequentialFeatureSelector(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    k_features=8,  # number of features to select\n",
    "    forward=True,\n",
    "    floating=False,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    ")\n",
    "sfs.fit(x_train, y_train)\n",
    "\n",
    "df = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "df.head(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "plot_sfs(sfs.get_metric_dict(), kind=\"std_dev\")\n",
    "\n",
    "plt.title(\"Sequential Forward Selection (StdDev)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD-DEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/non_greedy.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описанный выше подход довольно быстрый: в нем столько итераций, сколько признаков в выборке. Но при этом он слишком жадный: перебирается слишком мало вариантов, и мы можем оказаться в плохой локальной точке. Процедуру можно усложнить. Один из подходов к усложнению — это алгоритм **ADD-DEL**, который не только добавляет, но и удаляет признаки из оптимального множества.\n",
    "\n",
    "Алгоритм начинается с процедуры жадного добавления. Множество признаков наращивается до тех пор, пока получается уменьшить ошибку, затем признаки жадно удаляются из подмножества, то есть перебираются все возможные варианты удаления признака, оценивается ошибка и удаляется тот признак, который приводит к наибольшему уменьшению ошибки на выборке. Эта процедура повторяет добавление и удаление признаков до тех пор, пока уменьшается ошибка. Алгоритм ADD-DEL всё еще жадный, но при этом он менее жадный, чем предыдущий, поскольку *может исправлять ошибки*, сделанные в начале перебора: если вначале был добавлен неинформативный признак, то на этапе удаления от него можно избавиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/add_del.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sffs = SequentialFeatureSelector(\n",
    "    LogisticRegression(max_iter=1000),  # represents the classifier\n",
    "    k_features=8,  # the number of features you want to select\n",
    "    forward=True,  # add features\n",
    "    floating=True,  # remove features\n",
    "    scoring=\"accuracy\",  # means that the selection will be decided by the accuracy of the classifier.\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    ")\n",
    "\n",
    "sffs.fit(x_train.values, y_train)  # performs the actual SFFS algorithm\n",
    "df = pd.DataFrame.from_dict(sffs.get_metric_dict()).T\n",
    "df.head(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sfs(sffs.get_metric_dict(), kind=\"std_dev\")\n",
    "\n",
    "plt.title(\"Sequential Forward Selection (StdDev)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков на основе моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование весов признаков\n",
    "\n",
    "Во многих моделях (линейных) перед признаками стоят веса. Если признаки масштабированы, то веса при признаках можно интерпретировать как информативности: чем больше по модулю вес при признаке $j$, тем больший вклад этот признак вносит в ответ модели. Однако если признаки не масштабированы, то так использовать веса уже нельзя. Например, если есть два признака, и один по масштабу в 1000 раз меньше другого, то вес первого признака может быть очень большим, только чтобы признаки были одинаковыми по масштабу.\n",
    "Если необходимо обнулить как можно больше весов, чтобы линейная модель учитывала только те признаки, которые наиболее важны для нее, можно использовать L1-регуляризацию. Чем больше коэффициент при L1-регуляризаторе, тем меньше признаков будет использовать линейная модель.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/dot_product.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "df = pd.DataFrame(lr.coef_[0], x_train.columns, columns=[\"Coef\"]).sort_values(\n",
    "    \"Coef\", key=abs, ascending=False\n",
    ")\n",
    "df.head(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 1. A SelectFromModel instance selects the features\n",
    "# whose coefficients are non-zero when the feature is included in the model.\n",
    "# 2. The LogisticRegression instance runs the logistic regression\n",
    "# algorithm on the training data.\n",
    "\n",
    "# selecting features based on importance weights\n",
    "lr_selector = SelectFromModel(LogisticRegression(max_iter=1000))\n",
    "lr_selector.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns[lr_selector.get_support()]  # Get a mask of the features selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_selector.transform(x_train)  # select only relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# select features with RFC\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "\n",
    "rf_selector = SelectFromModel(rf)\n",
    "rf_selector.fit(x_train, y_train)  # Fit it on the training data\n",
    "\n",
    "x_train.columns[rf_selector.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomization/Permutation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для определения важности признака можно перемешать его значения. Это не изменит форму распределения признака, но сделает его бессмысленным. По качеству модели мы можем оценить, каков был вклад признака до изменения: если качество упало сильно, то он был значимым.\n",
    "\n",
    "В sklearn это реализовано как отдельный класс [`permutation_importance`](https://scikit-learn.org/stable/modules/permutation_importance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/permutation_importance_example.png\" alt=\"alttext\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение качества модели рассчитывается на объектах, которые не вошли в обучающую выборку, а остались для валидации (не путать с тестовой выборкой, которую мы не трогаем).\n",
    "\n",
    "У данного метода есть три особенности:\n",
    "1. Значение **важности признака** будет зависеть от **результата перемешивания**, которое носит случайный характер. Поэтому функция `permutation_importance` имеет параметр `n_repeats`, который определяет, сколько раз признак будет перемешиваться случайным образом.\n",
    "2. Значение **важности признака** будет зависеть от **модели** и того, насколько “удачно” она обучена. “Плохая” и “хорошая” модель могут считать важным разный набор признаков. Поэтому полезно сравнить важность признаков модели, обучив модель на различных выборках (например, используя cross-валидацию).\n",
    "3. Данный метод неадекватно оценивает **важность коррелированных признаков**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для борьбы с неадекватной оценкой **коррелированных признаков** можно собирать коррелированные признаки в кластеры и оставить  только один признак для каждого [кластера](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py).\n",
    "\n",
    "Одна из вариаций `permutation importance` подразумевает **обучение модели заново** после каждого **перемешивания признаков**. Это дает чуть более адекватную оценку важности, но очень долго."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример отбора признаков с помощью permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем `permutation importance`:\n",
    "\n",
    "1. Обучим модель\n",
    "2. Перемешаем значения в одном столбце (одного признака), сделаем предсказание на валидационном наборе данных и посчитаем ошибку. Ухудшение ошибки покажет важность признака, который перемешали.\n",
    "3. Возвращаем значения признака и повторяем шаг 2 со всеми остальными признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "r = permutation_importance(model, x_test, y_test, n_repeats=100, random_state=42)\n",
    "\n",
    "df = pd.DataFrame({\"name\": x_train.columns, \"imp\": r.importances_mean}).sort_values(\n",
    "    \"imp\", ascending=False\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.barplot(data=df, y=\"name\", x=\"imp\", color=\"blue\", orient=\"h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже выполняются те же операции, но уже с линейной регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "r = permutation_importance(model, x_test, y_test, n_repeats=100, random_state=42)\n",
    "\n",
    "df = pd.DataFrame({\"name\": x_train.columns, \"imp\": r.importances_mean}).sort_values(\n",
    "    \"imp\", ascending=False\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.barplot(data=df, y=\"name\", x=\"imp\", color=\"blue\", orient=\"h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, у некоторых признаков при переходе от древесных моделей к линейным значительно повысилась важность, что говорит об их скоррелированности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta развивает идею Permutation.\n",
    "\n",
    "Попробуем \"раздуть\" наш датасет, добавив в него \"теневые признаки\" $-$ перемешанные реальные. Таким образом, наш датасет точно будет содержать хорошие признаки (мы ничего не удаляем)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/boruta_idea.png\" alt=\"alttext\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого обучаем модель и отбираем те признаки, чей `feature_importance` больше, чем у лучшего из теневых. Повторяем процедуру несколько раз для того,  чтобы удалить случайные скачки качества.\n",
    "\n",
    "Таким образом, для каждого признака мы будем знать, сколько раз мы его отобрали. Получаем распределение. Самая большая неопределенность будет в середине (вероятность отобрать = 0.5):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/boruta_diagram.png\" alt=\"alttext\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Boruta Explained Exactly How You Wished Someone Explained to You](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор (в нашем случае из 20) испытаний Бернулли это биномиальное распределение. Поступаем просто. Со значимостью, допустим 0.05, берем все из хорошего хвоста и отбрасываем из плохого хвоста. С признаками из середины колокола ничего особо не сделаешь, увеличение числа итераций приведет к ужатию колокола, но глобально не поможет.\n",
    "\n",
    "Если нам нужна хорошо интерпретируемая модель, то надо брать только \"точно хорошие\" признаки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример отбора признаков с помощью Boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать реализацию github [boruta_py](https://github.com/scikit-learn-contrib/boruta_py). Возьмем Random Forest модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем применить алгоритм Boruta к этому же датасету, и сократить количество признаков.\n",
    "Оценим текущее качество модели, используя ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_proba(x_test)[:, 1]\n",
    "r1 = roc_auc_score(y_score=pred, y_true=y_test)\n",
    "\n",
    "print(f\"ROC-AUC: {r1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим Boruta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install boruta\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Питоновская реализация Boruta соответствует  API sklearn и может использоваться как в конвейере, так и самостоятельно.\n",
    "\n",
    "[[towardsdatascience] Boruta Feature Selection (an Example in Python)](https://towardsdatascience.com/simple-example-using-boruta-feature-selection-in-python-8b96925d5d7a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta работает с `numpy.array`, поэтому используем `.values()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_train.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем оценку признаков при помощи Boruta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "\n",
    "# define Boruta feature selection method\n",
    "model = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "\n",
    "\n",
    "feat_selector = BorutaPy(model, n_estimators=100, verbose=2, random_state=42)\n",
    "\n",
    "# find all relevant features\n",
    "feat_selector.fit(x_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем признаки с оценкой их важности и очистим датасет от маловажных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip my names, ranks, and decisions in a single iterable\n",
    "feature_ranks = list(\n",
    "    zip(x_train.columns, feat_selector.ranking_, feat_selector.support_)\n",
    ")\n",
    "\n",
    "# iterate through print out the results and remove features with low rank\n",
    "for feat in feature_ranks:\n",
    "    print(\"Feature: {:<25} Rank: {},  Keep: {}\".format(feat[0], feat[1], feat[2]))\n",
    "    if feat[2] == False:\n",
    "        del x_train[feat[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta из 21 признака оставила нам только три: **sqrt_Fare , Sex=male , Sex=female**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим алгоритм на признаках, отобранных Boruta.\n",
    "Давайте проверим, можем ли мы добиться такого же результата, если проанализируем и уберем «лишние» данные?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with the random forest algorithm\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "pred = model.predict_proba(x_test[x_train.columns])[:, 1]\n",
    "r1 = roc_auc_score(y_score=pred, y_true=y_test)\n",
    "\n",
    "print(f\"ROC-AUC: {r1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После удаления ~85% признаков качество упало на так сильно, как можно было бы ожидать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков — это тоже выбор гиперпарметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никогда не отбирайте признаки на том же наборе данных, на котором тестируетесь. Иначе получите завышенное качество вашей модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обучения на большом числе бесполезных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем следующий датасет.\n",
    "\n",
    "У нас есть по 500 пациентов, больных и здоровых.\n",
    "Для каждого известно 100000 **случайных** бинарных признаков.\n",
    "Что будет, если мы попросим нашу модель научиться отделять здоровых от больных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "pat_cnt = 500  # patients\n",
    "snv_count = 100000  # all features(binary)\n",
    "\n",
    "genes = [f\"SNP{ind}\" for ind in range(snv_count)]  # features names\n",
    "\n",
    "# Generate 2 data sets, healthy and diseased patients.\n",
    "# Each data set is a binary vector of length `snv_count`,\n",
    "# in other words a SNV count vector of length 100000.\n",
    "\n",
    "genes = [f\"SNP{ind}\" for ind in range(snv_count)]\n",
    "healthy = pd.DataFrame(\n",
    "    np.random.choice([0, 1], size=(pat_cnt, snv_count)), columns=genes\n",
    ")\n",
    "# We add a `State` column, indicating whether it's healthy or diseased.\n",
    "healthy[\"State\"] = \"H\"\n",
    "diseased = pd.DataFrame(\n",
    "    np.random.choice([0, 1], size=(pat_cnt, snv_count)), columns=genes\n",
    ")\n",
    "diseased[\"State\"] = \"D\"\n",
    "\n",
    "patients = pd.concat([healthy, diseased], axis=0)\n",
    "\n",
    "# We drop the State column to get a `x` and a `y` matrix.\n",
    "x = patients.drop(\"State\", axis=1)\n",
    "y = patients[\"State\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Без отбора признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "\n",
    "# 1. Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y == \"D\", test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Train a logistic regression model on the train set\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 3. Predict the probabilities for train and test sets\n",
    "# 4. Calculate ROCAUC and PRAUC scores for the prediction of train and test sets\n",
    "# 5. Compare the performance of the model on train and test sets using the scores\n",
    "\n",
    "y_train_pred = model.predict_proba(x_train)[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_prauc = average_precision_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_accuracy = accuracy_score(y_pred=y_train_pred > 0.5, y_true=y_train)\n",
    "print(\"Train quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy:  {train_accuracy:.02f}\")\n",
    "# Test\n",
    "y_test_pred = model.predict_proba(x_test)[:, 1]\n",
    "test_rocauc = roc_auc_score(y_score=y_test_pred, y_true=y_test)\n",
    "test_prauc = average_precision_score(y_score=y_test_pred, y_true=y_test)\n",
    "test_accuracy = accuracy_score(y_pred=y_test_pred > 0.5, y_true=y_test)\n",
    "print(\"\\nTest quality:\")\n",
    "print(f\"ROCAUC : {test_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {test_prauc:.02f}\")\n",
    "print(f\"Accuracy:  {test_accuracy:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель идеально выучила данные обучения, но с тестом беда (как и должно быть)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### С неправильной процедурой отбора признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем те признаки, для которых средняя разница для больных и здоровых максимальна. Заметьте, мы даже не используем чего-то сильно сложного."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Take the mean of all the reads for each gene in healthy\n",
    "#  and each gene in diesised.\n",
    "# 2. Subtract the mean number of reads for each gene in diesised\n",
    "# from the mean number of reads for each gene in healthy.\n",
    "\n",
    "diffs = x[y == \"H\"].mean(axis=0) - x[y == \"D\"].mean(axis=0)\n",
    "# 3. Look at the top k most different genes\n",
    "# by sorting the values in the resulting array from largest to smallest.\n",
    "top = np.abs(diffs).sort_values(ascending=False)[0:10]\n",
    "genes = top.index\n",
    "\n",
    "# Print the gene names of the top k genes.\n",
    "print(\"Genes\", genes)\n",
    "\n",
    "# Select x\n",
    "x_selected = x[genes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим на качество модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_selected, y == \"D\", test_size=0.3, random_state=42\n",
    ")\n",
    "# 2. Train a logistic regression model on the train set\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 3. Predict the probabilities for train and test sets\n",
    "# 4. Calculate ROCAUC and PRAUC scores for the prediction of train and test sets\n",
    "# 5. Compare the performance of the model on train and test sets using the scores\n",
    "y_train_pred = model.predict_proba(x_train)[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_prauc = average_precision_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_accuracy = accuracy_score(y_pred=y_train_pred > 0.5, y_true=y_train)\n",
    "print(\"Train quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy: accuracy {train_accuracy:.02f}\")\n",
    "# Test\n",
    "y_test_pred = model.predict_proba(x_test)[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_test_pred, y_true=y_test)\n",
    "train_prauc = average_precision_score(y_score=y_test_pred, y_true=y_test)\n",
    "train_accuracy = accuracy_score(y_pred=y_test_pred > 0.5, y_true=y_test)\n",
    "print(\"\\nTest quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy: accuracy {train_accuracy:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внезапно качество на тесте выглядит разумным. Да, не очень классное, но есть. А должно быть соответствующее случайной модели — признаки-то случайные.\n",
    "\n",
    "Дело в том, что мы изначально выбрали те признаки, которые работали хорошо по случайным причинам на всем нашем искусственном датасете, а не только на тренировочной выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### С правильной процедурой отбора признаков\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets (with two sizes)\n",
    "x_fs_train, x_test, y_fs_train, y_test = train_test_split(\n",
    "    x, y == \"D\", test_size=0.3, random_state=42\n",
    ")\n",
    "# split again\n",
    "x_fs, x_train, y_fs, y_train = train_test_split(\n",
    "    x_fs_train, y_fs_train, test_size=0.8, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбираем признаки на одном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find the difference between the mean expression\n",
    "#    of the genes\n",
    "# 2. Sort the resulting list according to the difference in means\n",
    "#    (from greatest difference to least)\n",
    "# 3. Take the top K genes and return them\n",
    "\n",
    "diffs = x_fs[np.logical_not(y_fs)].mean(axis=0) - x_fs[y_fs].mean(axis=0)\n",
    "top = np.abs(diffs).sort_values(ascending=False)[0:10]\n",
    "genes = top.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель на втором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train[genes], y_train)\n",
    "y_train_pred = model.predict_proba(x_train[genes])[:, 1]\n",
    "\n",
    "y_train_pred = model.predict_proba(x_train[genes])[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_prauc = average_precision_score(y_score=y_train_pred, y_true=y_train)\n",
    "train_accuracy = accuracy_score(y_pred=y_train_pred > 0.5, y_true=y_train)\n",
    "print(\"Train quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy: accuracy {train_accuracy:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем на третьем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_proba(x_test[genes])[:, 1]\n",
    "train_rocauc = roc_auc_score(y_score=y_test_pred, y_true=y_test)\n",
    "train_prauc = average_precision_score(y_score=y_test_pred, y_true=y_test)\n",
    "train_accuracy = accuracy_score(y_pred=y_test_pred > 0.5, y_true=y_test)\n",
    "print(\"Test quality:\")\n",
    "print(f\"ROCAUC : {train_rocauc:.02f}\")\n",
    "print(f\"PRAUC : {train_prauc:.02f}\")\n",
    "print(f\"Accuracy: {train_accuracy:.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача понижения размерности\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто мы хотим данные из пространства высокой размерности преобразовать в пространство более низкой размености с сохранением одного или нескольких свойств, например:\n",
    "* объекты реконструируются с минимальной ошибкой\n",
    "* расстояние между объектами сохраняется\n",
    "\n",
    "\n",
    "Мотивация понижения размерности:\n",
    "\n",
    "1. Многие алгоритмы показывают себя плохо на пространствах большой размерности ([проклятие размерности](https://en.wikipedia.org/wiki/Curse_of_dimensionality)).\n",
    "\n",
    "2. Некоторые просто будут значительно дольше работать, при этом качество их работы не изменится от уменьшения размерности.\n",
    "\n",
    "3. Понижение размерности позволяет использовать память более эффективно и подавать модели на обучение за один раз больше объектов.\n",
    "\n",
    "4. Избавиться от шумовых признаков, обсудим это дальше.\n",
    "\n",
    "5. Задача визуализации: хочется взглянуть на наши объекты, а делать это в 100-мерном или 100000-мерном пространстве невозможно.\n",
    "\n",
    "6. Удаление выбросов — в пространствах меньшей размерности можем их увидеть глазами.\n",
    "\n",
    "7. Можем увидеть закономерности в данных: что они разделяются на явные кластеры и т. д.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В машинном обучении часто используют предположение о многообразии (manifold assumption). Это предположение о том, что между признаками, описывающими реальные объекты, существуют некоторые нетривиальные связи. Вследствие этого данные заполняют не весь объем пространства признаков, а лежат на некоторой поверхности — на **многообразии** (manifold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если предположение верно, то каждый объект может быть достаточно точно описан новыми признаками в пространстве значительно меньшей размерности, чем исходное пространство признаков.\n",
    "\n",
    "При этом мы будем терять часть информации об объектах. Но при выполнении предположения о многообразии (а оно почти всегда выполняется) и при правильных настройках алгоритма понижения размерности эти потери будут незначительны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/manifold_assumption.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве случаев это действительно правда. Например, лица людей даже на фотографиях 300x300, лежат в пространстве меньшей размерности, нежели 90000. Не каждая матрица 300 на 300, заполненная какими-то значениями от 0 до 1, даст нам изображение лица человека."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/faces_in_space_of_smaller_dimension.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Метод главных компонент)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод главных компонент (Principal Component Analysys, PCA) — простейший линейный метод снижения размерности, описан [Пирсоном в 1901 году](http://pca.narod.ru/pearson1901.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/problem_statement_pca.png\" width=\"850\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Максимизация дисперсии выборки после понижения размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим простой пример для понимания интуиции, которая стоит за методом главных компонент.\n",
    "\n",
    "У нас есть выборка объектов с двумя признаками $x_1$ и $x_2$, отобразим их на плоскости. Мы хотим, чтобы у нас был только один признак, который характеризует наши данные. Тогда наша задача — провести прямую через наши точки так, чтобы она наилучшим образом характеризовала наши данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/pca_dataset_example.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем прямую вдоль максимального изменения данных и спроецируем точки на нее. Это и будет наша первая главная компонента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/first_component_example.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем использовать только один признак, который лежит на новой оси, а любые отклонения от нее это просто шум (в действительности, определить, что является шумом в данных это нетривиальная задача)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/proection_component.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это то, как можно понимать интуицию работы PCA.\n",
    "\n",
    "В более многомерном случае (например, при количестве признаков 200) мы бы делали то же самое:\n",
    "\n",
    "1. Выбираем направление максимального изменения данных — это первая главная компонента.\n",
    "2. Если данные описаны не полностью, то выбираем еще одно направление — перпендикулярное к первому, так, чтобы описать оставшееся изменение в данных.\n",
    "3. Повторяем пункт 2, пока компонент не будет достаточно, вплоть до размерности исходного пространства. При этом каждая следующая компонента будет перпендикулярна предыдущим и объяснять меньше дисперсии, чем любая из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/3d_to_2d_pca.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с Титаником"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять, как использовать PCA на практике, найдем главные компоненты для датасета Titanic и посмотрим, как распределится между ними дисперсия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# The categorical-to-numerical function\n",
    "# Changed to automatically add column names\n",
    "def cat_to_num(data):  # one-hot encoding\n",
    "    categories = set(data)\n",
    "    features = {}\n",
    "    for cat in categories:\n",
    "        binary = data == cat\n",
    "        if len(set(binary)) == 1:\n",
    "            # Ignore features where all values equal\n",
    "            continue\n",
    "        new_key = f\"{data.name}={cat}\"\n",
    "\n",
    "        features[new_key] = binary.astype(\"int\")\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "\n",
    "def cabin_features(data):\n",
    "    features = []\n",
    "    for cabin in data:\n",
    "        cabins = str(cabin).split(\" \")\n",
    "        n_cabins = len(cabins)\n",
    "        # First char is the cabin_char\n",
    "        try:\n",
    "            cabin_char = cabins[0][0]\n",
    "        except IndexError:\n",
    "            cabin_char = \"X\"\n",
    "            n_cabins = 0\n",
    "        # The rest is the cabin number\n",
    "        try:\n",
    "            cabin_num = int(cabins[0][1:])\n",
    "        except:\n",
    "            cabin_num = -1\n",
    "        # Add 3 features for each passanger\n",
    "        features.append([cabin_char, cabin_num, n_cabins])\n",
    "    features = np.array(features)\n",
    "    dic_of_features = {\n",
    "        \"Cabin_num\": features[:, 1].astype(\"int\"),\n",
    "        \"N_cabins\": features[:, 2].astype(\"int\"),\n",
    "    }\n",
    "    out = pd.DataFrame(dic_of_features)\n",
    "    char_column = pd.DataFrame({\"Cabin_char\": features[:, 0]})\n",
    "    cabin_ch = cat_to_num(char_column[\"Cabin_char\"])\n",
    "    return out.join(cabin_ch)\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"Takes a dataframe of raw data and returns ML model features\"\"\"\n",
    "\n",
    "    # Initially, we build a model only on the available numerical values\n",
    "    features = data.drop(\n",
    "        [\n",
    "            \"PassengerId\",\n",
    "            \"Survived\",\n",
    "            \"Fare\",\n",
    "            \"Name\",\n",
    "            \"Sex\",\n",
    "            \"Ticket\",\n",
    "            \"Cabin\",\n",
    "            \"Embarked\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Setting missing age values to -1\n",
    "    features[\"Age\"] = data[\"Age\"].fillna(-1)\n",
    "\n",
    "    # Adding the sqrt of the fare feature\n",
    "    features[\"sqrt_Fare\"] = np.sqrt(data[\"Fare\"])\n",
    "\n",
    "    # Adding gender categorical value\n",
    "    features = features.join(cat_to_num(data[\"Sex\"]))\n",
    "\n",
    "    # Adding Embarked categorical value\n",
    "    features = features.join(cat_to_num(data[\"Embarked\"]))\n",
    "\n",
    "    # Split cabin\n",
    "    features = features.join(cabin_features(data[\"Cabin\"]))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Без стандартизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет и подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Importing the data from the .csv\n",
    "# 2. Pre-processing the data and creating a feature set\n",
    "# 3. Splitting the data into training and test data and labels\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/titanic.csv\"\n",
    ")\n",
    "features = prepare_data(dataset)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, dataset[\"Survived\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезной информацией является коэффициент объясненной дисперсии (explained variance ratio) главной компоненты. Этот коэффициент является отношением между дисперсией главной компоненты и суммой дисперсий всех главных компонент. Он указывает долю выборочной дисперсии, которая лежит вдоль оси каждой главной компоненты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модуле PCA, после `fit` можно получить explained variance ratio посредством обращения к полю `explained_variance_ratio_`, а explained variance — посредством обращения к полю `explained_variance_`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график с `explained_variance_ratio_` всех компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# performing PCA with default number of principal components.\n",
    "titanic_pca = sklearn.decomposition.PCA()\n",
    "titanic_pca.fit(x_train)  # fitting our PCA model with the training data.\n",
    "# calculating the explained variance of each of the components.\n",
    "evr = titanic_pca.explained_variance_ratio_\n",
    "\n",
    "# We are plotting the explained variance ratios.\n",
    "plt.bar(range(evr.shape[0]), evr)\n",
    "plt.title(\"Variance by components\")\n",
    "plt.xlabel(\"Components\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_pca.explained_variance_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графика не совсем понятно, сколько компонент брать, резко доля объясняемой дисперсии меняется в районе 2-компоненты. Посмотрим, сколько компонент нужно модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1.The first thing we do is to fit a PCA model to the training set\n",
    "#   of the Titanic dataset with n components from 1 to 10.\n",
    "# 2.We then fit a logistic regression model to the transformed training data\n",
    "#   and make predictions on the transformed test set.\n",
    "\n",
    "for i in range(1, 11):\n",
    "    titanic_pca = sklearn.decomposition.PCA(n_components=i)\n",
    "    titanic_pca.fit(x_train)\n",
    "\n",
    "    x_train_reduced = titanic_pca.transform(x_train)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(x_train_reduced, y_train)\n",
    "\n",
    "    x_test_reduced = titanic_pca.transform(x_test)\n",
    "\n",
    "    # prints the number of PCA components and the score\n",
    "    # for the corresponding model.\n",
    "    print(\"%i first components %.2f\" % (i, model.score(x_test_reduced, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что уже первых 7 компонент достаточно для достижения качества, которое далее не меняется.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дело в том, что мы забыли сделать **стандартизацию** наших данных.\n",
    "\n",
    "В нашем датасете переменные имеют совершенно разные масштабы. Из-за этого часть из них \"перетегягивает\" на себя всю дисперсию.\n",
    "\n",
    "В результате по доле дисперсии судить о важности компонент нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Со стандартизацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предварительно стандартизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we import the StandardScaler module.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Next, we create a StandardScaler object called scaler by calling the\n",
    "# StandardScaler() function.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#  We then fit the scaling model to our training data.\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "# We transform your test set by applying the same scaling model.\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# performing PCA with default number of principal components.\n",
    "titanic_pca = sklearn.decomposition.PCA()\n",
    "titanic_pca.fit(x_train)  # fitting our PCA model with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В `PCA.components_` лежат вектора главных компонент, которые располагаются там построчно (`n_components`, `n_features`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что PCA упорядочивает собственные вектора. Это значит, что собственный вектор, соответствующий главной компоненте и, соответственно, имеющей максимальную дисперсию, будет находиться в первой строке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the explained variance of each of the components.\n",
    "evr = titanic_pca.explained_variance_ratio_\n",
    "\n",
    "# We are plotting the explained variance ratios.\n",
    "plt.bar(range(evr.shape[0]), evr)\n",
    "plt.title(\"Variance by components\")\n",
    "plt.xlabel(\"Components\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь видим, что не только первые две компоненты дают вклад. Это происходит потому, что переменные отмасштабированы.\n",
    "\n",
    "Но и значимого спада в доле объясняемой дисперсии мы не видим, поэтому встает вопрос: как выбрать число компонент так, чтобы взять нужное и отсечь ненужное?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как выбирать оптимальное число  компонент\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем датасет, на котором польза от понижения размерности видна более явно.\n",
    "\n",
    "В данном датасете хранятся признаки (нам сейчас не важно, какие), характеризующие примерно 8000 клеток крови.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reading in the scRNAseq data.\n",
    "scRNAseq = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/scRNAseq_CITEseq.txt\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "\n",
    "x_scRNAseq = scRNAseq.iloc[:, 0:-1]  # features\n",
    "y_scRNAseq = scRNAseq.iloc[:, -1]  # labels\n",
    "\n",
    "# 2. taking the log of the data.\n",
    "x_scRNAseq = np.log(x_scRNAseq + 1)\n",
    "print(f\"dataset shape: {scRNAseq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбирать число компонент можно по-разному"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### По доле объясняемой дисперсии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем выбрать порог по доле объясняемой дисперсии, например 95%. Это быстрый споособ, но не ясно почему именно 95%, а не 90%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We're calculating the explained variance ratio for\n",
    "#    each component of the PCA.\n",
    "# 2. We're plotting these ratios in a chart.\n",
    "\n",
    "pca = PCA(n_components=x_train.shape[1])\n",
    "pca.fit(x_train)\n",
    "\n",
    "ths = 0.95\n",
    "total_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.plot(np.arange(1, total_explained.shape[0] + 1), total_explained)\n",
    "plt.axhline(xmin=0, xmax=1000, y=ths, c=\"red\", ls=\"--\")\n",
    "chosen_number = np.where(total_explained >= 0.95)[0][0] + 1\n",
    "plt.axvline(x=chosen_number, ymin=0, ymax=ths, c=\"red\", ls=\"--\")\n",
    "plt.xticks(np.arange(1, x_train.shape[1]))\n",
    "plt.ylabel(\"total sum of  proportion  of the explained variance\")\n",
    "plt.xlabel(\"Num of components\", size=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взяли 15 компонент. При выборе другого порога получили бы другое количество компонент.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### По правилу локтя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно построить график, отражающий, сколько дисперсии объясняет каждая из компонент, и на основе графика выбрать нужное число компонент.\n",
    "\n",
    "Идея подхода простая: переход от компонент, объясняющих что-то важное в данных, к компонентам, объясняющим шум, должен сопровождаться резким снижением доли объясняемой дисперсии.\n",
    "\n",
    "Или можно сказать иначе: выбранные нами компоненты должны быть устойчивы к добавлению шума в данные. Если мы нашли резкий скачок в доле объясняемой дисперсии, то маловероятно, что добавление шума этот скачок нивелирует — наш способ отбора компонент устойчив к шуму.\n",
    "\n",
    "В идеальном случае график будет выглядеть как-то так. Но на практике таких склонов может не быть, а может быть несколько."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/elbow_rule.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, we create a PCA object that you fit to the training data.\n",
    "# 2. Then, we create a a scatter plot where we plot the explained\n",
    "#    variance ratio as a function of the number of PCA components.\n",
    "# 3. We also plot the explained variance ratio as a function of the\n",
    "#    number of components, but with a smooth curve.\n",
    "# 4. Finally, we show the plot.\n",
    "\n",
    "n_comp = x_train.shape[1]\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(np.arange(1, n_comp + 1), explained)\n",
    "plt.plot(np.arange(1, n_comp + 1), explained)\n",
    "plt.title(\"Dependence of  variance on the number of components\", size=14)\n",
    "plt.xlabel(\"Num of components\", size=14)\n",
    "plt.ylabel(\"proportion of the explained variance\", size=14)\n",
    "plt.xticks(np.arange(1, x_train.shape[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понять, какое число компонент стоит выбрать, из такого графика сложно. Наверно, стоит выбрать последний склон — в нашем случае мы возьмем первые 12 компонент.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно применить критерий локтя и к нашему изначальному графику с суммарной долей объясненной дисперсии (в таком случае ожидаем скачок). Но там скачок также трудно обнаружить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перестановочный метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одним способом выбрать число главных компонент является перестановочный метод. Он заключается в следующем:\n",
    "\n",
    " 1) Перемешиваем значения каждого признака.\n",
    "\n",
    " 2)  Получаем матрицу признаков, которая не содержит никакой информации об исходном многообразии.\n",
    "\n",
    " 3) Делаем PCA.\n",
    "\n",
    " 4) Любая explained variance — просто из-за природы данных.\n",
    "\n",
    " 5) Делаем так 100-1000 раз.\n",
    "\n",
    " 6) Пусть на реальных данных k-я компонента объясняет\n",
    "n% дисперсии.\n",
    "\n",
    " 7) Смотрим на распределение доли дисперсии,\n",
    "объясняемой k-компонентой для случайных данных\n",
    "(полученных перемешиванием).\n",
    "\n",
    " 8) Можем сравнить и принять решение, объясняет ли k-я\n",
    "компонента что-то реальное или является просто шумом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset):\n",
    "    random_data = {}\n",
    "    for col in dataset.columns:\n",
    "        random_data[col] = np.random.permutation(dataset.loc[:, col].values)\n",
    "    random_data = pd.DataFrame(random_data)\n",
    "    return random_data\n",
    "\n",
    "\n",
    "def get_variance_by_chance(dataset, n_replics, n_components):\n",
    "    variance_explained_by_chance = np.zeros((n_replics, n_components))\n",
    "    for i in tqdm(range(n_replics)):\n",
    "        random_data = shuffle_dataset(dataset)\n",
    "        random_pca = PCA(n_components=n_components)\n",
    "        random_pca.fit(random_data)\n",
    "        variance_explained_by_chance[i, :] = random_pca.explained_variance_ratio_\n",
    "    return variance_explained_by_chance\n",
    "\n",
    "\n",
    "def get_pc_variance(dataset, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(dataset)\n",
    "    return pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "def plot_mean_and_CI(\n",
    "    ax,\n",
    "    values,\n",
    "    label,\n",
    "    ci_level=0.95,\n",
    "    alpha_transparency=0.5,\n",
    "    color_mean=None,\n",
    "    color_shading=None,\n",
    "):\n",
    "    mean = values.mean(axis=0)\n",
    "\n",
    "    std = values.std(axis=0)\n",
    "    n = values.shape[1]\n",
    "    se = std / np.sqrt(n)\n",
    "\n",
    "    q_alpha = (1 - ci_level) / 2\n",
    "    ci_num = np.abs(norm.ppf(q_alpha, loc=0, scale=1))\n",
    "\n",
    "    lb = mean - ci_num * se\n",
    "    ub = mean + ci_num * se\n",
    "\n",
    "    # plot the shaded range of the confidence intervals\n",
    "    ax.fill_between(\n",
    "        range(mean.shape[0]), ub, lb, color=color_shading, alpha=alpha_transparency\n",
    "    )\n",
    "    # plot the mean on top\n",
    "    ax.plot(mean, c=color_mean, lw=3, label=label)\n",
    "\n",
    "\n",
    "def plot_explained_variance(ax, variance):\n",
    "    ax.plot(variance, label=\"real\", lw=3)\n",
    "    ax.scatter(np.arange(0, variance.shape[0]), variance)\n",
    "\n",
    "\n",
    "def plot_variance_by_change(ax, variance_by_chance):\n",
    "    plot_mean_and_CI(\n",
    "        ax, variance_by_chance, label=\"chance\", color_mean=\"red\", color_shading=\"red\"\n",
    "    )\n",
    "\n",
    "\n",
    "def calc_permutat_pval(real_values, permut_values, eps=None):\n",
    "    eps = eps or (1 / (permut_values.shape[0] * 10))\n",
    "\n",
    "    p_values = np.zeros_like(real_values)\n",
    "    for i in range(0, p_values.shape[0], 1):\n",
    "        p_values[i] = (permut_values[:, i] >= real_values[i]).mean() + eps\n",
    "    return p_values\n",
    "\n",
    "\n",
    "def plot_explained_vs_chance(\n",
    "    ax, explained_variance, variance_by_chance, dataset_name, step=1\n",
    "):\n",
    "    plot_explained_variance(ax, explained_variance)\n",
    "    plot_variance_by_change(ax, variance_by_chance)\n",
    "\n",
    "    ax.set_title(f\"PCA {dataset_name}\", size=25)\n",
    "    ax.set_xlabel(\"Component number\", size=15)\n",
    "    ax.set_ylabel(\"Explained variance ration\", size=15)\n",
    "    ax.set_xticks(np.arange(0, explained_variance.shape[0], step))\n",
    "    ax.set_xticklabels(np.arange(1, explained_variance.shape[0] + 1, step), size=10)\n",
    "\n",
    "    ax.tick_params(labelsize=10, size=8)\n",
    "    ax.set_ylim(0, explained_variance[0] + 0.1)\n",
    "    ax.legend(fontsize=15)\n",
    "\n",
    "\n",
    "def plot_pval_plot(ax, p_values, dataset_name, alpha_level=0.05, logscale=True, step=1):\n",
    "    if logscale:\n",
    "        p_values = -np.log10(p_values)\n",
    "        alpha_level = -np.log10(alpha_level)\n",
    "\n",
    "    ax.set_title(f\"PC significance, {dataset_name}\", size=25)\n",
    "    ax.plot(p_values, lw=3)\n",
    "    ax.scatter(np.arange(0, p_values.shape[0]), p_values, lw=3)\n",
    "\n",
    "    ax.set_xlabel(\"Component number\", size=15)\n",
    "    ax.set_ylabel(\"-log(pvalue + eps)\", size=15)\n",
    "    ax.set_xticks(np.arange(0, p_values.shape[0], step))\n",
    "\n",
    "    ax.set_xticklabels(labels=np.arange(1, p_values.shape[0] + 1, step), size=10)\n",
    "    ax.tick_params(labelsize=10, size=8)\n",
    "\n",
    "    ax.hlines(\n",
    "        y=alpha_level,\n",
    "        xmin=0,\n",
    "        xmax=p_values.shape[0],\n",
    "        color=\"red\",\n",
    "        linestyles=\"dashed\",\n",
    "        lw=3,\n",
    "    )\n",
    "\n",
    "\n",
    "def pca_analysis(ax1, ax2, dataset, title, n_components=None, n_replics=1000, step=1):\n",
    "    n_components = n_components or dataset.shape[1]\n",
    "    explained_variance = get_pc_variance(dataset, n_components)\n",
    "    variance_by_chance = get_variance_by_chance(dataset, n_replics, n_components)\n",
    "    p_values = calc_permutat_pval(explained_variance, variance_by_chance)\n",
    "    plot_explained_vs_chance(ax1, explained_variance, variance_by_chance, title)\n",
    "    plot_pval_plot(ax2, p_values, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "f, (ax1, ax2) = plt.subplots(2, 1)\n",
    "f.set_figheight(7)\n",
    "f.set_figwidth(7)\n",
    "plt.subplots_adjust(top=1.7)\n",
    "pca_analysis(ax1, ax2, pd.DataFrame(x_train), \"Titanic\", n_replics=10, n_components=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явно видим, что, согласно этому подходу, надо взять 6 компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья с рассуждениями на тему \"Почему это работает?\"](https://arxiv.org/abs/1710.00479)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вклад исходных признаков в компоненты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем также посмотреть, какие признаки внесли вклад в компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_component = titanic_pca.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "b = sns.barplot(\n",
    "    x=first_component,\n",
    "    y=features.columns,\n",
    "    orient=\"h\",\n",
    "    hue=[z < 0 for z in first_component],\n",
    "    palette=[\"blue\", \"red\"],\n",
    ")\n",
    "b.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или абсолютные вклады"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "sns.barplot(x=np.abs(first_component), y=features.columns, orient=\"h\", color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что наибольший вклад в дисперсию вносят такие признаки, как класс пассажира и класс кабины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важность стандартизации\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем абсолютно случайную выборку, в которой нет никакой внутренней структуры.\n",
    "\n",
    "Пусть у нас 5 признаков, описывающих выборку, пришли из стандартного нормально распределения, а еще один будет равновероятно принимать значение 0 и 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate x with N rows and P columns using the normal distribution.\n",
    "# 2. Make a new vector with a third element that's either 0 or 3.\n",
    "# 3. Append this vector to x, so that we now have a Nxp matrix.\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "N = 200\n",
    "P = 5\n",
    "\n",
    "x = np.random.normal(size=[N, P])\n",
    "print(\"x before\", x.shape)\n",
    "x = np.append(x, np.random.choice([0, 3], size=[N, 1]), axis=1)\n",
    "print(\"x after\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fitting PCA to the data, and reducing it to 2 components\n",
    "# 2. Transforming that data using the PCA to get the low-dimensional representation\n",
    "# 3. Plotting it with matplotlib\n",
    "\n",
    "pca = PCA(2)\n",
    "low_d = pca.fit_transform(x)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(low_d[:, 0], low_d[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без предварительной нормализации мы получили два явных кластера в данных. Только вот кластеров этих по идее быть не должно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalize the data\n",
    "# 2. Take the output from the PCA function and assign it to our variable low_d\n",
    "# 3. Plot the first two components of low_d as a scatter plot.\n",
    "\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "low_d = pca.fit_transform(x_scaled)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(low_d[:, 0], low_d[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После стандартизации кластеры исчезли. Поэтому у вас должны быть весомые аргументы для того, чтобы не применять стандартизацию или другой метод, переводящий ваши данные в одну шкалу со средним 0, перед применением PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с RNA-Seq — нахождение выбросов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные биологического анализа пациентов с раком и без него.\n",
    "\n",
    "В данном случае наши данные уже предварительно обработаны, поэтому стандартизацию проводить не будем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First we import the pandas Python package.\n",
    "# 2. Then we read in the file that contains the RNA-seq data.\n",
    "#    The RNA-seq data is stored in a tab-delimited file (.tab extension),\n",
    "#    which is why we use the read_table pandas method.\n",
    "#    The read_table method is named this way because\n",
    "#    it reads tab-delimited data by default.\n",
    "#    We want to tell the read_table method that the data is tab delimited,\n",
    "#    that is why we supply the '\\t' argument.\n",
    "# 3. We want to tell the read_table method that the first column\n",
    "#    contains our sample IDs, which is the column with index 0.\n",
    "#    We do this by telling it to use the index_col argument and\n",
    "#    setting that equal to 0.\n",
    "# 4. We don't actually need the header information in this specific file,\n",
    "#    so we don't have to tell pandas to parse the header information.\n",
    "# 5. We assign the RNA-seq data to an object named rnadata\n",
    "\n",
    "rnadataset = pd.read_table(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/rnaseq_data.tab.txt\",\n",
    "    index_col=0,\n",
    "    header=None,\n",
    ")\n",
    "print(\"dataset shape: \", rnadataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnadataset.columns = list(rnadataset.columns[:-2]) + [\"dataset\", \"sample type\"]\n",
    "rnadataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the dataset and sample type columns from the data frame and\n",
    "# store the data frame in X\n",
    "x = rnadataset.drop(labels=[\"dataset\", \"sample type\"], axis=1)\n",
    "\n",
    "# We store the dataset and sample type columns in the labels data frame\n",
    "labels = rnadataset.loc[:, [\"dataset\", \"sample type\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the top two principal components of the data\n",
    "pca_decomposer = PCA(n_components=2)\n",
    "pca_decomposer.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on the features\n",
    "x_reduced = pca_decomposer.transform(x)\n",
    "\n",
    "# Display a scatterplot of the transformed dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"PCA plot\", size=24)\n",
    "plt.xlabel(\"PC1\", size=16)\n",
    "plt.ylabel(\"PC2\", size=16)\n",
    "sns.scatterplot(x=x_reduced[:, 0], y=x_reduced[:, 1], hue=labels[\"sample type\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даже не понимая, что за признаки у нас в колонках, мы видим несколько интересных вещей.\n",
    "\n",
    "Во-первых, синяя точка на оранжевой территории и оранжевая на синей — видимо, выбросы.\n",
    "\n",
    "Кроме этого мы видим, что почему-то у нас есть 4 почти равноудаленных кластера, по два на рак и нормальную ткань"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA on the features\n",
    "x_reduced = pca_decomposer.transform(x)\n",
    "\n",
    "# Display a scatterplot of the transformed dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"PCA plot\", size=24)\n",
    "plt.xlabel(\"PC1\", size=16)\n",
    "plt.ylabel(\"PC2\", size=16)\n",
    "sns.scatterplot(\n",
    "    x=x_reduced[:, 0],\n",
    "    y=x_reduced[:, 1],\n",
    "    hue=labels[\"sample type\"],\n",
    "    style=labels[\"dataset\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что в пределах данных одного эксперимента рак и опухоль разделяются хорошо. Но при этом данные между разными экспериментами отличаются так же, как и рак от опухоли.\n",
    "\n",
    "Этот эффект называется батч-эффектом и говорит о том, что нужно нормализовывать данные в пределах эксперимента каким-то хитрым образом, чтобы научить модель на новых данных отличать рак от нормальной ткани.\n",
    "\n",
    "Таким образом, с помощью PCA мы нашли выбросы и артефакт в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример с лицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как PCA применялся для решения практической задачи распознавания лиц*.\n",
    "\n",
    "Датасет:\n",
    "[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/)\n",
    "\n",
    "\n",
    "\\* Сейчас для этого используются более эффективные подходы, основанные на нейронных сетях.\n",
    "\n",
    "Загрузим датасет и распакуем его на диск VM Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# http://conradsanderson.id.au/lfwcrop/ (LFWcrop Face Dataset, greyscale version)\n",
    "# 1. Downloading the LFWcrop dataset from the website.\n",
    "# 2. Unzipping the dataset from the downloaded file.\n",
    "# 3. Opening the directory with the faces.\n",
    "\n",
    "dir = \"lfwcrop_grey/faces\"\n",
    "\n",
    "# http://conradsanderson.id.au/lfwcrop/ (LFWcrop Face Dataset, greyscale version)\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/lfwcrop_grey.zip\n",
    "!unzip lfwcrop_grey.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "\n",
    "def show_faces(imgs, titles, h=64, w=64):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for i in range(min(imgs.shape[0], 5)):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(imgs[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i])\n",
    "\n",
    "\n",
    "# Get first 1000 files\n",
    "celebrity_photos = os.listdir(dir)[1:1001]\n",
    "celebrity_imgs = [dir + \"/\" + photo for photo in celebrity_photos]\n",
    "# Load images from disk\n",
    "imgs = np.array([plt.imread(img) for img in celebrity_imgs], dtype=np.float64)\n",
    "# Extract real celebrity name from file name\n",
    "celebrity_names = [\n",
    "    name[: name.find(\"0\") - 1].replace(\"_\", \" \") for name in celebrity_photos\n",
    "]\n",
    "print(imgs[0].shape)\n",
    "show_faces(imgs, celebrity_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка данных:\n",
    "\n",
    "Преобразуем изображения в вектора и центрируем их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stretch to vector\n",
    "x = imgs.reshape(imgs.shape[0], 64 * 64)\n",
    "print(x.shape)\n",
    "mean = np.mean(x, axis=0)\n",
    "# Center: substract mean\n",
    "centered_faces = x - mean\n",
    "plt.imshow(mean.reshape(64, 64), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем собственные вектора. Аналогия с фотороботом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "# n_components == min(n_samples, n_features)\n",
    "\n",
    "# 1. Load the faces data.\n",
    "# 2. Apply PCA.\n",
    "# 3. Extract the principal components.\n",
    "# 4. Reshape the principal components so that they have the right shape.\n",
    "# 5. Display the reshaped principal components.\n",
    "\n",
    "pca_faces = sklearn.decomposition.PCA()  # 1000x4096\n",
    "\n",
    "pca_faces.fit(centered_faces)\n",
    "eigenfaces = pca_faces.components_\n",
    "reshaped_eigenfaces = eigenfaces.reshape((1000, 64, 64))\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(reshaped_eigenfaces.shape[0])]\n",
    "show_faces(reshaped_eigenfaces, eigenface_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Восстановим лица с использованием n < 4096 компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(img, n_components):\n",
    "    # Generate embedding for first image using only 10 first components\n",
    "    img = img.reshape(64 * 64) - mean\n",
    "    emb = np.dot(img, eigenfaces[:n_components].T)  # (1,4096) * (4096,1)\n",
    "    # print(emb,emb.shape) # 10 - 500 numbers only!\n",
    "\n",
    "    # Recover image from embedding\n",
    "    recovered_img = np.dot(emb, eigenfaces[:n_components])\n",
    "    recovered_img += mean  # shift by mean\n",
    "    return emb, recovered_img\n",
    "\n",
    "\n",
    "# Show images recovered from embeddings of various sizes\n",
    "original_img = imgs[0]\n",
    "titles = []\n",
    "img_list = []\n",
    "for n in [10, 25, 100, 500]:\n",
    "    embedding, recovered = create_embedding(original_img, n)\n",
    "    img_list.append(recovered)\n",
    "    titles.append(f\"Components {n}\")\n",
    "img_list.append(original_img)\n",
    "titles.append(\"Original\")\n",
    "\n",
    "show_faces(np.array(img_list, dtype=object), titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае требование на ортогональность компонент, выделяемых PCA, фактически мешает нам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы  PCA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Интересное направление в данных может не совпадать с направлением максимальной дисперсии.\n",
    "\n",
    "Рассмотрим случай выборки, которая сгенерирована из двух вытянутых нормальных распределений, чьи основные оси неортогональны друг другу:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/problem_pca_2.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выбранные оси могут вообще не подходить для нашей задачи\n",
    "\n",
    "\n",
    "В примере ниже дисперсии не отражают интересующих нас направлений в данных:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/problem_pca_1.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае метод главных компонент будет считать вертикальную компоненту более значимой для описания набора данных, чем горизонтальную.\n",
    "\n",
    "Но, например, в случае, когда данные из левого и правого кластера относятся к разным классам, для их линейной разделимости вертикальная компонента является шумовой. Несмотря на это, метод главных компонент никогда ее шумовой не признает, и есть вероятность, что отбор признаков с его помощью выкинет из ваших данных значимые для решаемой вами задачи компоненты просто потому, что вдоль них значения имеют низкую дисперсию.\n",
    "\n",
    "Справляться с такими ситуациями могут некоторые другие методы уменьшения размерности данных, например, метод независимых компонент (Independent Component Analysis, ICA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Недостатки линейного PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также бывают ситуации, когда оптимально спроецировать не на некоторую плоскость, а на многообразие (кривая плоскость), как показано на картинке ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/s_manifold.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае оптимально спроецировать на S-образную кривую.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA Ядровой (нелинейный) метод главных компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как уже упоминалось, иногда невозможно захватить всю информацию линейной проекцией, хотя кривая поверхность с такой же размерностью это позволяет сделать. Одним из подходов к решению данной проблемы является задача перевода признаков в нелинейное пространство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Trick избегает явного перевода наших признаков в пространство новых признаков, ведь пространства бывают очень большие, а нам бы хотелось сэкономить память компьютера.\n",
    "Оказывается, для PCA не важны собственно признаки объектов, а важны скалярные произведения между объектами.\n",
    "\n",
    "И это скалярное произведением позволяет подсчитывать напрямую функция $k(\\mathbf {x} ,\\mathbf {x'})$, которую часто называют *ядром или ядерной функцией (kernel, kernel function)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бывают разные ядра, которые считают скалярное произведение в разных пространствах\n",
    "\n",
    "\n",
    "* $\\displaystyle k(x_i, x_j) = \\frac{1}{z} e^{-\\frac{h(x_i, x_j)^2}{h}}$ — радиальная базисная функция (RBF)\n",
    "* $k(x_i, x_j) = (<x_i, x_j> + c)^d; c, d \\in \\mathbb{R}$ — полиномиальное ядро\n",
    "* $k(x_i, x_j) = \\sigma((<x_i, x_j>)$ — ядро с функцией активации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Make_circles creates a data set of 400 points that form concentric circles with a gap of 50 points.\n",
    "# 2. The factor parameter controls the size of the inner circles.\n",
    "# 3. The noise parameter controls the amount of noise added to the data.\n",
    "# 4. The result is a 360-feature dataset of concentric circles with gaps.\n",
    "\n",
    "x, y = make_circles(n_samples=400, factor=0.3, noise=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем две концентрические окружности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.title(\"Original space\")\n",
    "reds = y == 0\n",
    "blues = y == 1\n",
    "\n",
    "plt.scatter(x[reds, 0], x[reds, 1], c=\"red\", s=20, edgecolor=\"k\")\n",
    "plt.scatter(x[blues, 0], x[blues, 1], c=\"blue\", s=20, edgecolor=\"k\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычный PCA не может их разделить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "x_pca = pca.fit_transform(x)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(x_pca[reds, 0], x_pca[reds, 1], c=\"red\", s=20, edgecolor=\"k\")\n",
    "plt.scatter(x_pca[blues, 0], x_pca[blues, 1], c=\"blue\", s=20, edgecolor=\"k\")\n",
    "plt.title(\"Projection by PCA\")\n",
    "plt.xlabel(\"1st principal component\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот KernelPCA  справляется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# 1. Create a PCA object to perform the PCA transformation\n",
    "#    using the RBF kernel (specified using kernel=\"rbf\").\n",
    "#    Setting fit_inverse_transform=True. This will make the object use the\n",
    "#    transformed data from the first step when transforming new, unseen data points.\n",
    "# 2. Let the PCA object fit and transform the data,\n",
    "#    then get the transformed data back.\n",
    "\n",
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
    "x_kpca = kpca.fit_transform(x)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(x_kpca[reds, 0], x_kpca[reds, 1], c=\"red\", s=20, edgecolor=\"k\")\n",
    "plt.scatter(x_kpca[blues, 0], x_kpca[blues, 1], c=\"blue\", s=20, edgecolor=\"k\")\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(r\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя, конечно, и восстанавливать он будет не идеально: работал-то он в пространстве бОльшей размерности и оси строил там."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The 'kpca' variable is a KernelPCA object that is initialized with 'n_components' set to 2.\n",
    "# 2. Then it applies the kernel function specified in the 'kernel' variable  and then transforms the data based on the kernel, and gets the transformed data.\n",
    "# 3. Then it returns the transformed data.\n",
    "# 4. Then we get the inverse transformation by simply calling \"kpca.inverse_transform(x_kpca)\"\n",
    "# 5. Finally, we plot the transformed data.\n",
    "\n",
    "x_back = kpca.inverse_transform(x_kpca)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(x_back[reds, 0], x_back[reds, 1], c=\"red\", s=20, edgecolor=\"k\")\n",
    "plt.scatter(x_back[blues, 0], x_back[blues, 1], c=\"blue\", s=20, edgecolor=\"k\")\n",
    "plt.title(\"Original space after inverse transform\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel PCA довольно чувствителен к выбору ядра.\n",
    "\n",
    "К примеру, для данных, расположенных на трех окружностях:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/kernel_pca_three_circles.png\" width=\"260\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в зависимости от выбора ядра мы будем получать совершенно разные отображение в спрямляющее пространство:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/kernel_pca_different_kernels.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В связи с вышеописанными случаями, ниже мы рассмотрим более сильные методы.\n",
    "\n",
    "Существуют и другие методы понижения размерности в данных. К примеру, t-SNE и UMAP.\n",
    "\n",
    "Они немного иначе решают поставленную задачу: из пространства новой размерности мы не должны легко переходить в прежнее пространство. Взамен этого требуется, чтобы сохранялись расстояния между объектами. Причем, особое внимание уделяется расстояниям между близкими объектами. При этом расстояния между далекими объектами могут не сохраняться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE (t-distributed Stochastic Neighbor Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея состоит в том, чтобы не напрямую максимизировать дисперсию, а найти такое пространство, в котором расстояние между объектами будет сохраняться или по крайне мере не сильно меняться.\n",
    "\n",
    "При этом будем больше беспокоиться о расстоянии между близкими объектами, нежели о расстоянии между далекими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим алгоритм пошагово:\n",
    "\n",
    "**Шаг 1:** Вычисляем матрицу попарных сходств объектов в пространстве исходной размерности. Затем преобразуем попарные сходства в условные вероятности.\n",
    "\n",
    "Получаем, что объекты, считающиеся близкими в пространстве высокой размерности, имеют более высокую условную вероятность быть соседями и в пространстве низкой размерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/tsne_get_distance_matrix.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 2:** Иициализируем начальное положение объектов в пространстве низкой размерности. Так как мы сгенирировали его случайно, видим, что они сильно отличаются.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/tsne_two_matrix_distance.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 3:** Оптимизация. Осталось только привести сгенерированную матрицу к исходной. Делаем это с помощью градиентного спуска, минимизируя функцию потерь [дивергенцию Кульбака — Лейблера](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0):\n",
    "\n",
    "\n",
    "$$KL(P||Q) = \\sum_{i \\neq j}{p_{ij} log \\frac{p_{ij}}{q_{ij}}}$$\n",
    "\n",
    "где:\n",
    "\n",
    "$p_{ij}$ — условная вероятность близости между объектами $i$ и $j$ в пространстве исходной размерности\n",
    "\n",
    "$q_{ij}$ — условная вероятность близости между объектами $i$ и $j$ в пространстве низкой размерности\n",
    "\n",
    "\n",
    "В итоге мы хотим получить такое представление, чтобы объекты, которые находились близко в исходном пространстве, **наиболее вероятно** находились близко и в новом представлении.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Физическая аналогия:\n",
    "\n",
    "Представьте себе комнату, в которой в воздухе висят шарики (наши объекты), которые соединены недеформированными пружинками и время заморожено. Мы хотим из 3-мерного пространства (шарики в объеме комнаты), сделать представление в 2-мерном (шарики лежат на полу).\n",
    "\n",
    "Разморозив время, шарики упадут на пол. Тогда, пружинки начнут стягивать шарики, которые были близки, и оттолкнут шарики, которые упали близко, но были далеки. После того, как все шарики прекратят движение по полу и система из связанных шариков окажется в состоянии покоя, распределение расстояний между шариками на плоскости будет приближением для исходного распределения расстояний в объеме комнаты. Эта физическая аналогия покаызывает то, что происходит при минимизации дивергенции Кульбака — Лейблера.\n",
    "\n",
    "Можем увидеть такой эффект на демонстрации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://bost.ocks.org/mike/\n",
    "from IPython.display import HTML\n",
    "!wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L04/tSNE.html\n",
    "\n",
    "HTML(filename='./tSNE.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример применения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы t-SNE сходился лучше, в качестве изначальных координат точек в новом пространстве можно использовать не случайный шум, а первые две компоненты PCA.\n",
    "\n",
    "Преследуем две цели: уменьшить время работы t-SNE (который работает очень медленно) и убрать эффект шума на t-SNE — он может на него реагировать, особенно при условии, что схождения к минимуму мы можем не дождаться.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold\n",
    "\n",
    "# 1. Firstly we reduce the dimensionality of the data to 6 features using PCA.\n",
    "# 2. Then we take the first two PCA components and use this\n",
    "#    as an initial approximation for the T-SNE algorithm.\n",
    "# 3. Then we fit T-SNE on the data and plot the first two dimensions\n",
    "#    of the T-SNE output, which are represented in green.\n",
    "# 4. The visualization makes clear that there are distinct clusters in our data\n",
    "\n",
    "x_reduced = PCA(n_components=6).fit_transform(x_scRNAseq)\n",
    "model = sklearn.manifold.TSNE(\n",
    "    n_components=2,\n",
    "    init=x_reduced[:, 0:2],  # often use as a reasonable approximation\n",
    "    perplexity=40,  # important parameter\n",
    "    verbose=2,\n",
    "    learning_rate=\"auto\",\n",
    ")\n",
    "\n",
    "manifold = model.fit_transform(x_reduced)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(manifold[:, 0], manifold[:, 1], c=y_scRNAseq, cmap=\"tab20\", s=20)\n",
    "plt.title(\"TSNE: scRNAseq\", fontsize=25)\n",
    "plt.xlabel(\"TSNE1\", fontsize=22)\n",
    "plt.ylabel(\"TSNE2\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "И покрасим по разметке, которая нам известна из эксперимента. Видим, что разделение очень хорошее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важные параметры t-SNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`perplexity` — чем больше **perplexity**, тем более глобально мы смотрим на структуру.\n",
    "\n",
    "`metric` — как считаются расстояния между точками — **metric**. По умолчанию используется евклидово расстояние, но часто помогают и другие (например, косинусное).\n",
    "\n",
    "`learning_rate` — шаг градиентного спуска, тоже влияет на полученное представление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы t-SNE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Стохастичность**\n",
    "\n",
    "Низкоразмерное представление, которое вы получите, будет отличаться между\n",
    "запусками, если не зафиксировать **random seed**. Может отличаться довольно сильно.\n",
    "\n",
    "* **Добавление новых точек**\n",
    "\n",
    "Если у вас появились новые данные, то добавить их на  представление, полученное при помощи t-SNE ранее — нетривиальная задача.\n",
    "Для разных областей есть свои \"подгоны\", но все это эвристика.\n",
    "\n",
    "* **Расстояния не отражают реальную структуру**\n",
    "\n",
    "Расстояния между кластерами точек могут ничего не значить (плохо сохраняются далекие расстояния)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/tsne_problems_distances_between_clusters_no_matter.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Искажение размера кластеров**\n",
    "\n",
    "Размеры кластеров ничего не значат и могут сильно отличаться от исходных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/tsne_problems_cluster_size_no_matter.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Искажение структуры**\n",
    "\n",
    "Можно увидеть не ту структуру, которая по идее должна быть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/tsne_problems_erroneous_structure.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Подробнее](https://distill.pub/2016/misread-tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Из-за указанных  недостатков результат t-SNE нужно с осторожностью использовать для кластеризации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Хорошее видео про t-SNE](https://www.youtube.com/watch?v=NEaUSP4YerM)\n",
    "\n",
    "[Статья по применению t-SNE в биологии](https://www.nature.com/articles/s41467-019-13056-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[UMAP](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html) — uniform manifold approximation and projection. [Видео](https://www.youtube.com/watch?v=94ZMJ8tq1Wk)\n",
    "\n",
    "[Красивая демонстрация](https://pair-code.github.io/understanding-umap/)\n",
    "\n",
    "\n",
    "Использует похожую на t-SNE идею, но иначе, в результате чего получает много выгодных бонусов.\n",
    "\n",
    "Внутри себя метод строит граф, в котором ребрами соединены между собой $k$ ближайших соседей. При этом эти ребра неравноправны: если для данной пары точек расстояние между ними сильно больше, чем расстояния между ними и другими точками, то и ребро будет иметь маленький вес."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/umap.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее задача состоит в том, чтобы в пространстве более низкой размерности получился граф, похожий на тот, который был в пространстве высокой размерности.\n",
    "Для этого оптимизируем низкоразмерное представление градиентным спуском."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример применения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install umap-learn\n",
    "!pip install --upgrade tbb\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Converts the original expression matrix (scRNAseq) into a 9-dimensional PCA space\n",
    "x_reduced = PCA(n_components=9).fit_transform(x_scRNAseq)\n",
    "\n",
    "# Initializes UMAP with the PCA components\n",
    "model = UMAP(\n",
    "    n_components=2,\n",
    "    min_dist=1,\n",
    "    n_neighbors=93,\n",
    "    init=x_reduced[:, 0:2],\n",
    "    # it is recommended to use the first two components of PCA for initialization of UMAP and t-SNE\n",
    "    n_epochs=1000,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "# Runs the UMAP algorithm on the PCA transformed data\n",
    "umap = model.fit_transform(x_reduced)\n",
    "clear_output()\n",
    "# Plots the results of the UMAP transformation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(umap[:, 0], umap[:, 1], c=y_scRNAseq, cmap=\"tab20\", s=20)\n",
    "plt.title(\"UMAP: scRNAseq\", fontsize=25)\n",
    "plt.xlabel(\"UMAP1\", fontsize=22)\n",
    "plt.ylabel(\"UMAP2\", fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Важные параметры UMAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_neighbors` — число соседей, которые ищутся для каждой точки. Влияет на то, насколько глобально мы смотрим на структуру данных.\n",
    "\n",
    "\n",
    "`min_dist` — влияет на то, насколько близко могут находиться между собой точки в новом представлении.\n",
    "\n",
    "`metric` — определяет, как считаются расстояния между точками. По умолчанию Евклидово расстояние, но это не всегда дает лучший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преимущества перед t-SNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Скорость работы**\n",
    "\n",
    "Выдает результаты, похожие на t-SNE, но  работает в разы быстрее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L04/umap_tsne_performance.png\" width=\"450\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://umap-learn.readthedocs.io/en/latest/performance.html\">Performance Comparison of Dimension Reduction Implementations</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Возможность проецировать новые точки**\n",
    "\n",
    "UMAP может проецировать точки из новых датасетов на уже имеющееся представление.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Может объединять представления**\n",
    "\n",
    "Если у вас есть признаки, сильно отличающиеся по своим свойствам, то можно построить для них представления отдельно с разными метриками, а далее объединить их.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Более интуитивные параметры**\n",
    "\n",
    "Параметры `n_neighbours` и `min_dist` намного понятнее их аналога — `perplexity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Можно использовать для отображения и в пространства бо́льшей размерности**\n",
    "\n",
    "Кроме того, его можно использовать для понижения размерности не только до 2-3 (в целях визуализации), но и для больших размерностей, с которыми потом работать другими методами (хотя здесь надо быть аккуратным, он тоже склонен деформировать дальние расстояния)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Расстояния более информативны**\n",
    "\n",
    "Плохая идея — интерпретировать расстояния между кластерами и их размеры для 2D представлений.\n",
    "\n",
    "Но в случае UMAP это менее выражено.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если же отображать в пространство размерности большей, чем два, то можно получить и очень хорошие представления. Но это все эвристика, может и не повезти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Supervised UMAP**\n",
    "\n",
    "UMAP позволяет передать в него метки объектов — вы можете получать представление, которое оптимизировано под имеющуюся у вас информацию о кластерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Semi-Supervised UMAP**\n",
    "\n",
    "UMAP позволяет передать в него метки **только для части** объектов — вы можете получать представление, которое оптимизировано под имеющуюся у вас информацию о кластерах, но при этом не оставляет без внимания объекты без меток."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда говорят про задачу кластеризации подразумевают примерно следующее:\n",
    "\n",
    "У нас есть неразмеченные данные, и мы хотим поделить эти данные по каким-то группам, где в каждой группе будут объекты, похожие друг на друга. Под похожестью как правило понимается выбранная метрика расстояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/clustering_task.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные цели кластеризации:\n",
    "\n",
    "* Исследование данных. Мы хотим понять, какие кластеры являются ярко выраженными, и сделать дополнительные выводы о наших данных.\n",
    "\n",
    "* Нахождение аномалий/выбросов. Мы можем искать объекты, которые сильно отличаются от всех остальных и не принадлежат никакому кластеру.\n",
    "\n",
    "* Создание дополнительных признаков. Кластер для объекта уже является дополнительным категориальным признаком.\n",
    "\n",
    "* Непосредственно классификация. Например, можем считать получившиеся кластеры классами и классифицировать объекты таким образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмов кластеризации существует большое количество, и все работают по-разному, имеют разные параметры для настройки. Мы рассмотрим два наиболее популярных: K-Means и DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L04/sphx_glr_plot_cluster_comparison.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods\"> Overview of clustering methods</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/kmeans.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Выбираем количество кластеров $k$, которое нам кажется оптимальным для наших данных.\n",
    "2. Определяем начальное положение центроидов кластеров, случайным образом или по какому-либо правилу.\n",
    "3. Для каждой точки нашего набора данных считаем расстояние до центроидов. Относим точки к ближайшим центроидам.\n",
    "4. Перемещаем каждый центроид в центр получившихся кластеров.\n",
    "5. Повторяем шаги 3–4 до тех пор, пока не сойдемся (пока центроиды не перестанут значительно сдвигаться).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Интерактивная визуализация алгоритма K-Means](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенности K-Means:\n",
    "\n",
    "* Сильно зависит от параметра $k$ и начального положения центроидов.\n",
    "* Кластеры получаются гиперсферическими, с более сложными формами алгоритм не справляется.\n",
    "* Все точки относятся к какому то кластеру, явных выбросов нет.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L04/out/dbscan.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-based spatial clustering of applications with noise, плотностной алгоритм пространственной кластеризации с присутствием шума), как следует из названия, оперирует плотностью данных. На вход требуется матрица близости и два загадочных параметра — радиус $\\varepsilon$-окрестности и количество соседей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм выглядит следующим образом:\n",
    "1. Берем случайную точку из данных\n",
    "2. Если рядом с ней меньше трёх точек, переносим ее в список возможных одиночек и выбираем другую точку.\n",
    "\n",
    "Иначе:\n",
    "3. Исключаем ее из списка точек, которые надо посмотреть.\n",
    "4. Ставим ей зеленую метку и создаем новый кластер, к которому относится пока только одна точка.\n",
    "5. Просматриваем всех ее соседей. Если ее сосед уже в списке потенциальных одиночек и рядом с ним мало других точек, то перед нами край кластера. Для простоты можно сразу пометить его жёлтым флагом, присоединить к группе и продолжить обход. Если сосед тоже оказывается «зелёным», то он не стартует новую группу, а присоединяется к уже созданной. Кроме этого мы добавляем в список обхода соседей соседа. Повторяем этот пункт, пока список обхода не окажется пуст.\n",
    "6. Повторяем шаги 1–5, пока так или иначе не посмотрим все точки.\n",
    "7. Разбираемся со списком одиночек. Если мы уже раскидали всех краевых, то в нём остались только выбросы-одиночки — можно сразу закончить. Если нет, то нужно как-нибудь распределить точки, оставшиеся в списке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Подробнее про DBSCAN](https://habr.com/ru/post/322034/)\n",
    "\n",
    "[Интерактивная визуализация алгоритма DBSCAN](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Понижение размерности для улучшения качества кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы понижения размерности, такие как UMAP и t-SNE, могут быть использованы как эффективный метод предобработки данных для последующей передачи в [алгоритмы кластеризации на основе плотности](https://en.wikipedia.org/wiki/DBSCAN). Данные методы понижения размерности не полностью сохраняют информацию о плотности распределения объектов при осуществлении проекции в низкоразмерное пространство, поэтому следует относиться к такой предобработке с некоторой осторожностью (рекомендуем ознакомиться с [вот этим обсуждением](https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne) на  stackexchange). Несмотря на эти опасения, все же есть веские причины для использования данных методов понижения размерности в качестве этапа предварительной обработки для кластеризации. Как и при любом подходе к кластеризации, необходимо провести исследование и оценку полученных кластеров, чтобы попытаться валидировать разбиение на них, если это возможно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с одноканальными $8 \\times 8$ изображениями рукописных цифр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits(n_class=10)\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = x.shape\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понизим размерность данных $64 \\rightarrow 2$ при помощи t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "\n",
    "# t-SNE embedding of the digits dataset\n",
    "tsne = manifold.TSNE(n_components=2, init=\"pca\", random_state=42, learning_rate=\"auto\")\n",
    "x_tsne = tsne.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понизим размерность данных $64 \\rightarrow 2$ при помощи UMAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UMAP(n_neighbors=5)\n",
    "x_umap = umap.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import offsetbox\n",
    "\n",
    "\n",
    "def plot_embedding(x, title=None):\n",
    "    x_min, x_max = np.min(x, 0), np.max(x, 0)\n",
    "    x = (x - x_min) / (x_max - x_min)  # normalization of x to (0..1) range\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(x.shape[0]):\n",
    "        plt.text(\n",
    "            x[i, 0],\n",
    "            x[i, 1],\n",
    "            str(y[i]),\n",
    "            color=plt.cm.Set1(y[i] / 10.0),\n",
    "            fontdict={\"weight\": \"bold\", \"size\": 9},\n",
    "        )\n",
    "\n",
    "    if hasattr(offsetbox, \"AnnotationBbox\"):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_imgs = np.array([[1.0, 1.0]])  # just something big\n",
    "        for i in range(x.shape[0]):\n",
    "            dist = np.sum((x[i] - shown_imgs) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_imgs = np.r_[shown_imgs, [x[i]]]\n",
    "            img_box = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), x[i]\n",
    "            )\n",
    "            ax.add_artist(img_box)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(x.shape[0]):\n",
    "        plt.text(\n",
    "            x[i, 0],\n",
    "            x[i, 1],\n",
    "            str(y[i]),\n",
    "            color=plt.cm.Set1(y[i] / 10.0),\n",
    "            fontdict={\"weight\": \"bold\", \"size\": 9},\n",
    "        )\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE embedding of the digits dataset\n",
    "plot_embedding(x_tsne, \"t-SNE embedding of the digits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP embedding of the digits\n",
    "plot_embedding(x_umap, \"UMAP embedding of the digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как нетрудно заметить, при использовании UMAP точки из датасета при проекции в низкоразмерное пространство оказались расположены более \"кучно\", нежели чем при использовании t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем кластеризовать наши данные при помощи K-Means. Для оценки качества воспользуемся стандартными метриками [`adjusted_rand_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) и [`adjusted_mutual_info_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) из `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as cluster\n",
    "\n",
    "kmeans_labels_on_raw = cluster.KMeans(n_clusters=10, n_init=\"auto\").fit_predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "\n",
    "\n",
    "def plot_clustering_metrics(true_l, pled_l, title):\n",
    "    ari = adjusted_rand_score(true_l, pled_l)\n",
    "    ami = adjusted_mutual_info_score(true_l, pled_l)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    plt.title(f\"Clustering metrics for {title}\\n\\n 1.0 is best\")\n",
    "    width = 0.75\n",
    "    ind = np.arange(2)\n",
    "    ax.barh(ind, [ari, ami], width)\n",
    "    ax.grid(axis=\"x\")\n",
    "    ax.set_xlim([0, 1.0])\n",
    "    for i, v in enumerate([ari, ami]):\n",
    "        ax.text(v + 0.01, i, f\"{v:1.2f}\", color=\"black\")\n",
    "    ax.set_yticks(ind)\n",
    "    ax.set_yticklabels([\"ARI\", \"AMI\"], minor=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_metrics(y, kmeans_labels_on_raw, \"kNN on raw dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь передадим K-Means данные после понижения размерности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels_on_x_tsne = cluster.KMeans(n_clusters=10, n_init=\"auto\").fit_predict(\n",
    "    x_tsne\n",
    ")\n",
    "kmeans_labels_on_x_umap = cluster.KMeans(n_clusters=10, n_init=\"auto\").fit_predict(\n",
    "    x_umap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_metrics(y, kmeans_labels_on_x_tsne, \"kNN on t-SNE data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering_metrics(y, kmeans_labels_on_x_umap, \"kNN on UMAP data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяя методы понижения размерности на этапе предобработки данных, нам удалось значительно улучшить качество кластеризации. Использование UMAP в разобранном нами примере позволило достичь лучших результатов по сравнению с t-SNE и получить прирост практически в 20% относительно метрик качества кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошие источники:\n",
    "\n",
    "1. [Feature Selection for High-Dimensional Data](https://www.springer.com/gp/book/9783319218571)\n",
    "2. [How to Win a Data Science Competition: Learn from Top Kagglers](https://blog.coursera.org/learn-top-kagglers-win-data-science-competition/)\n",
    "3. [Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists Paperback](https://books.google.ru/books/about/Feature_Engineering_for_Machine_Learning.html?id=Ho0UvgAACAAJ&redir_esc=y)\n",
    "4. [Сайт](https://dyakonov.org/) и [курс](https://github.com/Dyakonov/PZAD) Дьяконова\n",
    "5. Серия статей на towardsdatascience, [первая из серии](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "6. [A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "7. [Про кодирование циклических признаков](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
