{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Введение в машинное обучение</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  AI, ML, DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место глубокого обучения и нейронных сетей в ИИ**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/ai_ml_dl.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Искусственный интеллект (AI/ИИ)**  — область IT/Computer science, связанная с моделированием интеллектуальных или творческих видов человеческой деятельности.\n",
    "\n",
    "**Машинное обучение (ML)** — подраздел ИИ, связанный с разработкой алгоритмов и статистических моделей, которые компьютерные системы используют для выполнения задач без явных инструкций. \n",
    "\n",
    "**Глубокое обучение (Deep Learning, DL)** — совокупность методов машинного обучения, основанных на искуственных нейронных сетях и обучении представлениям (**feature/representation learning**). Данный класс методов автоматически выделяет из необработанных данных необходимые признаки (представления), в отличие от методов ML, в которых признаки создают люди вручную (**feature engineering**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует множество определений сильного и слабого ИИ, рассуждений о появлении искусственного сознания и восстании машин.\n",
    "\n",
    "Всё намного **приземлённее**. Есть набор **объектов $X$**, набор **ответов $Y$**. Пары \"объект-ответ\" составляют **обучающую выборку**.\n",
    "\n",
    "Мы будем заниматься **восстановлением решающей функции $F$**, которая переводит признаки, описывающие объекты $X$, в ответы $Y$.\n",
    "\n",
    "$$ F: X \\xrightarrow\\ Y $$\n",
    "\n",
    "\n",
    "\n",
    "Позже мы уточним постановку задачи, увидим, что мы на самом деле восстанавливаем приближённую функцию ${\\hat F}$ с какой-то погрешностью, в каких-то задачах нет ответов $Y$, а где-то мы создаём новые объекты ${\\hat X}$ на основе исходных объектов $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Области применения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/ai_ml_dl_cv_nlp_sr.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последнее время именно такого рода модели показывают высокую эффективность в тех областях, с которыми ранее могли справиться только люди. В частности:\n",
    "* **человеко-компьютерное зрение** (Computer Vision, CV), \n",
    "* **распознавание и анализ речи** (NLP, извлечение смысла, Speech recognition, машинный перевод).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Связь с наукой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/ai_ml_dl_cv_nlp_sr_science.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Успехи происходят в тех областях, где производится моделирование человеческой деятельности. Но ведь мы ещё и думаем! Давайте попробуем добавить это в ИИ. \n",
    "\n",
    "Научные исследования таковы, что результаты у них в известной степени непредсказуемы. Одна из задач нашего курса — **научиться применять нейросети к решению новых задач**, в том числе в областях, где ранее такие технологии активно не использовались.\n",
    "\n",
    "В первую очередь для нас важны задачи слушателей курса, а успешным прохождением мы считаем **решённую научную задачу** и написанную по этому поводу **статью**. \n",
    "\n",
    "В течении 15 лекций мы будем рассказывать теорию и практиковаться, далее плотно займёмся научной работой. Хотя, в целом, её можно начинать уже прямо сейчас.\n",
    "\n",
    "Преподаватели будут выступать в качестве менторов и помогать вам с выбором подходящих моделей, проверкой гипотез, поиском ошибок. По ходу курса будет несколько воркшопов, где мы всем коллективом будем давать советы по вашим задачам. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обзор курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 1 Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая лекция посвящена базовым понятиям, описанию процессов загрузки и валидации данных, основным инструментам, оценке и валидации результатов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 2 Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Базовые алгоритмы**:\n",
    "\n",
    "*   Линейная регрессия\n",
    "*   Логистическая регрессия\n",
    "*   Полиномиальная регрессия\n",
    "\n",
    "**Работа с моделями**:\n",
    "\n",
    "*  Батчи, стохастикое обучение\n",
    "*  Регуляризация\n",
    "*  Кросс-энтропия\n",
    "\n",
    "Примерно так будет выглядеть исследование по решению регрессионной задачи с помощью полиномов различной степени. На код сейчас не обращайте внимание, мы будем разбирать подобные вещи позже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x = np.linspace(0, 2*np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "\n",
    "x_true = np.linspace(0, 2*np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "\n",
    "x_train = x.reshape(-1,1)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "for i, degree in enumerate([0,1,3,9]):\n",
    "\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1,1))\n",
    "\n",
    "    fig.add_subplot(2,2,i+1)\n",
    "    plt.plot(x_true, y_plot, c='red', label=f'M={degree}')\n",
    "    plt.scatter(x, y, s=50, facecolors='none', edgecolors='b')\n",
    "    plt.plot(x_true, y_true, c='lime')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 3 Классическое машинное обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Погружаемся в машинное обучение:\n",
    "\n",
    "*   Деревья решений\n",
    "*   Леса деревьев\n",
    "*   Градиентный бустинг\n",
    "*   Оценка важности признаков\n",
    "*   Метрики для оценки качества модели\n",
    "\n",
    "Больше узнаем об алгоритмах на основе деревьев решений. Научимся строить ансамбли моделей. Узнаем, что такое бустинг.\n",
    "\n",
    "Начнём анализировать работу \"чёрных ящиков\" и понимать, какие признаки важнее.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L03/out/random_forest.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 4 Генерация и отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим внимательнее на данные:\n",
    "\n",
    "*  Отбор признаков\n",
    "*  Методы понижения размерности\n",
    "*  Визуализация многомерных данных\n",
    "\n",
    "Поговорим о том, как не только посмотреть на красивые картинки с нашими данными, но и улучшить качество работы алгоритмов, выкидывая лишние признаки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L04/pca_tsne_umap_on_mnist.jpg\" width=\"850\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 5 Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В леции будет подробно рассказано о том, как учатся нейронные сети. Посмотрим на:\n",
    "\n",
    "*  Автоматический подсчёт градиентов в PyTorch\n",
    "*  Функции активации\n",
    "*  Визуализацию процесса обучения\n",
    "*  Критерии прекращения обучения\n",
    "*  Подбор оптимальных парамеров моделей\n",
    "\n",
    "Так, например, вы научитесь понимать вот такие вот графики функций активации и поймёте, зачем вообще они нужны:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/activation-function.png\" width=\"600\" ></center>\n",
    "\n",
    "<center><em>CS231n: Deep Learning for Computer Vision</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 6 Свёрточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научимся работать с двухмерными данными на примере изображений:\n",
    "\n",
    "*  Свёртки\n",
    "*  Свёрточные слои в нейросети, их параметры\n",
    "*  Построение свёрточных нейросетей (CNN)\n",
    "*  1D и 3D свёртки\n",
    "\n",
    "Посмотрим, как решать реальные задачи с помощью CNN, создавать эмбеддинги. \n",
    "А ещё на то, как понижать размерность данных и делать кластеризацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Поиск болезней у растений**\n",
    "\n",
    "Классический пример — автоматизация поиска болезней у растений. Более 40% урожая теряется из-за несвоевременного нахождения больного растения — приходится уничтожать изрядную часть урожая.\n",
    "\n",
    "Кроме того, отслеживание корректного роста маржинальных растений (например, грибов ресторанного типа) позволяет добиться контролируемого результата в аграрном хозяйстве.\n",
    "\n",
    "Своевременное определение болезни на ранней стадии позволяет купировать проблему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/gentelmens.jpg\" width=\"700\" ></center>\n",
    "<center><em>Кадр из к/ф Джентельмены</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[J.A.R.V.I.S. и помидорки](https://habr.com/ru/post/679218/) \n",
    "\n",
    "[A Review of Machine Learning Approaches in Plant Leaf Disease Detection and Classification](https://ieeexplore.ieee.org/document/9388488)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 7 Улучшение сходимости нейросетей и борьба с переобучением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научимся учить сети эффективно. Затронем:\n",
    "\n",
    "* Регуляризацию весов и её эффект\n",
    "* Как правильно нормализовать данные на входе?\n",
    "* Как заставить отдельные нейроны учить простые паттерны?\n",
    "* Как правильно инициализировать начальные веса?\n",
    "* Как их оптимизировать в процессе?\n",
    "* Как учить глубокую сеть?\n",
    "\n",
    "Поговорим о практических приёмах обучения, о доверии к предсказанию модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/batchnorm.jpg\" width=\"700\" ></center>\n",
    "<center><em>Ayoosh Kathuria. blog.paperspace.com/</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 8 Реккурентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Познакомимся с новым типом нейронных сетей, который умеет учитывать зависимости в последовательностях данных. В таких, как котировки акций или видеоряды. А ещё с помощью них можно переводить тексты! И не только.\n",
    "\n",
    "Узнаете, что означают такие слова, как:\n",
    "\n",
    "* RNN\n",
    "* LSTM\n",
    "* GRU\n",
    "\n",
    "А ещё поговорим о том, как анализировать работу таких сетей применительно к текстам и сигналам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/nlp_speech_recognition.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Область применения DL не ограничивается только изображениями. \n",
    "\n",
    "Они также весьма эффективны для распознавания голоса и машинного перевода.\n",
    "В отличие от изображений, входные данные имеют другую структуру: это последовательность, длина которой заранее неизвестна.\n",
    "\n",
    "Для работы с такого рода данными используются сети другого типа — рекуррентные (RNN — **Recurrent Neural Networks**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/rnn_architecture.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные подаются последовательно.\n",
    "Каждый элемент данных может оказывать влияние на выход модели и менять ее состояние. \n",
    "\n",
    "Выход модели — это тоже последовательность (например, вход — это предложение на английском языке, выход — на русском). Преобразование одной последовательности в другую — это задача трансформации (**seq2seq**).\n",
    "\n",
    "\n",
    "Дальше в курсе вы узнаете о развитии идей RNN, об архитектуре под названием **Transformer**, котрая практически вытеснила RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 8 Трансформеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем, что у рекуррентных сетей есть ряд проблем, которые решаются с помощью архитеутур под названием \"Трансформеры\". Раскроем понятие механизма внимания - тем, на основе чего зиждется эта архитектура.\n",
    "\n",
    "* Трансформеры для текста\n",
    "* Трансформеры для изображений\n",
    "* Трансформеры для других задач\n",
    "\n",
    "А как можно научить свой трансформер в Colab? А как использовать уже готовый для своей задачи? А как анализировать его работу?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью Трансформеров решаются задачи в совершенно разных доменов. Например, детектировать землетрясения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Earthquake transformer—an attentive deep-learning model for simultaneous earthquake\n",
    "detection and phase picking](https://www.nature.com/articles/s41467-020-17591-w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/bert0.png\" width=\"700\" ></center>\n",
    "<center><em>chernobrovov.ru</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 9 Архитектуры CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем, что у рекуррентных сетей есть ряд проблем, которые решаются с помощью архитеутур под названием \"Трансформеры\". Раскроем понятие механизма внимания — того, на основе чего зиждется эта архитектура.\n",
    "\n",
    "* Трансформеры для текста\n",
    "* Трансформеры для изображений\n",
    "* Трансформеры для других задач\n",
    "\n",
    "А как можно научить свой трансформер в Colab? А как использовать уже готовый для своей задачи? А как анализировать его работу?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L09/out/senet_architecture.png\"  width=\"1000\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 10 Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детально рассмотрим фишки, которые могут помочь вам в исследованиях.\n",
    "\n",
    "Изучим методы Explainability для методов ML и DL, включающие такие методики, как визуализация весов и карт активаций нейросети, вывод отдельных частей изображения, соответствующих различным классам, и многое другое.\n",
    "\n",
    "*  SHAP\n",
    "*  LIME\n",
    "*  Grad-Cam\n",
    "\n",
    "Например, изучим, как именно нейросети определяют то, что изображено на картинке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/lime.png\" width=\"800\" ></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://homes.cs.washington.edu/~marcotcr/blog/lime/\">LIME — Local Interpretable Model-Agnostic Explanations\n",
    "</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 11 Обучение на реальных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Столкнёмся с жестоким реальным миром:\n",
    "\n",
    "* Несбалансированные данные и методы работы с ними\n",
    "* Как быть, если данных мало?\n",
    "* Как быть, если данных совсем-совсем мало?\n",
    "* Как обрабатывать данные с разной модальностью?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Классификация сложных трёхмерных объектов**\n",
    "\n",
    "В реальных задачах могут возникнуть такие специфические данные, как, например, 3D CAD модели. Данные крайне несбалансированы! Болтиков сотни различных вариантов, тогда как специфичных агрегатов единицы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения проблемы с классификацией таких разнородных объектов можно делать массу вещей:\n",
    "* искать похожие данные\n",
    "* использовать специальные предобученные сети\n",
    "* генерировать новые данные\n",
    "* специальные метрики и штрафы при обучении\n",
    "* специальные стратегрии обучения\n",
    "\n",
    "Это и многое другое мы и будем обсуждать на лекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/transferlearning.png\" width=\"700\" ></center>\n",
    "\n",
    "\n",
    "<center><em>medium.com/nerd-for-tech/domain-adaptation-problems-in-machine-learning-ddfdff1f227c</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 12 Сегментация и детектирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научимся решать новые задачи: выделять отдельные сегменты в данных и детектировать объекты.\n",
    "\n",
    "* Варианты постановки задач\n",
    "* Наборы данных и структуры датасетов\n",
    "* Специальные метрики для этих задач\n",
    "* Типичные архитектуры\n",
    "* А что делать, если несколько объектов разом на картинке? \n",
    "* А как оценить качество?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Детектирование**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/object_detection.jpg\" width=\"500\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[КЛАССИФИКАЦИЯ](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8) + [РЕГРЕССИЯ](https://proglib.io/p/ml-regression) ~= [ДЕТЕКТИРОВАНИЕ](https://robocraft.ru/blog/computervision/3640.html/)\n",
    "\n",
    "[Nvidia example: How Does a Self-Driving Car See?](https://blogs.nvidia.com/blog/2019/04/15/how-does-a-self-driving-car-see/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача поиска местоположения объекта в кадре —  задача **детектирования**. Она тесно связана с задачей классификации:\n",
    "\n",
    "* Сначала обучается сеть, которая **классифицирует** изображения. То есть определяет, что на изображении присутствует человек. \n",
    "\n",
    "* Затем к такой сети добавляются несколько слоев, которые предсказывают координаты объектов. То есть решают задачу **регрессии**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знание координат объектов вокруг позволяет решать различные более сложные задачи: \n",
    "\n",
    "• Трекинг (отслеживание перемещения);\n",
    "\n",
    "• Предсказание действий;\n",
    "\n",
    "• SLAM (*simultaneous localization and mapping* — одновременная локализация и построение карты);\n",
    "\n",
    "• Оценка расстояний до объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://www.researchgate.net/profile/Udo-Frese/publication/220633576/figure/fig1/AS:671529876598789@1537116607128/What-is-Simultaneous-Localization-and-Mapping-SLAM-A-robot-observes-the-environment.png\" width=\"600\" >\n",
    "<center><em>SLAM. Робот наблюдает за окружающей средой относительно своей собственной неизвестной позы. Также измеряется относительное движение робота. На основе этих входных данных алгоритм SLAM вычисляет оценки позы робота и геометрии окружающей среды. Камера робота измеряет относительное положение искусственных элементов на полу (желтые линии). Результат: положение робота в пространстве и его поза относительно кружков на полу. </em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сегментация**\n",
    "\n",
    "Порой недостаточно знать, что на изображении есть объект определенного класса. Важно, где именно расположен объект. В ряде случаев нужно знать еще и точные границы объекта. Например, если речь идет о рентгеновском снимке или изображении клеток ткани, полученном с микроскопа.\n",
    "\n",
    "**Сегментация — определение того, какие фрагменты изображения принадлежат объектам определенных классов**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/segmentation.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[U-Net: нейросеть для сегментации изображений](https://neurohive.io/ru/vidy-nejrosetej/u-net-image-segmentation/)\n",
    "\n",
    "Задача: поиск аномалий (опухолей) и определение их четких границ.\n",
    "\n",
    "Эта задача очень похожа на детектирование. Нужно найти, где находится объект. Но в данном случае нужно найти четкие границы. Желательно с точностью до пикселя. То есть **для каждого пикселя нужно предсказать, к какому объекту он относится**. \n",
    "\n",
    "**Получается попиксельный классификатор.**\n",
    "\n",
    "Но проблема состоит в том, что пикселей много, и решать её в лоб — неэффективно.\n",
    "\n",
    "Поэтому при сегментации используется подход, подобный тому, что при **кластеризации лиц:**\n",
    "\n",
    "• Часть нейросети **сжимает изображение** — получается **карта признаков** (карта, потому что структура сохраняется);\n",
    "\n",
    "• На этой карте **предсказываются границы** объектов;\n",
    "\n",
    "• Вторая часть сети (симметрично первой) **восстанавливает размеры**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 13 Автоэнкодеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим архитектуры автокодировщика и где их применяют:\n",
    "\n",
    "* Очистка данных от шумов\n",
    "* Понижение размерности данных\n",
    "* Извлечение зависимостей из данных\n",
    "\n",
    "Коснёмся проблем при их обучении, развитием архитектур для решения разнообразных задач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Поиск аномалий**\n",
    "\n",
    "Все технологии, о корых мы упомянули, подразумевали наличие **размеченных данных**.\n",
    "Но как быть, если требуется **обнаружить что-то** абсолютно **новое**? Например, инопланетный космический корабль, которго никто никогда раньше не видел?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/supervised_learning.png\" alt=\"alttext\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/unsupervised_learning.png\" alt=\"alttext\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этой задачи так же используется энкодер-декодер подход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Автоэнкодер**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/nn_encoder_nn_decoder.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность входных данных понижается. Фактически изображение превращется в embedding. Затем из него восстанавливаеется исходное изображение.\n",
    "\n",
    "Такую модель можно учить на имеющихся снимках, без необходимости их размечать.\n",
    "\n",
    "\n",
    "И модель научится восстанавливать звезды, планеты галлактики - все объекты которые обычно поподают в поле зрения телескопа. Однако НЛО она восстановить не соможет. И сравнив исходное изображение с восстановленным мы сможем обнаружить редкий объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Очистка данных**\n",
    "\n",
    "Ту же технику можно применять для удаления шума из данных\n",
    "\n",
    "Очистка снимков полученных с крипто-электронного микроскопа **cryo-EM** при помощи каскада автоэнкодеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://www.frontiersin.org/files/Articles/627746/fgene-11-627746-HTML/image_m/fgene-11-627746-g004.jpg\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первом ряду находятся исходные изображения. Во 2-м ряду показаны проекции структуры, в 3-м ряду показаны изображения с шумоподавлением, а в последнем ряду показаны изображения с шумоподавлением.\n",
    "\n",
    "[2021 CDAE: A Cascade of Denoising Autoencoders for Noise Reduction in the Clustering of Single-Particle Cryo-EM Images](https://www.frontiersin.org/articles/10.3389/fgene.2020.627746/full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 14 Генеративные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта лекция будет сильно связана с предыдущей.\n",
    "\n",
    "* Что нам сделать, чтобы создать новые данные, которых ранее не существовало?\n",
    "* Или перенести стиль с одной картинки на другую? \n",
    "* А если мы хотим генерировать объекты с заданными свойствами?\n",
    "\n",
    "Также мы рассмотрим альтернативные генераторы данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/face_generation_gan.png\" width=\"1000\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На базе концепции энкодер-декодер появились **генеративно-состязательные сети**.\n",
    "**Сжатое представление**, которое получается на внутреннем слое сети, содержит **ключевые признаки** изображения, по которым декодер восстанавливает изображение.\n",
    "\n",
    "Но сжатое представление содержит не всю информацию об изображении. А в зависимости от того, как будет идти процедура восстановления, мы можем получить **разные вариации** изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Почему состязательные?*\n",
    "\n",
    "Поскольку сеть генерирует совершенно новое изображение, у нас нет шаблона, с которым мы могли бы его сравнить и дать оценку работы модели.\n",
    "Поэтому для оценки качества работы первой сети (генератора) обучается вторая (дискриминатор), которая дает ответ на вопрос: похоже ли полученное изображение на настоящее или нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/style_transfer_gan.jpg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С результатами работы таких моделей вы все хорошо знакомы по соцсетям и мобильным приложениям.\n",
    "Помимо использования в развлекательных целях, эта технология широко используется и в научных работах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Очистка изображений галактик**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/delete_noise_gan.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Подробнее}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображения, полученные при помощи телескопов, оказываются зашумленными по причинам:\n",
    "- атмосферных помех;\n",
    "- шумам, которые даёт сенсор телескопа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классический способ борьбы с этой проблемой состоит в подборе сигнала, похожего на суммарный шум (point spread function (PSF)), и последующей сверке изображения с этим сигналом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[В данной работе](https://academic.oup.com/mnrasl/article/467/1/L110/2931732) для решения той же проблемы авторы статьи обучили GAN.\n",
    "В качестве входных данных использовались изображения галактик.\n",
    "\n",
    "*4550 galaxies from the Sloan Digital Sky Survey Data Release 12 (York et al. 2000; Alam et al. 2015)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/galaxys_after_gan.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображения преобразовывались в RGB формат. И к ним искусственно добавлялись искажения, имитирующие шум от сенсора и размытие, возникающее за счёт потоков воздуха в атмосфере (PSF).\n",
    "Датасет состоял из пар зашумленных и чистых изображений.\n",
    "\n",
    "Генератор обучался на основе изображения с шумом генерировать чистое изображение, а дискриминатор отличал зашумленные изображения от чистых.\n",
    "\n",
    "На первый взгляд результат впечатляет. Однако авторы признают, что на фотографиях, где присутствуют объекты, редко появляющиеся в датасете, результат хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лекция 15 Обучение с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К этой лекции мы уже подробно познакомимся с различными видами обучения. Данная лекция будет ещё об одном, сильно отличающемся типе обучения, основывающемся на понятиях **среда** и **субъект**, который в ней действует, получая награды.\n",
    "\n",
    "Например, в эту область входит такая задача: какую статегию нужно выбрать студенту МГУ в этом семестре? Пойти ли на курс по нейросетям в науке или пойти работать в криптостартап? А может быть лучше посвятить этот семестр медитациям и чтению Капитала? А что будет эффективнее на промежутке в 5 лет?\n",
    "\n",
    "Рассмотрим стандартные модельные задачи вроде балансировки маятника на тележке, посмотрим на программные пакеты, которые помогают решать подобные задачи, а также научимся писать своё решение с нуля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/gangnam.gif\" width=\"900\" ></center>\n",
    "\n",
    "<center><em>Learning Acrobatics by Watching YouTube.\n",
    "Xue Bin (Jason) Peng and Angjoo Kanazawa  </em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Исследования в Беркли по обучению роботов двигаться](https://bair.berkeley.edu/blog/2018/10/09/sfv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Комбинированные задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что существуют работы, которые комбинируют в себе несколько задач разом. \n",
    "\n",
    "Зная структуру белка, можно рационально подойти к созданию лекарственного препарата, который должен с ним взаимодействовать. Или скорректировать крутой существующий белок из животных так, чтобы он не вызывал иммунного ответа у человека, и лечить человека им. А если обладать возможностью по последовательности строить структуру белка, то можно даже сделать свой искусственный белок, который будет выполнять функцию, для которой не существует белка природного — например, расщеплять пластик. В общем, пространство для маневра не ограничено, а горизонты широки.\n",
    "\n",
    "Именно поэтому над проблемой получения структуры белка бьются многие научные (и не только) группы. Авторы AlphaFold2 обучают нейросеть, которая предсказывает расстояния и углы между атомами аминокислот в конечном белке, а также предсказывает структуру белка в 3D-виде."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaFold появился на свет благодаря тому, что члены его команды обладали компетенциями в области биологии, физики, математики, алгоритмов глубокого обучения и оптимизации — то есть в области вычислительной биологии. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/alphafold.png\" width=\"1000\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Подробнее про AlphaFold}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент существует важная задача — фолдинг белка. Белок синтезируются в клетке в виде соединённых последовательно аминокислот. Эта последовательность обычно известна и кодируется в геноме.\n",
    "\n",
    "Однако после цепочка будет складываться в сложные пространственные структуры и от того, как эта структура будет выглядеть, зависит то, как белок будет себя вести в дальнейшем. Соответственно, понимание его особенностей служит ключом для производства лекарств и интерпретации генетических аномалий. \n",
    "\n",
    "В геноме человека могут происходить мутации — ошибки, которые могут приводить к изменению аминокислотной последовательности белка. \n",
    "\n",
    "Далеко не все мутации, которые есть в геноме, приведут к проблемам в конечной структуре, и если мы заранее будем знать, что одна мутация белка с дефектом, а другая безопасная, мы сможем предсказать вероятность развития болезни. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку эти связи обусловлены достаточно сложными внутримолекулярными взаимодействиями, просчитать заранее, как будет выглядеть молекула, достаточно трудозатратно. На каждом этапе формирования действуют разные связи между аминокислотами (водородные связи, гидрофобные взаимодействия и пр.), тем самым образуя сложную структуру.\n",
    "\n",
    "Рассчитать аналитически структуру, которая образуется в ходе этих взаимодействий, сложно, но можно применить для этого нейросеть, подав ей аминокислотную последовательность белка. \n",
    "\n",
    "Авторы AlphaFold2 обучают нейросеть, которая предсказывает расстояния и углы между атомами аминокислот в конечном белке, причем делает это итеративно, улучшая предсказание с предыдущего шага. Далее, специальный алгоритм преобразует это в набор координат атомов, что и является пространственной структурой белка. \n",
    "Этот трехмерный массив координат атомов можно визуализировать, и дальше структурные биологи могут делать вывод о том, получится конкретный белок дефектным или нет. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А эти знания уже применяются в области производства лекарств. Например, есть база веществ и известно, какая у них структура. Имея структуру белка, можно предсказать, в каком месте и с какой формой белка они могут и должны соединяться. Соответственно, по базе данных можно найти вещества, которые будут с этим белком связаны и уже подобрать из имеющихся “претендентов” тот, который наилучшим образом будет работать. \n",
    "\n",
    "Про мутации. Есть базы данных нормальных и мутагенных белков, и мы можем сравнить то, что получилось у нас в результате фолдинга с эталоном и посмотреть, насколько большая разница в структурах (опять же, для этого нужны структурные биологи, так как не все части структуры белка равнозначны). И на основе этого уже предсказать, насколько опасна будет та или иная мутация. \n",
    "\n",
    "\n",
    "\n",
    "[Заявление от deepmind](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery)\n",
    "\n",
    "[Мнение структурных биологов](https://yakovlev.me/para-slov-za-alphafold2/?fbclid=IwAR23L8XigP7byPcx10o-4y5L3VTLRzDmuioqw99iKZ0SkPrauezrJJJayiM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/alphafold_nn.png\" width=\"900\" >\n",
    "\n",
    "[Improved protein structure prediction](https://www.nature.com/articles/s41586-019-1923-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Базовые задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае **задача классификация выглядит следующим образом**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/classification_task.png\" width=\"700\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация — **отнесение образца к одному из нескольких попарно не пересекающихся множеств**.\n",
    "\n",
    "В качестве образцов могут выступать различные по своей природе объекты, например: \n",
    "* символы текста, \n",
    "* изображения, \n",
    "* звуки. \n",
    "\n",
    "При обучении сети предлагаются **пары образец - класс**. Образец, как правило, представляется как **вектор значений признаков**. При этом совокупность всех признаков должна однозначно определять класс, к которому относится образец. В случае, если признаков недостаточно, сеть может соотнести один и тот же образец с несколькими классами, что неверно. По окончании обучения сети ей можно предъявлять неизвестные ранее образы и получать ответ об их принадлежности к определённому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/regression_task.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Способности нейронной сети к прогнозированию напрямую следуют из её **способности к обобщению** и **выделению скрытых зависимостей** между входными и выходными данными. После обучения сеть способна **предсказать будущее значение** некой последовательности на основе нескольких предыдущих значений и (или) каких-то существующих в настоящий момент факторов. \n",
    "\n",
    "Прогнозирование возможно только тогда, когда предыдущие изменения действительно в какой-то степени предопределяют будущие. Например, прогнозирование котировок акций на основе котировок за прошлую неделю может оказаться успешным (а может и не оказаться), тогда как прогнозирование результатов завтрашней лотереи на основе данных за последние 50 лет почти наверняка не даст никаких результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/clustering_task.png\" width=\"700\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластеризация — **разбиение множества входных сигналов на классы, при том, что ни количество, ни признаки классов заранее не известны**. После обучения модель способна определять, к какому классу относится входной сигнал. Модель также может сигнализировать о том, что входной сигнал не относится ни к одному из выделенных классов — это является признаком новых, отсутствующих в обучающей выборке, данных. Таким образом, подобная модель может выявлять новые, неизвестные ранее классы сигналов. Соответствие между классами, выделенными сетью, и классами, существующими в предметной области, устанавливается человеком.\n",
    "\n",
    "Относится к задачам **обучения без учителя**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План исследования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где можно добыть данные?\n",
    "\n",
    "* Эксперименты в вашей лаборатории\n",
    "* [Соревнования Kaggle](https://www.kaggle.com/)\n",
    "* [Google Datasets](https://datasetsearch.research.google.com/)\n",
    "* [Сайт Papers with Code](https://paperswithcode.com/)\n",
    "\n",
    "Пройдитесь по соседним лабораториям. Напишите письма авторам статей.\n",
    "\n",
    "По [ссылке](https://msu.ai/tpost/m6zdyji4l1-opublikovana-statya-na-habre) представлен гайд о том, какие типичные ошибки совершают исследователи. Изучите его подробно, когда будете обрабатывать данные. Особенно, если они собраны из нескольких источников."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы используете данные, скачанные из сети, проверьте, откуда они. Описаны ли они в статье? Если да, посмотрите на документ; убедитесь, что он был опубликован в авторитетном месте, и проверьте, упоминают ли авторы какие-либо ограничения используемых датасетов.\n",
    "\n",
    "Если данные использовались в ряде работ, это не гарантирует его высокое качество. **Иногда данные используются только потому, что их легко достать**.\n",
    "\n",
    "Даже широко распорстранённые датасеты могут иметь ошибки или какую-то странную специфику. Например, при исследовании **ImageNet** были обнаружены миллионы изображений темнокожих, которые были помечены как \"преступник\". В итоге большая часть набора данных ImageNet была удалена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/imagenet_bugs.png\" width=\"700\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют [исследования](https://arxiv.org/abs/2211.01866), которые связывают странное поведение современных нейронных сетей и ошибки в разметке.\n",
    "\n",
    "Если вы обучаете свою модель на плохих данных, то, скорее всего, у вас получится плохое решение задачи. Существует соответствующий термин **garbage in garbage out**. Всегда начинайте с проверки данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите **разведывательный анализ данных**. Ищите недостающие или непоследовательные записи. Гораздо проще сделать это сейчас, до обучения модели, чем потом, когда вы будете пытаться объяснить рецензентам, почему вы использовали плохие данные.\n",
    "\n",
    "Анализ важно провести независимо от того, используете ли вы существующие наборы данных или генерируете новые данные в рамках своего исследования (в этом случае учитывайте, что разведывательный анализ может быть ценен сам по себе, а результаты этого анализа могут стать важной частью вашей статьи)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отдельным пунктом необходимо отметить, что помимо \"содержания\", важна и \"форма\" данных. Формат хранения ваших данных повлияет на скорость, с которой вы сможете завершить свое исследование. Например, у вас есть массив, который называется `ID` и в нем хранятся следующие данные `[1,30,111,221,234]` в формате `float64`. Проверьте, а точно ли тут нужен `float64`, возможно, ваши данные представлены целыми положительными числами, и для их хранения будет достаточно формата `uint32` или даже uint16 (подробный обзор форматов данных в [Understanding Data Types](https://jakevdp.github.io/PythonDataScienceHandbook/02.01-understanding-data-types.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разберем на конкретном примере**\n",
    "Скачаем датасет: **\"Когда и где кого-то покусала собака в NYC**\" и загрузим его в pandas. Подробно посмотрим только на 2 признака. Подробный анализ смотри [тут](https://habr.com/ru/post/664102/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "\n",
    "#!wget https://data.cityofnewyork.us/api/views/rsgh-akpg/rows.csv?accessType=DOWNLOAD -O dogs.csv\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L01/dogs.csv -O dogs.csv\n",
    "\n",
    "# Load into pandas and display a sample\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('dogs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на содержание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, есть ли дубликаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dataset) == len(dataset.drop_duplicates()):\n",
    "    print('Очевидных дупликатов нет')\n",
    "else:\n",
    "    print('%.2f процентов данных являются дубликатами' % len(dataset.drop_duplicates())/len(dataset) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UniqueID**\n",
    "\n",
    "Мы ожидаем, что в этой колонке каждому объявлению был присвоен уникальный `ID`. Судя по сэмплу, это просто порядковый номер, начинающийся с 1. Можем визуализировать эту колонку, что бы убедиться что там никаких сюрпризов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(len(dataset))\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('UniqueID')\n",
    "plt.scatter(x, dataset['UniqueID'], s=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что уникальных идентификаторов меньше чем строк в датафрейме. Давайте убедимся:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['UniqueID'].max(), len(dataset['UniqueID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть ID повторяются? Судя по всему, в какой-то момент времени нумерация была запущена заново. А значит ID совсем даже не unique => использовать эту колонку как уникальный идентификатор мы не можем.\n",
    "\n",
    "В каком формате хранятся данные в этой колонке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['UniqueID'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В int64 можно записывать целые числа в диапазоне **от -9223372036854775808 до 9223372036854775807**. Мы уже по графику видим, что знак нам не нужен, и что наше максимальное значение явно меньше. Определим какой у нас максимум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['UniqueID'].min(), dataset['UniqueID'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значит, нам подойдет **uint16** целое число без знака в диапазоне от 0 до 65535."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered = dataset.copy()\n",
    "dataset_filtered['UniqueID'] = dataset['UniqueID'].astype('uint16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сколько памяти мы выиграли?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resources_gain(column = 'UniqueID', orig_dataset=dataset, filtered_dataset=dataset_filtered):\n",
    "    original_memory = orig_dataset[column].memory_usage(deep=True)\n",
    "    memory_after_conversion = filtered_dataset[column].memory_usage(deep=True)\n",
    "    gain = original_memory/memory_after_conversion\n",
    "    print(f'Gain: {round(gain, 2)}')\n",
    "\n",
    "resources_gain(column='UniqueID', orig_dataset=dataset, filtered_dataset=dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь колонка UniqueID занимает в 4 раза меньше места (а значит, и обрабатывается быстрее)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DateOfBite**\n",
    "\n",
    "В **DateOfBite**, судя по всему, записано время укуса, но в формате `str`. Нам было бы удобнее работать с timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['DateOfBite'] = pd.to_datetime(dataset['DateOfBite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим выигрыш в ресурсах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_gain(column='DateOfBite', orig_dataset=dataset, filtered_dataset=dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим нет ли каких-то странных дат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['DateOfBite'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С датами все в порядке. Кстати можно заметить, что во время Ковида собакам было меньше кого кусать =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение закономерностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечение закономерностей — закон Ньютона \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/newtons_law.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе наблюдений люди выявляют закономерности и делают обобщение, наблюдая за реальным миром.\n",
    "Результатом такой умственной деятельности является модель, описывающая некоторые процессы реального мира.\n",
    "\n",
    "Она может быть описана при помощи математических формул или алгоритмического языка.\n",
    "\n",
    "Сейчас появилась технология, которая может это делать вместо человека."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/newtons_law_and_nn.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Это ML**\n",
    "\n",
    "**ML** — это технология, которая позволяет выявлять закономерности в данных и обобщать их. \n",
    "\n",
    "Вы можете использовать её для поиска закономерностей, которые люди ещё не обнаружили и таким образом совершить открытие.\n",
    "Правда, результатом обучения такой модели будет не компактная формула, а набор весов. \n",
    "По сути это набор коэффициентов для некоторого математического выражения.\n",
    "\n",
    "Для этого нужно две вещи: **данные** и **валидация результата.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/predictions_by_nn.png\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как человеку, так и алгоритму машинного обучения требуется подготовка данных.\n",
    "\n",
    "Законы Ньютоны **не** сформулированы для яблок. Для описания закономерностей в науке используются абстракции: *сила, масса, ускорение.*\n",
    "\n",
    "Данные для **ML** моделей тоже должны быть подготовлены. Типичная форма такой абстракции — вектор или n-мерный массив чисел.\n",
    "\n",
    "Именно с такой формой представления данных работает большинство современных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/nn_predict_patterns.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация результата"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй элемент, который потребуется для процесса обучения —  разработка способа оценки результата (*валидации*).\n",
    "\n",
    "Вне зависимости от того, какой метод обучения используется — с учителем или без, требуется некий критерий, по которому будет оцениваться выход модели и впоследствии корректироваться веса.\n",
    "\n",
    "В базовом варианте: полученный результат сравнивают с эталонным и если разница велика — корректируют модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример ML задачи "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Поясним эту идею на конкретном примере. Допустим, у нас есть наручный шагомер, который фиксирует перемещения в пространстве. Скорее всего, в нем встроен акселерометр, который способен фиксировать перемещения по трем осям. На выходе мы получаем сигнал с трёх датчиков.\n",
    "\n",
    "Если задача состоит в том, чтобы подсчитать количество шагов, то к её решению можно подойти двумя способами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/accelerometer_task.jpg\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант №1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классический: напишем программу. \n",
    "Если появилось ускорение по одной из осей, которое больше определенного порога, то мы создаем то условие, которое срабатывает. Позже мы выясним, что подобные сигнатурные сигналы с датчика могут поступить и при других определенных движениях, не связанных с шагами, например, во время плавания.\n",
    "Добавляется дополнительное условие, которое фильтрует подобные ситуации. \n",
    "\n",
    "Находятся всё новые и новые исключения из общего правила, программа и ее алгоритмическая сложность будет расти.\n",
    "\n",
    "Программу будет сложнее поддерживать из-за большого объема кода в ней. \n",
    "Изменение в одной из частей потребует внесение правок в другой код  и т.п."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/accelerometr_solution_standart.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С появлением машинного обучения мы можем применить принципиально другой подход. \n",
    "Не задумываясь о том, что значат показания каждого из акселерометров, мы можем просто собрать некоторый архив данных за определенное время (возможно разбив на более короткие промежутки времени). Всё, что нам потребуется помимо этих данных — это информация о том, сколько было сделано реальных шагов. После этого данные загружаются в модель, и она на этих данных учится. При достаточном количестве данных и адекватно подобранной модели (чем мы и будем заниматься) мы сможем научить ее решать конкретные задачи (в данном случае — считать шаги). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/accelerometr_solution_nn.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что по сути модели всё равно, что считать: шаги, сердечный ритм, количество калорий, ударов по клавиатуре и пр. Нет необходимости писать под каждый пример отдельную программу, достаточно собрать данные и мы сможем решить множество абсолютно разных задач. \n",
    "\n",
    "Важно лишь понимать, какую модель предпочтительнее выбрать. С этим мы и будем разбираться в ходе курса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/different_type_of_tasks.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/how_compute_model_accuracy.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Невозможно создать хорошее решение, не определив меру \"хорошести\". Нужно определиться с тем, как оценивать результат.\n",
    "Очень часто приходится слышать от заказчика вопрос со слайда.\n",
    "Чаще всего ответ “99%” их более чем устраивает. \n",
    "\n",
    "В большинстве случаев такой ответ приводит к проблемам. Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/accuracy_problem_example.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/important_accuracy_factors.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Скорость перемещения  машины зависит от дороги: на дорогах бывают пробки, ограничивающие знаки, наконец, дороги бывают очень разного качества.\n",
    "\n",
    "Всё это влияет на скорость перемещения и порой радикально.\n",
    "\n",
    "Также и точность наших моделей в первую очередь зависит от данных, на которых мы будем их оценивать. Модель, которая отлично работает на одном датасете, может намного хуже работать на другом или не работать вовсе.\n",
    "\n",
    "2. Машина может быть подвергнута тюнингу. Например, внедорожный тюнинг поможет преодолеть участок бездорожья, на котором неподготовленный автомобиль застрянет. Но при этом скорость на дорогах общего пользования может снизиться. Также и модель, как правило, имеет ряд параметров (гиперпараметров), от которых зависит её работа. Они могут подбираться в зависимости от задачи (ошибки первого и второго рода) и качества данных.\n",
    "\n",
    "3. Само понятие скорости допускает вариации: речь идет о средней или максимальной скорости? Аналогично и для оценки моделей существует несколько метрик, применение которых, опять же, зависит от целей заказчика и особенностей данных.\n",
    "\n",
    "**«На датасете X модель Y по метрике Z показала 99%».**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/simple_way_to_compute_accuracy.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуитивно понятной, очевидной и почти неиспользуемой метрикой является accuracy — доля правильных ответов алгоритма.\n",
    "\n",
    "**Какие есть недостатки у такого способа?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/problem_of_simple_way_to_compute_accuracy.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy нельзя использовать, если **данные не сбалансированы**. То есть в одном из классов больше представителей, чем в другом.\n",
    "\n",
    "Также она не подойдет для задач сегментации и детектирования, если требуется не только определить наличие объекта на изображении, но и найти место, где он находится, то весьма желательно учитывать разницу в координатах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для избегания этих проблем вводятся метрики \"точность\" и \"полнота\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/precision-recall.png\" width=\"800\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для численного описания этих метрик необходимо ввести важную концепцию для описания в терминах ошибок классификации — **confusion matrix** (матрица ошибок).\n",
    "Допустим, что у нас есть два класса и алгоритм, предсказывающий принадлежность каждого объекта одному из классов, тогда матрица ошибок классификации будет выглядеть следующим образом:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|      |$\\large y=1$  |$\\large y=0$   |\n",
    "| ---  |---  |---   |\n",
    "| $\\large \\widehat{y}=1$    |$\\large True Positive (TP) $   | $\\large False Positive (FP)   $  |\n",
    "| $\\large \\widehat{y}=0$    |$\\large False Negative (FN)$   | $\\large True Negative (TN)     $ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Precision, recall**\n",
    "\n",
    "Для оценки качества работы алгоритма на каждом из классов по отдельности введем метрики **precision (точность)** и **recall (полнота)**.\n",
    "\n",
    "\n",
    "$\\large precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "\n",
    "$\\large recall = \\frac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Именно** введение **precision** не позволяет нам записывать все объекты в один класс, так как в этом случае мы получаем рост уровня False Positive. **Recall демонстрирует способность алгоритма обнаруживать данный класс вообще, а precision — способность отличать этот класс от других классов.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,6))\n",
    "fig.tight_layout(pad=3.0)\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "#font = {'size':'21'}\n",
    "ax[0].set_title(\"Balanced data\")\n",
    "ax[1].set_title(\"Unbalanced data\")\n",
    "\n",
    "labels = ['Airplane', 'Auto', 'Bird']\n",
    "\n",
    "#Balanced data\n",
    "air, auto, bird = 150, 150,150\n",
    "actual_b = np.array([0]*air + [1]*auto + [2]*bird)\n",
    "predicted_b = np.array([0]*(air-10) + [1]*(auto+20) + [2]*(bird-10))\n",
    "\n",
    "#Unbalanced data\n",
    "air, auto, bird = 430, 10, 10\n",
    "actual_ub = np.array([0]*air + [1]*auto + [2]*bird)\n",
    "predicted_ub = np.array([0]*(air+20) + [1]*(auto-10) + [2]*(bird-10))\n",
    "\n",
    "metrics.ConfusionMatrixDisplay(\n",
    "confusion_matrix = metrics.confusion_matrix(actual_b, predicted_b), display_labels = labels).plot(ax=ax[0])\n",
    "\n",
    "metrics.ConfusionMatrixDisplay(\n",
    "confusion_matrix = metrics.confusion_matrix(actual_ub, predicted_ub), display_labels = labels).plot(ax=ax[1])\n",
    "\n",
    "label_font = {'size':'15'}  # Adjust to fit\n",
    "ax[0].set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax[0].set_ylabel('True labels', fontdict=label_font);\n",
    "ax[1].set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax[1].set_ylabel('True labels', fontdict=label_font);\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Accuracy Balanced  :', round(metrics.accuracy_score(actual_b, predicted_b), 2))\n",
    "print('Accuracy Unbalanced:', round(metrics.accuracy_score(actual_ub, predicted_ub), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**\n",
    "\n",
    "Accuracy также можно посчитать через матрицу ошибок.\n",
    "\n",
    "$\\large accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    " В случае многоклассовой классификации термины TP, FP, TN, FN считаются для каждого класса:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/confmatrix.png\" width=\"600\" >\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large Multiclass Accuracy = \\frac{1}{n}\\sum_{i=1}^{n} [actual_{i}==predicted_{i}]  =   \\frac{\\sum_{k=1}^{N} TP_{Ck} }{\\sum_{k=1}^{N} (TP_{Ck} + TN_{Ck} + FP_{Ck} + FN_{Ck})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Balanced accuracy**\n",
    "\n",
    "В случае дисбаланса классов есть специальный аналог точности – сбалансированная точность. \n",
    "\n",
    "$\\ BA = \\frac{R_1 + R_0}{2} = \\frac{1}{2} (\\frac{TP}{TP + FN} + \\frac{TN}{TN + FP})$\n",
    "\n",
    "Для сбалансированного и несбалансированного случая она будет равна $0. 96$ и $0.33$ соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Banalced accuracy for Balanced data  :', round(metrics.balanced_accuracy_score (actual_b, predicted_b), 2))\n",
    "print('Banalced accuracy for Unalanced data :', round(metrics.balanced_accuracy_score(actual_ub, predicted_ub), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты запоминания – это среднее полноты всех классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-мера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибки классификации бывают двух видов: **False Positive** и **False Negative**. Первый вид ошибок называют **ошибкой I-го рода**, второй — **ошибкой II-го рода**. Пусть студент приходит на экзамен. Если он учил и знает, то принадлежит классу с меткой 1, иначе — имеет метку 0 (знающего студента называем «положительным»). Пусть экзаменатор выполняет роль классификатора: ставит зачёт (т.е. метку 1) или отправляет на пересдачу (метку 0). Самое желаемое для студента «не учил, но сдал» соответствует ошибке 1 рода, вторая возможная ошибка «учил, но не сдал» – 2 рода.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/1_2_errors.png\" width=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто в реальной практике стоит задача найти **оптимальный** **баланс** между **Presicion и Recall**. Классическим примером является задача определения оттока клиентов.\n",
    "\n",
    "**F-мера** (в общем случае $\\ F_\\beta$) — среднее гармоническое precision и recall :\n",
    "\n",
    "$\\large \\ F_\\beta = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{(\\beta^2 \\cdot precision) + recall}$\n",
    "\n",
    "\n",
    "$\\beta$ в данном случае определяет вес точности в метрике, и при $\\beta = 1$ это среднее гармоническое (с множителем 2, чтобы в случае precision = 1 и recall = 1 иметь $\\ F_1 = 1$).\n",
    "F-мера достигает максимума при полноте и точности, равными единице, и близка к нулю, если один из аргументов близок к нулю.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сбалансированная F-мера, β=1:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/f1_balanced.png\" width=\"500\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При перекосе в точность ($β=1/4$):\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/f1_unbalanced.png\" width=\"500\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более наглядно: низкие значения точности не позволяют метрике F вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/f1_lines.png\" width=\"600\" >\n",
    "\n",
    "<center><em>Зависимость F1-меры от полноты при фиксированной точности. При точности 10% F1-мера не может быть больше 20%.</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В sklearn есть удобная функция **sklearn.metrics.classification_report**, возвращающая recall, precision и F-меру для каждого из классов, а также количество экземпляров каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_pred = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC-ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть решается задача бинарной классификации, и необходимо оценить важность признака $j$ для решения именно этой задачи. В этом случае можно попробовать построить классификатор, который использует лишь этот один признак $j$, и оценить его качество. Например, можно рассмотреть очень простой классификатор, который берёт значение признака $j$ на объекте, сравнивает его с порогом $t$, и если значение меньше этого порога, то он относит объект к первому классу, если же меньше порога — то к другому, нулевому или минус первому, в зависимости от того, как мы его обозначили. Далее, поскольку этот классификатор зависит от порога $t$, то его качество можно измерить с помощью таких метрик, как площадь под ROC-кривой или Precision-Recall кривой, а затем по данной площади отсортировать все признаки и выбирать лучшие.\n",
    "\n",
    "Но вначале разберёмся, что такое **AUC-ROC**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC-кривой (ROC, receiver operating characteristic, кривой ошибок) традиционно называют график кривой, которая характеризует качество предсказаний бинарного классификатора на некоторой фиксированной выборке при всех значениях порога классификации. Площадь под графиком ROC кривой AUC (area under the curve) является численной характеристикой качества классификатора. Определим, как именно строится ROC-кривая через рассмотрение примера.\n",
    "\n",
    "Вывод некоторого бинарного классификатора представлен в табл. 1. Упорядочим строки данной таблицы по убыванию значения вывода нашего бинарного классификатора и запишем результат в табл. 2. Если наш алгоритм справился с задачей классификации, то мы увидим в последней колонке также упорядоченные по убыванию значения (или случайное распределение меток 0 и 1 в противном случае)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/L04/out/roc_auc_data_example.png\" alt=\"alttext\" width=600/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приступим непосредственно к изображению графика ROC-кривой. Начнём с квадрата единичной площади и изобразим на нём прямоугольную координатную сетку, равномерно нанеся $m$ горизонтальных линий и $n$ - вертикальных. Число горизонтальных линий $m$ соответствует количеству объектов класса $1$ из рассматриваемой выборки, а число $n$ -- количеству объектов класса $0$. В нашем примере $m=3$ и $n=4$. Таким образом, квадрат единичной площади разбился на $m \\times n$ прямоугольных блоков (на $12$ штук согласно нашему примеру)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начиная из точки $(0, 0)$ построим ломанную линию в точку $(1, 1)$ по узлам получившейся решетке по следующему алгоритму: \n",
    "- рассмотрим последовательно все строки табл. 2\n",
    "- оценка алгоритма для объекта из текущей строки не равна оценке для объекта из следующей:\n",
    "- - если в строке содержится объект с меткой класса $1$, рисуем линию до следующего узла вертикально вверх\n",
    "- - если в строке содержится объект с меткой класса $0$, рисуем линию до следующего узла горизонтально направо\n",
    "- оценки для объектов в нескольких последующих строках совпадают:\n",
    "- - нарисовать линию из текущего узла в узел, располагающийся на $k$ углов вертикально выше и на $l$ узлов левее. $k$ и $l$ соответственно равны количеству объектов класса $1$ и $0$ среди группы повторяющихся значений оценок классификатора\n",
    "\n",
    "(всего потребуется не более $n + m$ шагов — столько же, сколько строк в нашей таблице)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/L04/out/make_roc_curve.png\" alt=\"alttext\" width=500/></center>\n",
    "\n",
    "<center><em>Рис.1. Построение ROC-кривой.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Справа на рис. 1 показана полученная для нашего примера кривая – эта изображенная на единичном квадрате ломанная линия и называется ROC-кривой. \n",
    "\n",
    "Вычислим площадь под получившийся кривой -- **AUC-ROC**. В нашем примере AUC-ROC $= 9.5 / 12 ~ 0.79$ и именно это значение является искомой метрикой качества работы нашего бинарного классификатора.\n",
    "(Так как мы начали свое построение с квадрата единичной площади, то AUC-ROC может принимать значения в $[0,1]$) \n",
    "\n",
    "\n",
    "1. ROC-кривая абсолютно точного бинарного классификатора имеет вид $(0,0) \\rightarrow (1,0) \\rightarrow (1,1)$. ROC-AUC для такого идеального классификатора равен площади всего единичного квадрата.\n",
    "2. ROC-кривая для всегда ошибающегося бинарного классификатора имеет вид $(0,0) \\rightarrow (0,1) \\rightarrow (1,1)$. ROC-AUC в этом случае равен нулю.\n",
    "3. Если наш бинарный классификатор для всех объектов предскажет одно и то же значение, то его ROC-кривая будет иметь вид $(0,0) \\rightarrow (1,1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/L04/out/various_roc_curves.png\" alt=\"alttext\" width=500/></center>\n",
    "\n",
    "<center><em>Рис. 2. ROC-кривые для наилучшего (AUC=1), константного (AUC=0.5) и наихудшего (AUC=0) алгоритма.</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Смысл метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить на рис. 3, координатная сетка, описанная в нашем алгоритме построения ROC кривой, разбила единичный квадрат на столько прямоугольников, сколько существовало в пар объектов класс-$0$ -- класс-$1$ в исследуемой выборке данных. Если теперь посчитать количество оказавшихся под ROC-кривой прямоугольников, то можно заметить что оно в точности равно числу верно классифицированных алгоритмом пар объектов -- то есть таких пар объектов противоположных классов, для которых алгоритм поставил большую по величине оценку для объекта класса $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/L04/out/roc_auc_pairs_descripton.png\" alt=\"alttext\" width=200/></center>\n",
    "\n",
    "<center><em>Рис. 3. Каждый блок соответствует паре объектов.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, **ROC-AUC равен части верно упорядоченных оценкой классификатора пар объектов противоположных классов (в которой объект класса $0$ получил оценку исследуемым классификатором ниже, чем объект класса $1$)**. Это явно записывается формулой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\text{ROC-AUC} = \\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{N} I[y_{i} < y_{j}]I'[a_{i} < a_{j}] } {\\sum_{i=1}^{N} \\sum_{j=1}^{N} I[y_{i} < y_{j}]} $$\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "I'[a_{i}< a_{j}] =\n",
    " \\begin{cases}\n",
    "   0, & \\quad a_{i} > a_{j}, \n",
    "   \\\\\n",
    "   0.5, & \\quad a_{i} = a_{j},\n",
    "   \\\\\n",
    "   1, & \\quad a_{i} < a_{j}.\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "I[y_{i}< y_{j}] =\n",
    " \\begin{cases}\n",
    "   0, & \\quad y_{i} \\geq y_{j}, \n",
    "   \\\\\n",
    "   1, & \\quad y_{i} < y_{j}.\n",
    " \\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "$ a_{i} $ — выходное значение классификатора на $i$-м  объекте, $ y_{i} $ — априорно верная метка класса для того же объекта, $N$ — полное число объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данное определение можно обобщить на задачу классификации непрерывного множества объектов. Пусть мы взяли два случайных объекта разных классов: $x_i$ класса $0$ и $x_j$, принадлежащий классу $1$. Тогда метрика ROC-AUC равна вероятности того, что в такой паре объектов объект класса $1$ получил оценку выше, нежели объект класса $0$:\n",
    "\n",
    "$$\\text{ROC-AUC}(a) = P(a(x_i) < a(x_j) | y_i=0, y_j=1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инструменты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим примеры решения задач классификации на различных типах данных.\n",
    "\n",
    "Будем использовать библиотеки:\n",
    "\n",
    "* [NumPy](https://numpy.org/) — поддержка больших многомерных массивов и быстрых математических функций для операций с этими массивами.\n",
    "* [scikit-learn](https://scikit-learn.org/stable/) — ML алгоритмы, \"toy\" — датасеты;\n",
    "* [pandas](https://pandas.pydata.org/) — Удобная работа с табличными данными.\n",
    "\n",
    "* [**PyTorch**](https://pytorch.org/) — Основной фреймворк машинного обучения, который будет использоваться на протяжении всего курса.\n",
    "\n",
    "* [Matplotlib](https://matplotlib.org/) - Основная библиотека для визуализации. Вывод различных графиков.\n",
    "\n",
    "* [Seaborn](https://seaborn.pydata.org/) - Удобная библиотека для визуализации статистик. Прямо из коробки вызываются и гистограммы, и тепловые карты, и визуализация статистик по датасету, и многое другое."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/sns.png\" width=\"1100\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Связность данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/types_of_data.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство процессов и объектов, с которыми научились работать ML/DL модели, можно отнести к одному из перечисленных типов. Наша задача будет состоять в том, как данные из вашей предметной области свести к одному из них и представить в виде набора чисел. \n",
    "\n",
    "Для работы с различными типами данных используют разные типы моделей:\n",
    "\n",
    "**Табличный**  — классические ML модели либо полносвязанные NN;\n",
    "\n",
    " **Последовательности** — рекуррентные сети + свёртка;\n",
    " \n",
    " **Изображения/видео** — 2,3 .. ND свёрточные сети.\n",
    " \n",
    "\n",
    "\n",
    "В разных типах данных количество связей между элементами разное и зависит только от типа этих данных. Важно НЕ количество элементов, а СВЯЗИ между ними.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/connectivity_of_data_types.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные мы можем условно делить по степени связанности. Это степень взаимного влияния между соседними элементами. \n",
    "Например, в таблице, в которой есть определенные параметры (например: рост, вес) данные между собой связаны, но порядок столбцов значения не имеет.\n",
    "Если мы поменяем столбцы местами, то не потеряем никакой важной информации. \n",
    "\n",
    "Такие данные можно представить в виде вектора, но порядок элементов в нем не важен.\n",
    "\n",
    "При работе с изображениями нам становится важно, как связаны между собой пиксели и по горизонтали, и по вертикали. \n",
    "При добавлении цвета появляются 3 RGB канала, и значения в каждом канале также связаны между собой. Эту связь нельзя терять, если мы хотим корректно извлечь максимум информации из данных. Соответственно, если дано цветное изображение, то у нас уже есть три измерения, в которых мы должны эти связи учитывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Контейнеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с массивами 3 типов, которые переходят друг в друга:\n",
    "\n",
    "* list — стандартный тип в Python\n",
    "* numpy — массив\n",
    "* torch.tensor\n",
    "\n",
    "Где используются:\n",
    "\n",
    "*   **ML**: list, numpy \n",
    "*   **DL**: torch.tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_list = [[1,2,3],[4,5,6]]\n",
    "python_list_various = ['a',15,123.8,[99,\"I love you\"],[True,True,False]]\n",
    "\n",
    "print(python_list_various)\n",
    "print(python_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В списке могут быть данные различных типов, в том числе подтипов произвольной длины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Массив может содержать данные только одного типа;   \n",
    "* Размер данных во всех измерениям кроме 0-го должен совпадать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_arr = np.array(python_list, dtype = float)\n",
    "print(numpy_arr)\n",
    "\n",
    "# This code will cause an error\n",
    "# invalid_numpy_arr = np.array([[1,2,3],[4,5]],dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благодаря этому над numpy-массивами можно выполнять различные математические операции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([1,0,0])\n",
    "row_diff = numpy_arr - vector\n",
    "print(\"Substract row from array\", row_diff)\n",
    "\n",
    "scalar_product = numpy_arr.dot(vector)\n",
    "print(\"Scalar product\", scalar_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        'is a multi-dimensional matrix containing elements of a single data type.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения ограничений и функционала [torch.Tensor](https://pytorch.org/docs/stable/tensors.html) эквивалентен numpy-массиву.\n",
    "Но дополнительно этот объект поддерживает две важных операции:\n",
    "\n",
    "* Перенос данных на видеокарту (`my_tensor.to('cuda:0')`)\n",
    "* Автоматический расчет градиентов  (`my_tensor.backward()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти возможности понадобятся нам в дальнейшем. Поэтому надо разобраться, как  работать с данными в этом формате. Тем более, что torch.Tensor легко преобразуется в numpy-массив и обратно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "my_tensor = torch.tensor(numpy_arr)\n",
    "print(\"torch.Tensor\\n\",my_tensor,\"\\nshape =\", my_tensor.shape)\n",
    "\n",
    "squared_numpy = my_tensor.pow(2).numpy()\n",
    "print(\"Numpy\\n\",squared_numpy,\"\\nshape =\", squared_numpy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и визуализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Табличные данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример работы с табличными данными. \n",
    "Классифицируем вина из [датасета](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача будет состоять в том, чтобы по химическому составу определить производителя вина.\n",
    "\n",
    "Количество экземпляров, полученных на тест от каждого из трех производителей, не одинаково."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производитель №1 (class_1) 59 бутылок  \n",
    "Производитель №2 (class_2) 71 бутылка  \n",
    "Производитель №3 (class_3) 48 бутылок  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот датасет можно загрузить, используя модуль sklearn.datasets библиотеки [sklearn](https://scikit-learn.org/stable/), чем мы и воспользуемся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "from sklearn.datasets import load_wine\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine\n",
    "\n",
    "# Download dataset\n",
    "dataset = load_wine(return_X_y = True) # also we can get data in Bunch (dictionary) or pandas DataFrame\n",
    "\n",
    "features = dataset[0] # array 178x13 (178 bottles each with 13 features)\n",
    "class_labels = dataset[1] # array of 178 elements, each element is a number the class: 0,1 2  \n",
    "print(\"features shape:\",features.shape)\n",
    "print(\"class_labels shape:\",class_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Визуализация данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Если параметр \n",
    "    \n",
    "    return_X_y == False\n",
    "\n",
    "то данные вернутся не в виде массива, а в объекте [Bunch](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html#sklearn.utils.Bunch).\n",
    "\n",
    "Обращаться к нему можно, как к обычному словарю в Python. Кроме того, у него есть свойство, соответствующее каждому полю данных.\n",
    "\n",
    "Чтобы отобразить данные в виде таблицы, преобразуем их в формат pandas.DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library to work with tabular data: https://pandas.pydata.org/\n",
    "import pandas as pd \n",
    "\n",
    "dataset_bunch = load_wine(return_X_y = False)\n",
    "print(dataset_bunch.keys())\n",
    "\n",
    "df = pd.DataFrame(dataset_bunch.data, columns=dataset_bunch.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая строка в таблице может быть интерпретирована как вектор из 13 элементов. Можно интерпретировать такой вектор как координаты точки в 13-мерном пространстве. Именно с таким представлением работает большинство алгоритмов машинного обучения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузка**\n",
    "\n",
    "Загрузим датасет CIFAR-10. Он состоит из 60000 цветных изображений размером 32x32. На картинках объекты 10 классов.\n",
    "\n",
    "Для его загрузки используем библиотеку torchvision.\n",
    "\n",
    "Пакет torchvision входит в число предустановленных в colab.\n",
    "\n",
    "Датасеты из torcvision изначально поддерживают механизм transforms \n",
    "и разбивку на тестовые и проверочные подмножества. Нам не придется добавлять их вручную.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_set = datasets.CIFAR10(\"content\", train = True,  download = True)\n",
    "val_set = datasets.CIFAR10(\"content\", train = False, download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем несколько картинок вместе с метками. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "# load labels names for visualization\n",
    "with open(\"content/cifar-10-batches-py/batches.meta\",'rb') as infile:\n",
    "  cifar_meta = pickle.load(infile)\n",
    "labels_name = cifar_meta['label_names']\n",
    "\n",
    "for j in range(10):\n",
    "  img, label = train_set[j]\n",
    "  plt.subplot(1, 10 ,j+1)\n",
    "  plt.imshow(img)\n",
    "  plt.axis('off')  \n",
    "  plt.title(labels_name[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, в каком виде хранятся картинки в памяти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оказывается, в формате [PIL](https://pillow.readthedocs.io/en/stable/reference/Image.html).\n",
    "\n",
    "Чтобы обучать модель, нам придётся преобразовать их в тензоры. \n",
    "Используем для этого transforms и Dataloder.\n",
    "\n",
    "Выведем размеры получившихся тензоров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "val_set.transform = transforms.Compose([ transforms.ToTensor() ]) # PIL Image to Pytorch tensor\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)\n",
    "\n",
    "for batch in val_loader:\n",
    "  imgs, labels = batch\n",
    "  print(len(batch))\n",
    "  print(\"Images: \",imgs.shape)\n",
    "  print(\"Labels: \",labels.shape)\n",
    "  print(labels)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разберемся с размерностями:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждой итерации dataloader возвращает кортеж из двух элементов.\n",
    "* Первый элемент — это изображения;\n",
    "* Второй — метки классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество элементов в каждом равно batch_size, в данном примере — 8.\n",
    "\n",
    "Изображение:  \n",
    "3 — C, каналы (в отличие от PIL и OpenCV они идут сначала);  \n",
    "32 — H, высота;  \n",
    "32 — W, ширина. \n",
    "\n",
    "Метки: числа от 0 до 9 по количеству классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим модель-заглушку**\n",
    "\n",
    "Она не будет ничего предсказывать, только возвращать случайный номер класса.\n",
    "\n",
    "В методе fit данные просто запоминаются. Этот фрагмент кода можно будет использовать при выполнении практического задания.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.train_data = None\n",
    "    self.train_labels = None\n",
    "\n",
    "  def fit(self,x,y):\n",
    "    # Simple store all data\n",
    "    self.train_data = torch.vstack((self.train_data,x)) if self.train_data != None else x\n",
    "    self.train_labels = torch.hstack((self.train_labels,y)) if self.train_labels != None else y\n",
    "   \n",
    "  def forward(self,x):\n",
    "    # x is a batch, not a single sample!\n",
    "    # Return random number instead of predictions\n",
    "    class_count = torch.unique(self.train_labels).shape[0]\n",
    "    # https://pytorch.org/docs/stable/generated/torch.randint.html#torch-randint\n",
    "    # size is shape of output tensor\n",
    "    label = torch.randint(low = 0, high = class_count-1, size = (x.shape[0],)) \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запустим процесс \"обучения\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.transform = transforms.Compose([ transforms.ToTensor(),  ]) # PIL Image to Pytorch tensor\n",
    "train_loader = DataLoader(train_set, batch_size=1024, shuffle=True)\n",
    "\n",
    "model = FakeModel()\n",
    "\n",
    "for img_batch, labels_batch in train_loader:\n",
    "  model.fit(img_batch, labels_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим работу модели на нескольких изображениях из тестового набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch, label_batch = next(iter(val_loader))\n",
    "predicted_labels = model(img_batch)\n",
    "\n",
    "for i, predicted_label in enumerate(predicted_labels):\n",
    "  img = img_batch[i].permute(1,2,0).numpy()*255  \n",
    "  plt.subplot(1, len(predicted_labels),i+1)\n",
    "  plt.imshow(img.astype(int))\n",
    "  plt.axis('off')\n",
    "  plt.title(labels_name[int(predicted_label)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = []\n",
    "for img_batch, labels_batch in val_loader:\n",
    "  predicted = model(img_batch)\n",
    "  batch_accuracy = accuracy_score(labels_batch, predicted)\n",
    "  accuracy.append(batch_accuracy)\n",
    "\n",
    "print(\"Accuracy\",torch.tensor(accuracy).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем повышать точность. В ходе выполнения практического задания заменим заглушку в методе predict реальным алгоритмом. Используем алгоритм [K- Nearest Neighbor](https://colab.research.google.com/drive/1_5tGxAoxrWulPmwK2Ht9BHGsS-EpxVo0?usp=sharing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Временные ряды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L01/timeseries.png\" width=\"850\"></center>\n",
    "<center><i>Типичный пример временного ряда</center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенно хотелось бы поговорить о временных рядах. Особенностью таких данных является связность, наличие \"настоящего\", \"прошедшего\" и \"будущего\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/ts_split.png\" width=\"850\"></center>\n",
    "<center><i>Разбиение данных временных рядов на подвыборки</center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья на хабре об анализе временных рядов на Python](https://habr.com/ru/company/ods/blog/327242/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка результата"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение train-validation-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте ограничим пространство гипотез только линейными функциями от $m + 1$ аргумента, будем считать, что нулевой признак для всех объектов равен единице $x_0 = 1$:\n",
    "\n",
    "$\\Large \\begin{array}{rcl} \\forall h \\in \\mathcal{H}, h\\left(\\vec{x}\\right) &=& w_0 x_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_m x_m \\\\ &=& \\sum_{i=0}^m w_i x_i \\\\ &=& \\vec{x}^T \\vec{w} \\end{array}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмпирический риск (функция стоимости) принимает форму среднеквадратичной ошибки:\n",
    "\n",
    "$\\Large \\begin{array}{rcl}\\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\vec{x}_i^T \\vec{w}_i\\right)^2 \\\\ &=& \\frac{1}{2n} \\left\\| \\vec{y} - X \\vec{w} \\right\\|_2^2 \\\\ &=& \\frac{1}{2n} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) \\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строки матрицы $X$ — это признаковые описания наблюдаемых объектов. Один из алгоритмов обучения $\\mathcal{M}$ такой модели — это метод наименьших квадратов. Вычислим производную функции стоимости:\n",
    "$\\Large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} &=& \\frac{\\partial}{\\partial \\vec{w}} \\frac{1}{2n} \\left( \\vec{y}^T \\vec{y} -2\\vec{y}^T X \\vec{w} + \\vec{w}^T X^T X \\vec{w}\\right) \\\\ &=& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) \\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приравняем к нулю и найдем решение в явном виде:\n",
    "\n",
    "$\\Large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) = 0 \\\\ &\\Leftrightarrow& -X^T \\vec{y} + X^T X \\vec{w} = 0 \\\\ &\\Leftrightarrow& X^T X \\vec{w} = X^T \\vec{y} \\\\ &\\Leftrightarrow& \\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y} \\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поздравляю, дамы и господа, мы только что с вами вывели алгоритм машинного обучения. Реализуем же этот алгоритм.\n",
    "\n",
    "Начнем с датасета, состоящего всего из одного признака. Будем брать случайную точку на синусе и добавлять к ней шум — таким образом получим целевую переменную; признаком в этом случае будет координата $x$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wave_set(n_support=1000, n_train=25, std=0.3):\n",
    "    data = {}\n",
    "    # выберем некоторое количество точек из промежутка от 0 до 2*pi\n",
    "    data['support'] = np.linspace(0, 2*np.pi, num=n_support)\n",
    "    # для каждой посчитаем значение sin(x) + 1\n",
    "    # это будет ground truth\n",
    "    data['values'] = np.sin(data['support']) + 1\n",
    "    # из support посемплируем некоторое количество точек с возвратом, это будут признаки\n",
    "    data['x_train'] = np.sort(np.random.choice(data['support'], size=n_train, replace=True))\n",
    "    # опять посчитаем sin(x) + 1 и добавим шум, получим целевую переменную\n",
    "    data['y_train'] = np.sin(data['x_train']) + 1 + np.random.normal(0, std, size=data['x_train'].shape[0])\n",
    "    return data\n",
    "\n",
    "data = generate_wave_set(1000, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "print ('Shape of X is', data['x_train'].shape)\n",
    "print ('Head of X is', data['x_train'][:10])\n",
    "\n",
    "margin = 0.3\n",
    "plt.plot(data['support'], data['values'], 'b--', alpha=0.5, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='data')\n",
    "plt.xlim(data['x_train'].min() - margin, data['x_train'].max() + margin)\n",
    "plt.ylim(data['y_train'].min() - margin, data['y_train'].max() + margin)\n",
    "plt.legend(loc='upper right', prop={'size': 20})\n",
    "plt.title('True manifold and noised data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем алгоритм обучения, используя магию NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим колонку единиц к единственному столбцу признаков\n",
    "X = np.array([np.ones(data['x_train'].shape[0]), data['x_train']]).T\n",
    "# перепишем, полученную выше формулу, используя numpy\n",
    "# шаг обучения - в этом шаге мы ищем лучшую гипотезу h\n",
    "w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), data['y_train'])\n",
    "# шаг применения: посчитаем прогноз\n",
    "y_hat = np.dot(w, X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.3\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "plt.plot(data['support'], data['values'], 'b--', alpha=0.5, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='data')\n",
    "\n",
    "plt.plot(data['x_train'], y_hat, 'r', alpha=0.8, label='fitted')\n",
    "\n",
    "plt.xlim(data['x_train'].min() - margin, data['x_train'].max() + margin)\n",
    "plt.ylim(data['y_train'].min() - margin, data['y_train'].max() + margin)\n",
    "plt.legend(loc='upper right', prop={'size': 20})\n",
    "plt.title('Fitted linear regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, линия не очень-то совпадает с настоящей кривой. Среднеквадратичная ошибка равна 0.26704 условных единиц. Очевидно, что если бы вместо линии мы использовали кривую третьего порядка, то результат был бы куда лучше. И, на самом деле, с помощью линейной регрессии мы можем обучать нелинейные модели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейной регрессии мы ограничивали пространство гипотез только линейными функциями от признаков. Давайте теперь расширим пространство гипотез до всех полиномов степени $p$. Тогда в нашем случае, когда количество признаков равно одному $m=1$, пространство гипотез будет выглядеть следующим образом:\n",
    "\n",
    "$\\Large \\begin{array}{rcl} \\forall h \\in \\mathcal{H}, h\\left(x\\right) &=& w_0 + w_1 x + w_1 x^2 + \\cdots + w_n x^p \\\\ &=& \\sum_{i=0}^p w_i x^i \\end{array}$\n",
    "\n",
    "\n",
    "Если заранее предрассчитать все степени признаков, то задача опять сводится к описанному выше алгоритму — методу наименьших квадратов. Попробуем отрисовать графики нескольких полиномов разных степеней.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список степеней p полиномов, который мы протестируем\n",
    "degree_list = [1, 2, 3, 5, 7, 10, 13]\n",
    "\n",
    "cmap = plt.get_cmap('jet')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, len(degree_list))]\n",
    "\n",
    "margin = 0.3\n",
    "figure(figsize=(10, 8), dpi=80)\n",
    "plt.plot(data['support'], data['values'], 'b--', alpha=0.5, label='manifold')\n",
    "plt.scatter(data['x_train'], data['y_train'], 40, 'g', 'o', alpha=0.8, label='data')\n",
    "\n",
    "w_list = []\n",
    "err = []\n",
    "\n",
    "for ix, degree in enumerate(degree_list):\n",
    "    # список с предрасчитанными степенями признака\n",
    "    dlist = [np.ones(data['x_train'].shape[0])] + \\\n",
    "                list(map(lambda n: data['x_train']**n, range(1, degree + 1)))\n",
    "    X = np.array(dlist).T\n",
    "    w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), data['y_train'])\n",
    "    w_list.append((degree, w))\n",
    "    y_hat = np.dot(w, X.T)\n",
    "    err.append(np.mean((data['y_train'] - y_hat)**2))\n",
    "    plt.plot(data['x_train'], y_hat, color=colors[ix], label='poly degree: %i' % degree)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Разбиение данных на обучающую и тестовую выборки "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым простым способом научиться чему-либо является \"запомнить всё\".\n",
    "\n",
    "Вспомним \"Таблицу умножения\". Если мы хотим проверить умение умножать, то проверки примерами из таблицы умножения будет недостаточно, ведь она может быть полностью запомнена. Нужно давать новые примеры, которых не было в таблице умножения (обучающей выборке)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если модель \"запомнит всё\", то она будет идеально работать на данных, которые мы ей показали, но может вообще не работать на любых других данных.\n",
    "\n",
    "С практической точки зрения важно, как модель будет вести себя именно на незнакомых для неё данных. То есть насколько хорошо она научилась обобщать закономерности, которые в данных присутствовали (если они вообще существуют)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки этой способности набор данных разделяют на две, а иногда даже на три части:\n",
    "\n",
    "* train — Данные, на которых модель учится;\n",
    "* validation/test — Данные, на которых идет проверка.\n",
    "\n",
    "В `sklearn.model_selection` есть модель для разделения массива данных на тренировочную и тестовую часть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data['x_train'], data['y_train'], test_size=0.2) # 80% training and 20% test\n",
    "\n",
    "print(\"X_train shape\",X_train.shape)\n",
    "print(\"X_test shape\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А мы в дальнейшем будем пользоваться аналогичными инструментами библиотеки PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры ошибок в данных и при разбиении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинное обучение не только об алгоритмах, **но и о данных**. В этой части лекции мы разберем ряд ошибок, допускаемых при работе с данными и их разбиением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утечка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Утечкой данных называется ситуация, когда в процессе обучения модели используется информация, которая не будет доступна при ее последующем использовании. Это приводит к **завышению** оценки **качества** модели.\n",
    "\n",
    "Самый простой пример утечки данных — это **дублирование** одних и тех же объектов в **train** и **test** выборках. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Дублирование данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дублирование данных часто случается при сборе данных из различных источников. Посмотрим, чем оно опасно. \n",
    "\n",
    "Для **примера** возьмем 10 картинок из CIFAR-10. Будем считать это train данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "dataset = datasets.CIFAR10(\"content\", train=True, download=True)\n",
    "\n",
    "data, _, labels, _ = train_test_split(dataset.data / 255,   # normalize\n",
    "                                      np.array(dataset.targets),\n",
    "                                      train_size=10,        # get only 10 imgs\n",
    "                                      random_state=42,\n",
    "                                      stratify=dataset.targets)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    axs[i//5][i%5].imshow(data[i])\n",
    "    axs[i//5][i%5].set_title(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, картинка из train оказалась в test. Выберем картинку из этих 10 и применим алгоритм k-nearest neighbors (k-NN, k=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data[3]\n",
    "\n",
    "# L1 distance\n",
    "def compute_L1(a, b):\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n",
    "# distance calculation\n",
    "distances = []\n",
    "for i in range(10):\n",
    "    l1 = compute_L1(x_test, data[i])\n",
    "    distances.append(l1)\n",
    "\n",
    "distances = np.array(distances)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = np.argmin(distances)\n",
    "print(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test, _, labels_test, _ = train_test_split(dataset.data / 255,   # normalize\n",
    "                                      np.array(dataset.targets),\n",
    "                                      train_size=10,        # get only 10 imgs\n",
    "                                      random_state=24,\n",
    "                                      stratify=dataset.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ближайшим соседом для картинки, просочившейся в test, стала эта же картинка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все данные из test будут присутствовать в train, то мы просто будем искать эту же картинку в train с чем алгоритм k-nearest neighbors с $k=1$ справляется идеально. Итогом станет $accuracy = 1$ на выходе. Но с применением на незнакомой картинке результат будет хуже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    axs[i//5][i%5].imshow(data_test[i])\n",
    "    axs[i//5][i%5].set_title(labels_test[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data_test[1]\n",
    "\n",
    "# distance calculation\n",
    "distances = []\n",
    "for i in range(10):\n",
    "    l1 = compute_L1(x_test, data[i])\n",
    "    distances.append(l1)\n",
    "\n",
    "distances = np.array(distances)\n",
    "print(distances)\n",
    "\n",
    "indx = np.argmin(distances)\n",
    "print(labels[indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим картинки с train. Ближайшим соседом для кота стала лягушка. \n",
    "\n",
    "**Если вы получили $accuracy = 1$, то, скорее всего, вы что-то делаете не так!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Утечка, спрятанная в признаках "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные, используемые для обучения могут содержать “подсказки” для модели, которых не будет в реальных данных.\n",
    "\n",
    "Самый простой **пример**: таблица со столбцом - порядковым номером строки `row_number`, в которую сначала записали все данные, принадлежащие классу -1, а потом все данные, принадлежащие классу +1. Если не удалить этот столбец из данных, то вместо выделения сложных закономерностей модель будет искать решение в виде `if row_number > N`. В реальных данных записи не будут упорядочены. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примером** датасета, в котором нумерация строк может “все испортить”, является  [Iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris). Посмотрим на значения target этого набора данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Утечка часто может прятаться в метаданных записи: столбец id, название файла, время записи/загрузки и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда “подсказки” спрятаны внутри признаков, поэтому важно понимать с какими данными вы работаете и какую задачу решаете. \n",
    "\n",
    "**Пример:** у вас есть истории болезней пациентов с подозрением на онкологию. Необходимо решить задачу постановки диагнозов для новых пациентов. Вы обучаете модель и получаете подозрительно хороший результат. Начинаете разбираться и понимаете, что в данных, на которых обучалась модель присутствует отметка о назначении пациенту химиотерапии, которая назначается только после определения диагноза. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut learning и репрезентативность данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut learning — это термин, которым обозначается ситуация, когда модель принимает правильное решение по неправильной причине: “right for the wrong reasons\". \n",
    "\n",
    "**Пример:** необходимо было обучить нейросеть отличать хаски от волка. Для обучения были выбраны фотографии хаски на фоне зелени и волков на фоне снега, вместо того, чтобы учиться отличать мордочки нейросеть научилась делать предсказание по фону. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/husky.png\" width=\"850\"></center>\n",
    "<center><i>Первая строка — обучающие данные (обратите внимания, что все хаски были сфотографированы летом, а волки зимой). Вторая строка — тестовые данные (нейросеть выучила, что лето — это признак хасок, поэтому она неправильно предсказывает класс волков).</center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень важно, чтобы данные, на которых происходит обучение были **репрезентативными**.\n",
    "\n",
    "**Пример**: рассмотрим датасет фотографий объектов, где изображения в обучающем и тестовом наборе были получены на открытом воздухе в солнечный день. Нет гарантии что модель, обученная на таких данных, будет хорошо работать в любую погоду, а значит при оценке на тесте мы получим завышенный результат. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример:** для обучения вы используете томограф из одного госпиталя, а оценку качества модели проводите на томографе из другого госпиталя. Если модель не учитывает характеристики оборудования, она, скорее всего, не будет обобщаться на томограф из второй больницы, но на датасете из первого госпиталя, эту ошибку не отловить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение на train и test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перемешивание данных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы уже упоминали, метки классов в датасете могут быть распределены неравномерно. Для того, чтобы сохранить соотношение классов при разделении на train и test, необходимо указать параметр `stratify` при разбиении.\n",
    "\n",
    "Еще одним параметром, используемым при разбиении, является `shuffle` (значение по умолчанию `True`). При `shuffle = True` датасет перед разбиением перемешивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на разбиение датасета Iris. Для наглядности будем делить датасет \n",
    "пополам. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lables(lables):\n",
    "    lable_count = {}\n",
    "    for item in lables:\n",
    "        if item not in lable_count:\n",
    "            lable_count[item] = 0\n",
    "        lable_count[item] += 1\n",
    "    return lable_count\n",
    "\n",
    "def print_split_stat(X_train, X_test, y_train, y_test):\n",
    "    print(\"Train labels: \", y_train)\n",
    "    print(\"Test labels:  \", y_test)\n",
    "    print(\"Train statistics: \", count_lables(y_train))\n",
    "    print(\"Test statistics:  \", count_lables(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, labels = load_iris(return_X_y=True)\n",
    "print(\"DataSet labels: \", labels)\n",
    "print(\"DataSet statistics: \", count_lables(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.5,\n",
    "                                                    shuffle = False, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print_split_stat(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.5, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print_split_stat(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.5, \n",
    "                                                    random_state=42, stratify=labels)\n",
    "\n",
    "print_split_stat(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых случаях данные нельзя перемешивать. Это касается задач в которых мы пытаемся предсказать будущее. В таких задачах train должен предшествовать test по времени. Более подробно об этом будет рассказано в лекции про рекуррентные нейронные сети. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Данные из различных источников"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании данных из различных источников нужно учитывать это при разбиении. \n",
    "\n",
    "**Пример:** вы анализируете данные ЭКГ на предмет патологий. У вас есть три источника данных:\n",
    "* аппарат ЭКГ в кардиологическом отделении (много патологий),\n",
    "* аппарат ЭКГ, который используют на медосмотрах (мало патологий),\n",
    "* аппарат ЭКГ из приемного покоя больницы (среднее число патологий). \n",
    "\n",
    "Каждый прибор имеет свои особенности: характерные шумы, точность измерения и т.п. Если модель научится определять с какого прибора пришли данные она получит “подсказку”, которой не будет при поступлении данных с “незнакомого” прибора. Хорошим решением будет оставить данные с аппарата ЭКГ из приемного покоя больницы для test, а обучаться только на данных с аппарата ЭКГ в кардиологическом отделении и аппарата ЭКГ, который используют на медосмотрах. Это позволит оценить, как обученная модель работает с “незнакомым\" прибором.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбор гиперпараметров на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В домашнем задании вы будете решать задачу классификации изображений CIFAR-10 (набор фотографий разделенных на 10 классов) метдом ближайших соседей k-NN. С использованием расстояний L1 (*Manhatten distance* — сумма абсолютных разностей между пикселями) и L2 (*Euclidian distance*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/l1_manhattan_and_l2_euclidian_distance.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С метриками L1 и L2 мы будем сталкиваться часто: и в качестве loss функции (функции ошибки, которую мы будем пытаться минимизировать при обучении), и в качестве регуляризации (для ограничения величины весовых коэффициентов в линейных слоях с целью сокращения переобучения). Более подробно о применении L1 и L2 вы узнаете позже, в этой и последующих лекциях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим параметром модели, который вы будете варьировать, будет количество ближайших соседей k.\n",
    "\n",
    "Итого, у нас есть два параметра модели (будем называть их гиперпараметрами), которые мы можем настраивать:\n",
    "* метрика расстояния,\n",
    "* количество ближайших соседей k.\n",
    "\n",
    "В дальнейшем мы столкнемся с другими гиперпараметрами. Например, мы \n",
    "можем попробовать использовать другую модель и выбор модели тоже станет гиперпараметром решаемой задачи. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема подбора гиперпараметров \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как решать проблему подбора гиперпараметров? Первое, что приходит в голову: давайте посчитаем accuracy для тестовой выборки для множества гиперпараметров и выберем лучший. Каждый раз, когда мы заглядываем в test чтобы изменить параметры мы подстраиваемся под test, и это плохо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуицию о том, что это плохо можно получить сравнив Public и Private Score в Leaderbord соревнования на [Kaggle](https://www.kaggle.com/). Kaggle - это платформа для соревнований по анализу данных и машинному обучению. Чаще всего соревнование проводится следующим образом: участникам предоставляются датасеты, разделенные на train с целевой разметкой target и test, для которого необходимо сделать предсказание predict в нужном формате. Отправленное предсказание predict делится на две части: Public и Private. По этим частям считаются очки Score, характеризующие качество результата и формируются таблицы лидеров Leaderboard. Public Leaderboard - доступен всем желающим в любое время. Private Leaderboard - доступен только по окончанию соревнования и именно по нему раздают призы. Частая ошибка новичка - начать подстраивать модель по Public Leaderboard.\n",
    "Часто в Private Leaderboard можно видеть участника, который сдвинулся на десятки строчек вниз.\n",
    "\n",
    "[Leaderbord соревнования](https://www.kaggle.com/competitions/allstate-purchase-prediction-challenge/leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять, что происходит, попробуем смоделировать ситуацию, когда мы просто пытаемся угадать predict, не анализируя данные, но ориентируясь на Public Score.\n",
    "Предположим, у нас есть соревнование по бинарной классификации (два класса 1 и 0), со следующим target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed = 42\n",
    "target = [random.randint(0, 1) for _ in range(200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим на Public и Private."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_public = target[:100]\n",
    "target_private = target[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подбора ответа будем использовать следующий алгоритм:\n",
    "1. Генерируем случайный вектор.\n",
    "2. Считаем Public accuracy\n",
    "3. Если это Public accuracy лучше предыдущего, сохраняем вектор, как predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "public_accuracy_list = []\n",
    "private_accuracy_list  = []\n",
    "best_public_accuracy = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    ans = [random.randint(0, 1) for _ in range(200)]\n",
    "\n",
    "    public_accuracy = accuracy_score(target_public, ans[:100])\n",
    "    private_accuracy = accuracy_score(target_private, ans[100:])\n",
    "\n",
    "    if public_accuracy > best_public_accuracy:\n",
    "        predict = ans\n",
    "        best_public_accuracy = public_accuracy\n",
    "        best_private_accuracy = private_accuracy\n",
    "\n",
    "    public_accuracy_list.append(best_public_accuracy)\n",
    "    private_accuracy_list.append(best_private_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "plt.plot(range(1000), public_accuracy_list, label='Public accuracy')\n",
    "plt.plot(range(1000), private_accuracy_list, label='Private accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом мы можем случайным образом подстроиться под Public, завысив оценку целевой метрики (что видно на Private).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Метод k-ближайших соседей](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) (англ. k-nearest neighbors algorithm, k-NN) — метрический алгоритм для классификации или регрессии. В случае классификации алгоритм сводится к следующему:\n",
    "\n",
    "1. Рассматриваются объекты из обучающей выборки, для которых известно к какому классу они принадлежат.\n",
    "1. Между подлежащими классификации объектами и объектами тренировочной выборки вычисляется матрица попарных расстояний согласно выбранной метрике.\n",
    "1. На основе полученной матрицы расстояний для каждого из подлежащих классификации объектов определяется k ближайших объектов тренировочной выборки -- k ближайших соседей.\n",
    "1. Подлежащим классификации объектам приписывается тот класс, который чаще всего встречается у их k ближайших соседей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/knn_idea.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве примера работы с алгоритмом k-NN классифицируем изображение корабля из тестовой выборки CIFAR 10 с использованием [реализации алгоритма в scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.kneighbors_graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что мы работаем с тренировочным датасетом CIFAR-10 и хотим решить хрестоматийную задачу классификации: определить те картинки и тестового набора данных, которые относятся к классу cat. Эта задача является частным примером общей задачи классификации данных CIFAR-10, разные подходы к решению которой мы ещё неоднократно рассмотрим в ходе первых лекций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет CIFAR-10 содержит, как следует из названия, 10 различных классов изображений:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/knn_on_cifar10.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все изображения представляют собой матрицы чисел, которые кодируют цвета отдельных пикселей. Для изображений высоты $H$, ширины $W$ с $C$ цветовыми каналами получаем упорядоченный набор  $H \\times W \\times C$ чисел. В данном разделе пока не будем учитывать, что значения соседних пикселей изображения могут быть значительно связаны и будем решать задачу классификации для наивного представления изображения в виде точки в  $H \\times W \\times C$-мерном вещественном пространстве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/img_to_array.png\" width=\"700\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет CIFAR-10 содержит цветные (трехцветные) изображения размером $32 \\times 32$ пикселя. Таким образом, каждое изображение из датасета является точкой в $3072$-мерном ($32 \\times 32 \\times 3 = 3072$) вещественном пространстве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Близость данных согласно метрике"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пара изображений будет выглядеть практически идентично, если значения цветов соответствующих пикселей будут похожи по величине. Другими словами, практически идентичным изображениям будут соответствовать **близкие** точки нашего многомерного вещественного пространства. Для численной характеристики **близости**, можно определить функцию подсчета расстояния между парой точек — метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известны различные способы задания [функции расстояния между парой точек](https://en.wikipedia.org/wiki/Metric_(mathematics)). Простейшим примером является широкого известная **Евклидова** ($L_2$) метрика:\n",
    "$$L_2 (X, Y) = \\sum_i (X_i - Y_i)^2,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но, кроме неё, величина расстояния между парой точек может быть выражена рядом других функций.\n",
    "\n",
    "$L_1$-расстояние (манхэттенская метрика):\n",
    "$$L_1 (X, Y) = \\sum_i |X_i - Y_i|,$$\n",
    "\n",
    "угловое расстояние:\n",
    "$$ang (X, Y) = \\frac{1}{\\pi} \\arccos \\frac{\\sum_i X_i Y_i}{\\sqrt{\\sum_i X_i^2} \\sqrt{\\sum_i Y_i^2}} ,$$\n",
    "\n",
    "и многие другие. От выбора конкретной функции расстояния между точками будет явно зависеть представление о **близости** точек --- объекты, близкие по одной из метрик, вовсе не обязаны оказаться близкими по согласно другой. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем вычислить $L_1$ расстояние между несколькими первыми изображениями из тестового набора данных CIFAR-10 с использованием реализованного в пакете [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric.get_metric) класса `sklearn.metrics.DistanceMetric`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L01/out/metric_to_compare_train_and_test_imgs.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from torchvision.datasets\n",
    "from torchvision import datasets\n",
    "\n",
    "train_set = datasets.CIFAR10(\"content\", train=True,  download=True)\n",
    "val_set = datasets.CIFAR10(\"content\", train=False, download=True)\n",
    "labels_names = train_set.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем три изображения из тестового набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_1 = train_set.data[0]\n",
    "img_2 = train_set.data[1]\n",
    "img_3 = train_set.data[2]\n",
    "\n",
    "fix, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "ax[0].set_title('First image in CIFAR10 train data')\n",
    "ax[0].imshow(img_1)\n",
    "ax[1].set_title('Second image in CIFAR10 train data')\n",
    "ax[1].imshow(img_2)\n",
    "ax[2].set_title('Third image in CIFAR10 train data')\n",
    "ax[2].imshow(img_3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ship_img = val_set.data[18]\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_ship_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# in order to limit computational time\n",
    "index_limiter = 5000\n",
    "X = train_set.data.reshape(train_set.data.shape[0], -1)[:index_limiter]\n",
    "y = train_set.targets[:index_limiter]\n",
    "\n",
    "for metric_type in ['euclidean', 'manhattan', 'chebyshev']:\n",
    "    print()\n",
    "    for k in range(3, 7, 1):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric_type)\n",
    "        knn.fit(X, y)\n",
    "        result_class_id = knn.predict([sample_ship_img.flatten()])[0]\n",
    "        result_class = train_set.classes[result_class_id]\n",
    "        print(f'{k}-NN with {metric_type} metric\\npredicted class is: {result_class}\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30 признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n",
    "\n",
    "Можно иметь сколь угодно хороший алгоритм для классификации - но до тех пор, пока данные на входе - мусор, на выходе из нашего чудесного классификатора мы тоже будем получать мусор **(*garbage in - garbage out*)**. Давайте разберемся, что конкретно надо сделать, чтобы k-NN реально заработал.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer() # load data\n",
    "\n",
    "x = cancer.data # features\n",
    "y = cancer.target # labels(classes)\n",
    "print(f'x shape: {x.shape}, y shape: {y.shape}') \n",
    "print(f'x[0]: \\n {x[0]}') \n",
    "print(f'y[0]: \\n {y[0]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько данных в классе `0` и сколько данных в классе `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5)) # set fig size \n",
    "plt.bar(1,y[y==1].shape, label=cancer.target_names[0]) # 1 label \n",
    "plt.bar(0,y[y==0].shape, label=cancer.target_names[1]) # 0 label\n",
    "plt.title('Class balance')\n",
    "plt.ylabel('Num examples')\n",
    "plt.xticks(ticks=[1,0], labels=['1','0']) \n",
    "plt.legend(loc='upper left') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк в каждой, из которой, по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.boxenplot(data=pd.DataFrame(x), orient=\"h\", palette=\"Set2\")\n",
    "ax.set(xscale='log', xlim=(1e-4, 1e4), xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы адекватно сравнить данные между собой нам следует использовать нормализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализация, выбор Scaler**\n",
    "\n",
    "Нормализацией называется процедура приведения входных данных к единому масштабу (диапазону) значений. Фактически, это означает построение взаимно однозначного соответствия между некоторыми размерными величинами (которые измеряются в метрах, килограммах, годах и т. п.) и их безразмерными аналогами, принимающими значение в строго определенном числовом диапазоне (скажем, на отрезке $[0,1]$). Преобразование данных к единому числовому диапазону (иногда говорят *домену*) позволяет считать их равноправными признаками и единообразно передавать их на вход модели. В некоторых источниках данная процедура явно называется *масштабирование*.\n",
    "\n",
    "$$\\text{scaling map} \\; : \\text{some arbitrary feature domain} \\rightarrow \\text{definite domain} $$\n",
    "\n",
    "Иногда под нормализацией данных понимают процедуру *стандартизации*, то есть приведение множеств значений всех признаков к стандартному нормальному распределению -- распределению с нулевым среднем значением и единичной дисперсией.\n",
    "\n",
    "$$\\text{standartization map} : f_i \\rightarrow (f_i - \\text{mean} (\\{f_i\\})) \\cdot \\frac{1}{\\text{std} (\\{f_i\\})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим небольшой пример. Пусть у нас есть данные о некоторой группе людей, содержащие два признака: *возраст* (в годах) и *размер дохода* (в рублях). Возраст может измениться в диапазоне от 18 до 70 ( интервал 70-18 = 52). А доход от 30 000 р до 500 000 р (интервал 500 000 - 30 000 = 470 000). В таком варианте разница в возрасте имеет меньшее влияние, чем разница в доходе. Получается, что доход становится более важным признаком, изменения в котором влияют больше при сравнении схожести двух людей.\n",
    "\n",
    "Должно быть так, чтобы максимальные изменения любого признака в «основной массе объектов» были одинаковы. Тогда потенциально все признаки будут равноценны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось определиться с выбором инструмента, часто используют следующие варианты: `MinMaxScaler`, `StandardScaler`, `RobustScaler`.\n",
    "\n",
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для признака `data[:,0]`. **Обратите внимание на ось X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # setting the initialization parameter for random values\n",
    "\n",
    "# generate random values from 1 to 255, shape (30,1)\n",
    "test = x[:,0].reshape(-1,1)\n",
    "\n",
    "plt.figure(1, figsize=(24, 5))  \n",
    "plt.subplot(141)  # set location\n",
    "plt.scatter(test, range(len(test)), c=y)  \n",
    "plt.ylabel(\"Num examples\", fontsize=15)  \n",
    "plt.xticks(fontsize=15)  \n",
    "plt.yticks(fontsize=15)  \n",
    "plt.title(\"Non scaled data\", fontsize=18)  \n",
    "\n",
    "# scale data with MinMaxScaler\n",
    "test_scaled = MinMaxScaler().fit_transform(test)  \n",
    "plt.subplot(142)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"MinMaxScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with StandardScaler\n",
    "test_scaled = StandardScaler().fit_transform(test)  \n",
    "plt.subplot(143)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"StandardScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with RobustScaler\n",
    "test_scaled = RobustScaler().fit_transform(test)  \n",
    "plt.subplot(144)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"RobustScaler\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные в диапазоне от 0 до 1. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (e.g., масштабирование RGB пикселей)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$z=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "$X_{min}$ и $X_{max}$ задаются как минимальное и максимальное допустимое значение, по умолчанию:  $X_{min}=0$  и $X_{max}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение 0 и стандартное отклонение 1. Большинство значений будет в  диапазоне от -1 до 1. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-u}{s}$$\n",
    "\n",
    "$u$ — среднее значение (или 0 при `with_mean=False`) и $s$ — стандартное отклонение (или 0 при `with_std=False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И StandardScaler и MinMaxScaler очень чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. k-й процентиль – это величина, равная или не превосходящая k процентов чисел во всем имеющемся распределении. Например, 50-й процентиль (медиана) распределения таково, что 50% чисел из распределения не меньше данного числа. Соответственно, RobustScaler не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-X_{median}}{IQR}$$\n",
    "\n",
    "$X_{median}$ — значение медианы, $IQR$ — межквартильный диапазон равный разнице между 75-ым и 25-ым процентилями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = StandardScaler().fit_transform(x)  # scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_norm).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.boxenplot(data=pd.DataFrame(X_norm), \n",
    "                   orient=\"h\", \n",
    "                   palette=\"Set2\")\n",
    "ax.set(xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель на данных без нормировки и с нормировкой для 10-ти соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# split data to train/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, random_state=25)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Without normalization\")\n",
    "accuracy_train = accuracy_score(y_pred=knn.predict(X_train), y_true=Y_train)\n",
    "print('accuracy_train', accuracy_train)\n",
    "accuracy_test = accuracy_score(y_pred=knn.predict(X_test), y_true=Y_test)\n",
    "print('accuracy_test', accuracy_test)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train_norm = scaler.transform(X_train)  # scaling data\n",
    "X_test_norm = scaler.transform(X_test)  # scaling data\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train_norm, Y_train)\n",
    "\n",
    "print(\"With normalization\")\n",
    "accuracy_train = accuracy_score(y_pred=knn.predict(X_train_norm), y_true=Y_train)\n",
    "print('accuracy_train', accuracy_train)\n",
    "accuracy_test = accuracy_score(y_pred=knn.predict(X_test_norm), y_true=Y_test)\n",
    "print('accuracy_test', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Параметры и гиперпараметры модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжим с классификацией методом ближайших соседей (k-NN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-NN для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике, метод ближайших соседей для классификации используется крайне редко.\n",
    "Проблема заключается в следующем: предположим, что точность классификации нас устраивает. Теперь давайте применим k-NN на больших данных (e.g. миллион картинок). Для определения класса каждой из картинок, нам нужно сравнить ее со всеми другими картинками в базе данных, а такие расчеты, даже в существенно оптимизированном виде, занимают много времени. Мы же хотим, чтобы обученная модель работала быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тем не менее, метод ближайших соседей используется в других задачах, где без него обойтись сложно. Например, в задаче распознавания лиц. Представим, что у нас у нас есть большая база данных с фотографиями лиц (например, по 5 разных фотографий всех сотрудников, которые работают в офисном здании, как на примере выше) и есть камера, установленная на входе в это здание. Мы хотим узнать, кто и во сколько пришел на работу. Для того чтобы понять кто прошел перед камерой, нам нужно зафиксировать лицо этого человека и сравнить его со всеми фотографиями лиц в базе. В такой формулировке мы не пытаемся определить конкретный класс фотографии, а всего лишь определяем “похож-не похож”. Мы смотрим на k ближайших соседей и, например, если из k соседей, 5 — это фотографии Джеки Чана, то, скорее всего, под камерой прошел именно он. В таких случаях k-NN метод вполне полезен. Похожим образом работает и поиск дубликатов в базах данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры эффективной реализации метода на основе k-NN:\n",
    "* [Facebook AI Research Similarity Search](https://github.com/facebookresearch/faiss) – разработка команды Facebook AI Research для быстрого поиска ближайших соседей и кластеризации в векторном пространстве. Высокая скорость поиска позволяет работать с очень большими данными – до нескольких миллиардов векторов.\n",
    "* Алгоритм поиска ближайших соседей [Hierarchical Navigable Small World](https://arxiv.org/abs/1603.09320). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим k-NN для общей выборки данных, при разном значении количества соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  # array of the number of neighbors\n",
    "\n",
    "quality = np.zeros(\n",
    "    n_nei_rng.shape[0]\n",
    ")  \n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  # for all elements\n",
    "    # create knn for all num neighbors \n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_nei_rng[ind]\n",
    "    )  \n",
    "    knn.fit(X_train_norm, Y_train)  \n",
    "    q = accuracy_score(y_pred=knn.predict(X_test_norm), y_true=Y_test)  # accuracy\n",
    "    quality[ind] = q  # fill quality\n",
    "\n",
    "plt.figure(figsize=(12, 5))  \n",
    "plt.title(\"KNN on train\", size=20)  \n",
    "plt.xlabel(\"Neighbors\", size=10)  \n",
    "plt.ylabel(\"Accuracy\", size=10)  \n",
    "plt.plot(n_nei_rng, quality)  \n",
    "plt.xticks(n_nei_rng) \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество на 1 соседе - самое лучшее. Но это и понятно - ближайшим соседом элемента из обучающей выборки будет сам объект. Мы просто **запомнили** все объекты.\n",
    "\n",
    "Если теперь мы попробуем взять какой-то новый образец опухоли и классифицировать его - у нас скорее всего ничего не получится. В таких случаях мы говорим, что наша модель не умеет обобщать (*generalization*).\n",
    "\n",
    "Для того, чтобы знать заранее обобщает ли наша модель или нет, мы можем разбить все имеющиеся у нас данныe на 2 части. Но одной части мы будем обучать классификатор (*train set*), а на другой тестировать насколько хорошо он работает (*test set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nei_rng = np.arange(1, 31)  \n",
    "train_quality = np.zeros(n_nei_rng.shape[0])  # quality on train data\n",
    "test_quality = np.zeros(n_nei_rng.shape[0])  # quality on test data\n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=n_nei_rng[ind])  \n",
    "    knn.fit(X_train_norm, Y_train)  \n",
    "    \n",
    "    # accuracy on train data\n",
    "    trq = accuracy_score(y_pred=knn.predict(X_train_norm), y_true=Y_train)  \n",
    "    train_quality[ind] = trq  \n",
    "\n",
    "    # accuracy on test data\n",
    "    teq = accuracy_score(y_pred=knn.predict(X_test_norm), y_true=Y_test)  \n",
    "    test_quality[ind] = teq  \n",
    "\n",
    "# accuracy plot  on train and test data\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.xlabel(\"Neighbors\", size=12)\n",
    "plt.ylabel(\"Accuracy\", size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот, теперь мы видим, что 1 сосед был \"ложной тревогой\". Такие случаи мы называем *переобучением*. Чтобы действительно предсказывать что-то полезное, нам надо выбирать число соседей, начиная минимум с 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кросс-валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Алгоритм кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте все-таки разберемся, как подобрать гиперпараметры.\n",
    "\n",
    "Результат работы модели будет зависеть от разбиения. Поэкспериментируем с k-NN и датасетом Iris и посмотрим, как результат работы модели зависит от `random_state` для `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataset = sklearn.datasets.load_iris() # load data\n",
    "X = dataset.data   # features\n",
    "Y = dataset.target # labels(classes)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def split_and_train(X, Y, random_state):\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X, Y, train_size=0.8, stratify=Y, random_state=random_state)\n",
    "    \n",
    "    max_neighbors_cnt = 30 \n",
    "    k_neighbors_numb = np.arange(1, max_neighbors_cnt+1)  # array of the number of neighbors\n",
    "\n",
    "    train_accurecy = np.zeros(max_neighbors_cnt)\n",
    "    val_accurecy = np.zeros(max_neighbors_cnt)\n",
    "\n",
    "    for k in k_neighbors_numb:\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, Y_train)\n",
    "\n",
    "        train_accurecy[k-1] = accuracy_score(y_pred=knn.predict(X_train), y_true=Y_train) \n",
    "        val_accurecy[k-1] = accuracy_score(y_pred=knn.predict(X_val), y_true=Y_val)\n",
    "    \n",
    "    # accuracy plot on train and test data\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(\"KNN on train vs val\", size=20)\n",
    "    plt.plot(k_neighbors_numb, train_accurecy, label=\"train\")\n",
    "    plt.plot(k_neighbors_numb, val_accurecy, label=\"val\")\n",
    "    plt.legend()\n",
    "    plt.xticks(k_neighbors_numb)\n",
    "    plt.xlabel(\"Neighbors\", size=12)\n",
    "    plt.ylabel(\"Accuracy\", size=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_train(X, Y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_and_train(X, Y, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат зависит от того, как нам повезло или не повезло с разбиением данных на обучение и тест. Для одного разбиения хорошо выбрать k=3, а для другого — k=13. Кроме того, опять же - фактически, мы сами выступаем в роли модели, которая учит гиперпараметры (а не параметры) под видимую ей выборку.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Получается, что если подбирать гиперпараметры модели на *train set*, то:\n",
    "1. Можно переобучитьcя, просто на более \"высоком\" уровне. Особенно если гиперпараметров у модели много и все они разнообразны\n",
    "2. Нельзя быть уверенным, что выбор параметров не зависит от разбиения на обучение и тест "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы:\n",
    "\n",
    "1. Подбираем гиперпараметры моделей на отдельном датасете, называемым валидационным. Получаем мы его разбиением обучающего датасета на собственно обучающий и валидационный "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/split_dataset_for_train_val_test.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Чаще всего делаем несколько таких разбиений по какой-то схеме, чтобы получить уверенность оценок качества для моделей с разными гиперпараметрами - **кросс-валидация**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/out/cross_validation_on_train_data.png\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто применяется следующий подход, называемый [K-Fold кросс-валидацией](https://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "\n",
    "Берется тренировочная часть датасета, разбивается на части — блоки. Дальше мы будем использовать для проверки первую часть (Fold 1), а на остальных учиться. И так последовательно для всех частей. В результате у нас будут информация о точности для разных фрагментов данных и уже на основании этого можно понять, насколько значение этого параметра, который мы проверяем, зависит или не зависит от данных. То есть если у нас от разбиения точность при одном и том же К меняться не будет, значит мы подобрали правильное К. Если она будет сильно меняться в зависимости от того, на каком куске данных мы проводим тестирование, значит, надо попробовать другое К и если ни при каком не получилось - то это такие данные. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как работает k-Fold. Обратите внимание, что по умолчанию `shuffle = False`. Для упорядоченных данных это проблема."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "print('index without shuffle')\n",
    "kf = KFold(n_splits=3)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "print('index with shuffle')\n",
    "kf = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения стратифицированного разбиения (соотношение классов в частях разбиения сохраняется) нужно использовать [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Оценка результата кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Посмотрим на результат кросс-валидации для k-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "dataset = sklearn.datasets.load_iris() # load data\n",
    "X = dataset.data   # features\n",
    "Y = dataset.target # labels(classes)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, \n",
    "    stratify=Y, random_state=42)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "accuracy = cross_val_score(knn, X_train, Y_train, cv=cv, scoring='accuracy')\n",
    "print('3NN accuracy: ', accuracy)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % \n",
    "      (accuracy.mean(), accuracy.std()))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "accuracy = cross_val_score(knn, X_train, Y_train, cv=cv, scoring='accuracy')\n",
    "print('5NN accuracy: ', accuracy)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % \n",
    "      (accuracy.mean(), accuracy.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В идеальном случае выбираются гиперпараметры, для которых матожидание метрик качества выше, а дисперсия меньше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Типичные ошибки при кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Можно ли делать только кросс-валидацию (без теста)?**\n",
    "\n",
    "Нет, нельзя. Кросс-валидация не до конца спасает от подгона параметров модели под выборку, на которой она проводится. Оценка конечного качества модели должно производиться на отложенной тестовой выборке. Если у вас очень мало данных, можно рассмотреть [вложенную кросс-валидацию](https://weina.me/nested-cross-validation/), Речь об этом пойдет позже, в последующих лекциях. Но даже в этом случае придется анализировать поведение модели, чтобы показать, что она учит что-то разумное. Кстати, вложенную кросс-валидацию можно использовать, чтобы просто получить более устойчивую оценку поведения модели на тесте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-валидация для научных исследований: на что обратить внимание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При кросс-валидации, чтобы получить адекватную оценку метрик, следует соблюдать те же правила, что и при разбиении на train и test, а именно:\n",
    "* избегать дублирования данных,\n",
    "* перемешивать упорядоченные данные и сохранять баланс классов,\n",
    "* разделять данные из различных источников. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подбора параметров модели используется **GridSearchCV**.\n",
    "\n",
    "GridSearchCV – это инструмент для автоматического подбора параметров моделей машинного обучения. GridSearchCV находит наилучшие параметры путем обычного перебора: он создает модель для каждой возможной комбинации параметров из заданной сетки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет Iris маловат для подбора параметров, поэтому создадим свой датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, Y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.scatter(X[:,0], X[:,1], c=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отложим test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, \n",
    "    stratify=Y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем подобрать параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from IPython.display import clear_output\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\"\"\"\n",
    "Parameters for GridSearchCV:\n",
    "estimator — model\n",
    "cv — num of fold to cross-validation splitting \n",
    "param_grid — parameters names\n",
    "scoring — metrics \n",
    "n_jobs - number of jobs to run in parallel, -1 means using all processors.\n",
    "\"\"\"\n",
    "\n",
    "model = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    cv=KFold(5, shuffle=True, random_state=42),\n",
    "    param_grid={\n",
    "        \"n_neighbors\": np.arange(1, 31),\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train, Y_train)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем лучшие гиперпараметры для модели, которые подобрали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metric:\", model.best_params_[\"metric\"])\n",
    "print(\"Num neighbors:\", model.best_params_[\"n_neighbors\"])\n",
    "print(\"Weigths:\", model.best_params_[\"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Объект GridSearchCV можно использовать как обычную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print(f\"Percent correct predictions {np.round(accuracy_score(y_pred=Y_pred, y_true=Y_test)*100,2)} %\")\n",
    "print(f\"Percent correct predictions(balanced classes) {np.round(balanced_accuracy_score(y_pred=Y_pred, y_true=Y_test)*100,2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем извлечь дополнительные данные о кросс-валидации и по ключу обратиться к результатам всех моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем для примера mean_test_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(model.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"mean_test_score\", size=20)\n",
    "plt.xlabel(\"Num of experiment\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим, например, при фиксированных остальных параметрах (равных лучшим параметрам), качество модели на валидации в зависимости от числа соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_means = []\n",
    "selected_std = []\n",
    "n_nei = []\n",
    "for ind, params in enumerate(model.cv_results_[\"params\"]):\n",
    "    if (\n",
    "        params[\"metric\"] == model.best_params_[\"metric\"]\n",
    "        and params[\"weights\"] == model.best_params_[\"weights\"]\n",
    "    ):\n",
    "        n_nei.append(params[\"n_neighbors\"])\n",
    "        selected_means.append(model.cv_results_[\"mean_test_score\"][ind])\n",
    "        selected_std.append(model.cv_results_[\"std_test_score\"][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим error bar, для сравнения разброса ошибки при разном количестве соседей Neighbors. \n",
    "\n",
    "Видим, что на самом деле большой разницы в числе соседей, начиная с 11, и нет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(f\"KNN CV, {params['metric']}, {params['weights']}\", size=20)\n",
    "plt.errorbar(n_nei, selected_means, yerr=selected_std, linestyle=\"None\", fmt=\"-o\")\n",
    "plt.xticks(n_nei)\n",
    "plt.ylabel(\"Mean_test_score\", size=15)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RandomizedSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативой GridSearch является [RandomizedSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). Если в GridSearch поиск параметров происходит по фиксированному списку значений, то RandomizedSearch умеет работать с непрерывными значениями, случайно выбирая тестируемые значения, что может привести к более точной настройке гиперпараметров."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
