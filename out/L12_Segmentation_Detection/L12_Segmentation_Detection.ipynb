{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L12_Segmentation_Detection_gan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahVgVn5oGhxE"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
        "\n",
        "Кроме классификации CV решает и другие задачи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In5ldMKUQX2v"
      },
      "source": [
        "## COCO\n",
        "\n",
        "Common objects in context\n",
        "\n",
        "\n",
        "- один из наиболее популярных датасатов содержащий данные для сегментации и детектирования.\n",
        "\n",
        "- categoryes\n",
        "- masks\n",
        "- bounding boxes\n",
        "- captions\n",
        "- person_keypoints\n",
        "...\n",
        "\n",
        "https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AdGMnQwQY_V"
      },
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "!unzip annotations_trainval2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHXV3kO8WCuF"
      },
      "source": [
        "Для работы с датасетом используется пакет `pycocotools`\n",
        "\n",
        "В COCO 90 категорий объектов\n",
        "\n",
        "https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5abXyerDTNdE"
      },
      "source": [
        "from pycocotools.coco import COCO\n",
        "\n",
        "coco=COCO('annotations/instances_val2017.json')\n",
        "\n",
        "cat_ids = coco.getCatIds()\n",
        "print(\"Categories count\",len(cat_ids))\n",
        "print(cat_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWU2rsgGWKIi"
      },
      "source": [
        "0 - не используется в качестве номера категории. Обычно его используют для обозначения класса фона."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUzMGpdyUqxR"
      },
      "source": [
        "# display COCO categories and supercategories\n",
        "\n",
        "cats = coco.loadCats(coco.getCatIds())\n",
        "num2cat =  {}\n",
        "print('COCO categories: ')\n",
        "for cat in cats:\n",
        "  num2cat[cat['id']] = cat['name']\n",
        "  print( cat['id'], \":\" , cat['name'],end=\"   \" )\n",
        "#print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Re-0mQWbKj"
      },
      "source": [
        "Есть так же суперкатегории"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSVZjzJXU3kp"
      },
      "source": [
        "print(cats[2])\n",
        "print(cats[3])\n",
        "\n",
        "\n",
        "nms = set([cat['supercategory'] for cat in cats])\n",
        "print('COCO supercategories: \\n{}'.format(' '.join(nms)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBbsFsT_W2US"
      },
      "source": [
        "Датесет большой, поэтому удобно выгружать данные частями"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvFbQwCCW-po"
      },
      "source": [
        "# get all images containing given categories, select one at random\n",
        "catIds = coco.getCatIds(catNms=['person','cat']); # person and cat\n",
        "imgIds = coco.getImgIds(catIds=catIds );\n",
        "#imgIds = coco.getImgIds() #imgIds = [324158]\n",
        "print(\"Total images with person and cat \",len(imgIds))\n",
        "print(imgIds)\n",
        "img_list = coco.loadImgs(imgIds[0])\n",
        "img = img_list[0]\n",
        "print(\"Image data\", img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKViu19eHz33"
      },
      "source": [
        "Load image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv7WBDvVH13e"
      },
      "source": [
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "I = io.imread(img['coco_url'])\n",
        "plt.axis('off')\n",
        "plt.imshow(I)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU16f6RBIsjT"
      },
      "source": [
        "Конвертация в PIL формат"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xTqZnEcI5m4"
      },
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def coco2pil(url):\n",
        "  print(url)\n",
        "  response = requests.get(img['coco_url'])\n",
        "  return Image.open(BytesIO(response.content))\n",
        "\n",
        "pil_img = coco2pil(img['coco_url'])\n",
        "plt.imshow(pil_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkYH1oG9J36h"
      },
      "source": [
        "Информация о разметке\n",
        "\n",
        "https://cocodataset.org/#format-data\n",
        "\n",
        "\n",
        "```\n",
        "  \"segmentation\" : RLE or [polygon],\n",
        "  \"area\" : float,\n",
        "  \"bbox\" : [x,y,width,height],\n",
        "  \"iscrowd\" : 0 or 1,\n",
        "```\n",
        "\n",
        "Полигон это набор координат [x1,y1, x2,y2 ... ]\n",
        "\n",
        "объект может описываться несколькими полигонами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cddQBoKWJ6z_"
      },
      "source": [
        "# load and display instance annotations\n",
        "plt.imshow(I); plt.axis('off')\n",
        "annIds = coco.getAnnIds(imgIds=img['id'])\n",
        "anns = coco.loadAnns(annIds)\n",
        "\n",
        "def dump_anns(anns):\n",
        "  for i, a in enumerate(anns):\n",
        "    print(f\"#{i}\")\n",
        "    for k in a.keys():\n",
        "      if k == 'category_id' and num2cat.get(a[k],None):\n",
        "        print(k,\": \",a[k], num2cat[a[k]]) # Show cat. name\n",
        "      else:\n",
        "        print(k,\": \",a[k])\n",
        "\n",
        "dump_anns(anns)\n",
        "coco.showAnns(anns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-ywQiAcMZv-"
      },
      "source": [
        "Что такое [RLE](https://en.wikipedia.org/wiki/Run-length_encoding) ?\n",
        "\n",
        "run-length encoding\n",
        "\n",
        "https://www.youtube.com/watch?v=h6s61a_pqfM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8b8DiNvMfaC"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (160,80)\n",
        "print(catIds)\n",
        "annIds = coco.getAnnIds(catIds=catIds, iscrowd = True)\n",
        "anns = coco.loadAnns(annIds[0:1])\n",
        "\n",
        "dump_anns(anns)\n",
        "\n",
        "img = coco.loadImgs(anns[0]['image_id'])[0]\n",
        "I = io.imread(img['coco_url'])\n",
        "plt.imshow(I); #plt.axis('off')\n",
        "coco.showAnns(anns) # People in the stands \n",
        "seg = anns[0]['segmentation']\n",
        "\n",
        "print('Counts',len(seg['counts']))\n",
        "print('Size',seg['size'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7lnz4idJ2Zv"
      },
      "source": [
        "Как получить маску в виде массива?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3IO9QOfckNh"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
        "import numpy as np\n",
        "annIds = coco.getAnnIds(imgIds=[448263])\n",
        "\n",
        "anns = coco.loadAnns(annIds)\n",
        "msk = np.zeros(seg['size'])\n",
        "for i in range(len(anns)):\n",
        "  msk += coco.annToMask(anns[i])\n",
        "print(msk.shape)\n",
        "plt.figure()\n",
        "plt.imshow(msk)\n",
        "\n",
        "print(msk)\n",
        "print(np.unique(msk))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCE11hoZtr-x"
      },
      "source": [
        "## Semantic segmentation\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-007.png\" width=\"700\">\n",
        "\n",
        "Постановка задачи:\n",
        "\n",
        "Предсказать класс для каждого пикселя.\n",
        "\n",
        "Входные данные маска: \n",
        "\n",
        "[ x,y - > class_num ] \n",
        "\n",
        "Выходные данные маска:\n",
        "\n",
        "[ x,y - > class_num ] \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvycryQcvA-x"
      },
      "source": [
        "Способы решения\n",
        "\n",
        "*  а) Наивный.\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-012.png\" width=\"700\">\n",
        "\n",
        "\n",
        "Скользящим окном пройтись по изображению и предсказать клас для каждого пикселя с учетом его соседей.\n",
        "\n",
        "*  б) Разумный\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-015.png\" width=\"700\">\n",
        "\n",
        "\n",
        "Убрать линейный слой в конце сети. По признакам во всех каналах определять класс каждого пикселя.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_15.png\"  width=\"700\">\n",
        "\n",
        "В лекции №8 мы говорили о том что сверту 1x1 можно рассматривать как полносвязанный слой.\n",
        "\n",
        "\n",
        "Именно так она и будет использоваться при сегментации.\n",
        "\n",
        "Количество классов будет соответствовать числу каналов.\n",
        "\n",
        "\n",
        "Проблемы:\n",
        "- нужно большое рецептивное поле, следовательно много слоев ( L 3х3 conv -> 1+2L receptive field)\n",
        "- очень медленно на полноразмерных картах активации\n",
        "\n",
        "*  в) Эффективный\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-017.png\" width=\"700\">\n",
        "\n",
        "\n",
        "Вернмся к традиционной структуре со сжатием пространственных размеров. А после нее добавим разжимающий блок. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SycYXvdyAy8"
      },
      "source": [
        "## Автокодировщик\n",
        "\n",
        "Такая архитектура довольно популярна и применяется не только для сегментации: \n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-07.png\" width=\"700\">\n",
        "\n",
        "\n",
        "- сглаживание шума;\n",
        "- снижение размерности -> вектор признак\n",
        "- генерация данных\n",
        "... \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2OQ9c7Pyjs7"
      },
      "source": [
        "## Как устроен разжимающий(upsample) блок?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP487Kw5zJm0"
      },
      "source": [
        "### Изменение размеров изображений \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-08.png\" width=\"700\">\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-10.png\" width=\"700\">\n",
        "\n",
        "С картами признаков можно обращаться так же как и с пикселями. Для этого в Pytorchществует метод Upsample\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-11.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfyGBfOfzV-W"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def upsample( pil, mode='nearest' ):\n",
        "  tensor = TF.to_tensor(pil)\n",
        "  upsampler = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "  tensor_128 = upsampler(tensor.unsqueeze(0))\n",
        "  im_128 = TF.to_pil_image(tensor_128.squeeze()).convert(\"RGB\")\n",
        "  fig = plt.figure()\n",
        "  fig.suptitle(mode)\n",
        "  plt.imshow(im_128)\n",
        "\n",
        "\n",
        "man_with_cat = coco2pil('http://images.cocodataset.org/val2017/000000223747.jpg')\n",
        "pil_64 = man_with_cat.resize((64,64))\n",
        "plt.figure()\n",
        "plt.imshow(pil_64)\n",
        "\n",
        "\n",
        "upsample( pil_64, mode='nearest' )\n",
        "upsample( pil_64, mode='bilinear' )\n",
        "upsample( pil_64, mode='bicubic' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8fGcmyWhLZs"
      },
      "source": [
        "### Bed of Nails\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-12.png\" width=\"700\">\n",
        "\n",
        "Способ восстановления размерности когда наэлементы из начальной карты признаков копируются без изменения, а новые ячейки  вокруг них заполняются нулями."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeoFzo8khoWi"
      },
      "source": [
        "### MaxUnpooling\n",
        "\n",
        "Принципиальная разница в том что индексы элементов запоминаются.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-019.png\" width=\"700\">\n",
        "\n",
        "Сохраняем индексы каждого max pooling слоя\n",
        "При повышении разрешения копируем значения из выхода max pooling слоя с учетом запомненных индексов\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-14.png\" width=\"700\">\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6yI0TnxilbL"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms.functional as TF\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def tensor_show( tensor,title = ''):\n",
        "  im = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
        "  fig = plt.figure()\n",
        "  fig.suptitle(title + str(im.size))\n",
        "  plt.imshow(im)\n",
        "\n",
        "pool = nn.MaxPool2d(kernel_size = 2, return_indices = True) # False by default\n",
        "unpool = nn.MaxUnpool2d(kernel_size = 2)\n",
        "\n",
        "pil = coco2pil('http://images.cocodataset.org/val2017/000000223747.jpg')\n",
        "fig = plt.figure()\n",
        "fig.suptitle('original ' +str(pil.size))\n",
        "plt.imshow(pil)\n",
        "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
        "print(\"Initial shape\",tensor.shape)\n",
        "\n",
        "# Downsample\n",
        "tensor_half_res, indexes1  = pool(tensor)\n",
        "print(\"Indexes shape\",indexes1.shape)\n",
        "tensor_show( tensor_half_res,\"1/2 down\")\n",
        "\n",
        "\n",
        "\n",
        "tensor_q_res, indexes2  = pool(tensor_half_res)\n",
        "tensor_show( tensor_q_res, \"1/4 down\")\n",
        "\n",
        "# Upsample\n",
        "tensor_half_res1 = unpool(tensor_q_res,indexes2)\n",
        "\n",
        "#https://pytorch.org/docs/stable/nn.html#padding-layers\n",
        "pad = nn.ZeroPad2d((0,0,0,1))\n",
        "tensor_half_res1 = pad(tensor_half_res1)\n",
        "tensor_show( tensor_half_res1 ,\"1/2 up\")\n",
        "\n",
        "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
        "tensor_show( tensor_recovered, \"full size up\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw4WCDjwq72S"
      },
      "source": [
        "Зачем нужен pad?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef_rFbEOrDn1"
      },
      "source": [
        "### Transpose convolution\n",
        "\n",
        "Способы восстановления пространственных размерностей которые мы рассмотрели, не содержали обучаемых параметров.\n",
        "\n",
        "\n",
        "Обычная свертка:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-16.png\" width=\"700\">\n",
        "\n",
        "\n",
        "Upsample/transpose convolution\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-17.png\" width=\"700\">\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-15.png\" width=\"700\">\n",
        "\n",
        "https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544\n",
        "\n",
        "\n",
        "Pytorch\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-19.png\" width=\"700\">\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYvxvYbDsRAI"
      },
      "source": [
        "# With square kernels and equal stride\n",
        "m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n",
        "# non-square kernels and unequal stride and with padding\n",
        "m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
        "input = torch.randn(20, 16, 50, 100)\n",
        "output = m(input)\n",
        "# exact output size can be also specified as an argument\n",
        "input = torch.randn(1, 16, 12, 12)\n",
        "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
        "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
        "h = downsample(input)\n",
        "print(h.size())\n",
        "output = upsample(h, output_size=input.size())\n",
        "print(output.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LLWHPwctQ3j"
      },
      "source": [
        "## Unet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qk_wndhsgK3"
      },
      "source": [
        "## Предобученные модели для сегментации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3tHdZL4QAkw"
      },
      "source": [
        "FCN \n",
        "\n",
        "Fully Convolutional Network\n",
        "для токо что бы не было путаницы с Fully Connected Network\n",
        "последние именуют MLP (Multi Layer Perceptron)\n",
        "\n",
        "Usage example\n",
        "https://pytorch.org/hub/pytorch_vision_fcn_resnet101/\n",
        "\n",
        "https://pytorch.org/vision/stable/models.html#semantic-segmentation\n",
        "\n",
        "The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in references/segmentation/coco_utils.py. The classes that the pre-trained model outputs are the following, in order:\n",
        "['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        " 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        " 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AfjUfUeQAQG"
      },
      "source": [
        "import torchvision\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "classes = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        " 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        " 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "fcn_model = torchvision.models.segmentation.fcn_resnet50(pretrained=True,  num_classes=21)\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #ImageNet\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(pil_img)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = fcn_model(input_tensor.unsqueeze(0))#['out'][0]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCmW6VBdo0id"
      },
      "source": [
        "Возвращаются 2 массива\n",
        "\n",
        "* out - at each location, there are unnormalized probabilities corresponding to the prediction of each class\n",
        "\n",
        "* aux - contains the auxillary loss values per-pixel. In inference mode, output['aux'] is not usefulcontains the auxillary loss values per-pixel. In inference mode, output['aux'] is not useful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsy2uhG90E1U"
      },
      "source": [
        "a = torch.tensor([\n",
        "                  [[1,4],[7,0.99]],\n",
        "                  [[0.8,1],[9,12]]\n",
        "                ]\n",
        "                  )\n",
        "a\n",
        "torch.argmax(a, dim=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4GU1E55pbmC"
      },
      "source": [
        "print(output.keys()) # Ordered dictionary\n",
        "print(\"out\", output['out'].shape,\"Batch, class_num, h, w\") \n",
        "print(\"aux\", output['aux'].shape,\"Batch, class_num, h, w\") \n",
        "# aux and output['aux'] contains the auxillary loss values per-pixel. In inference mode, output['aux'] is not usefulcontains the auxillary loss values per-pixel. In inference mode, output['aux'] is not useful\n",
        "\n",
        "#at each location, there are unnormalized probabilities corresponding to the prediction of each class\n",
        "output_predictions = output['out'][0].argmax(0)  # for first element of batch\n",
        "print(output_predictions.shape)\n",
        "print(output_predictions,torch.max(output_predictions))\n",
        "indexes = output_predictions\n",
        "for i, cls_name in enumerate(classes):\n",
        "  #print(cls_name)\n",
        "  #print(indexes,indexes.shape)\n",
        "  mask = torch.zeros(indexes.shape)\n",
        "  mask[indexes == i] = 255 \n",
        "  #print(i,class_prob.shape)\n",
        "  #mask = class_prob.byte()\n",
        "  #print(mask,mask.shape,torch.max(mask))\n",
        "  fig = plt.figure()\n",
        "  fig.suptitle(cls_name)\n",
        "  plt.imshow(mask)\n",
        "  #color = i *10\n",
        "  #break;\n",
        "  #at each location, there are unnormalized probabilities corresponding to the prediction of each class\n",
        "  output_predictions += class_prob.argmax(0) * color\n",
        "\n",
        "#print(output_predictions)\n",
        "#plt.imshow(output_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8-L4xmRkMTc"
      },
      "source": [
        "# create a color pallette, selecting a color for each class\n",
        "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
        "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
        "colors = (colors % 255).numpy().astype(\"uint8\")\n",
        "\n",
        "# plot the semantic segmentation predictions of 21 classes in each color\n",
        "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(pil_img.size)\n",
        "r.putpalette(colors)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(r)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrAVWsYIPl8F"
      },
      "source": [
        "DeepLab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAyPbe1JPo2b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToP9uPsctEif"
      },
      "source": [
        "## Оценка точности"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhZKSnT1tRmE"
      },
      "source": [
        "### IoU\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-115.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZk05YL8Iy2p"
      },
      "source": [
        "### COCO mAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAHexXHGJ8NW"
      },
      "source": [
        "Конвертация результатов сегментации в COCO формат.\n",
        "\n",
        "https://www.javaer101.com/en/article/18652684.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h18JTYuW_rC"
      },
      "source": [
        "Синхронизируем метки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AunvkGROXALy"
      },
      "source": [
        "pascal2coco = {}\n",
        "print(classes)\n",
        "def find_in_dic(dic,val):\n",
        "  for key in dic.keys():\n",
        "    if dic.get(key,None) == val:\n",
        "      return key\n",
        "  return 0 # Assign missed classes to bg\n",
        "\n",
        "print(num2cat)\n",
        "for i in range(1, len(classes)): # Skip BG\n",
        "  #if cats.get()\n",
        "  coco_ind = find_in_dic(num2cat,classes[i])\n",
        "  pascal2coco[i] = coco_ind\n",
        "\n",
        "print(pascal2coco) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qF1kK4d-I1"
      },
      "source": [
        "Create gt file\n",
        "\n",
        "\n",
        "annIds = coco.getAnnIds(catIds=catIds, iscrowd = True)\n",
        "anns = coco.loadAnns(annIds[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YslfqiBhW-fN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMrptHeS-4CX"
      },
      "source": [
        "from pycocotools import mask\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "\n",
        "def binary_mask_to_rle(binary_mask):\n",
        "    rle = {'counts': [], 'size': list(binary_mask.shape)}\n",
        "    counts = rle.get('counts')\n",
        "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n",
        "        if i == 0 and value == 1:\n",
        "            counts.append(0)\n",
        "        counts.append(len(list(elements)))\n",
        "    return rle\n",
        "\n",
        "detection_res = []\n",
        "for i, cls_name in enumerate(classes):\n",
        "  binary_mask = torch.zeros(indexes.shape)\n",
        "  binary_mask[indexes == i] = 1 \n",
        "\n",
        "  if i > 0 and torch.max(binary_mask) > 0  :\n",
        "  #print(binary_mask.shape,torch.max(binary_mask))\n",
        "    uncompressed_rle = binary_mask_to_rle(binary_mask.numpy()) #encoded_gt,\n",
        "    fortran_gt_binary_mask = np.asfortranarray(binary_mask).astype('uint8')\n",
        "    #print(fortran_ground_truth_binary_mask)\n",
        "    encoded_gt = mask.encode(fortran_ground_truth_binary_mask)\n",
        "    #decoded = mask.decode(encoded_ground_truth)\n",
        "    #print(decoded)\n",
        "    bbox = list(mask.toBbox(encoded_ground_truth))\n",
        "    print(bbox)\n",
        "  #ground_truth_area = mask.area(encoded_ground_truth)\n",
        "  #ground_truth_bounding_box = mask.toBbox(encoded_ground_truth)\n",
        "\n",
        "\n",
        "    detection_res.append({\n",
        "        'score': 1., #dummy\n",
        "        'category_id': pascal2coco[i],\n",
        "        #'segmentation' : uncompressed_rle,\n",
        "        'bbox': bbox, #[80.0, 6.49, 223.89, 223.36], #\n",
        "        'image_id': 448263,\n",
        "        #'iscrowd' : 1\n",
        "    })\n",
        "\n",
        "print(detection_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXtCIC0Ad9yY"
      },
      "source": [
        "import json \n",
        "'''\n",
        "for anno in anns:\n",
        "        detection_res.append({\n",
        "            'score': 1.,\n",
        "            'category_id': anno['category_id'],\n",
        "            'bbox': anno['bbox'],\n",
        "            'image_id': anno['image_id']\n",
        "        })\n",
        "'''    \n",
        " \n",
        "with open('seg_gt.json', 'w', encoding='utf-8') as f:\n",
        "  json.dump(anns, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfA1-0PcbqbB"
      },
      "source": [
        "    from pycocotools.coco import COCO\n",
        "    from pycocotools.cocoeval import COCOeval\n",
        "    import json\n",
        "    from tempfile import NamedTemporaryFile\n",
        "     \n",
        "    # json file in coco format, original annotation data\n",
        "    #anno_file = 'annotations/instances_val2017.json'\n",
        "    #coco_gt = COCO('seg_gt.json')\n",
        "     \n",
        "     # Use GT box as prediction box for calculation, the purpose is to get detection_res\n",
        "    #with open(anno_file, 'r') as f:\n",
        "    #    json_file = json.load(f)\n",
        "    #annotations = json_file['annotations']\n",
        "\n",
        "\n",
        "    #detection_res = []\n",
        "  \n",
        "   \n",
        "\n",
        "    import json\n",
        "    with open('seg_res.json', 'w', encoding='utf-8') as f:\n",
        "      json.dump(detection_res, f, ensure_ascii=False, indent=4)\n",
        "    #tf = file.open('seg_res.json'):\n",
        "    # Due to subsequent needs, first convert detection_res to binary and then write it to the json file\n",
        "    #content = json.dumps(detection_res).encode(encoding='utf-8')\n",
        "    #tf.write(content)\n",
        "    res_path = tf.name\n",
        "     \n",
        "    # loadRes will generate a new COCO type instance based on coco_gt and return\n",
        "    coco_dt = coco_gt.loadRes('seg_res.json')\n",
        "    coco_gt = coco.loadRes('seg_gt.json')\n",
        "  \n",
        "    cocoEval = COCOeval(coco_gt, coco_dt,'bbox') # 'segm', 'bbox'\n",
        "    #cocoEval.params.useSegm = True\n",
        "    cocoEval.evaluate()\n",
        "    cocoEval.accumulate()\n",
        "    cocoEval.summarize()\n",
        "     \n",
        "    print(cocoEval.stats)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}