{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEXUmzjm7jbk"
   },
   "source": [
    "# ДЗ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TM-Tjqnw8Dw_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
    "\n",
    "model = ['KNN','LC','FCN','CNN',\"CNN+BN\",\"Resnet\",\"Resnet + aug.\"]\n",
    "accuracy = [0.35,0.39,0.52,0.7,0.78,0.82,0.9]\n",
    "plt.bar(model,accuracy)\n",
    "plt.title('CIFAR10 accuracy')\n",
    "plt.xlabel('model')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrlVhpyF_c9T"
   },
   "source": [
    "Siamese Net\n",
    "\n",
    "\n",
    "051/04_051.png\t051_forg/01_0120051.PNG\t1 (**1 == Forged**)\n",
    "\n",
    "for CosineEmbedding\n",
    "\n",
    "1 -> -1\n",
    "\n",
    "0 -> 1\n",
    "\n",
    "\n",
    "https://www.kaggle.com/robinreni/signature-classification-using-siamese-pytorch/comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahVgVn5oGhxE"
   },
   "source": [
    "# Задачи\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "Кроме классификации CV решает и другие задачи.\n",
    "\n",
    "Сегментация, детектирование ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In5ldMKUQX2v"
   },
   "source": [
    "## COCO\n",
    "\n",
    "Прежде чем говорить о способах их решения разберемся с форматами фходных данных.\n",
    "\n",
    "Common objects in context\n",
    "\n",
    "- один из наиболее популярных датасатов содержащий данные для сегментации и детектирования.\n",
    "\n",
    "- categoryes\n",
    "- masks\n",
    "- bounding boxes\n",
    "- captions\n",
    "- person_keypoints\n",
    "...\n",
    "\n",
    "https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AdGMnQwQY_V"
   },
   "outputs": [],
   "source": [
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHXV3kO8WCuF"
   },
   "source": [
    "Для работы с датасетом используется пакет `pycocotools`\n",
    "\n",
    "В COCO 90 категорий объектов\n",
    "\n",
    "https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5abXyerDTNdE"
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco=COCO('annotations/instances_val2017.json')\n",
    "\n",
    "cat_ids = coco.getCatIds()\n",
    "print(\"Categories count\",len(cat_ids))\n",
    "print(cat_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWU2rsgGWKIi"
   },
   "source": [
    "0 - не используется в качестве номера категории. Обычно его используют для обозначения класса фона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUzMGpdyUqxR"
   },
   "outputs": [],
   "source": [
    "# display COCO categories and supercategories\n",
    "\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "num2cat =  {}\n",
    "print('COCO categories: ')\n",
    "for cat in cats:\n",
    "  num2cat[cat['id']] = cat['name']\n",
    "  print( cat['id'], \":\" , cat['name'],end=\"   \" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08Re-0mQWbKj"
   },
   "source": [
    "Есть так же суперкатегории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSVZjzJXU3kp"
   },
   "outputs": [],
   "source": [
    "print(cats[2])\n",
    "print(cats[3])\n",
    "\n",
    "\n",
    "nms = set([cat['supercategory'] for cat in cats])\n",
    "print('COCO supercategories: \\n{}'.format(' '.join(nms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBbsFsT_W2US"
   },
   "source": [
    "Датесет большой, поэтому удобно выгружать данные частями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvFbQwCCW-po"
   },
   "outputs": [],
   "source": [
    "# get all images containing given categories, select one at random\n",
    "catIds = coco.getCatIds(catNms=['person','cat']); # person and cat\n",
    "imgIds = coco.getImgIds(catIds=catIds );\n",
    "print(\"Total images with person and cat \",len(imgIds))\n",
    "print(imgIds)\n",
    "img_list = coco.loadImgs(imgIds[0])\n",
    "img = img_list[0]\n",
    "print(\"Image data\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKViu19eHz33"
   },
   "source": [
    "Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iv7WBDvVH13e"
   },
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "I = io.imread(img['coco_url'])\n",
    "plt.axis('off')\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU16f6RBIsjT"
   },
   "source": [
    "Конвертация в PIL формат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xTqZnEcI5m4"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def coco2pil(url):\n",
    "  print(url)\n",
    "  response = requests.get(img['coco_url'])\n",
    "  return Image.open(BytesIO(response.content))\n",
    "\n",
    "pil_img = coco2pil(img['coco_url'])\n",
    "plt.imshow(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkYH1oG9J36h"
   },
   "source": [
    "Информация о разметке\n",
    "\n",
    "https://cocodataset.org/#format-data\n",
    "\n",
    "\n",
    "```\n",
    "  \"segmentation\" : RLE or [polygon],\n",
    "  \"area\" : float,\n",
    "  \"bbox\" : [x,y,width,height],\n",
    "  \"iscrowd\" : 0 or 1,\n",
    "```\n",
    "\n",
    "Полигон это набор координат [x1,y1, x2,y2 ... ]\n",
    "\n",
    "объект может описываться несколькими полигонами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cddQBoKWJ6z_"
   },
   "outputs": [],
   "source": [
    "# load and display instance annotations\n",
    "plt.imshow(I); plt.axis('off')\n",
    "annIds = coco.getAnnIds(imgIds=img['id'])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "def dump_anns(anns):\n",
    "  for i, a in enumerate(anns):\n",
    "    print(f\"#{i}\")\n",
    "    for k in a.keys():\n",
    "      if k == 'category_id' and num2cat.get(a[k],None):\n",
    "        print(k,\": \",a[k], num2cat[a[k]]) # Show cat. name\n",
    "      else:\n",
    "        print(k,\": \",a[k])\n",
    "\n",
    "dump_anns(anns)\n",
    "coco.showAnns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-ywQiAcMZv-"
   },
   "source": [
    "Что такое [RLE](https://en.wikipedia.org/wiki/Run-length_encoding) ?\n",
    "\n",
    "run-length encoding\n",
    "\n",
    "https://www.youtube.com/watch?v=h6s61a_pqfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8b8DiNvMfaC"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (160,80)\n",
    "print(catIds)\n",
    "annIds = coco.getAnnIds(catIds=catIds, iscrowd = True)\n",
    "anns = coco.loadAnns(annIds[0:1])\n",
    "\n",
    "dump_anns(anns)\n",
    "\n",
    "img = coco.loadImgs(anns[0]['image_id'])[0]\n",
    "I = io.imread(img['coco_url'])\n",
    "plt.imshow(I); \n",
    "coco.showAnns(anns) # People in the stands \n",
    "seg = anns[0]['segmentation']\n",
    "\n",
    "print('Counts',len(seg['counts']))\n",
    "print('Size',seg['size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7lnz4idJ2Zv"
   },
   "source": [
    "Как получить маску в виде массива?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3IO9QOfckNh"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "import numpy as np\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "\n",
    "anns = coco.loadAnns(annIds)\n",
    "msk = np.zeros(seg['size'])\n",
    "for i in range(len(anns)):\n",
    "  msk += coco.annToMask(anns[i])*20\n",
    "print(msk.shape)\n",
    "plt.figure()\n",
    "plt.imshow(msk)\n",
    "\n",
    "print(msk)\n",
    "print(np.unique(msk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCE11hoZtr-x"
   },
   "source": [
    "## Semantic segmentation\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-007.png\" width=\"900\">\n",
    "\n",
    "Постановка задачи:\n",
    "\n",
    "Предсказать класс для каждого пикселя.\n",
    "\n",
    "Входные данные маска: \n",
    "\n",
    "[ x,y - > class_num ] \n",
    "\n",
    "Выходные данные маска:\n",
    "\n",
    "[ x,y - > class_num ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvycryQcvA-x"
   },
   "source": [
    "###Способы решения\n",
    "\n",
    "**а) Наивный.**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-012.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Скользящим окном пройтись по изображению и предсказать клас для каждого пикселя с учетом его соседей.\n",
    "\n",
    "\n",
    "**б) Разумный**\n",
    "\n",
    "\n",
    "Убрать линейный слой в конце сети. По признакам во всех каналах определять класс каждого пикселя.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-015.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_15.png\"  width=\"700\">\n",
    "\n",
    "В лекции №8 мы говорили о том что сверту 1x1 можно рассматривать как полносвязанный слой.\n",
    "\n",
    "\n",
    "Именно так она и будет использоваться при сегментации.\n",
    "\n",
    "Количество классов будет соответствовать числу каналов.\n",
    "\n",
    "\n",
    "Проблемы:\n",
    "- нужно большое рецептивное поле, следовательно много слоев ( L 3х3 conv -> 1+2L receptive field)\n",
    "- очень медленно на полноразмерных картах активации\n",
    "\n",
    "\n",
    "**в) Эффективный**\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-017.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Вернмся к традиционной структуре со сжатием пространственных размеров. А после нее добавим разжимающий блок. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SycYXvdyAy8"
   },
   "source": [
    "## Автокодировщик\n",
    "\n",
    "Такая архитектура довольно популярна и применяется не только для сегментации: \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-07.png\" width=\"700\">\n",
    "\n",
    "\n",
    "- сглаживание шума;\n",
    "- снижение размерности -> вектор признак\n",
    "- генерация данных\n",
    "... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2OQ9c7Pyjs7"
   },
   "source": [
    "## Разжимающий(upsample) блок?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP487Kw5zJm0"
   },
   "source": [
    "### Изменение размеров изображений \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-08.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-10.png\" width=\"700\">\n",
    "\n",
    "Билинейная интерполяция рассматривает квадрат 2x2 известных пикселя, окружающих неизвестный. В качестве интерполированного значения используется взвешенное усреднение этих четырёх пикселей.\n",
    "\n",
    "\n",
    "Бикубическая интерполяция идёт на один шаг дальше билинейной, рассматривая массив из 4x4 окружающих пикселей — всего 16. Поскольку они находятся на разных расстояниях от неизвестногопикселя, ближайшие пиксели получают при расчёте больший вес.\n",
    "\n",
    "\n",
    "\n",
    "С картами признаков можно обращаться так же как и с пикселями. Для этого в Pytorchществует метод Upsample\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfyGBfOfzV-W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def upsample( pil, mode='nearest' ):\n",
    "  tensor = TF.to_tensor(pil)\n",
    "  upsampler = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "  tensor_128 = upsampler(tensor.unsqueeze(0))\n",
    "  im_128 = TF.to_pil_image(tensor_128.squeeze()).convert(\"RGB\")\n",
    "  fig = plt.figure()\n",
    "  fig.suptitle(mode)\n",
    "  plt.imshow(im_128)\n",
    "\n",
    "\n",
    "man_with_cat = coco2pil('http://images.cocodataset.org/val2017/000000223747.jpg')\n",
    "pil_64 = man_with_cat.resize((64,64))\n",
    "plt.figure()\n",
    "plt.imshow(pil_64)\n",
    "\n",
    "\n",
    "upsample( pil_64, mode='nearest' )\n",
    "upsample( pil_64, mode='bilinear' )\n",
    "upsample( pil_64, mode='bicubic' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8fGcmyWhLZs"
   },
   "source": [
    "### Bed of Nails\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-12.png\" width=\"700\">\n",
    "\n",
    "Способ восстановления размерности когда элементы из начальной карты признаков копируются без изменения, а новые ячейки  вокруг них заполняются нулями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeoFzo8khoWi"
   },
   "source": [
    "### MaxUnpooling\n",
    "\n",
    "Разница в том что индексы элементов запоминаются.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-019.png\" width=\"700\">\n",
    "\n",
    "Сохраняем индексы каждого max pooling слоя\n",
    "При повышении разрешения копируем значения из выхода max pooling слоя с учетом запомненных индексов\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-14.png\" width=\"700\">\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6yI0TnxilbL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tensor_show( tensor,title = ''):\n",
    "  im = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
    "  fig = plt.figure()\n",
    "  fig.suptitle(title + str(im.size))\n",
    "  plt.imshow(im)\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size = 2, return_indices = True) # False by default\n",
    "unpool = nn.MaxUnpool2d(kernel_size = 2)\n",
    "\n",
    "pil = coco2pil('http://images.cocodataset.org/val2017/000000223747.jpg')\n",
    "fig = plt.figure()\n",
    "fig.suptitle('original ' +str(pil.size))\n",
    "plt.imshow(pil)\n",
    "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
    "print(\"Initial shape\",tensor.shape)\n",
    "\n",
    "# Downsample\n",
    "tensor_half_res, indexes1  = pool(tensor)\n",
    "print(\"Indexes shape\",indexes1.shape)\n",
    "tensor_show( tensor_half_res,\"1/2 down\")\n",
    "\n",
    "\n",
    "tensor_q_res, indexes2  = pool(tensor_half_res)\n",
    "tensor_show( tensor_q_res, \"1/4 down\")\n",
    "\n",
    "# Upsample\n",
    "tensor_half_res1 = unpool(tensor_q_res,indexes2)\n",
    "\n",
    "#https://pytorch.org/docs/stable/nn.html#padding-layers\n",
    "#pad = nn.ZeroPad2d((0,0,0,1))\n",
    "#tensor_half_res1 = pad(tensor_half_res1)\n",
    "\n",
    "tensor_show( tensor_half_res1 ,\"1/2 up\")\n",
    "print(indexes1.shape)\n",
    "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
    "tensor_show( tensor_recovered, \"full size up\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hw4WCDjwq72S"
   },
   "source": [
    "Зачем нужен pad?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef_rFbEOrDn1"
   },
   "source": [
    "### Transpose convolution\n",
    "\n",
    "Способы восстановления пространственных размерностей которые мы рассмотрели, не содержали обучаемых параметров.\n",
    "\n",
    "\n",
    "Обычная свертка:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-16.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Upsample/transpose convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-17.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-15.png\" width=\"700\">\n",
    "\n",
    "https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544\n",
    "\n",
    "\n",
    "Pytorch\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-19.png\" width=\"700\">\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYvxvYbDsRAI"
   },
   "outputs": [],
   "source": [
    "# With square kernels and equal stride\n",
    "m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "output = m(input)\n",
    "# exact output size can be also specified as an argument\n",
    "input = torch.randn(1, 16, 12, 12)\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "h = downsample(input)\n",
    "print(h.size())\n",
    "output = upsample(h, output_size=input.size())\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LLWHPwctQ3j"
   },
   "source": [
    "## Unet\n",
    "\n",
    "Популярная архитектура для сегментации. Изначально создавалась для анализха  медицинских изображений?\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-20.png\" width=\"700\">\n",
    "\n",
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "https://github.com/milesial/Pytorch-UNet\n",
    "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5\n",
    "\n",
    "\n",
    "Обратите внимание на серые стрелки на схеме ....\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-21.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "При конкатенации пространственные размеры должны совпадать.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-22.png\" width=\"700\">\n",
    "\n",
    "После upsample блоков ReLU не используется.\n",
    "\n",
    "https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4a1wTLIWtH9"
   },
   "source": [
    "## Несколько классов\n",
    "\n",
    "Если требуется классифицировать нестколько класов объектов, то в выходной слой нужно добавить соответствующее количество слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qk_wndhsgK3"
   },
   "source": [
    "# Предобученные модели для сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3tHdZL4QAkw"
   },
   "source": [
    "FCN \n",
    "\n",
    "Fully Convolutional Network\n",
    "для токо что бы не было путаницы с Fully Connected Network\n",
    "последние именуют MLP (Multi Layer Perceptron)\n",
    "\n",
    "Usage example\n",
    "https://pytorch.org/hub/pytorch_vision_fcn_resnet101/\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html#semantic-segmentation\n",
    "\n",
    "The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in references/segmentation/coco_utils.py. The classes that the pre-trained model outputs are the following, in order:\n",
    "\n",
    "\n",
    "['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    " 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    " 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AfjUfUeQAQG"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "classes = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    " 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    " 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "fcn_model = torchvision.models.segmentation.fcn_resnet50(pretrained=True,  num_classes=21)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), #ImageNet\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(pil_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = fcn_model(input_tensor.unsqueeze(0))#['out'][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCmW6VBdo0id"
   },
   "source": [
    "Возвращаются 2 массива\n",
    "\n",
    "* out - at each location, there are unnormalized probabilities corresponding to the prediction of each class\n",
    "\n",
    "* aux - contains the auxillary loss values per-pixel. In inference mode, output['aux'] is not usefulcontains the auxillary loss values per-pixel. In inference mode, output['aux'] is not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nsy2uhG90E1U"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([\n",
    "                  [[1,4],[7,0.99]],\n",
    "                  [[0.8,1],[9,12]]\n",
    "                ]\n",
    "                  )\n",
    "a\n",
    "torch.argmax(a, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4GU1E55pbmC"
   },
   "outputs": [],
   "source": [
    "print(output.keys()) # Ordered dictionary\n",
    "print(\"out\", output['out'].shape,\"Batch, class_num, h, w\") \n",
    "print(\"aux\", output['aux'].shape,\"Batch, class_num, h, w\") \n",
    "# aux and output['aux'] contains the auxillary loss values per-pixel. In inference mode, output['aux'] is not usefulcontains the auxillary loss values per-pixel. In inference mode, output['aux'] is not useful\n",
    "\n",
    "#at each location, there are unnormalized probabilities corresponding to the prediction of each class\n",
    "output_predictions = output['out'][0].argmax(0)  # for first element of batch\n",
    "print(output_predictions.shape)\n",
    "print(output_predictions,torch.max(output_predictions))\n",
    "indexes = output_predictions\n",
    "for i, cls_name in enumerate(classes):\n",
    "  #print(cls_name)\n",
    "  #print(indexes,indexes.shape)\n",
    "  mask = torch.zeros(indexes.shape)\n",
    "  mask[indexes == i] = 255 \n",
    "  #print(i,class_prob.shape)\n",
    "  #mask = class_prob.byte()\n",
    "  #print(mask,mask.shape,torch.max(mask))\n",
    "  fig = plt.figure()\n",
    "  fig.suptitle(cls_name)\n",
    "  plt.imshow(mask)\n",
    "  #color = i *10\n",
    "  #break;\n",
    "  #at each location, there are unnormalized probabilities corresponding to the prediction of each class\n",
    "  output_predictions += class_prob.argmax(0) * color\n",
    "\n",
    "#print(output_predictions)\n",
    "#plt.imshow(output_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8-L4xmRkMTc"
   },
   "outputs": [],
   "source": [
    "# create a color pallette, selecting a color for each class\n",
    "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
    "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
    "colors = (colors % 255).numpy().astype(\"uint8\")\n",
    "\n",
    "# plot the semantic segmentation predictions of 21 classes in each color\n",
    "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(pil_img.size)\n",
    "r.putpalette(colors)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(r)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrAVWsYIPl8F"
   },
   "source": [
    "DeepLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAyPbe1JPo2b"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "li3nMf48CaN9"
   },
   "source": [
    "# DeepLab\n",
    "\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html#deeplabv3\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-24.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdliigNsAsCh"
   },
   "source": [
    "## Spatial pyramid pooling layer\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-23.png\" width=\"700\">\n",
    "\n",
    "В pytorch нет отдельного модуля, но результат может быть получени применением нескольких AdaptiveMaxPool2d с разными размерами выходов ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfm3iU8fCV6f"
   },
   "outputs": [],
   "source": [
    "# Demo code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcI7pK8DCp4g"
   },
   "source": [
    "## Atros (Dilated) Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-25.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-26.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsFKqsc6C51a"
   },
   "outputs": [],
   "source": [
    "# Atros example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzGd11SJC9gp"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-27.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oy5ij3oRDHkA"
   },
   "outputs": [],
   "source": [
    "#DeepLab example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJnIr3DMDjbx"
   },
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUrRzOB3EBGc"
   },
   "source": [
    "Входные данные\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-28.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhZKSnT1tRmE"
   },
   "source": [
    "### IoU - ценка точности\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-52.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H79ZHiXvEGTS"
   },
   "outputs": [],
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygCXxVcFDxeL"
   },
   "source": [
    "### Pixel-wise cross entropy loss\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-29.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6J0c3-XEWBR"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bA9B1HuLEXcF"
   },
   "source": [
    "### DiceLoss\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-30.png\" width=\"700\">\n",
    "\n",
    "https://www.jeremyjordan.me/semantic-segmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeOJ-tS2nC7V"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FEI_e1WXUEv"
   },
   "source": [
    "# Детекстирование (Object detection)\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
    "\n",
    "Постановка задачи:\n",
    "\n",
    "- опредлить координаты прямоугольника в котором заключен объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7vrg6CNX7b1"
   },
   "source": [
    "Пример из задания для семинара:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-05.png\" width=\"700\">\n",
    "\n",
    "Задачу семантической сегментации мы решали через классификацию. Для детектирования разумно использовать регрессию.\n",
    "\n",
    "В случает для одного объекта можно обучить модель предсказывать числа:\n",
    "\n",
    "* координаты центра + ширину и высоту\n",
    "* координаты правого верхнего и левого нижнего углов\n",
    "* координаты вершин полигона ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuS-LZoSZFAW"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "resnet_detector = resnet18(pretrained = True)\n",
    "resnet_detector.fc = nn.Linear(resnet_detector.fc.in_features,4) # x1,y1 x2,y2\n",
    "\n",
    "criterion  = nn.MSELoss()\n",
    "\n",
    "input = torch.rand((1,3,224,224))\n",
    "target = torch.tensor([[.1,.1,.5,.5]]) # x1,y1 x2,y2 or x,y w,h\n",
    "\n",
    "output = resnet_detector(input)\n",
    "loss = criterion(output,target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgXGmjciZGlh"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-07.png\" width=\"700\">\n",
    "\n",
    "\n",
    "На прошлом семинаре мы упоминали про модели которые ищут ключевые точки на лице человека (MTCNN). Можно использовать тот же подход для поиска любых точек.\n",
    "\n",
    "\n",
    "Начнем с ситуации когда объект один.\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-08.png\" width=\"500\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DFJ6VU92fn-"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnXxFVYK2Ye2"
   },
   "source": [
    "У нас есть предобученный классификатор.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-09.png\" width=\"700\">\n",
    "\n",
    "и мы хотим научить его определять местоположение объекта.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-10.png\" width=\"700\">\n",
    "\n",
    "Для этого можно использовать две лосс функции: одна будет оценивать ошибку классификации другая локализации.\n",
    "\n",
    "Результаты потребуется каким-то образом объединить. В простейшем случае просто сложить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-0du0EBbUsL"
   },
   "source": [
    " ## Regression loss\n",
    "\n",
    "\n",
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-11.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-12.png\" width=\"700\">\n",
    "\n",
    "MAE vs MSE\n",
    "\n",
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-13.png\" width=\"700\">\n",
    "\n",
    "Huber vs Log-cos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b67-sqDCcaXU"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAB-G_Avbu0x"
   },
   "source": [
    "## Multitask loss\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-14.png\" width=\"500\">\n",
    "\n",
    "\n",
    "[Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics](https://arxiv.org/pdf/1705.07115.pdf)\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/1705.07115\n",
    "\n",
    "https://github.com/Hui-Li/multi-task-learning-example-PyTorch/blob/master/multi-task-learning-example-PyTorch.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e4JPCJGclWj"
   },
   "source": [
    "## Детектирование нескольких объектов\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-042.gif\" width=\"700\">\n",
    "\n",
    "\n",
    "**a) Наивный способ решения: скользящее окно**\n",
    "\n",
    "Перебрать все возможные местопоположения объектов и классифицировать фрагменты.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-044.gif\" width=\"700\">\n",
    "\n",
    "- очень долго\n",
    "\n",
    "\n",
    "\n",
    "**b) Эвристика**\n",
    "\n",
    " Выбрать области в которых вероятность нахождения объекта наиболее высока (ROI = regions of interest) и проводить поиск только в них.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-049.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gx8lZ8QKfO70"
   },
   "source": [
    "#### Selective search\n",
    "\n",
    "Один из таких алгоритмов:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-18.png\" width=\"1000\">\n",
    "\n",
    "http://www.huppelen.nl/publications/selectiveSearchDraft.pdf\n",
    "\n",
    "Возвращает порядка 2000 прямоугольников для изображения.\n",
    "\n",
    "С таким количеством уже можно работать ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twU_RsKVgS3u"
   },
   "source": [
    "### RCNN\n",
    "Region CNN \n",
    "Построенна по такому принципу:\n",
    "\n",
    "- на изображении ищутся ROI \n",
    "- для кажого делается resize \n",
    "- каждый ROI обрабатывается сверточной сетью\n",
    "которая предсказывает класс кобъекта который в него попал\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-055.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Кроме класса модель предсказывала смещения для каждого bounding box\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYodES3CfToV"
   },
   "source": [
    "### NMS\n",
    "\n",
    "Теперь возникает другая проблема в районе объекта алгоритм генерирует множество ограничивающих прямоугольников (bounding box) которые частично прекрывают друг друга.\n",
    "\n",
    "\n",
    "Что бы избавиться от них используется другой алгоритм\n",
    "Non maxima suppression\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-22.png\" width=\"700\">\n",
    "\n",
    "Его задача избавиться об bbox которые накладиваются на истинный\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-23.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Для оценки схожести обычно используется метрика IoU\n",
    "а заначение IoU при котором bbox считаются принадлежащими обному объекту является гиперпараметром (часто 0.5)\n",
    "\n",
    "\n",
    "Soft NMS\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-24.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-25.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPN_EBOincmU"
   },
   "source": [
    "RCNN\n",
    "\n",
    "Проблеммой описанного выше подхода является скорость.\n",
    "Так как мы вынужденны применять CNN порядка 2000 раз (в зависимости от эвристики которая генерирует ROI)\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-26.png\" width=\"700\">\n",
    "\n",
    "И решением является поиск ROI не на самом изображении а на карте признаком, полочкнной после обработке всего изображения CNN. В таком случае большая часть сверток выполняется только один раз.\n",
    "\n",
    "\n",
    "Fast <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-27.png\" width=\"700\">\n",
    "\n",
    "Это радикально ускоряет процесс\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTAW0LgHpZig"
   },
   "source": [
    "### ROI Pooling\n",
    "\n",
    "Появляется новая задача - 'resize' ROI на карте признаков.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-28.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-29.png\" width=\"700\">\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_pool\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CbG8QoHp9iW"
   },
   "source": [
    "### ROI Align\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-30.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-31.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plbL1qU9qYiQ"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLlRn54urCkj"
   },
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-077.png\" width=\"700\">\n",
    "\n",
    "Скорость работы CNN снизилась и теперь узким местом становится эвристика для поиска ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9Cj6XTmrZXw"
   },
   "source": [
    "### Faster-RCNN\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-078.png\" width=\"700\">\n",
    "\n",
    "Идею: пусть сеть сама предсказывает ROI по карте признаков\n",
    "\n",
    "\n",
    "Для обучения требуется посчитать 4 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hix3UFkksgV0"
   },
   "source": [
    "### Region proposal network\n",
    "\n",
    "Карта признаков имеет фиксированные и отноститьно небольшие пространственные размеры (например 20x15)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-34.png\" width=\"700\">\n",
    "\n",
    "Поэтому можно вернуться к идее скользящего окна которая была отвергнута в самом начале.\n",
    "\n",
    "При этом можно использовать окна нескольких форм\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-35.png\" width=\"700\">\n",
    "\n",
    "Предсказываются два значения:\n",
    "\n",
    "* вероятность того что в ROI находится объект\n",
    "* смещения\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-086.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0H0fISPUuW8X"
   },
   "source": [
    "### Two stage detector\n",
    "\n",
    "Faster RCNN == Two stage detector\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/\n",
    "L12_Segmentation_Detection/img/lecture_12-089.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfnhB_wtu8Ul"
   },
   "source": [
    "## One Stage detector\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-1-1.png\" width=\"700\">\n",
    "\n",
    "YOLO, SSD, RetinaNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-1-2.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html#runtime-characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3Ls4lb3wlri"
   },
   "source": [
    "### SSD\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/ssd.png\" width=\"700\">\n",
    "\n",
    "* VGG-16 model, pre-trained on ImageNet\n",
    "* manually defines a collection of aspect ratios to use for the B bounding boxes at each grid cell + offsets (x,y,w,h)\n",
    "* directly predict the probability that a class is present in a given bounding box.\n",
    "* class for \"background\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmJ3HRqZH3hp"
   },
   "source": [
    "### Retina net\n",
    "\n",
    "[Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcsJ2lKGIy48"
   },
   "source": [
    "#### FocalLoss\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-43.png\" width=\"1000\">\n",
    "\n",
    "https://amaarora.github.io/2020/06/29/FocalLoss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0uDHPLnIvrM"
   },
   "source": [
    "##Feature pyramyd network\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)\n",
    "\n",
    "Это feature extractor для детекторов.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-21.png\" width=\"700\">\n",
    "\n",
    "Так же как и в случае с сегментацией, точность повышается если делать предсказания на картах признаков разных масштабов.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn1.jpeg\" width=\"400\">\n",
    "\n",
    "Но первые слои содержат мало семантической информации (только низкоуровневые признаки). Из за этого детектирование(и сегментация) мелких объектов удается хуже.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn2.png\" width=\"400\">\n",
    "\n",
    "идея состоит в том что бы делать предсказание с учетом семантической информации полученной на более глубоких слоях. \n",
    "\n",
    "\n",
    "При этом признаки суммируются.\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn3.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Затем к новым картам признаков может применяться дополнительная свертка.\n",
    "\n",
    "\n",
    "На выходе получаем карты признаков P2 - P5 на которых уже предсказываются bounding box.\n",
    "\n",
    "\n",
    "В случае 2-stage детектора (RCNN) карты подаются на вход RPN \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn5.jpeg\" width=\"700\">\n",
    "\n",
    "А признаки для предсказаний используются из backbone \n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn6.jpeg\" width=\"700\">\n",
    "\n",
    "\n",
    "RetinaNet использует выходы FPN и для предсказаний класса и bbox. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-42.png\" width=\"700\">\n",
    "\n",
    "\n",
    "https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TmT6krhIKKr"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JjWUy9uPQW5"
   },
   "source": [
    "\n",
    "##YOLO\n",
    "\n",
    "*  2016 You Only Look Once: Unified, Real-Time Object Detection. \n",
    "* 2017 YOLO9000: Better, Faster, Stronger.\n",
    "* 2018 YOLOv3: An Incremental Improvement\n",
    "\n",
    "Joseph Redmon\n",
    "\n",
    "* April 2020, Alexey Bochkovskiy  “YOLOv4: Optimal Speed and Accuracy of Object Detection\n",
    "* 9 June 2020 YOLOv5 Glenn Jocher https://github.com/ultralytics/yolov5\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-47.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z06JkZ3hrJMy"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM0AXCEIrVhT"
   },
   "outputs": [],
   "source": [
    "! wget https://github.com/Gan4x4/CV-HSE2019/raw/master/data/2bikes.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wL8xifds2mS"
   },
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=['person','bicycle']); # person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds );\n",
    "img_list = coco.loadImgs(imgIds[5])\n",
    "img = img_list[0]\n",
    "print(\"Image data\", img)\n",
    "\n",
    "pil_img = coco2pil(img['coco_url'])\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img['id'])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns,draw_bbox = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNu0yhQ-u2ru"
   },
   "source": [
    "Загрузка модели с Torch Hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lc7A0Fzru15y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load model from torch\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XSmha1-sIRQ"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Apply yolov5 model\n",
    "results = model(pil_img)\n",
    "results.print()\n",
    "results.save() # image on disk\n",
    "\n",
    "print(type(results.xyxy),len(results.xyxy) , results.xyxy[0].shape)\n",
    "print(results.xyxy[0])\n",
    "results.pandas().xyxy[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xv6URcHxr2m"
   },
   "outputs": [],
   "source": [
    "dummy_input = torch.rand((1,3,416,416))\n",
    "results = model(dummy_input)\n",
    "print(type(results),len(results))\n",
    "print(results[0].shape)\n",
    "print(type(results[1]),len(results[1]))\n",
    "for e in results[1]:\n",
    "  print(e.shape)\n",
    "#print(results[0], results[1].shape) # list of two elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvVpXsSbQht1"
   },
   "source": [
    "## Нard Example Mining\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-48.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Online hard example mining\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-49.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7p8DRxBD906X"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uI3hfj3Y97eC"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-50.png\" width=\"700\">\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/pdf/1604.03540.pdf)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors](https://arxiv.org/pdf/1804.04606.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToP9uPsctEif"
   },
   "source": [
    "# Оценка точности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZk05YL8Iy2p"
   },
   "source": [
    "## COCO mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgp914Iidawh"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-51.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18t3fOlVdawh"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-52.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEUxR2I8dawi"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-54.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4U5x9vTdawi"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-55.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYEoxbZWdawj"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-56.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d56lMiINdawj"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-57.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Go7SjEsSdawk"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-58.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAHexXHGJ8NW"
   },
   "source": [
    "Конвертация результатов сегментации в COCO формат.\n",
    "\n",
    "https://www.javaer101.com/en/article/18652684.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4h18JTYuW_rC"
   },
   "source": [
    "Синхронизируем метки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AunvkGROXALy"
   },
   "outputs": [],
   "source": [
    "pascal2coco = {}\n",
    "print(classes)\n",
    "def find_in_dic(dic,val):\n",
    "  for key in dic.keys():\n",
    "    if dic.get(key,None) == val:\n",
    "      return key\n",
    "  return 0 # Assign missed classes to bg\n",
    "\n",
    "print(num2cat)\n",
    "for i in range(1, len(classes)): # Skip BG\n",
    "  #if cats.get()\n",
    "  coco_ind = find_in_dic(num2cat,classes[i])\n",
    "  pascal2coco[i] = coco_ind\n",
    "\n",
    "print(pascal2coco) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6qF1kK4d-I1"
   },
   "source": [
    "Create gt file\n",
    "\n",
    "\n",
    "annIds = coco.getAnnIds(catIds=catIds, iscrowd = True)\n",
    "anns = coco.loadAnns(annIds[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YslfqiBhW-fN"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMrptHeS-4CX"
   },
   "outputs": [],
   "source": [
    "from pycocotools import mask\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "\n",
    "def binary_mask_to_rle(binary_mask):\n",
    "    rle = {'counts': [], 'size': list(binary_mask.shape)}\n",
    "    counts = rle.get('counts')\n",
    "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n",
    "        if i == 0 and value == 1:\n",
    "            counts.append(0)\n",
    "        counts.append(len(list(elements)))\n",
    "    return rle\n",
    "\n",
    "detection_res = []\n",
    "for i, cls_name in enumerate(classes):\n",
    "  binary_mask = torch.zeros(indexes.shape)\n",
    "  binary_mask[indexes == i] = 1 \n",
    "\n",
    "  if i > 0 and torch.max(binary_mask) > 0  :\n",
    "  #print(binary_mask.shape,torch.max(binary_mask))\n",
    "    uncompressed_rle = binary_mask_to_rle(binary_mask.numpy()) #encoded_gt,\n",
    "    fortran_gt_binary_mask = np.asfortranarray(binary_mask).astype('uint8')\n",
    "    #print(fortran_ground_truth_binary_mask)\n",
    "    encoded_gt = mask.encode(fortran_ground_truth_binary_mask)\n",
    "    #decoded = mask.decode(encoded_ground_truth)\n",
    "    #print(decoded)\n",
    "    bbox = list(mask.toBbox(encoded_ground_truth))\n",
    "    print(bbox)\n",
    "  #ground_truth_area = mask.area(encoded_ground_truth)\n",
    "  #ground_truth_bounding_box = mask.toBbox(encoded_ground_truth)\n",
    "\n",
    "\n",
    "    detection_res.append({\n",
    "        'score': 1., #dummy\n",
    "        'category_id': pascal2coco[i],\n",
    "        #'segmentation' : uncompressed_rle,\n",
    "        'bbox': bbox, #[80.0, 6.49, 223.89, 223.36], #\n",
    "        'image_id': 448263,\n",
    "        #'iscrowd' : 1\n",
    "    })\n",
    "\n",
    "print(detection_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXtCIC0Ad9yY"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "'''\n",
    "for anno in anns:\n",
    "        detection_res.append({\n",
    "            'score': 1.,\n",
    "            'category_id': anno['category_id'],\n",
    "            'bbox': anno['bbox'],\n",
    "            'image_id': anno['image_id']\n",
    "        })\n",
    "'''    \n",
    " \n",
    "with open('seg_gt.json', 'w', encoding='utf-8') as f:\n",
    "  json.dump(anns, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfA1-0PcbqbB"
   },
   "outputs": [],
   "source": [
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    import json\n",
    "    from tempfile import NamedTemporaryFile\n",
    "     \n",
    "    # json file in coco format, original annotation data\n",
    "    #anno_file = 'annotations/instances_val2017.json'\n",
    "    #coco_gt = COCO('seg_gt.json')\n",
    "     \n",
    "     # Use GT box as prediction box for calculation, the purpose is to get detection_res\n",
    "    #with open(anno_file, 'r') as f:\n",
    "    #    json_file = json.load(f)\n",
    "    #annotations = json_file['annotations']\n",
    "\n",
    "\n",
    "    #detection_res = []\n",
    "  \n",
    "   \n",
    "\n",
    "    import json\n",
    "    with open('seg_res.json', 'w', encoding='utf-8') as f:\n",
    "      json.dump(detection_res, f, ensure_ascii=False, indent=4)\n",
    "    #tf = file.open('seg_res.json'):\n",
    "    # Due to subsequent needs, first convert detection_res to binary and then write it to the json file\n",
    "    #content = json.dumps(detection_res).encode(encoding='utf-8')\n",
    "    #tf.write(content)\n",
    "    res_path = tf.name\n",
    "     \n",
    "    # loadRes will generate a new COCO type instance based on coco_gt and return\n",
    "    coco_dt = coco_gt.loadRes('seg_res.json')\n",
    "    coco_gt = coco.loadRes('seg_gt.json')\n",
    "  \n",
    "    cocoEval = COCOeval(coco_gt, coco_dt,'bbox') # 'segm', 'bbox'\n",
    "    #cocoEval.params.useSegm = True\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "     \n",
    "    print(cocoEval.stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBJa5D3xXHAU"
   },
   "source": [
    "# Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOYZNV4xTTX1"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-03.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QzAoAfLS2z3"
   },
   "source": [
    "COCO panoptic\n",
    "https://cocodataset.org/#panoptic-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxyAdNWoVOn9"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-096.png\" width=\"700\">\n",
    "\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html#mask-r-cnn\n",
    "\n",
    "\n",
    "Приме запуска Mask R-CNN есть в документации Pytorch: \n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "L12_Segmentation_Detection_gan.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
