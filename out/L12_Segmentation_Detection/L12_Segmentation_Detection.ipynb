{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи компьютерного зрения\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "Кроме классификации CV решает и другие задачи.\n",
    "\n",
    "Сегментация, детектирование и многое другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формат разметки COCO\n",
    "\n",
    "Прежде чем говорить о способах решения задач компьютерного зрения разберемся с форматами входных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COCO - Common Objects in COntext\n",
    "\n",
    "Один из наиболее популярных датасатов содержащий данные для сегментации и детектирования.\n",
    "\n",
    "Содержит: \n",
    "- Категории\n",
    "- Маски\n",
    "- Ограничивающие боксы (*bounding boxes*)\n",
    "- Описания (*captions*)\n",
    "- Ключевые точки (*keypoints*)\n",
    "- И многое другое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с датасетом используется пакет `pycocotools`\n",
    "\n",
    "[Подробнее о том как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся с форматом на примере одной записи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"cat\"])  # Получаем ID котиков\n",
    "print('ID класса \"кот\" = %i' % catIds[0])\n",
    "\n",
    "imgIds = coco.getImgIds(catIds=catIds)  # Фильтруем датасет по тегу\n",
    "print(\"Всего изображений с котами: %i\" % len(imgIds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем что у нас в метаданных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = coco.loadImgs(imgIds[0])  # Берем только первую картинку\n",
    "img = img_list[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконвертируем в PIL формат для удобства дальнейшей работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(pil_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Категории в COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на примеры категорий в нашем датасете. Отобразим каждую 10ую категорию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = coco.loadCats(coco.getCatIds())  # Грузим категории\n",
    "num2cat = {}  # Создаем словарь для хранения\n",
    "print(\"Категории COCO: \")\n",
    "for cat in cats:\n",
    "    num2cat[cat[\"id\"]] = cat[\"name\"]\n",
    "    if cat[\"id\"] in range(0, 80, 10):\n",
    "        print(cat[\"id\"], \":\", cat[\"name\"], end=\"   \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете так же есть категория **0**. Ее используют для обозначения класса фона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть так же суперкатегории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cats[2])\n",
    "print(cats[3])\n",
    "\n",
    "nms = set([cat[\"supercategory\"] for cat in cats])\n",
    "print(\"COCO supercategories: \\n{}\".format(\"\\n\".join(nms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вернемся к метаданным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо метаданных нам доступна разметка ([подробнее о разметке](!https://cocodataset.org/#format-data)), давайте ее загрузим и отобразим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "plt.imshow(I)\n",
    "plt.axis(\"off\")\n",
    "coco.showAnns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберем из чего состоит разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_anns(anns):\n",
    "    for i, a in enumerate(anns):\n",
    "        print(f\"#{i}\")\n",
    "        for k in a.keys():\n",
    "            if k == \"category_id\" and num2cat.get(a[k], None):\n",
    "                print(k, \": \", a[k], num2cat[a[k]])  # Show cat. name\n",
    "            else:\n",
    "                print(k, \": \", a[k])\n",
    "\n",
    "\n",
    "dump_anns(anns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для объектов, которых слишком много существует отдельная метка `iscorwd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (120, 60)\n",
    "\n",
    "catIds = coco.getCatIds(catNms=[\"people\"])\n",
    "annIds = coco.getAnnIds(catIds=catIds, iscrowd=True)\n",
    "anns = coco.loadAnns(annIds[0:1])\n",
    "\n",
    "dump_anns(anns)\n",
    "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(I)\n",
    "coco.showAnns(anns)  # People in the stands\n",
    "seg = anns[0][\"segmentation\"]\n",
    "print(\"Counts\", len(seg[\"counts\"]))\n",
    "print(\"Size\", seg[\"size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как получить маску в виде массива?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "msk = np.zeros(seg[\"size\"])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        msk = coco.annToMask(anns[i])\n",
    "        ax[row, col].imshow(msk)\n",
    "        ax[row, col].set_title(num2cat[anns[i][\"category_id\"]])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще у нас есть bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "RGB_img = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "for i in range(len(anns)):\n",
    "    x, y, width, heigth = anns[i][\"bbox\"]\n",
    "    x, y, width, heigth = int(x), int(y), int(width), int(heigth)\n",
    "    if anns[i][\"category_id\"] == 1:\n",
    "        color = (255, 255, 255)\n",
    "    if anns[i][\"category_id\"] == 37:\n",
    "        color = (255, 0, 0)\n",
    "    if anns[i][\"category_id\"] == 40:\n",
    "        color = (0, 255, 0)\n",
    "    RGB_img = cv2.rectangle(RGB_img, (x, y), (x + width, y + heigth), color, 2)\n",
    "cv2_imshow(RGB_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И еще куча всего"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$ \n",
    "#### Еще более глубокое понимание разметки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое [run-length encoding - RLE](https://en.wikipedia.org/wiki/Run-length_encoding)?\n",
    "\n",
    "[Видео-разбор](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантическая сегментация (*Semantic segmentation*)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-007.png\" width=\"900\">\n",
    "\n",
    "Постановка задачи:\n",
    "\n",
    "Предсказать класс для каждого пикселя.\n",
    "\n",
    "Входные данные маска: \n",
    "\n",
    "[ x,y - > class_num ] \n",
    "\n",
    "Выходные данные маска:\n",
    "\n",
    "[ x,y - > class_num ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Способы решения\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **a) Наивный.**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-012.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Скользящим окном пройтись по изображению и предсказать клас для каждого пикселя с учетом его соседей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **б) Разумный**\n",
    "\n",
    "Убрать линейный слой в конце сети. По признакам во всех каналах определять класс каждого пикселя.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-015.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_15.png\"  width=\"700\">\n",
    "\n",
    "В лекции №8 мы говорили о том что сверту 1x1 можно рассматривать как полносвязанный слой.\n",
    "\n",
    "\n",
    "Именно так она и будет использоваться при сегментации.\n",
    "\n",
    "Количество классов будет соответствовать числу каналов.\n",
    "\n",
    "\n",
    "Проблемы:\n",
    "- нужно большое рецептивное поле, следовательно много слоев ( L 3х3 conv -> 1+2L receptive field)\n",
    "- очень медленно на полноразмерных картах активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **в) Эффективный**\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-017.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Вернмся к традиционной структуре со сжатием пространственных размеров. А после нее добавим разжимающий блок. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "\n",
    "Такая архитектура довольно популярна и применяется не только для сегментации: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-07.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- сглаживание шума;\n",
    "- снижение размерности -> вектор признак\n",
    "- генерация данных\n",
    "\n",
    "\n",
    "Об этом будет целая отдельная лекция чуть позже\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разжимающий (upsample) блок\n",
    "\n",
    "#### Изменение размеров изображений "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-08.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/bilinear.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Билинейная интерполяция рассматривает квадрат 2x2 известных пикселя, окружающих неизвестный. В качестве интерполированного значения используется взвешенное усреднение этих четырёх пикселей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/bicubic.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бикубическая интерполяция идёт на один шаг дальше билинейной, рассматривая массив из 4x4 окружающих пикселей — всего 16. Поскольку они находятся на разных расстояниях от неизвестного пикселя, ближайшие пиксели получают при расчёте больший вес."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsample в Pytorch\n",
    "С картами признаков можно обращаться так же как и с пикселями. Для этого в Pytorch используется метод Upsample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "def upsample(pil, ax, mode=\"nearest\"):\n",
    "    tensor = TF.to_tensor(pil)\n",
    "    upsampler = nn.Upsample(scale_factor=2, mode=mode)\n",
    "    tensor_128 = upsampler(tensor.unsqueeze(0))\n",
    "    im_128 = TF.to_pil_image(tensor_128.squeeze()).convert(\"RGB\")\n",
    "    ax.imshow(im_128)\n",
    "    ax.set_title(mode)\n",
    "    ax.set_xlim(0, 20 * 2)\n",
    "    ax.set_ylim(20 * 2, 0)\n",
    "\n",
    "\n",
    "pic = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "pil_64 = pic.resize((64, 64))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(15, 5))\n",
    "ax[0].imshow(pil_64)\n",
    "ax[0].set_title(\"Resized image\")\n",
    "ax[0].set_xlim(0, 20)\n",
    "ax[0].set_ylim(20, 0)\n",
    "\n",
    "\n",
    "upsample(pil_64, mode=\"nearest\", ax=ax[1])\n",
    "upsample(pil_64, mode=\"bilinear\", ax=ax[2])\n",
    "upsample(pil_64, mode=\"bicubic\", ax=ax[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание что размер изображения увеличился в 2 раза!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaxUnpooling\n",
    "\n",
    "Разница с предыдущим методом в том что индексы элементов запоминаются.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-019.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем индексы каждого max pooling слоя.\n",
    "\n",
    "При повышении разрешения копируем значения из выхода max pooling слоя с учетом запомненных индексов\n",
    "\n",
    "[Документация к MaxPool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "[Документация к MaxUnpool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-14.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_show(tensor, title=\"\", ax=ax):\n",
    "    im = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
    "    ax.set_title(title + str(im.size))\n",
    "    ax.imshow(im)\n",
    "\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=2, return_indices=True)  # False by default\n",
    "unpool = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "pil = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].set_title(\"original \" + str(pil.size))\n",
    "ax[0].imshow(pil)\n",
    "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
    "print(\"Initial shape\", tensor.shape)\n",
    "\n",
    "# Downsample\n",
    "tensor_half_res, indexes1 = pool(tensor)\n",
    "print(\"Indexes shape\", indexes1.shape)\n",
    "tensor_show(tensor_half_res, \"1/2 down\", ax=ax[1])\n",
    "\n",
    "\n",
    "tensor_q_res, indexes2 = pool(tensor_half_res)\n",
    "tensor_show(tensor_q_res, \"1/4 down\", ax=ax[2])\n",
    "\n",
    "# Upsample\n",
    "tensor_half_res1 = unpool(tensor_q_res, indexes2)\n",
    "tensor_show(tensor_half_res1, \"1/2 up\", ax=ax[3])\n",
    "\n",
    "print(indexes1.shape)\n",
    "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
    "tensor_show(tensor_recovered, \"full size up\", ax=ax[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем нужен pad?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "array = torch.ones((24, 24), dtype=int)\n",
    "sns.heatmap(array, annot=True, fmt=\"d\", ax=ax[0], cbar=False, vmin=0, vmax=1)\n",
    "print(\"Размеры массива:\", array.size())\n",
    "\n",
    "array_padded = F.pad(array, pad=[4, 4])\n",
    "sns.heatmap(array_padded, annot=True, fmt=\"d\", ax=ax[1], cbar=False)\n",
    "print(\"Размеры массива с padding:\", array.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose convolution\n",
    "\n",
    "Способы восстановления пространственных размерностей которые мы рассмотрели, не содержали обучаемых параметров.\n",
    "\n",
    "\n",
    "Обычная свертка:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-16.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsample/transpose convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-17.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-15.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост про 2d свертки с помощью пеермножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)\n",
    "\n",
    "[Документация к ConvTranspose2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-19.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "\n",
    "input = torch.randn(1, 16, 16, 16)\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "    h = downsample(input)\n",
    "    print(\"Downsampled size\", h.size())\n",
    "\n",
    "    output = upsample(h, output_size=input.size())\n",
    "    print(\"Upsampled size\", output.size())\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "sns.heatmap(input[0, 0, :, :], ax=ax[0], cbar=False, vmin=-2, vmax=2)\n",
    "ax[0].set_title(\"Input\")\n",
    "sns.heatmap(h[0, 0, :, :], ax=ax[1], cbar=False, vmin=-2, vmax=2)\n",
    "ax[1].set_title(\"Downsampled\")\n",
    "sns.heatmap(output[0, 0, :, :], ax=ax[2], cbar=False, vmin=-2, vmax=2)\n",
    "ax[2].set_title(\"Upsampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "\n",
    "Популярная архитектура для сегментации. Изначально была предложена ([оригинальная статья](https://arxiv.org/abs/1505.04597)) для анализа  медицинских изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1505.04597)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Реализация на PyTorch](https://github.com/milesial/Pytorch-UNet)\n",
    "\n",
    "[U-Net на PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)\n",
    "\n",
    "[Статья-разбор](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на серые стрелки на схеме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-21.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При конкатенации пространственные размеры должны совпадать.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-22.png\" width=\"700\">\n",
    "\n",
    "После upsample блоков ReLU не используется.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мультиклассовая сегментация\n",
    "\n",
    "Если требуется сегментировать несколько класов объектов сразу, то выходной слой нужно изменить соответствующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем несколько архитектур для мультиклассовой сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2014 FCN \n",
    "\n",
    "Fully Convolutional Network\n",
    "для того что бы не было путаницы с Fully Connected Network\n",
    "последние именуют MLP (Multi Layer Perceptron)\n",
    "\n",
    "[Пример реализации 1](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)\n",
    "\n",
    "[Пример реализации 2](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
    "\n",
    "Предобученная модель была обучена на части датасета COCO train2017 (на 20 категориях, представленых так же в датасете  Pascal VOC). Использовались следующие классы:\n",
    "\n",
    "`['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**КАРТИНКУ ЗАГРУЗИТЬ НА СЕРВЕР**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://miro.medium.com/max/1016/1*NXNGhfSyzQcKzoOSt-Z0Ng.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает довольно просто. Берем любой *back-bone* (например `ResNet50` или `VGG16`) и прикручиваем слой `upsample` до нужных нам размеров в конце. Посмотрим как работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "classes = [\"__background__\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\",\n",
    "           ]\n",
    "\n",
    "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
    "    pretrained=True, num_classes=21\n",
    ")\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # ImageNet\n",
    "    ]\n",
    ")\n",
    "\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "input_tensor = preprocess(pil_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = fcn_model(input_tensor.unsqueeze(0))  # ['out'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращаются 2 массива\n",
    "\n",
    "* out - каждый пиксель отражает ненормированную вероятность, соответствующую предсказанию каждого класса.\n",
    "\n",
    "* aux - содержит значения *auxillary loss* на пиксель. На инференсе output['aux'] бесполезный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.keys())  # Ordered dictionary\n",
    "print(\"out\", output[\"out\"].shape, \"Batch, class_num, h, w\")\n",
    "print(\"aux\", output[\"aux\"].shape, \"Batch, class_num, h, w\")\n",
    "\n",
    "output_predictions = output[\"out\"][0].argmax(0)  # for first element of batch\n",
    "print(output_predictions.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "indexes = output_predictions\n",
    "fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for row in range(4):\n",
    "    for col in range(5):\n",
    "        mask = torch.zeros(indexes.shape)\n",
    "        mask[indexes == i] = 255\n",
    "        # if mask.max() > 0:\n",
    "        ax[row, col].set_title(classes[i])\n",
    "        ax[row, col].imshow(mask)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018 DeepLabv3\n",
    "\n",
    "[Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1802.02611v3)\n",
    "\n",
    "[Реализация на PyTorch](https://pytorch.org/vision/stable/models.html#deeplabv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-24.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В DeepLab появились следубщие новшевства: слои `Spatial pyramid pooling` и `Atros (dilated) convilutions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial pyramid pooling (SPP) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-23.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/1406.4729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spatial Pyramid Pooling (SPP)** - это *pooling* слой, который устраняет ограничение фиксированного размера сети, т.е. CNN не требует входного изображения фиксированного размера. В частности, мы добавляем слой SPP поверх последнего конволюционного слоя. \n",
    "\n",
    "Слой SPP объединяет признаки и генерирует выходные данные фиксированной длины, которые затем поступают в MLP (или другие классификаторы). Другими словами, мы выполняем некоторую агрегацию информации на более глубоком этапе иерархии сети (между сверточными слоями и полностью связанными слоями), чтобы избежать необходимости обрезать или деформировать изображение в начале."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В pytorch нет отдельного модуля, но результат может быть получени применением нескольких AdaptiveMaxPool2d с разными размерами выходов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atros (Dilated) Convolution\n",
    "\n",
    "**Dilated convolution** (расширенные свертки) - это тип свертки, который \"раздувает\" ядро, вставляя отверстия между элементами ядра. Дополнительный параметр (скорость расширения) указывает, насколько сильно расширяется ядро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-25.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-26.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "# Atros example\n",
    "with torch.no_grad():\n",
    "    input = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "    print(\"Input shape:\", input.size())\n",
    "\n",
    "    conv = nn.Conv2d(1, 1, kernel_size=2, dilation=1, bias=False)\n",
    "\n",
    "    conv.weight = nn.Parameter(torch.tensor([[[[2, 2], [2, 2]]]], dtype=torch.float))\n",
    "\n",
    "    out = conv(input.unsqueeze(0))\n",
    "    print(\"Output:\", out)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    out[0, 0, :, :],\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Kernel\")\n",
    "ax[2].set_title(\"Output\")\n",
    "fig.suptitle(\"Dilation = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(\n",
    "    1, 1, kernel_size=2, dilation=2, bias=False\n",
    ")  # Fell free to change dilation\n",
    "\n",
    "conv.weight = nn.Parameter(torch.tensor([[[[2, 2], [2, 2]]]], dtype=torch.float))\n",
    "\n",
    "out = conv(input.unsqueeze(0))\n",
    "print(out)\n",
    "print(out.shape)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    out[0, 0, :, :].detach(),\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Kernel\")\n",
    "ax[2].set_title(\"Output\")\n",
    "fig.suptitle(\"Dilation = 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([[[0, 1, 0], [1, 1, 1], [0, 1, 0]]], dtype=torch.float)\n",
    "out = conv(input.unsqueeze(0))\n",
    "print(out)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    out[0, 0, :, :].detach(),\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Kernel\")\n",
    "ax[2].set_title(\"Output\")\n",
    "fig.suptitle(\"Dilation = 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-27.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### IoU - ценка точности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-52.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://datahacker.rs/deep-learning-intersection-over-union/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pixel-wise cross entropy loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-29.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "one_class_out = torch.randn(1, 1, 32, 32)\n",
    "one_class_target = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "loss = bce_loss(one_class_out, one_class_target)\n",
    "print(\"BCE\", loss)\n",
    "\n",
    "\n",
    "two_class_out = torch.randn(1, 2, 32, 32)\n",
    "two_class_target = torch.randint(1, (1, 32, 32))\n",
    "\n",
    "print(two_class_out.shape)\n",
    "print(two_class_target.shape)\n",
    "\n",
    "loss = cross_entropy(two_class_out, two_class_target)\n",
    "\n",
    "print(\"Cross entropy loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DiceLoss\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/dice.jpeg\" width=\"700\">\n",
    "\n",
    "[Блог-пост про семантическую сегментацию](https://www.jeremyjordan.me/semantic-segmentation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Soft Dice loss of binary class\n",
    "    Args:\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "       Returns:\n",
    "        Loss tensor\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=2, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = p  # pow degree\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        predict = predict.flatten(1)\n",
    "        target = target.flatten(1)\n",
    "\n",
    "        # https://pytorch.org/docs/stable/generated/torch.mul.html\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.epsilon\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.epsilon\n",
    "        loss = 1 - 2 * num / den\n",
    "\n",
    "        return loss.mean()  # over batch\n",
    "\n",
    "\n",
    "criterion = BinaryDiceLoss()\n",
    "output = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "\n",
    "target = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "soft_loss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n",
    "print(\"Loss\", soft_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детектирование (Object detection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- опредлить координаты прямоугольника в котором заключен объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример из задания для семинара:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-05.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачу семантической сегментации мы решали через классификацию. Для детектирования разумно использовать регрессию.\n",
    "\n",
    "В случае для одного объекта можно обучить модель предсказывать числа:\n",
    "\n",
    "* координаты центра + ширину и высоту\n",
    "* координаты правого верхнего и левого нижнего углов\n",
    "* координаты вершин полигона ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Возьмем обученную сеть\n",
    "resnet_detector = resnet18(pretrained=True)\n",
    "\n",
    "# Заменим голову на предсказание 4х точек (x1,y1 x2,y2)\n",
    "resnet_detector.fc = nn.Linear(resnet_detector.fc.in_features, 4)  # x1,y1 x2,y2\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Это случайный пример. Не ожидайте результатов\n",
    "input = torch.rand((1, 3, 224, 224))\n",
    "target = torch.tensor([[0.1, 0.1, 0.5, 0.5]])  # x1,y1 x2,y2 or x,y w,h\n",
    "output = resnet_detector(input)\n",
    "loss = criterion(output, target)\n",
    "print(output)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-07.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://arxiv.org/abs/2011.12619)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом семинаре мы упоминали про модели которые ищут ключевые точки на лице человека (MTCNN). Можно использовать тот же подход для поиска любых точек.\n",
    "\n",
    "\n",
    "Начнем с ситуации когда объект один."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-08.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть предобученный классификатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-09.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "и мы хотим научить его определять местоположение объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-10.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого можно использовать две лосс функции: одна - будет оценивать ошибку классификации, другая - локализации.\n",
    "\n",
    "Результаты потребуется каким-то образом объединить. В простейшем случае - сложить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Regression loss\n",
    "\n",
    "$\\mathrm{MSE} = \\frac{\\sum^n_{i=1}(y_i-y_i^p)^2}{n}$ - L2/ MSE/ Mean Square Error/ Среднеквадратичная ошибка \n",
    "\n",
    "\n",
    "$\\mathrm{MAE} = \\frac{\\sum^n_{i=1}|y_i-y_i^p|}{n}$ - L1/ MAE/ Mean Absolute Error/ Средняя ошибка\n",
    "\n",
    "\n",
    "$\\\n",
    "    L_{\\delta}(y, f(x))=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  \\frac{1}{2}(y-f(x))^2 \\qquad \\mathrm{for } |y-f(x)| \\leq \\delta\\\\\n",
    "                  \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 \\qquad \\mathrm{otherwise}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "  $ - Huber Loss/ Smooth Mean ABsolute Error/ Функция потерь Хьюбера\n",
    "\n",
    "\n",
    "$L(y,y^p)=\\sum_{i=1}^n log(cosh(y^p_i-y_i))$ - Log-Cosh Loss/ Логарифм гиперболического косинуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE vs MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber vs Log-cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multitask loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-14.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics](https://arxiv.org/pdf/1705.07115.pdf)\n",
    "\n",
    "[Пример реализации MultiTask learning](https://github.com/Hui-Li/multi-task-learning-example-PyTorch/blob/master/multi-task-learning-example-PyTorch.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектирование нескольких объектов\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-042.gif\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a) Наивный способ решения: скользящее окно**\n",
    "\n",
    "Перебрать все возможные местопоположения объектов и классифицировать фрагменты.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-044.gif\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- очень долго"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b) Эвристика**\n",
    "\n",
    " Выбрать области в которых вероятность нахождения объекта наиболее высока (ROI = regions of interest) и проводить поиск только в них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-049.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective search\n",
    "\n",
    "Один из таких алгоритмов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-18.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "Возвращает порядка 2000 прямоугольников для изображения.\n",
    "\n",
    "С таким количеством уже можно работать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN - Region CNN \n",
    "Построенна по такому принципу:\n",
    "\n",
    "- на изображении ищутся ROI \n",
    "- для кажого делается resize \n",
    "- каждый ROI обрабатывается сверточной сетью\n",
    "которая предсказывает класс кобъекта который в него попал\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-055.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме класса модель предсказывает смещения для каждого bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS\n",
    "\n",
    "Теперь возникает другая проблема в районе объекта алгоритм генерирует множество ограничивающих прямоугольников (bounding box) которые частично прекрывают друг друга.\n",
    "\n",
    "Что бы избавиться от них используется другой алгоритм\n",
    "Non maxima suppression\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-22.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Его задача избавиться об bbox которые накладиваются на истинный"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-23.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки схожести обычно используется метрика IoU\n",
    "а заначение IoU при котором bbox считаются принадлежащими одному объекту является гиперпараметром (часто 0.5)\n",
    "\n",
    "Soft NMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-24.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-25.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast R-CNN\n",
    "\n",
    "Проблеммой описанного выше подхода является скорость.\n",
    "Так как мы вынужденны применять CNN порядка 2000 раз (в зависимости от эвристики которая генерирует ROI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-26.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И решением является поиск ROI не на самом изображении, а на карте признаков, полученной после обработки всего изображения CNN. В таком случае большая часть сверток выполняется только один раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-27.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это радикально ускоряет процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Pooling\n",
    "\n",
    "Появляется новая задача - 'resize' ROI на карте признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-28.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Документация Roi Pooling](https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-29.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-077.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорость работы CNN снизилась и теперь узким местом становится эвристика для поиска ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-078.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ссылка](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Идея: пусть сеть сама предсказывает ROI по карте признаков**\n",
    "\n",
    "Для обучения требуется посчитать 4 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region proposal network\n",
    "\n",
    "Карта признаков имеет фиксированные и относительно небольшие пространственные размеры (например 20x15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-34.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому можно вернуться к идее скользящего окна которая была отвергнута в самом начале.\n",
    "\n",
    "При этом можно использовать окна нескольких форм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-35.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываются два значения:\n",
    "\n",
    "* вероятность того что в ROI находится объект\n",
    "* смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама сеть при этом может быть очень простой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/rpn.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате скорость увеличивается почте в 10 раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-086.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Модель на PyTorch](https://pytorch.org/vision/stable/models.html#faster-r-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "fr_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    pretrained=True, progress=True, num_classes=91, pretrained_backbone=True\n",
    ")\n",
    "fr_rcnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Снова грузим данные\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "coco = COCO(\"annotations/instances_val2017.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"])\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(\n",
    "    imgIds[12]\n",
    ")  # http://images.cocodataset.org/val2017/000000370208.jpg\n",
    "img = img_list[0]\n",
    "print(\"Image data\", img)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from PIL import ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    tensor = TF.pil_to_tensor(pil_img) / 255\n",
    "    output = fr_rcnn(tensor.unsqueeze(0))\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "    for i, bbox in enumerate(output[0][\"boxes\"]):\n",
    "        if output[0][\"scores\"][i] > 0.5:\n",
    "            draw.rectangle((tuple(bbox[:2].numpy()), tuple(bbox[2:].numpy())), width=2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two stage detector\n",
    "\n",
    "Faster RCNN == Two stage detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/\n",
    "L12_Segmentation_Detection/img/lecture_12-089.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На среднем и верхнем слое выполняются очень похожие операции. Разница в том что на последнем слое предсказывается класс объекта, а промежуточном только вероятность его присутствия (objectness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Stage detector\n",
    "\n",
    "Если сразу предсказывать класс, то можно избавиться от второй стадии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-1-1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детекторы работающие \"за один проход\":\n",
    "\n",
    "YOLO, SSD, RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-1-2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Сравнение скорости моделей](https://pytorch.org/vision/stable/models.html#runtime-characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/ssd.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* модель VGG-16, предобученная на ImageNet\n",
    "* manually defines a collection of aspect ratios to use for the B bounding boxes at each grid cell + offsets (x,y,w,h)\n",
    "* напрямую предсказывает вероятность того, что класс присутствует в bounding box.\n",
    "* есть класс для \"background\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retina Net - [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-43.png\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог-пост: Что такое Focal Loss и когда его использовать](https://amaarora.github.io/2020/06/29/FocalLoss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature pyramyd network\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)\n",
    "\n",
    "Это feature extractor для детекторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-21.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же как и в случае с сегментацией, точность повышается если делать предсказания на картах признаков разных масштабов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn1.jpeg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но первые слои содержат мало семантической информации (только низкоуровневые признаки). Из за этого детектирование(и сегментация) мелких объектов удается хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея состоит в том что бы делать предсказание с учетом семантической информации полученной на более глубоких слоях. \n",
    "\n",
    "\n",
    "При этом признаки суммируются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ВОТ ТУТ ПОКА ЗАКОНЧИЛА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем к новым картам признаков может применяться дополнительная свертка.\n",
    "\n",
    "\n",
    "На выходе получаем карты признаков P2 - P5 на которых уже предсказываются bounding box.\n",
    "\n",
    "\n",
    "В случае 2-stage детектора (RCNN) карты подаются на вход RPN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn5.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А признаки для предсказаний используются из backbone "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn6.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "RetinaNet использует выходы FPN и для предсказаний класса и bbox. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-42.png\" width=\"700\">\n",
    "\n",
    "\n",
    "[Блог-пост про FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###YOLO\n",
    "\n",
    "* [2016 You Only Look Once: Unified, Real-Time Object Detection.](https://arxiv.org/pdf/1506.02640.pdf) \n",
    "* [2017 YOLO9000: Better, Faster, Stronger.](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "* [2018 YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767.pdf)\n",
    "\n",
    "* Апрель 2020. Alexey Bochkovskiy  “[YOLOv4: Optimal Speed and Accuracy of Object Detection\"](https://arxiv.org/abs/2004.10934)\n",
    "* Июнь 2020. [YOLOv5 Glenn Jocher](https://github.com/ultralytics/yolov5)\n",
    "* Июль 2021. [YOLOX: Exceeding YOLO Series in 2021](https://arxiv.org/abs/2107.08430)\n",
    "\n",
    "Первые версии проигрывали конкурентам. Но проект развивался. В настоящий момент это пожалуй оптимальный детектор по соотношению качество разпознавания/скорость.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$ \n",
    "Старые версии YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### YOLOv3\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/yolov3.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### YOLOv4\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/yolov4.jpeg\" width=\"700\">\n",
    "- SPP block\n",
    "- Dense Block\n",
    "- Больше слоев\n",
    "- online Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLOv5\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-47.png\" width=\"700\">\n",
    "\n",
    "Статья не публиковалась.\n",
    "Точность сравнима  с v4 но модель определенно лучше упакованна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"])\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(imgIds[5])\n",
    "img = img_list[0]\n",
    "print(\"Image data\", img)\n",
    "\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка модели с Torch Hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load model from torch\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из коробки работает с изображениями в разных форматах и даже url, автоматически меняет размер входного изображения, возвращает объект с результатами ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Apply yolov5 model\n",
    "results = model(pil_img)\n",
    "results.print()\n",
    "results.save()  # image on disk\n",
    "\n",
    "print(type(results.xyxy), len(results.xyxy), results.xyxy[0].shape)\n",
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import numpy as np\n",
    "\n",
    "cv_img = np.array(pil_img)\n",
    "RGB_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "annos = results.pandas().xyxy[0]\n",
    "\n",
    "for i in range(len(annos)):\n",
    "    x_min, y_min, x_max, y_max = (\n",
    "        int(annos[\"xmin\"].iloc[i]),\n",
    "        int(annos[\"ymin\"].iloc[i]),\n",
    "        int(annos[\"xmax\"].iloc[i]),\n",
    "        int(annos[\"ymax\"].iloc[i]),\n",
    "    )\n",
    "    # width, height = int(annos['xmax']-annos['xmin']), int(annos['ymax']-annos['ymin'])\n",
    "    if annos[\"name\"].iloc[i] == \"person\":\n",
    "        color = (255, 255, 255)\n",
    "    if annos[\"name\"].iloc[i] == \"bicycle\":\n",
    "        color = (0, 0, 255)\n",
    "    if annos[\"name\"].iloc[i] == \"backpack\":\n",
    "        color = (0, 255, 0)\n",
    "    RGB_img = cv2.rectangle(RGB_img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "cv2_imshow(RGB_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако если подать на вход модели тензор, выходы радикально меняются. YOLO переходит в режим обучения, что довольно не очевидно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.rand((1, 3, 416, 416))\n",
    "results = model(dummy_input)\n",
    "print(type(results), len(results))\n",
    "print(results[0].shape)\n",
    "print(type(results[1]), len(results[1]))\n",
    "for e in results[1]:\n",
    "    print(e.shape)\n",
    "# print(results[0], results[1].shape) # list of two elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нard Example Mining\n",
    "\n",
    "Представим что камера видеонаблюдения установленна на улице.\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-48.png\" width=\"700\">\n",
    "\n",
    "Целевые объекты, могут появляться достаточно редко (особено ночью)\n",
    "Но на каждом из кадров будет фон, который будет сильно меняться в зависимости от освещения погодных условий и.т.п.\n",
    "\n",
    "В результате возникнут изображения с образами которых не было в датасетах и они приведут к ложным срабатываниям.\n",
    "\n",
    "Что бы дообучить сеть можно сохранять кадры с такими срабатываниями и добавлять их в датасет.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online hard example mining\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-49.png\" width=\"700\">\n",
    "\n",
    "Можно делать это непосредственно при обучении во время формирования batch-а\n",
    "\n",
    "[Блог пост про Hard Mining Example](https://erogol.com/online-hard-example-mining-pytorch/)\n",
    "\n",
    "Или даже заложить в структуру модели:\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/pdf/1604.03540.pdf)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors](https://arxiv.org/pdf/1804.04606.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-03.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[COCO panoptic](https://cocodataset.org/#panoptic-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mask R-CNN** (Detectron) - концептуально простая, гибкая и общая схема сегментации объектов. Подход эффективно обнаруживает объекты на изображении и одновременно генерирует высококачественную маску сегментации для каждого объекта. \n",
    "\n",
    "Метод, названный Mask R-CNN, расширяет Faster R-CNN (который мы обсуждали ранее), добавляя ветвь для предсказания маски объекта параллельно с существующей ветвью для распознавания *bounding boxes*. \n",
    "\n",
    "Код доступен [тут](https://github.com/facebookresearch/Detectron)\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-096.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Модель Mask R-CNN](https://pytorch.org/vision/stable/models.html#mask-r-cnn)\n",
    "\n",
    "[Пример запуска Mask R-CNN есть в документации Pytorch](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Align\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выравнивание области интереса, или **RoI Align**, - это операция извлечения небольшой карты признаков из каждого RoI (Region of Interest) в задачах обнаружения и сегментации. Она устраняет жесткое квантование *RoI pool*, правильно выравнивая извлеченные признаки с входными данными. Чтобы избежать квантования границ или бинов RoI, RoIAlign использует билинейную интерполяцию для вычисления точных значений входных признаков в четырех регулярно дискретизированных местах в каждом бине RoI, а затем результат агрегируется (используя max или average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-30.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-31.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества детекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mAP - mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP (*Average Precision* - средняя точность) - это популярная метрика для измерения качества детекторов объектов, таких как Faster R-CNN, SSD и др. Средняя точность вычисляет среднее значение *precision* (точности) для значения *recall* от 0 до 1. Звучит сложно, но на самом деле довольно просто. Давайте разберем на конкретных примерах. Но перед этим давайте вкратце разберем что такое *precision*, *recall* и IoU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision & recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** измеряет, насколько точны ваши предсказания, т.е. процент правильных предсказаний.\n",
    "\n",
    "**Recall** измеряет, насколько хорошо вы находите все положительные срабатывания (*positives*). Например, мы можем найти 80% возможных положительных срабатываний в наших K лучших предсказаниях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот их математические определения:\n",
    "\n",
    "$\\mathrm{Precision} = \\frac{TP}{TP+FP}$\n",
    "\n",
    "$\\mathrm{Recall} = \\frac{TP}{TP+FN}$\n",
    "\n",
    "где $TP$ - True Positive, $TN$ - True Negative, $FP$ - False Positive, $FN$ - False Negative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы пытаемся детектировать яблоки на фотографиях. Предположим, мы обработали 20 фотографий (на 10 фотографиях по одному яблоку и на 10 фотографиях яблок нет) и обнаружили что:\n",
    "\n",
    "* в 7 случаях наша нейросеть обнаружила яблоко там где оно было на самом деле (True Positive),\n",
    "* в 3 случаях не обнаружила яблоко там где оно было (False Negative),\n",
    "* в 4 случаях обнаружила яблоко там где его не было (False Positive)\n",
    "* в 6 случаях правильно определила что на фотографии яблок нету (True Negative),\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем precision и recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(TP, FP):\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "pres = precision(TP=7, FP=4)\n",
    "rec  = recall(TP=7, FN=3)\n",
    "\n",
    "print('Precision = %.2f' % pres)\n",
    "print('Recall = %.2f' % rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU (Intersection over union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU измеряет перекрытие между двумя границами. Мы используем его для измерения того, насколько сильно наша предсказанная граница совпадает с истиной (границей реального объекта). В некоторых наборах данных мы заранее определяем порог IoU (например, 0.5) для классификации того, является ли предсказание True Positive или False Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, рассмотрим предсказание сети для фотографии яблока:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://st.depositphotos.com/1003272/1632/i/600/depositphotos_16322913-stock-photo-red-apple.jpg -O img.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "img = Image.open('img.jpg')\n",
    "array = np.array(img)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "x0 = 75\n",
    "y0 = 80\n",
    "w0 = 450\n",
    "h0 = 440\n",
    "\n",
    "ground_truth = Rectangle((x0, y0), w0, h0, linewidth=2, edgecolor='g', linestyle='--', facecolor='none')\n",
    "\n",
    "x1 = 90\n",
    "y1 = 120\n",
    "w1 = 500\n",
    "h1 = 310\n",
    "predicted = Rectangle((x1, y1), w1, h1, linewidth=2, edgecolor='b', facecolor='none')\n",
    "\n",
    "ax.add_patch(ground_truth)\n",
    "ax.add_patch(predicted)\n",
    "\n",
    "ax.imshow(array)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IoU опредлена как\n",
    "\n",
    "$\\mathrm{IoU} = \\frac{\\text{area of overlap}}{\\text{area of union}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем *area of overlap* и *area of union*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1):\n",
    "    x0_max = x0 + w0\n",
    "    y0_max = y0 + h0\n",
    "    x1_max = x1 + w1\n",
    "    y1_max = y1 + h1\n",
    "    dx = min(x0_max, x1_max) - max(x0, x1)\n",
    "    dy = min(y0_max, y1_max) - max(y0, y1)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy\n",
    "\n",
    "def area_of_rectangle(w, h):\n",
    "    return w*h\n",
    "# Найдем площадь пересечения предсказаний\n",
    "a_of_overlap = area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1)\n",
    "\n",
    "# Посчитаем индивидуальные площади прямоугольников\n",
    "a_0 = area_of_rectangle(w0, h0)\n",
    "a_1 = area_of_rectangle(w1, h1)\n",
    "\n",
    "# Найдем площадь их union\n",
    "a_of_union = a_0+a_1-a_of_overlap\n",
    "\n",
    "print('Area of overlap = %i' % a_of_overlap)\n",
    "print('Area of union = %i' % a_of_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посчитаем IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IoU = a_of_overlap/a_of_union\n",
    "print('IoU = %.2f' % IoU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как будет меняться IoU в зависимости от качества предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def plot_predictions_and_calculate_IoU(x1, y1, w1, h1):\n",
    "    fig,ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "    x0 = 75\n",
    "    y0 = 80\n",
    "    w0 = 450\n",
    "    h0 = 440\n",
    "\n",
    "    ground_truth = Rectangle((x0, y0), w0, h0, linewidth=2, edgecolor='g', linestyle='--', facecolor='none')\n",
    "    predicted = Rectangle((x1, y1), w1, h1, linewidth=2, edgecolor='b', facecolor='none')\n",
    "\n",
    "    ax.add_patch(ground_truth)\n",
    "    ax.add_patch(predicted)\n",
    "\n",
    "    ax.imshow(array)\n",
    "    ax.axis('off');\n",
    "\n",
    "    a_of_overlap = area_of_overlap(x0, y0, x1, y1, w0, w1, h0, h1)\n",
    "    a_0 = area_of_rectangle(w0, h0)\n",
    "    a_1 = area_of_rectangle(w1, h1)\n",
    "    a_of_union = a_0+a_1-a_of_overlap\n",
    "    IoU = a_of_overlap/a_of_union\n",
    "    print('IoU = %.2f' % IoU)\n",
    "\n",
    "interact(plot_predictions_and_calculate_IoU, \n",
    "         x1 = widgets.IntSlider(min=0, max=array.shape[0], step=10, value=90), \n",
    "         y1 = widgets.IntSlider(min=0, max=array.shape[1], step=10, value=120), \n",
    "         w1 = widgets.IntSlider(min=0, max=array.shape[0], step=10, value=500),\n",
    "         h1 = widgets.IntSlider(min=0, max=array.shape[1], step=10, value=310)\n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что чем лучше предсказание совпадает среальностью - тем выше у нас значение метрики IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжая тему яблок, возьмем все те же 20 фотографий на 10 из которых яблоки были, а на 10 яблок не было. И представим что у нас есть некая нейросеть, которая выдает следующие предсказания по всему датасету (или например по батчу):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nn_preds = pd.DataFrame({'IoU' : [0.6, 0.98, 0.4, 0.3, 0.1, 0.96, 0.7, 0.3, 0.2, 0.8],  \n",
    "                        'precision' : [1,1,0.67,0.5,0.4,0.5,0.57,0.5,0.44,0.5],\n",
    "                        'recall' : [0.2,0.4,0.4,0.4,0.4,0.6,0.8,0.8,0.8,1]})\n",
    "nn_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем считать что если $\\mathrm{IoU} \\geq 0.5$ - то предсказание правильное"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_preds['correct'] = False\n",
    "nn_preds['correct'][nn_preds['IoU'] >= 0.5] = True\n",
    "\n",
    "nn_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что precision имеет зигзагообразный характер - она снижается при ложных срабатываниях и снова повышается при истинных срабатываниях. \n",
    "\n",
    "Давайте построим график precision от recall и убедимся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision)\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По определению, что бы найти AP нужно найти площадь под кривой recall-precision:\n",
    "\n",
    "$AP = \\int_0^1p(r)dr$\n",
    "\n",
    "Господи, интеграл! Какая жуть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision и recall всегда находятся в пределах от 0 до 1. Поэтому AP также находится в пределах от 0 до 1. Перед расчетом AP для обнаружения объекта мы часто сначала сглаживаем зигзагообразный рисунок (на каждом уровне recall мы заменяем каждое значение precision максимальным значением точности справа от этого уровня отзыва)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_precision(nn_preds):\n",
    "    smooth_prec = []\n",
    "    for i in range(len(nn_preds.precision.values)):\n",
    "        max = nn_preds.precision.values[i:].max()\n",
    "        smooth_prec.append(max)\n",
    "    nn_preds['smooth_precision'] = smooth_prec\n",
    "    return nn_preds\n",
    "\n",
    "nn_preds = smooth_precision(nn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим как это выглядит на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision, label='precision', color='blue')\n",
    "ax.plot(nn_preds.recall, nn_preds.smooth_precision, label='smooth precision', color='red')\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем нам нужно сглаживание? Что бы снизить влияние случайных выбросов и \"прыжков\" в предсказаниях модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посчитаем таки AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(nn_preds.recall, nn_preds.precision, label='precision', color='blue')\n",
    "ax.plot(nn_preds.recall, nn_preds.smooth_precision, label='smooth precision', color='red')\n",
    "ax.fill_between(nn_preds.recall, nn_preds.smooth_precision, \n",
    "                np.zeros_like(nn_preds.smooth_precision), \n",
    "                color='red', alpha=0.1,\n",
    "                label='Area under the curve')\n",
    "ax.grid('on')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_ylim(0.35,1.05)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "AP = auc(nn_preds.recall, nn_preds.smooth_precision)\n",
    "\n",
    "print('AP = %.2f' % AP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последних исследовательских работах, как правило, приводятся результаты только для набора данных COCO. Для COCO AP - это среднее значение (*mean*) по нескольким IoU (минимальный IoU, который следует считать положительным совпадением). AP@[.5:.95] соответствует среднему AP для IoU от 0.5 до 0.95 с шагом 0.05. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем посчитать mAP. Для этого посчитает AP для каждого уровня IoU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nn_prediction_at_iou = []\n",
    "APs = []\n",
    "for iou in np.arange(0.5,1,0.05):\n",
    "    nn_preds_limited = nn_preds[nn_preds['IoU'] >= iou]\n",
    "    nn_preds_limited = smooth_precision(nn_preds_limited)\n",
    "    AP = auc(nn_preds_limited.recall, nn_preds_limited.smooth_precision)\n",
    "    APs.append(AP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0.5,1,0.05), APs, color='black')\n",
    "plt.axhline(np.mean(APs), color='red', ls='--', label='mAP')\n",
    "plt.xlabel('IoU')\n",
    "plt.ylabel('AP')\n",
    "plt.grid('on')\n",
    "plt.legend();\n",
    "\n",
    "print('mAP@[0.5:0.95] = %.2f' % np.mean(APs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть несколько различных определений mAP, которые разняться от соревнования к соревнованию (суть одинковая, но разница в деталях подхода), поэтому для каждого соревнования лучше использовать их собственные библиотеки для расчета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$ \n",
    "COCO mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-51.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-52.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "РАЗБИРАЕМ КАК СЧИТАЕТСЯ mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/map_data.jpeg\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/map.jpeg\" width=\"1000\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-54.png\" width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-56.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-57.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Конвертация результатов сегментации в COCO формат](https://www.javaer101.com/en/article/18652684.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синхронизируем метки Pascal2Coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Грузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "pascal2coco = {}\n",
    "\n",
    "classes = [\"__background__\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\",\n",
    "           ]\n",
    "\n",
    "\n",
    "def find_in_dic(dic, val):\n",
    "    for key in dic.keys():\n",
    "        if dic.get(key, None) == val:\n",
    "            return key\n",
    "    return 0  # Assign missed classes to bg\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")\n",
    "cats = coco.loadCats(coco.getCatIds())  # Грузим категории\n",
    "num2cat = {}  # Создаем словарь для хранения\n",
    "print(\"Категории COCO: \")\n",
    "for cat in cats:\n",
    "    num2cat[cat[\"id\"]] = cat[\"name\"]\n",
    "    if cat[\"id\"] in range(0, 80, 10):\n",
    "        print(cat[\"id\"], \":\", cat[\"name\"], end=\"   \\n\")\n",
    "\n",
    "for i in range(1, len(classes)):  # Skip BG\n",
    "    # if cats.get()\n",
    "    coco_ind = find_in_dic(num2cat, classes[i])\n",
    "    pascal2coco[i] = coco_ind\n",
    "\n",
    "print(pascal2coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = coco.loadImgs(448263)[0]\n",
    "print(img)\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools import mask\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "def binary_mask_to_rle(binary_mask):\n",
    "    rle = {\"counts\": [], \"size\": list(binary_mask.shape)}\n",
    "    counts = rle.get(\"counts\")\n",
    "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order=\"F\"))):\n",
    "        if i == 0 and value == 1:\n",
    "            counts.append(0)\n",
    "        counts.append(len(list(elements)))\n",
    "    return rle\n",
    "\n",
    "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
    "    pretrained=True, num_classes=21\n",
    ")\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "input_tensor = preprocess(pil_img)\n",
    "                                             \n",
    "with torch.no_grad():\n",
    "    output = fcn_model(input_tensor.unsqueeze(0))  # ['out'][0]\n",
    "\n",
    "indexes = output[\"out\"][0].argmax(0)\n",
    "\n",
    "detection_res = []\n",
    "for i, cls_name in enumerate(classes):\n",
    "    binary_mask = torch.zeros_like(indexes)\n",
    "    binary_mask[indexes == i] = 1  # Form FCN for baseball\n",
    "\n",
    "    if i > 0 and torch.max(binary_mask) > 0:\n",
    "        uncompressed_rle = binary_mask_to_rle(binary_mask.numpy())  # encoded_gt,\n",
    "        fortran_gt_binary_mask = np.asfortranarray(binary_mask).astype(\"uint8\")\n",
    "        encoded_gt = mask.encode(fortran_gt_binary_mask)\n",
    "        bbox = list(mask.toBbox(encoded_gt))\n",
    "        print(bbox)\n",
    "\n",
    "        detection_res.append(\n",
    "            {\n",
    "                \"score\": 1.0,  # dummy\n",
    "                \"category_id\": pascal2coco[i],\n",
    "                \"segmentation\": uncompressed_rle,\n",
    "                \"bbox\": bbox,\n",
    "                \"image_id\": 448263,\n",
    "                \"iscrowd\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(detection_res)\n",
    "with open(\"seg_res.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(detection_res, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "plt.imshow(pil_img)\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "plt.title(\"Annotation from COCO\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pil_img)\n",
    "coco.showAnns(detection_res, draw_bbox=True)\n",
    "plt.title(\"Detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущем шаге мы посчитали предсказаниее от YOLO. Давайте оценим его точность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"seg_gt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(anns, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# loadRes will generate a new COCO type instance based on coco_gt and return\n",
    "coco_dt = coco.loadRes(\"seg_res.json\")\n",
    "coco_gt = coco.loadRes(\"seg_gt.json\")\n",
    "\n",
    "cocoEval = COCOeval(coco_gt, coco_dt, \"bbox\")  # 'segm', 'bbox'\n",
    "# cocoEval.params.useSegm = True\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()\n",
    "\n",
    "print(cocoEval.stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO - Self-supervised representation learning (with segmentation capabilities)\n",
    "[Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)\n",
    "\n",
    "[Отличное видео объяснение статьи](https://www.youtube.com/watch?v=h3ij3F3cPIk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка к запуску"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВНИМАНИЕ: ПОСЛЕ ЗАПУСКА ЭТОЙ ЯЧЕЙКИ ТРЕБУЕТСЯ ПЕРЕЗАПУСТИТЬ СРЕДУ ВЫПОЛНЕНИЯ \n",
    "# (Среда выполнения -> перезапустить среду выполнения)\n",
    "!pip install -U augly\n",
    "!sudo apt-get install python3-magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала подгрузим модель DINO (self-**DI**stillation with **NO** labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/dino.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузим случайную картинку (можно выбрать любую, просто замените ссылку на свою)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = 'https://www.abenteuer-regenwald.de/uploads/media/content-l/01/1271-Yellow-headed_caracara_%28Milvago_chimachima%29_on_capybara_%28Hydrochoeris_hydrochaeris%29.jpg?v=1-0'\n",
    "URL = 'https://limanpravda.com/files/uploads/2020/06/Samyj-smeshnoj-gryzun-kapibara.jpg'\n",
    "!wget $URL -O test.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты DINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим что с ней может сделать DINO, а затем обсудим как это работает и что вообще происходит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/dino/visualize_attention.py --image_path /content/test.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока не вдаваясь в детали, посмотрим на картинки которые генерирует DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "    fig,ax = plt.subplots(nrows=rows, ncols=cols, figsize=(28,8))\n",
    "    for num, img in enumerate(imgs):\n",
    "        img_PIL = Image.open(img)\n",
    "        ax[num].imshow(img_PIL)\n",
    "        ax[num].set_xticks([])\n",
    "        ax[num].set_yticks([])\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "image_grid(imgs=sorted(glob('*.png'))[::-1], rows=1, cols=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы понять что мы видим, давайте разберем архитектуру DINO и пойдем как ее обучали.\n",
    "На самом деле, DINO не столько архитектура, сколько метод - то есть в качества *backbone* можно использовать любую нейросеть (например ResNet или ViT). Самые лучшие реузльтаты показали DINO на основе VIT, соответственно в этом блокноте будем разбирать именно эту конфигурацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**КАРТИНКУ ПОДГРУЗИТЬ НА СЕРВЕР**\n",
    "\n",
    "<img src =\"https://theaisummer.com/static/60c2cbe8edb845502c8bbc3b9e88791d/ee604/vision-transformer.png\" width=\"700\">\n",
    "\n",
    "Давайте вспомним как работает ViT. **Vi**sual **T**ransformer получает на вход картинку разбитую на кусочки (*patches*) размеров 8 или 16 пикселей. Затем эти кусочки расправляются в вектор (*flatten*) и пропускаются через энкодер трансформера. Поверх трансформера прикручена MLP голова, которая собирает информацию с голов трансформера и предсказывает класс для картинки.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**КАРТИНКУ ПОДГРУЗИТЬ НА СЕРВЕР**\n",
    "\n",
    "<img src =\"https://neurohive.io/wp-content/uploads/2021/05/dino3-1-426x422.png\" width=\"400\">\n",
    "\n",
    "DINO - это self-supervised метод, а значит классы ему недоступны. Что же делает эта сеть?\n",
    "\n",
    "Разберем по шагам. DINO на вход получает изображение $x$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import augly.image as imaugs\n",
    "import augly.utils as utils\n",
    "from IPython.display import display\n",
    "\n",
    "input_img = imaugs.scale('test.jpg', factor=0.5)\n",
    "display(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше к этому изображению применяются две различные аугментации - $x_1$ и $x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_x1 = imaugs.Compose(\n",
    "    [\n",
    "     imaugs.Crop(0.1,0.1,0.6,0.6),\n",
    "     imaugs.ColorJitter(\n",
    "         brightness_factor=np.random.uniform(0.5,1),\n",
    "         contrast_factor=np.random.uniform(0.5,1),\n",
    "         saturation_factor=np.random.uniform(0.5,1)\n",
    "     ),\n",
    "     imaugs.RandomBlur(),\n",
    "     imaugs.Resize(128,128)\n",
    "     ]\n",
    "    )\n",
    "\n",
    "transform_x2 = imaugs.Compose(\n",
    "    [\n",
    "     imaugs.Crop(0.4,0.3,0.9,0.7),\n",
    "     imaugs.ColorJitter(\n",
    "         brightness_factor=np.random.uniform(0.5,1),\n",
    "         contrast_factor=np.random.uniform(0.5,1),\n",
    "         saturation_factor=np.random.uniform(0.5,1)\n",
    "     ),\n",
    "     imaugs.RandomBlur(max_radius=1),\n",
    "     imaugs.Resize(128,128)\n",
    "     ]\n",
    "    )\n",
    "\n",
    "aug_image_x1 = transform_x1(input_img)\n",
    "aug_image_x2 = transform_x2(input_img)\n",
    "\n",
    "fig,ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow(aug_image_x1)\n",
    "ax[1].imshow(aug_image_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее каждая из этих аугментация проходит через свою собственную версию **ViT** - *student* и *teacher*. Такой подход, когда с помощью большой сети учат сеть поменьше - называется *distillation*. В случае с DINO, они говорят о *self-distillation*, так как дистилляция происходит внутри одной сети.\n",
    "\n",
    "В реальности, обучается только студент, а веса учителя обновляется как **E**xponential **M**oving **A**verage весов студента. Но пожалуй не будем залезать еще глубже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16') = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конечном счете обе ветки сети выдают какое-то представление (*representation*) данных, которая, как мы надеемся будет близка для похожих изображений и далека для непохожих. Давайте на него посмотрим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Сконвертируем каждую аугментированную картинку в тензор\n",
    "x1_tensor = transform(aug_image_x1).unsqueeze(0) \n",
    "x2_tensor = transform(aug_image_x2).unsqueeze(0)\n",
    "\n",
    "# И прогоним через нашу обученную DINO\n",
    "x1_representation = vits16(x1_tensor)\n",
    "x2_representation = vits16(x2_tensor)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow(x1_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[1].imshow(x2_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[0].set_title('$x_1$');\n",
    "ax[1].set_title('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит и впрямь довольно похоже. А что если мы засунем туда что-то совершенно непохожее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? imaugs.RandomEmojiOverlay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_x3 = imaugs.Compose(\n",
    "    [\n",
    "     imaugs.Crop(0.0,0.0,0.2,0.2),\n",
    "     imaugs.Resize(128,128),\n",
    "     imaugs.RandomEmojiOverlay(emoji_size=0.4),\n",
    "     imaugs.Resize(128,128),\n",
    "     ]\n",
    "    )\n",
    "\n",
    "aug_image_x3 = transform_x3(input_img)\n",
    "\n",
    "aug_image_x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_tensor = transform(aug_image_x3).unsqueeze(0) \n",
    "\n",
    "x3_representation = vits16(x3_tensor[:,:3,:,:])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3)\n",
    "ax[0].imshow(x1_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[1].imshow(x2_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[2].imshow(x3_representation.view(24,16).detach().cpu().numpy())\n",
    "ax[0].set_title('$x_1$');\n",
    "ax[1].set_title('$x_2$');\n",
    "ax[2].set_title('$x_3$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, выглядит не очень похоже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss для такой сети можно записать следующим образом: \n",
    "\n",
    "$\\text{loss} = H(t_1, s_2)/2 + H(t_2, s_1)/2$,\n",
    "\n",
    "где $ H(a, b) = −a \\space log(b)$, $t_i$ - выученное представление i-ой аугментации teacher network и $s_i$ -  выученное представление i-ой аугментации student network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте вновь посмотрим на результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(imgs=sorted(glob('*.png'))[::-1], rows=1, cols=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим 6 карт внимания (*self-attention maps* - веса слоя self-attention) на 6 головах Visual Transformer. В результате self-supervised обучения по методике DINO, трансформер **САМОСТОЯТЕЛЬНО** придумал обращать внимание на различные части изображения, таким образом производя семантическую сегментацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем все эти карты внимания объеденить в одно изображение, и просто назначить каждой карте свой цвет, а в качестве прозрачности использовать интенсивность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "def overlay(img, segmentations):\n",
    "    img_PIL = Image.open(img)\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(10,10))\n",
    "    ax[0].imshow(img_PIL)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "    \n",
    "    ax[1].imshow(img_PIL.convert('LA'), alpha=0.5)\n",
    "    for num, img in enumerate(segmentations):\n",
    "        segment_PIL = Image.open(img).convert('LA')\n",
    "        segment_arr = np.array(segment_PIL)\n",
    "        colors = [(*cm.tab10(num)[:-1], c) for c in np.linspace(0,0.75,100)]\n",
    "        cmap = mcolors.LinearSegmentedColormap.from_list('mycmap', colors, N=5)\n",
    "        ax[1].imshow(segment_arr[:,:,0], cmap=cmap)\n",
    "    ax[1].set_facecolor('black')\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "overlay('/content/img.png', sorted(glob('*.png'))[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что DINO сегментирует разные части нашей картинки на разные семантические группы. В случае с капибарой - это голова, лицо (нос, глаза), животик и тело"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация видео"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но если вы думали что на этом все? То DINO может еще удивить. Например она умеет сегментировать видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip install --upgrade youtube_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем видос (можно любой)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://pixabay.com/videos/download/video-53486_large.mp4'\n",
    "!youtube-dl -o video.mp4 $URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сгенерируем сегментированное видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/dino/video_generation.py --input_path /content/video.mp4 --output_path  /content/video_segmented --resize 360 640\n",
    "!ffmpeg -i /content/video.mp4 -vf scale=640:360 /content/video_scaled.mp4\n",
    "!ffmpeg \\\n",
    "  -i /content/video_scaled.mp4 \\\n",
    "  -i /content/video_segmented/video.mp4 \\\n",
    "  -filter_complex '[0:v]pad=iw*2:ih[int];[int][1:v]overlay=W/2:0[vid]' \\\n",
    "  -map '[vid]' \\\n",
    "  -c:v libx264 \\\n",
    "  -crf 23 \\\n",
    "  -preset veryfast \\\n",
    "  output.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что получилось. Напоминаю, эта сеть вообще в режиме self-supervision училась!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open('/content/output.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=800 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**КАРТИНКУ ПОДГРУЗИТЬ НА СЕРВЕР**\n",
    "\n",
    "А еще DINO умеет кластеризовать изображения. Выполнять не будем, так как процесс не быстрый, но можем посмотреть на результаты из их статьи\n",
    "\n",
    "<img src =\"https://scontent-vie1-1.xx.fbcdn.net/v/t39.2365-6/177871790_363624551852633_263144119734802610_n.gif?_nc_cat=105&ccb=1-3&_nc_sid=ad8a9d&_nc_ohc=aPyzhjD7npoAX_5yiBg&_nc_ht=scontent-vie1-1.xx&oh=7c93d5f7b5666f361571b278c68582d6&oe=611050F6\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Список использованной литературы\n",
    "[Подробнее о том как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch).\n",
    "\n",
    "[Видео-разбор Run-Length Encoding](https://www.youtube.com/watch?v=h6s61a_pqfM)\n",
    "\n",
    "[Блог-пост про 2d свертки с помощью пеермножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)\n",
    "\n",
    "[Статья U-Net](https://arxiv.org/abs/1505.04597)\n",
    "[Блог-пост про U-Net](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5)\n",
    "\n",
    "[Блог-пост про семантическую сегментацию](https://www.jeremyjordan.me/semantic-segmentation/)\n",
    "\n",
    "[Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics](https://arxiv.org/pdf/1705.07115.pdf)\n",
    "\n",
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "[Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "[Блог-пост: Что такое Focal Loss и когда его использовать](https://amaarora.github.io/2020/06/29/FocalLoss.html)\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)\n",
    "\n",
    "[2016 You Only Look Once: Unified, Real-Time Object Detection.](https://arxiv.org/pdf/1506.02640.pdf) \n",
    "\n",
    "[2017 YOLO9000: Better, Faster, Stronger.](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "\n",
    "[2018 YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767.pdf)\n",
    "\n",
    "[YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/abs/2004.10934)\n",
    "\n",
    "[YOLOv5 Glenn Jocher](https://github.com/ultralytics/yolov5)\n",
    "\n",
    "[Блог пост про Hard Mining Example](https://erogol.com/online-hard-example-mining-pytorch/)\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/pdf/1604.03540.pdf)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors](https://arxiv.org/pdf/1804.04606.pdf)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
