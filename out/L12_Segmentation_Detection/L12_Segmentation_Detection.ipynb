{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer vision task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поговорим о зрении человека. Поскольку зрение относится к области искусственного интеллекта, значит, наиболее известный эталон – зрение человека. По оценкам, 25% мозга занято решением задачи зрения. С помощью глаз мы извлекаем визуальные подсказки, какие-то характеристики реальных объектов, сопоставляем их с наши априорными знаниями об устройстве окружающего мира и делаем выводы.\n",
    "\n",
    "Мы можем распознать объекты на изображении, используя следующие подсказки:\n",
    "1) Цвет\n",
    "\n",
    "2) Контур\n",
    "\n",
    "3) Текстура\n",
    "\n",
    "4) Тени и освещение\n",
    "\n",
    "5) Контекст (окружение объекта) \n",
    "\n",
    "К априорным знаниям об объекте можно отнести следующее:\n",
    "1) Размеры\n",
    "\n",
    "2) Контекст\n",
    "\n",
    "3) Типичный диапазон распределения конкретной характеристики\n",
    "\n",
    "4) Эталонные примеры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЗАдача сегментации - понять где именно на изображении располагаются объекты. С одной стороны может быть интересно где находится объект (область в границе) в грубом приближении. С другой стороны есть задача - выделить гарницы объхекта более четко, где нас интересует не область, а пиксельная маска. \n",
    "\n",
    "В общем случае задача сегментации – разделить изображение на фрагменты (группы пикселей) по некоторому общему критерию. В зависимости от критерия получаются разные задачи сегментации изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Извлечение объекта – выделение конкретного произвольного объекта, указанного пользователем или по-другому заданного. Например, пользователь может выделить объект ограничивающим прямоугольником или нарисовать контур объекта.\n",
    "2. Сегментация без учителя – разделение изображения на регионы, однородные по своим визуальным характеристикам и отличающиеся от соседних регионов. Например, нужно выделить области, соответствующие разным фрагментам фотографии котика (земля, небо, котик)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-006.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку такие методы разбивают объекты обычно на несколько сегментов, то их еще называют методами пересегментации, потому что мы сегментируем чрезмерно по сравнению с тем, как хотелось бы. Другой вариант – семантическая сегментация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семантическая сегментация – попиксельная разметка изображения, где каждая метка соответствует определенному объекту. \n",
    "\n",
    "При этом:\n",
    "* Разные пиксели одного объекта существенно отличаются друг от друга по признакам (яркости, цвету, текстуре окрестности)\n",
    "* Единственное, что у них общее – «семантика»\n",
    "* Поэтому задача сегментации тесно связана с задачей распознавания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оценка точности сегментации**\n",
    "\n",
    "Как оценивается точность сегментации? Во-первых, есть попиксельные метрики, то есть мы оцениваем долю правильно отсегментированных пикселей на изображении независимо от того, какую задачу мы рассматриваем. Если метка пикселя верна – это\n",
    "хорошо, и рассматриваем долю верных меток пикселей по отношению ко всем пикселям.\n",
    "\n",
    "\n",
    "Второй вариант оценки примерно такой же, как мы использовали в детектировании объектов, то есть у нас имеется множество сегментов, мы сопоставляем сегменты друг другу и считаем критерий IOU (intersection over union) (рис. 98). Мы для всех сегментов считаем долю правильно классифицированных пикселей по отношению к комбинации правильно классифицированных, ложно позитивных срабатываний и ложноотрицательных срабатываний для каждого рассматриваемого класса объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-115.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-007.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-009.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-010.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-012.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-013.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-015.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-017.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Network upsampling: Max Unpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из направлений было исследование новых способов повышения разрешения карт признаков с использованием какой-то дополнительной информацией. Так получился слой max unpooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что при понижении разрешения изображения мы обычно используем max pooling, например, с шагом 2х2, то есть из каждого блока 2х2 пикселя мы сохраняем только одно максимальное значение. Запомним индекс, из которого было получено максимальное значение, поскольку нам важно знать, где именно в картинке было это максимальное значение. Если мы запомним этот индекс, тогда сможем выполнить операцию повышения разрешения, которая будет не просто делать пиксели с равномерным шагом, а заносить целевой пиксель именно туда, откуда мы взяли изначально максимальное значение. Уже после этого будем проходиться по этой карте\n",
    "набором сверток, которые будут интерполировать промежуточные значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Сохраняем индексы каждого max pooling слоя\n",
    "* При повышении разрешения копируем значения из выхода max pooling слоя с учетом запомненных индексов, применяем обученные свертки для сглаживания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью операции unpooling появилась архитектура для сегментации изображения, фактически для декодирования признаков, Encoder-Decoder with Max Unpooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-119.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь между соответствующими словами кодировщика и декодировщика передаётся информация в виде индексов операции max-pooling, то есть не вся информация со слоев высокого разрешения теряется, она сохраняется и передается на декодировщик. Эта архитектура оказалась точнее предыдущей.\n",
    "\n",
    "В одной из работ эту архитектуру довели до абсолюта, то есть построили такой кодировщик, который превращает картинку в вектор 1х1 пикселя (из картинки сохраняется только контекст). Используя сохраненные max-pooling индексы, повышаем разрешение до исходного, предсказывая карту разметки. Как показали эксперименты, контекст вообще можно проигнорировать и заменить вектор на константный, так как в индексах операции max-pooling хранится достаточно информации для того, чтобы предсказать карту сегментации для исходного изображения. Поскольку свертки связаны с визуальными образами, информации о том, где свертка дала максимальное значение, достаточно, чтобы детектировать объект. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-019.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnable upsampling: Transpose convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-020.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-021.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnable upsampling: 1D example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-031.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution as matrix multiplication (1D example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-033.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic segmentation idea: fully convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-034.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic segmentation: summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-035.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-036.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection: single object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-039.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection: multiple objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-042.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-044.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping features: Rol Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-064.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-048.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instant segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая постановка задачи – сегментация экземпляров (Instance segmentation). Мы фиксируем классы объектов, которые хотим выделить и каждый экземпляр заданных классов мы помечаем своей меткой. Задача сегментации экземпляров – это дальнейшее развитие задачи детектирования объектов. Если в обычной задаче детектирования мы локализовывали объекты, например, типа «автомобили» и\n",
    "показывали, где они расположены с помощью прямоугольной рамки, то здесь нужно указать точную область, которую занимает каждый конкретный экземпляр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-093.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче сегментации экземпляров мы выделяем объекты типа things, которые хорошо локализованы в пространстве. В задаче семантической сегментации мы можем пометить области, которые соответствуют другой категории объектов, таких как небо и\n",
    "дорога. Разумеется, возникает задача, в которой и семантическая сегментация, и сегментация экземпляров являются подзадачами, такая задача получила название паноптическая сегментация (panoptic segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instant segmentation: mask R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-095.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: lots of computer vision taska"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-037.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-038.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region proposal: selective search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-049.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая нейросеть на основе каскада – Region-based convolution networks (R-CNN), по сути двухэтапный каскад (рис.77). Мы используем какой-то внешний алгоритм как генератор гипотезы объектов, с помощью него генерируем заданное количество гипотез, чтобы обеспечить достаточно высокую полноту. Затем каждую из этих гипотез мы вырезаем из изображения и подаем на вход классификатору."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод генерации гипотез, который использовался в R-CNN (регионально сверточных сетях), - один из методов иерархической сегментации, то есть этот метод последовательно разбивает изображение на однородные сегменты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку нейросети на тот момент принимали на вход только квадратные изображения фиксированного разрешения, а сегменты, которые находил метод были произвольных пропорций, то нам нужно было их преобразовывать. При этом в тот\n",
    "момент эталонные коллекции для обучения были не очень большими по сравнению с теми коллекциями, ан которых обучались для задачи классификации. \n",
    "\n",
    "Поэтому примеров в этих коллекциях было недостаточно для того, чтобы обучить нейросеть с нуля. Поэтому нейросеть использовали только как метод извлечения признаков, брали выход одного из слоев, обычно полносвязного, и подавали его на вход методу опорных векторов. Фактически взяли метод HOG+CVN, но вместо того, чтобы использовать гистограмму ориентированных градиентов как признаки, брались нейросетевые признаки с сети классификатора. Это было достаточно сложно и медленно, но точность высокая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-116.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом очень сильно повысить точность помогла еще одна идея – уточнение ограничивающего прямоугольника или регрессия bbox.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-117.png\" width=\"700\">\n",
    "\n",
    "Суть идеи заключается в следующем:\n",
    "\n",
    "* Исходные гипотезы bbox могут быть очень неточными, поскольку мы получаем их от внешнего метода и не знаем насколько точно они локализованы. За счет того, что нейросетевой метод устойчив к положению объектов внутри этого ограничивающего прямоугольника, даже если локализация не очень точная, он все равно выдаст правильный ответ: есть объект или нет\n",
    "\n",
    "* Чтобы повысить точность локализации, вместо классификатора на тех же самых признаках обучим линейную регрессию, то есть мы будем находить сдвиг ограничивающего прямоугольника относительно исходного, где именно должен располагаться объект\n",
    "\n",
    "* Такую регрессию можно легко обучить, и она существенно улучшает точность классификации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-118.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, метод регрессии bbox позволяет взять неточную гипотезу и уточнить ее. Такого не было до нейросетевых методов, до этого мы полагались на ограничивающий прямоугольник и для него только предсказывали метку: он или не он.\n",
    "\n",
    "Теперь мы можем не только предсказывать метку, но и уточнять ее, за счет этого мы можем просматривать гораздо меньше прямоугольников, чем раньше. Теперь мы можем брать прямоугольники, фрагменты с достаточно большим шагом на меньшем\n",
    "числе масштабов за счет того, что мы будем уточнять наши изначальные гипотезы значительно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У R-CNN было много недостатков:\n",
    "\n",
    "1. Вычисляем нейросетевые признаки независимо для каждого окна-гипотезы. Поскольку они пересекаются, это ведет к избыточным вычислениям\n",
    "\n",
    "2. Нужно масштабировать фрагменты-гипотез до нужного разрешения \n",
    "\n",
    "3. Сложная процедура обучения\n",
    "\n",
    "4. Зависимость от внешнего алгоритма генерации гипотез"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-055.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-056.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-057.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основании ROI-pooling сделали модификацию метода R-CNN – Fast R-CNN.\n",
    "\n",
    "**Основные идеи этого метода:**\n",
    "\n",
    "Вычислять сверточные признаки по всему изображению, извлекать из конкретных гипотез вектор-признаки с помощью слоя ROI-pooling. И за тем вместо того, чтобы подавать их на вход SVM, подавать их на вход обычным полносвязным слоям нейросетевого классификатора и одновременно по этим признакам делать и классификацию с помощью soft max, и регрессию bbox. При этом мы можем обучать нашу сеть в режиме многоцелевого обучения. Таким образом, мы можем и дообучить и обучить сверточные признаки, которые мы вычисляем по всему изображению. Для того, чтобы использовать особенность этого метода, которая заключается в том, что он\n",
    "не требует повторного вычисления признаков, нам нужно с каждого изображения брать много гипотез. Гипотезой в нашем случае будет какое-то окно интересов, потому что мы вычислим для этого окна интересов ошибку, затем применим обратное распространение ошибки для получения градиента по всей нейросети. Соответственно одним элементом у нас является именно регион конкретного изображения (регион интересов).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты от всей нейросети занимают много места в памяти, поэтому обычно берут малое количество изображений в одном mini-batch, но с одного изображения собирают много гипотез. Из-за огромного размера современных нейросетей сейчас mini-batch может состоять из одной картинки и нескольких десятков гипотез"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы обеспечить сбалансированность выборки, нужно брать примерно одинаковое количество положительных и отрицательных гипотез внутри изображения. Если, например, мы можем в каждом изображении взять 64 гипотезы, нам нужно, чтобы 32 гипотезы из этой выборки были положительными и 32- отрицательными. Отрицательные гипотезы могут быть разными по качеству, поэтому на этом этапе\n",
    "можем применить процедуру, похожую на процедуру поиска сложных примеров для обучения нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-063.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN vs Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-077.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast-ER R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий шаг, который нужно было сделать для улучшения сети – избавиться от внешнего генератора гипотез. Так появилась Faster R-CNN. Faster R-CNN – пример нейросетевой архитектуры, которая используется до сих пор. По сути Faster R-CNN = Fast R-CNN + RPN (нейросетевого генератора гипотез). \n",
    "\n",
    "Нейросетевой генератор гипотез – маленькая нейросеть, которая по тем же самым сверточным признакам генерирует гипотезы. Наиболее вероятные гипотезы подаются на вход ROI-pooling слою и затем классифицируются более мощным классификатором."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-078.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region proposal network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Маленькое скользящее окно по карте признаков (feature map)\n",
    "* Маленькая нейросеть для\n",
    "    - Классификации объект/не объект\n",
    "    - Регрессии bbox\n",
    "* Позиция окна показывает локализацию объекта относительно изображения\n",
    "* Регрессия bbox показывает положение bbox относительно положения скользящего окна "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-080.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast-ER R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-085.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-086.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-087.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-089.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-stage Object detection: YOLO/SSD/RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод Faster R-CNN достаточно мощный, но зачастую нам нужны методы побыстрее. Самое медленное в этом методе – классификация гипотез с помощью полносвязного классификатора (второй этап).\n",
    "\n",
    "Поэтому родилась следующая идея: выкинуть второй этап и сделать одностадийный метод. Для этого нужно усилить RPN. Так появились одностадийные детекторы, которые работают быстрее, но менее точно, чем Faster R-CNN. Одним из таких детекторов является YOLO (сокращение от названия статьи You Only Look Once). \n",
    "\n",
    "Как устроен этот детектор?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем изображение сеткой и сделаем такую нейросеть, которая будет для каждой ячейки предсказывать, есть ли объект, ограничивающий прямоугольник которого - центр данной ячейки. Фактически, разбив изображение сеткой 7х7, мы сделаем 49 гипотез. Поскольку объекты бывают разных размеров и предсказывать их одним классификатором неудобно, мы воспользуемся похожей на якори идеей и будем в каждой ячейки выдвигать 2 гипотезы: одну маленькую, а другую большую. \n",
    "\n",
    "В сумме наши 49 ячеек сделают 98 гипотез. Параллельно каждую ячейку заставим предсказывать класс объекта. Для каждой гипотезы у нас есть вероятность классификатора. Применим метод подавления немаксимумов жадного подавления гипотез и оставшиеся гипотезы обрезаем по порогу по степени уверенности. Если говорить о параметризации ответа (что мы хотим получить на выходе сети), то на\n",
    "выходе мы хотим получить трехмерную матрицу размера 7 х 7 х (2 х 5 + 20) = 7 х 7 х 30, так как"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сетка 7 х 7, 2 гипотезы объектов, 20 классов. То есть сеть на вход берет изображение. А на выходе предсказывает 7 х 7 параметров. Эта сеть называется DarkNet.\n",
    "\n",
    "DarkNet устроен следующим образом: он состоит из сверточных слоев таким образом, что в конце у нас появляется матрица размером 7 х 7 х 1024. Далее эту матрицу преобразуют в один полносвязный слой длиной 4096, этот слой содержит всю информацию про изображение. Из этого полносвязного слоя генерируется ответ 7 х 7 х 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-090.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть другой вариант, который был предложен в Google – Single Shot Detector (SSD). \n",
    "\n",
    "В отличие от детектора YOLO в нем нет этапа преобразования в полносвязный слой, в нем генерируются гипотезы для объектов по признакам, собранным со сверточных слоев. И в отличие от детектора YOLO, который делал одну сетку для всего изображения, в детекторе SSD генерировалось несколько масштабов, несколько разбиений изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединив идеи одностадийного алгоритма детектирования объектов, построения пирамиды признаков и применение RPN сети по этим признакам, состоящей из 4 слоев 3 х 3 пикселей, сделали архитектуру Retina Net, которая является одной из наиболее точных архитектур для детектирования объектов в настоящее время.\n",
    "\n",
    "\n",
    "RetinaNet = SSD + FPN + FocalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-020.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от детектора YOLO в нем нет этапа преобразования в полносвязный слой, в нем генерируются гипотезы для объектов по признакам, собранным со сверточных слоев. И в отличие от детектора YOLO, который делал одну сетку для всего изображения, в детекторе SSD генерировалось несколько масштабов, несколько разбиений изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection: Lots of variables..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-091.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection: faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-094.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-096.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask R-CNN: example mask training targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-100.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask R-CNN: very good results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-101.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask R-CNN: also does pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-102.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond 2D object detection.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-104.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-105.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-106.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense video captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-107.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects+relationships = scene graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-108.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-108.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene graph prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-109.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-110.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D object detection: simple camera model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-111.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D object detection: monocular camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-112.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D shape protection: mesh R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-113.png\" width=\"700\">"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
