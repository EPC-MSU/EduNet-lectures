{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи компьютерного зрения\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "Кроме классификации CV решает и другие задачи.\n",
    "\n",
    "Сегментация, детектирование и многое другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формат разметки COCO\n",
    "\n",
    "Прежде чем говорить о способах решения задач компьютерного зрения разберемся с форматами входных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COCO - Common Objects in COntext\n",
    "\n",
    "Один из наиболее популярных датасатов содержащий данные для сегментации и детектирования.\n",
    "\n",
    "Содержит: \n",
    "- Категории\n",
    "- Маски\n",
    "- Ограничивающие боксы (*bounding boxes*)\n",
    "- Описания (*captions*)\n",
    "- Ключевые точки (*keypoints*)\n",
    "- И многое другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "r = requests.get(\n",
    "    \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    ")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с датасетом используется пакет `pycocotools`\n",
    "\n",
    "[Подробнее о том как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "coco = COCO(\"annotations/instances_val2017.json\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся с форматом на примере одной записи"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "catIds = coco.getCatIds(catNms=[\"cat\"])  # Получаем ID котиков\n",
    "print('ID класса \"кот\" = %i' % catIds[0])\n",
    "\n",
    "imgIds = coco.getImgIds(catIds=catIds)  # Фильтруем датасет по тегу\n",
    "print(\"Всего изображений с котами: %i\" % len(imgIds))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем что у нас в метаданных"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img_list = coco.loadImgs(imgIds[0])  # Берем только первую картинку\n",
    "img = img_list[0]\n",
    "img"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на изображение"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(I)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сконвертируем в PIL формат для удобства дальнейшей работы"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def coco2pil(url):\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    return Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(pil_img)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Категории в COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на примеры категорий в нашем датасете. Отобразим каждую 10ую категорию"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cats = coco.loadCats(coco.getCatIds())  # Грузим категории\n",
    "num2cat = {}  # Создаем словарь для хранения\n",
    "print(\"Категории COCO: \")\n",
    "for cat in cats:\n",
    "    num2cat[cat[\"id\"]] = cat[\"name\"]\n",
    "    if cat[\"id\"] in range(0, 80, 10):\n",
    "        print(cat[\"id\"], \":\", cat[\"name\"], end=\"   \\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете так же есть категория **0**. Ее используют для обозначения класса фона."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть так же суперкатегории"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(cats[2])\n",
    "print(cats[3])\n",
    "\n",
    "nms = set([cat[\"supercategory\"] for cat in cats])\n",
    "print(\"COCO supercategories: \\n{}\".format(\"\\n\".join(nms)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вернемся к метаданным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо метаданных нам доступна разметка ([подробнее о разметке](!https://cocodataset.org/#format-data)), давайте ее загрузим и отобразим"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "plt.imshow(I)\n",
    "plt.axis(\"off\")\n",
    "coco.showAnns(anns)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберем из чего состоит разметка"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def dump_anns(anns):\n",
    "    for i, a in enumerate(anns):\n",
    "        print(f\"#{i}\")\n",
    "        for k in a.keys():\n",
    "            if k == \"category_id\" and num2cat.get(a[k], None):\n",
    "                print(k, \": \", a[k], num2cat[a[k]])  # Show cat. name\n",
    "            else:\n",
    "                print(k, \": \", a[k])\n",
    "\n",
    "\n",
    "dump_anns(anns)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для объектов, которых слишком много существует отдельная метка `iscorwd`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (120, 60)\n",
    "\n",
    "catIds = coco.getCatIds(catNms=[\"people\"])\n",
    "annIds = coco.getAnnIds(catIds=catIds, iscrowd=True)\n",
    "anns = coco.loadAnns(annIds[0:1])\n",
    "\n",
    "dump_anns(anns)\n",
    "img = coco.loadImgs(anns[0][\"image_id\"])[0]\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(I)\n",
    "coco.showAnns(anns)  # People in the stands\n",
    "seg = anns[0][\"segmentation\"]\n",
    "print(\"Counts\", len(seg[\"counts\"]))\n",
    "print(\"Size\", seg[\"size\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как получить маску в виде массива?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "msk = np.zeros(seg[\"size\"])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for row in range(4):\n",
    "    for col in range(4):\n",
    "        msk = coco.annToMask(anns[i])\n",
    "        ax[row, col].imshow(msk)\n",
    "        ax[row, col].set_title(num2cat[anns[i][\"category_id\"]])\n",
    "        i += 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще у нас есть bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "\n",
    "RGB_img = cv2.cvtColor(I, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "for i in range(len(anns)):\n",
    "    x, y, width, heigth = anns[i][\"bbox\"]\n",
    "    x, y, width, heigth = int(x), int(y), int(width), int(heigth)\n",
    "    if anns[i][\"category_id\"] == 1:\n",
    "        color = (255, 255, 255)\n",
    "    if anns[i][\"category_id\"] == 37:\n",
    "        color = (255, 0, 0)\n",
    "    if anns[i][\"category_id\"] == 40:\n",
    "        color = (0, 255, 0)\n",
    "    RGB_img = cv2.rectangle(RGB_img, (x, y), (x + width, y + heigth), color, 2)\n",
    "cv2_imshow(RGB_img)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И еще куча всего"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Еще более глубокое понимание разметки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое [run-length encoding - RLE](https://en.wikipedia.org/wiki/Run-length_encoding)?\n",
    "\n",
    "[Видео-разбор](https://www.youtube.com/watch?v=h6s61a_pqfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантическая сегментация (*Semantic segmentation*)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-007.png\" width=\"900\">\n",
    "\n",
    "Постановка задачи:\n",
    "\n",
    "Предсказать класс для каждого пикселя.\n",
    "\n",
    "Входные данные маска: \n",
    "\n",
    "[ x,y - > class_num ] \n",
    "\n",
    "Выходные данные маска:\n",
    "\n",
    "[ x,y - > class_num ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Способы решения\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **a) Наивный.**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-012.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Скользящим окном пройтись по изображению и предсказать клас для каждого пикселя с учетом его соседей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **б) Разумный**\n",
    "\n",
    "Убрать линейный слой в конце сети. По признакам во всех каналах определять класс каждого пикселя.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-015.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L09_CNN_Architectures/img/L09_CNN_Architectures_15.png\"  width=\"700\">\n",
    "\n",
    "В лекции №8 мы говорили о том что сверту 1x1 можно рассматривать как полносвязанный слой.\n",
    "\n",
    "\n",
    "Именно так она и будет использоваться при сегментации.\n",
    "\n",
    "Количество классов будет соответствовать числу каналов.\n",
    "\n",
    "\n",
    "Проблемы:\n",
    "- нужно большое рецептивное поле, следовательно много слоев ( L 3х3 conv -> 1+2L receptive field)\n",
    "- очень медленно на полноразмерных картах активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **в) Эффективный**\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-017.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Вернмся к традиционной структуре со сжатием пространственных размеров. А после нее добавим разжимающий блок. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автокодировщик\n",
    "\n",
    "Такая архитектура довольно популярна и применяется не только для сегментации: \n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-07.png\" width=\"700\">\n",
    "\n",
    "\n",
    "- сглаживание шума;\n",
    "- снижение размерности -> вектор признак\n",
    "- генерация данных\n",
    "\n",
    "\n",
    "Об этом будет целая отдельная лекция чуть позже\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разжимающий (upsample) блок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение размеров изображений \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-08.png\" width=\"700\">\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/bilinear.png\" width=\"300\">\n",
    "\n",
    "Билинейная интерполяция рассматривает квадрат 2x2 известных пикселя, окружающих неизвестный. В качестве интерполированного значения используется взвешенное усреднение этих четырёх пикселей.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/bicubic.png\" width=\"300\">\n",
    "\n",
    "\n",
    "Бикубическая интерполяция идёт на один шаг дальше билинейной, рассматривая массив из 4x4 окружающих пикселей — всего 16. Поскольку они находятся на разных расстояниях от неизвестного пикселя, ближайшие пиксели получают при расчёте больший вес.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample в Pytorch\n",
    "С картами признаков можно обращаться так же как и с пикселями. Для этого в Pytorch используется метод Upsample\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def upsample(pil, mode=\"nearest\", ax=ax):\n",
    "    tensor = TF.to_tensor(pil)\n",
    "    upsampler = nn.Upsample(scale_factor=2, mode=mode)\n",
    "    tensor_128 = upsampler(tensor.unsqueeze(0))\n",
    "    im_128 = TF.to_pil_image(tensor_128.squeeze()).convert(\"RGB\")\n",
    "    ax.imshow(im_128)\n",
    "    ax.set_title(mode)\n",
    "    ax.set_xlim(0, 20 * 2)\n",
    "    ax.set_ylim(20 * 2, 0)\n",
    "\n",
    "\n",
    "pic = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "pil_64 = pic.resize((64, 64))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(15, 5))\n",
    "ax[0].imshow(pil_64)\n",
    "ax[0].set_title(\"Resized image\")\n",
    "ax[0].set_xlim(0, 20)\n",
    "ax[0].set_ylim(20, 0)\n",
    "\n",
    "\n",
    "upsample(pil_64, mode=\"nearest\", ax=ax[1])\n",
    "upsample(pil_64, mode=\"bilinear\", ax=ax[2])\n",
    "upsample(pil_64, mode=\"bicubic\", ax=ax[3])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание что размер изображения увеличился в 2 раза!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxUnpooling\n",
    "\n",
    "Разница с предыдущим методом в том что индексы элементов запоминаются.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-019.png\" width=\"700\">\n",
    "\n",
    "Сохраняем индексы каждого max pooling слоя.\n",
    "\n",
    "При повышении разрешения копируем значения из выхода max pooling слоя с учетом запомненных индексов\n",
    "\n",
    "[Документация к MaxPool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "[Документация к MaxUnpool2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html?highlight=unpooling)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-14.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def tensor_show(tensor, title=\"\", ax=ax):\n",
    "    im = TF.to_pil_image(tensor.squeeze()).convert(\"RGB\")\n",
    "    ax.set_title(title + str(im.size))\n",
    "    ax.imshow(im)\n",
    "\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=2, return_indices=True)  # False by default\n",
    "unpool = nn.MaxUnpool2d(kernel_size=2)\n",
    "\n",
    "pil = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, figsize=(20, 5), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].set_title(\"original \" + str(pil.size))\n",
    "ax[0].imshow(pil)\n",
    "tensor = TF.to_tensor(pil).unsqueeze(0)\n",
    "print(\"Initial shape\", tensor.shape)\n",
    "\n",
    "# Downsample\n",
    "tensor_half_res, indexes1 = pool(tensor)\n",
    "print(\"Indexes shape\", indexes1.shape)\n",
    "tensor_show(tensor_half_res, \"1/2 down\", ax=ax[1])\n",
    "\n",
    "\n",
    "tensor_q_res, indexes2 = pool(tensor_half_res)\n",
    "tensor_show(tensor_q_res, \"1/4 down\", ax=ax[2])\n",
    "\n",
    "# Upsample\n",
    "tensor_half_res1 = unpool(tensor_q_res, indexes2)\n",
    "tensor_show(tensor_half_res1, \"1/2 up\", ax=ax[3])\n",
    "\n",
    "print(indexes1.shape)\n",
    "tensor_recovered = unpool(tensor_half_res1, indexes1)\n",
    "tensor_show(tensor_recovered, \"full size up\", ax=ax[4])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем нужен pad?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "array = torch.ones((24, 24), dtype=int)\n",
    "sns.heatmap(array, annot=True, fmt=\"d\", ax=ax[0], cbar=False, vmin=0, vmax=1)\n",
    "print(\"Размеры массива:\", array.size())\n",
    "\n",
    "array_padded = F.pad(array, pad=[4, 4])\n",
    "sns.heatmap(array_padded, annot=True, fmt=\"d\", ax=ax[1], cbar=False)\n",
    "print(\"Размеры массива с padding:\", array.size())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose convolution\n",
    "\n",
    "Способы восстановления пространственных размерностей которые мы рассмотрели, не содержали обучаемых параметров.\n",
    "\n",
    "\n",
    "Обычная свертка:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-16.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Upsample/transpose convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-17.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-15.png\" width=\"700\">\n",
    "\n",
    "[Блог-пост про 2d свертки с помощью пеермножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)\n",
    "\n",
    "[Документация к ConvTranspose2d](\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html?highlight=transpose#convtranspose2d)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-19.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "\n",
    "# exact output size can be also specified as an argument\n",
    "input = torch.randn(1, 16, 16, 16)\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "# ...\n",
    "with torch.no_grad():\n",
    "    upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "    h = downsample(input)\n",
    "    print(\"Downsampled size\", h.size())\n",
    "\n",
    "    output = upsample(h, output_size=input.size())\n",
    "    print(\"Upsampled size\", output.size())\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "sns.heatmap(input[0, 0, :, :], ax=ax[0], cbar=False, vmin=-2, vmax=2)\n",
    "ax[0].set_title(\"Input\")\n",
    "sns.heatmap(h[0, 0, :, :], ax=ax[1], cbar=False, vmin=-2, vmax=2)\n",
    "ax[1].set_title(\"Downsampled\")\n",
    "sns.heatmap(output[0, 0, :, :], ax=ax[2], cbar=False, vmin=-2, vmax=2)\n",
    "ax[2].set_title(\"Upsampled\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "\n",
    "Популярная архитектура для сегментации. Изначально была предложена ([оригинальная статья](https://arxiv.org/abs/1505.04597)) для анализа  медицинских изображений.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-20.png\" width=\"700\">\n",
    "\n",
    "[Реализация на PyTorch](https://github.com/milesial/Pytorch-UNet)\n",
    "\n",
    "[U-Net на PyTorch Hub](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/)\n",
    "\n",
    "[Статья-разбор](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Обратите внимание на серые стрелки на схеме ....\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-21.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При конкатенации пространственные размеры должны совпадать.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-22.png\" width=\"700\">\n",
    "\n",
    "После upsample блоков ReLU не используется.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Несколько классов\n",
    "\n",
    "Если требуется классифицировать несколько класов объектов, то выходной слой нужно изменить соответствующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем несколько архитектур для мультиклассовой сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###FCN \n",
    "\n",
    "Fully Convolutional Network\n",
    "для токо что бы не было путаницы с Fully Connected Network\n",
    "последние именуют MLP (Multi Layer Perceptron)\n",
    "\n",
    "[Пример реализации 1](https://pytorch.org/hub/pytorch_vision_fcn_resnet101/)\n",
    "\n",
    "[Пример реализации 2](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
    "\n",
    "Предобученная модель была обучена на части датасета COCO train2017 (на 20 категориях, представленых так же в датасете  Pascal VOC). Использовались следубщие классы:\n",
    "\n",
    "`['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classes = [\n",
    "    \"__background__\",\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]\n",
    "\n",
    "fcn_model = torchvision.models.segmentation.fcn_resnet50(\n",
    "    pretrained=True, num_classes=21\n",
    ")\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # ImageNet\n",
    "    ]\n",
    ")\n",
    "\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "input_tensor = preprocess(pil_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = fcn_model(input_tensor.unsqueeze(0))  # ['out'][0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращаются 2 массива\n",
    "\n",
    "* out - at each location, there are unnormalized probabilities corresponding to the prediction of each class\n",
    "\n",
    "* aux - contains the auxillary loss values per-pixel. In inference mode, output['aux'] is not usefulcontains the auxillary loss values per-pixel. In inference mode, output['aux'] is not useful"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(output.keys())  # Ordered dictionary\n",
    "print(\"out\", output[\"out\"].shape, \"Batch, class_num, h, w\")\n",
    "print(\"aux\", output[\"aux\"].shape, \"Batch, class_num, h, w\")\n",
    "# aux and output['aux'] contains the auxillary loss values per-pixel. In inference mode, output['aux'] is not usefulcontains the auxillary loss values per-pixel. In inference mode, output['aux'] is not useful\n",
    "\n",
    "# at each location, there are unnormalized probabilities corresponding to the prediction of each class\n",
    "output_predictions = output[\"out\"][0].argmax(0)  # for first element of batch\n",
    "print(output_predictions.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "indexes = output_predictions\n",
    "fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\n",
    "\n",
    "i = 0\n",
    "for row in range(4):\n",
    "    for col in range(5):\n",
    "        mask = torch.zeros(indexes.shape)\n",
    "        mask[indexes == i] = 255\n",
    "        # if mask.max() > 0:\n",
    "        ax[row, col].set_title(classes[i])\n",
    "        ax[row, col].imshow(mask)\n",
    "        i += 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepLab\n",
    "\n",
    "\n",
    "[Реализация на PyTorch](https://pytorch.org/vision/stable/models.html#deeplabv3)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-24.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial pyramid pooling (SPP) layer\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-23.png\" width=\"700\">\n",
    "\n",
    "В pytorch нет отдельного модуля, но результат может быть получени применением нескольких AdaptiveMaxPool2d с разными размерами выходов ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atros (Dilated) Convolution\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-25.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-26.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Atros example\n",
    "with torch.no_grad():\n",
    "    input = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "    print(\"Input shape:\", input.size())\n",
    "\n",
    "    conv = nn.Conv2d(1, 1, kernel_size=2, dilation=1, bias=False)\n",
    "\n",
    "    conv.weight = nn.Parameter(torch.tensor([[[[2, 2], [2, 2]]]], dtype=torch.float))\n",
    "\n",
    "    out = conv(input.unsqueeze(0))\n",
    "    print(\"Output:\", out)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    out[0, 0, :, :],\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Kernel\")\n",
    "ax[2].set_title(\"Output\")\n",
    "fig.suptitle(\"Dilation = 1\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "conv = nn.Conv2d(\n",
    "    1, 1, kernel_size=2, dilation=2, bias=False\n",
    ")  # Fell free to change dilation\n",
    "\n",
    "conv.weight = nn.Parameter(torch.tensor([[[[2, 2], [2, 2]]]], dtype=torch.float))\n",
    "\n",
    "out = conv(input.unsqueeze(0))\n",
    "print(out)\n",
    "print(out.shape)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    out[0, 0, :, :].detach(),\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Kernel\")\n",
    "ax[2].set_title(\"Output\")\n",
    "fig.suptitle(\"Dilation = 2\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input = torch.tensor([[[0, 1, 0], [1, 1, 1], [0, 1, 0]]], dtype=torch.float)\n",
    "out = conv(input.unsqueeze(0))\n",
    "print(out)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5), sharex=False, sharey=False)\n",
    "sns.heatmap(\n",
    "    input[0], ax=ax[0], annot=True, fmt=\".0f\", cbar=False, vmin=0, vmax=8, linewidths=1\n",
    ")\n",
    "sns.heatmap(\n",
    "    conv.weight.detach()[0, 0, :, :],\n",
    "    ax=ax[1],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "sns.heatmap(\n",
    "    out[0, 0, :, :].detach(),\n",
    "    ax=ax[2],\n",
    "    annot=True,\n",
    "    fmt=\".0f\",\n",
    "    cbar=False,\n",
    "    vmin=0,\n",
    "    vmax=8,\n",
    "    linewidths=1,\n",
    ")\n",
    "\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"Kernel\")\n",
    "ax[2].set_title(\"Output\")\n",
    "fig.suptitle(\"Dilation = 2\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-27.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные данные\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-28.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU - ценка точности\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-52.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel-wise cross entropy loss\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-29.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "\n",
    "one_class_out = torch.randn(1, 1, 32, 32)\n",
    "one_class_target = torch.randn(1, 1, 32, 32)\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "loss = bce_loss(one_class_out, one_class_target)\n",
    "print(\"BCE\", loss)\n",
    "\n",
    "\n",
    "two_class_out = torch.randn(1, 2, 32, 32)\n",
    "two_class_target = torch.randint(1, (1, 32, 32))\n",
    "\n",
    "print(two_class_out.shape)\n",
    "print(two_class_target.shape)\n",
    "\n",
    "loss = cross_entropy(two_class_out, two_class_target)\n",
    "\n",
    "print(\"Cross entropy loss\", loss)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiceLoss\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/dice.jpeg\" width=\"700\">\n",
    "\n",
    "[Блог-пост про семантическую сегментацию](https://www.jeremyjordan.me/semantic-segmentation/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    \"\"\"Soft Dice loss of binary class\n",
    "    Args:\n",
    "        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n",
    "        predict: A tensor of shape [N, *]\n",
    "        target: A tensor of shape same with predict\n",
    "       Returns:\n",
    "        Loss tensor\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=2, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = p  # pow degree\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        predict = predict.flatten(1)\n",
    "        target = target.flatten(1)\n",
    "\n",
    "        # https://pytorch.org/docs/stable/generated/torch.mul.html\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.epsilon\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.epsilon\n",
    "        loss = 1 - 2 * num / den\n",
    "\n",
    "        return loss.mean()  # over batch\n",
    "\n",
    "\n",
    "criterion = BinaryDiceLoss()\n",
    "output = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "\n",
    "target = torch.tensor([[[1, 1, 1], [1, 1, 1], [1, 1, 1]]], dtype=torch.float)\n",
    "\n",
    "soft_loss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n",
    "print(\"Loss\", soft_loss)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детекстирование (Object detection)\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
    "\n",
    "Постановка задачи:\n",
    "\n",
    "- опредлить координаты прямоугольника в котором заключен объект."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример из задания для семинара:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-05.png\" width=\"700\">\n",
    "\n",
    "Задачу семантической сегментации мы решали через классификацию. Для детектирования разумно использовать регрессию.\n",
    "\n",
    "В случае для одного объекта можно обучить модель предсказывать числа:\n",
    "\n",
    "* координаты центра + ширину и высоту\n",
    "* координаты правого верхнего и левого нижнего углов\n",
    "* координаты вершин полигона ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Возьмем обученную сеть\n",
    "resnet_detector = resnet18(pretrained=True)\n",
    "\n",
    "# Заменим голову на предсказание 4х точек (x1,y1 x2,y2)\n",
    "resnet_detector.fc = nn.Linear(resnet_detector.fc.in_features, 4)  # x1,y1 x2,y2\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Это случайный пример. Не ожидайте результатов\n",
    "input = torch.rand((1, 3, 224, 224))\n",
    "target = torch.tensor([[0.1, 0.1, 0.5, 0.5]])  # x1,y1 x2,y2 or x,y w,h\n",
    "output = resnet_detector(input)\n",
    "loss = criterion(output, target)\n",
    "print(output)\n",
    "print(\"Loss:\", loss)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-07.png\" width=\"700\">\n",
    "\n",
    "\n",
    "На прошлом семинаре мы упоминали про модели которые ищут ключевые точки на лице человека (MTCNN). Можно использовать тот же подход для поиска любых точек.\n",
    "\n",
    "\n",
    "Начнем с ситуации когда объект один.\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-08.png\" width=\"500\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть предобученный классификатор.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-09.png\" width=\"700\">\n",
    "\n",
    "и мы хотим научить его определять местоположение объекта.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-10.png\" width=\"700\">\n",
    "\n",
    "Для этого можно использовать две лосс функции: одна - будет оценивать ошибку классификации, другая - локализации.\n",
    "\n",
    "Результаты потребуется каким-то образом объединить. В простейшем случае - сложить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Regression loss\n",
    "\n",
    "$\\mathrm{MSE} = \\frac{\\sum^n_{i=1}(y_i-y_i^p)^2}{n}$ - L2/ MSE/ Mean Square Error/ Среднеквадратичная ошибка \n",
    "\n",
    "\n",
    "$\\mathrm{MAE} = \\frac{\\sum^n_{i=1}|y_i-y_i^p|}{n}$ - L1/ MAE/ Mean Absolute Error/ Средняя ошибка\n",
    "\n",
    "\n",
    "$\\\n",
    "    L_{\\delta}(y, f(x))=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  \\frac{1}{2}(y-f(x))^2 \\qquad \\mathrm{for } |y-f(x)| \\leq \\delta\\\\\n",
    "                  \\delta|y-f(x)|-\\frac{1}{2}\\delta^2 \\qquad \\mathrm{otherwise}\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "  $ - Huber Loss/ Smooth Mean ABsolute Error/ Функция потерь Хьюбера\n",
    "\n",
    "\n",
    "$L(y,y^p)=\\sum_{i=1}^n log(cosh(y^p_i-y_i))$ - Log-Cosh Loss/ Логарифм гиперболического косинуса\n",
    "\n",
    "\n",
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-12.png\" width=\"700\">\n",
    "\n",
    "MAE vs MSE\n",
    "\n",
    " <img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-13.png\" width=\"700\">\n",
    "\n",
    "Huber vs Log-cos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask loss\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-14.png\" width=\"500\">\n",
    "\n",
    "\n",
    "[Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics](https://arxiv.org/pdf/1705.07115.pdf)\n",
    "\n",
    "[Пример реализации MultiTask learning](https://github.com/Hui-Li/multi-task-learning-example-PyTorch/blob/master/multi-task-learning-example-PyTorch.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детектирование нескольких объектов\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-042.gif\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a) Наивный способ решения: скользящее окно**\n",
    "\n",
    "Перебрать все возможные местопоположения объектов и классифицировать фрагменты.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-044.gif\" width=\"700\">\n",
    "\n",
    "- очень долго\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b) Эвристика**\n",
    "\n",
    " Выбрать области в которых вероятность нахождения объекта наиболее высока (ROI = regions of interest) и проводить поиск только в них.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-049.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective search\n",
    "\n",
    "Один из таких алгоритмов:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-18.png\" width=\"1000\">\n",
    "\n",
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "Возвращает порядка 2000 прямоугольников для изображения.\n",
    "\n",
    "С таким количеством уже можно работать ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-CNN - Region CNN \n",
    "Построенна по такому принципу:\n",
    "\n",
    "- на изображении ищутся ROI \n",
    "- для кажого делается resize \n",
    "- каждый ROI обрабатывается сверточной сетью\n",
    "которая предсказывает класс кобъекта который в него попал\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-055.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме класса модель предсказывает смещения для каждого bounding box\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMS\n",
    "\n",
    "Теперь возникает другая проблема в районе объекта алгоритм генерирует множество ограничивающих прямоугольников (bounding box) которые частично прекрывают друг друга.\n",
    "\n",
    "Что бы избавиться от них используется другой алгоритм\n",
    "Non maxima suppression\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-22.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Его задача избавиться об bbox которые накладиваются на истинный\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-23.png\" width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки схожести обычно используется метрика IoU\n",
    "а заначение IoU при котором bbox считаются принадлежащими одному объекту является гиперпараметром (часто 0.5)\n",
    "\n",
    "Soft NMS\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-24.png\" width=\"700\">\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-25.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast R-CNN\n",
    "\n",
    "Проблеммой описанного выше подхода является скорость.\n",
    "Так как мы вынужденны применять CNN порядка 2000 раз (в зависимости от эвристики которая генерирует ROI)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-26.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И решением является поиск ROI не на самом изображении, а на карте признаков, полученной после обработки всего изображения CNN. В таком случае большая часть сверток выполняется только один раз.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-27.png\" width=\"700\">\n",
    "\n",
    "Это радикально ускоряет процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Pooling\n",
    "\n",
    "Появляется новая задача - 'resize' ROI на карте признаков.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-28.png\" width=\"700\">\n",
    "\n",
    "[Документация Roi Pooling](https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_pool)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-29.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-077.png\" width=\"700\">\n",
    "\n",
    "Скорость работы CNN снизилась и теперь узким местом становится эвристика для поиска ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster R-CNN\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-078.png\" width=\"700\">\n",
    "\n",
    "Идеz: пусть сеть сама предсказывает ROI по карте признаков\n",
    "\n",
    "Для обучения требуется посчитать 4 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region proposal network\n",
    "\n",
    "Карта признаков имеет фиксированные и относительно небольшие пространственные размеры (например 20x15)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-34.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Поэтому можно вернуться к идее скользящего окна которая была отвергнута в самом начале.\n",
    "\n",
    "При этом можно использовать окна нескольких форм\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-35.png\" width=\"700\">\n",
    "\n",
    "Предсказываются два значения:\n",
    "\n",
    "* вероятность того что в ROI находится объект\n",
    "* смещения\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама сеть при этом может быть очень простой:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/rpn.jpeg\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате скорость увеличивается почте в 10 раз\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-086.png\" width=\"700\">\n",
    "\n",
    "[Модель на PyTorch](https://pytorch.org/vision/stable/models.html#faster-r-cnn)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torchvision\n",
    "\n",
    "fr_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    pretrained=True, progress=True, num_classes=91, pretrained_backbone=True\n",
    ")\n",
    "fr_rcnn.eval()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"])\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(\n",
    "    imgIds[12]\n",
    ")  # http://images.cocodataset.org/val2017/000000370208.jpg\n",
    "img = img_list[0]\n",
    "print(\"Image data\", img)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from PIL import ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    tensor = TF.pil_to_tensor(pil_img) / 255\n",
    "    output = fr_rcnn(tensor.unsqueeze(0))\n",
    "    #   print(output[0].keys())\n",
    "    #   print(output[0]['boxes'])\n",
    "    #   print(output[0]['labels'])\n",
    "    #   print(output[0]['scores'])\n",
    "\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "    for i, bbox in enumerate(output[0][\"boxes\"]):\n",
    "        if output[0][\"scores\"][i] > 0.5:\n",
    "            draw.rectangle((tuple(bbox[:2].numpy()), tuple(bbox[2:].numpy())), width=2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two stage detector\n",
    "\n",
    "Faster RCNN == Two stage detector\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/\n",
    "L12_Segmentation_Detection/img/lecture_12-089.png\" width=\"700\">\n",
    "\n",
    "На среднем и верхнем слое выполняются очень похожие операции. Разница в том что на последнем слое предсказывается класс объекта, а промежуточном только вероятность его присутствия (objectness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Stage detector\n",
    "\n",
    "Если сразу предсказывать класс, то можно избавиться от второй стадии.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-1-1.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Детекторы работающие \"за один проход\":\n",
    "\n",
    "YOLO, SSD, RetinaNet\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-1-2.png\" width=\"700\">\n",
    "\n",
    "[Сравнение скорости моделей](https://pytorch.org/vision/stable/models.html#runtime-characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/ssd.png\" width=\"700\">\n",
    "\n",
    "* модель VGG-16, предобученная на ImageNet\n",
    "* manually defines a collection of aspect ratios to use for the B bounding boxes at each grid cell + offsets (x,y,w,h)\n",
    "* напрямую предсказывает вероятность того, что класс присутствует в bounding box.\n",
    "* есть класс для \"background\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retina Net - [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FocalLoss\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-43.png\" width=\"1000\">\n",
    "\n",
    "[Блог-пост: Что такое Focal Loss и когда его использовать](https://amaarora.github.io/2020/06/29/FocalLoss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Feature pyramyd network\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)\n",
    "\n",
    "Это feature extractor для детекторов.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-21.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же как и в случае с сегментацией, точность повышается если делать предсказания на картах признаков разных масштабов.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn1.jpeg\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но первые слои содержат мало семантической информации (только низкоуровневые признаки). Из за этого детектирование(и сегментация) мелких объектов удается хуже.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn2.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "идея состоит в том что бы делать предсказание с учетом семантической информации полученной на более глубоких слоях. \n",
    "\n",
    "\n",
    "При этом признаки суммируются.\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn3.png\" width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем к новым картам признаков может применяться дополнительная свертка.\n",
    "\n",
    "\n",
    "На выходе получаем карты признаков P2 - P5 на которых уже предсказываются bounding box.\n",
    "\n",
    "\n",
    "В случае 2-stage детектора (RCNN) карты подаются на вход RPN \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn5.jpeg\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А признаки для предсказаний используются из backbone \n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/fpn6.jpeg\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "RetinaNet использует выходы FPN и для предсказаний класса и bbox. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-42.png\" width=\"700\">\n",
    "\n",
    "\n",
    "[Блог-пост про FPN](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###YOLO\n",
    "\n",
    "* [2016 You Only Look Once: Unified, Real-Time Object Detection.](https://arxiv.org/pdf/1506.02640.pdf) \n",
    "* [2017 YOLO9000: Better, Faster, Stronger.](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "* [2018 YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767.pdf)\n",
    "\n",
    "Joseph Redmon\n",
    "\n",
    "* April 2020, Alexey Bochkovskiy  “[YOLOv4: Optimal Speed and Accuracy of Object Detection\"](https://arxiv.org/abs/2004.10934)\n",
    "* 9 June 2020 [YOLOv5 Glenn Jocher](https://github.com/ultralytics/yolov5)\n",
    "\n",
    "Первые версии проигрывали конкурентам. Но проект развивался. В настоящий момент это пожалуй оптимальный детектор по соотношению качество разпознавания/скорость.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv3\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/yolov3.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv4\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/yolov4.jpeg\" width=\"700\">\n",
    "- SPP block\n",
    "- Dense Block\n",
    "- Больше слоев\n",
    "- online Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv5\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-47.png\" width=\"700\">\n",
    "\n",
    "Статья не публиковалась.\n",
    "Точность сравнима  с v4 но модель определенно лучше упакованна."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "catIds = coco.getCatIds(catNms=[\"person\", \"bicycle\"])\n",
    "# person and bicycle\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "img_list = coco.loadImgs(imgIds[5])\n",
    "img = img_list[0]\n",
    "print(\"Image data\", img)\n",
    "\n",
    "pil_img = coco2pil(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pil_img)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns, draw_bbox=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка модели с Torch Hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "# Load model from torch\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из коробки работает с изображениями в разных форматах и даже url, автоматически меняет размер входного изображения, возвращает объект с результатами ..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Apply yolov5 model\n",
    "results = model(pil_img)\n",
    "results.print()\n",
    "results.save()  # image on disk\n",
    "\n",
    "print(type(results.xyxy), len(results.xyxy), results.xyxy[0].shape)\n",
    "results.pandas().xyxy[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# annIds = coco.getAnnIds(imgIds=[448263])\n",
    "# anns = coco.loadAnns(annIds)\n",
    "\n",
    "cv_img = np.array(pil_img)\n",
    "RGB_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "annos = results.pandas().xyxy[0]\n",
    "\n",
    "for i in range(len(annos)):\n",
    "    x_min, y_min, x_max, y_max = (\n",
    "        int(annos[\"xmin\"].iloc[i]),\n",
    "        int(annos[\"ymin\"].iloc[i]),\n",
    "        int(annos[\"xmax\"].iloc[i]),\n",
    "        int(annos[\"ymax\"].iloc[i]),\n",
    "    )\n",
    "    # width, height = int(annos['xmax']-annos['xmin']), int(annos['ymax']-annos['ymin'])\n",
    "    if annos[\"name\"].iloc[i] == \"person\":\n",
    "        color = (255, 255, 255)\n",
    "    if annos[\"name\"].iloc[i] == \"bicycle\":\n",
    "        color = (0, 0, 255)\n",
    "    if annos[\"name\"].iloc[i] == \"backpack\":\n",
    "        color = (0, 255, 0)\n",
    "    RGB_img = cv2.rectangle(RGB_img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "cv2_imshow(RGB_img)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако если подать на вход модели тензор, выходы радикально меняются. YOLO переходит в режим обучения, что довольно не очевидно"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dummy_input = torch.rand((1, 3, 416, 416))\n",
    "results = model(dummy_input)\n",
    "print(type(results), len(results))\n",
    "print(results[0].shape)\n",
    "print(type(results[1]), len(results[1]))\n",
    "for e in results[1]:\n",
    "    print(e.shape)\n",
    "# print(results[0], results[1].shape) # list of two elements"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нard Example Mining\n",
    "\n",
    "Представим что камера видеонаблюдения установленна на улице.\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-48.png\" width=\"700\">\n",
    "\n",
    "Целевые объекты, могут появляться достаточно редко (особено ночью)\n",
    "Но на каждом из кадров будет фон, который будет сильно меняться в зависимости от освещения погодных условий и.т.п.\n",
    "\n",
    "В результате возникнут изображения с образами которых не было в датасетах и они приведут к ложным срабатываниям.\n",
    "\n",
    "Что бы дообучить сеть можно сохранять кадры с такими срабатываниями и добавлять их в датасет.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online hard example mining\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-49.png\" width=\"700\">\n",
    "\n",
    "Можно делать это непосредственно при обучении во время формирования batch-а\n",
    "\n",
    "[Блог пост про Hard Mining Example](https://erogol.com/online-hard-example-mining-pytorch/)\n",
    "\n",
    "Или даже заложить в структуру модели:\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/pdf/1604.03540.pdf)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors](https://arxiv.org/pdf/1804.04606.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка точности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-51.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-52.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "РАЗБИРАЕМ КАК СЧИТАЕТСЯ mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/map_data.jpeg\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/gan/map.jpeg\" width=\"1000\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-54.png\" width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-56.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-57.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Конвертация результатов сегментации в COCO формат](https://www.javaer101.com/en/article/18652684.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синхронизируем метки Pascal2Coco"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pascal2coco = {}\n",
    "print(classes)\n",
    "\n",
    "\n",
    "def find_in_dic(dic, val):\n",
    "    for key in dic.keys():\n",
    "        if dic.get(key, None) == val:\n",
    "            return key\n",
    "    return 0  # Assign missed classes to bg\n",
    "\n",
    "\n",
    "print(num2cat)\n",
    "for i in range(1, len(classes)):  # Skip BG\n",
    "    # if cats.get()\n",
    "    coco_ind = find_in_dic(num2cat, classes[i])\n",
    "    pascal2coco[i] = coco_ind\n",
    "\n",
    "print(pascal2coco)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "img = coco.loadImgs(448263)[0]\n",
    "print(img)\n",
    "annIds = coco.getAnnIds(imgIds=[448263])\n",
    "anns = coco.loadAnns(annIds)\n",
    "I = io.imread(img[\"coco_url\"])\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(I)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pycocotools import mask\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import json\n",
    "\n",
    "\n",
    "def binary_mask_to_rle(binary_mask):\n",
    "    rle = {\"counts\": [], \"size\": list(binary_mask.shape)}\n",
    "    counts = rle.get(\"counts\")\n",
    "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order=\"F\"))):\n",
    "        if i == 0 and value == 1:\n",
    "            counts.append(0)\n",
    "        counts.append(len(list(elements)))\n",
    "    return rle\n",
    "\n",
    "\n",
    "detection_res = []\n",
    "for i, cls_name in enumerate(classes):\n",
    "    binary_mask = torch.zeros(indexes.shape)\n",
    "    binary_mask[indexes == i] = 1  # Form FCN for baseball\n",
    "\n",
    "    if i > 0 and torch.max(binary_mask) > 0:\n",
    "        uncompressed_rle = binary_mask_to_rle(binary_mask.numpy())  # encoded_gt,\n",
    "        fortran_gt_binary_mask = np.asfortranarray(binary_mask).astype(\"uint8\")\n",
    "        encoded_gt = mask.encode(fortran_gt_binary_mask)\n",
    "        bbox = list(mask.toBbox(encoded_gt))\n",
    "        print(bbox)\n",
    "\n",
    "        detection_res.append(\n",
    "            {\n",
    "                \"score\": 1.0,  # dummy\n",
    "                \"category_id\": pascal2coco[i],\n",
    "                \"segmentation\": uncompressed_rle,\n",
    "                \"bbox\": bbox,\n",
    "                \"image_id\": 448263,\n",
    "                \"iscrowd\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(detection_res)\n",
    "with open(\"seg_res.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(detection_res, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pil_img = coco2pil(\"http://images.cocodataset.org/val2017/000000448263.jpg\")\n",
    "plt.imshow(pil_img)\n",
    "coco.showAnns(anns, draw_bbox=True)\n",
    "plt.title(\"Annotation from COCO\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(pil_img)\n",
    "coco.showAnns(detection_res, draw_bbox=True)\n",
    "plt.title(\"Detection\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В предыдущем шаге мы посчитали предсказаниее от YOLO. Давайте оценим его точность"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "with open(\"seg_gt.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(anns, f, ensure_ascii=False, indent=4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# loadRes will generate a new COCO type instance based on coco_gt and return\n",
    "coco_dt = coco.loadRes(\"seg_res.json\")\n",
    "coco_gt = coco.loadRes(\"seg_gt.json\")\n",
    "\n",
    "cocoEval = COCOeval(coco_gt, coco_dt, \"bbox\")  # 'segm', 'bbox'\n",
    "# cocoEval.params.useSegm = True\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()\n",
    "\n",
    "print(cocoEval.stats)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-005.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L8-03.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[COCO panoptic](https://cocodataset.org/#panoptic-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/lecture_12-096.png\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI Align\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-30.png\" width=\"700\">\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L12_Segmentation_Detection/img/L9-31.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Модель Mask R-CNN](https://pytorch.org/vision/stable/models.html#mask-r-cnn)\n",
    "\n",
    "[Пример запуска Mask R-CNN есть в документации Pytorch](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Список использованной литературы\n",
    "[Подробнее о том как создать свой COCO датасет с нуля](https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch).\n",
    "\n",
    "[Видео-разбор Run-Length Encoding](https://www.youtube.com/watch?v=h6s61a_pqfM)\n",
    "\n",
    "[Блог-пост про 2d свертки с помощью пеермножения матриц](https://medium.com/@_init_/an-illustrated-explanation-of-performing-2d-convolutions-using-matrix-multiplications-1e8de8cd2544)\n",
    "\n",
    "[Статья U-Net](https://arxiv.org/abs/1505.04597)\n",
    "[Блог-пост про U-Net](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5)\n",
    "\n",
    "[Блог-пост про семантическую сегментацию](https://www.jeremyjordan.me/semantic-segmentation/)\n",
    "\n",
    "[Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics](https://arxiv.org/pdf/1705.07115.pdf)\n",
    "\n",
    "[Статья про Selective Search](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)\n",
    "\n",
    "[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)\n",
    "\n",
    "[Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
    "\n",
    "[Блог-пост: Что такое Focal Loss и когда его использовать](https://amaarora.github.io/2020/06/29/FocalLoss.html)\n",
    "\n",
    "[Feature Pyramid Networks for Object Detection](https://arxiv.org/pdf/1612.03144.pdf)\n",
    "\n",
    "[2016 You Only Look Once: Unified, Real-Time Object Detection.](https://arxiv.org/pdf/1506.02640.pdf) \n",
    "\n",
    "[2017 YOLO9000: Better, Faster, Stronger.](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "\n",
    "[2018 YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767.pdf)\n",
    "\n",
    "[YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/abs/2004.10934)\n",
    "\n",
    "[YOLOv5 Glenn Jocher](https://github.com/ultralytics/yolov5)\n",
    "\n",
    "[Блог пост про Hard Mining Example](https://erogol.com/online-hard-example-mining-pytorch/)\n",
    "\n",
    "[Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/pdf/1604.03540.pdf)\n",
    "\n",
    "[Loss Rank Mining: A General Hard ExampleMining Method for Real-time Detectors](https://arxiv.org/pdf/1804.04606.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {}
}
