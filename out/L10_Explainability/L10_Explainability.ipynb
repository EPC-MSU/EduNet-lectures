{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIBOXXW6pQTj"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oioQ_vjDVWR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD8JJHpEpQTp"
   },
   "source": [
    "# 1 Общая идея"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtoYDw-epQTp"
   },
   "source": [
    "## 1.1 Why Should I Trust You?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgAy1UEBpQTp"
   },
   "source": [
    "Машинное обучение лежит в основе многих последних достижений в области науки и технологии.\n",
    "\n",
    "Когда компьютеры победили профессионалов в таких играх, как Go, многие люди начали спрашивать: могут ли машины стать лучшими водителями или даже лучшими врачами?\n",
    "\n",
    "Во многих приложениях машинного обучения пользователей просят доверять модели, которая поможет им принимать решения. Врач определенно не станет оперировать пациента просто потому, что «так сказала модель».\n",
    "\n",
    "Даже в ситуациях с более низкими рисками принять неправильное решение на основании предсказания модели — например, при выборе фильма для просмотра в Netflix, требуется определенная степень доверия.\n",
    "\n",
    "Несмотря на то, что многие модели машинного обучения представляют собой черный ящик, понимание причин, лежащих в основе прогнозов модели, безусловно, поможет пользователям решить, когда им доверять, а когда нет.\n",
    "\n",
    "К сожалению, компьютеры обычно не объясняют своих прогнозов. \n",
    "И это становится препятствием для внедрения моделей машинного обучения.\n",
    "А если пользователи не доверяют модели или прогнозу, то они будут с опаской использовать их на практике.\n",
    "\n",
    "Поэтому вопрос в том, как помочь пользователям доверять результатам, полученным с помощью моделей машинного или глубокого обучения.\n",
    "\n",
    "Важно различать два разных (но связанных) определения доверия: \n",
    "1. Доверие к предсказанию, то есть доверяет ли пользователь отдельному предсказанию настолько, чтобы предпринять какие-то действия на его основе?\n",
    "2. Доверие к модели, то есть доверяет ли пользователь модели так, чтобы применять ее на практике?\n",
    "\n",
    "Оба напрямую подвержены влиянию того, насколько человек понимает поведение модели, чтобы не рассматривать ее как черный ящик. \n",
    "\n",
    "Определение доверия к индивидуальным прогнозам является важной задачей, когда модель используется для принятия решений. Ведь, например, при медицинской диагностике или обнаружении террористов предсказания не могут быть выполнены бездумно, так как последствия могут быть катастрофическими.\n",
    "\n",
    "Помимо доверия индивидуальным предсказаниям, необходимо доверять модели \"в целом\". Чтобы принять это решение, пользователи должны быть уверены, что модель будет хорошо работать на реальных данных согласно интересующим метрикам. Реальные данные часто значительно отличаются, и, кроме того, метрика оценки может не указывать на цель продукта. Изучение индивидуальных прогнозов и их объяснение — важная задача, которая может помочь пользователям, особенно для больших наборов данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCmmZPfhpQTq"
   },
   "source": [
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/impact_assessment.png\" alt=\"alttext\" width=450/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rz6v1umpQTq"
   },
   "source": [
    "Практики часто **переоценивают точность своих моделей**. Примером тому может послужить известный случай с моделью, обученной отличать хаски от волка.\n",
    "\n",
    "Часто **артефакты сбора данных могут вызвать нежелательные корреляции**, которые классификаторы улавливают во время обучения. Эти проблемы может быть очень трудно определить, просто взглянув на необработанные данные и прогнозы.\n",
    "\n",
    "Пример идеально решенной задачи, не применимой на практике — задача различить на фотографиях волков и эскимосских собак (хаски). Исследователи обучили классификатор логистической регрессии из 20 изображений, отобранных вручную так, чтобы на всех фотографиях волков на фоне был снег, а на фотографиях хаски — нет.\n",
    "\n",
    "Этот эксперимент демонстрирует полезность объяснения отдельных визуальных признаков для понимания работы классификаторов перед тем, как принять решения о доверии к ним.\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/2_bad_models_prediction.png\" alt=\"alttext\" width=400/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcucVRYlpQTq"
   },
   "source": [
    "#### Критерии, которым должны отвечать Интерпретаторы\n",
    "\n",
    "1. **Результат должен быть интерпретируемым**, т. е. обеспечивать качественное понимание взаимосвязи между входными переменными и ответом. Отметим, что интерпретируемость должна учитывать ограничения пользователя. Объяснения должны быть легкими для понимания, не обязательно соответствовать функциям, используемым моделью. Ну и конечно, понятие интерпретируемости зависит от целевой аудитории.\n",
    "\n",
    "Например, возможное интерпретируемое представление для классификации текста — это двоичный вектор, обозначающий наличие или отсутствие слова, даже если классификатор может использовать более сложные (и непонятные) функции, такие как вложения слов. Аналогичным образом для классификации изображений, интерпретируемое представление может быть двоичным вектором, указывающим «наличие» или «отсутствие» смежного участка аналогичных пикселей (суперпиксель), тогда как классификатор может представлять изображение в виде тензора с тремя цветовыми каналами на пиксель.\n",
    "\n",
    "2. Еще один важный критерий — **локальная точность**. Чтобы иметь смысл, обьяснение должно соответствовать тому, как модель ведет себя конкретно для предсказываемого случая.\n",
    "\n",
    "Отметим, что локальная точность не подразумевает глобальной точности: факторы, которые важны в глобальном масштабе, могут не иметь значения на местном контексте, и наоборот. \n",
    "В то время как глобальная точность не всегда будет означать локальную, выделяя глобально достоверные объяснения сложных моделей, которые сложно интерпретировать для конкретного примера.\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/3_1_structure.png\" alt=\"alttext\" width=750/>\n",
    "\n",
    "Описание схемы...\n",
    "\n",
    "\n",
    "Некоторые метрики, такие как accuracy, **часто может быть неподходящей метрикой для оценки модели**. Например, когда речь идет о несбалансированных датасетах.\n",
    "\n",
    "Другой пример: модель прогнозирования оттока клиентов: модель может сказать вам, что конкретный клиент с вероятностью 90% откажется от услуг, но без четкого понимания причины не ясно, что можно сделать, чтобы предотвратить отток.\n",
    "\n",
    "Самая точная модель в мире бесполезна, если она не используется для принятия решений и действий.\n",
    "\n",
    "Поэтому крайне важно сделать модель максимально прозрачной и понятной для заинтересованных сторон, чтобы ее можно было использовать и действовать соответствующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o-0m3CvpQTr"
   },
   "source": [
    "## 1.2 Пример анализа влияния данных на результаты модели (Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAc6arU7pQTr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvrIK7GzpQTr"
   },
   "source": [
    "Мы рассмотрели примеры того, как можно самостоятельно, используя различные базовые библиотеки, интерпретировать результаты моделей.\n",
    "\n",
    "Далее рассмотрим библиотеки, которые позволяют интепретировать модель более наглядно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5tEoMuPpQTr"
   },
   "source": [
    "За исключением простых линейных моделей, таких как линейная регрессия, где вы можете легко посмотреть на коэффициенты признаков, модели машинного обучения часто могут быть чем-то вроде черного ящика.\n",
    "\n",
    "Может быть очень сложно понять, почему модель предсказывает конкретный результат, или убедиться, что результат имеет интуитивный смысл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELNLqjCkpQTr"
   },
   "source": [
    "# 2 Библиотеки для реализации explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmFgD1cxpQTs"
   },
   "source": [
    "Мы рассмотрим две библиотеки: SHAP и LIME.\n",
    "\n",
    "Логика работы **SHAP (SHapley Additive exPlanations)** основана на значении Шепли.\n",
    "Значение Шепли — это среднее значение предельных вкладов по всем перестановкам.\n",
    "\n",
    "SHAP учитыват все возможные перестановки, таким образом, SHAP представляет собой единый подход, обеспечивающий глобальную и локальную согласованность и интерпретируемость.\n",
    "\n",
    "Однако его цена — время, так как алгоритму нужно вычислить все перестановки, чтобы получить результаты.\n",
    "\n",
    "Напротив, **LIME (Local Interpretable Model-agnostic Explanations)** строит дискретные линейные модели вокруг индивидуального прогноза в его локальной окрестности. LIME на самом деле является подмножеством SHAP, но не имеет тех же свойств.\n",
    "\n",
    "**Преимущество LIME — скорость**.\n",
    "Алгоритм LIME изменяет данные вокруг отдельного прогноза для построения модели, в то время как SHAP должен вычислять все перестановки глобально, чтобы получить локальную точность. Кроме того, модуль SHAP Python пока еще не имеет специально оптимизированных алгоритмов для всех типов алгоритмов (таких как KNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KavtPqdIpQTs"
   },
   "source": [
    "## 2.1 [SHAP (SHapley Additive exPlanations)](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/4_shap_diagram.png\" alt=\"alttext\" width=400/>\n",
    "\n",
    "\n",
    "Цель SHAP — объяснить предсказание экземпляра x путем вычисления вклада каждой функции в предсказание. Для этого вычисляются значения Шепли из теории игр. А значения признаков экземпляра данных действуют как игроки в коалиции.\n",
    "\n",
    "Значения Шепли говорят нам, как справедливо распределить «выплату» (= прогноз) между функциями. \n",
    "* Игрок может иметь индивидуальное значение функции, например для табличных данных. \n",
    "* Или может быть группой значений характеристик. Например, для объяснения изображения пиксели могут быть сгруппированы в суперпиксели, а прогноз распределяется между ними.\n",
    "\n",
    "SHAP определяет объяснение как:\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/l10_formula.png\" alt=\"alttext\" width=210/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYidrs5ROvcd"
   },
   "source": [
    "где g — модель объяснения\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msup>\n",
    "    <mi>z</mi>\n",
    "    <mo>&#x2032;</mo>\n",
    "  </msup>\n",
    "  <mo>&#x2208;<!-- ∈ --></mo>\n",
    "  <mo fence=\"false\" stretchy=\"false\">{</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <mn>1</mn>\n",
    "  <msup>\n",
    "    <mo fence=\"false\" stretchy=\"false\">}</mo>\n",
    "    <mi>M</mi>\n",
    "  </msup>\n",
    "</math> — вектор коалиции, M — максимальный размер коалиции и\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>&#x03D5;<!-- ϕ --></mi>\n",
    "    <mi>j</mi>\n",
    "  </msub>\n",
    "  <mo>&#x2208;<!-- ∈ --></mo>\n",
    "  <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "    <mi mathvariant=\"double-struck\">R</mi>\n",
    "  </mrow>\n",
    "</math> — атрибуция признака j, значения Шепли.\n",
    "\n",
    "Таким образом, например, изображения не представлены на уровне пикселей, а агрегируются в суперпиксели.\n",
    "\n",
    "Вы можете найти эту формулу в обьяснении [Shapley value ](https://christophm.github.io/interpretable-ml-book/shapley.html#shapley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw4P19U8Q79v"
   },
   "source": [
    "Прогноз может быть объяснен, если предположить, что каждое значение признака экземпляра является «игроком» в игре, где прогноз является выплатой.\n",
    "\n",
    "Критерий Шепли говорит нам, как справедливо распределить «выплату» между функциями.\n",
    "\n",
    "Предположим следующий сценарий:\n",
    "\n",
    "Вы обучили модель машинного обучения прогнозированию цен на квартиры. Для одной квартиры он прогнозирует 300 000 евро, и вам необходимо объяснить этот прогноз. Квартира площадью 50 м2, находится на 2 этаже, рядом парк и кошкам запрещено:\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/l10_shapley-instance.png\" alt=\"alttext\" width=500/>\n",
    "\n",
    "\n",
    "Наша цель — объяснить, как каждое из значений этих характеристик повлияло на прогноз.\n",
    "\n",
    "Ответ прост для моделей линейной регрессии: эффект каждой функции — это вес функции, умноженный на ее значение. Но это работает только из-за линейности модели.\n",
    "\n",
    "Для более сложных моделей нам понадобится другое решение.\n",
    "\n",
    "Такое решение исходит из теории кооперативных игр: значение Шепли, введенное Шепли (1953), представляет собой метод назначения выплат игрокам в зависимости от их вклада в общую выплату. Игроки сотрудничают в коалиции и получают от этого сотрудничества определенную прибыль.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFg7m1-DSKRk"
   },
   "source": [
    "Для каждой из этих коалиций мы вычисляем прогнозируемую цену квартиры и берем разницу, чтобы получить маржинальный вклад.\n",
    "\n",
    "Значение Шепли представляет собой (взвешенное) среднее значение маржинальных взносов.\n",
    "\n",
    "Мы заменяем значения признаков, которые не входят в группу, случайными значениями признаков из набора данных квартиры, чтобы получить прогноз на основе модели машинного обучения.\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/l10_shapley-coalitions.png\" alt=\"alttext\" width=450/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mok9B7DFUmG_"
   },
   "source": [
    "Посмотрим, как реализуется этот подход в NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nAaN0MspQTs"
   },
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "clear_output()\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J96eO4TRpQTt"
   },
   "source": [
    "#### Пример обьяснения перевода с английского на русский\n",
    "Рассмотрим пример интерпретации модели для предварительно обученной модели машинного перевода\n",
    "[Machine Translation Example](https://shap.readthedocs.io/en/stable/example_notebooks/text_examples/translation/Machine%20Translation%20Explanation%20Demo.html). И, раз уж мы будем для перевода использовать предобученную модель-транформер, то переведем начало статьи [How Transformers Work](https://towardsdatascience.com/transformers-141e32e69591)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7x39YuIQpQTt"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZzYBRpZpQTt"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZN5EUWcpQTu"
   },
   "outputs": [],
   "source": [
    "lang = \"en\"\n",
    "target_lang = \"ru\"\n",
    "model_name = f'Helsinki-NLP/opus-mt-{lang}-{target_lang}'\n",
    "\n",
    "# load the model and tokenizer\n",
    "# Можно попробовать перевод и разными предобученными моделями\n",
    "# Download the model and the tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)#.cuda() # если мы не используем ускоритель GPU, то .cuda() нужно убрать\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# define the input sentences we want to translate\n",
    "data = [\n",
    "    \"Transformers are a type of neural network architecture that have been gaining popularity. Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..\"\n",
    "]\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32,
     "referenced_widgets": [
      "f031d720d22342d6b3297d5c40013714",
      "213a03de4f9f41fd9e4f25869d335fa6",
      "c084706d78404217a954794a1aabec10",
      "67c323a2c6ea47f1a3516254f5580c47",
      "9a16ed584d974a709934b8c14000b795",
      "d5fc50e995e049908df2ca95742c560a",
      "282cfaf3beac45e0a0dbad73fb5aeacb",
      "6efd2e4bb47b496cad2a5e2fc90acce9"
     ]
    },
    "id": "xrrDiZFQpQTu",
    "outputId": "05df1367-7ec5-442f-d22a-c91d15ccd606"
   },
   "outputs": [],
   "source": [
    "# we build an explainer by passing the model we want to explain and\n",
    "# the tokenizer we want to use to break up the input strings\n",
    "explainer = shap.Explainer(model, tokenizer)\n",
    "\n",
    "# explainers are callable, just like models\n",
    "shap_values = explainer(data, fixed_context=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "2JaKUdTFpQTu",
    "outputId": "36fe4d43-b338-4202-99e8-eb1bbf63e964"
   },
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnwdVrKvpQTu"
   },
   "source": [
    "## 2.2 LIME\n",
    "\n",
    "Поскольку мы хотим быть независимыми от модели, то следующее, что мы можем сделать, чтобы изучить поведение базовой модели — это изменить входные данные и посмотреть, как меняются прогнозы.\n",
    "\n",
    "Это оказывается преимуществом с точки зрения интерпретируемости, потому что мы можем изменять компоненты на входе, которые имеют смысл для людей (например, слова или части изображения), даже если модель использует гораздо более сложные компоненты в качестве функций (например, word embeddings).\n",
    "\n",
    "LIME генерирует объяснение, аппроксимируя базовую модель интерпретируемой (например, линейной моделью с несколькими ненулевыми коэффициентами), изученной при возмущениях исходной модели (например, удалении слов или скрытии частей изображения).\n",
    "\n",
    "Ключевая идея, лежащая в основе LIME, заключается в том, что гораздо проще аппроксимировать модель черного ящика простой моделью локально, чем пытаться обьяснить модель целиком (глобально).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6OZU_KmpQTv"
   },
   "outputs": [],
   "source": [
    "!pip install lime\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_box4ippQTv"
   },
   "source": [
    "#### Пример обьяснения классфикации статей по религиозному принципу\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/5_christian_or_atheist_2.png\" alt=\"alttext\" width=900/>\n",
    "\n",
    "Используем датасет [fetch_20newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html): \n",
    "\n",
    "Данные [«The 20 Newsgroups»](http://qwone.com/~jason/20Newsgroups/) — это коллекция примерно из 20000 новостных документов, разделенная (приблизительно) равномерно между 20 различными категориями. Изначально она собиралась Кеном Ленгом (Ken Lang), возможно, для его работы «Newsweeder: Learning to filter netnews» («Новостной обозреватель: учимся фильтровать новости из сети»).\n",
    "\n",
    "Коллекция «The 20 newsgroups» стала популярным набором данных для экспериментов с техниками машинного обучения для текстовых приложений, таких как классификация текста или его кластеризация.\n",
    "\n",
    "https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html\n",
    "\n",
    "Тут мы будем использовать Multinomial Naive Bayes для классификации [(справка)](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomial%20naive%20bayes#sklearn.naive_bayes.MultinomialNB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuhXNioopQTv",
    "outputId": "259d6459-7ef9-4f26-f2d3-903b6a1bfa62"
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "#from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "# making class names shorter\n",
    "class_names = [x.split('.')[-1] if 'misc' not in x else '.'.join(x.split('.')[-2:]) for x in newsgroups_train.target_names]\n",
    "class_names[3] = 'pc.hardware'\n",
    "class_names[4] = 'mac.hardware'\n",
    "\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEwT8HdAWv6u"
   },
   "source": [
    "LIME используется для объяснения множества классификаторов (таких как RandomForest или SVM и нейронные сети) при анализе моделей NLP и CV.\n",
    "\n",
    "Знаменитый набор данных из 20 групп новостей является эталоном и использовался для сравнения различных моделей в нескольких статьях.\n",
    "\n",
    "Мы берем два класса, которые трудно различить, потому что в них много схожих слов: христианство и атеизм.\n",
    "\n",
    "Обучая модель, мы получаем точность тестового набора 83,5%, что является удивительно высоким показателем. Если бы точность была нашим единственным мерилом доверия, мы бы точно доверились этому классификатору.\n",
    "\n",
    "Однако давайте посмотрим на объяснение на рисунке для произвольного экземпляра в тестовом наборе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ul3NOx3pQTv",
    "outputId": "fe9d4b31-6a4c-4624-e7f8-67279bb9fcdc"
   },
   "outputs": [],
   "source": [
    "# Again, let's use the tfidf vectorizer, commonly used for text.\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Обучим модель\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(alpha=.01)\n",
    "nb.fit(train_vectors, newsgroups_train.target)\n",
    "\n",
    "# Сделаем предсказагие обученной модели\n",
    "pred = nb.predict(test_vectors)\n",
    "sklearn.metrics.f1_score(newsgroups_test.target, pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnFjjW__pQTv"
   },
   "source": [
    "Мы видим, что этот классификатор имеет очень высокий F1_score. Руководство sklearn для 20 newsgroups указывает, что Multinomial Naive Bayes переучивается на этом наборе данных, изучая нерелевантные взаимосвязи, такие как заголовки.\n",
    "\n",
    "Теперь мы используем LIME для объяснения индивидуальных прогнозов...\n",
    "\n",
    "**Вопрос:** что означает 0.8350184193998174 и как модель анализовала данные, классифицируя их?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MNc-XELpQTv"
   },
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorizer, nb)\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC1FfBMFpQTw"
   },
   "source": [
    "В случае мультикласса мы должны определить, для каких меток хотим получим объяснения с помощью параметра «labels».\n",
    "Сгенерируем пояснения для меток 0 и 17:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVyCIAQzpQTw",
    "outputId": "d8104867-16df-4d82-bb92-4984e21998dd"
   },
   "outputs": [],
   "source": [
    "idx = 1340\n",
    "exp = explainer.explain_instance(newsgroups_test.data[idx],\n",
    "                                 c.predict_proba, num_features=6,\n",
    "                                 labels=[0, 17])\n",
    "print('Document id: %d' % idx)\n",
    "print('Predicted class =', class_names[nb.predict(test_vectors[idx]).reshape(1,-1)[0,0]])\n",
    "print('True class: %s' % class_names[newsgroups_test.target[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWdHIkUQpQTy"
   },
   "source": [
    "Обратите внимание, что положительный и отрицательный знаки относятся к конкретной метке — так что слова, отрицательные по отношению к классу 0, могут быть положительными по отношению к классу 15, и наоборот. Другая альтернатива — попросить LIME сгенерировать метки для верхних K классов.\n",
    "\n",
    "Это показано ниже с K = 2.\n",
    "\n",
    "Чтобы увидеть, какие ярлыки имеют объяснения, используйте функцию available_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dC2p_3ZSpQTy",
    "outputId": "3128dd79-bbe6-4d22-a979-e9d62c4e903c"
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(newsgroups_test.data[idx],\n",
    "                                 c.predict_proba, num_features=6,\n",
    "                                 top_labels=2)\n",
    "print(exp.available_labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TiB-obRpQTz"
   },
   "source": [
    "Теперь давайте посмотрим на визуализацию объяснений.\n",
    "Обратите внимание, что для каждого класса слова в правой части строки являются «положительными», а слова в левой части — «отрицательными» для обьясняемого класса.\n",
    "\n",
    "Также видно, что в классификаторе используются как разумные слова (такие как «геноцид», «Лютер», «семитский» и т. д.), так и неразумные («рис», «сова»).\n",
    "\n",
    "Давайте увеличим масштаб и просто посмотрим на объяснения класса «атеизм»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "Oe4V68dtpQT0",
    "outputId": "6eced8dc-18ec-4d49-d906-ab265e2619c5"
   },
   "outputs": [],
   "source": [
    "#exp.show_in_notebook(text=False)\n",
    "exp.show_in_notebook(text=newsgroups_test.data[idx], labels=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAwEQVXlpQT1"
   },
   "source": [
    "Глядя на этот пример, можно увидеть, что в заголовке или кавычках может быть и полезный сигнал, который будет помогать обобщению (например,  в строке «Тема»).\n",
    "\n",
    "А есть и слова, которые нельзя обобщать (например, адреса электронной почты и названия учреждений)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX2RqBI_pQT1"
   },
   "source": [
    "## 2.3 Другие библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WFHHuRqpQT1"
   },
   "source": [
    "[DEN](https://github.com/isaacrob/DEN) : основан на новом режиме обучения сиамской нейронной сети без учителя и функции потерь, который называется Differentiating EmbeddingNetworks (DEN).\n",
    "\n",
    "Сиамская нейронная сеть находит отличительные или похожие черты между конкретными парами образцов в наборе данных и использует эти функции, чтобы встроить набор данных в пространство более низкой размерности, где он может быть визуализирован.\n",
    "\n",
    "В отличие от существующих алгоритмов визуализации, таких как UMAP ort-SNE, DEN является параметрическим, то есть его можно интерпретировать такими методами, как SHAP.\n",
    "\n",
    "[XAI](https://towardsdatascience.com/xai-build-your-own-deep-learning-interpretation-algorithm-6e471b59af7) \n",
    "\n",
    "[Boruta](https://github.com/scikit-learn-contrib/boruta_py) — это метод выбора всех релевантных функций, изобретенный Витольдом Р. Рудницки и разработанный Мироном Б. Курса из ICM UW.\n",
    "\n",
    "Boruta — выбирает все необходимые признаки, в то время как другие являются не оптимальными. Он пытается найти все признаки, несущие информацию, используемую для прогнозирования, вместо того, чтобы находить возможно компактное подмножество признаков, по которым некоторый классификатор имеет минимальную ошибку.\n",
    "\n",
    "Зачем беспокоиться о выборе всех необходимых признаков?\n",
    "\n",
    "Когда вы пытаетесь понять феномен, создавший ваши данные, вы должны заботиться о тех факторах, которые способствуют этому, а не только о самых грубых признаках этого в контексте вашей методологии. Но помните, что минимальный оптимальный набор признаков по определению зависит от вашего выбора классификатора. В примере ниже можно увеличить число слоев и, отвергнутые ранее признаки окажутся востребованными моделью.\n",
    "\n",
    "[Boruta Feature Selection](https://towardsdatascience.com/simple-example-using-boruta-feature-selection-in-python-8b96925d5d7a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPFh8JbfpQT1"
   },
   "source": [
    "#### Простота применения: [boruta_py](https://github.com/scikit-learn-contrib/boruta_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77zIq5E5pQT2"
   },
   "outputs": [],
   "source": [
    "!pip install boruta\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzNdRdCppQT2"
   },
   "source": [
    "Boruta соответствует  API sklearn и может использоваться как в конвейере, так и самостоятельно.\n",
    "\n",
    "[Boruta Feature Selection (an Example in Python)](https://towardsdatascience.com/simple-example-using-boruta-feature-selection-in-python-8b96925d5d7a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "JppPYtvrpQT2",
    "outputId": "4251b03c-971d-47ee-ec8a-f3304ff19b2e"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# load the data trainging set\n",
    "datasets = load_breast_cancer()\n",
    "X = pd.DataFrame(datasets['data'], columns = datasets['feature_names'])\n",
    "y = pd.Series(datasets['target'], name = 'target_values')\n",
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0P7UXh6bpQT3",
    "outputId": "4752a413-d369-45a2-ae7c-74b0beec2b43"
   },
   "outputs": [],
   "source": [
    "X.shape, type(X), y.shape, type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eMYSGTY2pQT3",
    "outputId": "74298bd3-78a9-4c6b-e115-ad3eb7a72bb5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDBOKYCQpQT3",
    "outputId": "9c289185-c314-4d1c-ba7e-f6ffd081b1c0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# define random forest classifier\n",
    "forest = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=7)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test, forest.predict(X_test)))\n",
    "print(accuracy_score(y_test, forest.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_lT-rVNpQT3"
   },
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "\n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(forest, n_estimators='auto', verbose=2, random_state=1)\n",
    "\n",
    "# find all relevant features\n",
    "feat_selector.fit(X_train, y_train)\n",
    "\n",
    "# check selected features\n",
    "feat_selector.support_\n",
    "\n",
    "# check ranking of features\n",
    "feat_selector.ranking_\n",
    "\n",
    "# call transform() on X to filter it down to selected features\n",
    "X_filtered = feat_selector.transform(X_train)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "CTXTmN-8pQT3",
    "outputId": "f0957d81-527d-4b09-9dce-24650a00f967"
   },
   "outputs": [],
   "source": [
    "# zip my names, ranks, and decisions in a single iterable\n",
    "feature_ranks = list(zip(datasets.feature_names, \n",
    "                         feat_selector.ranking_, \n",
    "                         feat_selector.support_))\n",
    "\n",
    "# iterate through and print out the results\n",
    "for feat in feature_ranks:\n",
    "    print('Feature: {:<25} Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    "    if feat[2] == False: del X[feat[0]]\n",
    "\n",
    "X[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClYxLPGMpQT3"
   },
   "source": [
    "#### Повторим обучение уже БЕЗ «лишних» фичей\n",
    "Давайте проверим, можем ли мы добиться такого же результата, если проанализируем и уберем «лишние» данные?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7g40rhfqpQT3",
    "outputId": "0c039b54-d0ab-4511-b3cc-e9abe0f552a0"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape, type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SFsit2ppQT3",
    "outputId": "abc2331b-e4fd-457c-e48b-fccf34d08718"
   },
   "outputs": [],
   "source": [
    "# define random forest classifier\n",
    "forest = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=7)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, forest.predict(X_test)))\n",
    "print(accuracy_score(y_test, forest.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9scketSBpQT3"
   },
   "source": [
    "Теперь, когда мы определили признаки, которые можно и нужно удалить, мы можем с уверенностью отказаться от них и продолжить обучение модели.\n",
    "\n",
    "Обратите внимание на скорость и эффективность работы алгоритма Boruta!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psFFGsoypQT3"
   },
   "source": [
    "# 3 Примеры explanations для разных видов данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYG2WvUYpQT3"
   },
   "source": [
    "## 2.1 Tabular examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC9yyCrUDSsF"
   },
   "source": [
    "Для примера скачаем датасет жилья Бостона (boston_dataset), в котором проанализируем зависимость цены на жилье от параметров жилья и района, в котором оно находится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olXKv0MzpQT4"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# load dataset\n",
    "boston_dataset = load_boston()\n",
    "X = pd.DataFrame(data=boston_dataset['data'], columns=boston_dataset['feature_names'])\n",
    "y = boston_dataset['target']\n",
    "\n",
    "# Split the data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Build the model with the random forest regression algorithm\n",
    "model = RandomForestRegressor(max_depth=6, random_state=0, n_estimators=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khS8jUDBDOLA"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNja42mAATxM",
    "outputId": "b99ae237-cde9-4ef9-848e-dfa298244c2c"
   },
   "outputs": [],
   "source": [
    "# Можно посмотреть детальное описание датасета\n",
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrEWvrLoDEFO"
   },
   "source": [
    "**Force plots** \n",
    "\n",
    "Хороший способ визуализировать вклад каждого фактора в конкретный прогноз — использовать график сил.\n",
    "\n",
    "В приведенном ниже примере показан график силы для 2-го случая в тестовом наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "-fUHA0M9ATuh",
    "outputId": "f6c92508-1dc3-4aee-d33b-174a9d5a027e"
   },
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "# visualize the first prediction’s explanation\n",
    "shap.force_plot(explainer.expected_value, shap_values[1,:], X.iloc[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkdFXL1QDx28"
   },
   "source": [
    "* f (x) или прогноз модели равен 20.98. Это средний прогноз по всему набору тестовых данных. Или, другими словами, это значение, которое можно было бы спрогнозировать, если бы мы не знали каких-либо характеристик текущего примера.\n",
    "\n",
    "* Элементы, которые способствуют увеличению цены, показаны красным, а те, которые уменьшают — синим.\n",
    "\n",
    "ПРИМЕР:\n",
    "* LSTAT (% lower status of the population) оказывает сильное положительное влияние на цену дома и подталкивает прогноз вправо.\n",
    "* Другими важными характеристиками, повышающими стоимость дома, являются НАЛОГИ и NOX (nitric oxides concentration (parts per 10 million)).\n",
    "* А вот RM (average number of rooms per dwelling) оказывает сильное негативное влияние на цену жилья, за ней следует DIS (weighted distances to five Boston employment centres)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6BeeQFLU23m"
   },
   "source": [
    "**Waterfall_plot**\n",
    "\n",
    "Другой способ понимания влияния факторов для конкретного примера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "RGvJLjkUTIsG",
    "outputId": "2810738e-6396-4cb0-abc9-7c884c75ea67"
   },
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation using waterfall\n",
    "i = 1 # смотрим влияние факторов для 2го примера\n",
    "features = list(X.columns) # формируем список признаков\n",
    "class ShapObject:\n",
    "    def __init__(self, base_values, data, values, feature_names):\n",
    "        self.base_values = base_values # Single value\n",
    "        self.data = data # Raw feature values for 1 row of data\n",
    "        self.values = values # SHAP values for the same row of data\n",
    "        self.feature_names = feature_names # Column names\n",
    "        \n",
    "shap_object = ShapObject(base_values = explainer.expected_value[0],\n",
    "                         values = shap_values[i,:],\n",
    "                         feature_names = features,\n",
    "                         data = X[features].iloc[i,:])\n",
    "\n",
    "shap.waterfall_plot(shap_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCayHYeAVp_w"
   },
   "source": [
    "Этот график обьясняет движущие силы конкретного прогноза:\n",
    "\n",
    "По оси абсцисс отложено значение SHAP (или логарифмическое отношение вероятностей изменения цены). В самом низу E [f (x)] = -22,403 указывает исходное логарифмическое отношение шансов изменения цены. Точное соотношение между общим отношением шансов и вероятностью равно log (p / (1-p)), где p — вероятность.\n",
    "\n",
    "Ось Y — это названия функций, представленных стрелками, вместе с их соответствующими значениями.\n",
    "\n",
    "Влияние каждой отдельной функции (менее значимые функции объединяются в одну группу) представлено стрелками, которые перемещают логарифмическое отношение шансов влево и вправо, начиная с базового значения.\n",
    "\n",
    "Красные стрелки увеличивают логарифмическое отношение вероятностей, а синие стрелки уменьшают логарифмическое отношение вероятностей.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5MSvDyYFWct"
   },
   "source": [
    "Приведенный выше пример приведен только для одного случая.\n",
    "\n",
    "Если мы возьмем много пояснений Force plots, повернем их на 90 градусов, а затем сложим их по горизонтали, мы сможем увидеть объяснения для всего набора данных (в notebook этот график является интерактивным):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "rIeg8gaBATsV",
    "outputId": "152f2a55-a22d-4ca5-e6d8-4a8bb88a258a"
   },
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "# visualize the training set predictions\n",
    "shap.force_plot(explainer.expected_value, shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aZEhj9mGQOq"
   },
   "source": [
    "**Summary plot**\n",
    "\n",
    "Сводный график с plot_type = 'bar' даст нам график важности переменной.\n",
    "\n",
    "Функции с высокой предсказательной способностью показаны вверху, а с низкой предсказательной силой — внизу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "usUeNQtZGbmR",
    "outputId": "ba51ec00-cd2d-4437-fc3d-35b289e47c01"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_OOBWREJcGb"
   },
   "source": [
    "Здесь мы только что рассмотрели алгоритм TreeExplainer для интерпретации модели.\n",
    "\n",
    "Вы можете изучить остальные алгоритмы — DeepExplainer, kernelExplainer, LinearExplainer и GradientExplainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE4Oto-JpQT4"
   },
   "source": [
    "## 2.2 NLP: Пример абстрактного обобщения текста (потребуется время)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f85jJ-6fpQT4"
   },
   "source": [
    "В этом примере интерпретируется модель генерации объяснений для предварительно обученной модели для составления краткого резюме статьи.\n",
    "\n",
    "Используется датасет Extreme Summarization (XSum)(https://huggingface.co/sshleifer/distilbart-xsum-12-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIOw1u_3pQT4"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "biUcCQgtpQT4"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-xsum-12-6\")\n",
    "model =  AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-xsum-12-6\")#.cuda() # если мы не используем ускоритель GPU, то .cuda() нужно убрать\n",
    "# помним, что .cuda() нужно убрать, если Вы не используете аппаратный ускоритель\n",
    " \n",
    "dataset = load_dataset('xsum',split='train') # загружаем датасет\n",
    "s = dataset['document'][0:1]                 # slice inputs from dataset to run model inference on\n",
    "explainer = shap.Explainer(model,tokenizer)  # create an explainer object\n",
    "shap_values = explainer(s)                   # Compute shap values\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pJ9BlMwOpQT4",
    "outputId": "07533761-2997-4930-875d-12f6076a63f3"
   },
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values)                 # Visualize shap explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlzDqUKWpQT4"
   },
   "source": [
    "## 2.3 Image examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY5vOjBrJO-M"
   },
   "source": [
    "### LIME\n",
    "[Local Interpretable Model-Agnostic Explanations (LIME): An Introduction](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/)\n",
    "\n",
    "[Using Lime with Pytorch](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9OQ1pjBpQT4"
   },
   "source": [
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/7_Google_Inception_neural_network.png\" alt=\"alttext\" width=750/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzRjo21kRE-F"
   },
   "source": [
    "Давайте разберемся, как работает такое эффектное представление областей, по которым нейронная сеть осуществляет предсказание класса.\n",
    "\n",
    "На рисунке ниже показан пример того, как LIME работает для классификации изображений.\n",
    "\n",
    "Представьте, что мы хотим объяснить классификатор, который предсказывает, насколько вероятно, что изображение содержит древесную лягушку.\n",
    "\n",
    "Мы берем изображение слева и делим его на интерпретируемые компоненты (смежные суперпиксели)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEiUWIjfRElW"
   },
   "source": [
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/l10_figure3.jpg\" alt=\"alttext\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW8KUlDWREc9"
   },
   "source": [
    "Далее мы отключаем некоторые из интерпретируемых компонентов.\n",
    "\n",
    "Для каждой такой картинки мы получаем вероятность того, что на изображении есть древесная лягушка, согласно модели.\n",
    "\n",
    "Затем мы изучаем простую (линейную) модель на этом наборе данных, которая имеет локальный вес и анализируем ошибки в измененных экземплярах, которые больше похожи на исходное изображение.\n",
    "\n",
    "В конце концов, мы представляем суперпиксели с наивысшими положительными весами в качестве объяснения, убирая все остальное.\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/l10_figure4.jpg\" alt=\"alttext\" width=650/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRUVP75xREUu"
   },
   "source": [
    "Давайте объясним нейронную сеть Google Inception. Посмотрим, почему ее классификатор предсказывает «древесную лягушку» как наиболее вероятный класс, за которым следуют «бильярдный стол» и «воздушный шар» с более низкими вероятностями.\n",
    "\n",
    "Мы видим, что классификатор в первую очередь фокусируется на морде лягушки как на объяснении предсказанного класса.\n",
    "\n",
    "Это также проливает свет на то, почему «бильярдный стол» имеет ненулевую вероятность: руки и глаза лягушки напоминают бильярдные шары, особенно на зеленом фоне. Точно так же сердце похоже на красный воздушный шар.\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/l10_figure6.jpg\" alt=\"alttext\" width=650/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L74HKQgTxkU_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "#import numpy as np\n",
    "import os, json\n",
    "\n",
    "#import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zT_7HbdHsh2i"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1iKj9wjq-E5bMI4cS1xdj0exaYcmcEVIX' -O dogs.png\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fMdiCrIULzLv1hJTlPFsJhI_hL_ZsPIh' -O cat_dog_5.jpg\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1duqIPPXWjADBW6F5LjTxoGrqxLMs94ot' -O imagenet_class_index.json\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "id": "335CTkm0xkRL",
    "outputId": "30f833d8-7476-4998-b778-38edf843e100"
   },
   "outputs": [],
   "source": [
    "def get_image(path):\n",
    "    with open(os.path.abspath(path), 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB') \n",
    "        \n",
    "img = get_image('cat_dog_5.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "nkIL2YHWK6pC",
    "outputId": "b9b8faab-ff76-4296-ea57-6821f7ac0f1e"
   },
   "outputs": [],
   "source": [
    "img2 = get_image('dogs.png')\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJ5F057b05b4"
   },
   "source": [
    "Теперь нам нужно преобразовать это изображение в тензор Pytorch, а также его нормализовать для использорвания в нашей предварительно обученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLNugt-xxkNT"
   },
   "outputs": [],
   "source": [
    "# resize and take the center part of image to what our model expects\n",
    "def get_input_transform():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])       \n",
    "    transf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize])    \n",
    "    return transf\n",
    "\n",
    "def get_input_tensors(img):\n",
    "    transf = get_input_transform()\n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transf(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNOEQRbd1Nsp"
   },
   "source": [
    "\n",
    "Загрузим предобученную модель Resnet50, доступную в Pytorch и классы изобрражений из ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Zyqcxgg1Wfz"
   },
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "#model = models.resnet152(pretrained=True)\n",
    "\n",
    "idx2label, cls2label, cls2idx = [], {}, {}\n",
    "with open(os.path.abspath('/content/imagenet_class_index.json'), 'r') as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}\n",
    "    cls2idx = {class_idx[str(k)][0]: k for k in range(len(class_idx))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8GKeLrU458A"
   },
   "source": [
    "Получим предикт. А после этого полученные нами прогнозы — логиты, пропустим это через softmax, чтобы получить вероятности и метки классов для 5 лучших прогнозов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYoA9FFO1Wb0",
    "outputId": "8d382fa1-cd32-4f47-8acc-9f7fb528a079"
   },
   "outputs": [],
   "source": [
    "img_t = get_input_tensors(img)\n",
    "model.eval()\n",
    "logits = model(img_t)\n",
    "\n",
    "probs = F.softmax(logits, dim=1)\n",
    "probs5 = probs.topk(5)\n",
    "tuple((p,c, idx2label[c]) for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60z-J0CJ5UA8"
   },
   "source": [
    "Lime производит массив изображений из исходного входного изображения с помощью алгоритма пертубации.\n",
    "\n",
    "Таким образом, нам нужно предоставить две вещи:\n",
    "1. Исходное изображение в виде массива numpy\n",
    "2. Функцию классификации, которая будет принимать массив искаженных изображений в качестве входных данных и генерировать вероятности для каждого класса для каждого изображения в качестве выходных.\n",
    "\n",
    "Для Pytorch сначала нам нужно определить два отдельных преобразования:\n",
    "1. Взять изображение PIL, изменить его размер и обрезать\n",
    "2. Взять обрезанное изображение с измененным размером и применить нормализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTMjJNgM1WXY"
   },
   "outputs": [],
   "source": [
    "def get_pil_transform(): \n",
    "    transf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224)])    \n",
    "    return transf\n",
    "\n",
    "def get_preprocess_transform():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])     \n",
    "    transf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize])    \n",
    "    return transf    \n",
    "\n",
    "pill_transf = get_pil_transform()\n",
    "preprocess_transform = get_preprocess_transform()\n",
    "\n",
    "def batch_predict(images):\n",
    "    model.eval()\n",
    "    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    logits = model(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTiguVHO6Rhv"
   },
   "source": [
    "Применим LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65,
     "referenced_widgets": [
      "1ad7d6b52423497b8fde6720ce150912",
      "ca568e6a8cb14ab9962d53b70a5861d9",
      "7089ec6553b741f1852420f3756dbea2",
      "a8d789f9c1c44410ac918bf3d05e489b",
      "2a84c3bfd3354b39b2bbf5b3c909300d",
      "73d6a6252bbc46ee8b91f8b48d97947d",
      "1387cae7fe524369a428e5cf1ddec4dd",
      "d8a4590855ea469fbbd76308cb941e58"
     ]
    },
    "id": "dyCnxwvM5uH0",
    "outputId": "7dfa903a-82b0-484a-b8d7-7e27efe7c181"
   },
   "outputs": [],
   "source": [
    "from lime import lime_image\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(np.array(pill_transf(img)), \n",
    "                                         batch_predict, # classification function\n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) # number of images that will be sent to classification function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGaUCobw6YBn"
   },
   "source": [
    "Воспользуемся маской на изображении и посмотрим области, которые дают лучший прогноз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgncDkQP6tXh"
   },
   "source": [
    "Обозначим зеленым цветом область наивысшего прогноза и оранжевым — области, которые меньше всего соответствуют нашему прогнозу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgZDrV3W_qmk"
   },
   "source": [
    "Интересно, что именно нос влияет на отнесение к классу собаки в данном примере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "A6zirzbbGgNZ",
    "outputId": "1ec252d4-eca0-416e-c484-bbf9958151a3"
   },
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[1], positive_only=False, num_features=5, hide_rest=False)\n",
    "img_boundry2 = mark_boundaries(temp/255.0, mask)\n",
    "plt.imshow(img_boundry2)\n",
    "# Обратите внимание на:\n",
    "# кошка: top_labels[1], собака: top_labels[0]\n",
    "# количество кластеров, которые нужно показать на рисунке: num_features=3\n",
    "# показать или нет отрицательно влияющие кластеры: positive_only=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjVQci1WNSjz"
   },
   "source": [
    "И другое изображение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334,
     "referenced_widgets": [
      "31fc3cdbcc7b49ca867bf35774e0ecea",
      "3fe8f1e9adcd4cf98db89d503b7470a4",
      "6986feb0a42347d381162bfc1ef6cf89",
      "d6bd2b2a2d3147ad8f9c097d095ded4a",
      "75e8a495c5f84b9ebef2a9d287403b1d",
      "ca1266a1d12645a992e48cb562604ba6",
      "be6bfc3e227c461983f9d222254e265e",
      "096bbba5fabb4ed59cda465a7e412d1c"
     ]
    },
    "id": "nN1I5MDM7mkk",
    "outputId": "436520e2-264b-4e4a-c1d8-7874ec2f2d2d"
   },
   "outputs": [],
   "source": [
    "img = get_image('dogs.png')\n",
    "#plt.imshow(img)\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "idx2label, cls2label, cls2idx = [], {}, {}\n",
    "with open(os.path.abspath('/content/imagenet_class_index.json'), 'r') as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}\n",
    "    cls2idx = {class_idx[str(k)][0]: k for k in range(len(class_idx))}\n",
    "\n",
    "img_t = get_input_tensors(img)\n",
    "model.eval()\n",
    "logits = model(img_t)\n",
    "\n",
    "probs = F.softmax(logits, dim=1)\n",
    "probs5 = probs.topk(5)\n",
    "tuple((p,c, idx2label[c]) for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy()))\n",
    "\n",
    "pill_transf = get_pil_transform()\n",
    "preprocess_transform = get_preprocess_transform()\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(np.array(pill_transf(img)), \n",
    "                                         batch_predict, # classification function\n",
    "                                         top_labels=5, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=1000) # number of images that will be sent to classification function\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[1], positive_only=False, num_features=3, hide_rest=False)\n",
    "img_boundry2 = mark_boundaries(temp/255.0, mask)\n",
    "plt.imshow(img_boundry2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyg0oTj8brOj"
   },
   "source": [
    "В то время как большинство методов, описанных в первом сообщении блога, являются глобальными методами, LIME — это метод, имеющий локальную область применения.\n",
    "\n",
    "Таким образом, LIME позволяет нам объяснить конкретные прогнозы любого классификатора.\n",
    "\n",
    "Структура LIME является гибкой в ​​том смысле, что любую интерпретируемую модель можно использовать для объяснения прогнозов.\n",
    "\n",
    "Кроме того, концепция интерпретируемых компонентов позволяет применять LIME для обьяснения данных большой размерности, таких как классификация изображений или текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Cod1EjHpQT4"
   },
   "source": [
    "### Explain ResNet50 on ImageNet multi-class output using SHAP Partition Explainer\n",
    "\n",
    "[Explain ResNet50 on ImageNet multi-class output using SHAP Partition Explainer](https://github.com/slundberg/shap/blob/master/notebooks/image_examples/image_classification/Image%20Multi%20Class.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kib9xXk_pQT4"
   },
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!conda install opencv\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qFbLsT4pQT4"
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ngQ_N_uRpQT4",
    "outputId": "b9959a48-5902-4e84-b844-11d012f13553"
   },
   "outputs": [],
   "source": [
    "# load pre-trained model and data\n",
    "model = ResNet50(weights='imagenet')\n",
    "X, y = shap.datasets.imagenet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COwxBFSrpQT4"
   },
   "outputs": [],
   "source": [
    "# getting ImageNet 1000 class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "with open(shap.datasets.cache(url)) as file:\n",
    "    class_names = [v[1] for v in json.load(file).values()]\n",
    "#print(\"Number of ImageNet classes:\", len(class_names))\n",
    "#print(\"Class names:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2acd7343b41a4f138bfd6939e78d26f1",
      "bb840b367def493a9a4d31588a728e4e",
      "093d51eda5574ee38912d8b4ac6533f0",
      "8e47d3d475594db4938a06126eb96146",
      "08f9d36e4b574067905193fc6affaea7",
      "443d2d2021c24e77a18a5b347baef416",
      "9c4243571170403784cb0f018f4d0f14",
      "9982f12895d94348a134a88c77587ddf",
      "617f4acd420b486b85a599b5d3404c45",
      "11564ff8213a4715b904a588252c1039",
      "de6c8839b6f64eee8ea9813caaf4fd15",
      "5e7c07ff58a344c3b6418ab6073dafc0",
      "38f34d77dab442e8b3b445c4d3c7d0e9",
      "9b19545d5ef541cb8d0565c1d8df6322",
      "60337838378e4ba2af604369a4d8e44b",
      "bb41641b93794947aa692b56e59eadcd"
     ]
    },
    "id": "Princa5GpQT4",
    "outputId": "108eaeef-5264-45b3-c026-e9aca07c7edd"
   },
   "outputs": [],
   "source": [
    "# python function to get model output; replace this function with your own model function. \n",
    "def f(x):\n",
    "    tmp = x.copy()\n",
    "    preprocess_input(tmp)\n",
    "    return model(tmp)\n",
    "\n",
    "# define a masker that is used to mask out partitions of the input image. \n",
    "masker = shap.maskers.Image(\"inpaint_telea\", X[0].shape)\n",
    "\n",
    "# create an explainer with model and image masker \n",
    "explainer = shap.Explainer(f, masker, output_names=class_names)\n",
    "\n",
    "# here we explain two images using 500 evaluations of the underlying model to estimate the SHAP values\n",
    "shap_values = explainer(X[1:3], max_evals=300, batch_size=50, outputs=shap.Explanation.argsort.flip[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "VtwtSOtypQT5",
    "outputId": "2f4fd847-281e-48c9-b8c0-fa3d84c7798b"
   },
   "outputs": [],
   "source": [
    "# output with shap values\n",
    "shap.image_plot(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPEd98FTpQT5"
   },
   "source": [
    "Интерпретация объяснения вывода SHAP:\n",
    "\n",
    "В первом примере данное изображение птицы классифицируется как американская цапля, а следующие вероятные классы — это журавль, цапля и фламинго. «Шишка» на шее птицы, по которой она классифицируется, как американская цапля против журавля, цапли или фламинго. Вы можете увидеть область шеи птицы, выделенную красным суперпикселем.\n",
    "\n",
    "Во втором примере именно форма лодки позволяет классифицировать ее как быстроходный катер, а не как фонтан, спасательную шлюпку или снегоочиститель (выделено красным суперпикселями).\n",
    "\n",
    "В наборе ImageNet есть множество изображений и вы сможете самостоятельно поэкспериментировать, меняя номера изображений в коде: shap_values = explainer(X[1:10]... и управляя качеством обучения с помощью ..max_evals=1000, batch_size=50.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TTX7bk8uw_6"
   },
   "source": [
    "# Вернемся к 7й лекции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7kyryJWpQT5"
   },
   "source": [
    "##  Bonus 1: Visualizing the Loss Landscape of Neural Nets\n",
    "\n",
    "[Git](https://github.com/tomgoldstein/loss-landscape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsocL6d4pQT5"
   },
   "source": [
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/8_model.jpg\" alt=\"alttext\" width=300/>\n",
    "\n",
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/9_resnet56_noshort_small.jpg\" alt=\"alttext\" width=300/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5L_-ZZc7pQT5"
   },
   "source": [
    "## Bonus 2: Визуализация методов оптимизации нейронных сетей\n",
    "\n",
    "[Методы оптимизации нейронных сетей](https://habr.com/ru/post/318970/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtPX3bkapQT5"
   },
   "source": [
    "\n",
    "<img src=\"http://edunet.kea.su/repo/src/L10_Explainability/img/10_Hills_and_Canyon.gif\" alt=\"alttext\" width=400/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ho-ji76pQT5"
   },
   "source": [
    "# 3. Список литературы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-jjsyOqpQT5"
   },
   "source": [
    "### Статьи\n",
    "\n",
    "[How to Interpret Machine Learning Models with SHAP](https://www.youtube.com/watch?v=ylytZegK--I)\n",
    "\n",
    "[Бесплатный курс от Kaggle: Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "\n",
    "[EXPLAINABLE AI IN CREDIT RISK MANAGEMENT](https://arxiv.org/pdf/2103.00949v1.pdf)\n",
    "\n",
    "[Predicting Driver Fatigue in Automated Driving with Explainability](https://arxiv.org/pdf/2103.02162v1.pdf)\n",
    "\n",
    "[Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods](https://arxiv.org/pdf/1911.02508v2.pdf)\n",
    "\n",
    "### SHAP\n",
    "[Welcome to the SHAP documentation](https://shap.readthedocs.io/en/stable/index.html)\n",
    "\n",
    "[Git](https://github.com/slundberg/shap)\n",
    "\n",
    "[A Unified Approach to Interpreting Model Predictions](https://arxiv.org/pdf/1705.07874v2.pdf)\n",
    "\n",
    "[SHAP (SHapley Additive exPlanations)](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "\n",
    "### LIME\n",
    "\n",
    "[“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)\n",
    "\n",
    "[What does LIME really see in images?](https://arxiv.org/pdf/2102.06307v1.pdf)\n",
    "\n",
    "[Git](https://github.com/marcotcr/lime)\n",
    "\n",
    "### BORUTA\n",
    "[Feature Selection with the Boruta Package](https://www.jstatsoft.org/index.php/jss/article/view/v036i11/v36i11.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljtkYps0dtLo"
   },
   "source": [
    "# 4. Заключение:\n",
    "---\n",
    "* В ходе урока мы убедились в важнности интерпретации работы моделей Искусственного Интеллекта.\n",
    "* Рассмотрели основные библиотеки, которые используются для интерпретирования: SHAP и LIME. А также библиотеку BORUTA, которая позволяет проводить анализ влияния признаков датасета на качество обучения. И дает рекомендации по исключению тех, которые влияют на результата отрицательно.\n",
    "* Рассмотрели применение бибилиотек на примерах:\n",
    "    - Табличные данные\n",
    "    - NLP (машинный перевод текста, создание резюме статьи и классификации текстов)\n",
    "    - CV\n",
    "\n",
    "Пренебрежение обьяснением того, почему модель дала тот или иной результат, ведет к недоверию не только к самой модели, но и к конкретным прогнозам. А, следовательно, является существенным препятствием для дальнейшего введения Вашей идеи в production.\n",
    "\n",
    "**Помните об этом!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXTE7T--ipge"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3Cod1EjHpQT4",
    "5TTX7bk8uw_6",
    "f7kyryJWpQT5",
    "5L_-ZZc7pQT5",
    "_ho-ji76pQT5"
   ],
   "machine_shape": "hm",
   "name": "L10_Explainability.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08f9d36e4b574067905193fc6affaea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "093d51eda5574ee38912d8b4ac6533f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_443d2d2021c24e77a18a5b347baef416",
      "max": 148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08f9d36e4b574067905193fc6affaea7",
      "value": 148
     }
    },
    "096bbba5fabb4ed59cda465a7e412d1c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11564ff8213a4715b904a588252c1039": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1387cae7fe524369a428e5cf1ddec4dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ad7d6b52423497b8fde6720ce150912": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7089ec6553b741f1852420f3756dbea2",
       "IPY_MODEL_a8d789f9c1c44410ac918bf3d05e489b"
      ],
      "layout": "IPY_MODEL_ca568e6a8cb14ab9962d53b70a5861d9"
     }
    },
    "213a03de4f9f41fd9e4f25869d335fa6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "282cfaf3beac45e0a0dbad73fb5aeacb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a84c3bfd3354b39b2bbf5b3c909300d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2acd7343b41a4f138bfd6939e78d26f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_093d51eda5574ee38912d8b4ac6533f0",
       "IPY_MODEL_8e47d3d475594db4938a06126eb96146"
      ],
      "layout": "IPY_MODEL_bb840b367def493a9a4d31588a728e4e"
     }
    },
    "31fc3cdbcc7b49ca867bf35774e0ecea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6986feb0a42347d381162bfc1ef6cf89",
       "IPY_MODEL_d6bd2b2a2d3147ad8f9c097d095ded4a"
      ],
      "layout": "IPY_MODEL_3fe8f1e9adcd4cf98db89d503b7470a4"
     }
    },
    "38f34d77dab442e8b3b445c4d3c7d0e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3fe8f1e9adcd4cf98db89d503b7470a4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "443d2d2021c24e77a18a5b347baef416": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e7c07ff58a344c3b6418ab6073dafc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb41641b93794947aa692b56e59eadcd",
      "placeholder": "​",
      "style": "IPY_MODEL_60337838378e4ba2af604369a4d8e44b",
      "value": " 242/? [00:01&lt;00:00, 138.55it/s]"
     }
    },
    "60337838378e4ba2af604369a4d8e44b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "617f4acd420b486b85a599b5d3404c45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de6c8839b6f64eee8ea9813caaf4fd15",
       "IPY_MODEL_5e7c07ff58a344c3b6418ab6073dafc0"
      ],
      "layout": "IPY_MODEL_11564ff8213a4715b904a588252c1039"
     }
    },
    "67c323a2c6ea47f1a3516254f5580c47": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6efd2e4bb47b496cad2a5e2fc90acce9",
      "placeholder": "​",
      "style": "IPY_MODEL_282cfaf3beac45e0a0dbad73fb5aeacb",
      "value": " 62/? [00:14&lt;00:00,  3.71it/s]"
     }
    },
    "6986feb0a42347d381162bfc1ef6cf89": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca1266a1d12645a992e48cb562604ba6",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_75e8a495c5f84b9ebef2a9d287403b1d",
      "value": 1000
     }
    },
    "6efd2e4bb47b496cad2a5e2fc90acce9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7089ec6553b741f1852420f3756dbea2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73d6a6252bbc46ee8b91f8b48d97947d",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a84c3bfd3354b39b2bbf5b3c909300d",
      "value": 1000
     }
    },
    "73d6a6252bbc46ee8b91f8b48d97947d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75e8a495c5f84b9ebef2a9d287403b1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8e47d3d475594db4938a06126eb96146": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9982f12895d94348a134a88c77587ddf",
      "placeholder": "​",
      "style": "IPY_MODEL_9c4243571170403784cb0f018f4d0f14",
      "value": " 242/? [00:02&lt;00:00, 97.14it/s]"
     }
    },
    "9982f12895d94348a134a88c77587ddf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a16ed584d974a709934b8c14000b795": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9b19545d5ef541cb8d0565c1d8df6322": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c4243571170403784cb0f018f4d0f14": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8d789f9c1c44410ac918bf3d05e489b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8a4590855ea469fbbd76308cb941e58",
      "placeholder": "​",
      "style": "IPY_MODEL_1387cae7fe524369a428e5cf1ddec4dd",
      "value": " 1000/1000 [00:21&lt;00:00, 45.86it/s]"
     }
    },
    "bb41641b93794947aa692b56e59eadcd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb840b367def493a9a4d31588a728e4e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be6bfc3e227c461983f9d222254e265e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c084706d78404217a954794a1aabec10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5fc50e995e049908df2ca95742c560a",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a16ed584d974a709934b8c14000b795",
      "value": 48
     }
    },
    "ca1266a1d12645a992e48cb562604ba6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca568e6a8cb14ab9962d53b70a5861d9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5fc50e995e049908df2ca95742c560a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6bd2b2a2d3147ad8f9c097d095ded4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_096bbba5fabb4ed59cda465a7e412d1c",
      "placeholder": "​",
      "style": "IPY_MODEL_be6bfc3e227c461983f9d222254e265e",
      "value": " 1000/1000 [00:05&lt;00:00, 171.45it/s]"
     }
    },
    "d8a4590855ea469fbbd76308cb941e58": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de6c8839b6f64eee8ea9813caaf4fd15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b19545d5ef541cb8d0565c1d8df6322",
      "max": 148,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_38f34d77dab442e8b3b445c4d3c7d0e9",
      "value": 148
     }
    },
    "f031d720d22342d6b3297d5c40013714": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c084706d78404217a954794a1aabec10",
       "IPY_MODEL_67c323a2c6ea47f1a3516254f5580c47"
      ],
      "layout": "IPY_MODEL_213a03de4f9f41fd9e4f25869d335fa6"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
