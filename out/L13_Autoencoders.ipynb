{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Автоэнкодеры</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоэнкодер (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.* — **Yann LeCun**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Сегодня мы разберем подход, который относится к задаче обучения без учителя (**unsupervised learning**), когда объекты известны, но им не сопоставлены метки, которые мы должны тем или иным способом предсказывать.\n",
    "\n",
    "Разницу между двумя задачами можно понять при помощи картинки, представленной ниже. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/supervised_learning.png\" alt=\"alttext\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/unsupervised_learning.png\" alt=\"alttext\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае **supervised learning** для каждого объекта нам известна метка, что вот это — изображение яблок, это — изображение груш и т.д. Далее модель учится по изображению определять фрукт. \n",
    "\n",
    "В случае же **unsupervised learning** модель просматривает все изображения фруктов, не зная, где какой фрукт находится, и далее формирует представление, которое **неявно** делит фрукты по похожести. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем вообще изучать такой тип задачи? \n",
    "\n",
    "1. Иногда у нас слишком мало размеченных объектов, чтобы учить на них какую-либо задачу классификации и т. д. При этом у нас огромное количество неразмеченных данных. Мы можем **надеяться**, что если мы как-то обработаем наши данные, то они сами разделятся каким-то образом, согласующимся с метками. \n",
    "\n",
    "2. Человеческий мозг в основном учится в unsupervised манере. Возможно, для того чтобы решать задачи, которые легко по сравнению с компьютером решает человек, нам стоит и компьютер учить похожим образом.\n",
    "\n",
    "3. Часто обучение без учителя дает результаты, которые в дальнейшем позволяют быстро адаптироваться к новым задачам обучения с учителем и переключаться между ними. Причем делать это **эффективнее**, чем transfer learning. (*Stop learning tasks, start learning skills* — Satinder Singh)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из областей, тесно связанных с unsupervised learning, является обучение представлениям (**representation learning**).\n",
    "\n",
    "Как уже обсуждалось ранее, крайне важным шагом в решении задач машинного обучения может стать предобработка данных, зависящая от используемой модели. В течение долгого времени исследователям приходилось вручную готовить данные: к примеру, используя **domain knowledge** или разведочный анализ данных (**EDA** — **exploratory data analysis**), отбрасывать малозначимые признаки или создавать новые признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведён пример задачи классификации, при решении которой с помощью линейной модели нельзя обойтись без создания новых признаков:  \n",
    "- каждый из объектов располагается на плоскости $(x_0, x_1)$\n",
    "\n",
    "- истинное правило классификации выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  f(\\mathbf{x}) = \n",
    "  \\begin{cases}\n",
    "    0, & \\left\\Vert\\mathbf{x}\\right\\Vert_2 > 5 \\\\\n",
    "    1, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.uniform(-10, 10, size=(1000, 2))\n",
    "y = (((x**2).sum(axis=1) ** 0.5) <= 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 7), dpi=80)\n",
    "plt.scatter(x[y == 0][:, 0], x[y == 0][:, 1])\n",
    "plt.scatter(x[y == 1][:, 0], x[y == 1][:, 1])\n",
    "\n",
    "plt.xlabel(\"x_1\")\n",
    "plt.ylabel(\"x_2\")\n",
    "plt.legend([\"y=0\", \"y=1\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно попробовать обучить логистическую регрессию на данном наборе данных, однако результат будет неудовлетворительным:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Upgrade mlxtend library for correct matplotlib work\n",
    "!pip install mlxtend --upgrade --no-deps\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plt.figure(figsize=(7, 7), dpi=80)\n",
    "plot_decision_regions(x_test, y_test, clf=lr, legend=2)\n",
    "plt.xlabel(\"x_0\")\n",
    "plt.ylabel(\"x_1\")\n",
    "plt.title(\"LogReg on test data\")\n",
    "clear_output()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте численно измерим качество работы обученной модели. Использовать будем F1-score, поскольку классы не сбалансированы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pred = lr.predict(x_test)\n",
    "f1_lr = f1_score(y_test, pred)\n",
    "print(f\"F1-score for naive model: {round(f1_lr, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, значение довольно низкое, следовательно, необходимо попробовать неким образом предобработать данные. Как мы понимаем, просто удаление признака не улучшит качество модели, поэтому можно попробовать придумать новый, более значимый признак.\n",
    "\n",
    "Мы можем заметить, что форма разделяющей кривой в данном случае напоминает окружность. Соответственно, можно воспользоваться неким вариантом радиально-базисной функции (**radial basis function**). В качестве метрики расстояния воспользуемся $L_2$, и попробуем найти центр окружности. \n",
    "\n",
    "Центр можно искать несколькими способами, однако мы воспользуемся простейшим: усредним значения точек, в которых $f(\\mathbf{x}) = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empiric_center = x_train[y_train == 1].mean(axis=0)\n",
    "print(f\"Empiric value of the center: {empiric_center}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7), dpi=80)\n",
    "plt.scatter(x[y == 0][:, 0], x[y == 0][:, 1])\n",
    "plt.scatter(x[y == 1][:, 0], x[y == 1][:, 1])\n",
    "plt.scatter(empiric_center[0], empiric_center[1], c=\"r\")\n",
    "plt.scatter(0, 0, c=\"g\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"x_0\")\n",
    "plt.ylabel(\"x_1\")\n",
    "plt.legend([\"y=1\", \"y=0\", \"empiric center\", \"real center\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте создадим признак, соответствующий $L_2$ расстоянию от каждой из точек до предполагаемого центра окружности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = ((x - empiric_center) ** 2).sum(axis=1) ** 0.5\n",
    "x_train_new = ((x_train - empiric_center) ** 2).sum(axis=1) ** 0.5\n",
    "x_test_new = ((x_test - empiric_center) ** 2).sum(axis=1) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте визуально и численно оценим, насколько хорошо новый признак подходит для решения задачи классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.hist(x_new[y == 0], bins=20, alpha=0.7)\n",
    "plt.hist(x_new[y == 1], bins=20, alpha=0.7)\n",
    "\n",
    "plt.ylabel(\"Sample count\")\n",
    "plt.xlabel(\"Distance to center\")\n",
    "plt.legend([\"y=0\", \"y=1\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(x_train_new.reshape(-1, 1), y_train)\n",
    "pred = lr.predict(x_test_new.reshape(-1, 1))\n",
    "f1_rbf = f1_score(y_test, pred)\n",
    "print(f\"F1-score for RBF model: {round(f1_rbf, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь качество работы модели на тестовых данных довольно высоко, однако признаки создавались вручную с использованием гипотезы, которая, к счастью, оказалась верной. В реальности, при использовании моделей классического машинного обучения приходится довольно упорно исследовать данные, рисовать большое количество графиков, выдвигать и опровергать огромное количество гипотез, тратя уйму времени.\n",
    "\n",
    "К счастью, глубокие нейронные сети позволяют решить все эти проблемы, автоматизируя поиск осмысленных и полезных комбинаций признаков, обращая внимание на важные признаки и не уделяя внимание признакам неважным. В процессе работы нейронной сети от слоя к слою формируются различные **представления данных**. \n",
    "\n",
    "По сути представление объекта является неким численным описанием данного объекта. В примере выше исходным представлением объекта было численное описание положения объекта на плоскости $(x_1, x_2)$. После этого мы создали новое представление объекта, являющееся описанием $L_2$-расстояния от положения объекта на плоскости $(x_1, x_2)$ до эмпирического центра окружности $\\left(x_{1_c}, x_{2_c}\\right)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/representation_change.png\" alt=\"alttext\" width=\"1450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоговом представлении мы сохранили информацию, пригодную лишь для того, чтобы решать поставленную перед нами задачу — **полученное итоговое представление не универсально**. Оно подходит для решения крайне узкого круга задач, связанных с расстоянием до определённой точки. \n",
    "\n",
    "К примеру, созданное нами представление (расстояние от точки до эмпирического центра окружности) слабо подходит для решения такой задачи классификации, как: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  f(\\mathbf{x}) = \n",
    "  \\begin{cases}\n",
    "    0, & x_0 \\leq 0 \\\\\n",
    "    1, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = x[:, 0] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.hist(x_new[y_new == 0], bins=20, alpha=0.7)\n",
    "plt.hist(x_new[y_new == 1], bins=20, alpha=0.7)\n",
    "\n",
    "plt.ylabel(\"Sample count\")\n",
    "plt.xlabel(\"Distance to center\")\n",
    "plt.legend([\"y_new=0\", \"y_new=1\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим, что в нашем случае, **чем лучше избранное представление подходит для решения изначальной задачи, тем хуже оно подходит для решения второй задачи** (чем точнее найден центр окружности, тем ближе друг к другу будут результирующие представления двух точек $(x_0, t)$ и $(-x_0, t)$). \n",
    "\n",
    "Аналогичным образом, автоматически найденные в нейронных сетях представления данных пригодны лишь для решения поставленной перед ними задачи. Однако постановка задачи может быть разной: можно либо обучить нейронную сеть предсказывать определённую величину (**supervised** подход, которым мы пользовались на протяжении всего курса), либо каким-то образом заставить нейронную сеть \"понять\" структуру данных в общем (**unsupervised** подход). \n",
    "\n",
    "**Unsupervised** подход в общем случае позволяет создать более универсальные скрытые представления данных, учитывая при этом практически всю доступную информацию о данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Снижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одной областью, тесно связанной с предыдущими двумя, является задача снижения размерности — когда мы хотим данные из пространства высокой размерности преобразовать в пространство более низкой с сохранением одного или нескольких свойств, например:\n",
    "\n",
    "* данные реконструируются обратно почти без ошибки;\n",
    "* расстояние между объектами сохраняется.\n",
    "\n",
    "Зачем это нужно? По многим причинам.\n",
    "\n",
    "Многие алгоритмы показывают себя плохо на пространствах большой размерности в принципе ([проклятье размерности](https://en.wikipedia.org/wiki/Curse_of_dimensionality)). \n",
    "\n",
    "Некоторые алгоритмы просто будут значительно дольше работать, при этом качество их работы не изменится от уменьшения размерности. \n",
    "\n",
    "Понижение размерности позволяет использовать память более эффективно и подавать модели на обучение за один раз больше объектов. \n",
    "\n",
    "Также понижение размерности помогает избавиться от шума. Как? Обсудим дальше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоэнкодер — архитектура нейросети, которая сначала с помощью нейросети-энкодера сжимает призаковое описание объекта в вектор небольшой размерности (он называется скрытым представлением), а затем восстанавливает этот вектор в исходное признаковое пространство с помощью нейросети-декодера. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практика показывает, что, например при работе с изображениями, скрытое представление картинки позволяет делать очень интересные и красивые вещи — например, очищать изображение от шума, проводить гладкую интерполяцию между 2 написанными от руки цифрами, генерировать новую рукописную цифру со стилем от имеющейся. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откуда берутся эти свойства? Они являются следствием сжатия информации. Одна из форм сжатия — это классификация, которую мы уже делали. Если это цифры, то вместо изображения можно сохранить только один признак — какая это цифра. Это предельное сжатие информации, но при попытке перевести цифру в картинку мы уже не имеем достаточно информации, чтобы картинка получалось разной. Если не так сильно ограничивать информацию в точке максимального сжатия, то кроме класса цифры сохранится еще что-то и изображение удастся восстановить с большим количеством сохранённых деталей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/autoencoder_architecture.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сжатие информации и потери"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоэнкодер может быть без потерь и с потерями (lossless и lossy). В какой-то степени это альтернативно методам сжатия архиваторов и кодирования контента (zip, mp3, jpeg, flac, ...). Можно ли сделать сжатие на нейронных сетях с помощью автоэнкодеров? Да, это будет работать. Размер сети будет большим, но сжатие может превзойти другие алгоритмы. Практический пример — [проект Google Lyra](https://ai.googleblog.com/2021/02/lyra-new-very-low-bitrate-codec-for.html), в котором подобный подход был применен для компрессии звука, и проект [NVIDIA Maxine](https://developer.nvidia.com/maxine), где в свою очередь сжимают видео.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/autoencoder_scheme.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему это может работать? Дело в том, что нейронная сеть может сформулировать набор правил, по которому на основе латентного представления приближенно или точно кодировать (за счет кодировщика), а затем восстанавливать исходный объект (при помощи декодировщика).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/original_space_to_latent_space.png\" alt=\"alttext\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А почему мы уверены, что такой набор правил будет существовать, и мы вообще имеем право понижать размерность пространства?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold assumption "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В глубоком обучении часто используют предположение о многообразии (manifold assumption). Это предположение о том, что реальные данные не распределены равномерно по пространству признаков, а занимают лишь его малую часть — **многообразие** (manifold).\n",
    "\n",
    "Если предположение верно, то каждый объект может быть достаточно точно описан новыми признаками в пространстве значительно меньшей размерности, чем исходное пространство признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/manifold_assumption.png\" alt=\"alttext\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве случаев это действительно так. Например, лица людей на фотографиях 300x300, очевидно, лежат в пространстве меньшей размерности, нежели 90&nbsp;000. Ведь не каждая матрица 300 на 300, заполненная какими-то значениями от 0 до 1, даст нам изображение человека."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/manifold_assumption_faces_example.png\" alt=\"alttext\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод главных компонент (Principal Component Analysis) — это метод отображения векторов свойств объектов (помним, что у нас объект всегда описывается вектором свойств, длина вектора — это количество свойств) в вектора производных свойств (**компонент**) меньшей длины с помощью линейной комбинации, чтобы обратной операцией можно было восстановить значения векторов свойств как можно ближе к исходным. То есть PCA тоже выполняет сжатие информации, он тоже работает для группы объектов (а нейронная сеть автоэнкодера учится под определённую группу объектов). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно PCA работает для центрированных переменных. Каждая следующая компонента проводится перпендикулярно предыдущим и так, чтобы объяснить наибольшую часть дисперсии, не объясненной предыдущими компонентами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/3d_to_2d_pca.png\" width=1000/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графически можно представить PCA, как поиск подпространства, проекция точек на которое минимально меняет координаты в исходном пространстве. Например, для объектов на плоскости PCA можно сделать в одномерное пространство — на&nbsp;прямую.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/pca_decomposition.png\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямая определяется только вектором нормали, то есть линия проекции проходит через 0. \n",
    "\n",
    "\n",
    "Возвращаясь к примеру с лицами — долгое время для распознавания лиц размерности 128\\*128 использовалось представление, полученное при помощи PCA. Для хорошего качества восстановления хватает около 100 компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналогия AE и PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия PCA и AE в том, что PCA выполняет линейную комбинацию над компонентами исходного вектора свойств объекта, а AE — как правило, нелинейную. PCA вычисляется однозначно, а AE обучается без гарантии нахождения наилучшего положения. PCA гарантирует ортогональный базис для разложения сжатых свойств, а AE — нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA будет частным случаем AE, если в AE сделать только **один полносвязный скрытый слой** с количеством нейронов, равным требуемому числу компонент, сделать линейную функцию активации, и использовать среднеквадратическую функцию потерь (MSE). Кроме этого, необходимо будет нормировать признаки перед подачей их на вход AE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/pca_autoencoder.png\" alt=\"alttext\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда PCA позволяет рассчитать веса для нейронов такого автоэнкодера. При этом гарантировав (в отличие от градиентного спуска) наилучшее решение задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очищение изображения от шумов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересное применение автоэнкодеров — очищение входной картинки от шумов. Такое принципиально возможно из-за того, что размерность латентного пространства очень мала по сравнению с размерностью входного пространства — в нём попросту нет места случайному шуму, но зато есть место для общих закономерностей из входного пространства."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы подаём на обучении автоэнкодера такой незашумлённый датасет, что в нём на самом деле есть некое пространство свойств, которое его описывает. На выходе энкодера в изображении останутся именно эти свойства. Шум является внешним свойством и не сможет закодироваться.\n",
    "\n",
    "Иными словами, за счет кодировщика и декодировщика автоэнкодер выучивается «проектировать» объекты на латентное пространство и восстанавливать их из него. Если шум небольшой, то автоэнкодер спроецирует объект в нужное место в латентном пространстве и обратно восстановит его уже без шума.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/autoencoder_clean_noise.png\" alt=\"alttext\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом важно понимать, что если шум поместит наш объект так, что автоэнкодеру придется выбирать между разными вариантами проекции, могут возникнуть артефакты. \n",
    "\n",
    "В случае, приведенном на рисунке, зашумленному $x$ соответствуют две группы объектов из реального датасета. Если мы, к примеру, оптимизируем MSE, то автоэнкодеру «экономнее» всего будет восстанавливать нечто между двумя группами. При этом этого «нечто» в природе не существует или оно очень маловероятно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/reconstructed_between_2_distribution.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление шума к исходной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в случае отсутствия шума в изначальной выборке, ее малом размере и т.д. можно добавлять шум к самим исходным данным, получая из объекта $x$ объект $\\tilde{x}$ и требуя от энкодера восстановить на основе зашумленного объекта исходный. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/denoising_autoencoder.png\" alt=\"alttext\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот подход может работать и является примером *аугментации данных*. Он может дополнительно заставить автоэнкодер выучивать полезные признаки, т.е. его можно использовать, даже если целью не является получение автоэнкодера, избавляющего данные от шума.\n",
    "\n",
    "С ним, однако, надо быть очень аккуратным:\n",
    "\n",
    "1. Шум, который вы добавляете, не должен сильно менять исходный объект. Если это происходит, то либо автоэнкодер легко будет находить места, где был добавлен шум, и при этом делать ему это будет легче, чем учить сжатое представление данных. Либо автоэнкодер выучит о ваших данных что-то такое, чего там и в помине быть не может. К примеру, если добавить к признакам, которые всегда целые, нормальный шум, ничего хорошего не выйдет. \n",
    "\n",
    "\n",
    "2. Шум должен соответствовать «естественному шуму». Если реальный шум в данных отличается от того, на котором учился автоэнкодер, есть вероятность, что он не будет очищать данные от исходного шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA для избавления от шума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим PCA, как простейший автоэнкодер, для очищения от шумов изображений базы MNIST. Нам потребуется база MNIST, NumPy, библиотека отрисовки matplotlib и сам PCA, который есть в пакете sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "\n",
    "root = \"./data\"\n",
    "\n",
    "train_set = MNIST(\n",
    "    root=root, train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "test_set = MNIST(\n",
    "    root=root, train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и приведём размерность к двумерной, чтобы это был набор векторов свойств."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set.data.numpy()\n",
    "y_train = train_set.targets.numpy()\n",
    "x_test = test_set.data.numpy()\n",
    "y_test = test_set.targets.numpy()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # normalize data to [0; 1]\n",
    "xt_shape = x_train.shape\n",
    "print(\"Initial shape \", xt_shape)\n",
    "xt_flat = x_train.reshape(\n",
    "    -1, xt_shape[1] * xt_shape[2]\n",
    ")  #  reshape to vector, 28*28 => 784\n",
    "print(\"Reshaped to \", xt_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но графически:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "ax[0].imshow(x_train[0])\n",
    "ax[1].imshow(xt_flat[0].reshape(1, -1), aspect=50)\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[1].set_title(\"Flattened image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь заведём класс PCA и настроим его, чтобы он отобрал столько компонент, чтобы объяснялось 90% дисперсии. Обучим его и посмотрим, сколько ему потребовалось свойств, для описания каждой картинки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.90)\n",
    "xt_encoded = pca.fit_transform(xt_flat)\n",
    "print(\"Encoded features \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер (он же декодер, ведь это просто обратная матрица от энкодера PCA) обучен. Теперь можно проверить, как он закодирует и раскодирует тестовую выборку. Для этого проведём такие же преобразования размерности для неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_shape = x_test.shape\n",
    "xtest_flat = x_test.reshape(-1, xtest_shape[1] * xtest_shape[2])\n",
    "xtest_encoded = pca.transform(xtest_flat)\n",
    "xtest_decoded = pca.inverse_transform(xtest_encoded).reshape(xtest_shape)\n",
    "print(\"xtest_decoded shape is \", xtest_decoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно определить функцию для отрисовки изображений MNIST. Она будет выводить несколько изображений в ряд, поэтому будет принимать трёхмерный массив. Шкала не должна быть автоподстраиваемой, так как после обработки изображения выйдут за диапазон $[0;1]$, в котором заданы исходные изображения. Мы зафиксируем шкалу в диапазоне $[0;1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs(imgs, title):\n",
    "    fig = plt.figure(figsize=(16, 3))\n",
    "    columns = imgs.shape[0]\n",
    "    rows = 1\n",
    "    for i in range(columns):\n",
    "        fig.add_subplot(rows, columns, i + 1)\n",
    "        plt.imshow(imgs[i], cmap=\"gray_r\", clim=(0, 1))\n",
    "    fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем исходное и закодированное-раскодированное изображение для некоторых объектов, которые мы случайным образом выберем из всей тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "sample_indices = np.random.choice(x_test.shape[0], 6)\n",
    "samples_orig = x_test[sample_indices]\n",
    "samples_decoded = xtest_decoded[sample_indices]\n",
    "plot_imgs(samples_orig, \"Original x_test\")\n",
    "plot_imgs(samples_decoded, \"PCA encoded-decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что `pca.n_components_` (87 для 90% PCA) достаточно для описания картинок MNIST вместо 784 исходных пикселей. Но при этом нужно хранить матрицу кодирования-декодирования, а изображения получаются немного зашумлёнными. Мы получили способ сжатия с потерями для рукописных цифр, где изображение центрировано и отмасштабировано по рамке из 28х28 пикселей (подробней смотрите правила базы MNIST). \n",
    "\n",
    "Степень сжатия у нас условно 87/784 ~= 0.11. То есть сжатие в 9 раз. «Условно», так как сжатое изображение хранится во float, а исходное в uint8, который требует в 4 раза меньше байт, плюс мы должны хранить матрицы для кодирования и декодирования. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим, как наш автоэнкодер без нейросетей справится с очисткой от зашумления. Для этого сделаем функцию добавления шумов к MNIST и посмотрим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import random_noise\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x_test_noisy = random_noise(x_test, mode=\"gaussian\")\n",
    "samples_noisy = x_test_noisy[sample_indices]\n",
    "plot_imgs(samples_noisy, \"x_test with added noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно провести ту же операцию PCA энкодера и декодера, что выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCArecode(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1] * dataset.shape[2])\n",
    "    encoded = pca.transform(dataset_flat)\n",
    "    decoded = pca.inverse_transform(encoded).reshape(dataset.shape)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "x_filtered = PCArecode(x_test_noisy)\n",
    "samples_filtered = x_filtered[sample_indices]\n",
    "plot_imgs(samples_filtered, \"PCA denoised x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты сравнения напишем функцию, которая будет строить зашумленные и восстановленные образцы друг под другом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(*args, invert_colors=True, digit_size=28, name=None, single_size=2):\n",
    "    args = [x.squeeze() for x in args]\n",
    "    n = min([x.shape[0] for x in args])\n",
    "    figure = np.zeros((digit_size * len(args), digit_size * n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(len(args)):\n",
    "            figure[\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "            ] = args[j][i].squeeze()\n",
    "\n",
    "    if invert_colors:\n",
    "        figure = 1 - figure\n",
    "\n",
    "    plt.figure(figsize=(single_size * n, single_size * len(args)))\n",
    "\n",
    "    plt.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "\n",
    "    plt.grid(False)\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if name is not None:\n",
    "        plt.savefig(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(samples_noisy, samples_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, шумы стали значительно меньше, но и артефакты вокруг цифр усилились. Это неудивительно, ведь мы сжимали информацию линейным образом всего в 87 компонент. Повышение уровня сжатия приведёт к еще большему количеству артефактов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Латентное представление цифр после PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим теперь на то, как разделяются изображения разных цифр в латентном представлении. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_latent(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1] * dataset.shape[2])\n",
    "    return pca.transform(dataset_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_manifold(latent_r, labels=None, alpha=0.9, title=None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    if labels is None:\n",
    "        plt.scatter(latent_r[:, 0], latent_r[:, 1], alpha=alpha)\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "    else:\n",
    "        plt.scatter(latent_r[:, 0], latent_r[:, 1], c=labels, cmap=\"tab10\", alpha=alpha)\n",
    "        plt.colorbar()\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим на двумерной плоскости первые две компоненты признаков, выделенных PCA — они объясняют максимум дисперсии исходных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_r = pca_latent(x_test)\n",
    "plot_manifold(latent_r, y_test, title=\"PCA manifold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что латентное представление слабо разделяет картинки по тому, какие цифры на них изображены. Только единицы расположены более-менее обособленно и плотно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Брать лишь первые две компоненты из 87 не совсем честно. Поэтому переведем признаки, полученные методом PCA, из 87-мерного пространства в 2-мерное методом UMAP и посмотрим, выделяются ли классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_latent_r = StandardScaler().fit_transform(latent_r)\n",
    "reducer = umap.UMAP()\n",
    "latent_r_2d = reducer.fit_transform(scaled_latent_r)\n",
    "plot_manifold(latent_r_2d, y_test, title=\"2D UMAP over PCA manifold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В латентном представлении цифры в среднем неплохо разделяются на отдельные кластеры, но многие отдельные изображения оказываются в \"чужих\" кластерах. Для такого простого датасета результат неудовлетворительный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, вспомним, что в автоэнкодере одна сеть переводит пространство свойств в пространство меньшей размерности, а другая сеть обучается восстанавливать исходные объекты. Вместо вычисления коэффициентов сети мы будем её обучать. Для обучения нужно определить функцию потерь. Обычно используют среднеквадратичное расстояние (MSE). То есть мы требуем, чтобы значения пикселей исходного изображения и восстановленного отличались несильно. В нашем примере, мы будем использовать  Binary Cross-Entropy, она обеспечивает лучшую сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/nn_encoder_nn_decoder.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем использовать любую сеть для энкодера и декодера: на полносвязных слоях или на свёрточных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно задать архитектуру модели. Мы будем использовать последовательную модель (Sequential) и свёрточную  архитектуру. В конце кодировщика должен быть вектор с размером `latent_dim`. И декодировщик должен принимать этот вектор и восстанавливать до целого изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim  # latent space size\n",
    "        hidden_dims = [32, 64, 128, 256, 512]  # num of filters in layers\n",
    "        modules = []\n",
    "        in_channels = 1  # initial value of channels\n",
    "        for h_dim in hidden_dims[:-1]:  # conv layers\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=in_channels,  # num of input channels\n",
    "                        out_channels=h_dim,  # num of output channels\n",
    "                        kernel_size=3,\n",
    "                        stride=2,  # convolution kernel step\n",
    "                        padding=1,  # save shape\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim  # changing number of input channels for next iteration\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=256, out_channels=512, kernel_size=1\n",
    "                ),  # changing the kernel size, because  size of the array (2*2)\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "        )\n",
    "        modules.append(nn.Flatten())  # to vector, size 512 * 2*2 = 2048\n",
    "        modules.append(nn.Linear(512 * 2 * 2, latent_dim))\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dims = [512, 256, 128, 64, 32]  # num of filters in layers\n",
    "        self.linear = nn.Linear(in_features=latent_dim, out_features=512)\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):  # define ConvTransopse layers\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels=hidden_dims[i],\n",
    "                        out_channels=hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=hidden_dims[-1],\n",
    "                    out_channels=hidden_dims[-1],\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(\n",
    "                    in_channels=hidden_dims[-1],\n",
    "                    out_channels=1,\n",
    "                    kernel_size=7,\n",
    "                    padding=1,\n",
    "                ),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)  # from latents space to Linear\n",
    "        x = x.view(-1, 512, 1, 1)  # reshape\n",
    "        x = self.decoder(x)  # reconstruction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем основную функцию для обучения нейросети. `single_pass_handler` и `loss_handler` будут меняться в зависимости от сети, которую мы обучаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "Function to train model, parameters: \n",
    "  enc - encoder\n",
    "  dec - decoder\n",
    "  loader - loader of data\n",
    "  optimizer - optimizer\n",
    "  single_pass_handler - return reconstructed image, use for loss \n",
    "  loss_handler - loss function \n",
    "  epoch - num of epochs\n",
    "  log_interval - output interval\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train(\n",
    "    enc,\n",
    "    dec,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    single_pass_handler,\n",
    "    loss_handler,\n",
    "    epoch,\n",
    "    log_interval=500,\n",
    "):\n",
    "    for batch_idx, (data, lab) in enumerate(loader):\n",
    "        batch_size = data.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        lab = lab.to(device)\n",
    "\n",
    "        latent, output = single_pass_handler(\n",
    "            encoder, decoder, data, lab\n",
    "        )  # reconstructed image drom decoder\n",
    "\n",
    "        loss = loss_handler(data, output, latent)  # compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(loader.dataset),\n",
    "                    100.0 * batch_idx / len(loader),\n",
    "                ).ljust(40),\n",
    "                \"Loss: {:.6f}\".format(loss.item()),\n",
    "            )\n",
    "\n",
    "\n",
    "def ae_pass_handler(encoder, decoder, data, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    recon = decoder(latent)\n",
    "    return latent, recon\n",
    "\n",
    "\n",
    "def ae_loss_handler(data, recon, *args, **kwargs):\n",
    "    return F.binary_cross_entropy(recon, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим загрузчики наших данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим нашу нейросеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from itertools import chain\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2  # size of latent space\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на архитектуру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print(\">>> Encoder\")\n",
    "print(summary(encoder, (1, 28, 28)))\n",
    "print(\">>> Decoder\")\n",
    "print(summary(decoder, (1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим в течение 5 эпох:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, чтобы удобно прогонять датасет через обученную нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function return transforms results to numpy for visualization \n",
    "\n",
    "encoder - encoder\n",
    "decoder - decoder\n",
    "loader - loader of data\n",
    "single_pass_handler - return latent and reconstruction transform\n",
    "return_real - return original images, True/False, default = True\n",
    "return_recon - return transformed image from decoder, True/False, default = True\n",
    "return_latent - return latent representation from encoder, True/False, default = True\n",
    "return_labels - return labels, True/False, default = True\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_eval(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    loader,\n",
    "    single_pass_handler,\n",
    "    return_real=True,\n",
    "    return_recon=True,\n",
    "    return_latent=True,\n",
    "    return_labels=True,\n",
    "):\n",
    "    if return_real:\n",
    "        real = []\n",
    "    if return_recon:\n",
    "        reconstr = []\n",
    "    if return_latent:\n",
    "        latent = []\n",
    "    if return_labels:\n",
    "        labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(loader):\n",
    "            if return_labels:\n",
    "                labels.append(label.numpy())\n",
    "            if return_real:\n",
    "                real.append(data.numpy())\n",
    "\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            rep, rec = single_pass_handler(encoder, decoder, data, label)\n",
    "\n",
    "            if return_latent:\n",
    "                latent.append(rep.cpu().numpy())\n",
    "            if return_recon:\n",
    "                reconstr.append(rec.cpu().numpy())\n",
    "\n",
    "    result = {}\n",
    "    if return_real:\n",
    "        real = np.concatenate(real)\n",
    "        result[\"real\"] = real.squeeze()\n",
    "    if return_latent:\n",
    "        latent = np.concatenate(latent)\n",
    "        result[\"latent\"] = latent\n",
    "    if return_recon:\n",
    "        reconstr = np.concatenate(reconstr)\n",
    "        result[\"reconstr\"] = reconstr.squeeze()\n",
    "    if return_labels:\n",
    "        labels = np.concatenate(labels)\n",
    "        result[\"labels\"] = labels\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала оценим то, как ведет себя наш автоэнкодер и как работает в целом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(run_res[\"real\"][0:9], run_res[\"reconstr\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, какое латетное представление он выучил. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res[\"latent\"], run_res[\"labels\"], title=\"AE manifold (latent_dim=2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь обучим автоэнкодер с латентным слоем размера 24 и посмотрим, как он будет бороться с шумом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем dataloader, который добавляет в наш датасет шум автоматически"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0.0, std=1.0):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(mean={0}, std={1})\".format(\n",
    "            self.mean, self.std\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим MNIST с добавленным шумом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "test_noise_set = MNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(), AddGaussianNoise(0.0, 0.30)]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_noised_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_noise_set, list(range(64))),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(run_res[\"real\"][0:9], run_res[\"reconstr\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сжатия мы оценили визуально выше. Если обратить внимание, то можно заметить, что исходные картинки даже почистились от мелких шумов и странностей изображения и больше стали похожи на непрерывные линии. Размерность латентного пространства `latent_dim` равна 24, что значительно меньше исходного количества признаков (784), поэтому мы получили неплохое сжатие изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо понижения размерности и очистки данных от шума, автоэнкодеры имеют еще несколько полезных способов применения такие, как **обнаружение аномалий** и **предобучение на неразмеченных данных**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обнаружение аномалий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы имеем дело с задачей, когда у нас есть много данных, которые можно считать типичными, или \"нормальными\", и небольшое количество данных, являющихся нетипичными, или \"аномальными\". На практике, в нашем распоряжении может вообще не быть аномальных примеров, но мы можем ожидать, что они появятся в будущем, и мы бы хотели, чтобы модель могла отличить аномальные примеры от нормальных.\n",
    "\n",
    "В такой постановке можно **обучить автоэнкодер только на данных, которые мы считаем нормальными**. В тестовую выборку мы включим аномальные примеры, если они имеются, а также некоторое количество нормальных примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/ae_anomaly_detection_data.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать автоэнкодер только на нормальных данных (иллюстрация ниже, слева). При обучении автоэнкодер будет минимизировать ошибку между входом и выходом (*ошибка реконструкции*).\n",
    "\n",
    "Завершив обучение, мы вычисляем значения ошибки реконструкции на обучающих примерах, и строим распределение этих ошибок, по которому мы выбираем порог ошибки реконструкции (threshold). Порог выбирается исходя из задачи и требований к детекции. Он зависит от того, что для нас важнее: чаще находить аномалии или не допускать ложных срабатываний на \"нормальных\" объектах.\n",
    "\n",
    "На этапе применения модели (inference, иллюстрация ниже, справа) мы подаем на обученный автоэнкодер как нормальные, так и аномальные примеры. **Нормальные примеры будут восстанавливаться автоэнкодером с малой ошибкой**, так как подобные примеры встречались во время обучения. Аномальные же примеры не встречались автоэнкодеру во время обучения, и он не сможет их качественно восстанавливать. **Ошибка реконструкции аномальных объектов будет большой.** \n",
    "\n",
    "Сравнивая ошибку реконструкции с вычисленным ранее порогом, мы сможем обнаруживать и отделять аномальные примеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/ae_anomaly_detection_method.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобучение на неразмеченных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одним практическим примером использования автоэнкодеров является предобучение на неразмеченных данных. Пусть мы имеем **большое количество неразмеченных данных и немного размеченных** (semi-supervised подход).\n",
    "\n",
    "Мы можем обучить автоэнкодер на неразмеченных данных, ожидая, что он обучится эффективно представлять данные в латентном пространстве.\n",
    "\n",
    "Затем мы используем **энкодер как предобученный экстрактор признаков**, добавляя к нему дополнительный классификатор, и обучаем такую модель на размеченных данных.\n",
    "\n",
    "Такой подход похож на transfer learning, где мы тоже используем предобученный экстрактор признаков для решения задачи на небольшом количестве размеченных данных. Отличие в том, что **при transfer learning экстрактор признаков обучается на другой задаче с другими данными**, часто даже на данных из другого домена.\n",
    "\n",
    "В случае использования автоэнкодера, предобучение происходит на данных той же природы, и мы можем ожидать, что полученный таким образом экстрактор признаков будет более эффективно представлять данные в латентном пространстве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/ae_pretrain_encoder.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодер как генератор и его ограничения. Плавная интерполяция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоэнкодер переводит объекты в из исходного признакового пространства в латентное пространство меньшей размерности. Мы можем попытаться использовать **обученный декодер как генератор новых данных**: он будет получать на вход некий вектор из латентного пространства и на выходе восстанавливать изображение. При этом мы можем ожидать, что это изображение окажется в некотором смысле похожим на то, на чем учился автоэнкодер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/autoencoder_as_generator.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какое значение вектора выбрать? Мы же никак не управляли латентным пространством. Непонятно какие числа подставлять. Поэтому мы можем выбрать промежуточные значения между представлениями двух исходных изображений в латентном пространстве и получить плавную интерполяцию между изображениями. Постепенно свойства одного изображения будут исчезать, а свойства другого — появляться.\n",
    "\n",
    "Обучим сначала обычный автоэнкодер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем несколько изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space1 = encoder(imgs[labels == 7][0:1].to(device))\n",
    "latent_space2 = encoder(imgs[labels == 6][0:1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_steps = 10\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1.repeat(interp_steps, 1),\n",
    "    latent_space2.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(nrows=1, ncols=interp_steps, figsize=(16, 4))\n",
    "for label in range(0, interp_steps):\n",
    "    figure = iterp_imgs[label].cpu().detach().numpy()\n",
    "    figure = figure.reshape(28, 28)\n",
    "    ax = axs[label]\n",
    "    ax.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы увидеть более плавные изменения, можем сделать видео. Для этого можно использовать уже известный нам OpenCV. Он умеет делать видеофайлы из массивов чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "interp_steps = 200\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1.repeat(interp_steps, 1),\n",
    "    latent_space2.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "\n",
    "resize_coeff = 10\n",
    "imgs = np.squeeze(iterp_imgs.cpu().detach().numpy())\n",
    "size = (imgs.shape[1] * resize_coeff, imgs.shape[2] * resize_coeff)\n",
    "\n",
    "\n",
    "imgs = [\n",
    "    Image.fromarray(np.uint8(img * 255)).resize(size).convert(\"RGB\") for img in imgs\n",
    "]\n",
    "imgs[0].save(\n",
    "    \"ae_img.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    optimize=False,\n",
    "    duration=40,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as iImage\n",
    "\n",
    "iImage(open(\"ae_img.gif\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так себе интерполяция вышла. Старое изображение затухает, а новое изображение появляется. Хочется, чтобы в промежуточных кадрах не было каких-то непонятных очертаний, а изображение было чем-то промежуточным по смыслу между стартовым и конечным изображением.\n",
    "\n",
    "Причина неудачи в том, что в в результате обучения в латентном пространстве возникли зоны, которые умеют декодироваться в хорошие изображения. Но совсем не обязательно между этими зонами будет что-то адекватное (что мы видели из представления)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/autoencoder_as_generator_problem.png\" alt=\"alttext\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим это графически. Пусть наш очень умный, содержащий очень много коэффициентов, энкодер и декодер смог разложить все входные объекты на одной оси (размерность латентного пространства — 1). По сути, он каждому входному изображению присвоил номер и по номеру может это изображение вспомнить. То есть, автоэнкодер очень переобученный. Тогда если мы возьмём промежуточный номер (пытаемся интерполировать), то какое изображение мы собираемся получить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/autoencoder_as_generator_problem_explanation.png\" alt=\"alttext\" width=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы хотим, чтобы декодированные промежуточные латентные состояния имели черты близких к ним объектов, то надо притянуть латентные координаты похожих объектов. Например, вот так:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/irregular_and_regular_latent_space.png\" alt=\"alttext\" width=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариационные автоэнкодеры (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мотивация:\n",
    "\n",
    "Хотим вместо представления слева получить представление справа:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/latent_space_with_and_witout_regularization.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "При этом зоны пересечения должны действительно содержать переходные картины:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/vae_latent_space.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решение с помощью регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем попробовать заставить наши объекты «лежать» рядом — будем штрафовать латентные представления, которые далеко уходят от начала координат. \n",
    "\n",
    "Можем использовать как отдельно L1 или L2 регуляризацию, так и их комбинацию — elastic loss.\n",
    "\n",
    "Однако это приведет просто к масштабированию распределения. Нам надо одновременно получить связное латентное представление, чтобы у нас не возникало зон в латентном представлении, которым не соответствует ничего, и при этом представление, в котором цифры будут отделены друг от друга. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/reconstruction_loss_only.png\" alt=\"alttext\" width=\"450\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\">Intuitively Understanding Variational Autoencoders</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы имеем ситуацию как на картинке слева. Переход-интерполяция между объектами проходит через зону отсутствующих в обучении объектов, декодирование которых даст несуществующие в реальности объекты. Нам не удастся погенерировать новые картинки, преобразовывая случайную точку из латентного пространства в случайную картинку. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация вариационного автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи с автоэнкодером говорит нам, что существует некое пространство меньшей размерности $Z$, которое и обуславливает процесс генерации объектов из $X$. Все остальные различия — следствия случайности: один и тот же человек может по-разному нарисовать цифру 5. \n",
    "\n",
    "Будем искать латентное пространство $Z$, которое удовлетворяет следующему условию:\n",
    "\n",
    "$$p(x) = \\int p(x, z)dz $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, пусть объекты из $Z$ легко генерировать. \n",
    "\n",
    "По формуле совместной вероятности:\n",
    "\n",
    "$$p(x, z) = p(x|z)p(z) $$\n",
    "\n",
    "Осталось только подобрать такие параметры, чтобы все работало.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, сделать это в таком виде не получится. Пространство $X$ может быть высокоразмерным. \n",
    "\n",
    "Но мы можем существенно сузить область поиска, ведь каждому $x$ из пространства $X$ соответствует лишь небольшая возможная область в $Z$.\n",
    "\n",
    "Для этого будем также учить отображение из пространства $X$ в пространство $Z$, т.е., пытаться выучить $p(z|x)$. Назовем функцию, которой будем его приближать $q(z|x)$. \n",
    "\n",
    "Что же в случае автоэнкодера выполняет роль $p(x|z)$ и $q(z|x)$?\n",
    "Очевидно, кодировщик и декодировщик соответственно. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/vae_as_two_functions.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Чтобы все получилось, нужно сделать с кодировщиком две вещи. Заметьте, что декодировщик мы оставим без изменений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первая модификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть наш кодировщик генерирует на основе объекта $X$ вектор средних и вектор стандартных отклонений.\n",
    "\n",
    "Этих двух векторов хватает нам для того, чтобы задать многомерное нормальное распределение с независимыми компонентами (чтобы матрица ковариаций была диагональной), соответствующее данному объекту.\n",
    "\n",
    "Чтобы получить латентное представление объекта, отличающегося от $X$ только в силу случайности, нам достаточно сгенерировать вектор из нормального распределения с такими параметрами. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/vae_architecture_first_modification.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы можем требовать, чтобы из полученного латентного представления декодировшик восстанавливал объект, похожий на исходный. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparametrization trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь, однако, сразу возникает проблема с тем, что граф вычислений, соответствующий предыдущей структуре, не может пропускать градиент — как пропустить градиент через генератор случайного нормального числа? Если считать из определения, то даже малейшему изменению параметра могут соответствовать бесконечные изменения генерируемого числа (нормальное распределение определено на бесконечности).\n",
    "\n",
    "Но мы можем вспомнить замечательное свойство одномерного нормального распределения:\n",
    "\n",
    "$$N(\\mu,\\sigma^2) = N(0,1) * \\sigma + \\mu$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняется это и для многомерного случая. Потому сделаем следующее: будем генерировать значение из нормального распределения со средними 0 и дисперсиями 1, а затем домножать это на вектор стандартных отклонений и прибавлять вектор средних. Получится вот такое преобразование, которое называется **reparametrization trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/reparametrization_trick.png\" alt=\"alttext\" width=\"850\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от левого случая, в правом мы спокойно можем пропустить градиент через детерминистичные ноды. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но если просто применить такой принцип, то он снова имеет проблему предыдущего детерминистического подхода, так как вероятностное распределение сможет свернуться в дельта-функцию — зачем нейросети мучиться с объектами, немного отличающимися от тех, что есть в обучающей выборке, и пытаться нормально их восстанавливать, если можно просто начать генерировать стандартные отклонения, близкие к нулю, и тем самым получить $\\delta$-функцию, которая будет нашему объекту всегда сопоставлять одну точку в латентном представлении. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/Dirac_function_approximation.gif\" alt=\"alttext\" width=\"240\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вторая модификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому нам надо ввести регуляризацию, требующую от каждого распределения быть близким к нормальному распределению вокруг нуля координат латентного пространства с дисперсией 1 (наше $P(z)$). \n",
    "\n",
    "Для этого нам нужна некая мера расстояния между двумя вероятностными распределениями. В базовом случае в качестве такой меры расстояния используется **дивергенция Кульбака-Лейблера**, или KL-дивергенция."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дивергенция Кульбака-Лейблера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Дивергенция Кульбака-Лейблера](https://ru.wikipedia.org/wiki/Расстояние_Кульбака_—_Лейблера) между двумя вероятностными распределениями $P$ и $Q$ определяется следующим образом: \n",
    "\n",
    "$$KL(P||Q) = \\int_X p(x)\\log \\dfrac {p(x)} {q(x)} dx$$\n",
    "\n",
    "В теории информации $p$ считается целевым (истинным) распределением, а $q$ — тем, с которым мы его сравниваем (проверяемым). \n",
    "Важно понимать, что $KL$ не является мерой расстояния, т.к. в общем случае $KL(P||Q) \\neq KL(Q||P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Только KL-дивергенция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы распределение $Q(z|x)$ в латентном пространстве $Z$ походило на нормальное, мы будем минимизировать KL-дивергенцию между ним и стандартным нормальным распределением $N(0,1)$.\n",
    "\n",
    "$$ Loss =  KL(Q(z|x)||N(0,1)) $$\n",
    "\n",
    "Данное выражение может быть [записано аналитически](https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes):\n",
    "\n",
    "\n",
    "$$KL(N(\\mu, \\sigma) || N(0, 1)) = -\\frac {1} {2}(\\log {\\sigma^2} - \\sigma^2 - \\mu^2 + 1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/pure_kl_loss.png\" alt=\"alttext\" width=\"350\">\n",
    "\n",
    "<left><p><em>Source: <a href=\"https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\">Intuitively Understanding Variational Autoencoders</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Видим, что мы забываем про декодировщик — он может выдавать все, что угодно. Потому логично ожидать, что обучится только кодировщик, и обучится он отражать наши точки в нормальное распределение со средним 0 и дисперсией 1. И все, большего ему в жизни и не надо. Можем проверить это. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class VAEEncoder(Encoder):\n",
    "    def __init__(self, latent_dim):\n",
    "        if latent_dim % 2 != 0:  # check for the parity of the latent space\n",
    "            raise Exception(\"Latent size for VAEEncoder must be even\")\n",
    "        super().__init__(latent_dim)\n",
    "\n",
    "\n",
    "def vae_split(latent):\n",
    "    size = latent.shape[1] // 2  # divide the latent representation into mu and log_var\n",
    "    mu = latent[:, :size]\n",
    "    log_var = latent[:, size:]\n",
    "    return mu, log_var\n",
    "\n",
    "\n",
    "def vae_reparametrize(mu, log_var):\n",
    "    sigma = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn(mu.shape[0], mu.shape[1]).to(device)\n",
    "    return eps * sigma + mu\n",
    "\n",
    "\n",
    "def vae_pass_handler(encoder, decoder, data, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    mu, log_var = vae_split(latent)\n",
    "    sample = vae_reparametrize(mu, log_var)\n",
    "    recon = decoder(sample)\n",
    "    return latent, recon\n",
    "\n",
    "\n",
    "def kld_loss(mu, log_var):\n",
    "    var = log_var.exp()\n",
    "    kl_loss = torch.mean(-0.5 * torch.sum(log_var - var - mu**2 + 1, dim=1), dim=0)\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "def kl_loss_handler(data, recon, latent, kld_weight=0.1, *args, **kwargs):\n",
    "    mu, log_var = vae_split(latent)\n",
    "    kl_loss = kld_loss(mu, log_var)\n",
    "    return kld_weight * kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 3):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=kl_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae_split(run_res[\"latent\"])\n",
    "var = np.exp(log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все генерируемые средние почти неотличимы от нуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mu.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все генерируемые дисперсии почти неотличимы от 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var.ravel());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получили практически неразделимые объекты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "pal = sns.color_palette(\"Paired\", n_colors=10)\n",
    "plot_manifold(mu, run_res[\"labels\"], title=\"Manifold mu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещаем ошибку восстановления и KL-дивергению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы должны сохранить исходный loss — декодировщик штрафуется за то, что не может нормально реконструировать объект. \n",
    "\n",
    "Формально это записывается следующим образом: \n",
    "\n",
    "$$ vae\\_loss = E_{z \\sim Q(z|x)}[logP(x|z)] + KL[Q(z|x)||N(0,1)]$$\n",
    "\n",
    "А в итоге:\n",
    "\n",
    "$$ vae\\_loss = BCE(x , \\tilde{x}) -\\frac {1} {2}(\\log {\\sigma^2} - \\sigma^2 - \\mu^2 + 1)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Вторая компонента осталась без изменений, а первая — это красиво записанное требование корректно восстанавливать объекты из обучающей выборки и чтобы при этом объекты, полученные их небольшим изменением за счет случайности, также восстанавливались в объекты, близкие к объектам из тренировочной выборки. И удовлетворять этой компоненте loss мы можем за счет того же loss, который использовали в обычном автоэнкодере. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учет обеих компонент позволяет нам получить то, что мы хотели — непрерывное пространство, где нет «дыр» в представлении, и при этом близкие по смыслу объекты расположены рядом, а далекие — далеко. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/kl_repr_loss.png\" alt=\"alttext\" width=\"350\">\n",
    "\n",
    "<left><p><em>Source: <a href=\"https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\">Intuitively Understanding Variational Autoencoders</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем наш новый loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_handler(data, recons, latent, kld_weight=0.005, *args, **kwargs):\n",
    "    mu, log_var = vae_split(latent)\n",
    "    kl_loss = kld_loss(mu, log_var)\n",
    "    # add bce loss(reconstruction)\n",
    "    loss = F.binary_cross_entropy(recons, data) + kld_weight * kl_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим наш VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae_split(run_res[\"latent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette(\"Paired\", n_colors=10)\n",
    "plot_manifold(mu, run_res[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что цифры разделились в пространстве, но при этом жмутся друг к другу. При этом, что интересно, 4 и 9 почти неотличимы. Это можно объяснить тем, что двух компонент недостаточно, чтобы разделить настолько похожие цифры (по сути, все отличие в заполненности области между двумя рожками 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как теперь получится интерполировать между 7 и 6. Для большей красоты и сравнимости с обычным автоэнкодером картин возьмем latent space такого же размера, как у него (24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "latent_space1_mu, _ = vae_split(encoder(imgs[labels == 7][0:1].to(device)))\n",
    "latent_space2_mu, _ = vae_split(encoder(imgs[labels == 6][0:1].to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_steps = 10\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1_mu.repeat(interp_steps, 1),\n",
    "    latent_space2_mu.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "_, axs = plt.subplots(nrows=1, ncols=interp_steps, figsize=(16, 4))\n",
    "for label in range(0, interp_steps):\n",
    "    figure = iterp_imgs[label].cpu().detach().numpy()\n",
    "    figure = figure.reshape(28, 28)\n",
    "    ax = axs[label]\n",
    "    ax.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим плавную интерполяцию. Посмотрим на примере с видео."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "interp_steps = 200\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1_mu.repeat(interp_steps, 1),\n",
    "    latent_space2_mu.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "\n",
    "\n",
    "resize_coeff = 10\n",
    "imgs = np.squeeze(iterp_imgs.cpu().detach().numpy())\n",
    "size = (imgs.shape[1] * resize_coeff, imgs.shape[2] * resize_coeff)\n",
    "\n",
    "\n",
    "imgs = [\n",
    "    Image.fromarray(np.uint8(img * 255)).resize(size).convert(\"RGB\") for img in imgs\n",
    "]\n",
    "imgs[0].save(\n",
    "    \"vae_img.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    optimize=False,\n",
    "    duration=40,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as iImage\n",
    "\n",
    "iImage(open(\"vae_img.gif\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все переходы понятны, и в процессе не возникает невозможных цифр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы используем размерность латентного пространства 2, то это позволит нам получать распределение классов цифр на плоскости, типа такого:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/vae_sampling.png\" alt=\"alttext\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Это не просто интерполяция по двум направлениям. Тут именно все 10 цифр должны так занять место на плоскости, чтобы плавно перетекать друг в друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, наконец, что вариационный автоэнкодер работает как автоэнкодер и может, к примеру, убирать шум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_loader, vae_pass_handler)\n",
    "plot_samples(run_res[\"real\"][0:9], run_res[\"reconstr\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что автоэнкодер работает, пусть не лучше, чем обычный, возможно, даже хуже. Можно добиться улучшения его работы, поставив меньший вес KL-дивергенции и увеличив латентное пространство. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично посмотрим, как он восстанавливает изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)\n",
    "plot_samples(run_res[\"real\"][0:9], run_res[\"reconstr\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает, но изображения получаются \"размытыми\". Это следствие сэмплирования из нормального распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторная арифметика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе, можно даже в латентном пространстве брать разницу черт написания двух одинаковых цифр, прибавлять к другой цифре, получая в результате цифру, написанную немного по-другому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Такое можно делать и для других примеров — добавлять людям на изображении очки. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/vector_arithmetic_add_new_property.png\" alt=\"alttext\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "или получать нечто среднее между двумя объектами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/vector_arithmetic_get_half_property.png\" alt=\"alttext\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее: \n",
    "\n",
    "У нас есть 1, написанная без наклона и 1, написанная с наклоном. \n",
    "И у нас есть 9 без наклона.\n",
    "\n",
    "Вычитаем из девятки единицу без наклона и прибавляем единицу с наклоном. Если все пройдет хорошо, получим девятку с наклоном.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/vector_arithmetic_example.png\" alt=\"alttext\" width=\"550\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем это сделать сами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "real_9_straight = imgs[labels == 9][6:7]  # find some straight \"nine\"\n",
    "real_1_straight = imgs[labels == 1][3:4]  # find some straight \"one\"\n",
    "real_1_tilted = imgs[labels == 1][0:1]    # find some tilted \"one\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "Image.fromarray(np.uint8(np.squeeze(real_9_straight.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(real_1_straight.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(real_1_tilted.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_9_straight, _ = vae_split(encoder(real_9_straight.to(device)))\n",
    "latent_1_straight, _ = vae_split(encoder(real_1_straight.to(device)))\n",
    "latent_1_tilted, _ = vae_split(encoder(real_1_tilted.to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_9_tilted = latent_9_straight - latent_1_straight + latent_1_tilted\n",
    "gen_9_tilted = decoder(latent_9_tilted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(gen_9_tilted.cpu().detach().numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось перенести наклон единицы на девятку. Получилось неплохо. Обычно для получения возможности использовать векторную арифметику, используют специальные функции потерь и архитектуры. Простой VAE не гарантирует того, что \"фокус удастся\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблемы  «ванильного» VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из проблем VAE, с которой можно столкнуться, состоит в том, что две компоненты функции потерь конфликтуют друг с другом. Если будет доминировать KL-loss, то мы получим представление, из которого наши объекты очень плохо восстанавливаются — они раскиданы по представлению, как угодно. \n",
    "\n",
    "Если же, наоборот, будет доминировать reconstruction loss, то мы получим ситуацию, в которой объекты восстанавливаются нормально, но никакой непрерывностью и не пахло. \n",
    "\n",
    "Проблема возникает и с самой KL-дивергенцией, у которой есть ряд существенных недостатков. Есть другие способы оценки близости двух распределений, которые порой дают лучшие результаты. К ним относится дивергенция Йенсена — Шеннона, которую мы вскользь затронем далее, и метрика Вассерштейна (используется в Wasserstein autoencoders), изучение которой выходит за рамки курса. \n",
    "\n",
    "Кроме того, в случае, когда декодировщик содержит значительно больше параметров, нежели кодировщик, может возникать ситуация, при которой сгенерированное латентное представление игнорируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодеры с условием (CAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мотивация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как, используя обычный VAE, сгенерировать картинку с заданной меткой? \n",
    "\n",
    "На самом деле, задача нетривиальна. Как вариант, мы можем понять, в какую область латентного пространства VAE отображает все 0 и затем сэмплировать уже из этой области. \n",
    "\n",
    "Хорошо, а если мы хотим нарисовать 1 тем же почерком, которым нарисована данная нам тройка? В этом случае классический VAE вообще не получится использовать. \n",
    "\n",
    "На самом деле, есть еще одна проблема. Что если распределение объектов действительно сильно зависит от какой-то дополнительной информации, например, того, какую цифру хотел изобразить человек? Тогда KL-loss будет пытаться «скрестить ежа с ужом», и в результате мы получим очень странное представление. И опять же, на границах могут получаться несуществующие в реальности мутанты (если внимательно посмотрите на предыдущую картинку — так и получается). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Несвязные компоненты и автокодировщик с условиями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрировать мы это можем на модельной задаче. Сгенерируем два несвязных набора точек в двумерном пространстве, каждый из которых представляет собой некий паттерн с добавленным шумом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# create dataset\n",
    "x1 = np.linspace(-2.2, 2.2, 2000)\n",
    "fx = np.sin(x1)\n",
    "dots1 = np.vstack([x1, fx]).T\n",
    "\n",
    "t = np.linspace(0, 2 * np.pi, num=2000)\n",
    "dots2 = 0.5 * np.array([np.sin(t), np.cos(t)]).T + np.array([1.5, -0.5])[None, :]\n",
    "\n",
    "dots = np.vstack([dots1, dots2])\n",
    "noise = 0.06 * np.random.randn(*dots.shape)\n",
    "\n",
    "labels = np.array([0] * x1.shape[0] + [1] * t.shape[0])\n",
    "noised = dots + noise\n",
    "\n",
    "\n",
    "# Visualization\n",
    "colors = [\"b\"] * x1.shape[0] + [\"g\"] * t.shape[0]\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простой автоэнкодер на полносвязных слоях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлых примерах мы этим пренебрегали, но, строго говоря, автоэнкодер тоже может переобучаться. Потому сделаем разбиение на обучение и тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(noised, test_size=0.25, random_state=42)\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "x_test = torch.from_numpy(x_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сильно не мучаться, поставим просто scheduler, который автоматически уменьшает learning rate нашей сети, если она переобучается или просто не улучшает качество на валидационном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encdec = SimpleEncoderDecoder()\n",
    "optimizer = optim.Adam(encdec.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \"min\", patience=50\n",
    ")  # to optimize learning rate\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5000)):\n",
    "    optimizer.zero_grad()\n",
    "    x_restored = encdec(x_train)\n",
    "    loss = criterion(x_train, x_restored)\n",
    "    loss.backward()\n",
    "    if optimizer.param_groups[0][\"lr\"] < 10e-7:  # if learning step becomes too small\n",
    "        print(epoch)\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_restored = encdec(x_test)\n",
    "        val_loss = criterion(x_test, x_restored)\n",
    "    scheduler.step(val_loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_restored = encdec(x_test)\n",
    "    dots_restored = x_restored.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что выучил автоэнкодер. Видим, что он показывает связь там, где она явно отсутствует. Требование получить одно и то же представление для точек из двух паттернов мешает автоэнкодеру нормально выучить эти паттерны — они получаются смазанными или даже неверными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.scatter(dots_restored[:, 0], dots_restored[:, 1], color=\"grey\", linewidth=4)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что наш автоэнкодер восстанавливает часть объектов в область, где ничего нет. Потому что у него нет возможности понять, что это две несвязные компоненты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что будет, если мы будем передавать в кодировщик и в декодировщик метку объекта? \n",
    "Тогда окажется, что наш автокодировщик работает в разы лучше:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConditionalEncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat(\n",
    "            [x, y.view(-1, 1)], dim=1\n",
    "        )  # combine the labels with X, change the dimension of the labels\n",
    "        z = self.encoder(x)\n",
    "        x = torch.cat([z, y.view(-1, 1)], dim=1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    noised, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "encdec = SimpleConditionalEncoderDecoder()\n",
    "optimizer = optim.Adam(encdec.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=50)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5000)):\n",
    "    optimizer.zero_grad()\n",
    "    x_restored = encdec(x_train, y_train)\n",
    "    loss = criterion(x_train, x_restored)\n",
    "    loss.backward()\n",
    "    if optimizer.param_groups[0][\"lr\"] < 10e-7:\n",
    "        print(epoch)\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_restored = encdec(x_test, y_test)\n",
    "        val_loss = criterion(x_test, x_restored)\n",
    "    scheduler.step(val_loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_restored = encdec(x_test, y_test)\n",
    "    dots_restored = X_restored.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.scatter(dots_restored[:, 0], dots_restored[:, 1], color=\"grey\", linewidth=4)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ситуация стала лучше. То, что мы применили, называется условными автоэнкодерами (Conditional AE). Конкретно — вместе с признаковым описанием объекта мы также передаем метки, которые указывают на то, что он относится к каким-то важным группам объектов, для которых, возможно, сети нужно учить отличное от других представление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Условные вариационные автоэнкодеры (CVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация вариационного автоэнкодера с условиями, CVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычные автокодировщики с условиями применяются редко — они по-прежнему не гарантируют нам связность представления в пределах одной метки. \n",
    "\n",
    "Однако добавление меток в вариационный автокодировщик часто помогает решать уже описанные задачи на хорошем уровне. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/cvae_scheme.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как подмешивать метку к изображению, чтобы передать это полностью свёрточной нейронной сети — не очевидно. Да обычно и не надо нам это. Часто достаточно передавать метку только декодеру. Энкодер имеет в распоряжении изначальный объект — при желании может предсказать его метку сам.\n",
    "\n",
    "Напишем код для CVAE. По сути надо поменять только декодер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dims = [512, 256, 128, 64, 32]\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=latent_dim + 10,\n",
    "            out_features=hidden_dims[0],  # add +10(num of labels) to latent space\n",
    "        )\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        hidden_dims[i],\n",
    "                        hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    hidden_dims[-1],\n",
    "                    hidden_dims[-1],\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(hidden_dims[-1], out_channels=1, kernel_size=7, padding=1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x, lab):\n",
    "        x = torch.cat([x, lab], dim=1)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, 512, 1, 1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_pass_handler(encoder, decoder, data, label, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    mu, log_var = vae_split(latent)\n",
    "    sample = vae_reparametrize(mu, log_var)\n",
    "    label = torch.nn.functional.one_hot(label, num_classes=10)  # labels to ohe\n",
    "    recon = decoder(sample, label)\n",
    "    return latent, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-2\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = CDecoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        enc=encoder,\n",
    "        dec=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=cvae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, cvae_pass_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавив передачу метки в декодер, мы позволили автоэнкодеру отображать все цифры в \"одно место\". За счет этого ему легче стало учиться, loss стал чуть ниже. \n",
    "\n",
    "При этом, если нарисовать латентное представление для всех наших цифр разом, получится комок, сосредоточенный в области нормального распределения. Это не значит, что оно плохое. Просто наша картина не учитывает, что нейросеть различает цифры теперь по меткам, а не по латентному представлению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res[\"latent\"], run_res[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у каждой цифры \"свое\" нормальное распределение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res[\"latent\"][run_res[\"labels\"] == 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res[\"latent\"][run_res[\"labels\"] == 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация заданных цифр из латентного распределения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядит наше латентное представление, скажем, для четверок, которых мы до этого почти не видели (сливались с 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20\n",
    "space1 = torch.linspace(-2, 2, steps)\n",
    "space2 = torch.linspace(-2, 2, steps)\n",
    "grid = torch.cartesian_prod(space1, space2)\n",
    "label = torch.full((grid.shape[0],), 4)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)\n",
    "with torch.no_grad():\n",
    "    imgs = decoder(grid.to(device), label.to(device))\n",
    "    imgs = imgs.cpu().numpy().squeeze()\n",
    "\n",
    "plot_samples(\n",
    "    *[imgs[x : x + steps] for x in range(0, steps * steps, steps)], single_size=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, как у нас четверки плавно расположены по стилю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20\n",
    "space1 = torch.linspace(-2, 2, steps)\n",
    "space2 = torch.linspace(-2, 2, steps)\n",
    "grid = torch.cartesian_prod(space1, space2)\n",
    "label = torch.full((grid.shape[0],), 9)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)\n",
    "with torch.no_grad():\n",
    "    imgs = decoder(grid.to(device), label.to(device))\n",
    "    imgs = imgs.cpu().numpy().squeeze()\n",
    "\n",
    "plot_samples(\n",
    "    *[imgs[x : x + steps] for x in range(0, steps * steps, steps)], single_size=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При желании можно посмотреть на процесс того, как нейросеть все это учит.\n",
    "\n",
    "Ниже приведено то, как хорошо выполняют задачу генерации заданной цифры нейросеть по мере обучения, и то, как выглядит латентное представление объектов, относящихся к данной цифре. \n",
    "\n",
    "Для цифры 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/gen_cvae_3.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/lat_cvae_3.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для цифры 7:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/gen_cvae_7.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/lat_cvae_7.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация заданных цифр с переносом стиля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также успешно такая нейросеть справится в задаче, где мы используем латентное представление одной цифры  для того, чтобы сгенерировать цифру с таким же стилем написания. \n",
    "\n",
    "Чтобы сделать это, достаточно просто получить латентное представление для 7, а затем передать его в декодер с меткой 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/generation_indicated_digits_from_latent_space.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты переноса стилей для нескольких разных 7 представлены ниже.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/style_transfer_7.png\" alt=\"alttext\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем то же самое для наших двоек и пятерок. \n",
    "Выберем двойку и сгенерим несколько 5 с ее стилем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "real = imgs[labels == 2][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "Image.fromarray(np.uint8(np.squeeze(real.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "mu, log_var = vae_split(encoder(real.to(device)))\n",
    "sigma = torch.exp(0.5 * log_var)\n",
    "z = torch.randn(sample_size, mu.shape[1]).to(device)\n",
    "latent = z * sigma + mu\n",
    "\n",
    "label = torch.full((sample_size,), 5)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = decoder(latent.to(device), label.to(device))\n",
    "    imgs = np.squeeze(imgs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Состязательные автокодировщики (AAE = AE + GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе курса вы уже познакомились с генеративными состязательными сетями. Возникает искушение как-то использовать принципы, лежащие в их основе, для VAE. Действительно, так делают. Такие нейросети называются **adversarial autoencoders**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать мы будем дискриминатор. К какой части нейросети мы можем его «приделать»? На самом деле — к любой. Но самое распространенное — а давайте уберем наши мучения с KL-дивергенцией. Пусть теперь дискриминатор будет отличать латентное представление, которое мы сгенерировали, от стандартного нормального распределения. Если может отличить хорошо, то штрафуем энкодер за это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/adversarial_autoencoder_scheme.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внезапно это избавляет нас от необходимости даже делать какие-то дополнительные манипуляции с кодировщиком — нам не нужно теперь, чтобы он выдавал средние и дисперсии, а потом мы использовали их для генерации объектов из реального распределения. Нам достаточно того, что дискриминатор не может отличить латентное представление, которое мы получаем, от нормального распределения. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, оказывается, что AAE может генерировать более «качественные» объекты, нежели ванильный VAE. Можно теоретически показать, что это является следствием того, что он минимизирует не KL-divergence, а более эффективную дивергенцию Йенсена-Шеннона. Однако подробный разбор этого выходит за рамки курса. \n",
    "\n",
    "Кроме того, для AAE легче добиться того, чтобы ваше латентное представление выглядело специфичным образом. Просто генерируем выборку из нужного распределения и говорим дискриминатору пытаться отличать сгенерированную выборку от объектов из латентного представления. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/different_latent_spaces.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "И все это без введения дополнительной функции потерь! Фактически GAN и есть наша loss function. Мы используем выход одной нейросети в качестве функции потерь для обучения другой нейросети. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим дискриминатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.discriminator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какое бы латетное распределение нам выбрать? Давайте выберем равномерное распределение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform(shape):  # U[-2.5, 2.5]\n",
    "    return torch.rand(*shape) * 5 - 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "manifold = generate_uniform((10000, 2))\n",
    "plot_manifold(manifold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем части нашей нейросети. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "discriminator = Discriminator(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# optimizer for AE, how well it restore the image\n",
    "ae_optimizer = optim.Adam(\n",
    "    chain(\n",
    "        encoder.parameters(),\n",
    "        decoder.parameters(),\n",
    "    ),\n",
    "    weight_decay=weight_decay,\n",
    "    lr=learning_rate,\n",
    ")\n",
    "# optimizer for generator, how well generates real images\n",
    "gen_optimizer = optim.Adam(\n",
    "    encoder.parameters(), weight_decay=weight_decay, lr=learning_rate\n",
    ")\n",
    "# optimizer for discriminator\n",
    "dis_optimizer = optim.Adam(\n",
    "    discriminator.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы увеличиваем размер батча для лучшей сходимости. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 512\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После каждого шага будем визуализировать латентное представление. \n",
    "Обучение нейросети идет не очень быстро — у нас считаются сразу три функции потерь, по которым делаются три обновления сети за шаг. \n",
    "Кроме того, мы сделали большой размер батча реальных примеров равномерного распределения для дискриминатора, чтобы ему легче было изучить латентное пространство, что также улучшает сходимость."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# simple uniform\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for ind, batch in tqdm(enumerate(train_loader)):\n",
    "        # testing AE\n",
    "        ae_optimizer.zero_grad()\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        latent = encoder(imgs)\n",
    "        restored = decoder(latent)\n",
    "        ae_loss = F.binary_cross_entropy(restored, imgs)\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        # train encoder(generator)\n",
    "        latent = encoder(imgs)  # generating a latent distribution from encoder\n",
    "        gen_optimizer.zero_grad()\n",
    "        neg_pred = discriminator(\n",
    "            latent\n",
    "        )  # discriminator predicts ones for this latent space\n",
    "        fake_labs = torch.ones(latent.shape[0]).view(-1, 1).to(device)  # generate ones\n",
    "        gen_loss = F.binary_cross_entropy(neg_pred, fake_labs) * 0.01  # compute loss\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # train discriminator\n",
    "        dis_optimizer.zero_grad()\n",
    "        negative = latent.detach()  # use detach(fake and real values are equivalent)\n",
    "        positive = generate_uniform(\n",
    "            (\n",
    "                latent.shape[0]\n",
    "                * 20,  # huge sample to sample all latent space and prevent racing between discriminator and generator\n",
    "                latent.shape[1],\n",
    "            )\n",
    "        ).to(device)\n",
    "        neg_pred = discriminator(negative)\n",
    "        pos_pred = discriminator(positive)\n",
    "\n",
    "        dis_labs = (\n",
    "            torch.cat([torch.zeros(negative.shape[0]), torch.ones(positive.shape[0])])\n",
    "            .view(-1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        pred_lab = torch.cat([neg_pred, pos_pred])\n",
    "        dis_loss = F.binary_cross_entropy(pred_lab, dis_labs)\n",
    "\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res = run_eval(encoder, decoder, test_loader, ae_pass_handler)\n",
    "        plot_manifold(res[\"latent\"], res[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = run_eval(encoder, decoder, test_loader, ae_pass_handler)\n",
    "    plot_manifold(res[\"latent\"], res[\"labels\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что дискриминатор уделяет бОльшее внимание форме распределения, нежели его точным границам. \n",
    "Это отчасти обусловлено слоями BatchNorm в нашей сети. Если поучить нейросеть большее число эпох, то латентное представление \"засунется\" в требуемые границы.\n",
    "\n",
    "Построим картинки из этого латентного представления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin = res[\"latent\"].min(axis=0)\n",
    "xmax, ymax = res[\"latent\"].max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 50\n",
    "space1 = torch.linspace(xmin, xmax, steps)\n",
    "space2 = torch.linspace(ymin, ymax, steps)\n",
    "grid = torch.cartesian_prod(space1, space2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    imgs = decoder(grid.to(device))\n",
    "    imgs = imgs.cpu().numpy().squeeze()\n",
    "\n",
    "plot_samples(\n",
    "    *[imgs[x : x + steps] for x in range(0, steps * steps, steps)], single_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первый взгляд, получилась хорошая интерполяция. Как минимум, не хуже той, что нам давал обычный VAE.\n",
    "\n",
    "Теперь же покажем, что в принципе мы можем выбрать любую форму латентного представления, которая нам будет удобна. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a distribution generator\n",
    "def generator_uniform(size, xmin, xmax, ymin, ymax):\n",
    "    x = torch.FloatTensor(size, 1).uniform_(xmin, xmax)\n",
    "    y = torch.FloatTensor(size, 1).uniform_(ymin, ymax)\n",
    "    return torch.cat([x, y], dim=1)\n",
    "\n",
    "\n",
    "# оcombining distributions into an object\n",
    "def generate_on_grid(size_for_bound, grid, generator):\n",
    "    samples = []\n",
    "    for boundaries in grid:\n",
    "        s = generator(size_for_bound, *boundaries)\n",
    "        samples.append(s)\n",
    "    sample = torch.cat(samples, dim=0)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничто не мешает нам требовать вот такое латентное представление (кроме того, что выбранное здесь оказывается не непрерывным, поэтому идеальное отображение мы получим вряд ли)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# for uniforms\n",
    "# fmt: off\n",
    "grids = [\n",
    "    (x1, x2, x3, x4)\n",
    "    for (x1, x2), (x3, x4) in product([[-1.5, -0.7], [-0.5, 0.3], [0.5, 1.3]], \n",
    "                                      [[-1.5, -0.7], [-0.5, 0.3], [0.5, 1.3]])\n",
    "    ]\n",
    "# fmt: on\n",
    "grids.append((-0.5, 0.3, -2.3, -1.70))\n",
    "\n",
    "\n",
    "plot_manifold(generate_on_grid(516, grids, generator_uniform).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "discriminator = Discriminator(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "ae_optimizer = optim.Adam(\n",
    "    chain(\n",
    "        encoder.parameters(),\n",
    "        decoder.parameters(),\n",
    "    ),\n",
    "    weight_decay=weight_decay,\n",
    "    lr=learning_rate,\n",
    ")\n",
    "gen_optimizer = optim.Adam(\n",
    "    encoder.parameters(), weight_decay=weight_decay, lr=learning_rate\n",
    ")\n",
    "dis_optimizer = optim.Adam(\n",
    "    discriminator.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    for ind, batch in tqdm(enumerate(train_loader)):\n",
    "        # train AE\n",
    "        ae_optimizer.zero_grad()\n",
    "        imgs, labels = batch\n",
    "        imgs = imgs.to(device)\n",
    "        latent = encoder(imgs)\n",
    "        labels = torch.nn.functional.one_hot(labels, num_classes=10).to(device)\n",
    "        restored = decoder(latent)\n",
    "        ae_loss = F.binary_cross_entropy(restored, imgs)\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        # train encoder(generator)\n",
    "        latent = encoder(imgs)\n",
    "        gen_optimizer.zero_grad()\n",
    "        neg_pred = discriminator(latent)\n",
    "        fake_labs = torch.ones(latent.shape[0]).view(-1, 1).to(device)\n",
    "        gen_loss = F.binary_cross_entropy(neg_pred, fake_labs) * 0.01\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # train dicriminator\n",
    "        dis_optimizer.zero_grad()\n",
    "        negative = latent.detach()\n",
    "        positive = generate_on_grid(128, grids, generator_uniform).to(device)\n",
    "        neg_pred = discriminator(negative)\n",
    "        pos_pred = discriminator(positive)\n",
    "\n",
    "        dis_labs = (\n",
    "            torch.cat([torch.zeros(negative.shape[0]), torch.ones(positive.shape[0])])\n",
    "            .view(-1, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        pred_lab = torch.cat([neg_pred, pos_pred])\n",
    "        dis_loss = F.binary_cross_entropy(pred_lab, dis_labs)\n",
    "\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res = run_eval(encoder, decoder, test_loader, ae_pass_handler)\n",
    "        plot_manifold(res[\"latent\"], res[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()\n",
    "with torch.no_grad():\n",
    "    res = run_eval(encoder, decoder, test_loader, ae_pass_handler)\n",
    "    plot_manifold(res[\"latent\"], res[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе, получаем что-то похожее на то, что мы хотели. Правда, результат зависит от запуска, фазы луны и т.д. — GAN капризны.\n",
    "\n",
    "Сможете попробовать подобное в домашнем задании.\n",
    "\n",
    "Балансируя вес лосса генератора, можно это представление делать более/менее похожим. Единственное — параметры надо менять аккуратно. GAN капризны, и AAE не исключение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение (disentangling) стиля и метки "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В CVAE мы полагались на то, что если мы передаем нашим нейросетям метку объекта, то на латентном слое эта метка убирается.\n",
    "Что ж, это существенное допущение. Если этого не произойдет, то в нашей сгенерированной 7 будет немного 5 и так далее.\n",
    "\n",
    "А давайте просто сделаем так, чтобы метка отделялась в латентном слое в явном виде. И будем за неверное предсказание метки нейросеть штрафовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/disentangle_aae.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нам это дает? Теперь мы можем использовать стиль объекта отдельно и для простых ситуаций быть уверены, что в стиль объекта не замешалась метка. \n",
    "Для более сложных ситуаций нейросеть может нас и обмануть: мы никак не прописали явно, что в z не должно быть информации о метке. Ну что же, мы можем добавить это требование. А как? Добавим еще нейросеть, которая будет пытаться предсказать метку на основе только вектора z. \n",
    "\n",
    "Если эта нейросеть предсказывает хорошо, то энкодер не избавился от информации о метке и мы его за это штрафуем. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/label_information_penalty.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semisupervised AAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А еще мы можем кормить в нашу нейросетку неразмеченные объекты! Для них нейросетка сама будет пытаться определить label. Просто не будем штрафовать нейросеть за неверное предсказание метки (мы же ее и не знаем). А кроме них будем кормить и размеченные. Получаем semisupervised learning. \n",
    "\n",
    "\n",
    "Чтобы на неразмеченных объектах нейросеть не генерировала мусор, можем потребовать от нее, чтобы метка, которую она генерирует, приходила из категориального распределения. Как это сделать? Да опять же, добавим дискриминатор, который будет отличать получившееся распределение меток от реального ([категориального](https://en.wikipedia.org/wiki/Categorical_distribution)), задающегося априорно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L14/out/semisupervised_aae.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Полезные материалы</font>\n",
    "\n",
    "<font size=\"5\">Про unsupervised learning при помощи нейросетей</font>\n",
    "\n",
    "Главы из учебника Гудфеллоу по теме:\n",
    "1. [Representation learning](https://www.deeplearningbook.org/contents/representation.html)\n",
    "2. [Генеративные модели](https://www.deeplearningbook.org/contents/generative_models.html)\n",
    "\n",
    "Про все увеличивающуюся роль unsupervised learning: \n",
    "[Unsupervised Deep Learning - Google DeepMind & Facebook Artificial Intelligence NeurIPS 2018](https://www.youtube.com/watch?v=rjZCjosEFpI)\n",
    "\n",
    "[Лекция по генеративным моделям](https://www.youtube.com/watch?v=5WoItGTWV54)\n",
    "\n",
    "Про проклятье размерности: \n",
    "1. [В целом](https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2)\n",
    "2. [Для классификации](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)\n",
    "3. [Немного другой взгляд](https://towardsdatascience.com/the-curse-of-dimensionality-f07c66128fe1)\n",
    "\n",
    "<font size=\"5\">Автоэнкодеры</font>\n",
    "\n",
    "[Главы из учебника Гудфеллоу по теме](https://www.deeplearningbook.org/contents/autoencoders.html)\n",
    "\n",
    "\n",
    "\n",
    "[Более подробно про PCA и ссылка на его применение для MNIST](https://ryanwingate.com/intro-to-machine-learning/unsupervised/pca-on-mnist/)\n",
    "\n",
    "[Способы понижения размерности, PCA и разные типы автокодировщиков, лекция Техносферы](https://www.youtube.com/watch?v=W5JLSKcuaQo)\n",
    "\n",
    "[Eigenfaces](https://ieeexplore.ieee.org/document/139758)\n",
    "\n",
    "\n",
    "Удаление шума из\n",
    "1. [Изображений](https://debuggercafe.com/autoencoder-neural-network-application-to-image-denoising/)\n",
    "2. [Текста](https://debuggercafe.com/denoising-text-image-documents-using-autoencoders/)\n",
    "\n",
    "[Введение в автоэнкодеры на kaggle](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)\n",
    "\n",
    "<font size=\"5\">Вариационные автоэнкодеры</font>\n",
    "\n",
    "[Введение в автоэнкодеры, вариационные автоэнкодеры, PCA](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "\n",
    "Введение в автоэнкодеры на Хабре\n",
    "1. [Введение](https://habr.com/ru/post/331382/)\n",
    "2. [Manifold learning и скрытые (latent) переменные](https://habr.com/ru/post/331500/)\n",
    "3. [Вариационные автоэнкодеры (VAE)](https://habr.com/ru/post/331552/)\n",
    "4. [Conditional VAE](https://habr.com/ru/post/331664/)\n",
    "5. [GAN(Generative Adversarial Networks)](https://habr.com/ru/post/332000/)\n",
    "6. [GAN + VAE](https://habr.com/ru/post/332074/)\n",
    "\n",
    "\n",
    "[Оригинальная статья по VAE](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "[Ali Ghodsi, Лекция по VAE](https://www.youtube.com/watch?v=uaaqyVS9-rM)\n",
    "[Irhum Shafkat, Введение в автоэнкодеры, векторная арифметика](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)\n",
    "\n",
    "[Jeremy Jordan, введение в автоэнкодеры](https://www.jeremyjordan.me/autoencoders/)\n",
    "\n",
    "[Jeremy Jordan, вариационные автоэнкодеры](https://www.jeremyjordan.me/variational-autoencoders/)\n",
    "\n",
    "[Туториал по VAE с arxiv](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "\n",
    "[Еще одно введение в вариационные автоэнкодеры](https://livebook.manning.com/book/deep-learning-with-python/chapter-8/)\n",
    "\n",
    "[Туториал по VAE от Google по tensorflow](https://www.tensorflow.org/tutorials/generative/cvae)\n",
    "\n",
    "[Векторная арифметика в VAE при генерации изображений](https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)\n",
    "\n",
    "[Генерация анимированных персонажей](https://mlexplained.wordpress.com/category/generative-models/vae/)\n",
    "\n",
    "[Генерация лиц, можно менять пол, заставлять знаменитостей улыбаться](https://towardsdatascience.com/variational-autoencoders-vaes-for-dummies-step-by-step-tutorial-69e6d1c9d8e9)\n",
    "\n",
    "[VAE на pytorch с пояснениями](https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/)\n",
    "\n",
    "\n",
    "[Введение в условные вариационные автоэнкодеры](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)\n",
    "\n",
    "[Репозиторий с различными модификациями вариационных автоэнкодеров](https://github.com/AntixK/PyTorch-VAE)\n",
    "\n",
    "\n",
    "Взгляд на VAE как на игру с двумя участниками:\n",
    "1. [Часть 1](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b)\n",
    "2. [Часть 2](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46)\n",
    "3. [Часть 3](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600)\n",
    "\n",
    "\n",
    "<font size=\"5\">KL-дивергенция</font>\n",
    "\n",
    "[Википедия по дивергенции Кульбака-Лейблера](https://ru.wikipedia.org/wiki/Расстояние_Кульбака_—_Лейблера)\n",
    "[Мотивация KL-дивергенции](https://math.stackexchange.com/questions/90537/what-is-the-motivation-of-the-kullback-leibler-divergence)\n",
    "\n",
    "Объяснение проблем и разницы между KL-дивергенцией, дивергенцией Йенсена-Шеннона и расстоянием Вассерштейна:\n",
    "1. [Часть 1, проблемы KL-дивергенции. Дивергенция Йенсена-Шеннона](https://www.youtube.com/watch?v=_z9bdayg8ZI) и \n",
    "2. [Часть 2, проблемы KL-дивергенции и дивергенции Йенсена-Шеннона. Расстояние Вассерштейна](https://www.youtube.com/watch?v=y8LGAhzCOxQ)\n",
    "\n",
    "\n",
    "<font size=\"5\">AAE</font> \n",
    "\n",
    "Цикл статей по AAE:\n",
    "1. [Автоэнкодеры](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4)\n",
    "2. [Добавление дискриминатора](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9)\n",
    "3. [Разделение стиля и содержания при помощи AAE](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-3-disentanglement-of-style-and-content-89262973a4d7)\n",
    "4. [Semisupervised learning при помощи AAE](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-4-classify-mnist-using-1000-labels-2ca08071f95)\n",
    "\n",
    "[Примеры AAE на mlxnet](https://github.com/nicklhy/AdversarialAutoEncoder)\n",
    "\n",
    "[Здесь в 6 и 8 лекции тоже можно найти примеры](https://github.com/che-shr-cat/deep-learning-for-biology-hse-2019-course)\n",
    "\n",
    "<font size=\"5\">Модификации автоэнкодеров</font>\n",
    "\n",
    "[Contractive Autoencoders](http://www.icml-2011.org/papers/455_icmlpaper.pdf) — автоэнкодеры, родственные шумоподавляющим автокодировщикам.\n",
    "\n",
    "[Variational losssy autoencoder](https://arxiv.org/pdf/1611.02731.pdf) — один из типов VAE, который пытается решить проблему того, что сильный декодер может игнорировать латентное представление. \n",
    "\n",
    "[$\\beta$- VAE](https://arxiv.org/pdf/1804.03599.pdf) — еще одно возможное улучшение VAE\n",
    "\n",
    "[Wassershtein autoencoders](https://arxiv.org/pdf/1711.01558.pdf)\n",
    "\n",
    "[Concrete autoencoders](https://arxiv.org/abs/1901.09346) — якобы позволяют выделять наиболее важные признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> Примеры практического применения </font>\n",
    "\n",
    "\n",
    "1. [Age Progression/Regression — предсказание того, как будет выглядеть человек в другом возрасте](https://arxiv.org/abs/1702.08423)\n",
    "\n",
    "2. [druGAN, генерация новых химических веществ](https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.7b00346)\n",
    "\n",
    "3. [Генерация лекарств, специфически меняющих активность генов человека](https://www.frontiersin.org/articles/10.3389/fphar.2020.00269/full)\n",
    "\n",
    "4. [Генерация ингибиторов определенного белка](https://www.nature.com/articles/s41587-019-0224-x)\n",
    "\n",
    "5. [Получение латентных представлений транскриптомов](https://academic.oup.com/nar/article/48/10/e56/5814052)\n",
    "\n",
    "6. [MethylNet](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3443-8) — Использование метилирования генома для обучения латентного представления, помогающего в предсказании возраста и т.д.\n",
    "\n",
    "7. [scVAE](https://academic.oup.com/bioinformatics/article-abstract/36/16/4415/5838187?redirectedFrom=fulltext) — получение данных об экспрессии генов из single cell данных\n",
    "\n",
    "8. [U-Net](https://arxiv.org/abs/1505.04597) — сегментация медицинских изображений\n",
    "9. [W-Net](https://arxiv.org/abs/1711.08506) — unsupervised сегментация медицинских изображений  "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
