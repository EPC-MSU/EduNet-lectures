{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L11_Augmentation_Transfer_OneShot.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGxG39mYIlfM"
      },
      "source": [
        "## ResNet + CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJlUvlDiNnrl"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "\n",
        "model = ['KNN','LC','FCN','CNN',\"CNN+BN\",\"Resnet\"]\n",
        "accuracy = [0.35,0.39,0.52,0.7,0.78,0.82,]\n",
        "plt.bar(model,accuracy)\n",
        "plt.title('CIFAR10 accuracy')\n",
        "plt.xlabel('model')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWLDU1Fd6jdj"
      },
      "source": [
        "Как получить 90%+ точности для Resnet\n",
        "...\n",
        "\n",
        "\n",
        "Augmentation  + manual decrease LR "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omOba3tRJlYP"
      },
      "source": [
        "#Работа с реальными данными\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/real_data.png\" width=\"700\">\n",
        "\n",
        "\n",
        "Проблемы:\n",
        "\n",
        "- нехватка данных\n",
        "- недостаток размеченных данных\n",
        "- ошибки в разметке\n",
        "- дисбаланс датасета\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHbsUCAoMOu-"
      },
      "source": [
        "### ImageFolder\n",
        "\n",
        "Создадим датасет из своих данных для этого достаточно разложить изображения по папкам и использовать класс ImgeFolder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BowKHdTdOu1j"
      },
      "source": [
        "!wget  http://fmb.images.gan4x4.ru/hse/bt_dataset3.zip\n",
        "!unzip -oq bt_dataset3.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IWj1Jh1aO-a"
      },
      "source": [
        "!ls bike/bike_type/train\n",
        "!ls bike/bike_type/val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mys34WshO6WO"
      },
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_dataset = ImageFolder(\"/content/bike/bike_type/train\")\n",
        "val_dataset = ImageFolder(\"/content/bike/bike_type/val\")\n",
        "\n",
        "fig=plt.figure(figsize=(24, 6))\n",
        "for i in range(1, 2*7 +1):\n",
        "    img = val_dataset[i][0]\n",
        "    fig.add_subplot(2, 7, i)\n",
        "    plt.imshow(np.asarray(img))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rG99u7mUSAU"
      },
      "source": [
        "Обратите внимание: на многих кадрах один и тот же велосипед. Хорошо ли это? Что будет если часть кадров попадет в обучающую, а часть в теренировочную выборку."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhKflMW1ONDd"
      },
      "source": [
        "##Дисбаланс классов\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/unbalance.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsqLYU2RZLxh"
      },
      "source": [
        "print(\"Classes: \",val_dataset.classes)\n",
        "print(\"Sizes train:\",len(train_dataset), 'val', len(val_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQPtyPT1aAaj"
      },
      "source": [
        "У imageFolder есть свойство classes которое запослняется в соответствии с названиями папок."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0DqyPvSZ_az"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def class_hist(dataset):\n",
        "  plt.figure() \n",
        "  unique, counts = np.unique(dataset.targets, return_counts=True)\n",
        "  ax = plt.bar(unique, counts)\n",
        "  plt.title('Train objects')\n",
        "  plt.xticks(unique, dataset.classes)\n",
        "  plt.show()\n",
        "  return counts\n",
        "\n",
        "class_hist(train_dataset)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j4MFG1j_Sfc"
      },
      "source": [
        "Датасет не сбалансирован. Что делать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfhikz0Wcs-7"
      },
      "source": [
        "#### 1. Использовать адекватные метрики:\n",
        "- F1_score\n",
        "- PR_Curve\n",
        "- Confusion matrix\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# targets and preds must be calculated during training\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "conf_matrix = pd.DataFrame(confusion_matrix(targets, preds))\n",
        "\n",
        "conf_matrix.columns = dataset_val.classes\n",
        "conf_matrix.index = dataset_val.classes\n",
        "\n",
        "conf_matrix = conf_matrix.rename_axis('Real')\n",
        "conf_matrix = conf_matrix.rename_axis('Predicted', axis='columns')\n",
        "\n",
        "conf_matrix\n",
        "```\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/conf_matrix.png\" width=\"400\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ8WCMRYe_Gw"
      },
      "source": [
        "### 2. Использовать веса в Loss - функции\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropy#torch.nn.CrossEntropyLoss\n",
        "\n",
        "Как следует из описания к CrossEntropyLoss можно добавить веса классов. Сравним loss для с весам и без для оних и тех же данных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap11JzLufjFL"
      },
      "source": [
        "import torch\n",
        "# Without weights\n",
        "scores = torch.tensor([[2.,30.],[2.,30.]]) # Scores for two samples batch\n",
        "target = torch.tensor([0,1]) # First sample belongs to class 0 second to 1 and firse was misclassified\n",
        "weights = torch.tensor([1,1],dtype = torch.float32)\n",
        "criterion = torch.nn.CrossEntropyLoss( weight = weights)  \n",
        "criterion(scores,target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLfxNumPhtPX"
      },
      "source": [
        "Добавим к первому классу вес 10. Это условно соответствует ситуации с BMX велосипедами в нашем датасете: их примерно в 10 раз больше чем MTB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8r3WiFghZ3e"
      },
      "source": [
        "weights = torch.tensor([10,1],dtype = torch.float32)\n",
        "criterion = torch.nn.CrossEntropyLoss( weight = weights)  \n",
        "criterion(scores,target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfj8fhAeiDr1"
      },
      "source": [
        "Лосс вырос так как класс на котором возникла ошибка оказался редким."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv8qFZiwibqL"
      },
      "source": [
        "Рассчет весов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to5vboMHiddC"
      },
      "source": [
        "#numpy\n",
        "import numpy as np\n",
        "_, counts = np.unique(train_dataset.targets, return_counts=True)\n",
        "weights= np.max(counts) / counts\n",
        "weights = torch.FloatTensor(weights)\n",
        "print('Веса классов: ', weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdu1W-xwi0jc"
      },
      "source": [
        "#torch\n",
        "# https://pytorch.org/docs/stable/generated/torch.unique.html\n",
        "_, counts = torch.unique(torch.tensor(train_dataset.targets),return_counts = True)\n",
        "print(counts.max())\n",
        "weights = counts.max() / counts\n",
        "print('Классы: ',train_dataset.classes)\n",
        "print('Веса классов: ', weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yk9OFyTkVK7"
      },
      "source": [
        "# Sklearn\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n",
        "from sklearn.utils import class_weight\n",
        "weights = class_weight.compute_class_weight('balanced',np.unique(train_dataset.targets),train_dataset.targets)\n",
        "print(weights)\n",
        "weights / min(weights) # The same as before\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7bvuwq_qb9G"
      },
      "source": [
        "### 3. Выборка с повышением или с понижением \n",
        "(Up-sample or Down-sample): одно из решений проблемы - сбалансировать данные.\n",
        "\n",
        "  \n",
        "\n",
        "```\n",
        "  Это может быть сделано либо за счет увеличения частоты класса меньшинства, либо за счет уменьшения частоты класса большинства с помощью методов случайной или кластерной выборки.\n",
        "\n",
        "    Выбор между избыточной или недостаточной выборкой и случайным или кластеризованным определяется бизнес-контекстом и размером данных.\n",
        "\n",
        "    Обычно `upsampling` предпочтителен, когда общий размер данных небольшой, а понижающая дискретизация полезна, когда у нас есть большой объем данных. Точно так же случайная или кластерная выборка определяется тем, насколько хорошо распределены данные.\n",
        "```\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_34.png\" width=\"600\">\n",
        "\n",
        "Фактически мы либо удаляем часть объектов одного из классов либо копируем."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAtwOmchIxWm"
      },
      "source": [
        "## Недостаток данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ebJ1Je_kQP"
      },
      "source": [
        "Теперь посмотрим на распределение данных по класам в валидационном датасете"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jImgxrC_bH8U"
      },
      "source": [
        "class_hist(val_dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXj75eeecay8"
      },
      "source": [
        "На 3-х изображениях мы не сможем оценить точность. Давайте на них посмотрим"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSzqZLdhrMvq"
      },
      "source": [
        "Очевидно что выкидывать тут уже нечего, и 100 копий одного велосипеда не решат проблемму. В данном случае можно переместить несколько велосипедов из тренировочной выборки в валидационную но поблему дефицита BMX-ов в целом это не решит. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2lk1xbUrm5m"
      },
      "source": [
        "#### Генерация синтетических данных:\n",
        "\n",
        " хотя `upsampling` или `downsampling` помогает сбалансировать данные, дублирование данных увеличивает вероятность переобучения.\n",
        "\n",
        "Другой подход к решению этой проблемы - создание синтетических данных с помощью данных о классе меньшинств.\n",
        "\n",
        "Для табличных данных можно использовать методы \n",
        "\n",
        "[Synthetic Minority Over-sampling Technique (SMOTE)](https://rikunert.com/SMOTE_explained) или Modified- SMOTE - два таких метода, которые генерируют синтетические данные.\n",
        "\n",
        "Проще говоря, SMOTE берет точки данных класса меньшинства и создает новые точки данных, которые лежат между любыми двумя ближайшими точками данных, соединенными прямой линией.\n",
        "\n",
        "Для этого алгоритм вычисляет расстояние между двумя точками данных в пространстве признаков, умножает расстояние на случайное число от 0 до 1 и помещает новую точку данных на этом новом расстоянии от одной из точек данных, используемых для определения расстояния.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_35.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoM5PL_gvLUf"
      },
      "source": [
        "В ряде случаев можно генерировать и изображения.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/synthesis.png\" width=\"900\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaTJM2vgI-o2"
      },
      "source": [
        "## Аугментации\n",
        "\n",
        "\n",
        "Другой способ получения дополнительных данных.\n",
        "Применяются для изображений и легко позволяют получить дополнительные данные. \n",
        "\n",
        "Сам термин пришел из музыки:\n",
        "\n",
        "Аугмента́ция (позднелат. augmentatio — увеличение, расширение) — техника ритмической композиции в старинной музыке.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_12.png\" width=\"700\">\n",
        "\n",
        "Собственно механизм трансформаций torcvision который мы использовали с первого занятия, в основном предназначен для аугментации данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPmrnebytZi-"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_13.png\" width=\"1000\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcsl61SxRPCO"
      },
      "source": [
        "Functional-transforms\n",
        "\n",
        "https://pytorch.org/vision/stable/transforms.html#functional-transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxJ7R1IMQzPU"
      },
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "pil = Image.open('bike/bike_type/train/road/road_1150_5fccd8116e87e.jpeg')\n",
        "transformed_pil = TF.vflip(pil)\n",
        "plt.imshow(pil)\n",
        "plt.figure()\n",
        "plt.imshow(transformed_pil)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLxBwEBPV0_"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "def show_img(img):\n",
        "  plt.figure(figsize=(40,38))\n",
        "  npimg=img.numpy()\n",
        "  plt.imshow(np.transpose(npimg,(1,2,0)))\n",
        "  plt.show()\n",
        "\n",
        "train_dataset.transform = transforms.Compose([                                                            \n",
        "                              transforms.RandomRotation(50,expand=True),  \n",
        "                              transforms.Resize((164,164)),\n",
        "                              transforms.ToTensor(),\n",
        "                              ])\n",
        "\n",
        "Augmentation_dataloader=DataLoader(train_dataset,batch_size=8,shuffle=False)\n",
        "\n",
        "data=iter(Augmentation_dataloader)\n",
        "show_img(torchvision.utils.make_grid(data.next()[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyx8C7y_uw4k"
      },
      "source": [
        "#### Создание собственных аугментаций\n",
        "\n",
        "В том числе могут применяться в том числе  и к меткам!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oI_zDazT9v_"
      },
      "source": [
        "from PIL import ImageFilter\n",
        "\n",
        "def blur(pil_image,radius):\n",
        "  return pil_image.filter(ImageFilter.GaussianBlur(radius=radius))\n",
        "\n",
        "class PadToSquare:\n",
        "  def __call__(self, img):\n",
        "    diff = pil_image.size[0] - pil_image.size[1]\n",
        "    if diff == 0:\n",
        "      return pil_image\n",
        "    padding = (0,int(diff/2)) if diff >0 else (int(diff/2),0)\n",
        "    return transforms.functional.pad(img,padding)\n",
        "\n",
        "pil_image = Image.open(\"bike/bike_type/train/road/road_1150_5fccd8116e87e.jpeg\")\n",
        "transform=transforms.Compose([\n",
        "                              transforms.Lambda(lambda x: blur(x,5)),\n",
        "                              PadToSquare(),\n",
        "                              ])\n",
        "image = transform(pil_image)\n",
        "plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShF2cngdwy92"
      },
      "source": [
        "#### Целесообразность"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KYu4GO0AaLo"
      },
      "source": [
        "Не все аугментации имеют смысл для конкретной задачи.\n",
        "\n",
        "Например VerticalFlip. A HorizontalFlip ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKIS463qt5XG"
      },
      "source": [
        "#### Применение большого количества аугментаций может испортить изображение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53g3uTq5UUrh"
      },
      "source": [
        "t_list = [                              \n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.ColorJitter(brightness= (1,10), contrast=(1,10), saturation=(1,10), hue=0), # is chosen uniformly from ..\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.RandomRotation(50,expand=True),  \n",
        "          transforms.Resize((164,164)),\n",
        "          ]\n",
        "\n",
        "t= transforms.Compose(t_list)\n",
        "\n",
        "pil_image = Image.open(\"bike/bike_type/train/road/road_1150_5fccd8116e87e.jpeg\")\n",
        "transformed = t(pil_image)\n",
        "plt.figure()\n",
        "plt.imshow(pil_image)\n",
        "plt.figure()\n",
        "plt.imshow(transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZBccNOkt45Z"
      },
      "source": [
        "# RandomChoice из torchvision.transforms \n",
        "# see also RandomOrder\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(pil_image)\n",
        "\n",
        "for i in range(10):\n",
        "  t = transforms.Compose([transforms.RandomChoice(t_list)])\n",
        "  transformed = t(pil_image)\n",
        "  plt.figure()\n",
        "  plt.imshow(transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c976TzkBvLF"
      },
      "source": [
        "### Online Augmentation\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/aug_online.png\" width=\"700\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79DlFEH_uUmf"
      },
      "source": [
        "#### Аугментация как регуляризация\n",
        "\n",
        "\n",
        "- помогает бороться с переобучением\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/aug_overfit.png\" width=\"700\">\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/aug_reg.png\" width=\"300\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-M5GN22MoBf"
      },
      "source": [
        "#### Albumentation\n",
        "\n",
        "Если стандартных аугментаций pytorch недостаточно, можно воспользоваться сторонними библиотеками.\n",
        "\n",
        "Например:\n",
        "https://github.com/albumentations-team/albumentations\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/album1.png\" width=\"700\">\n",
        "\n",
        "Кроме расширенного набора аугментаций, albumentations позволяет применять их не только к самим изображениям, но к разметке: маскам, ограничивающим прямоугольникмам, ключевым точкам\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/album2.png\" width=\"700\">\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPR7qLhNYxxj"
      },
      "source": [
        "pip install -U albumentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMOTUaBqYviv"
      },
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "\n",
        "# Declare an augmentation pipeline\n",
        "transform = A.Compose([\n",
        "    A.RandomCrop(width=256, height=256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "])\n",
        "\n",
        "transformed = t(pil_image)\n",
        "plt.imshow(transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyVxVVd1M0OZ"
      },
      "source": [
        "### Unsupervised Learning\n",
        "\n",
        "Если мы можем автоматически генерировать данные, то нельзя ли их и размечать без участия человека?\n",
        "\n",
        "Можно всячески измениять данные, а потом учить сеть их восстанавливать:\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_16.png\" width=\"700\">\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/unsup2.png\" width=\"700\">\n",
        "\n",
        "[Dosovitskiy et al., 2015](https://arxiv.org/abs/1406.6909)\n",
        "\n",
        "[Gidaris et al. 2018](https://arxiv.org/abs/1803.07728)\n",
        "\n",
        "[Doersch et al. (2015)](https://arxiv.org/abs/1505.05192)\n",
        "[Noroozi, et al, 2017](https://arxiv.org/abs/1708.06734)\n",
        "\n",
        "[Pathak, et al., 2016](https://arxiv.org/abs/1604.07379)\n",
        "\n",
        "\n",
        "\n",
        "Зачем это нужно?\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/unsup1.png\" width=\"700\">\n",
        "\n",
        "Веса которые мы получим в ходе такого обучения будут очень похожи на те которые сформировались бы при работе с размеченным человеком датасетом.\n",
        "Особенно на первых слоях.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh8SM1YxJDwX"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsHOgoMgZza4"
      },
      "source": [
        "Для таких типовых задач, как классификация изображений, можно воспользоваться готовой архитектурой (AlexNet, VGG, Inception, ResNet и т.д.) и обучить нейросеть на своих данных. Реализации таких сетей с помощью различных фреймворков уже существуют, так что на данном этапе можно использовать одну из них как черный ящик, не вникая глубоко в принцип её работы.\n",
        "\n",
        "Однако, глубокие нейронные сети требовательны к большим объемам данных для сходимости обучения. И зачастую, в нашей частной задаче недостаточно данных для того, чтобы хорошо натренировать все слои нейросети. `Transfer Learning` решает эту проблему. Зачем обучать сеть заново, если можно использовать уже обученную на миллионе изображений и дообучить на свой датасет?\n",
        "\n",
        "В PyTorch есть много предобученных сетей: [TORCHVISION.MODELS](https://pytorch.org/vision/stable/models.html)\n",
        "\n",
        "- AlexNet\n",
        "- VGG\n",
        "- ResNet\n",
        "...\n",
        "\n",
        "Для этого,  нужно отключить какие-то промежуточные слои. Тогда можно использовать то, что называется `Fine turning` - не нужно обучать всю модель, а достаточно только ее новую часть.\n",
        "\n",
        "Зачастую в конце классификационных сетей используется полносвязный слой. Так как мы заменили этот слой, использовать предобученные веса для него уже не получится. Придется тренировать его с нуля, инициализировав его веса случайными значениями. Веса для всех остальных слоев мы загружаем из предобученной модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkLUelSDxwcC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9EiMku2xiDs"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_21.png\" width=\"700\">\n",
        "\n",
        "Мы уже делали это когда сравнивали Resnet собственного изготовления библиотечной реализацией. Нам требовалось изменить колчество классов и мы просто заменяли у можели последний слой."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXrgbsIYyEry"
      },
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "model = resnet18()\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGDPfVybzWRt"
      },
      "source": [
        "Смотрим на вывод, линейный слой, и заменяем его другим с тем же количеством входов и нужным нам количеством выходов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-ezgscpzEeQ"
      },
      "source": [
        "from torch import nn\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "\n",
        "imagenet_input = torch.randn([1,3,224,224])\n",
        "\n",
        "out = model(imagenet_input)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBaZ7q1-0m2C"
      },
      "source": [
        "## Заморозка весов\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KigOQyed1C-g"
      },
      "source": [
        "for tag, param in model.named_parameters():\n",
        "  if not 'layer4.' in tag and not 'fc' in tag:\n",
        "  #if not any(map(tag.__contains__, ['layer4.','fc'])): another way\n",
        "    param.requires_grad = False\n",
        "  print(tag,type(param),param.shape,param.requires_grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q4SuaFL4F3F"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# or \n",
        "\n",
        "params_to_update = list(model.fc.parameters()) + list(model.layer4.parameters())\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq2ewTDH0UE1"
      },
      "source": [
        "#### Замена и удаление произвольных слоев"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-sDnP8T49bb"
      },
      "source": [
        "При отправке тензора с высотой и шириной 32x32 возникает ошибка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlcjVBRV0Jw-"
      },
      "source": [
        "cifar10_input = torch.randn([1,3,32,32])\n",
        "out = model(cifar10_input)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEuVi36c5IG7"
      },
      "source": [
        "Заменим 'stem' слои в начале сети, отвечающие за аггресивное сжатие изображения.\n",
        "\n",
        "P.S. Это потребуется при выполнении практической работы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDM4fIg_0hUY"
      },
      "source": [
        "from torch import nn\n",
        "model.conv1 = nn.Conv2d(3,64,kernel_size=(5, 5),stride = 1, padding =2, bias=False)\n",
        "model.maxpool = nn.Identity()\n",
        "out = model(cifar10_input)\n",
        "print(out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iJKr--JJrds"
      },
      "source": [
        "## Рекомендации по обучению\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/loss.png\" width=\"700\">\n",
        "\n",
        "Обучение новой модели.\n",
        "большой шаг ~1e-3\n",
        "\n",
        "Дообучение (Transfer learning) \n",
        "маленикий шаг ~1e-5\n",
        "\n",
        "\n",
        "*Выбор алгоритма оптимизации связан с выбором шагом обучения.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy3sQeuNQomZ"
      },
      "source": [
        "Форматы изображений (OpenCv, Pillow ...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE4OOBqlY15y"
      },
      "source": [
        "### Сохранение весов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8WO8LoYJfhW"
      },
      "source": [
        "#Few shot learning\n",
        "\n",
        "Не все задачи сводятся к классификации. Например распознавание лиц.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Предположим, мы хотим создать систему распознавания лиц для небольшой организации, в которой всего 10 сотрудников (небольшое количество упрощает задачу).\n",
        "\n",
        "Используя традиционный подход к классификации, мы можем придумать систему, которая выглядит следующим образом:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL1.jpeg\" width=\"700\">\n",
        "\n",
        "Проблемы:\n",
        "\n",
        "а) Чтобы обучить такую ​​систему, нам сначала потребуется много разных изображений каждого из 10 человек в организации, что может оказаться невозможным. (Представьте, что вы делаете это для организации с тысячами сотрудников).\n",
        "\n",
        "б) Что делать, если новый человек присоединяется к организации или покидает ее? Вам нужно снова взять на себя боль сбора данных и заново обучить всю модель.\n",
        "\n",
        "Это практически невозможно, особенно для крупных организаций, где набор и увольнение происходит почти каждую неделю."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyuQkNbLJ1C7"
      },
      "source": [
        "## Сиамские сети\n",
        "\n",
        "Вместо того что бы классифицировать изображения можно использовать карту признаков (вектор) полученную на последнем слое и сравнивать эти вектора между собой.\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/siamese.png\" width=\"800\">\n",
        "\n",
        "\n",
        "\n",
        "Можно кластеризовать эти вектора при помощи алгоритма кластеризации, и в зависимости от кого в какой кластер попал объект предсказывать его класс.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/few_shot.png\" width=\"800\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xadqmUI2Lqpo"
      },
      "source": [
        "### Косинусное расстояние\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/cos_dist.png\" width=\"300\">\n",
        "\n",
        "Для сравнения эффективнее использовать косинусное расстояние. В отличие от Евклидова не зависит от масштаба."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffSuxFO4Q56R"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from  sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def dump_dist(a,b):\n",
        "  euclidean_distance = F.pairwise_distance(a, b);\n",
        "  cosine_distanse = F.cosine_similarity(a, b);\n",
        "\n",
        "  print(f\"Euclidean {euclidean_distance.item():.2f} \", )\n",
        "  print(f\"Cosine {cosine_distanse.item():.2f} deg. {math.degrees(math.acos(cosine_distanse)):.2f} \", )\n",
        "\n",
        "a = torch.tensor([2.,5.]).view(1,-1) # Because all torch functions works with batches\n",
        "b = torch.tensor([4.,2.]).unsqueeze(0) # the same\n",
        "print(a,b)\n",
        "\n",
        "dump_dist(a,b)\n",
        "\n",
        "print(\"Now vectors are scaled\")\n",
        "a *= 10\n",
        "b *= 10\n",
        "\n",
        "dump_dist(a,b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf8ovS53LuP5"
      },
      "source": [
        "### Contrastive Loss\n",
        "\n",
        "Для обучения потребуется специальная лосс-функция.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/contr_loss.png\" width=\"800\">\n",
        "\n",
        "[2006 Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)\n",
        "\n",
        "Считается для набора значений. Где два принадлежат одному классу остальные нет"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2PVFn2_ZyUH"
      },
      "source": [
        "#### CosineEmbeddingLoss\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html\n",
        "\n",
        "В Pytorch реализован более удобный вариант требующий на вход пару изображений. Идея похожа на SVMLoss: дистанция между векторами из разных классов должна быть больше порога.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/cos_loss.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_o8wZban8FJ"
      },
      "source": [
        "## Пример"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBxWoxpVZNbE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22PIxa2enTij"
      },
      "source": [
        "!wget http://edunet.kea.su/repo/src/L11_Transfer_learning/small_face_dataset.zip\n",
        "!unzip small_face_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f5ShlyvfAL1"
      },
      "source": [
        "#!wget http://www.anefian.com/research/GTdb_crop.zip\n",
        "#!unzip GTdb_crop.zip\n",
        "# http://conradsanderson.id.au/lfwcrop/ (LFWcrop Face Dataset, greyscale version)\n",
        "#!wget http://conradsanderson.id.au/lfwcrop/lfwcrop_grey.zip\n",
        "#!unzip lfwcrop_grey.zip\n",
        "#!wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "#!tar zxvf lfw.tgz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPV4iSg6oFwE"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SiameseNetworkDataset(Dataset):\n",
        "    \n",
        "    def __init__(self,imageFolderDataset,transform=None):\n",
        "        self.imageFolderDataset = imageFolderDataset    \n",
        "        self.transform = transform        \n",
        "    def __getitem__(self,index):\n",
        "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
        "        #we need to make sure approx 50% of images are in the same class\n",
        "        should_get_same_class = random.randint(0,1) \n",
        "        if should_get_same_class:\n",
        "            while True:\n",
        "                #keep looping till the same class image is found\n",
        "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
        "                if img0_tuple[1]==img1_tuple[1]:\n",
        "                    break\n",
        "        else:\n",
        "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
        "\n",
        "        img0 = Image.open(img0_tuple[0])\n",
        "        img1 = Image.open(img1_tuple[0])\n",
        "        img0 = img0.convert(\"L\")\n",
        "        img1 = img1.convert(\"L\")\n",
        "        \n",
        "\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "        \n",
        "  \n",
        "        return img0, img1 ,int(img1_tuple[1] == img0_tuple[1])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.imageFolderDataset.imgs)\n",
        "\n",
        "folder_dataset = torchvision.datasets.ImageFolder(root=\"faces/training\",transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                                                      transforms.ToTensor()\n",
        "                                                                      ]))\n",
        "\n",
        "\n",
        "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n",
        "                                        transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                                                      transforms.ToTensor(),\n",
        "                                                                      #transforms.Normalize(0.4371,0.1933)\n",
        "                                                                      ]))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnMcSn9WvjuG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def imshow(img,text=None,should_save=False):\n",
        "    npimg = img.numpy()\n",
        "    plt.axis(\"off\")\n",
        "    if text:\n",
        "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
        "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show() \n",
        "\n",
        "vis_dataloader = DataLoader(siamese_dataset,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2,\n",
        "                        batch_size=8)\n",
        "dataiter = iter(vis_dataloader)\n",
        "\n",
        "\n",
        "example_batch = next(dataiter)\n",
        "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
        "plt.figure(figsize=(20,10))\n",
        "imshow(torchvision.utils.make_grid(concatenated))\n",
        "print('1 = самозванцы и 0 = совпадения : \\n', example_batch[2].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKk__CT9wcGh"
      },
      "source": [
        "from torchvision.models import resnet18\n",
        "from torch import nn\n",
        "class SiameseNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SiameseNetwork, self).__init__()\n",
        "    self.backbone = resnet18()\n",
        "    self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "    self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 128)\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "    output1 = self.backbone(input1)\n",
        "    output2 = self.backbone(input2)\n",
        "    return output1, output2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7QIJAJHxkIC"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataloader = DataLoader(siamese_dataset,shuffle=True, num_workers=2, batch_size=64)\n",
        "model = SiameseNetwork()\n",
        "model.train()\n",
        "model.to(device)\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html\n",
        "criterion = nn.CosineEmbeddingLoss(0.3)\n",
        "optimizer = optim.Adam(model.parameters(),lr = 0.00005 )\n",
        "\n",
        "counter = []\n",
        "loss_history = [] \n",
        "iteration_number = 0\n",
        "\n",
        "for epoch in range(30):\n",
        "    for i, data in enumerate(train_dataloader,0):\n",
        "        img0, img1 , label = data\n",
        "        output1,output2 = model(img0.to(device),img1.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        label[label == 0] = -1 # To capability with torch.nn.CosineEmbeddingLoss \n",
        "        loss = criterion(output1,output2,label.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i %10 == 0 :\n",
        "            print(\"Epoch number {} Current loss {}\".format(epoch, loss.item()))\n",
        "            iteration_number +=10\n",
        "            counter.append(iteration_number)\n",
        "            loss_history.append(loss.item())\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN_OSRpJn0qc"
      },
      "source": [
        "def show_plot(iteration,loss):\n",
        "    plt.plot(iteration,loss)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "show_plot(counter[1:], loss_history[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E00PW_nDtfik"
      },
      "source": [
        "#### Test\n",
        "Последние 3 фотографии были исключены из обучения и будут использоваться для тестирования. Расстояние между каждой парой изображений обозначает степень сходства модели между двумя изображениями.\n",
        "\n",
        "Меньшее значение ошибки означает, что считает изображения похожими, в то время как более высокие значения указывают на то, что модель считает их разными людьми.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnUiPWzWsHdI"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "folder_dataset_test = torchvision.datasets.ImageFolder(root='faces/testing')\n",
        "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
        "                                        transform=transforms.Compose([transforms.Resize((100,100)),\n",
        "                                                                      transforms.ToTensor(),\n",
        "                                                                      transforms.Normalize(0.4371,0.1933)\n",
        "                                                                      ])\n",
        "                                       )\n",
        "\n",
        "test_dataloader = DataLoader(siamese_dataset,num_workers=2,batch_size=1,shuffle=True)\n",
        "dataiter = iter(test_dataloader)\n",
        "x0,_,_ = next(dataiter)\n",
        "\n",
        "for i in range(10):\n",
        "    x0,x1,label2 = next(dataiter)\n",
        "    concatenated = torch.cat((x0,x1),0)\n",
        "    \n",
        "    output1,output2 = model(x0.to(device),x1.to(device))\n",
        "    euclidean_distance = F.pairwise_distance(output1, output2).cpu()[0].item()\n",
        "    cos_distance = F.cosine_similarity(output1, output2)[0].item();\n",
        "    imshow(torchvision.utils.make_grid(concatenated),\n",
        "           f'Dissimilarity: {euclidean_distance :.2f} {cos_distance :.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amNI1YaffjK_"
      },
      "source": [
        "Учится но плохо..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7oaDaeGKWOa"
      },
      "source": [
        "## Предобработка\n",
        "\n",
        "Для улучшения качества фотографии предобрабатывают"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMGbbCyoKfIc"
      },
      "source": [
        "### Выравнивание\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/face_landmark.png\" width=\"800\">\n",
        "\n",
        "Цель состоит в том что бы важные точки такие как глаза, рот и, оказались на одних и тех же местах. \n",
        "\n",
        "Такое преобразование называется [Гомография](https://waksoft.susu.ru/2020/03/26/primery-gomogrfii-s-ispolzovaniem-opencv/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2cVStKiKbQx"
      },
      "source": [
        "### Ключевые точки\n",
        "\n",
        "Для получения матрицы гомографии  требуется знать какие точки в какие перейдут.\n",
        "\n",
        "Для этого требуется знать где какие точки будут на изображении. И это отдельная задача компьютерного зрения (Landmark Detection).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QujohxiamKgx"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/face_pipeline.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YJAFCBJKlya"
      },
      "source": [
        "### MTCNN\n",
        "\n",
        "Раньше за процессы предобработки должен был отвечать программист. В 2016 году вышла статья описывающая моддель которая осуществляет всю предобработку.\n",
        "\n",
        "[Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks](https://arxiv.org/abs/1604.02878)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntIGWsNunW_P"
      },
      "source": [
        "\n",
        "!wget http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/peoples.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04u9HIl4KKsQ"
      },
      "source": [
        "## Triplet Loss\n",
        "\n",
        "Другим усовершенствованием являетс замена лосс функции.\n",
        "\n",
        "\n",
        "Триплет состоит из анкора `anchor`, положительного и отрицательного образцов и в основном применяется для распознавания лиц.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL8.png\" width=\"600\">\n",
        "\n",
        "\n",
        "В `Triplet loss` расстояние между базовым (якорным) изображением  и другим изображением лица того же человека минимизируется, а расстояние до негативного примера максимизируется.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL9.png\" width=\"600\">\n",
        "\n",
        "Функция расстояния может быть произвольной.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL10.png\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/triplet_loss.png\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.htm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3fEsaI82sNx"
      },
      "source": [
        "import torch \n",
        "triplet_loss = torch.nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "anchor = torch.randn(100, 128, requires_grad=True)\n",
        "positive = torch.randn(100, 128, requires_grad=True)\n",
        "negative = torch.randn(100, 128, requires_grad=True)\n",
        "loss = triplet_loss(anchor, positive, negative)\n",
        "print(loss)\n",
        "#output.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbztBTFvsG9R"
      },
      "source": [
        "Как видно для ее использования нужен датасет который возвращает три изображения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HrFhukxKCvj"
      },
      "source": [
        "## Метрики\n",
        "\n",
        "Как использовать такого рода модели?\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/face_aim.png\" width=\"800\">\n",
        "\n",
        "\n",
        "Выбирается порог.\n",
        "\n",
        "И полученный на выходе модели вектор - признак сравнивается с эталонным или имеющимся в БД.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/metric.png\" width=\"800\">\n",
        "\n",
        "От выбора порога зависит какие ошибки будет допускать модель.\n",
        "\n",
        "\n",
        "\n",
        "Удобно использовать DET кривую - (detection error trade-off curve)\n",
        "Что бы понять как количество ошибок первого рода влияет на количество второго.\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/metric2.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CX-3U4rMCsI"
      },
      "source": [
        "### Кластеризация / Поиск\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/face_clustering.png\" width=\"600\">\n",
        "\n",
        "Фактически задача поиска решается методом K- ближайших соседей.\n",
        "\n",
        "Для больших БД используются оптимизированные библиотеки:\n",
        "\n",
        "[HNSW ](https://github.com/nmslib/nmslib)Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs\n",
        "https://github.com/nmslib/nmslib\n",
        "\n",
        "\n",
        "[Faiss](https://github.com/facebookresearch/faiss)\n",
        "Faiss is a library for efficient similarity search and clustering of dense vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCu6nDuNKt9j"
      },
      "source": [
        "### Демонстрация\n",
        "\n",
        "Давате посмотрим как все это работает:\n",
        "https://colab.research.google.com/drive/1OHgGRPc1sLcJK6_Q6o8vpUQ1G0IWgrHz"
      ]
    }
  ]
}