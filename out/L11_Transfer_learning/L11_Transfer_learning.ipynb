{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение на малом объеме данных. 7 (перенесла сюда), 10, 11 лекция\n",
    "\n",
    "Transfer learning, Augmentation. One-shot learning. Кластеризация (embedding, one-shot learning - ПРИМЕР С ЛИЦАМИ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По Ганичеву (7 лекция)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Подготовка данных (Data Preprocessing)\n",
    "2. Инициализация весов (Weight Initialization)\n",
    "3. Обучение (Optimization)*\n",
    "4. Пакетная нормализация (Batch Normalization)\n",
    "5. Переобучение (Overfitting)*:\n",
    "Регуляризация, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_4.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_4.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_5.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_6.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_7.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_7.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_8.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_9.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_10.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_11.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_14.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_15.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_16.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_17.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_18.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_19.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_21.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_22.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_23.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_24.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_25.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_26.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_27.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_28.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_29.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_30.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_31.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_32.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_33.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ДОПИНФОРМАЦИЯЯЯЯЯЯЯЯЯЯЯЯЯЯЯЯЯЯЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: как быстро обучить нейросеть на своих данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "источник: https://habr.com/ru/company/binarydistrict/blog/428255/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если перед нами встает задача распознавания изображений, можно воспользоваться готовым сервисом. Однако, если нужно обучить модель на собственном наборе данных, то придется делать это самостоятельно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для таких типовых задач, как классификация изображений, можно воспользоваться готовой архитектурой (AlexNet, VGG, Inception, ResNet и т.д.) и обучить нейросеть на своих данных. Реализации таких сетей с помощью различных фреймворков уже существуют, так что на данном этапе можно использовать одну из них как черный ящик, не вникая глубоко в принцип её работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, глубокие нейронные сети требовательны к большим объемам данных для сходимости обучения. И зачастую в нашей частной задаче недостаточно данных для того, чтобы хорошо натренировать все слои нейросети. Transfer Learning решает эту проблему."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning для классификации изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети, которые используются для классификации, как правило, содержат N выходных нейронов в последнем слое, где N — это количество классов. Такой выходной вектор трактуется как набор вероятностей принадлежности к классу. В нашей задаче распознавания изображений еды количество классов может отличаться от того, которое было в исходном датасете. В таком случае нам придётся полностью выкинуть этот последний слой и поставить новый, с нужным количеством выходных нейронов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_0.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачастую в конце классификационных сетей используется полносвязный слой. Так как мы заменили этот слой, использовать предобученные веса для него уже не получится. Придется тренировать его с нуля, инициализировав его веса случайными значениями. Веса для всех остальных слоев мы загружаем из предобученного снэпшота."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные стратегии дообучения модели. Мы воспользуемся следующей: будем тренировать всю сеть из конца в конец (end-to-end), а предобученные веса не будем фиксировать, чтобы дать им немного скорректироваться и подстроиться под наши данные. Такой процесс называется тонкой настройкой (fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Структурные компоненты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи нам понадобятся следующие компоненты:\n",
    "\n",
    "1. Описание модели нейросети\n",
    "2. Пайплайн обучения\n",
    "3. Инференс пайплайн\n",
    "4. Предобученные веса для этой модели\n",
    "5. Данные для обучения и валидации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем примере компоненты (1), (2) и (3) я буду брать из собственного репозитория, который содержит максимально легковесный код — при желании с ним можно легко разобраться. Наш пример будет реализован на популярном фреймворке TensorFlow. Предобученные веса (4), подходящие под выбранный фреймворк, можно найти, если они соответствуют одной из классических архитектур. В качестве датасета (5) для демонстрации я возьму Food-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве модели воспользуемся классической нейросетью VGG (точнее, VGG19). Несмотря на некоторые недостатки, эта модель демонстрирует довольно высокое качество. Кроме того, она легко поддается анализу. На TensorFlow Slim описание модели выглядит достаточно компактно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def vgg_19(inputs,\n",
    "           num_classes,\n",
    "           is_training,\n",
    "           scope='vgg_19',\n",
    "           weight_decay=0.0005):\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                biases_initializer=tf.zeros_initializer(),\n",
    "                padding='SAME'):\n",
    "        with tf.variable_scope(scope, 'vgg_19', [inputs]):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "            # Use conv2d instead of fully_connected layers\n",
    "            net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "            net = slim.dropout(net, 0.5, is_training=is_training, scope='drop6')\n",
    "            net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "            net = slim.dropout(net, 0.5, is_training=is_training, scope='drop7')\n",
    "            net = slim.conv2d(net, num_classes, [1, 1], scope='fc8',\n",
    "                activation_fn=None)\n",
    "            net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код обучения модели состоит из следующих шагов:\n",
    "\n",
    "1. Построение train/validation пайплайнов данных\n",
    "2. Построение train/validation графов (сетей)\n",
    "3. Надстраивание классификационной функция потерь (cross entropy loss) поверх train графа\n",
    "4. Код, необходимый для вычисления точности предсказания на валидационной выборке во время обучения\n",
    "5. Логика загрузки предобученных весов из снэпшота\n",
    "6. Создание различных структур для обучения\n",
    "7. Непосредственно сам цикл обучения (итерационная оптимизация)\n",
    "\n",
    "Последний слой графа конструируется с нужным нам количеством нейронов и исключается из списка параметров, загружаемых из предобученного снэпшота.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "import model\n",
    "import data\n",
    "\n",
    "###########################################################\n",
    "###  Settings\n",
    "###########################################################\n",
    "\n",
    "INPUT_SIZE = 224\n",
    "RANDOM_CROP_MARGIN = 10\n",
    "TRAIN_EPOCHS = 20\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VAL_BATCH_SIZE = 128\n",
    "LR_START = 0.001\n",
    "LR_END = LR_START / 1e4\n",
    "MOMENTUM = 0.9\n",
    "VGG_PRETRAINED_CKPT = 'data/vgg_19.ckpt'\n",
    "CHECKPOINT_DIR = 'checkpoints/vgg19_food'\n",
    "LOG_LOSS_EVERY = 10\n",
    "CALC_ACC_EVERY = 500\n",
    "\n",
    "###########################################################\n",
    "###  Build training and validation data pipelines\n",
    "###########################################################\n",
    "\n",
    "train_ds, train_iters = data.train_dataset(train_data,\n",
    "    TRAIN_BATCH_SIZE, TRAIN_EPOCHS, INPUT_SIZE, RANDOM_CROP_MARGIN)\n",
    "train_ds_iterator = train_ds.make_one_shot_iterator()\n",
    "train_x, train_y = train_ds_iterator.get_next()\n",
    "\n",
    "val_ds, val_iters = data.val_dataset(val_data,\n",
    "    VAL_BATCH_SIZE, INPUT_SIZE)\n",
    "val_ds_iterator = val_ds.make_initializable_iterator()\n",
    "val_x, val_y = val_ds_iterator.get_next()\n",
    "\n",
    "###########################################################\n",
    "###  Construct training and validation graphs\n",
    "###########################################################\n",
    "\n",
    "with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "    train_logits = model.vgg_19(train_x, num_classes, is_training=True)\n",
    "    val_logits = model.vgg_19(val_x, num_classes, is_training=False)\n",
    "\n",
    "###########################################################\n",
    "###  Construct training loss\n",
    "###########################################################\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "    labels=train_y, logits=train_logits)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "###########################################################\n",
    "###  Construct validation accuracy\n",
    "###  and related functions\n",
    "###########################################################\n",
    "\n",
    "def calc_accuracy(sess, val_logits, val_y, val_iters):\n",
    "    acc_total = 0.0\n",
    "    acc_denom = 0\n",
    "    for i in range(val_iters):\n",
    "        logits, y = sess.run((val_logits, val_y))\n",
    "        y_pred = np.argmax(logits, axis=1)\n",
    "        correct = np.count_nonzero(y == y_pred)\n",
    "        acc_denom += y_pred.shape[0]\n",
    "        acc_total += float(correct)\n",
    "        tf.logging.info('Validating batch [{} / {}] correct = {}'.format(\n",
    "            i, val_iters, correct))\n",
    "    acc_total /= acc_denom\n",
    "    return acc_total\n",
    "\n",
    "def accuracy_summary(sess, acc_value, iteration):\n",
    "    acc_summary = tf.Summary()\n",
    "    acc_summary.value.add(tag=\"accuracy\", simple_value=acc_value)\n",
    "    sess._hooks[1]._summary_writer.add_summary(acc_summary, iteration)\n",
    "\n",
    "###########################################################\n",
    "###  Define set of VGG variables to restore\n",
    "###  Create the Restorer\n",
    "###  Define init callback (used by monitored session)\n",
    "###########################################################\n",
    "\n",
    "vars_to_restore = tf.contrib.framework.get_variables_to_restore(\n",
    "    exclude=['vgg_19/fc8'])\n",
    "vgg_restorer = tf.train.Saver(vars_to_restore)\n",
    "\n",
    "def init_fn(scaffold, sess):\n",
    "    vgg_restorer.restore(sess, VGG_PRETRAINED_CKPT)\n",
    "\n",
    "###########################################################\n",
    "###  Create various training structures\n",
    "###########################################################\n",
    "\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "lr = tf.train.polynomial_decay(LR_START, global_step, train_iters, LR_END)\n",
    "tf.summary.scalar('learning_rate', lr)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum=MOMENTUM)\n",
    "training_op = slim.learning.create_train_op(\n",
    "    loss, optimizer, global_step=global_step)\n",
    "scaffold = tf.train.Scaffold(init_fn=init_fn)\n",
    "\n",
    "###########################################################\n",
    "###  Create monitored session\n",
    "###  Run training loop\n",
    "###########################################################\n",
    "\n",
    "with tf.train.MonitoredTrainingSession(checkpoint_dir=CHECKPOINT_DIR,\n",
    "                                       save_checkpoint_secs=600,\n",
    "                                       save_summaries_steps=30,\n",
    "                                       scaffold=scaffold) as sess:\n",
    "    start_iter = sess.run(global_step)\n",
    "    for iteration in range(start_iter, train_iters):\n",
    "\n",
    "        # Gradient Descent\n",
    "        loss_value = sess.run(training_op)\n",
    "\n",
    "        # Loss logging\n",
    "        if iteration % LOG_LOSS_EVERY == 0:\n",
    "            tf.logging.info('[{} / {}] Loss = {}'.format(\n",
    "                iteration, train_iters, loss_value))\n",
    "\n",
    "        # Accuracy logging\n",
    "        if iteration % CALC_ACC_EVERY == 0:\n",
    "            sess.run(val_ds_iterator.initializer)\n",
    "            acc_value = calc_accuracy(sess, val_logits, val_y, val_iters)\n",
    "            accuracy_summary(sess, acc_value, iteration)\n",
    "            tf.logging.info('[{} / {}] Validation accuracy = {}'.format(\n",
    "                iteration, train_iters, acc_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После запуска обучения можно посмотреть на его ход с помощью утилиты TensorBoard, которая поставляется в комплекте с TensorFlow и служит для визуализации различных метрик и других параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце обучения в TensorBoard мы наблюдаем практически идеальную картину: снижение Train loss и рост Validation Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате мы получаем сохранённый снэпшот в checkpoints/vgg19_food, который будем использовать во время тестирования нашей модели (inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЕСЛИ ЭТО АКТУАЛЬНО, МОЖНО ДОБАВИТЬ ЕЩЕ ИЗ ТОЙ ЖЕ СТАТЬИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_0.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
