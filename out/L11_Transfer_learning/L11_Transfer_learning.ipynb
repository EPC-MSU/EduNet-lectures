{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Обучение на реальных данных</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проблемы при работе с реальной задачей машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальных задачах, особенно если вы работаете над новой проблемой, вы столкнетесь с широким спектром проблем. Приведем часть из них и сопроводим примером на основе задач нахождения клеток крови на фотографии мазка крови\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/bloods_cell.jpg\" width=\"500px\">\n",
    "\n",
    "- **нехватка данных** - фотографий мазков крови может быть недостаточно для построения сложной модели с нуля\n",
    "\n",
    "- **недостаток размеченных данных** - возможно, существует достаточно большое количество фотографий мазков крови (например, в историях болезни), но очень малая часть из них размечена\n",
    "\n",
    "- **некачественная разметка** - мазок крови могли доверить анализировать студенту-практиканту. Размечать его мог вообще человек не из профессии - например, хотевший таким образом увеличить обучающую выборку для модели на конкурс kaggle. Даже в широко известных MNIST, CIFAR и ImageNet есть ошибки в разметке ([визуализация](https://labelerrors.com/))\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/wrong_imagenet_prediction.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "- **низкое качество данных** - не все фотографии будут в хорошем разрешении. Да и не все образцы были правильно подготовлены. А еще могут попадаться фотографии от пациентов, больных чем-нибудь экзотическим, в результате чего их клетки выглядят сильно отличительно от обычных. Или же в крови пациента просто плавают паразиты\n",
    "\n",
    "Серповидная клеточная анемия приводит к аномальным эритроцитам\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/normal_and_sickle_shaped_erythrocytes.JPG\" width=\"500px\">\n",
    "\n",
    "<p><em><a href=\"https://commons.wikimedia.org/wiki/File:Sicklecell3.jpg\">Изображение</a></p> </em>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "А так выглядит мазок крови при сонной болезни \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/trypanosoma_among_red_blood_cells.jpg\" width=\"500px\">\n",
    "\n",
    "  \n",
    "\n",
    "- **несбалансированность датасета** - клетки крови встречаются в разных пропорциях. Какие классы могут быть плохо представлены (**минорные классы**) Потому в вашем датасете может быть всего 10 фотографий, на которых присутствуют базофилы. Нейросети будет очень заманчиво вообще не пытаться найти базофилы (всего 10 ошибок). \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/leukocyte_blood_formula_table.png\" >\n",
    "\n",
    "- **полные дупликаты** - в данных могут быть полные дубликаты. Кто-то до вас агрегировал фотографии из разных источников и вы либо не обратили на это внимание, либо он забыл об этом сказать. Такие данные надо сразу помечать и использовать только после предварительного размышления, т.к они могут мешать вам и на этапе обучения модели, и на итоговой валидации ее качества (**если один и тот же объект попадет и в обучение**, и в валидацию). \n",
    "\n",
    "- **неполные дубликаты** - в данных могут быть данные от одного и того же пациента. Кажется, что если это разные мазки крови - то ок. На самом деле и это уменьшает количество информации, которую может извлечь нейросеть из данных. С такими данными также нужно аккуратно работать и не допускать попадания одного пациента и в обучение, и в тест\n",
    "\n",
    "- **малое число источников данных** - проблема, родственная предыдущей. В вашем датасете могут быть данные только от одного микроскопа или одной модели микроскопа. Могут быть данные, снятые только одним специалистом или в одной больнице или только у взрослых (фотографий мазков детей нет). Это так же может влиять на способность вашего алгоритма генерализовывать полученное решение и требует пристального внимания. \n",
    "\n",
    "\n",
    "Все это приводит к целому спектру проблем, из которых самой типичной будет переобучение модели - какую простую б модель вы не взяли, она все равно будет выучивать искажения вашего датасета. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общие подходы при работе с реальными данными\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Большее количество данных \n",
    "\n",
    "Если у вас мало данных - попробуйте найти еще данных для вашей задачи. \n",
    "Совет приводится во многих инструкциях по борьбе с малым количеством данных и может быть воспринят с юмором. \n",
    "Однако часто для вашей задачи действительно существуют данные, собранные другими людьми. Также часто можно найти данные, которые очень похожи на ваши и их можно использовать в обучении, но, например, учитывать с меньшим весом.\n",
    "Даже 20 дополнительных примеров могут сильно облегчить ситуацию. \n",
    "\n",
    "Вы также можете использовать данные, которые не совсем похожи на ваши, в качестве внешней валидации. Тем самым вы можете разбивать свой изначальный датасет только для кросс-валидации, и не выделять отдельную часть для теста. Тестом послужат как раз \"не совсем\" похожие данные. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изменение баланса класса сэмплированием \n",
    "\n",
    "Если в данных недостаток именно конкретного класса, то можно бороться с этим при помощи разных способов сэмплирования. \n",
    "\n",
    "Важно понимать, что в большинстве случаев данные, полученные таким способом должны использоваться в качестве **обучающего набора**, но ни в коем случае не в качестве **валидации** или **теста**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "\n",
    "wine = load_wine()\n",
    "_, counts = torch.unique(torch.tensor(wine['target']),return_counts = True)\n",
    "\n",
    "plt.bar(wine.target_names,counts)\n",
    "plt.title('CIFAR10 accuracy')\n",
    "plt.xlabel('model')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дублирование примеров меньшего класса (oversampling)\n",
    "\n",
    "Мы можем увеличить число объектов меньшего класса за счет дублирования. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/oversampling_scheme.png\" width=\"600\">\n",
    "\n",
    "В этом случае наша модель будет \"вынуждена\" обращать внимание на минорный класс. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уменьшение числа примеров бОльшего класса (undersampling)\n",
    "\n",
    "Аналогично, можно взять для обучения не всех представителей бОльшего класса. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/undersampling_scheme.png\" width=\"600\"> \n",
    "\n",
    "Это также вынуждает модель обращать внимание на оба класса. \n",
    "Минус подхода очевиден - мы можем выбросить важных представителей бОльшего класса, ответственных за существенное улучшение генерализации,  и за счет этого качество модели существенно ухудшится. \n",
    "Можно пытаться выбрасывать объекты бОльшего класса как-то по-умному - например, кластеризовать объекты бОльшего класса и брать по заданному количеству объектов из каждого класса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ансамбли + undersampling\n",
    "\n",
    "Можно использовать ансамбли вместе с undersampling. В этом случае мы можем, к примеру, делать сэмплирование только бОльшего класса, а объекты минорного класса оставлять как есть. \n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/ensembles_and_undersampling.png\" width=\"700px\">\n",
    "\n",
    "Или просто сэмплировать объектов и того, и другого класса равное количество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балансирование представленности объектов в батчах\n",
    "\n",
    "В случае нейросетей можно балансировать встречаемость каждого класса не на уровне датасета, а на уровне батча. Например, собираем каждый батч таким образом, чтобы в нем было поровну всех классов.\n",
    "\n",
    " Это может улучшать сходимость даже в случае небольшого дисбаланса или его отсутствия, т.к мы будем избегать шагов обучения нейросети, в которых она просто не увидела какого-то класса в силу случайных причин.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/batch_balancing.png\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# https://pytorch.org/docs/stable/generated/torch.unique.html\n",
    "\n",
    "_, counts = torch.unique(torch.tensor(wine['target']),return_counts = True)\n",
    "print(counts.max())\n",
    "weights = counts.max() / counts\n",
    "print('Классы: ',wine.target_names)\n",
    "print('Веса классов: ', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from torch.utils.data import DataLoader,  TensorDataset, WeightedRandomSampler\n",
    "\n",
    "tensor_x = torch.Tensor(wine.data) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(wine.target)\n",
    "print(tensor_x.shape)\n",
    "dataset = TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "batch_size= 8\n",
    "\n",
    "weight_for_every_sample = []\n",
    "for l in wine.target:\n",
    "  weight_for_every_sample.append(weights[l].item())\n",
    "\n",
    "sampler = WeightedRandomSampler(torch.tensor(weight_for_every_sample), len(dataset))\n",
    "loader = DataLoader(dataset, batch_size= batch_size, shuffle = False, num_workers=1, sampler = sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "for data, labels in loader:\n",
    "  print(labels, torch.unique(labels,return_counts = True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets and preds must be calculated during training\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( wine.data, wine.target, test_size=0.4, random_state=42)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(y_test, preds))\n",
    "print(wine.target_names)\n",
    "conf_matrix.columns = wine.target_names#wine.classes\n",
    "conf_matrix.index = wine.target_names\n",
    "\n",
    "conf_matrix = conf_matrix.rename_axis('Real')\n",
    "conf_matrix = conf_matrix.rename_axis('Predicted', axis='columns')\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация синтетических данных\n",
    "\n",
    "Другой подход к решению этой проблемы - создание синтетических данных. Делать это можно по-разному. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация данных на основе имеющихся \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE\n",
    "\n",
    "**Synthetic Minority Over-sampling Technique (SMOTE)** позволяет генерировать синтетические данные за счет реальных объектов из минорного класса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм работает следующим образом:\n",
    "\n",
    "1. для случайной точки из минорного класса выбираем $k$ ближайших соседей из того же класса. \n",
    "2. Для первого соседа проводим отрезок, соединяющий его и выбранную точку. На этом отрезке случайно выбираем точку. \n",
    "3. Эта точка - новый **синтетический** объект минорного класса.\n",
    "4. Повторяем процедуру для оставшихся соседей.\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/generate_synthetic_data.png\" width=\"700\">\n",
    "\n",
    "Число соседей, как и число раз, которое мы запускаем описанную выше процедуру можно регулировать.\n",
    "\n",
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что можно сделать, что бы как-то улучшить ситуацию, используя методы ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Можно **изменить функцию потерь**: для задач классификации мы часто используем кросс-энтропийный лосс и редко используем среднюю абсолютную ошибку или среднеквадратичную ошибку для обучения и оптимизации нашей модели.\n",
    "\n",
    "- В случае несбалансированных данных, модель учится распознавать доминирующий класс чаще чем все остальные, соответственно, она выучивает статистическое распределение классов и считает что чем чаще она предсказывает этот класс - тем меньше она ошибается. Что бы как-то решить эту проблему - мы можем **добавить веса к потерям**, соответствующим различным классам, чтобы выровнять это смещение данных. Например, если у нас есть два класса в соотношении 4:1, мы можем применить веса в соотношении 1:4 к вычислению функции потерь, чтобы данные были сбалансированы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Without weights\n",
    "scores = torch.tensor([[2.,30.],[2.,30.]]) # Scores for two samples batch\n",
    "target = torch.tensor([0,1]) # First sample belongs to class 0 second to 1 and first was misclassified\n",
    "weights = torch.tensor([1,1],dtype = torch.float32)\n",
    "criterion = torch.nn.CrossEntropyLoss( weight = weights)  \n",
    "criterion(scores,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([10,1],dtype = torch.float32)\n",
    "criterion = torch.nn.CrossEntropyLoss( weight = weights)  \n",
    "criterion(scores,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнаружение аномалий / изменений\n",
    "\n",
    "В случае сильно несбалансированных наборов данных, таких как мошенничество или машинный сбой, стоит задуматься, могут ли такие примеры рассматриваться как аномалия (выброс) или нет. Если такое событие и впрямь может считаться аномальным, мы можем использовать такие модели, как `OneClassSVM`, методы кластеризации или методы обнаружения гауссовских аномалий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти методы требуют изменения способа мышления: мы будем рассматривать аномалии, как отдельный класс выбросов. Это может помочь нам найти новые способы разделения и классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обнаружение изменений похоже на обнаружение аномалий, за исключением того, что мы ищем изменение или отличие, а не аномалию. Это могут быть изменения в поведении пользователя, наблюдаемые по шаблонам использования или банковским транзакциям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем пример обнаружения аномалий с помощью `OneClassSVM` из библиотеки Sk-Learn (там же, можно найти еще множество различных алгоритмов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create dataset\n",
    "X_data, _ = make_blobs(n_samples=1000, centers=1, cluster_std=1.1, center_box=(0, 0))\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1], alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроим наш детектор аномалий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "svm = OneClassSVM(kernel=\"rbf\", gamma=0.01, nu=0.03)\n",
    "print(f'SVM parameters:\\n{svm}')\n",
    "svm.fit(X_data)\n",
    "pred = svm.predict(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем аномальные точки предсказанные детектором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "anom_index = np.where(pred == -1)\n",
    "values = X_data[anom_index]\n",
    "\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1], alpha=0.25)\n",
    "plt.scatter(values[:, 0], values[:, 1], color=\"r\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не используйте точность для несбалансированных наборов данных\n",
    "\n",
    "Будьте осторожны с тем, какие метрики вы используете для оценки ваших ML-моделей. Например, в случае моделей классификации, наиболее часто используемой метрикой является точность (*accuracy*), которая представляет собой долю сэмплов в наборе данных, которые были правильно классифицированы моделью.\n",
    "\n",
    "Этот метод хорошо работает, если классы сбалансированы, т.е. каждый класс представлен одинаковым количество образцов в наборе данных. Но многие наборы данных не являются сбалансированными, и в этом случае точность может быть очень обманчивой метрикой. Рассмотрим, например, датасет, в котором 90% сэмплов представляют один класс, а 10% сэмплов представляют\n",
    "другой класс. Бинарный классификатор, который всегда выдает первый класс, независимо от его входных данных, будет иметь точность 90%, несмотря на то, что он совершенно бесполезен. В такой ситуации предпочтительнее использовать такие метрики, как коэффициент каппы Коэна или коэффициент корреляции Мэтьюса (MCC), которые относительно нечувствительны к дисбалансу размеров классов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аугментация\n",
    "Другой способ побороть маленькое количество данных для обучения - аугментация. Сам термин пришел из музыки:\n",
    "\n",
    "**Аугмента́ция** (позднелат. augmentatio — увеличение, расширение) — [техника ритмической композиции в старинной музыке](https://ru.wikipedia.org/wiki/Аугментация).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели глубокого обучения обычно требуют большого количества данных для обучения. В целом, чем больше данных, тем лучше для обучения модели. В то же время получение огромных объемов данных сопряжено со своими проблемами (например, с нехваткой размеченных данных или с трудозатратами сопряженными с разметкой).\n",
    "\n",
    "Вместо того, чтобы тратить дни на сбор данных вручную, мы можем использовать методы аугментации для автоматической генерации новых примеров из уже имеющихся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо увеличения размеченных датасетов, многие методы *self-supervised learning* построены на использовании разных аугментаций одного и того же сэмпла.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/augmentations_examples.png\" width=\"700\"></center>\n",
    "<center><em>Примеры аугментаций картинки. </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важный момент**: при обучении модели мы используем разбиение данных на `train-val-test`. Аугментации стоит применять только на `train`. Почему так? Конечная цель обучения нейросети - это применение на реальных данных, которые сеть не видела. Вот и портить их не надо.\n",
    "\n",
    "В любом случае, `test` должен быть отделен от данных еще до того как они попали в `DataLoader` или нейросеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое дело, что аугментации на тесте можно использовать как метод ансамблинга в случае классификации. Можно взять sample -> создать несколько его копий -> по разному их аугментировать -> предсказать класс на каждой из этих аугментированных копий -> а потом выбрать наиболее вероятный класс голосованием (такой функционал реализован например, в [YOLOv5](https://github.com/ultralytics/yolov5/blob/d204a61834d0f6b2e73c1f43facf32fbadb6b284/models/yolo.py#L121), о которой речь пойдет в следующих лекциях)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и отобразим пример картинки. Картинку отмасштабируем, чтобы она не занимала весь экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "# fix random_seed\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "URL = 'https://edunet.kea.su/repo/EduNet-web_dependencies/L12/capybara_image.jpg'\n",
    "!wget -q $URL -O test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "input_img = Image.open('/content/test.jpg')\n",
    "input_img = transforms.Resize(size=300)(input_img) \n",
    "display(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций картинок. С полным списком можно ознакомиться на сайте [[doc] документации torchvision](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие параметры принимает на вход `Random Rotation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.RandomRotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим переменную `transform`, в которую добавим нашу аугментацию и применим ее к исходному изображению. Затем запустим следующую ячейку несколько раз подряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.RandomRotation(degrees=(0, 180))\n",
    "\n",
    "def plot_augmented_img(transform, input_img):\n",
    "    \n",
    "    f,ax = plt.subplots(1,2,figsize=(15,15))\n",
    "    augmented_img = transform(input_img)\n",
    "    ax[0].imshow(input_img)\n",
    "    ax[0].set_title('Original img')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(augmented_img)\n",
    "    ax[1].set_title('Augmented img')\n",
    "    ax[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.GaussianBlur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.RandomErasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [ \n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=1),\n",
    "        transforms.ToPILImage()\n",
    "    ]\n",
    ")\n",
    "    \n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ColorJitter(brightness=.5, hue=.3)\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого будем использовать метод `Compose`. Нам нужно будет создать `list` со всеми аугментациями, которые будут применены последовательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0),\n",
    "        transforms.ColorJitter(brightness=.5, hue=.3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А что если мы хотим применять аугментации случайным образом?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого воспользуемся методом `RandomApply`, который на вход принимает метод аугментации, и вероятность `p`, что эта аугментация будет применена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.RandomApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomApply(\n",
    "    transforms=[\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5),\n",
    "        transforms.ColorJitter(brightness=.5, hue=.3)\n",
    "    ],\n",
    "    p=0.7\n",
    ")\n",
    "\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация внутри `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем папку с картинками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.chdir('/content')\n",
    "# download files\n",
    "!wget --no-check-certificate 'https://edunet.kea.su/repo/EduNet-web_dependencies/L11/for_transforms.Compose.zip' -O data.zip\n",
    "with ZipFile('data.zip', 'r') as folder: # Create a ZipFile Object and load sample.zip in it\n",
    "    folder.extractall() # Extract all the contents of zip file in current directory\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/content/for_transforms.Compose\")\n",
    "img_list = os.listdir()\n",
    "print(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, img_list, transforms=None):\n",
    "        self.img_list = img_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = plt.imread(self.img_list[i])\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "        img = np.array(img).astype(np.uint8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вспомогательную функцию для отображения картинок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.figure(figsize=(40, 38))\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `list` с аугментациями, которые мы хотим применить. Чтобы загрузить аугментации в `PyTorch`, нам необходимо эти картинки преобразовать в тензоры. Для этого воспользуемся родным торчевым преобразованием `transforms.ToTensor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose(\n",
    "    [   \n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обернем все в `DataLoader` и отобразим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, tensor_transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует и более сложные способы аугментации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mixup**\n",
    "\n",
    "Mixup можно представить с помощью этого простого уравнения:\n",
    "\n",
    "`newImage = alpha * image1 + (1-alpha) * image2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/mixup_augmentation_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее в статьях:\n",
    "\n",
    "[mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)\n",
    "\n",
    "[On Mixup Training](https://proceedings.neurips.cc/paper/2019/file/36ad8b5f42db492827016448975cc22d-Paper.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Аугментация при помощи синтеза данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/augmentation_using_data_synthesis.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме методов, реализованных в Pytorch, существуют и специализированные библиотеки для аугментации изображений. \n",
    "\n",
    "Например:\n",
    "- [lbumentations](https://albumentations.ai)\n",
    "- [imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "- [augly](https://github.com/facebookresearch/AugLy)\n",
    "\n",
    "**При выборе методов аугментации имеет смысл использовать те, которые будут в реальной жизни. Например, нет смысла делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным, или отражать человека вверх-ногами, если мы не предполагаем его таким распознавать.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций аудио. С полным списком можно ознакомиться здесь [[git] audiomentations](https://github.com/iver56/audiomentations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеку и посмотрим на пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "\n",
    "!pip install audiomentations\n",
    "\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L11/audio_example.wav\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Get input audio\n",
    "input_audio ='/content/audio_example.wav'\n",
    "\n",
    "display(Audio(input_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "data, sr = librosa.load('/content/audio_example.wav') # sr - sampling rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import AddGaussianSNR\n",
    "import numpy as np\n",
    "\n",
    "augment = AddGaussianSNR(min_snr_in_db=3, max_snr_in_db=7, p=1)\n",
    "\n",
    "# Augment/transform the audio data\n",
    "augmented_data = augment(samples=data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним волновые картины и спектрограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "\n",
    "def produce_plots(input_audio_arr, aug_audio, sr):\n",
    "    f,t, Sxx_in = spectrogram(input_audio_arr, fs=sr) # Compute spectrogram for the original signal (f - frequency, t - time)\n",
    "    f,t, Sxx_aug = spectrogram(aug_audio, fs=sr)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20,5)) \n",
    "\n",
    "    ax[0,0].plot(input_audio_arr)\n",
    "    ax[0,0].set_xlim(0,len(input_audio_arr))\n",
    "    ax[0,0].set_xticks([])\n",
    "    ax[0,0].set_title('Original audio')\n",
    "\n",
    "    ax[0,1].plot(aug_audio)\n",
    "    ax[0,1].set_xlim(0,len(input_audio_arr))\n",
    "    ax[0,1].set_xticks([])\n",
    "    ax[0,1].set_title('Augmented  audio')\n",
    "\n",
    "    ax[1,0].imshow(np.log(Sxx_in), \n",
    "                   extent = [t.min(), t.max(), f.min(), f.max()],\n",
    "                   aspect='auto',\n",
    "                   cmap='inferno')\n",
    "    ax[1,0].set_ylabel('Frequecny, Hz')\n",
    "    ax[1,0].set_xlabel('Time,s')\n",
    "    \n",
    "    ax[1,1].imshow(np.log(Sxx_aug), \n",
    "                   extent = [t.min(), t.max(), f.min(), f.max()],\n",
    "                   aspect='auto',\n",
    "                   cmap='inferno')\n",
    "    ax[1,1].set_ylabel('Frequecny, Hz')\n",
    "    ax[1,1].set_xlabel('Time,s')\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import TimeStretch\n",
    "\n",
    "augment = TimeStretch(min_rate=0.8, max_rate=1.5, p=1) \n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitch Shift\n",
    "\n",
    "Изменение тональности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import PitchShift\n",
    "\n",
    "augment = PitchShift(min_semitones=1, max_semitones=12, p=1)\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае с картинками мы можем совмещать несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, Shift\n",
    "\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=1),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=1),\n",
    "    Shift(min_fraction=-0.5, max_fraction=0.5, p=1),\n",
    "])\n",
    "\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные библиотеки для аугментации звука (и волновых функций в целом):\n",
    "- [torchaudio](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)\n",
    "- [torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations)\n",
    "- [Augly](https://github.com/facebookresearch/AugLy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим несколько примеров аугментаций текста. С полным списком можно ознакомиться здесь [[git] сайте библиотеки](https://github.com/makcedward/nlpaug)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input text\n",
    "text = \"Hello, future of AI for Science! How are you today?\"\n",
    "print(f'input text: {text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация символов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменой на похоже выглядящие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "augment = nac.OcrAug()\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С опечатками, которые учитывают расположение символов на клавиатуре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = nac.KeyboardAug()\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация слов\n",
    "\n",
    "С орфографическими ошибками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "augment = naw.SpellingAug()\n",
    "augmented_text = augment.augment(text, n=3)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С использованием модели для предсказания новых слов в зависимости от контекста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# model_type: word2vec, glove or fasttext\n",
    "augment = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=\"insert\")\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем переводить текстовые данные на какой-либо язык, а затем перевести их обратно на язык оригинала. Это может помочь сгенерировать текстовые данные с разными словами, сохраняя при этом контекст текстовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='facebook/wmt19-en-de', \n",
    "    to_model_name='facebook/wmt19-de-en'\n",
    ")\n",
    "augmented_text = back_translation_aug.augment(text)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные  библиотеки для аугментации текста:\n",
    "\n",
    "\n",
    "- [TextAugment](https://github.com/dsfsi/textaugment)\n",
    "- [Augly](https://github.com/facebookresearch/AugLy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "----\n",
    "Как обучить нейросеть на своих данных, когда их мало?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для таких типовых задач, как классификация изображений, можно воспользоваться одной из существующих архитектур (AlexNet, VGG, Inception, ResNet и т.д.) и просто обучить нейросеть на своих данных. Реализации таких сетей с помощью различных фреймворков уже существуют, так что на данном этапе можно использовать одну из них как черный ящик, не вникая в принцип её работы. Например, в PyTorch есть много уже реализованных известных архитектур: [`torchvision.models`](https://pytorch.org/vision/stable/models.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, глубокие нейронные сети требуют большие объемы данных для успешного обучения. И зачастую, в нашей частной задаче недостаточно данных для того, чтобы хорошо обучить нейросеть с нуля. `Transfer learning` решает эту проблему. По сути, мы пытаемся использовать опыт, полученный нейронной сетью при обучении на некоторой задаче $T_1$, чтобы решать схожую задачу $T_2$.\n",
    "\n",
    "К примеру, `transfer learning` можно использовать при решении задачи классификации изображений на небольшом наборе данных. Как уже ранее обсуждалось, при обработке изображений свёрточные нейронные сети в первых слоях \"реагируют\" на некие простые пространственные шаблоны (к примеру, углы), после чего комбинируют их в сложные осмысленные формы (к примеру, глаза или носы). Вся эта информация извлекается из изображения, создавая новые сложные представления данных, в результате классифицируемые линейной моделью. \n",
    "\n",
    "Идея заключается в том, что если изначально обучить модель на некоторой сложной и довольно общей задаче, можно надеяться что она (как минимум часть ее слоев), в общем, случае будет извлекать важную информацию из изображений, и полученные представления можно будет успешно использовать для классификации линейной моделью.\n",
    "\n",
    "Таким образом, берем часть модели, которая, по нашему представлению, отвечает за выделение хороших признаков (часто - все слои, кроме последнего) - feature extractor. Присоединяем к это части дополнительные слой/cлои для решения уже новой задачи. И учим только эти слои. Cлои feature extractor не учим - они \"заморожены\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/transfer_learning_change_classes_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятно, что не все фильтры модели будут использованы максимально эффективно - к примеру, если мы работаем с изображениями, связанными с едой, возможно, не все фильтры на скрытых слоях предобученной на ImageNet модели окажутся полезны для нашей задачи. Почему бы не попробовать **не только обучить новый классификатор, но и дообучить некоторые промежуточные слои**? При использовании этого подхода мы при обучении дополнительно \"настраиваем\" и промежуточные слои, называется он `fine-tuning`.\n",
    "\n",
    "В дальнейшем примере мы вообще не будем фиксировать веса - этот вариант, по сути, также относится к `fine-tuning`. В таком подходе learning rate ставится ниже, чем при обучении нейросети с нуля - мы знаем, что, по крайней мере, часть весов нейросети выполняют свою задачу хорошо, и не хотим испортить это быстрыми первыми изменениями. \n",
    "\n",
    "Кроме этого, можно делать комбинации этих методов - сначала учить только последние, добавленные нами слои сети. Затем учить еще и самые близкие к ним. Затем учить уже все веса нейросети вместе. То есть мы можем определить свою **стратегию fine-tuning**. \n",
    "\n",
    "Иногда **fine-tuning** считается синонимом \"transfer learning\", в этом случае часть от предтренированной сети называют **backbone**, а добавленную часть - **head**.  Подробнее про это можно прочитать [здесь](https://lightning-flash.readthedocs.io/en/latest/general/finetuning.html)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структурные компоненты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения данной задачи нам понадобятся несколько компонент.\n",
    "\n",
    "1. В первую очередь, нам необходима сама обученная модель, которую мы будем адаптировать под нашу локальную задачу. Модель можно получить из фреймворка, либо скачать по ссылке, приложенной авторами интересующей Вас статьи. \n",
    "\n",
    "2. Также, как и при работе с обычной моделью, понадобится кодовая база для обучения, включающая в себя известные шаги: подсчёт градиента, шаг оптимизатора, подсчёт качества на проверочной выборке и т.п.  \n",
    "\n",
    "3. После того как обучение будет выполнено, понадобится использовать модель в реальных условиях, либо как минимум проверить её качество на тестовой выборке. В данном случае, нам понадобится кодовая база для выполнения предсказаний. \n",
    "\n",
    "4. В некоторых случаях, веса модели находятся отдельно от описания (архитектуры) модели, в таком случае необходимо дополнительно озаботиться их получением и загрузкой в модель.  \n",
    "\n",
    "5. Последняя важная компонента - данные. Необходимо тщательно подготовить данные, на которых будет дообучаться модель, проверить разбиение на обучающую, проверяющую и тестировочную выборки, чтобы избежать утечек данных и т.п.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/structural_components_scheme.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практический пример Transfer Learning\n",
    "\n",
    "Давайте рассмотрим пример практической реализации такого подхода ([код переработан из этой статьи](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Загрузим датасет и удалим из него 90% файлов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L11/EuroSAT.zip # http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
    "!unzip EuroSAT.zip\n",
    "\n",
    "os.chdir('/content')\n",
    "path = '/content/2750/'\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "  files = os.listdir(path+folder)\n",
    "  for file in sample(files,int(len(files)*0.9)):\n",
    "      os.remove(path+folder+'/'+file)\n",
    "      \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "img_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "dataset = datasets.ImageFolder(root=path, transform = img_transforms )\n",
    "# split to train/valid/test\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.1), int(len(dataset)*0.1)])\n",
    "\n",
    "print(f'Train size: {len(train_set)}')\n",
    "print(f'Valid size: {len(valid_set)}')\n",
    "print(f'Test size: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size, valid_data_size = len(train_set), len(valid_set)\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "print('indexes to class: ')\n",
    "idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наборе данных не так уж и много изображений. При обучении с нуля нейросеть скорее всего не достигнет высокой точности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим AlexNet без весов и попробуем обучить с нуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "alexnet = models.alexnet(pretrained=False)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы заменяем последний слой модели AlexNet слоем с `num_classes` нейронами, в соответствии с числом классов в нашем подмножестве.\n",
    "\n",
    "То есть мы \"сказали\" нашей модели распознавать не `1000`, а только `num_classes` классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "import torch.nn as nn\n",
    "\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes) # change out classes, from 1000 to 10\n",
    "\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы определяем функцию потерь и оптимизатор, который будет использоваться для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-4)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренировки и валидации нашей модели напишем отдельную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_and_validate(model, criterion, optimizer, num_epochs=25, save_state = False):\n",
    "    \"\"\"\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "\n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clean existing gradients\n",
    "            outputs = model(inputs)  # Forward pass - compute outputs on input data using the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the parameters\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Compute correct predictions\n",
    "            train_correct += (torch.argmax(outputs, dim=-1)== labels).float().sum()\n",
    "\n",
    "        # Compute the mean train accuracy\n",
    "        train_accuracy = 100 * train_correct / (len(train_loader)*batch_size)\n",
    "\n",
    "        val_correct = 0\n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.eval()  # Set to evaluation mode\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass - compute outputs on input data using the model\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                valid_loss += loss.item() * inputs.size(0)  # Compute the total loss for the batch and add it to valid_loss\n",
    "\n",
    "                val_correct += (torch.argmax(outputs, dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Compute mean val accuracy       \n",
    "        val_accuracy = 100 * val_correct / (len(valid_loader)*batch_size)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss / (len(train_loader)*batch_size)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss / (len(valid_loader)*batch_size)\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss,train_accuracy.detach().cpu(), val_accuracy.detach().cpu()])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(\n",
    "            \"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
    "                epoch + 1,\n",
    "                avg_train_loss,\n",
    "                train_accuracy.detach().cpu(),\n",
    "                avg_valid_loss,\n",
    "                val_accuracy.detach().cpu(),\n",
    "                epoch_end - epoch_start,\n",
    "            )\n",
    "        )\n",
    "        #Saving state for fine_tuning (because we may overfit)\n",
    "        if save_state:\n",
    "          os.makedirs('check_points', exist_ok=True)\n",
    "          torch.save(model.state_dict(), f'check_points/fine_tuning_{epoch + 1}.pth')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), criterion, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_fresh.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "fig.suptitle(\"Fresh learning\", fontsize=14)\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты оставляют желать лучшего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем использовать *transfer learning*. \n",
    "\n",
    "Загрузим предобученную модель Alexnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del alexnet\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае, мы не дообучаем скрытые слои нашей модели, потому отключаем подсчёт градиентов (\"**замораживаем**\" параметры). Данный шаг гарантирует, что обучение (изменение параметров) будет происходить только на последнем слое модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Меняем последний слой таким образом, чтобы он вместо 1000 классов, ImageNet нам выдавал количество классов, которое у нас есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), criterion, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_transfer_learning.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "fig.suptitle(\"Transfer learning\", fontsize=14)\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сравним между собой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "fig.suptitle(\"Fresh Learning (FL) vs Transfer Learning (TL)\", fontsize=14)\n",
    "\n",
    "history_fresh = np.array(torch.load('history_fresh.pt'))\n",
    "history_transfer_learning = np.array(torch.load('history_transfer_learning.pt'))\n",
    "\n",
    "ax[0].plot(history_fresh[:,:2])\n",
    "ax[0].plot(history_transfer_learning[:,:2])\n",
    "ax[0].legend([\"Train Loss (FL)\", \"Val Loss (FL)\", \"Train Loss (TL)\", \"Val Loss (TL)\"])\n",
    "\n",
    "ax[1].plot(history_fresh[:,2:])\n",
    "ax[1].plot(history_transfer_learning[:,2:])\n",
    "ax[1].legend([\"Train Accuracy (FL)\", \"Val Accuracy (FL)\", \"Train Accuracy (TL)\", \"Val Accuracy (TL)\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определенно стало лучше =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём процедуру **fine-tuning**. \n",
    "В предыдущем варианте с Transfer Learning обучались только последние слои, добавленные вручную. Давайте проверим это, выведя те слои, в которых включён градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in alexnet.named_parameters(): \n",
    "    print (name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы оставим дообученную голову нейронной сети и продолжим **обучение всей сети с уменьшением темпа обучения**. \n",
    "\n",
    "**Разморозим** параметры. `Criterion` остаётся тот же, в `optimizer` уменьшим параметр `lr` на порядок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пройдём дополнительные 20 эпох и построим графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), criterion, optimizer, num_epochs,\n",
    "    save_state = True\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_finetuning.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "fig.suptitle(\"Transfer Learning (TL) AND Finetuning (FT)\", fontsize=14)\n",
    "\n",
    "history_transfer_learning = np.array(torch.load('history_transfer_learning.pt'))\n",
    "history_finetuning = np.array(torch.load('history_finetuning.pt'))\n",
    "\n",
    "train_val_loss = np.concatenate((history_transfer_learning[:,:2], history_finetuning[:,:2]), axis=0)\n",
    "ax[0].plot(train_val_loss)\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "\n",
    "ax[0].vlines(20, 0, 1.5,\n",
    "          color = 'g',\n",
    "          linewidth = 5,\n",
    "          linestyle = '--')\n",
    "\n",
    "train_val_acc = np.concatenate((history_transfer_learning[:,2:], history_finetuning[:,2:]), axis=0)\n",
    "\n",
    "ax[1].plot(train_val_acc)\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[1].vlines(20, 0, 100,\n",
    "          color = 'g',\n",
    "          linewidth = 5,\n",
    "          linestyle = '--')\n",
    "\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли эффект от **fine-tuning**?. После дообучения ещё на 20 эпохах мы наблюдаем следующие эффекты:\n",
    "\n",
    "\n",
    "*   На валидации: Loss и accuracy вышли на плато, ошибка уменьшилась, точность возросла.\n",
    "*   На обучении: Loss стремится к 0, тогда как точность к 100%. Началось **переобучение** примерно с 25 эпохи. Fine-tuning склонен к переобучению (этого можно попробовать избежать, делая \"голову\" нейросети многослойной, плавно уменьшая количество нейронов), следите за метриками и ошибкой на тренировочной выборке.\n",
    "*   В промежутке эпох 20-25 мы **добились заметного улучшения** работы модели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ожидая переобучения, мы сохранили состояния нейросети на каждой эпохе. Возьём состояние с 24 эпохи как наиболее оптимальное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.load_state_dict(torch.load('check_points/fine_tuning_4.pth')) #20 + 4\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_img_name, device):\n",
    "    \"\"\"\n",
    "    Function to predict the class of a single test image\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param test_img_name: Test image\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    transform = img_transforms[\"test\"]\n",
    "    test_img = torch.tensor(np.asarray(test_img_name)) \n",
    "    test_img = transforms.ToPILImage()(test_img)\n",
    "    plt.imshow(test_img)\n",
    "\n",
    "    test_img_tensor = test_img_name.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_img_tensor).to(device)\n",
    "        ps = torch.exp(out).to(device)\n",
    "        topk, topclass = ps.topk(3, dim=1)\n",
    "        for i in range(3):\n",
    "            print(\n",
    "                \"Predcition\",\n",
    "                i + 1,\n",
    "                \":\",\n",
    "                idx_to_class[topclass.cpu().numpy()[0][i]],\n",
    "                \", Score: \",\n",
    "                round(topk.cpu().numpy()[0][i], 2),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[0])\n",
    "predict(\n",
    "    trained_model.to(device),\n",
    "    valid_set[[np.where([x[1] == 0 for x in valid_set])[0]][0][1]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[6])\n",
    "predict(\n",
    "    trained_model,\n",
    "    valid_set[[np.where([x[1] == 6 for x in valid_set])[0]][0][10]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[8])\n",
    "predict(\n",
    "    trained_model,\n",
    "    valid_set[[np.where([x[1] == 8 for x in valid_set])[0]][0][5]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Мы увидели, как использовать предварительно обученную модель на 1000 классов ImageNet для нашей задачи на 10 классов.\n",
    "\n",
    "* Мы сравнили качество **обучения с нуля, transfer learning и fine-tuing** и научились добиваться максимального качества с помощью этих принципов.\n",
    "\n",
    "* И столкнулись с характерной **опасностью fine-tuning - переобучением**. Используйте **низкий learning rate** и отслеживайте Loss - возможно, вам будет достаточно **небольшого количества эпох**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few/One-Shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют задачи, где не представляется возможным разбить данные на классы так, чтобы в каждом классе было достаточно много объектов.\n",
    "\n",
    "Рассмотрим, например, задачу распознавания лиц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/videoanalytics.png\"  width=\"800\">\n",
    "\n",
    "На вход системе подается фото лица человека. Требуется сопоставить его с другим изображениям(и) например хранящимися в БД и таким образом идентифицировать человека на фотографии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На первый взгляд кажется, что это задача классификации.\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/classifier_scheme.png\"  width=\"700\">\n",
    "\n",
    "\n",
    "Все изображения одного человека будем считать относящимися к одному классу и модель будет этот класс предсказывать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для небольшой организации, в которой всего несколько десятков сотрудников такой подход может сработать.\n",
    "При этом возникнут проблемы:\n",
    "\n",
    "1. Чтобы обучить такую ​​систему, нам сначала потребуется много (сотни) разных \n",
    "изображений каждого сотрудника.\n",
    "\n",
    "2. Когда человек присоединяется к организации или покидает ее, придется поменять структуру модели и обучать ее заново.\n",
    "\n",
    "\n",
    "Это практически невозможно, для крупных организаций, где набор и увольнение происходит почти каждую неделю. И в принципе невозможно для города масштаба Москвы или Лондона, в котором миллионы жителей и сотни тысяч приезжих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование векторов-признаков (embedding)\n",
    "\n",
    "Поэтому используется другой подход.\n",
    "Вместо того что бы классифицировать изображения, модель учится выделять ключевые признаки и на их основе строить компактный вектор достаточно точно описывающий лицо.\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/face_as_embedding.png\"  width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "В англоязычной литературе такие вектора признаков называются **embedding** и мы тоже будем использовать это обозначение.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может возникнуть вопрос: не потеряем ли мы важную информацию, сжав изображение в несколько сотен чисел? \n",
    "\n",
    "Чтобы ответить на него вспомним, как работает [фоторобот](https://en.wikipedia.org/wiki/Facial_composite). \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/photorobot.png\"  width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "Для получения фотореалистичного изображения лица достаточно нескольких ключевых признаков: Глаза, волосы рот, нос ...\n",
    "\n",
    "Каждый, из которых кодируется максимум несколькими сотнями целых значений. \n",
    "\n",
    "Значит вектора - признака из 128 вещественных чисел будет более чем достаточно.\n",
    "Правда интерпретировать значения, которые закодирует в него нейросеть, будет не столь просто.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если нам удастся обучить модель кодировать в embedding признаки важные для сравнения, то мы сможем сравнивать вектора между собой.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/face_dist.png\"  width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "Если расстояние между векторами для лиц, которые похожи друг на друга будут маленькими, а у непохожих наоборот большими. То мы сможем экспериментально подобрать порог $d$ и, сравнивая с ним расстояние между двумя векторами, принимать решение: принадлежат ли они одному человеку или нет.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Можно оценивать не расстояние, а степень схожести similarity. В этом случае неравенства поменяют знак, но логика останется прежней*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, чтобы идентифицировать человека, требуется только одно изображение его лица. Эмбеддинг  этого изображения можно сравнить с эмбеддингами других лиц из БД используя  [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) или иной метод кластеризации.\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/EduNet-content/L11/img_license/search_in_embedding_space.jpg\"  width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая модель не учится классифицировать изображение напрямую по какому-либо из выходных классов. Она учится выделять признаки, важные при сравнении.\n",
    "\n",
    "Такой подход решает обе проблемы, о которых мы говорили выше:\n",
    "- Для обучения такой сети нам не требуется много экземпляров объектов одного класса, а   достаточно лишь нескольких.\n",
    "- Но самое большое преимущество в простоте ее обучения в случае появления новых объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сиамская сеть (Siamese Network)\n",
    "\n",
    "Какая архитектура должна быть у модели генерирующей вектора-признаков?\n",
    "\n",
    "Можно было бы использовать обычную сеть, обученную для задачи классификации, и затем удалить из нее последний(е) слой.\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/embedding_from_classifier.png\"  width=\"800\">\n",
    "\n",
    "\n",
    "\n",
    "Активации последнего слоя представляют собой отклики на некие высокоуровневые признаки потенциально важные для классификации и их можно интерпретировать как embedding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet\n",
    "import torch\n",
    "\n",
    "face1 = torch.randn((3,224,224))\n",
    "face2 = torch.randn((3,224,224))\n",
    "\n",
    "model = alexnet(pretrained = True)\n",
    "# remove classification layer \n",
    "model.fc = model.classifier[6] = torch.nn.Identity()\n",
    "\n",
    "# get embeddings\n",
    "embedding1 = model(face1.unsqueeze(0)) \n",
    "embedding2 = model(face2.unsqueeze(0)) \n",
    "\n",
    "diff = torch.nn.functional.pairwise_distance(embedding1, embedding2 )\n",
    "print(\"L2 distance: \", diff.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход будет работать.\n",
    "Однако можно заметно улучшить точность, используя лосс-функцию, которая оценивает именно качество сравнения, а не классификации.\n",
    "\n",
    "\n",
    "Рассмотрим подход, основанный на методологии, описанной в статье [Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/siamese_neural_network_scheme.png\"  width=\"800\">\n",
    "\n",
    "\n",
    "<center><em>Используются две копии одной и той же сети, отсюда и название Siamese Networks.</em></center>\n",
    "\n",
    "Два входных изображения ($x_1$ и $x_2$) проходят через одну и ту же сверточную сеть, на выходе для каждого изображения генерируется вектор признаков фиксированной длины $h_{x_{1}}$ и $h_{x_{2}}$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучается генерировать близкие вектора для изображений одного объекта и далекие для разных.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/siamese_neural_network_idea_scheme.png\"  width=\"700\">\n",
    "\n",
    "\n",
    "Оценивая расстояние между двумя векторами признаков \n",
    "которое будет малым для одних и тех же объектов и большим для различных мы сможем оценить их сходство.\n",
    "\n",
    "Это центральная идея сиамских сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss\n",
    "\n",
    "Какой loss использовать для обучения такой сети?\n",
    "\n",
    "Очевидно loss функция должна будет учитывать не один выход как минимум два.\n",
    "\n",
    "Популярной на сегодняшний день является  `Triplet loss`, которой требуется  три embedding вместо двух. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/triplet_loss_scheme.png\"  width=\"700\">\n",
    "\n",
    "\n",
    "Чтобы сгенерировать три эмбеддинга, модель должна получать на вход три изображения. \n",
    "\n",
    "Первые два должны относиться к одному и тому же объекту (человеку), а третье к другому.\n",
    "\n",
    "\n",
    "Таким образом, Триплет состоит из базового (\"якорного\" `anchor`), положительного(`positive`) и отрицательного(`negative`) образцов.\n",
    "\n",
    "Описание в статье [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Сама лосс функция будет выглядеть следующим образом:\n",
    "\n",
    "$$TripletLoss = \\sum_{1}^{N} L_i(x_i^{a},x_i^{p},x_i^{n})$$\n",
    "\n",
    "$$L_i(x_i^{a},x_i^{p},x_i^{n})=max(0,\\left\\| f(x_i^{a}) -f(x_i^{p}) \\right\\|_2^{2} - \\left\\| f(x_i^{a}) -f(x_i^{n}) \\right\\|_2^{2} + margin)$$\n",
    "\n",
    "Где:\n",
    "\n",
    "\n",
    "$x_i^{a}$ - базовое изображение (anchor)\n",
    "\n",
    "$x_i^{p}$ - изображение того же объекта (positive)\n",
    "\n",
    "$x_i^{n}$ - изображение другого объекта (negative)\n",
    "\n",
    "$f(x)$ - **нормированный** выход модели (embedding) для входа $x$\n",
    "\n",
    "\n",
    "$\\left\\| x \\right\\|_2$ -это L2 (Euclidean norm), соответственно $\\left\\| a \\right\\|_2^{2}$ это L2 в квадрате.\n",
    "\n",
    "$margin$ - это константа или минимальный 'зазор', на который расстояние до эмбеддинга негативного объекта обязано превосходить расстояние до позитивного (идея такая же, что в SVM Loss) .\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/triplet_loss_idea_scheme.png\"  width=\"900\">\n",
    "\n",
    "В ходе обучения с  Triplet loss расстояние между эмбеддингами опорного объекта и позитивного уменьшается, а для отрицательного увеличивается.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важным дополнением является то, что embedding-и нормируются. В результате нормировки каждый вектор-признак будет иметь единичную длину.\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/sphere_distance.png\"  width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "Теперь мы можем рассматривать embedding-и как точки на n-мерной сфере с радиусом 1. \n",
    "\n",
    "Это удобно, так как все расстояния между embedding будут лежать в интервале [0..2] и нам будет проще подобрать порог для сравнения.\n",
    "\n",
    "Кроме того, можно использовать другие меры расстояния, например [косинусное расстояние](https://buildwiki.ru/wiki/Cosine_similarity), которое определяется углом между векторами и лежит в интервале [-1 ..1] и будет соответствовать расстоянию между точками на поверхности сферы.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье авторы минимизируют Евклидово расстояние, но подход будет работать и для других метрик сходства, например косинусного расстояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Pytorch есть две реализации TripletLoss\n",
    "\n",
    "[TripletMarginLoss](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html) - минимизирует $L_p$ норму "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "loss = triplet_loss(anchor, positive, negative)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TripletMarginWithDistanceLoss](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss) позволяет задать произвольную функцию расстояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    " \n",
    "triplet_loss = nn.TripletMarginWithDistanceLoss( \n",
    "      margin = 1,\n",
    "      distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "    )\n",
    "loss = triplet_loss(anchor, positive, negative)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие Лосс - функции для сиамских сетей:\n",
    "\n",
    "Исторически первой появилась `Contrastive Loss`, о ней подробнее в статье [Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)\n",
    "\n",
    "\n",
    "В Pytorch есть реализация \n",
    "[CosineEmbeddingLoss](https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html), она позволяет обучать модель на парах изображений, минимизировав [косинусное расстояние](https://buildwiki.ru/wiki/Cosine_similarity) между embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация сиамской сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "Загрузим небольшой фрагмент датасета с лицами. Внутри архива фото лиц сгруппированы по папкам\n",
    "\n",
    "\n",
    "\n",
    "*   s1/\n",
    "**  photo1.pgm\n",
    "**  photo2.pgm\n",
    "**  ...\n",
    "*   s2/\n",
    "*   s2/\n",
    "*    ...\n",
    "*   sn/\n",
    "\n",
    "В каждой папке фото лица одного и того же человека."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L11/small_face_dataset.zip\n",
    "!unzip small_face_dataset.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы результаты воспроизводились, зафиксируем SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for TripletLoss\n",
    "\n",
    "Для TripletLoss потребуются три изображения: anchor, positive, negative и метод __get_item__ должен возвращать их нам. Первые два должны принадлежать одному человеку, а третье другому. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,dir=None,transform=None, splitter = '/'):\n",
    "        self.dir = dir\n",
    "        self.splitter = splitter\n",
    "        self.transform = transform        \n",
    "        self.files = glob(f\"{self.dir}/**/*.pgm\",recursive=True)\n",
    "        self.data =self.build_index()        \n",
    "    \n",
    "    def build_index(self):\n",
    "      index = {}      \n",
    "      for f in self.files:\n",
    "        id = self.path2id(f) \n",
    "        if not id in index:\n",
    "          index[id] = []\n",
    "        index[id].append(f)\n",
    "      return index\n",
    "    \n",
    "    def path2id(self,path):\n",
    "        return path.replace(self.dir,\"\").split(self.splitter)[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        anchor_path = self.files[index]\n",
    "        positive_path = self.find_positive(anchor_path)\n",
    "        negative_path = self.find_negative(anchor_path)\n",
    "        \n",
    "        # Loading the images\n",
    "        anchor = Image.open(anchor_path)\n",
    "        positive = Image.open(positive_path)\n",
    "        negative = Image.open(negative_path)\n",
    "                \n",
    "        if self.transform is not None:  # Apply image transformations           \n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive , negative\n",
    "\n",
    "    def find_positive(self,path):\n",
    "        id = self.path2id(path) \n",
    "        all_exept_my = self.data[id].copy()\n",
    "        all_exept_my.remove(path)\n",
    "        return random.choice(all_exept_my)\n",
    "\n",
    "    def find_negative(self,path):\n",
    "        all_exept_my_ids = list(self.data.keys())\n",
    "        id = self.path2id(path) \n",
    "        all_exept_my_ids.remove(id)\n",
    "        selected_id = random.choice(all_exept_my_ids)\n",
    "        return random.choice(self.data[selected_id])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем несколько изображений, чтобы убедиться, что класс датасета функционирует должным образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Create dataset instance\n",
    "siamese_dataset = SiameseNetworkDataset(\"faces/training/\",\n",
    "                                          transform=transforms.Compose([\n",
    "                                            transforms.Resize((105,105)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                          ])\n",
    "                                        )\n",
    "\n",
    "# Create dataloader & extract batch of data from it\n",
    "vis_dataloader = DataLoader(siamese_dataset, batch_size =8, shuffle=True)\n",
    "dataiter = iter(vis_dataloader)\n",
    "example_batch = next(dataiter) # anc, pos, neg\n",
    "\n",
    "# Show batch contents \n",
    "concatenated = torch.cat((example_batch[0],example_batch[1],example_batch[2]),0)\n",
    "grid = torchvision.utils.make_grid(concatenated)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1,2,0).numpy())\n",
    "plt.gcf().set_size_inches(20,60)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждом столбце тройка изображений. Первое и второе принадлежат одному человеку, третье - другому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели\n",
    "\n",
    "Нас устроит любая модель для работы с изображениями. Например, Resnet18. \n",
    "\n",
    "Все что от нас требуется это:\n",
    "- заменить последний слой\n",
    "- отправлять на анализ три изображения вместо одного. Соответственно на выходе тоже будут три вектора признаков (embedding)\n",
    "\n",
    "\n",
    "Пожалуй, единственный вопрос это размерность последнего слоя. В промышленных системах распознавания лиц, которые тренируются на датасетах из миллионов изображений, используются embedding размерностью от 128 до 512.\n",
    "\n",
    "Значит, для демонстрационной задачи нам с запасом хватить 64 значений. Значит,  выход последнего линейного слоя должен быть равен 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(pretrained = False)\n",
    "        # Because we use grayscale images reduce input channel count to one\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        # Replace ImageNet 1000 class classifier with 64- out linear layer \n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 64)\n",
    "    \n",
    "    def _forward(self,x):\n",
    "      out = self.model(x)\n",
    "      # normalize embedding to unit vector\n",
    "      out = torch.nn.functional.normalize(out) \n",
    "      return out\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        output1 = self._forward(anchor)\n",
    "        output2 = self._forward(positive)\n",
    "        output3 = self._forward(negative)\n",
    "        \n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "\n",
    "Загрузчики данных не отличаются от загрузчиков для обычной сети. \n",
    "Единственное отличие это две аугментации, которые добавили к данным:\n",
    "\n",
    "*   Случайный поворот по вертикали (RandomHorizontalFlip)\n",
    "*   Размытие (GaussianBlur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as img_transf\n",
    "\n",
    "# Apply augmentations on train data\n",
    "img_trans_train = img_transf.Compose(\n",
    "    [\n",
    "        img_transf.Resize((105, 105)),\n",
    "        img_transf.RandomHorizontalFlip(),\n",
    "        img_transf.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        img_transf.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_trans_test = img_transf.Compose(\n",
    "    [img_transf.Resize((105, 105)), img_transf.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = SiameseNetworkDataset(\"faces/training/\",transform = img_trans_train)\n",
    "val_dataset = SiameseNetworkDataset(\"faces/testing/\",transform = img_trans_test)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, num_workers=2, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, num_workers=2, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличие от  сетей для классификации в том, что у модели 3 выхода и все их надо передать в loss. При этом нет меток в явном виде.\n",
    "Определить, какой embedding относится к позитивному образцу, а какой к негативному, можно только порядком их следования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, criterion, optimizer, train_loader):\n",
    "    loss_history = []\n",
    "    l = []\n",
    "    model.train()\n",
    "    for epoch in range(0, num_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            anc, pos, neg = batch\n",
    "            output_anc, output_pos, output_neg = model(anc.to(device), pos.to(device), neg.to(device))\n",
    "            loss = criterion(output_anc, output_pos, output_neg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            l.append(loss.item())\n",
    "        last_epoch_loss =  torch.tensor(l[-len(train_loader):-1]).mean()\n",
    "        print(\"Epoch {} with {:.4f} loss\".format(epoch,last_epoch_loss))\n",
    "        \n",
    "    return l, last_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании GPU, обучение на 5-ти эпохах займет около 15 сек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SiameseNet().to(device)\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001 ) \n",
    "epochs = 7 \n",
    "l, _ = train(epochs, model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем график loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([i for i in range(len(l))], l)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('num of epochs')\n",
    "x = range(0,len(l),len(l) // epochs)\n",
    "labels = range(0,epochs)\n",
    "plt.xticks(x, labels)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что он график лосс уже вышел на плато, значит можно проверять результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выведем тройки изображений из проверочного датасета и посмотрим на расстояния для позитивных и негативных пар. Если модель обучилась, расстояния для позитивных пар будут меньше. Другими словами, расстояние между похожими лицами будет маленьким, между разными большим.\n",
    "\n",
    "P.S. По умолчанию TripletLoss минимизирует Евклидово расстояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for visualization\n",
    "def show(img, text=None):\n",
    "    img_np = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(75, 120, text, fontweight=\"bold\")\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def plot_imgs(model,test_loader):\n",
    "    distances_pos = []\n",
    "    distances_neg = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      for i, batch in enumerate(test_loader, 0):\n",
    "        anc, pos, neg = batch\n",
    "        output_anc, output_pos, output_neg = model(anc.to(device), pos.to(device), neg.to(device))\n",
    "        # compute euc. distance\n",
    "        distance_pos = F.pairwise_distance(output_anc, output_pos).item() \n",
    "        distance_neg = F.pairwise_distance(output_anc, output_neg).item() \n",
    "\n",
    "        distances_pos.append(distance_pos)\n",
    "        distances_neg.append(distance_neg)\n",
    "\n",
    "        if not i % 5 :\n",
    "                concatenated = torch.cat((anc, pos, neg))\n",
    "                result = \"OK\" if distance_neg - distance_pos > 0 else \"BAD\"\n",
    "                show(    \n",
    "                    torchvision.utils.make_grid(concatenated),\n",
    "                    f\"Positive / negative euclidean distances: {distance_pos:.3f} / {distance_neg:.3f} - {result}\",\n",
    "                )\n",
    "\n",
    "    return distances_pos, distances_neg\n",
    "\n",
    "\n",
    "distances_pos, distances_neg = plot_imgs(model,val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Если модель обучилась, расстояния для позитивных пар будут меньше чем для негативных.\n",
    "\n",
    "Но такая оценка субъективна, давайте посмотрим на распределение расстояний по категориям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas==1.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas \n",
    "\n",
    "distances = {\"The same person\": distances_pos, \"Another person\": distances_neg}\n",
    "\n",
    "ax = sns.displot(distances, kde=True, stat=\"density\")\n",
    "ax.set(xlabel=\"Pairwise distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно что для фото одного и того же человека, в большинстве случаев расстояние лежит в интервале от 0 до 1.25. \n",
    "\n",
    "А вот для фото разных людей от 0.5 до 1.75. \n",
    "\n",
    "Если бы мы проектировали систему распознавания лиц, нужно было бы выбрать порог что бы сравнивать с ним расстояние и принимать решение о том верифицировать фото как подлинное или нет.\n",
    "\n",
    "Соответственно, для нашего игрушечного датасета такой порог следует выбирать ~= 0.75 при условии, что ошибки первого и второго рода для нас равнозначны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shots learning in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и другой интересный вариант, как можно использовать few-shots learning. Огромные модели (такие как GPT-3 и варианты) в целом способны генерировать любой контент. Вопрос лишь в том, как бы им объяснить, что мы от них хотим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "GPT-3 не доступна для свободного пользования (OpenAI оказался не очень-то и open). Но, умельцы из сообщества ElutherAI сделали open-source версию **GPT-J**, которая работает сравнимо с оригинальной моделью. Более того, они ее захостили у себя на сайте, что бы каждый мог пользоваться без долгой и муторной подгрузки модели (установка зависимостей и загрузка весов для GPT-J занимает ~20 минут в колабе). Ей-то мы и воспользуемся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зайдем на [сайт модели](https://6b.eleuther.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед нами текстовый редактор, в который можно написать что угодно. Для примера давайте попробуем перефразирововать предложение: \n",
    "\n",
    "    Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.\n",
    "\n",
    "Например, сформулировав задачу так: *paraphrase* `...sentence...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то результаты не впечатляют. Тут-то на помощь и приходит идея Few-shots. \n",
    "\n",
    "Что если мы в качестве затравки (*prompt*) скормим моделе 2-3 примера в которых покажем, чего от нее хотим?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем вот такой prompt:\n",
    "\n",
    "    Original: Her life spanned years of incredible change for women as they gained more rights than ever before.\n",
    "    Paraphrase: She lived through the exciting era of women's liberation.\n",
    "\n",
    "    Original: Giraffes like Acacia leaves and hay, and they can consume 75 pounds of food a day.\n",
    "    Paraphrase: A giraffe can eat up to 75 pounds of Acacia leaves and hay daily.\n",
    "\n",
    "    Original: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.\n",
    "    Paraphrase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значительно лучше! Ответ, который получил автор блокнота с первого раза: \n",
    "\n",
    "**Paraphrase:** *Recent work has shown that you can get a lot of money for a lot of text if you pre-train it for a specific task, then fine-tune it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем с числовыми рядами.\n",
    "\n",
    "    Sequence: 2, 4, 6, 8\n",
    "    Continuation: 16, 32, 64, 128\n",
    "\n",
    "    Sequence: 3, 9, 27, 81\n",
    "    Continuation: 243, 729, 2187, 6561\n",
    "\n",
    "    Sequence: 4, 16, 64, 256\n",
    "    Continuation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И с геологической викториной:\n",
    "\n",
    "    Q: The molten rock that sits below the earth's surface is called what?\n",
    "    A: Magma\n",
    "\n",
    "    Q: Diamonds rank at number 10 on the Mohs scale, meaning they are the hardest mineral listed. Where would cubic zirconia (CZ) rank on the scale?\n",
    "    A: 8-8.5\n",
    "\n",
    "    Q: The emerald is a type of beryl. The distinctive green color of the emerald derives from trace amounts of what metallic elements?\n",
    "    A: \n",
    "\n",
    "Ответы лучше перепроверить, а вот вопросы для викторины готовы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не стесняйтесь придумывать свои собственные примеры и задачи! Have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто, когда мы пишем и обучаем сети (будь то с нуля или с помощью transfer learning) мы вынуждены угадывать гиперпараметры (lr, betas и тд). В случае с learning rate нам есть от чего оттолкнуться (маленький lr для transfer learning), [константа Karpathy (3e-4)](https://twitter.com/karpathy/status/801621764144971776?lang=en) для Adam, но все же, такой подход не кажется оптимальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации гиперпараметров существуют готовые решения, которые используют различные методы black-box оптимизации. Разберем одну из наиболее популярных библиотек - [**Optuna**](https://optuna.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте оптимизируем наш learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# define function which will optimized\n",
    "def objective(trial):\n",
    "    # boundaries for the optimizer's \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-2)\n",
    "    \n",
    "    ##### If you need more parameters for optimization, it is done like this:\n",
    "    #new_parameter =  trial.suggest_loguniform(\"new_parameter\", lower_bound, upper_bound)\n",
    "\n",
    "    # create new model(and all parameters) every iteration\n",
    "    model = SiameseNet().to(device)\n",
    "    criterion = nn.TripletMarginLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=lr\n",
    "    )  # learning step regulates by optuna\n",
    "    \n",
    "\n",
    "    # To save time, we will take only 3 epochs\n",
    "    _, last_epoch_loss = train(3, model, criterion, optimizer, train_loader)\n",
    "    return last_epoch_loss\n",
    "\n",
    "\n",
    "# Create \"exploration\"\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"Optimal lr\")\n",
    "\n",
    "#\n",
    "study.optimize(\n",
    "    objective, n_trials=10\n",
    ")  # The more iterations, the higher the chances of catching the most optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best params\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно же, можно оптимизировать сразу несколько параметров за раз, а еще в качестве параметров может выступать сама архитектура сети (например, количество слоев и каналов). Это можно легко сделать по аналогии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на историю оптимизации нашего lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, проверим, а станет ли реально лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNet().to(device)\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=study.best_params[\"lr\"]\n",
    ")  # take lr, which choosen Optuna\n",
    "#scheduler = ReduceLROnPlateau(optimizer, \"min\", verbose=True, factor=0.5, patience=3)\n",
    "\n",
    "l_optim, _ = train(5,  model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(l))], l, label=\"no optimization\")\n",
    "plt.plot([i for i in range(len(l_optim))], l_optim, label=\"optimal params\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('num of epochs')\n",
    "x = [0,6,12,18,24,30]\n",
    "labels = [0,1,2,3,4,5]\n",
    "plt.xticks(x, labels)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_pos, distances_neg = plot_imgs(model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_optim = {\"The same person\": distances_pos, \"Another person\": distances_neg}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "sns.histplot(distances, kde=True, stat=\"density\", alpha=0.5, ax=axes[0])\n",
    "sns.histplot(distances_optim, kde=True, stat=\"density\", alpha=0.5, ax=axes[1])\n",
    "\n",
    "axes[0].set(title='No optimization')\n",
    "axes[1].set(title='Otimization with Optuna')\n",
    "axes[0].set(xlabel=\"Pairwise distance\")\n",
    "axes[1].set(xlabel=\"Pairwise distance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\"> Заключение\n",
    "\n",
    "\n",
    "Мы затронули проблемы, которые возникают при обучении на реальных данных.\n",
    "\n",
    "Одна из основных проблем - малые датасеты. Для того, чтобы обучить нейронку на небольшом датасете, можно использовать:\n",
    "- `аугментацию`\n",
    "- `Tranfer learning`\n",
    "Однако необходимо помнить, что ни один из этих методов не защитит от ситуации, когда реальные данные будут сильно отличаться от тренировочных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае, когда у нас не только мало данных, но и еще и очень большое (возможно, неизвестное) число классов, можно воспользоваться `One-shot Learning`. В этом случае нейронка обучается не классифицировать изображения, а, наоборот, находить различия между классом и новыми данными. Для этого используются нейронные сети, относящиеся к классу сиамских нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же мы разобрали автоматическую оптимизацию гиперпараметров с помощью Optuna и показали, что это эффективный способ сделать нейросеть лучше малой ценой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\">Литература\n",
    "\n",
    "\n",
    "<font size = \"5\"> Обучение на реальных данных\n",
    "\n",
    "[How to avoid machine learning pitfalls: a guide for academic researchers (Lones, 2021)](https://arxiv.org/abs/2108.02497)\n",
    "\n",
    "[Understanding data augmentation for classification: when to warp? (Wong et al., 2016)](https://arxiv.org/abs/1609.08764) \n",
    "\n",
    "[Learning from class-imbalanced data: Review of methods and applications (Haixiang et al., 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175?via%3Dihub)\n",
    "\n",
    "<font size = \"5\"> Как решить проблему маленького количества данных?\n",
    "\n",
    "[Блог-пост о том как решать проблему малого количества данных.](https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d)\n",
    "\n",
    "<font size = \"5\"> Несбалансированные данные\n",
    "\n",
    "[Imbalanced Data: How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/).\n",
    "\n",
    "[SMOTE explained for noobs - Synthetic Minority Over-sampling TEchnique line by line](https://rikunert.com/SMOTE_explained)\n",
    "\n",
    "[Блог пост про 8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "\n",
    "[Метрики разработаные для работы с несбалансированными классами.](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/#:~:text=Classification%20Accuracy%20is%20Not%20Enough%3A%20More%20Performance%20Measures%20You%20Can%20Use,-By%20Jason%20Brownlee&text=When%20you%20build%20a%20model,This%20is%20the%20classification%20accuracy)\n",
    "\n",
    "[Творческий подход](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)\n",
    "\n",
    "<font size = \"5\"> Transfer Learning\n",
    "\n",
    "[Image Classification using Transfer Learning in Pytorch](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/)\n",
    "\n",
    "[How To Do Transfer Learning For Computer Vision | PyTorch Tutorial](https://www.youtube.com/watch?v=6nQlxJvcTr0)\n",
    "\n",
    "[Transfer learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "[Python Pytorch Tutorials # 2 Transfer Learning : Inference with ImageNet Models](https://www.youtube.com/watch?v=Upw4RaERZic)\n",
    "\n",
    "[PyTorch - The Basics of Transfer Learning with TorchVision and AlexNet](https://www.youtube.com/watch?v=8etkVC93yU4)\n",
    "\n",
    "\n",
    "<font size = \"5\">Augmentation\n",
    "\n",
    "[A survey on Image Data Augmentation for Deep Learning (Shorten and Khoshgoftaar, 2019)](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)\n",
    "\n",
    "[Data augmentation for improving deep learning in image classification problem](https://www.researchgate.net/publication/325920702_Data_augmentation_for_improving_deep_learning_in_image_classification_problem)\n",
    "\n",
    "\n",
    "<font size = \"5\"> Few-shot learning\n",
    "\n",
    "[One-Shot Learning with Siamese Networks using Keras](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d)\n",
    "\n",
    "[One-Shot image classification by meta learning](https://medium.com/nerd-for-tech/one-shot-learning-fe1087533585)\n",
    "\n",
    "[One-Shot Learning (Part 1/2): Definitions and fundamental techniques](https://heartbeat.fritz.ai/one-shot-learning-part-1-2-definitions-and-fundamental-techniques-1df944e5836a)\n",
    "\n",
    "[One-Shot Learning (Part 2/2): Facial Recognition Using a Siamese Network](https://heartbeat.fritz.ai/one-shot-learning-part-2-2-facial-recognition-using-a-siamese-network-5aee53196255)\n",
    "\n",
    "[FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff et al., 2015)](https://arxiv.org/abs/1503.03832)\n",
    "\n",
    "[One-Shot Learning explained using FaceNet](https://medium.com/intro-to-artificial-intelligence/one-shot-learning-explained-using-facenet-dff5ad52bd38)\n",
    "\n",
    "[Ищем знакомые лица](https://habr.com/ru/post/317798/)\n",
    "\n",
    "[Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "[Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)\n",
    "\n",
    "[Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "<font size = \"5\"> Hyperparameter optimization\n",
    "\n",
    "[Tuning Hyperparameters with Optuna](https://towardsdatascience.com/tuning-hyperparameters-with-optuna-af342facc549)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
