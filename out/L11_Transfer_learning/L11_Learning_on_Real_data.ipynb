{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mIhNebFYp8y"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import PIL.ImageOps   \n",
    "\n",
    "from IPython.display import clear_output\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# importing all the required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu8dNnq5k0vs"
   },
   "source": [
    "# Обучение на реальных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_zOUYK9Z-TY"
   },
   "source": [
    "С какими проблемами чаще всего сталкиваются исследователи-нейронщики в реальной жизни?\n",
    "- нехватка данных\n",
    "- недостаток размеченных данных\n",
    "- не качественная разметка\n",
    "- низкое качество изображений\n",
    "- не сбалансированность датасета\n",
    "- переобучение\n",
    "- какие еще?\n",
    "\n",
    "Сначала, давайте рассмотрим методы работы с малым количеством данных, когда классы не представлены одинаково.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGKNp9YYZLhN"
   },
   "source": [
    "## [Как решить проблему меньшего количества данных?](https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyaR_2anZLhN"
   },
   "source": [
    "ResNet, популярная архитектура классификации изображений, заняла 1-е место в классификационном конкурсе ILSVRC 2015 с улучшением примерно на 50% по сравнению с предыдущим уровнем развития техники.ResNet не только имела очень сложную и глубокую архитектуру, но и был обучен на 1,2 млн изображений.\n",
    "\n",
    "Как в отрасли, так и в академических кругах хорошо известно, что при достаточно большом количестве данных, очень разные алгоритмы DL работают практически одинаково. Следует отметить, что большие данные должны содержать значимую информацию, а не просто шум, чтобы модель могла извлекать уроки из них.\n",
    "\n",
    "На рисунке показаны основные проблемы, с которыми приходится сталкиваться при работе с небольшими наборами данных, а также возможные подходы и методы их решения.\n",
    "\n",
    "В этой части мы сосредоточимся только на методах, используемых в традиционном машинном обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-pzjohtZLhN"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_33.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJBIcH5cZLhO"
   },
   "source": [
    "Использование методов ML:\n",
    "- **Измените функцию потерь**: для задач классификации мы часто используем кросс-энтропийную потерю и редко используем среднюю абсолютную ошибку или среднеквадратичную ошибку для обучения и оптимизации нашей модели.\n",
    "\n",
    "    В случае несбалансированных данных модель становится более склонной к классу большинства, поскольку она оказывает большее влияние на окончательное значение потерь, и наша модель становится менее полезной.\n",
    "\n",
    "    В таких сценариях мы можем добавить веса к потерям, соответствующим различным классам, чтобы выровнять это смещение данных. Например, если у нас есть два класса с данными в соотношении 4: 1, мы можем применить веса в соотношении 1: 4 к вычислению функции потерь, чтобы данные были сбалансированы.\n",
    "\n",
    "    Этот метод помогает нам легко смягчить проблему несбалансированных данных и улучшить обобщение модели для разных классов. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sj2zviqiZLhO"
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "#class_weights = compute_class_weight('balanced', np.unique(y_train),y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmiD8EV4ZLhO"
   },
   "source": [
    "Мы также можем указать явные веса классов в соответствии с нашими требованиями.\n",
    "\n",
    "Дополнительные сведения см. В документации Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffxGdJ1mZLhO"
   },
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create decision tree classifer object\n",
    "#clf = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "\n",
    "# Train model\n",
    "#model = clf.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6esU2ViMZLhP"
   },
   "source": [
    "- **Обнаружение аномалий / изменений**: в случаях сильно несбалансированных наборов данных, таких как мошенничество или сбой машины, стоит задуматься, могут ли такие примеры рассматриваться как аномалия или нет.\n",
    "\n",
    "    Если данная проблема соответствует критерию аномалии, мы можем использовать такие модели, как OneClassSVM, методы кластеризации или методы обнаружения гауссовских аномалий.\n",
    "\n",
    "    Эти методы требуют изменения мышления: когда мы рассматриваем второстепенный класс как класс выбросов, что может помочь нам найти новые способы разделения и классификации.\n",
    "\n",
    "    Обнаружение изменений похоже на обнаружение аномалий, за исключением того, что мы ищем изменение или отличие, а не аномалию.Это могут быть изменения в поведении пользователя, наблюдаемые по шаблонам использования или банковским транзакциям.\n",
    "\n",
    "    В документации Scikit-Learn можно узнать, как реализовать обнаружение аномалий с помощью этой бибилиотеки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTMrUXig39UV"
   },
   "source": [
    "- **Выборка с повышением или с понижением** (Up-sample or Down-sample): одно из решений проблемы - сбалансировать данные.\n",
    "\n",
    "    Это может быть сделано либо за счет увеличения частоты класса меньшинства, либо за счет уменьшения частоты класса большинства с помощью методов случайной или кластерной выборки.\n",
    "\n",
    "    Выбор между избыточной или недостаточной выборкой и случайным или кластеризованным определяется бизнес-контекстом и размером данных.\n",
    "\n",
    "    Обычно `upsampling` предпочтителен, когда общий размер данных небольшой, а понижающая дискретизация полезна, когда у нас есть большой объем данных. Точно так же случайная или кластерная выборка определяется тем, насколько хорошо распределены данные.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lrMig-ZZLhP"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_34.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IakcgVs5H3I"
   },
   "source": [
    "Вот тут можно прочитать подробнее: [Imbalanced Data : How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/)\n",
    "\n",
    "`Resampling` может быть легко выполнен с помощью пакета [imbalanced-learn](https://pypi.org/project/imbalanced-learn/), как показано ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPvH0n1yZLhP"
   },
   "outputs": [],
   "source": [
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "#rus = RandomUnderSampler(random_state=42)\n",
    "#X_res, y_res = rus.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY21IvKrZLhP"
   },
   "source": [
    "- **Генерация синтетических данных**: хотя `upsampling` или `downsampling` помогает сбалансировать данные, дублирование данных увеличивает вероятность переобучения.\n",
    "\n",
    "    Другой подход к решению этой проблемы - создание синтетических данных с помощью данных о классе меньшинств.\n",
    "\n",
    "    **Synthetic Minority Over-sampling Technique (SMOTE)** или **Modified- SMOTE** - два таких метода, которые генерируют синтетические данные.\n",
    "\n",
    "    Проще говоря, SMOTE берет точки данных класса меньшинства и создает новые точки данных, которые лежат между любыми двумя ближайшими точками данных, соединенными прямой линией.\n",
    "\n",
    "    Для этого алгоритм вычисляет расстояние между двумя точками данных в пространстве признаков, умножает расстояние на случайное число от 0 до 1 и помещает новую точку данных на этом новом расстоянии от одной из точек данных, используемых для определения расстояния.\n",
    "\n",
    "Обратите внимание, что количество ближайших соседей, учитываемых для генерации данных, также является гиперпараметром и может быть изменено в зависимости от требований."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW2UeIKDZLhP"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_35.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUrIj2VHZLhQ"
   },
   "source": [
    "M-SMOTE - это модифицированная версия SMOTE, которая также принимает во внимание базовое распределение класса меньшинства.\n",
    "\n",
    "Алгоритм классифицирует образцы классов меньшинств на 3 отдельные группы - образцы безопасности / безопасности, образцы границ и образцы скрытого шума.\n",
    "\n",
    "Это делается путем вычисления расстояний между выборками класса меньшинства и выборками обучающих данных.\n",
    "\n",
    "В отличие от SMOTE, алгоритм случайным образом выбирает точку данных из k ближайших соседей для выборки безопасности, выбирает ближайшего соседа из выборок границ и ничего не делает для устранения скрытого шума.\n",
    "\n",
    "Вот тут можно прочитать подробнее: [SMOTE explained for noobs - Synthetic Minority Over-sampling TEchnique line by line](https://rikunert.com/SMOTE_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-Sl6cR4ZLhQ"
   },
   "source": [
    "- **Методы объединения** (Ensembling Techniques): идея объединения нескольких слабых учеников / разных моделей показала отличные результаты при работе с несбалансированными наборами данных.\n",
    "\n",
    "    `Bagging` и `Boosting` методы также показали отличные результаты при решении различных задач, и их следует изучить вместе с методами, описанными выше, для получения лучших результатов.\n",
    "\n",
    "    Вот тут подробнее: [Imbalanced Data : How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnXrVqiaZLhQ"
   },
   "source": [
    "**Вывод:**\n",
    "\n",
    "Мы рассмотрели несколько наиболее часто используемых методов работы с несбалансированными данными для традиционных алгоритмов машинного обучения.\n",
    "\n",
    "Один или многие из вышеперечисленных методов могут быть хорошей отправной точкой в ​​зависимости от конкретной бизнес-задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szXUxTyhzEva"
   },
   "source": [
    "### [8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTHdQdxQzTzd"
   },
   "source": [
    "1. **Определите, можете ли вы собрать больше данных?**\n",
    "2. **Попробуйте изменить метрику оценки модели.**\n",
    "    Accuracy не является показателем, который следует использовать при работе с несбалансированным набором данных. [Есть метрики, которые были разработаны для работы с несбалансированными классами](Classification Accuracy is Not Enough: More Performance Measures You Can Use).\n",
    "3. **Применить `Resampling`** (см. выше)\n",
    "4. **Попробовать создать синтетические образцы данных**\n",
    "5. **Не следует использовать свой любимый алгоритм для каждой задачи.**\n",
    "    Необходимо проверять различные типы алгоритмов для решения конкретной проблемы. Например, `Random Forest` часто хорошо работает с несбалансированными наборами данных.\n",
    "6. **Попробуйте модели с санкциями**\n",
    "    Штрафная классификация накладывает дополнительные штрафы на модель за ошибки классификации в классе меньшинства во время обучения. Эти штрафы могут склонить модель к уделению большего внимания классу меньшинства.\n",
    "7. **Попробуйте другую точку зрения**\n",
    "    Есть области исследований, посвященные несбалансированным наборам данных. Два, которые вы могли бы рассмотреть, - это [обнаружение аномалий](https://en.wikipedia.org/wiki/Anomaly_detection) и [обнаружение изменений](https://en.wikipedia.org/wiki/Change_detection).\n",
    "8. **[Попробуйте проявить творческий подход](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)**\n",
    "    По-настоящему залезьте внутрь своей проблемы и подумайте, как разбить ее на более мелкие проблемы, которые легче решить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIKgz1WR7yRe"
   },
   "source": [
    "**Перейдем к особенностям работы с несбалансированными классами в моделях нейронных сетей**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBunbfHuZLhW"
   },
   "source": [
    "## [Аугментация](https://en.wikipedia.org/wiki/Data_augmentation)\n",
    "\n",
    "Сам термин пришел из музыки:\n",
    "\n",
    "**Аугмента́ция** (позднелат. augmentatio — увеличение, расширение) — [техника ритмической композиции в старинной музыке](https://ru.wikipedia.org/wiki/Аугментация).\n",
    "\n",
    "Сейчас такая работа с данными активно применяется в работе с изображениями, но ее можно использоваться и для сигналов, и, иногда, для табличных данных.\n",
    "\n",
    "Модели глубокого обучения обычно требуют большого количества данных для обучения. В целом, чем больше данных, тем лучше для обучения модели.\n",
    "\n",
    "Но получение огромных объемов данных сопряжено со своими проблемами.\n",
    "\n",
    "Вместо того, чтобы тратить дни на сбор данных вручную, мы можем использовать методы увеличения их числа путем генерации новых из уже имеющихся.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHzeonojZLhW"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_12.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbbkyP0BZLhX"
   },
   "source": [
    "### Примеры аугументации в [TORCHVISION.TRANSFORMS](https://pytorch.org/vision/stable/transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkC8ZCqPZLhX"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESCKdH36Wvyg"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "# скачаем необходимые файлы\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=11LBkRRlagM4ijHsXkytgSae4NJtW98sB' -O data.zip\n",
    "with ZipFile('data.zip', 'r') as folder: # Create a ZipFile Object and load sample.zip in it\n",
    "   folder.extractall() # Extract all the contents of zip file in current directory\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08kAK8LApcl7",
    "outputId": "c7e45605-aa92-4384-a2c8-fcea285381d2"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/for_transforms.Compose')\n",
    "img_list = os.listdir()\n",
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYddaOEUBQl7"
   },
   "outputs": [],
   "source": [
    "class AugmentationDataset(Dataset):\n",
    "  def __init__(self,image_list,transforms=None):\n",
    "    self.image_list=image_list\n",
    "    self.transforms=transforms\n",
    "  def __len__(self):\n",
    "    return len(self.image_list)\n",
    "  def __getitem__(self,i):\n",
    "    img=plt.imread(self.image_list[i])\n",
    "    img=Image.fromarray(img).convert('RGB')\n",
    "    img=np.array(img).astype(np.uint8)\n",
    "\n",
    "    if self.transforms is not None:\n",
    "      img=self.transforms(img)\n",
    "    return torch.tensor(img,dtype=torch.float)\n",
    "\n",
    "def show_img(img):\n",
    "  plt.figure(figsize=(40,38))\n",
    "  npimg=img.numpy()\n",
    "  plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2DohkSEHtAA"
   },
   "source": [
    "Существует несколько методов аугментации изображений, и мы обсудим некоторые из наиболее распространенных и широко используемых.\n",
    "\n",
    "Вы можете попробовать другие методы в соответствии с вашими требованиями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0a6YOVnHH-TI"
   },
   "source": [
    "### Image Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "UFqkPPT3BHoc",
    "outputId": "86c10c02-856d-415d-98bd-83b8b1ef6805"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((164,164)),\n",
    "                              transforms.RandomRotation(50,expand=True),  \n",
    "                              transforms.Resize((164,164)),\n",
    "                              transforms.ToTensor(),\n",
    "                              ])\n",
    "\n",
    "Augmentation_dataloader=DataLoader(AugmentationDataset(img_list,transform),batch_size=8,shuffle=True)\n",
    "\n",
    "data=iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHGthhJwIYsX"
   },
   "source": [
    "### Random Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "d7t5k168pdUF",
    "outputId": "c7b7faae-c69d-40b0-80f9-3ffb6d30b797"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((164,164)),\n",
    "                              transforms.RandomCrop((120,120)),        \n",
    "                              transforms.ToTensor(),\n",
    "                              ])\n",
    "\n",
    "Augmentation_dataloader=DataLoader(AugmentationDataset(img_list,transform),batch_size=8,shuffle=True)\n",
    "\n",
    "data=iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul2TvSVUKNDs"
   },
   "source": [
    "### Random Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "3iKLmW6hpdRb",
    "outputId": "7985cb0d-c6bf-4706-88f0-59d4a4fb117a"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((164,164)),\n",
    "                              transforms.RandomPerspective(distortion_scale=0.5, p=1.0),      \n",
    "                              transforms.ToTensor(),\n",
    "                              ])\n",
    "\n",
    "Augmentation_dataloader=DataLoader(AugmentationDataset(img_list,transform),batch_size=8,shuffle=True)\n",
    "\n",
    "data=iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cNHR8_OK3K1"
   },
   "source": [
    "### Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "EaOvGUHKpdPH",
    "outputId": "0a507b28-cac8-4bdd-dca8-66f8ed0b9e66"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((164,164)),\n",
    "                              transforms.GaussianBlur(7, sigma=(0.1, 2.0)),\n",
    "                              transforms.ToTensor(),\n",
    "                              ])\n",
    "\n",
    "Augmentation_dataloader=DataLoader(AugmentationDataset(img_list,transform),batch_size=8,shuffle=True)\n",
    "\n",
    "data=iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ4_dfdtLCWh"
   },
   "source": [
    "### Random Erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "9aKuRpvRFte1",
    "outputId": "b025e4f0-51e9-41a4-d2ba-cb7b20c778c6"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((164,164)),   \n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.RandomErasing(),  \n",
    "                              ])\n",
    "\n",
    "Augmentation_dataloader=DataLoader(AugmentationDataset(img_list,transform),batch_size=8,shuffle=True)\n",
    "\n",
    "data=iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn73aWVlNBVR"
   },
   "source": [
    "Существует и более сложные способы аугментации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5OXwvd6ZLhY"
   },
   "source": [
    "- **Mixup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGP4vbu7ZLhY"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_16.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYlUSIJ2ZLhY"
   },
   "source": [
    "- **Аугументация при помощи синтеза данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfPQldeoZLhY"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_17.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NVmTqFVVrLD"
   },
   "source": [
    "В домашнем задании Вам предстоит провести эксперимент: обучить нейронную сеть на датасете без аугментации, и после нее.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBJp7kJPcAOp"
   },
   "source": [
    "Кроме методов, реализованных в Pytorch, существуют и специализированные ибилиотеки для аугментации изображений. \n",
    "\n",
    "Например:\n",
    "- [lbumentations](https://albumentations.ai)\n",
    "- [imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "**При выборе методов аугментации имеет смысл использовать те, которые будут в реальной жизни. Например, нет смысла делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным, или лотражать человека вверх-ногами, если мы не предполагаем его таким распознавать.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdsElHWvZLhe"
   },
   "source": [
    "## Transfer Learning:\n",
    "----\n",
    "как быстро обучить нейросеть на своих данных, когда их мало\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsHOgoMgZza4"
   },
   "source": [
    "Для таких типовых задач, как классификация изображений, можно воспользоваться готовой архитектурой (AlexNet, VGG, Inception, ResNet и т.д.) и обучить нейросеть на своих данных. Реализации таких сетей с помощью различных фреймворков уже существуют, так что на данном этапе можно использовать одну из них как черный ящик, не вникая глубоко в принцип её работы.\n",
    "\n",
    "Однако, глубокие нейронные сети требовательны к большим объемам данных для сходимости обучения. И зачастую, в нашей частной задаче недостаточно данных для того, чтобы хорошо натренировать все слои нейросети. `Transfer Learning` решает эту проблему. Зачем обучать сеть заново, если можно использовать уже обученную на миллионе изображений и дообучить на свой датасет?\n",
    "\n",
    "В PyTorch есть много предобученных сетей: [TORCHVISION.MODELS](https://pytorch.org/vision/stable/models.html)\n",
    "\n",
    "- AlexNet\n",
    "- VGG\n",
    "- ResNet\n",
    "- SqueezeNet\n",
    "- DenseNet\n",
    "- Inception v3\n",
    "- GoogLeNet\n",
    "- ShuffleNet v2\n",
    "- MobileNetV2\n",
    "- MobileNetV3\n",
    "- ResNeXt\n",
    "- Wide ResNet\n",
    "- MNASNet\n",
    "\n",
    "Для этого,  нужно отключить какие-то промежуточные слои. Тогда можно использовать то, что называется `Fine turning` - не нужно обучать всю модель, а достаточно только ее новую часть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-KTVXmnZLhe"
   },
   "source": [
    "Нейронные сети, которые используются для классификации, как правило, содержат N выходных нейронов в последнем слое, где N — это количество классов. Такой выходной вектор трактуется как набор вероятностей принадлежности к классу. \n",
    "\n",
    "В реальных задачах распознавания изображений, количество классов может отличаться от того, которое было в исходном датасете. В таком случае нам придётся полностью выкинуть этот последний слой и поставить новый, с нужным количеством выходных нейронов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h25zm-auZLhZ"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_21.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26x0NDBBZLhe"
   },
   "source": [
    "Зачастую в конце классификационных сетей используется полносвязный слой. Так как мы заменили этот слой, использовать предобученные веса для него уже не получится. Придется тренировать его с нуля, инициализировав его веса случайными значениями. Веса для всех остальных слоев мы загружаем из предобученной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3o27u8gZLhf"
   },
   "source": [
    "Существуют различные стратегии дообучения модели. Мы воспользуемся следующей: будем тренировать всю сеть из конца в конец (end-to-end), а предобученные веса не будем фиксировать, чтобы дать им немного скорректироваться и подстроиться под наши данные. Такой процесс называется тонкой настройкой (fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V58CEalMZLhf"
   },
   "source": [
    "### Структурные компоненты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhryXKoRZLhf"
   },
   "source": [
    "Для решения задачи нам понадобятся следующие компоненты:\n",
    "\n",
    "1. Описание модели нейросети\n",
    "2. Пайплайн обучения\n",
    "3. Инференс пайплайн\n",
    "4. Предобученные веса для этой модели\n",
    "5. Данные для обучения и валидации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8_QlZBPZLhf"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_1.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCZl39AyZLhf"
   },
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6tHWW33ZLhf"
   },
   "source": [
    "Код обучения модели состоит из следующих шагов:\n",
    "\n",
    "1. Построение train/validation пайплайнов данных\n",
    "2. Построение train/validation графов (сетей)\n",
    "3. Надстраивание классификационной функция потерь (cross entropy loss) поверх train графа\n",
    "4. Код, необходимый для вычисления точности предсказания на валидационной выборке во время обучения\n",
    "5. Логика загрузки предобученных весов из снэпшота\n",
    "6. Создание различных структур для обучения\n",
    "7. Непосредственно сам цикл обучения (итерационная оптимизация)\n",
    "\n",
    "Последний слой графа конструируется с нужным нам количеством нейронов и исключается из списка параметров, загружаемых из предобученного снэпшота.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbbT_LcIZLhf"
   },
   "source": [
    "### Модель\n",
    "\n",
    "Давайте рассмотрим пример практической реализации такого подхода ([код переработан из этой статьи](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmF8rpmtnEmQ"
   },
   "source": [
    "Обратите внимание на аугментацию изображений нашего датасета!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqqRdjDtnA4E"
   },
   "outputs": [],
   "source": [
    "# Applying Transforms to the Data\n",
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VATSkk6bX37H"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "# скачаем необходимые файлы\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1qkFa6Iu7EyzVuL1iEkzxBbqXYRoRMGcK' -O data.zip\n",
    "with ZipFile('data.zip', 'r') as folder: # Create a ZipFile Object and load sample.zip in it\n",
    "   folder.extractall() # Extract all the contents of zip file in current directory\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hpd6GzUPnMlB"
   },
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "# Set train and valid directory paths\n",
    "dataset = '/content/caltec256subset'\n",
    "\n",
    "train_directory = os.path.join(dataset, 'train')\n",
    "valid_directory = os.path.join(dataset, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJEDmh12nX6L",
    "outputId": "84c2dd6a-e5bb-477f-a752-e3ea7ffc5ab4"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 3\n",
    "\n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "print(idx_to_class)\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_data_loader = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid_data_loader = DataLoader(data['valid'], batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KBDNYbTnpGF"
   },
   "source": [
    "У нас не так уж и много изображений!! Обычная нейронка справиться не должна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JaziFVanhmV",
    "outputId": "670751ad-f540-4c1e-b58f-f161697450f3"
   },
   "outputs": [],
   "source": [
    "train_data_size, valid_data_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqkYPot4op1H"
   },
   "source": [
    "Загрузим предобученную Alexnet с сохранением весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547,
     "referenced_widgets": [
      "d0222af7dd7942ee9f1b6213775cff3d",
      "b6d4a77fa4ea4b43b8611891a00286f9",
      "048ee726373a4ea9927629b3508152a0",
      "1929e36a0f54498592420e0b4f080fa3",
      "5c5874f2c0944a8797ecfc88b817ab39",
      "9aaf33833e0841c9a030498e05d0ce76",
      "ba6c72ca7645409bbfb2ff06d5d2ceab",
      "e434faa1cdd04b69baf6a5062541293c"
     ]
    },
    "id": "l-giwSfOooum",
    "outputId": "2fa88bb9-adac-4cb8-9bb0-2f08aa9eeaa0"
   },
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ilk1CTqVpI0Z"
   },
   "source": [
    "Поскольку большинство параметров в нашей предварительно обученной модели уже обучены, мы сбрасываем значение поля requires_grad в значение false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgIp1Vmloza0"
   },
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUGbz9w3spFK"
   },
   "source": [
    "Затем мы заменяем последний слой модели Alexnet одним последовательным слоем с 4096 нейронами и который имеет `num_classes` выходов, соответствующих числу классов в нашем подмножестве.\n",
    "\n",
    "То есть мы \"сказали\" нашей модели распознавать не 1000, а только 3 класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeUCUmqQsf4i",
    "outputId": "645dd54a-124c-4255-c756-6d6198a3a379"
   },
   "outputs": [],
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaorFvIEy1xy"
   },
   "source": [
    "Затем мы определяем функцию потерь и оптимизатор, который будет использоваться для обучения.\n",
    "\n",
    "Мы используем функцию Negative Loss Likelihood, поскольку она полезна для классификации нескольких классов.\n",
    "\n",
    "И оптимизатор Adam - один из самых популярных оптимизаторов, потому что он может адаптировать скорость обучения для каждого параметра индивидуально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blNYpc3cyZ0I",
    "outputId": "f928e11b-3132-4e57-b3fc-850e305c6cea"
   },
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwL0KKCVzBbM"
   },
   "source": [
    "Для тренировки и валидации нашей можели напишем отдельную функцию, которую итмеет смысл изучить подробнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7jPeYEDyZxX"
   },
   "outputs": [],
   "source": [
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # Clean existing gradients\n",
    "            outputs = model(inputs) # Forward pass - compute outputs on input data using the model\n",
    "            loss = loss_criterion(outputs, labels) # Compute loss\n",
    "            loss.backward() # Backpropagate the gradients\n",
    "            optimizer.step() # Update the parameters\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            \n",
    "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    " \n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.eval() # Set to evaluation mode\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs) # Forward pass - compute outputs on input data using the model\n",
    "                loss = loss_criterion(outputs, labels) # Compute loss\n",
    "                valid_loss += loss.item() * inputs.size(0) # Compute the total loss for the batch and add it to valid_loss\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "            \n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_data_size \n",
    "        avg_train_acc = train_acc/train_data_size\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/valid_data_size \n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "                \n",
    "        epoch_end = time.time()\n",
    "    \n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "        \n",
    "        # Save if the model has best accuracy till now\n",
    "        #torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
    "            \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLs-6ztIzt_V",
    "outputId": "82f0264a-c6bb-416f-e2f4-c1d9d69a5d54"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "num_epochs = 5\n",
    "trained_model, history = train_and_validate(alexnet, loss_func, optimizer, num_epochs)\n",
    "\n",
    "torch.save(history, dataset+'_history.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y50lns9Jz0ep"
   },
   "outputs": [],
   "source": [
    "history = np.array(history)\n",
    "plt.plot(history)\n",
    "plt.legend(['Train Loss', 'Val Loss', 'Traint Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(dataset + '_loss_curve.png')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_wPpt900TFo"
   },
   "source": [
    "Модель настолько мощная, а данных так мало, что достаточно 5ти эпох, не смотря на аугментацию изображений.\n",
    "\n",
    "Теперь сделаем предикт на картинках, которые модель не видела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv7t025g0PdT"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_image_name):\n",
    "    device = torch.device('cpu')\n",
    "    '''\n",
    "    Function to predict the class of a single test image\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param test_image_name: Test image\n",
    "\n",
    "    '''\n",
    "    \n",
    "    transform = image_transforms['test']\n",
    "\n",
    "    test_image = Image.open(test_image_name)\n",
    "    plt.imshow(test_image)\n",
    "    \n",
    "    test_image_tensor = transform(test_image)\n",
    "    test_image_tensor = test_image_tensor.view(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_image_tensor).to(device)\n",
    "        ps = torch.exp(out).to(device)\n",
    "        topk, topclass = ps.topk(3, dim=1)\n",
    "        for i in range(3):\n",
    "            print(\"Predcition\", i+1, \":\", idx_to_class[topclass.numpy()[0][i]], \", Score: \", round(topk.numpy()[0][i], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSdCJkSi0nAr"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/caltec256subset/test')\n",
    "predict(trained_model.to(device), '009_0098.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2X_9lGf0qw0"
   },
   "outputs": [],
   "source": [
    "predict(trained_model, '090_0107.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsUkPnwy0tn9"
   },
   "source": [
    "И, сделаем проверку третьего класса \"все остальное\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFLfRemK0xuY"
   },
   "outputs": [],
   "source": [
    "predict(trained_model, '150_0044.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPiPOeGJ4hOS"
   },
   "source": [
    "Мы только что увидели, как использовать предварительно обученную модель, обученную для 1000 классов ImageNet.\n",
    "\n",
    "Она очень эффективно классифицирует изображения, принадлежащие к 3 различным классам, которые нас интересуют. Обратите внимание, что горилла иногда очень похожа на медведя!\n",
    "\n",
    "Также мы убедидлись в эффективности такого подхода для обучении классификации на небольшом наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN6arbg_dLBG"
   },
   "source": [
    "## One-Shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwLsSB4CdeuC"
   },
   "source": [
    "Глубокие сверточные нейронные сети стали самыми современными методами для задач классификации изображений. Однако одно из самых больших ограничений - они требуют большого количества помеченных данных.\n",
    "\n",
    "Во многих приложениях иногда невозможно собрать такой объем данных и `One Shot Learning` направлен на решение этой проблемы.\n",
    "\n",
    "Предполагается, что при классификации по методу `One Shot Learning` нам требуется только один обучающий пример для каждого класса. Отсюда и название One Shot. Давайте попробуем разобраться на реальном практическом примере.\n",
    "\n",
    "Предположим, мы хотим создать систему распознавания лиц для небольшой организации, в которой всего 10 сотрудников (небольшое количество упрощает задачу).\n",
    "\n",
    "Используя традиционный подход к классификации, мы можем придумать систему, которая выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFWL2SmRderY"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL1.jpeg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Bpd39-H-7rn"
   },
   "source": [
    "Проблемы:\n",
    "\n",
    "а) Чтобы обучить такую ​​систему, нам сначала потребуется много разных изображений каждого из 10 человек в организации, что может оказаться невозможным. (Представьте, что вы делаете это для организации с тысячами сотрудников).\n",
    "\n",
    "б) Что делать, если новый человек присоединяется к организации или покидает ее? Вам нужно снова взять на себя боль сбора данных и заново обучить всю модель.\n",
    "\n",
    "Это практически невозможно, особенно для крупных организаций, где набор и увольнение происходит почти каждую неделю.\n",
    "\n",
    "Теперь давайте разберемся, как подойти к этой проблеме, используя классификацию по `One Shot Learning`, которая помогает решить обе указанные выше проблемы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NDF5-SO_OlG"
   },
   "source": [
    "Вместо того, чтобы напрямую классифицировать входное (тестовое) изображение каждого из 10 человек в организации, эта сеть вместо этого принимает дополнительное эталонное изображение человека в качестве входных данных и будет производить оценку сходства, обозначающую шансы того, что два входных изображения принадлежат одному и тому же человеку.\n",
    "\n",
    "Обратите внимание, что эта сеть не учится классифицировать изображение напрямую по какому-либо из выходных классов. Скорее, он изучает функцию подобия, которая принимает два изображения в качестве входных данных и выражает, насколько они похожи.\n",
    "\n",
    "Как это решает две проблемы, о которых мы говорили выше?\n",
    "- Для обучения этой сети нам не требуется слишком много экземпляров класса, а для построения хорошей модели достаточно лишь нескольких экземпляров.\n",
    "- Но самое большое преимущество в простоте ее обучения в случае появления нового сотрудника.\n",
    "\n",
    "Теперь, чтобы сеть могла обнаружить его лицо, нам требуется только одно изображение его лица, которое будет сохранено в базе данных. Используя его в качестве эталонного изображения, сеть будет вычислять сходство для любого нового экземпляра,  представленного ей.\n",
    "\n",
    "Таким образом, мы говорим, что сеть предсказывает счет за `One Shot` (один выстрел)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMsUqpG7_Oh7"
   },
   "source": [
    "Код, который мы будем использовать, является реализацией методологии, описанной в этой [исследовательской статье](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) Грегори Коха и др.\n",
    "\n",
    "Архитектура модели соответствуtт описанию в статье. А прежде чем углубляться в детали, давайте сначала разберемся с архитектурой на высоком уровне.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07G4AS4TESLl"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL6.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QQaT9DKA5mQ"
   },
   "source": [
    "Две показанные выше сверточные нейронные сети не являются разными сетями, а представляют собой две копии одной и той же сети, отсюда и название Siamese Networks.\n",
    "\n",
    "В основном они имеют одинаковые параметры.\n",
    "\n",
    "Два входных изображения (x1 и x2) проходят через ConvNet, чтобы сгенерировать вектор признаков фиксированной длины для каждого (h (x1) и h (x2)).\n",
    "\n",
    "Предполагая, что модель нейронной сети обучена правильно, мы можем сделать следующую гипотезу: если два входных изображения принадлежат одному и тому же персонажу, то их векторы признаков также должны быть похожими, а если два входных изображения принадлежат разным символам, то их векторы признаков также будут разными.\n",
    "\n",
    "Таким образом, поэлементная абсолютная разница между двумя векторами признаков должна сильно отличаться в обоих вышеупомянутых случаях. И, следовательно, оценка сходства, генерируемая выходным сигмовидным слоем, также должна быть разной в этих двух случаях.\n",
    "\n",
    "Это центральная идея сиамских сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx4uWS-wA5if"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL5.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0wy9kCMBeCT"
   },
   "source": [
    "### Triplet Loss\n",
    "\n",
    "Для распознавания лиц, мы будем использовать `Triplet loss`, то есть подавать три изображения вместо двух.\n",
    "\n",
    "Триплет состоит из анкора `anchor`, положительного и отрицательного образцов и в основном применяется для распознавания лиц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrpOxR7MHNR2"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL10.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3GMPyDBBdvB"
   },
   "source": [
    "В `Triplet loss` расстояние между анкором и положительной выборки минимизировано, а расстояние между анкором и отрицательной выборки максимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB4IXb-GF_lU"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL8.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tis1Jizuze23"
   },
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/OShL9.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSO73aVFF_bu"
   },
   "source": [
    "### Но можно также использовать и `Contrastive Loss`\n",
    "\n",
    "о ней подробнее в статье [Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm2fGqzU_FJW"
   },
   "source": [
    "### Пример реализации сиамской сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ttI-K3gaidu"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "# скачаем необходимые файлы\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1axVcFAEZBtXSebZFzYZ_-sni3wiNVG7I' -O data.zip\n",
    "with ZipFile('data.zip', 'r') as folder: # Create a ZipFile Object and load sample.zip in it\n",
    "   folder.extractall() # Extract all the contents of zip file in current directory\n",
    "clear_output()\n",
    "# https://drive.google.com/file/d/1axVcFAEZBtXSebZFzYZ_-sni3wiNVG7I/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c43WoJMk7me3"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Um1pjDqH7mcJ"
   },
   "outputs": [],
   "source": [
    "# Вспомогательные фнкции\n",
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "class Config():\n",
    "    training_dir = \"/content/faces/training/\"\n",
    "    testing_dir = \"/content/faces/testing/\"\n",
    "    train_batch_size = 64\n",
    "    train_number_epochs = 50 # мы не будет обучать долго, чтобы не тратить учебное время"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEUiLKCxp5Yq"
   },
   "source": [
    "#### Custom Dataset Class\n",
    "0 для пары совпадений и 1 для пары самозванцев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lcvl6A7GpVOX"
   },
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            img1_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"L\")\n",
    "        img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F94BTagoqBD3"
   },
   "source": [
    "#### Using Image Folder Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIn7aUGBp9zt"
   },
   "outputs": [],
   "source": [
    "folder_dataset = datasets.ImageFolder(root=Config.training_dir)\n",
    "\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n",
    "                                        transform=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf0QoHh5qQ9G"
   },
   "source": [
    "#### Visualising some of the data\n",
    "Верхний ряд и нижний ряд любого столбца составляют одну пару.\n",
    "- 0 и 1 соответствуют столбцу изображения.\n",
    "- 0 означает несходство, а 1 означает сходство."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFw_N9noqG6v"
   },
   "outputs": [],
   "source": [
    "# Viewing the sample of images and to check whether its loading properly\n",
    "vis_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=8)\n",
    "dataiter = iter(vis_dataloader)\n",
    "\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print('1 = самозванцы и 0 = совпадения : \\n', example_batch[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXHuPu2-rD-f"
   },
   "source": [
    "#### Neural Net Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdFVbXbDqgf6"
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(1, 4, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(4),\n",
    "            \n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(4, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(8, 8, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(8*100*100, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(500, 2))\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXHwAaWdrJvK"
   },
   "source": [
    "#### Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bKArXHerH2r"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id7yJqB9rqZ4"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x76jVYOrMWm"
   },
   "outputs": [],
   "source": [
    "# Load the dataset as pytorch tensors using dataloader\n",
    "train_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=Config.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeGPQfsfrtGW"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Declare Siamese Network\n",
    "net = SiameseNetwork().cuda()\n",
    "# Decalre Loss Function\n",
    "criterion = ContrastiveLoss()\n",
    "# Declare Optimizer\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.0005 )\n",
    "# Declare Schedule\n",
    "scheduler = ReduceLROnPlateau(optimizer, \n",
    "                              'min',\n",
    "                              verbose = True,\n",
    "                              factor=0.9,\n",
    "                              patience = 3)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJPitl_7rzR7"
   },
   "outputs": [],
   "source": [
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number = 0\n",
    "\n",
    "for epoch in range(0, Config.train_number_epochs):\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        img0, img1 , label = data\n",
    "        img0, img1 , label = Variable(img0).cuda(), Variable(img1).cuda() , Variable(label).cuda()\n",
    "        output1,output2 = net(img0,img1)\n",
    "        optimizer.zero_grad()\n",
    "        loss_contrastive = criterion(output1,output2,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        if i %10 == 0 :\n",
    "            print(\"Epoch number {} Current loss {}\".format(epoch, loss_contrastive.item()))\n",
    "            iteration_number +=10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss_contrastive.item())\n",
    "            scheduler.step(loss_contrastive) # используем замедление скорости обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Khch51lc0mW5"
   },
   "outputs": [],
   "source": [
    "loss_contrastive.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4l1r6lF4IWi"
   },
   "outputs": [],
   "source": [
    "show_plot(counter[1:], loss_history[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E00PW_nDtfik"
   },
   "source": [
    "#### Test\n",
    "Последние 3 фотографии были исключены из обучения и будут использоваться для тестирования. Расстояние между каждой парой изображений обозначает степень сходства модели между двумя изображениями.\n",
    "\n",
    "Меньшее значение ошибки означает, что считает изображения похожими, в то время как более высокие значения указывают на то, что модель считает их разными людьми.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnUiPWzWsHdI"
   },
   "outputs": [],
   "source": [
    "folder_dataset_test = datasets.ImageFolder(root=Config.testing_dir)\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset_test,\n",
    "                                        transform=transforms.Compose([transforms.Resize((100,100)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)\n",
    "\n",
    "test_dataloader = DataLoader(siamese_dataset,num_workers=2,batch_size=1,shuffle=True)\n",
    "dataiter = iter(test_dataloader)\n",
    "x0,_,_ = next(dataiter)\n",
    "\n",
    "for i in range(10):\n",
    "    _,x1,label2 = next(dataiter)\n",
    "    concatenated = torch.cat((x0,x1),0)\n",
    "    \n",
    "    output1,output2 = net(Variable(x0).cuda(),Variable(x1).cuda())\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    imshow(torchvision.utils.make_grid(concatenated),\n",
    "           'Dissimilarity: {:.2f}'.format(euclidean_distance.cpu().data.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0CqR9FaA5ft"
   },
   "source": [
    "Итак, мы убедили в эффективности метода One-Shot Lerning.\n",
    "\n",
    "На сегодняшний день существует несколько сетей, которые специально обучены на распознавание лиц. Но наша задача была в обьяснении метода, когда нейронную сеть обучают распознавать не конкретный класс, а отличие отного класса от другого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGh7Cfai7Uo8"
   },
   "source": [
    "## Выводы\n",
    "---\n",
    "\n",
    "На этом уроке мы затронули проблемы, которые возникают при обучени на реальных данных.\n",
    "\n",
    "Одна из основных проблем - малые датасеты. Для того, чтобы обучить нейронку на небольшом датасете можно использовать:\n",
    "- `аугментацию`\n",
    "- `Tranfer learning`\n",
    "Однако необходимо помнить, что ни один из этих методов не защитит от ситуации, когда реальные данные будут сильно отличаться от тренировочных.\n",
    "\n",
    "В случае, когда у нас не только мало данных, но и еще и очень большое (возможно, неизвестное) число классов, можно воспользоваться `One-shot Learning`. В этом случае нейронка обучается не классифицировать изображения, а, наоборот, находить различия между классом и новыми данными. Для этого используются нейронные сети, относящиеся к классу сиамских нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9AauI577W3I"
   },
   "source": [
    "## Литература\n",
    "---\n",
    "\n",
    "### Augmentation\n",
    "\n",
    "[TORCHVISION.TRANSFORMS](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "[A survey on Image Data Augmentation for Deep Learning](https://link.springer.com/article/10.1186/s40537-019-0197-0)\n",
    "\n",
    "[Data augmentation for improving deep learning in image classification problem](https://www.researchgate.net/publication/325920702_Data_augmentation_for_improving_deep_learning_in_image_classification_problem)\n",
    "\n",
    "[Albumentations](https://github.com/albumentations-team/albumentations)\n",
    "\n",
    "\n",
    "\n",
    "### Transfer Learning\n",
    "[How To Do Transfer Learning For Computer Vision | PyTorch Tutorial](https://www.youtube.com/watch?v=6nQlxJvcTr0)\n",
    "\n",
    "[TRANSFER LEARNING FOR COMPUTER VISION TUTORIAL](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "[Python Pytorch Tutorials # 2 Transfer Learning : Inference with ImageNet Models](https://www.youtube.com/watch?v=Upw4RaERZic)\n",
    "\n",
    "[PyTorch - The Basics of Transfer Learning with TorchVision and AlexNet](https://www.youtube.com/watch?v=8etkVC93yU4)\n",
    "\n",
    "### One-shot Learning\n",
    "\n",
    "[One Shot Learning with Siamese Networks using Keras](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d)\n",
    "\n",
    "[One-shot image classification by meta learning](https://medium.com/nerd-for-tech/one-shot-learning-fe1087533585)\n",
    "\n",
    "[Siamese Neural Networks for One-shot Image Recognition](http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "[One-shot learning (Part 1/2): Definitions and fundamental techniques](https://heartbeat.fritz.ai/one-shot-learning-part-1-2-definitions-and-fundamental-techniques-1df944e5836a)\n",
    "\n",
    "[One-Shot Learning (Part 2/2): Facial Recognition Using a Siamese Network](https://heartbeat.fritz.ai/one-shot-learning-part-2-2-facial-recognition-using-a-siamese-network-5aee53196255)\n",
    "\n",
    "[FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "\n",
    "[One shot learning explained using FaceNet](https://medium.com/intro-to-artificial-intelligence/one-shot-learning-explained-using-facenet-dff5ad52bd38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "L11_Learning_on_Real_data.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "048ee726373a4ea9927629b3508152a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9aaf33833e0841c9a030498e05d0ce76",
      "max": 244418560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5c5874f2c0944a8797ecfc88b817ab39",
      "value": 244418560
     }
    },
    "1929e36a0f54498592420e0b4f080fa3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e434faa1cdd04b69baf6a5062541293c",
      "placeholder": "​",
      "style": "IPY_MODEL_ba6c72ca7645409bbfb2ff06d5d2ceab",
      "value": " 233M/233M [00:02&lt;00:00, 105MB/s]"
     }
    },
    "5c5874f2c0944a8797ecfc88b817ab39": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9aaf33833e0841c9a030498e05d0ce76": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6d4a77fa4ea4b43b8611891a00286f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba6c72ca7645409bbfb2ff06d5d2ceab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0222af7dd7942ee9f1b6213775cff3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_048ee726373a4ea9927629b3508152a0",
       "IPY_MODEL_1929e36a0f54498592420e0b4f080fa3"
      ],
      "layout": "IPY_MODEL_b6d4a77fa4ea4b43b8611891a00286f9"
     }
    },
    "e434faa1cdd04b69baf6a5062541293c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
