{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Обучение представлений</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже знаем, что модели классического машинного обучения способны искать закономерности в признаковых описаниях объектов и с помощью этого решать задачи, которые было бы невозможно решить путем прямого перенесения знаний в экспертные системы. Даже простые алгоритмы могут решать достаточно сложные задачи, такие как анализ тональности текста или предсказание погоды.\n",
    "\n",
    "Более того,\n",
    "\n",
    "> качество работы моделей классического машинного обучения напрямую зависит от того, как мы **представим данные** на входе модели.\n",
    "\n",
    "Например, если мы захотим классифицировать эмоциональную окраску текстов с помощью [наивного байесовского классификатора](https://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор), мы не сможем обучить его на сырых неструктурированных текстах.\n",
    "\n",
    "Прежде чем обучать классификатор, нам нужно найти способ **представить** тексты на входе модели. Если мы воспользуемся представлением текстов в виде \"мешка слов\" ([bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)), мы сможем получить приемлемое качество решения этой задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/bag_of_words_representation.png\"  width=\"900\"></center>\n",
    "\n",
    "<center><em> В представлении текстов в виде \"мешка слов\" сначала составляется словарь уникальных слов во всем корпусе текстов, а в представлении одного документа содержится количество раз, когда то или иное слово из словаря встретилось в этом документе.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем увидеть эффект от удачных представлений, рассмотрев иллюстрацию ниже. В этом классическом примере мы хотим обучить модель машинного обучения, скажем, логистическую регрессию, для поиска прямой линии, разделяющей голубые и оранжевые точки, описываемые признаками $x_1$ и $x_2$.\n",
    "\n",
    "Модель, которая способна строить лишь линейные разделяющие поверхности, не справится с этой задачей, потому что невозможно разделить два класса с помощью прямой, используя данные, *представленные в таком виде*.\n",
    "\n",
    "Если мы *изменим представление данных* для этой модели, и будем подавать ей на вход квадраты исходных признаков, то мы увидим, что ландшафт данных станет совершенно иным и линейная модель сможет разделить объекты по классам.\n",
    "\n",
    "**Действительно, представления имеют значение.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/two_circles_representation.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако в реальных задачах почти невозможно заранее узнать, как следует изменить представление данных, чтобы модель лучше решала задачу. Разные признаки обычно обладают разными свойствами, которые могут подходить или не подходить для решения конкретной задачи.\n",
    "\n",
    "Вот почему процесс ручного создания признаков является такой сложной задачей. И это то самое место, где глубокие нейронные сети находят свое применение. Применяя глубокое обучение, нам не нужно заботиться о том, как вручную преобразовать признаки для решения каждой конкретной задачи.  \n",
    "\n",
    "Представляя из себя серию линейных и нелинейных преобразований, составленных в некой иерархической последовательности, глубокие нейронные сети обладают способностью выучивать подходящие представления для решения разных задач. Они комбинируют примитивные представления для создания более сложных и выразительных структур.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/neural_network_layers_representations.png\" alt=\"alttext\" width=\"500\"></center>\n",
    "\n",
    "<center><em> Модель компьютерного зрения на разных слоях создает все более сложные представления: от примитивных на начальных слоях до более сложных на последних слоях.\n",
    "\n",
    "Source: <a href=\"https://www.deeplearningbook.org/\">Deep Learning Book</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С этой точки зрения, **глубокие нейронные сети являются с моделями обучения представлений**.\n",
    "\n",
    "Глубокие нейронные сети типично состоят из двух частей:\n",
    "\n",
    "*  **Энкодера** — экстрактора высокоуровных признаков.  Энкодер проецирует входные данные в новое пространство представлений.\n",
    "*  **Линейной модели**. Линейная модель, например, классификации, строит линейные разделяющие поверхности между классами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако обучение представлений в более широком понимании ставит перед собой немного иную **цель — переводить исходные представления в другие, обучаемые представления**, вместо того, чтобы переводить исходные представления в целевые переменные (как то делают модели классификации). Обычно задаются также целью делать эти **обучаемые представления достаточно компактными** и способными обобщаться на данные из схожих доменов.\n",
    "\n",
    "Мы бы хотели, чтобы обучаемые представления могли хорошо переноситься на другие задачи и помогали в тех случаях, когда трудно или даже невозможно получить разметку данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже встречались с моделями обучения представлений. Одним из примеров такого обучения является Transfer learning. Мы используем **обученные представления** на выходе предобученного на большом объеме данных экстрактора признаков, рассчитывая на то, что они будут \"хорошим\" образом представлять наши данные. Используя эти представления как отправную точку, мы можем обучать достаточно простую модель в 1-2 слоя на небольшом количестве размеченных данных и получать лучшее качество, чем если бы мы обучали простую модель на исходных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/transfer_learning_change_classes_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, одним из ограничений Transfer learning является то, что данные для предобучения могут сильно отличаться от данных для дообучения, и тогда надежда на удачность предобученных представлений теряется. **Хотелось бы иметь возможность обучать хорошие представления сразу под те данные, которые у нас есть**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsuprevised Representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем обучать такие эффективные представления вовсе без разметки — исследуя внутренее устройство данных в unsuprevised манере.\n",
    "\n",
    "Одним из примеров Unsupervised Representation learning являются **большие языковые модели**. Мы уже упомянули один способ представления текстов — в виде мешка слов. Такое представление позволяет классифицировать тексты по тематикам или по эмоциональному окрасу, однако есть недостаток — это представление не учитывает порядок слов и контекстные связи между словами в тексте. Это ограничивает нас в точности решения задач, а такие задачи, как генерация ответов на вопросы или машинный перевод не смогут решаться на таких ограниченных представлениях.\n",
    "\n",
    "Большие языковые модели, такие как BERT обучаются создавать контекстные представления слов **не используя разметку** под конкретную задачу, а обрабатывая сырые неструктурированные тексты. Напомним, что одна из задач, на которую обучается BERT — предсказание маскированного слова по окружающему контексту — вовсе не требует разметки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/bert_masked.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате BERT оказывается способен обучать богатые, семантически значимые представления, которые отражают разные значения слов в завсисимости от контекста.\n",
    "\n",
    "Наиболее полезным применением таких моделей, как BERТ, является то, что после обучения на большом объеме неструктурированного текста мы можем дообучать модель под различные задачи, не имея под рукой большого количества размеченных данных, и будем иметь ощутимый прирост в качестве по сравнению с обучением \"с нуля\".\n",
    "\n",
    "Это особенно важно, поскольку разметка данных — это одно из узких мест в процессе обучения глубоких нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A Few Words on Representation Learning](https://sthalles.github.io/a-few-words-on-representation-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из подобластей Representation learning является Metric learning.\n",
    "\n",
    "Целью Metric learning является обучение модели, которая отображает данные в пространство представлений — эмбеддингов. При этом вводится требование: **расстояние между векторами в пространстве эмбеддингов должно отражать схожесть исходных объектов** — схожие объекты должны отображаться в близкие вектора, а различающиеся — в далекие.\n",
    "\n",
    "Metric learning может применяться в задачах, когда не представляется возможным разбить данные на классы так, чтобы в каждом классе было достаточно много объектов.\n",
    "\n",
    "Рассмотрим, например, задачу распознавания лиц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/videoanalytics.png\"  width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход системе подается фото лица человека. Требуется сопоставить его с другим изображением или изображениями, например, хранящимися в базе данных, и таким образом идентифицировать человека на фотографии.\n",
    "\n",
    "На первый взгляд кажется, что это задача классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/classifier_scheme.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все изображения одного человека будем считать относящимися к одному классу, и модель будет этот класс предсказывать.\n",
    "\n",
    "Для небольшой организации, в которой всего несколько десятков сотрудников такой подход может сработать.\n",
    "При этом возникнут проблемы:\n",
    "\n",
    "1. чтобы обучить такую ​​систему, нам сначала потребуется много (десятки) разных изображений каждого сотрудника.\n",
    "\n",
    "2. когда человек присоединяется к организации или покидает ее, приходится менять структуру модели и обучать ее заново.\n",
    "\n",
    "\n",
    "Это практически невозможно для крупных организаций, где набор и увольнение происходит почти каждую неделю. И в принципе невозможно для города масштаба Москвы или Лондона, в котором миллионы жителей и сотни тысяч приезжих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование векторов-признаков (embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому используется другой подход.\n",
    "Вместо того, чтобы классифицировать изображения, модель учится выделять ключевые признаки и на их основе строить компактный вектор, достаточно точно описывающий лицо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/face_as_embedding.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В англоязычной литературе такие вектора признаков называются **embedding**, и мы тоже будем использовать это обозначение.\n",
    "\n",
    "Может возникнуть вопрос: не потеряем ли мы важную информацию, сжав изображение в несколько сотен чисел?\n",
    "\n",
    "Чтобы ответить на него, вспомним, как работает [фоторобот](https://en.wikipedia.org/wiki/Facial_composite)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/photorobot.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения фотореалистичного изображения лица достаточно нескольких ключевых признаков: глаза, волосы, рот, нос...\n",
    "Каждый из них кодируется максимум несколькими сотнями целочисленных значений.\n",
    "\n",
    "Значит, вектора-признака, скажем, из 128 вещественных чисел будет более чем достаточно.\n",
    "Правда интерпретировать значения, которые закодирует в него нейросеть, будет не столь просто.\n",
    "\n",
    "Если нам удастся обучить модель кодировать в embedding признаки, важные для сравнения, то мы сможем сравнивать векторы между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/face_dist.png\"  width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если расстояние между векторами для лиц, которые похожи друг на друга, будут маленькими, а у непохожих, наоборот, большими, то мы сможем экспериментально подобрать порог $d$ и, сравнивая с ним расстояние между двумя векторами, принимать решение: принадлежат ли они одному человеку или нет.\n",
    "\n",
    "*Можно оценивать не расстояние, а схожесть (similarity). В этом случае неравенства поменяют знак, но логика останется прежней*.\n",
    "\n",
    "Теперь, чтобы идентифицировать человека, требуется только одно изображение его лица. Эмбеддинг  этого изображения можно сравнить с эмбеддингами других лиц из БД, используя  [k-NN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) или иной метод кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/search_in_embedding_space.jpg\"  width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая модель не учится классифицировать изображение напрямую по какому-либо из выходных классов. Она учится выделять признаки, важные при сравнении.\n",
    "\n",
    "Такой подход решает обе проблемы, о которых мы говорили выше:\n",
    "- для обучения такой сети нам не требуется много экземпляров объектов одного класса, а   достаточно лишь нескольких,\n",
    "- простота обучения в случае появления новых объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сиамская сеть (Siamese Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какая архитектура должна быть у модели, генерирующей векторы признаков?\n",
    "\n",
    "Можно было бы использовать обычную сеть, обученную для задачи классификации, и затем удалить из нее один или несколько последних слоёв."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/embedding_from_classifier.png\"  width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Активации последнего слоя представляют собой отклики на некие высокоуровневые признаки, потенциально важные для классификации, и их можно интерпретировать как embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet\n",
    "import torch\n",
    "\n",
    "face1 = torch.randn((3, 224, 224))\n",
    "face2 = torch.randn((3, 224, 224))\n",
    "\n",
    "model = alexnet(weights=\"AlexNet_Weights.DEFAULT\")\n",
    "# remove classification layer\n",
    "model.fc = model.classifier[6] = torch.nn.Identity()\n",
    "\n",
    "# get embeddings\n",
    "embedding1 = model(face1.unsqueeze(0))\n",
    "embedding2 = model(face2.unsqueeze(0))\n",
    "\n",
    "diff = torch.nn.functional.pairwise_distance(embedding1, embedding2)\n",
    "print(\"L2 distance: \", diff.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход будет работать.\n",
    "Однако можно заметно улучшить точность, используя функцию потерь, которая оценивает именно качество сравнения, а не классификации.\n",
    "\n",
    "\n",
    "Рассмотрим подход, основанный на методологии, описанной в статье [Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/siamese_neural_network_scheme.png\"  width=\"800\">\n",
    "\n",
    "<center><em>Используются две копии одной и той же сети, отсюда и название Siamese Networks.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два входных изображения ($x_1$ и $x_2$) проходят через одну и ту же сверточную сеть, на выходе для каждого изображения генерируется вектор признаков фиксированной длины $h_1$ и $h_2$.\n",
    "\n",
    "Модель обучается генерировать близкие вектора для изображений одного объекта и далекие для разных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/siamese_neural_network_idea_scheme.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценивая расстояние между двумя векторами признаков,\n",
    "которое будет малым для схожих объектов и большим для различных, мы сможем оценить их сходство.\n",
    "\n",
    "Это центральная идея сиамских сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какую функцию потерь использовать для обучения такой сети?\n",
    "\n",
    "Очевидно, функция потерь должна будет учитывать не один выход модели, а как минимум два.\n",
    "\n",
    "Популярной на сегодняшний день является  `Triplet loss`, которой требуется  три embedding вместо двух."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/triplet_loss_scheme.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сгенерировать три эмбеддинга, модель должна получать на вход три изображения.\n",
    "\n",
    "Первые два должны относиться к одному и тому же объекту (человеку), а третье — к другому.\n",
    "\n",
    "\n",
    "Таким образом, триплет состоит из опорного (якорного, anchor), положительного (positive) и отрицательного (negative) образцов.\n",
    "\n",
    "Описание в статье [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Сама функция потерь будет выглядеть следующим образом:\n",
    "\n",
    "$$TripletLoss = \\sum_{i=1}^{N} L_i(x_i^{a},x_i^{p},x_i^{n})$$\n",
    "\n",
    "$$L_i(x_i^{a},x_i^{p},x_i^{n})=max(0,\\left\\| f(x_i^{a}) -f(x_i^{p}) \\right\\|_2^{2} - \\left\\| f(x_i^{a}) -f(x_i^{n}) \\right\\|_2^{2} + margin)$$\n",
    "\n",
    "Где:\n",
    "\n",
    "\n",
    "$x_i^{a}$ — базовое изображение (anchor),\n",
    "\n",
    "$x_i^{p}$ — изображение того же объекта (positive),\n",
    "\n",
    "$x_i^{n}$ — изображение другого объекта (negative),\n",
    "\n",
    "$f(x)$ — **нормированный** выход модели (embedding) для входа $x$,\n",
    "\n",
    "\n",
    "$\\left\\| x \\right\\|_2$ — это L2 (Euclidean norm), соответственно $\\left\\| a \\right\\|_2^{2}$ — это L2 в квадрате,\n",
    "\n",
    "$margin$ — это константа или минимальный \"зазор\", на который расстояние до эмбеддинга негативного объекта обязано превосходить расстояние до позитивного (идея такая же, как в SVM Loss) .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/triplet_loss_idea_scheme.png\"  width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе обучения с  Triplet Loss расстояние между эмбеддингами опорного и позитивного объектов уменьшается, а между эмбеддингами опорного и отрицательного — увеличивается.\n",
    "\n",
    "Важным дополнением является то, что embedding-и нормируются. В результате нормировки каждый вектор-признак будет иметь единичную длину."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/sphere_distance.png\"  width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем рассматривать embedding-и как точки на n-мерной сфере с радиусом 1.\n",
    "\n",
    "Это удобно, так как все расстояния между embedding будут лежать на отрезке $[0; 2]$, и нам будет проще подобрать порог для сравнения.\n",
    "\n",
    "Кроме того, можно использовать другие меры расстояния, например, [косинусное расстояние](https://en.wikipedia.org/wiki/Cosine_similarity), которое определяется углом между векторами, лежит на отрезке $[-1; 1]$ и соответствует расстоянию между точками на поверхности сферы.\n",
    "\n",
    "В [статье](https://arxiv.org/abs/1503.03832) авторы минимизируют Евклидово расстояние, но подход будет работать и для других метрик сходства, например, косинусного расстояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch есть две реализации TripletLoss\n",
    "\n",
    "[`TripletMarginLoss`](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html) — минимизирует $L_p$ норму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "loss = triplet_loss(anchor, positive, negative)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`TripletMarginWithDistanceLoss`](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss) — позволяет задать произвольную функцию расстояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "triplet_loss = nn.TripletMarginWithDistanceLoss(\n",
    "    margin=1.0, distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    ")\n",
    "loss = triplet_loss(anchor, positive, negative)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие функции потерь для сиамских сетей:\n",
    "\n",
    "Исторически первой появилась `Contrastive Loss`, о ней подробнее в статье [Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](https://www.researchgate.net/publication/4246277_Dimensionality_Reduction_by_Learning_an_Invariant_Mapping)\n",
    "\n",
    "\n",
    "В PyTorch есть реализация\n",
    "[CosineEmbeddingLoss](https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html), она позволяет обучать модель на парах изображений, минимизировав [косинусное расстояние](https://en.wikipedia.org/wiki/Cosine_similarity) между embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация сиамской сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим небольшой фрагмент датасета с лицами. Внутри архива фото лиц сгруппированы по папкам\n",
    "\n",
    "```\n",
    "faces/\n",
    "├── training/\n",
    "|   ├── s1/\n",
    "|   |   ├── 1.pgm\n",
    "|   |   ├ ...\n",
    "|   |   └── 10.pgm\n",
    "|   ├ ... (excluding 5...7)\n",
    "|   └── s40/\n",
    "|       ├── 1.pgm\n",
    "|       ├ ...\n",
    "|       └── 10.pgm\n",
    "└── testing/\n",
    "    ├── s5/\n",
    "    |   ├── 1.pgm\n",
    "    |   ├ ...\n",
    "    |   └── 10.pgm\n",
    "    ├ ...\n",
    "    └── s7/\n",
    "        ├── 1.pgm\n",
    "        ├ ...\n",
    "        └── 10.pgm\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "В каждой папке по 10 фото лица одного и того же человека."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/small_face_dataset.zip\n",
    "!unzip -qn small_face_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы результаты воспроизводились, зафиксируем SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for TripletLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для TripletLoss потребуются три изображения: anchor, positive, negative, и метод __get_item__ должен возвращать их нам. Первые два должны принадлежать одному человеку, а третье — другому.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self, dir=None, transform=None):\n",
    "        self.dir = dir\n",
    "        self.transform = transform\n",
    "        # list of paths to all images in self.dir\n",
    "        self.files = glob(f\"{self.dir}/**/*.pgm\", recursive=True)\n",
    "        self.person_index = self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"\n",
    "        Creates a dictionary \"person_index\" with such structure:\n",
    "        {'s1': ['faces/training/s1/1.pgm',\n",
    "                ...,\n",
    "                'faces/training/s1/10.pgm'],\n",
    "         's2': ...}\n",
    "\n",
    "        keys   — IDs of persons ('s1', 's2', ...)\n",
    "        values — lists of paths to person's photos\n",
    "        \"\"\"\n",
    "\n",
    "        person_index = {}\n",
    "        for path in self.files:\n",
    "            id = self.path2id(path)\n",
    "            if not id in person_index:\n",
    "                person_index[id] = []\n",
    "            person_index[id].append(path)\n",
    "        return person_index\n",
    "\n",
    "    def path2id(self, path):\n",
    "        \"\"\"\n",
    "        Takes full path like 'faces/training/s1/1.pgm'\n",
    "        and extracts ID from it, like 's1'\n",
    "        \"\"\"\n",
    "\n",
    "        return path.replace(self.dir, \"\").split(\"/\")[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_path = self.files[index]\n",
    "        positive_path = self.find_positive(anchor_path)\n",
    "        negative_path = self.find_negative(anchor_path)\n",
    "\n",
    "        # Loading the images\n",
    "        anchor = Image.open(anchor_path)\n",
    "        positive = Image.open(positive_path)\n",
    "        negative = Image.open(negative_path)\n",
    "\n",
    "        if self.transform is not None:  # Apply image transformations\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def find_positive(self, anchor_path):\n",
    "        id = self.path2id(anchor_path)\n",
    "        all_exept_my = self.person_index[id].copy()\n",
    "        all_exept_my.remove(anchor_path)\n",
    "        return random.choice(all_exept_my)\n",
    "\n",
    "    def find_negative(self, anchor_path):\n",
    "        all_exept_my_ids = list(self.person_index.keys())\n",
    "        id = self.path2id(anchor_path)\n",
    "        all_exept_my_ids.remove(id)\n",
    "        selected_id = random.choice(all_exept_my_ids)\n",
    "        return random.choice(self.person_index[selected_id])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем несколько изображений, чтобы убедиться, что класс датасета функционирует должным образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Create dataset instance\n",
    "siamese_dataset = SiameseNetworkDataset(\n",
    "    \"faces/training/\",\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((105, 105)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create dataloader & extract batch of data from it\n",
    "vis_dataloader = DataLoader(siamese_dataset, batch_size=8, shuffle=True)\n",
    "dataiter = iter(vis_dataloader)\n",
    "example_batch = next(dataiter)  # anc, pos, neg\n",
    "\n",
    "# Show batch contents\n",
    "concatenated = torch.cat((example_batch[0], example_batch[1], example_batch[2]), 0)\n",
    "grid = torchvision.utils.make_grid(concatenated)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "plt.gcf().set_size_inches(20, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждом столбце тройка изображений. Первое и второе принадлежат одному человеку, третье — другому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас устроит любая модель для работы с изображениями. Например, ResNet18.\n",
    "\n",
    "Все, что от нас требуется, это:\n",
    "- заменить последний слой\n",
    "- отправлять на анализ три изображения вместо одного. Соответственно на выходе тоже будут три вектора признаков (embedding)\n",
    "\n",
    "\n",
    "Пожалуй, единственный вопрос — это размерность последнего слоя. В промышленных системах распознавания лиц, которые тренируются на датасетах из миллионов изображений, используются embedding размерностью от 128 до 512.\n",
    "\n",
    "Для демонстрационной задачи нам должно хватить 32 значений. Количество выходов последнего линейного слоя установим равным 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(weights=None)\n",
    "        # Because we use grayscale images reduce input channel count to one\n",
    "        self.model.conv1 = nn.Conv2d(\n",
    "            1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
    "        )\n",
    "        # Replace ImageNet 1000 class classifier with 64- out linear layer\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, latent_dim)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        out = self.model(x)\n",
    "        # normalize embedding to unit vector\n",
    "        out = torch.nn.functional.normalize(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        output1 = self._forward(anchor)\n",
    "        output2 = self._forward(positive)\n",
    "        output3 = self._forward(negative)\n",
    "\n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузчики данных не отличаются от загрузчиков для обычной сети.\n",
    "Единственное отличие — это добавление аугментации в виде случайного отражения по вертикали к обучающим данным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply augmentations on train data\n",
    "img_trans_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((105, 105)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_trans_test = transforms.Compose(\n",
    "    [transforms.Resize((105, 105)), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = SiameseNetworkDataset(\"faces/training/\", transform=img_trans_train)\n",
    "val_dataset = SiameseNetworkDataset(\"faces/testing/\", transform=img_trans_test)\n",
    "\n",
    "batch_size = 300\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, num_workers=2, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, num_workers=2, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличие от  сетей для классификации в том, что у модели 3 выхода, и все их надо передать в loss. При этом нет меток в явном виде.\n",
    "Определить, какой embedding относится к позитивному образцу, а какой — к негативному, можно только порядком их следования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, criterion, optimizer, train_loader):\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "    for epoch in range(0, num_epochs):\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "\n",
    "            anc, pos, neg = batch\n",
    "            output_anc, output_pos, output_neg = model(\n",
    "                anc.to(device), pos.to(device), neg.to(device)\n",
    "            )\n",
    "            loss = criterion(output_anc, output_pos, output_neg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.detach().cpu().item()\n",
    "\n",
    "        loss_history.append(train_loss / len(train_loader))\n",
    "        last_epoch_loss = torch.tensor(loss_history[-1])\n",
    "        print(\"Epoch {} with {:.4f} loss\".format(epoch, last_epoch_loss))\n",
    "\n",
    "    return loss_history, last_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функции расстояния в TripletLoss возьмем косинусное расстояние (величина, обратная к косинусной близости)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 32\n",
    "model = SiameseNet(latent_dim).to(device)\n",
    "criterion = nn.TripletMarginWithDistanceLoss(\n",
    "    margin=1.0, distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 9\n",
    "loss_history, _ = train(num_epochs, model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем график loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"num of epochs\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выведем тройки изображений из проверочного датасета и посмотрим на косинусную близость (схожесть) для позитивных и негативных пар. Если модель обучилась, схожесть для позитивных пар будет больше, чем для негативных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for visualization\n",
    "def show(img, text=None):\n",
    "    img_np = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(75, 120, text, fontweight=\"bold\")\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))  # [CxHxW] -> [HxWxC] for imshow\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_imgs(model, test_loader):\n",
    "    similarity_pos = []\n",
    "    similarity_neg = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i, batch in enumerate(test_loader, 0):\n",
    "            anc, pos, neg = batch\n",
    "            output_anc, output_pos, output_neg = model(\n",
    "                anc.to(device), pos.to(device), neg.to(device)\n",
    "            )\n",
    "            # compute euc. distance\n",
    "            sim_pos = F.cosine_similarity(output_anc, output_pos).item()\n",
    "            sim_neg = F.cosine_similarity(output_anc, output_neg).item()\n",
    "\n",
    "            similarity_pos.append(sim_pos)\n",
    "            similarity_neg.append(sim_neg)\n",
    "\n",
    "            if not i % 5:\n",
    "                concatenated = torch.cat((anc, pos, neg))\n",
    "                result = \"OK\" if sim_neg < sim_pos else \"BAD\"\n",
    "                show(\n",
    "                    torchvision.utils.make_grid(concatenated),\n",
    "                    f\"Positive / negative similarities: {sim_pos:.3f} / {sim_neg:.3f} - {result}\",\n",
    "                )\n",
    "\n",
    "    return similarity_pos, similarity_neg\n",
    "\n",
    "set_random_seed(42)\n",
    "similarity_pos, similarity_neg = plot_imgs(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но такая оценка субъективна, давайте посмотрим на распределение схожестей по категориям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "similarities = {\"The same person\": similarity_pos, \"Another person\": similarity_neg}\n",
    "\n",
    "ax = sns.histplot(similarities, bins=20)\n",
    "ax.set(xlabel=\"Pairwise similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что схожесть между двумя фото одного и того же человека в среднем больше, чем схожесть между фото разных людей.\n",
    "\n",
    "Если бы мы проектировали систему распознавания лиц, нужно было бы выбрать порог, чтобы сравнивать с ним схожесть и принимать решение о том, верифицировать фото как подлинное или нет.\n",
    "\n",
    "Соответственно, для нашего игрушечного датасета такой порог следует выбирать $≈0.75$. При этом мы будем иметь некоторое количество ошибок и ложно распознавать постороннего человека."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\">Заключение по Mertric learning</font>\n",
    "\n",
    "В случае, когда у нас не только мало данных, но еще и очень большое (возможно, неизвестное) число классов, можно воспользоваться подходом Metric learning. В этом случае нейронная сеть обучается не классифицировать объекты, а отображать их в пространство признаков-эмбеддингов. При этом абстрактная схожесть исходных объектов должна отражаться в математической схожести векторов-эмбеддингов. Для этого используются нейронные сети, относящиеся к классу сиамских нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоэнкодеры (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим подходом к обучению представлений являются **автоэнкодеры** — специальный класс нейросетевых архитектур, способный **обучаться на данных без разметки (unsupervised learning)**.\n",
    "\n",
    "С помощью автоэнкодеров становится возможным обучать представления для данных не имея разметку. Эти представления могут затем использоваться для решения других задач:\n",
    "* понижение размерности\n",
    "* очистка данных от шумов\n",
    "* предобучение для дальнейшего дообучения на размеченных данных\n",
    "* обнаружение аномалий\n",
    "* и даже генерация новых данных!\n",
    "\n",
    "Разницу между двумя **supervised и unsupervised learning** можно понять при помощи илююстрации ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/supervised_learning.png\" alt=\"alttext\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/unsupervised_learning.png\" alt=\"alttext\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае **supervised learning** для каждого объекта нам известна метка, что вот это — изображение яблок, это — изображение груш и т.&nbsp;д. Далее модель учится по изображению определять фрукт.\n",
    "\n",
    "В случае же **unsupervised learning** модель просматривает все изображения фруктов, не зная, где какой фрукт находится, и далее формирует представление, которое **неявно** делит фрукты по похожести.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачем вообще изучать такой тип задачи?\n",
    "\n",
    "1. Иногда у нас слишком мало размеченных объектов, чтобы учить на них какую-либо задачу классификации и т. д. При этом у нас огромное количество неразмеченных данных. Мы можем **надеяться**, что если мы как-то обработаем наши данные, то они сами разделятся каким-то образом, согласующимся с метками.\n",
    "\n",
    "2. Человеческий мозг в основном учится в unsupervised манере. Возможно, для того чтобы решать задачи, которые легко по сравнению с компьютером решает человек, нам стоит и компьютер учить похожим образом.\n",
    "\n",
    "3. Часто обучение без учителя дает результаты, которые в дальнейшем позволяют быстро адаптироваться к новым задачам обучения с учителем и переключаться между ними. Причем делать это **эффективнее**, чем transfer learning. (*Stop learning tasks, start learning skills* — Satinder Singh)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понижение размерности — задача, близкая Representation и Unsuprevised learning.\n",
    "\n",
    "Задача понижения размерности возникает, когда мы хотим данные из пространства высокой размерности преобразовать в пространство более низкой размерности с сохранением одного или нескольких свойств, например:\n",
    "\n",
    "* данные реконструируются обратно почти без ошибки,\n",
    "* расстояние между объектами сохраняется.\n",
    "\n",
    "Зачем это нужно? По многим причинам.\n",
    "\n",
    "Многие алгоритмы показывают себя плохо на пространствах большой размерности в принципе ([проклятье размерности](https://en.wikipedia.org/wiki/Curse_of_dimensionality)).\n",
    "\n",
    "Некоторые алгоритмы просто работают значительно дольше, при этом качество их работы не изменится от уменьшения размерности.\n",
    "\n",
    "Понижение размерности позволяет использовать память более эффективно и подавать модели на обучение за один раз больше объектов.\n",
    "\n",
    "Также понижение размерности помогает избавиться от шума. Как? Обсудим дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоэнкодер — архитектура нейросети, которая сначала с помощью нейросети-энкодера сжимает признаковое описание объекта в вектор небольшой размерности (он называется скрытым представлением), а затем восстанавливает этот вектор в исходное признаковое пространство с помощью нейросети-декодера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практика показывает, что при работе с изображениями скрытое представление картинки позволяет делать очень интересные и красивые вещи — например, очищать изображение от шума, проводить гладкую интерполяцию между двумя написанными от руки цифрами, генерировать новую рукописную цифру со стилем от имеющейся.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откуда берутся эти свойства? Они являются следствием сжатия информации. Одна из форм сжатия — это классификация, которую мы уже делали. Если это цифры, то вместо изображения можно сохранить только один признак — какая это цифра. Это предельное сжатие информации, но при попытке перевести цифру в картинку мы уже не имеем достаточно информации, чтобы картинка получалась разной. Если не так сильно ограничивать информацию в точке максимального сжатия, то кроме класса цифры сохранится еще что-то и изображение удастся восстановить с большим количеством сохранённых деталей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/autoencoder_architecture.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сжатие информации и потери"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоэнкодер может быть без потерь и с потерями (lossless и lossy). В какой-то степени это альтернативно методам сжатия архиваторов и кодирования контента (zip, mp3, jpeg, flac, ...). Можно ли сделать сжатие на нейронных сетях с помощью автоэнкодеров? Да, это будет работать. Размер сети будет большим, но сжатие может превзойти другие алгоритмы. Практический пример — [проект Google Lyra](https://ai.googleblog.com/2021/02/lyra-new-very-low-bitrate-codec-for.html), в котором подобный подход был применен для компрессии звука, и проект [NVIDIA Maxine](https://developer.nvidia.com/maxine), где в свою очередь сжимают видео.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/autoencoder_scheme.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему это может работать? Дело в том, что нейронная сеть может сформулировать набор правил, по которому на основе латентного представления приближенно или точно кодировать (за счет кодировщика), а затем восстанавливать исходный объект (при помощи декодировщика).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/original_space_to_latent_space.png\" alt=\"alttext\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А почему мы уверены, что такой набор правил будет существовать, и мы вообще имеем право понижать размерность пространства?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В глубоком обучении часто используют предположение о многообразии (manifold assumption). Это предположение о том, что реальные данные не распределены равномерно по пространству признаков, а занимают лишь его малую часть — **многообразие** (manifold).\n",
    "\n",
    "Если предположение верно, то каждый объект может быть достаточно точно описан новыми признаками в пространстве значительно меньшей размерности, чем исходное пространство признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/manifold_assumption.png\" alt=\"alttext\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве случаев это действительно так. Например, лица людей на фотографиях 300x300, очевидно, лежат в пространстве меньшей размерности, нежели 90&nbsp;000. Ведь не каждая матрица 300 на 300, заполненная какими-то значениями от 0 до 1, даст нам изображение человека."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/manifold_assumption_faces_example.png\" alt=\"alttext\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод главных компонент (Principal Component Analysis) — это метод отображения векторов свойств объектов (помним, что у нас объект всегда описывается вектором свойств, длина вектора — это количество свойств) в вектора производных свойств (**компонент**) меньшей длины с помощью линейной комбинации, чтобы обратной операцией можно было восстановить значения векторов свойств как можно ближе к исходным. То есть PCA тоже выполняет сжатие информации, он тоже работает для группы объектов (а нейронная сеть автоэнкодера учится под определённую группу объектов).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно PCA работает для центрированных переменных. Каждая следующая компонента проводится перпендикулярно предыдущим и так, чтобы объяснить наибольшую часть дисперсии, не объясненной предыдущими компонентами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/3d_to_2d_pca.png\" width=1000/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графически можно представить PCA, как поиск подпространства, проекция точек на которое минимально меняет координаты в исходном пространстве. Например, для объектов на плоскости PCA можно сделать в одномерное пространство — на&nbsp;прямую.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/pca_decomposition.png\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямая определяется только вектором нормали, то есть линия проекции проходит через 0.\n",
    "\n",
    "Вернемся к примеру с лицами — долгое время для распознавания лиц размерности 128\\*128 использовалось представление, полученное при помощи PCA. Для хорошего качества восстановления хватает около 100 компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналогия AE и PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличия PCA и AE в том, что PCA выполняет линейную комбинацию над компонентами исходного вектора свойств объекта, а AE, как правило, нелинейную. PCA вычисляется однозначно, а AE обучается без гарантии нахождения наилучшего положения. PCA гарантирует ортогональный базис для разложения сжатых свойств, а AE — нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA будет частным случаем AE, если в AE сделать только **один полносвязный скрытый слой** с количеством нейронов, равным требуемому числу компонент, сделать линейную функцию активации, и использовать среднеквадратическую функцию потерь (MSE). Кроме этого, необходимо будет нормировать признаки перед подачей их на вход AE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/pca_autoencoder.png\" alt=\"alttext\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда PCA позволяет рассчитать веса для нейронов такого автоэнкодера. При этом гарантировав (в отличие от градиентного спуска) наилучшее решение задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очищение изображения от шумов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересное применение автоэнкодеров — очищение входной картинки от шумов. Такое принципиально возможно из-за того, что размерность латентного пространства очень мала по сравнению с размерностью входного пространства — в нём попросту нет места случайному шуму, но зато есть место для общих закономерностей из входного пространства."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы подаём на обучении автоэнкодера такой незашумлённый датасет, что в нём на самом деле есть некое пространство свойств, которое его описывает. На выходе энкодера в изображении останутся именно эти свойства. Шум является внешним свойством и не сможет закодироваться.\n",
    "\n",
    "Иными словами, за счет кодировщика и декодировщика автоэнкодер выучивается «проектировать» объекты на латентное пространство и восстанавливать их из него. Если шум небольшой, то автоэнкодер спроецирует объект в нужное место в латентном пространстве и обратно восстановит его уже без шума.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/autoencoder_clean_noise.png\" alt=\"alttext\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом важно понимать, что если шум поместит наш объект так, что автоэнкодеру придется выбирать между разными вариантами проекции, могут возникнуть артефакты.\n",
    "\n",
    "В случае, приведенном на рисунке, зашумленному $x$ соответствуют две группы объектов из реального датасета. Если мы, к примеру, оптимизируем MSE, то автоэнкодеру «экономнее» всего будет восстанавливать нечто между двумя группами. При этом этого «нечто» в природе не существует или оно очень маловероятно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/reconstructed_between_2_distribution.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление шума к исходной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в случае отсутствия шума в изначальной выборке, ее малом размере и т.д. можно добавлять шум к самим исходным данным, получая из объекта $x$ объект $\\tilde{x}$ и требуя от энкодера восстановить на основе зашумленного объекта исходный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/denoising_autoencoder.png\" alt=\"alttext\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот подход может работать и является примером *аугментации данных*. Он может дополнительно заставить автоэнкодер выучивать полезные признаки, т.е. его можно использовать, даже если целью не является получение автоэнкодера, избавляющего данные от шума.\n",
    "\n",
    "С ним, однако, надо быть очень аккуратным:\n",
    "\n",
    "1. Шум, который вы добавляете, не должен сильно менять исходный объект. Если это происходит, то либо автоэнкодер легко будет находить места, где был добавлен шум, и при этом делать ему это будет легче, чем учить сжатое представление данных. Либо автоэнкодер выучит о ваших данных что-то такое, чего там на самом деле быть не может. К примеру, если добавить к признакам, которые всегда целые, нормальный шум, ничего хорошего не выйдет.\n",
    "\n",
    "2. Шум должен соответствовать «естественному шуму». Если реальный шум в данных отличается от того, на котором учился автоэнкодер, есть вероятность, что он не будет очищать данные от исходного шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA для избавления от шума"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим PCA как простейший автоэнкодер для очищения от шумов изображений базы MNIST. Нам потребуется база MNIST, NumPy, библиотека отрисовки matplotlib и сам PCA, который есть в пакете sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "\n",
    "root = \"./data\"\n",
    "\n",
    "train_set = MNIST(\n",
    "    root=root, train=True, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "test_set = MNIST(\n",
    "    root=root, train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и приведём размерность к двумерной, чтобы это был набор векторов свойств."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set.data.numpy()\n",
    "y_train = train_set.targets.numpy()\n",
    "x_test = test_set.data.numpy()\n",
    "y_test = test_set.targets.numpy()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # normalize data to [0; 1]\n",
    "x_train_shape = x_train.shape\n",
    "print(\"Initial shape \", x_train_shape)\n",
    "x_train_flatten = x_train.reshape(\n",
    "    -1, x_train_shape[1] * x_train_shape[2]\n",
    ")  #  reshape to vector, 28*28 => 784\n",
    "print(\"Reshaped to \", x_train_flatten.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но графически:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "ax[0].imshow(x_train[0])\n",
    "ax[1].imshow(x_train_flatten[0].reshape(1, -1), aspect=50)\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[1].set_title(\"Flattened image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь заведём класс PCA и настроим его, чтобы он отобрал столько компонент, чтобы объяснялось 90% дисперсии. Обучим его и посмотрим, сколько ему потребовалось свойств, для описания каждой картинки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.90)\n",
    "x_train_encoded = pca.fit_transform(x_train_flatten)\n",
    "print(\"Encoded features \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер (он же декодер, ведь это просто обратная матрица от энкодера PCA) обучен. Теперь можно проверить, как он закодирует и раскодирует тестовую выборку. Для этого проведём такие же преобразования размерности для неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_shape = x_test.shape\n",
    "x_test_flat = x_test.reshape(-1, x_test_shape[1] * x_test_shape[2])\n",
    "x_test_encoded = pca.transform(x_test_flat)\n",
    "x_test_decoded = pca.inverse_transform(x_test_encoded).reshape(x_test_shape)\n",
    "print(\"x_test_decoded shape is \", x_test_decoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно определить функцию для отрисовки изображений MNIST. Она будет выводить несколько изображений в ряд, поэтому будет принимать трёхмерный массив. Шкала не должна быть автоподстраиваемой, так как после обработки изображения выйдут за диапазон $[0;1]$, в котором заданы исходные изображения. Мы зафиксируем шкалу в диапазоне $[0;1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imgs(imgs, title):\n",
    "    fig = plt.figure(figsize=(16, 3))\n",
    "    columns = imgs.shape[0]\n",
    "    rows = 1\n",
    "    for i in range(columns):\n",
    "        fig.add_subplot(rows, columns, i + 1)\n",
    "        plt.imshow(imgs[i], cmap=\"gray_r\", clim=(0, 1))\n",
    "    fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем исходное и закодированное-раскодированное изображение для некоторых объектов, которые мы случайным образом выберем из всей тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "sample_indices = np.random.choice(x_test.shape[0], 6)\n",
    "samples_orig = x_test[sample_indices]\n",
    "samples_decoded = x_test_decoded[sample_indices]\n",
    "plot_imgs(samples_orig, \"Original x_test\")\n",
    "plot_imgs(samples_decoded, \"PCA encoded-decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что `pca.n_components_` (87 для 90% PCA) достаточно для описания картинок MNIST вместо 784 исходных пикселей. Но при этом нужно хранить матрицу кодирования-декодирования, а изображения получаются немного зашумлёнными. Мы получили способ сжатия с потерями для рукописных цифр, где изображение центрировано и отмасштабировано по рамке из 28х28 пикселей (подробней смотрите правила базы MNIST).\n",
    "\n",
    "Степень сжатия у нас условно 87/784 ~= 0.11. То есть сжатие в 9 раз. «Условно», так как сжатое изображение хранится во float, а исходное в uint8, который требует в 4 раза меньше байт, плюс мы должны хранить матрицы для кодирования и декодирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим, как наш автоэнкодер без нейросетей справится с очисткой от зашумления. Для этого сделаем функцию добавления шумов к MNIST и посмотрим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util import random_noise\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x_test_noisy = random_noise(x_test, mode=\"gaussian\")\n",
    "samples_noisy = x_test_noisy[sample_indices]\n",
    "plot_imgs(samples_noisy, \"x_test with added noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно провести ту же операцию PCA энкодера и декодера, что выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCArecode(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1] * dataset.shape[2])\n",
    "    encoded = pca.transform(dataset_flat)\n",
    "    decoded = pca.inverse_transform(encoded).reshape(dataset.shape)\n",
    "    return decoded\n",
    "\n",
    "\n",
    "x_filtered = PCArecode(x_test_noisy)\n",
    "samples_filtered = x_filtered[sample_indices]\n",
    "plot_imgs(samples_filtered, \"PCA denoised x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты сравнения напишем функцию, которая будет строить зашумленные и восстановленные образцы друг под другом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(*args, invert_colors=True, digit_size=28, name=None, single_size=2):\n",
    "    args = [x.squeeze() for x in args]\n",
    "    n = min([x.shape[0] for x in args])\n",
    "    figure = np.zeros((digit_size * len(args), digit_size * n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(len(args)):\n",
    "            figure[\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "            ] = args[j][i].squeeze()\n",
    "\n",
    "    if invert_colors:\n",
    "        figure = 1 - figure\n",
    "\n",
    "    plt.figure(figsize=(single_size * n, single_size * len(args)))\n",
    "\n",
    "    plt.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "\n",
    "    plt.grid(False)\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if name is not None:\n",
    "        plt.savefig(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(samples_noisy, samples_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, шумы стали значительно меньше, но и артефакты вокруг цифр усилились. Это неудивительно, ведь мы сжимали информацию линейным образом всего в 87 компонент. Повышение уровня сжатия приведёт к еще большему количеству артефактов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Латентное представление цифр после PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим теперь на то, как разделяются изображения разных цифр в латентном представлении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_latent(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1] * dataset.shape[2])\n",
    "    return pca.transform(dataset_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_manifold(latent_r, labels=None, alpha=0.9, title=None):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    if labels is None:\n",
    "        plt.scatter(latent_r[:, 0], latent_r[:, 1], alpha=alpha)\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "    else:\n",
    "        plt.scatter(latent_r[:, 0], latent_r[:, 1], c=labels, cmap=\"tab10\", alpha=alpha)\n",
    "        plt.colorbar()\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим на двумерной плоскости первые две компоненты признаков, выделенных PCA — они объясняют максимум дисперсии исходных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_r = pca_latent(x_test)\n",
    "plot_manifold(latent_r, y_test, title=\"PCA manifold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что латентное представление слабо разделяет картинки по тому, какие цифры на них изображены. Только единицы расположены более-менее обособленно и плотно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Брать лишь первые две компоненты из 87 не совсем честно. Поэтому переведем признаки, полученные методом PCA, из 87-мерного пространства в 2-мерное методом UMAP и посмотрим, выделяются ли классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaled_latent_r = StandardScaler().fit_transform(latent_r)\n",
    "reducer = umap.UMAP()\n",
    "latent_r_2d = reducer.fit_transform(scaled_latent_r)\n",
    "plot_manifold(latent_r_2d, y_test, title=\"2D UMAP over PCA manifold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В латентном представлении цифры в среднем неплохо разделяются на отдельные кластеры, но многие отдельные изображения оказываются в \"чужих\" кластерах. Для такого простого датасета результат неудовлетворительный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, вспомним, что в автоэнкодере одна сеть переводит пространство свойств в пространство меньшей размерности, а другая сеть обучается восстанавливать исходные объекты. Вместо вычисления коэффициентов сети мы будем её обучать. Для обучения нужно определить функцию потерь. Обычно используют среднеквадратичное расстояние (MSE). То есть мы требуем, чтобы значения пикселей исходного изображения и восстановленного отличались несильно. В нашем примере, мы будем использовать  Binary Cross-Entropy, она обеспечивает лучшую сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/nn_encoder_nn_decoder.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем использовать любую сеть для энкодера и декодера: на полносвязных слоях или на свёрточных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно задать архитектуру модели. Мы будем использовать последовательную модель (Sequential) и свёрточную  архитектуру. В конце кодировщика должен быть вектор с размером `latent_dim`. И декодировщик должен принимать этот вектор и восстанавливать до целого изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim  # latent space size\n",
    "        hidden_dims = [32, 64, 128, 256, 512]  # num of filters in layers\n",
    "        modules = []\n",
    "        in_channels = 1  # initial value of channels\n",
    "        for h_dim in hidden_dims[:-1]:  # conv layers\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=in_channels,  # num of input channels\n",
    "                        out_channels=h_dim,  # num of output channels\n",
    "                        kernel_size=3,\n",
    "                        stride=2,  # convolution kernel step\n",
    "                        padding=1,  # save shape\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim  # changing number of input channels for next iteration\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=256, out_channels=512, kernel_size=1\n",
    "                ),  # changing the kernel size, because  size of the array (2*2)\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "        )\n",
    "        modules.append(nn.Flatten())  # to vector, size 512 * 2*2 = 2048\n",
    "        modules.append(nn.Linear(512 * 2 * 2, latent_dim))\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dims = [512, 256, 128, 64, 32]  # num of filters in layers\n",
    "        self.linear = nn.Linear(in_features=latent_dim, out_features=512)\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):  # define ConvTransopse layers\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels=hidden_dims[i],\n",
    "                        out_channels=hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels=hidden_dims[-1],\n",
    "                    out_channels=hidden_dims[-1],\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(\n",
    "                    in_channels=hidden_dims[-1],\n",
    "                    out_channels=1,\n",
    "                    kernel_size=7,\n",
    "                    padding=1,\n",
    "                ),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)  # from latents space to Linear\n",
    "        x = x.view(-1, 512, 1, 1)  # reshape\n",
    "        x = self.decoder(x)  # reconstruction\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем основную функцию для обучения нейросети. `single_pass_handler` и `loss_handler` будут меняться в зависимости от сети, которую мы обучаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "Function to train model, parameters:\n",
    "  encoder - encoder model\n",
    "  decoder - decoder model\n",
    "  loader - data loader\n",
    "  optimizer - optimizer\n",
    "  single_pass_handler - function for runing data through AE,\n",
    "                        returns latent representation\n",
    "                        and reconstructed image\n",
    "  loss_handler - loss function\n",
    "  epoch - number of current epoch, use for print log\n",
    "  log_interval - log printing interval\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    single_pass_handler,\n",
    "    loss_handler,\n",
    "    epoch,\n",
    "    log_interval=500,\n",
    "):\n",
    "    for batch_idx, (data, labels) in enumerate(loader):\n",
    "        batch_size = data.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        latent, recon = single_pass_handler(\n",
    "            encoder, decoder, data, labels\n",
    "        )  # latent vector and reconstructed image\n",
    "\n",
    "        loss = loss_handler(data, recon, latent)  # compute loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(loader.dataset),\n",
    "                    100.0 * batch_idx / len(loader),\n",
    "                ).ljust(40),\n",
    "                \"Loss: {:.6f}\".format(loss.item()),\n",
    "            )\n",
    "\n",
    "\n",
    "def ae_pass_handler(encoder, decoder, data, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    recon = decoder(latent)\n",
    "    return latent, recon\n",
    "\n",
    "\n",
    "def ae_loss_handler(data, recon, *args, **kwargs):\n",
    "    return F.binary_cross_entropy(recon, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим загрузчики наших данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим нашу нейросеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from itertools import chain\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2  # size of latent space\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на архитектуру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print(\">>> Encoder\")\n",
    "print(summary(encoder, (1, 28, 28)))\n",
    "print(\">>> Decoder\")\n",
    "print(summary(decoder, (1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим в течение 5 эпох:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    train(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, чтобы удобно прогонять датасет через обученную нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function returns results of run data through AE\n",
    "Parameters:\n",
    "  encoder - encoder model\n",
    "  decoder - decoder model\n",
    "  loader - data loader\n",
    "  single_pass_handler - function for runing data through AE,\n",
    "                        returns latent representation\n",
    "                        and reconstructed image\n",
    "  return_real - return original images, True/False, default = True\n",
    "  return_recon - return reconstructed images from decoder, True/False, default = True\n",
    "  return_latent - return latent representation from encoder, True/False, default = True\n",
    "  return_labels - return labels, True/False, default = True\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_eval(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    loader,\n",
    "    single_pass_handler,\n",
    "    return_real=True,\n",
    "    return_recon=True,\n",
    "    return_latent=True,\n",
    "    return_labels=True,\n",
    "):\n",
    "    if return_real:\n",
    "        real_list = []\n",
    "    if return_recon:\n",
    "        recon_list = []\n",
    "    if return_latent:\n",
    "        latent_list = []\n",
    "    if return_labels:\n",
    "        labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(loader):\n",
    "            if return_labels:\n",
    "                labels_list.append(labels.numpy())\n",
    "            if return_real:\n",
    "                real_list.append(data.numpy())\n",
    "\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            latent, recon = single_pass_handler(encoder, decoder, data, labels)\n",
    "\n",
    "            if return_latent:\n",
    "                latent_list.append(latent.cpu().numpy())\n",
    "            if return_recon:\n",
    "                recon_list.append(recon.cpu().numpy())\n",
    "\n",
    "    result = {}\n",
    "    if return_real:\n",
    "        real = np.concatenate(real_list)\n",
    "        result[\"real\"] = real.squeeze()\n",
    "    if return_latent:\n",
    "        latent = np.concatenate(latent_list)\n",
    "        result[\"latent\"] = latent\n",
    "    if return_recon:\n",
    "        recon = np.concatenate(recon_list)\n",
    "        result[\"recon\"] = recon.squeeze()\n",
    "    if return_labels:\n",
    "        labels = np.concatenate(labels_list)\n",
    "        result[\"labels\"] = labels\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала оценим то, как ведет себя наш автоэнкодер и как работает в целом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(run_res[\"real\"][0:9], run_res[\"recon\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, какое латетное представление он выучил."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res[\"latent\"], run_res[\"labels\"], title=\"AE manifold (latent_dim=2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь обучим автоэнкодер с латентным слоем размера 24 и посмотрим, как он будет бороться с шумом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем dataloader, который добавляет в наш датасет шум автоматически"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0.0, std=1.0):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"(mean={0}, std={1})\".format(\n",
    "            self.mean, self.std\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим MNIST с добавленным шумом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "test_noise_set = MNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor(), AddGaussianNoise(0.0, 0.30)]\n",
    "    ),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_noised_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_noise_set, list(range(64))),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(run_res[\"real\"][0:9], run_res[\"recon\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сжатия мы оценили визуально выше. Если обратить внимание, то можно заметить, что исходные картинки даже почистились от мелких шумов и странностей изображения и больше стали похожи на непрерывные линии. Размерность латентного пространства `latent_dim` равна 24, что значительно меньше исходного количества признаков (784), поэтому мы получили неплохое сжатие изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо понижения размерности и очистки данных от шума, автоэнкодеры имеют еще несколько полезных способов применения такие, как **обнаружение аномалий** и **предобучение на неразмеченных данных**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обнаружение аномалий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы имеем дело с задачей, когда у нас есть много данных, которые можно считать типичными, или \"нормальными\", и небольшое количество данных, являющихся нетипичными, или \"аномальными\". На практике, в нашем распоряжении может вообще не быть аномальных примеров, но мы можем ожидать, что они появятся в будущем, и мы бы хотели, чтобы модель могла отличить аномальные примеры от нормальных.\n",
    "\n",
    "В такой постановке можно **обучить автоэнкодер только на данных, которые мы считаем нормальными**. В тестовую выборку мы включим аномальные примеры, если они имеются, а также некоторое количество нормальных примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/ae_anomaly_detection_data.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать автоэнкодер только на нормальных данных (иллюстрация ниже, слева). При обучении автоэнкодер будет минимизировать ошибку между входом и выходом (*ошибка реконструкции*).\n",
    "\n",
    "Завершив обучение, мы вычисляем значения ошибки реконструкции на обучающих примерах, и строим распределение этих ошибок, по которому мы выбираем порог ошибки реконструкции (threshold). Порог выбирается исходя из задачи и требований к детекции. Он зависит от того, что для нас важнее: чаще находить аномалии или не допускать ложных срабатываний на \"нормальных\" объектах.\n",
    "\n",
    "На этапе применения модели (inference, иллюстрация ниже, справа) мы подаем на обученный автоэнкодер как нормальные, так и аномальные примеры. **Нормальные примеры будут восстанавливаться автоэнкодером с малой ошибкой**, так как подобные примеры встречались во время обучения. Аномальные же примеры не встречались автоэнкодеру во время обучения, и он не сможет их качественно восстанавливать. **Ошибка реконструкции аномальных объектов будет большой.**\n",
    "\n",
    "Сравнивая ошибку реконструкции с вычисленным ранее порогом, мы сможем обнаруживать и отделять аномальные примеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/ae_anomaly_detection_method.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобучение на неразмеченных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одним практическим примером использования автоэнкодеров является предобучение на неразмеченных данных. Пусть мы имеем **большое количество неразмеченных данных и немного размеченных** (semi-supervised подход).\n",
    "\n",
    "Мы можем обучить автоэнкодер на неразмеченных данных, ожидая, что он обучится эффективно представлять данные в латентном пространстве.\n",
    "\n",
    "Затем мы используем **энкодер как предобученный экстрактор признаков**, добавляя к нему дополнительный классификатор, и обучаем такую модель на размеченных данных.\n",
    "\n",
    "Такой подход похож на transfer learning, где мы тоже используем предобученный экстрактор признаков для решения задачи на небольшом количестве размеченных данных. Отличие в том, что **при transfer learning экстрактор признаков обучается на другой задаче с другими данными**, часто даже на данных из другого домена.\n",
    "\n",
    "В случае использования автоэнкодера, предобучение происходит на данных той же природы, и мы можем ожидать, что полученный таким образом экстрактор признаков будет более эффективно представлять данные в латентном пространстве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/ae_pretrain_encoder.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодер как генератор и его ограничения. Плавная интерполяция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоэнкодер переводит объекты из исходного признакового пространства в латентное пространство меньшей размерности. Мы можем попытаться использовать **обученный декодер как генератор новых данных**: он будет получать на вход некий вектор из латентного пространства и на выходе восстанавливать изображение. При этом мы можем ожидать, что это изображение окажется в некотором смысле похожим на то, на чем учился автоэнкодер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/autoencoder_as_generator.png\" alt=\"alttext\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какое значение вектора выбрать? Мы же никак не управляли латентным пространством. Непонятно, какие числа подставлять. Поэтому мы можем выбрать промежуточные значения между представлениями двух исходных изображений в латентном пространстве и получить плавную интерполяцию между изображениями. Постепенно свойства одного изображения будут исчезать, а свойства другого — появляться.\n",
    "\n",
    "Обучим сначала обычный автоэнкодер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=ae_pass_handler,\n",
    "        loss_handler=ae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем несколько изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space1 = encoder(imgs[labels == 7][0:1].to(device))\n",
    "latent_space2 = encoder(imgs[labels == 6][0:1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_steps = 10\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1.repeat(interp_steps, 1),\n",
    "    latent_space2.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(nrows=1, ncols=interp_steps, figsize=(16, 4))\n",
    "for label in range(0, interp_steps):\n",
    "    figure = iterp_imgs[label].cpu().detach().numpy()\n",
    "    figure = figure.reshape(28, 28)\n",
    "    ax = axs[label]\n",
    "    ax.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы увидеть более плавные изменения, можем сделать видео. Для этого можно использовать уже известный нам OpenCV. Он умеет делать видеофайлы из массивов чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "interp_steps = 200\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1.repeat(interp_steps, 1),\n",
    "    latent_space2.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "\n",
    "resize_coeff = 10\n",
    "imgs = np.squeeze(iterp_imgs.cpu().detach().numpy())\n",
    "size = (imgs.shape[1] * resize_coeff, imgs.shape[2] * resize_coeff)\n",
    "\n",
    "\n",
    "imgs = [\n",
    "    Image.fromarray(np.uint8(img * 255)).resize(size).convert(\"RGB\") for img in imgs\n",
    "]\n",
    "imgs[0].save(\n",
    "    \"ae_img.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    optimize=False,\n",
    "    duration=40,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as iImage\n",
    "\n",
    "iImage(open(\"ae_img.gif\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так себе интерполяция вышла. Старое изображение затухает, а новое изображение появляется. Хочется, чтобы в промежуточных кадрах не было каких-то непонятных очертаний, а изображение было чем-то промежуточным по смыслу между стартовым и конечным изображением.\n",
    "\n",
    "Причина неудачи в том, что в в результате обучения в латентном пространстве возникли зоны, которые умеют декодироваться в хорошие изображения. Но совсем не обязательно между этими зонами будет что-то адекватное (что мы видели из представления)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/autoencoder_as_generator_problem.png\" alt=\"alttext\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим это графически. Пусть наш очень умный, содержащий очень много коэффициентов автоэнкодер смог разложить все входные объекты на одной оси (размерность латентного пространства — 1). По сути он каждому входному изображению присвоил номер, и по номеру может это изображение вспомнить. То есть автоэнкодер очень переобученный. Тогда если мы возьмём промежуточный номер (пытаемся интерполировать), то какое изображение мы собираемся получить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/autoencoder_as_generator_problem_explanation.png\" alt=\"alttext\" width=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы хотим, чтобы декодированные промежуточные латентные состояния имели черты близких к ним объектов, то надо притянуть латентные координаты похожих объектов. Например, вот так:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/irregular_and_regular_latent_space.png\" alt=\"alttext\" width=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вариационные автоэнкодеры (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мотивация:\n",
    "\n",
    "Хотим вместо представления слева получить представление справа:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/latent_space_with_and_witout_regularization.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "При этом зоны пересечения должны действительно содержать переходные картины:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/vae_latent_space.png\" alt=\"alttext\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решение с помощью регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем попробовать заставить наши объекты «лежать» рядом — будем штрафовать латентные представления, которые далеко уходят от начала координат.\n",
    "\n",
    "Можем использовать как отдельно L1 или L2 регуляризацию, так и их комбинацию — elastic loss.\n",
    "\n",
    "Однако это приведет просто к масштабированию распределения. Нам надо одновременно получить связное латентное представление, чтобы у нас не возникало зон в латентном представлении, которым не соответствует ничего, и при этом представление, в котором цифры будут отделены друг от друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/reconstruction_loss_only.png\" alt=\"alttext\" width=\"450\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.jeremyjordan.me/variational-autoencoders/\">Variational Autoencoders</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если переход-интерполяция между объектами проходит через зону отсутствующих в обучении объектов, то их декодирование даст несуществующие в реальности объекты. Нам не удастся погенерировать новые картинки, преобразовывая случайную точку из латентного пространства в случайную картинку.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация вариационного автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи с автоэнкодером говорит нам, что существует некое пространство меньшей размерности $Z$, которое и обуславливает процесс генерации объектов из $X$. Все остальные различия — следствия случайности: один и тот же человек может по-разному нарисовать цифру 5.\n",
    "\n",
    "Будем искать латентное пространство $Z$, которое удовлетворяет следующему условию:\n",
    "\n",
    "$$\\large p(x) = \\int p(x, z)dz $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, пусть объекты из $Z$ легко генерировать.\n",
    "\n",
    "По формуле совместной вероятности:\n",
    "\n",
    "$$\\large p(x, z) = p(x|z)p(z) $$\n",
    "\n",
    "Осталось только подобрать такие параметры, чтобы все работало.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, сделать это в таком виде не получится. Пространство $X$ может быть высокоразмерным.\n",
    "\n",
    "Но мы можем существенно сузить область поиска, ведь каждому $x$ из пространства $X$ соответствует лишь небольшая возможная область в $Z$.\n",
    "\n",
    "Для этого будем также учить отображение из пространства $X$ в пространство $Z$, т. е. пытаться выучить $p(z|x)$. Назовем функцию, которой будем его приближать, $q(z|x)$.\n",
    "\n",
    "Что же в случае автоэнкодера выполняет роль $p(x|z)$ и $q(z|x)$?\n",
    "Очевидно, кодировщик и декодировщик соответственно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/vae_as_two_functions.png\" alt=\"alttext\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Чтобы все получилось, нужно сделать с кодировщиком две вещи. Заметьте, что декодировщик мы оставим без изменений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первая модификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть наш кодировщик генерирует на основе объекта $X$ вектор средних и вектор стандартных отклонений.\n",
    "\n",
    "Этих двух векторов хватает нам для того, чтобы задать многомерное нормальное распределение с независимыми компонентами (чтобы матрица ковариаций была диагональной), соответствующее данному объекту.\n",
    "\n",
    "Чтобы получить латентное представление объекта, отличающегося от $X$ только в силу случайности, нам достаточно сгенерировать вектор из нормального распределения с такими параметрами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/vae_architecture_first_modification.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы можем требовать, чтобы из полученного латентного представления декодировшик восстанавливал объект, похожий на исходный."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparametrization trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь, однако, сразу возникает проблема с тем, что граф вычислений, соответствующий предыдущей структуре, не может пропускать градиент — как пропустить градиент через генератор случайного нормального числа? Если считать из определения, то даже малейшему изменению параметра могут соответствовать бесконечные изменения генерируемого числа (нормальное распределение определено на бесконечности).\n",
    "\n",
    "Но мы можем вспомнить замечательное свойство одномерного нормального распределения:\n",
    "\n",
    "$$\\large N(\\mu,\\sigma^2) = N(0,1) * \\sigma + \\mu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполняется это и для многомерного случая. Потому сделаем следующее: будем генерировать значение из нормального распределения со средними 0 и дисперсиями 1, а затем домножать это на вектор стандартных отклонений и прибавлять вектор средних. Получится вот такое преобразование, которое называется **reparametrization trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/reparametrization_trick.png\" alt=\"alttext\" width=\"850\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от левого случая, в правом мы спокойно можем пропустить градиент через детерминистичные ноды.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но такой принцип также имеет проблему предыдущего детерминистического подхода, так как вероятностное распределение сможет свернуться в дельта-функцию — зачем нейросети мучиться с объектами, немного отличающимися от тех, что есть в обучающей выборке, и пытаться нормально их восстанавливать, если можно просто начать генерировать стандартные отклонения, близкие к нулю, и тем самым получить $\\delta$-функцию, которая будет нашему объекту всегда сопоставлять одну точку в латентном представлении.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/Dirac_function_approximation.gif\" alt=\"alttext\" width=\"240\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вторая модификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому нам надо ввести регуляризацию, требующую от каждого распределения быть близким к нормальному распределению вокруг нуля координат латентного пространства с дисперсией 1 (наше $P(z)$).\n",
    "\n",
    "Для этого нам нужна некая мера расстояния между двумя вероятностными распределениями. В базовом случае в качестве такой меры расстояния используется **дивергенция Кульбака-Лейблера**, или KL-дивергенция."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дивергенция Кульбака-Лейблера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Дивергенция Кульбака-Лейблера](https://ru.wikipedia.org/wiki/Расстояние_Кульбака_—_Лейблера) между двумя вероятностными распределениями $P$ и $Q$ определяется следующим образом:\n",
    "\n",
    "$$\\large KL(P||Q) = \\int_X p(x)\\log \\dfrac {p(x)} {q(x)} dx$$\n",
    "\n",
    "В теории информации $p$ считается целевым (истинным) распределением, а $q$ — тем, с которым мы его сравниваем (проверяемым).\n",
    "Важно понимать, что $KL$ не является мерой расстояния, т.к. в общем случае\n",
    "\n",
    "$$\\large KL(P||Q) \\neq KL(Q||P)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Только KL-дивергенция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы распределение $Q(z|x)$ в латентном пространстве $Z$ походило на нормальное, мы будем минимизировать KL-дивергенцию между ним и стандартным нормальным распределением $N(0,1)$.\n",
    "\n",
    "$$\\large Loss =  KL(Q(z|x)||N(0,1)) $$\n",
    "\n",
    "Данное выражение может быть [записано аналитически](https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes):\n",
    "\n",
    "\n",
    "$$\\large KL(N(\\mu, \\sigma) || N(0, 1)) = -\\frac {1} {2}(\\log {\\sigma^2} - \\sigma^2 - \\mu^2 + 1)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/pure_kl_loss.png\" alt=\"alttext\" width=\"350\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.jeremyjordan.me/variational-autoencoders/\">Variational Autoencoders</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что мы забываем про декодировщик — он может выдавать все, что угодно. Потому логично ожидать, что обучится только кодировщик, и обучится он отражать наши точки в нормальное распределение со средним 0 и дисперсией 1. Можем проверить это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class VAEEncoder(Encoder):\n",
    "    def __init__(self, latent_dim):\n",
    "        if latent_dim % 2 != 0:  # check for the parity of the latent space\n",
    "            raise Exception(\"Latent size for VAEEncoder must be even\")\n",
    "        super().__init__(latent_dim)\n",
    "\n",
    "\n",
    "def vae_split(latent):\n",
    "    size = latent.shape[1] // 2  # divide the latent representation into mu and log_var\n",
    "    mu = latent[:, :size]\n",
    "    log_var = latent[:, size:]\n",
    "    return mu, log_var\n",
    "\n",
    "\n",
    "def vae_reparametrize(mu, log_var):\n",
    "    sigma = torch.exp(0.5 * log_var)\n",
    "    eps = torch.randn(mu.shape[0], mu.shape[1]).to(device)\n",
    "    return eps * sigma + mu\n",
    "\n",
    "\n",
    "def vae_pass_handler(encoder, decoder, data, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    mu, log_var = vae_split(latent)\n",
    "    sample = vae_reparametrize(mu, log_var)\n",
    "    recon = decoder(sample)\n",
    "    return latent, recon\n",
    "\n",
    "\n",
    "def kld_loss(mu, log_var):\n",
    "    var = log_var.exp()\n",
    "    kl_loss = torch.mean(-0.5 * torch.sum(log_var - var - mu**2 + 1, dim=1), dim=0)\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "def kl_loss_handler(data, recon, latent, kld_weight=0.1, *args, **kwargs):\n",
    "    mu, log_var = vae_split(latent)\n",
    "    kl_loss = kld_loss(mu, log_var)\n",
    "    return kld_weight * kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 3):\n",
    "    train(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=kl_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae_split(run_res[\"latent\"])\n",
    "var = np.exp(log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все генерируемые средние почти неотличимы от нуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mu.ravel())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все генерируемые дисперсии почти неотличимы от 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var.ravel());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получили практически неразделимые объекты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "pal = sns.color_palette(\"Paired\", n_colors=10)\n",
    "plot_manifold(mu, run_res[\"labels\"], title=\"Manifold mu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещаем ошибку восстановления и KL-дивергению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы должны сохранить исходный loss — декодировщик штрафуется за то, что не может нормально реконструировать объект.\n",
    "\n",
    "Формально это записывается следующим образом:\n",
    "\n",
    "$$\\large vae\\_loss = E_{z \\sim Q(z|x)}[logP(x|z)] + KL[Q(z|x)||N(0,1)]$$\n",
    "\n",
    "А в итоге:\n",
    "\n",
    "$$\\large vae\\_loss = BCE(x , \\tilde{x}) -\\frac {1} {2}(\\log {\\sigma^2} - \\sigma^2 - \\mu^2 + 1)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая компонента осталась без изменений, а первая — это красиво записанное требование корректно восстанавливать объекты из обучающей выборки, чтобы при этом объекты, полученные их небольшим изменением за счет случайности, также восстанавливались в объекты, близкие к объектам из тренировочной выборки. И удовлетворять этой компоненте loss мы можем за счет того же loss, который использовали в обычном автоэнкодере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учет обеих компонент позволяет нам получить то, что мы хотели — непрерывное пространство, где нет «дыр» в представлении, и при этом близкие по смыслу объекты расположены рядом, а далекие — далеко."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/kl_repr_loss.png\" alt=\"alttext\" width=\"350\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.jeremyjordan.me/variational-autoencoders/\">Variational Autoencoders</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем наш новый loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_handler(data, recon, latent, kld_weight=0.005, *args, **kwargs):\n",
    "    mu, log_var = vae_split(latent)\n",
    "    kl_loss = kld_loss(mu, log_var)\n",
    "    # add bce loss(reconstruction)\n",
    "    loss = F.binary_cross_entropy(recon, data) + kld_weight * kl_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим наш VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae_split(run_res[\"latent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette(\"Paired\", n_colors=10)\n",
    "plot_manifold(mu, run_res[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что цифры разделились в пространстве, но при этом жмутся друг к другу. При этом, что интересно, 4 и 9 почти неотличимы. Это можно объяснить тем, что двух компонент недостаточно, чтобы разделить настолько похожие цифры (по сути, все отличие в заполненности области между двумя рожками 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как теперь получится интерполировать между 7 и 6. Для сравнения с результатом, полученным обычным автоэнкодером, возьмем latent space такого же размера, как у него (24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 24\n",
    "learning_rate = 1e-4\n",
    "\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=vae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "latent_space1_mu, _ = vae_split(encoder(imgs[labels == 7][0:1].to(device)))\n",
    "latent_space2_mu, _ = vae_split(encoder(imgs[labels == 6][0:1].to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_steps = 10\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1_mu.repeat(interp_steps, 1),\n",
    "    latent_space2_mu.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "_, axs = plt.subplots(nrows=1, ncols=interp_steps, figsize=(16, 4))\n",
    "for label in range(0, interp_steps):\n",
    "    figure = iterp_imgs[label].cpu().detach().numpy()\n",
    "    figure = figure.reshape(28, 28)\n",
    "    ax = axs[label]\n",
    "    ax.imshow(figure, cmap=\"Greys_r\", clim=(0, 1))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим плавную интерполяцию. Посмотрим на примере с видео."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "interp_steps = 200\n",
    "weight = torch.linspace(0, 1, steps=interp_steps)\n",
    "interp = torch.lerp(\n",
    "    latent_space1_mu.repeat(interp_steps, 1),\n",
    "    latent_space2_mu.repeat(interp_steps, 1),\n",
    "    weight=weight.view(-1, 1).to(device),\n",
    ")\n",
    "iterp_imgs = decoder(interp)\n",
    "\n",
    "\n",
    "resize_coeff = 10\n",
    "imgs = np.squeeze(iterp_imgs.cpu().detach().numpy())\n",
    "size = (imgs.shape[1] * resize_coeff, imgs.shape[2] * resize_coeff)\n",
    "\n",
    "\n",
    "imgs = [\n",
    "    Image.fromarray(np.uint8(img * 255)).resize(size).convert(\"RGB\") for img in imgs\n",
    "]\n",
    "imgs[0].save(\n",
    "    \"vae_img.gif\",\n",
    "    save_all=True,\n",
    "    append_images=imgs[1:],\n",
    "    optimize=False,\n",
    "    duration=40,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as iImage\n",
    "\n",
    "iImage(open(\"vae_img.gif\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все переходы понятны, и в процессе не возникает невозможных цифр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы используем размерность латентного пространства 2, то это позволит нам получать распределение классов цифр на плоскости, типа такого:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/vae_sampling.png\" alt=\"alttext\" width=\"350\">\n",
    "\n",
    "<em>Source: <a href=\"https://habr.com/ru/articles/331552/\">Автоэнкодеры в Keras, Часть 3: Вариационные автоэнкодеры (VAE)</a></em>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Это не просто интерполяция по двум направлениям. Тут именно все 10 цифр должны так занять место на плоскости, чтобы плавно перетекать друг в друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим наконец, что вариационный автоэнкодер работает как автоэнкодер и может, к примеру, убирать шум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_loader, vae_pass_handler)\n",
    "plot_samples(run_res[\"real\"][0:9], run_res[\"recon\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что VAE работает. Пусть не лучше, чем обычный автоэнкодер, возможно, даже хуже. Можно добиться улучшения его работы, поставив меньший вес KL-дивергенции и увеличив латентное пространство."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично посмотрим, как он восстанавливает изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, vae_pass_handler)\n",
    "plot_samples(run_res[\"real\"][0:9], run_res[\"recon\"][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает, но изображения получаются \"размытыми\". Это следствие сэмплирования из нормального распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторная арифметика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе, можно даже в латентном пространстве брать разницу черт написания двух одинаковых цифр, прибавлять к другой цифре, получая в результате цифру, написанную немного по-другому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Такое можно делать и для других примеров — добавлять людям на изображении очки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/vector_arithmetic_add_new_property.png\" alt=\"alttext\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "или получать нечто среднее между двумя объектами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/vector_arithmetic_get_half_property.png\" alt=\"alttext\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее:\n",
    "\n",
    "У нас есть 1, написанная без наклона, и 1, написанная с наклоном.\n",
    "И у нас есть 9 без наклона.\n",
    "\n",
    "Вычитаем из девятки единицу без наклона и прибавляем единицу с наклоном. Если все пройдет хорошо, получим девятку с наклоном.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/vector_arithmetic_example.png\" alt=\"alttext\" width=\"550\">\n",
    "\n",
    "<em>Source: <a href=\"https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/#:~:text=First%2C%20let%E2%80%99s%20get,the%20autoencoded%20version\">Generating Large Images from Latent Vectors</a></em>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем это сделать сами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "real_9_straight = imgs[labels == 9][6:7]  # find some straight \"nine\"\n",
    "real_1_straight = imgs[labels == 1][3:4]  # find some straight \"one\"\n",
    "real_1_tilted = imgs[labels == 1][0:1]    # find some tilted \"one\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "Image.fromarray(np.uint8(np.squeeze(real_9_straight.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(real_1_straight.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(real_1_tilted.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_9_straight, _ = vae_split(encoder(real_9_straight.to(device)))\n",
    "latent_1_straight, _ = vae_split(encoder(real_1_straight.to(device)))\n",
    "latent_1_tilted, _ = vae_split(encoder(real_1_tilted.to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_9_tilted = latent_9_straight - latent_1_straight + latent_1_tilted\n",
    "gen_9_tilted = decoder(latent_9_tilted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.squeeze(gen_9_tilted.cpu().detach().numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалось перенести наклон единицы на девятку. Получилось неплохо, однако простой VAE не гарантирует того, что \"фокус удастся\". Для получения возможности использовать векторную арифметику могут применяться специальные функции потерь и архитектуры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблемы  «ванильного» VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из проблем VAE, с которой можно столкнуться, состоит в том, что две компоненты функции потерь конфликтуют друг с другом. Если будет доминировать KL-loss, то мы получим представление, из которого наши объекты очень плохо восстанавливаются — они раскиданы по представлению, как угодно.\n",
    "\n",
    "Если же, наоборот, будет доминировать reconstruction loss, то мы получим ситуацию, в которой объекты восстанавливаются нормально, однако в латентном пространстве есть пустоты.\n",
    "\n",
    "Проблема возникает и с самой KL-дивергенцией, у которой есть ряд существенных недостатков. Есть другие способы оценки близости двух распределений, которые порой дают лучшие результаты. К ним относится дивергенция Йенсена — Шеннона, которую мы вскользь затронем далее, и метрика Вассерштейна (используется в Wasserstein autoencoders), изучение которой выходит за рамки курса.\n",
    "\n",
    "Кроме того, в случае, когда декодировщик содержит значительно больше параметров, нежели кодировщик, может возникать ситуация, при которой сгенерированное латентное представление игнорируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодеры с условием (CAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мотивация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как, используя обычный VAE, сгенерировать картинку с заданной меткой?\n",
    "\n",
    "На самом деле, задача нетривиальна. Как вариант, мы можем понять, в какую область латентного пространства VAE отображает все 0, и затем сэмплировать уже из этой области.\n",
    "\n",
    "Хорошо, а если мы хотим нарисовать единицу тем же почерком, которым нарисована данная нам тройка? В этом случае классический VAE вообще не получится использовать.\n",
    "\n",
    "Есть еще одна проблема. Если распределение объектов действительно сильно зависит от какой-то дополнительной информации, например, того, какую цифру хотел изобразить человек, то KL-loss будет пытаться «скрестить ежа с ужом», и в результате мы получим очень странное представление, а на границах могут получаться несуществующие в реальности мутанты (если внимательно посмотрите на предыдущую картинку — так и получается)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Несвязные компоненты и автокодировщик с условиями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрировать мы это можем на модельной задаче. Сгенерируем два несвязных набора точек в двумерном пространстве, каждый из которых представляет собой некий паттерн с добавленным шумом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# create dataset\n",
    "x1 = np.linspace(-2.2, 2.2, 2000)\n",
    "fx = np.sin(x1)\n",
    "dots1 = np.vstack([x1, fx]).T\n",
    "\n",
    "t = np.linspace(0, 2 * np.pi, num=2000)\n",
    "dots2 = 0.5 * np.array([np.sin(t), np.cos(t)]).T + np.array([1.5, -0.5])[None, :]\n",
    "\n",
    "dots = np.vstack([dots1, dots2])\n",
    "noise = 0.06 * np.random.randn(*dots.shape)\n",
    "\n",
    "labels = np.array([0] * x1.shape[0] + [1] * t.shape[0])\n",
    "noised = dots + noise\n",
    "\n",
    "\n",
    "# Visualization\n",
    "colors = [\"b\"] * x1.shape[0] + [\"g\"] * t.shape[0]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlim([-2.5, 2.5])\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем простой автоэнкодер на полносвязных слоях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлых примерах мы этим пренебрегали, но автоэнкодер тоже может переобучаться. Поэтому сделаем разбиение на обучение и тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(noised, test_size=0.25, random_state=42)\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "x_test = torch.from_numpy(x_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сильно не мучаться, поставим просто scheduler, который автоматически уменьшает learning rate нашей сети, если она переобучается или просто не улучшает качество на валидационном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encdec = SimpleEncoderDecoder()\n",
    "optimizer = optim.Adam(encdec.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \"min\", patience=50\n",
    ")  # to optimize learning rate\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5000)):\n",
    "    optimizer.zero_grad()\n",
    "    x_restored = encdec(x_train)\n",
    "    loss = criterion(x_train, x_restored)\n",
    "    loss.backward()\n",
    "    if optimizer.param_groups[0][\"lr\"] < 10e-7:  # if learning step becomes too small\n",
    "        print(epoch)\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_restored = encdec(x_test)\n",
    "        val_loss = criterion(x_test, x_restored)\n",
    "    scheduler.step(val_loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_restored = encdec(x_test)\n",
    "    dots_restored = x_restored.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что выучил автоэнкодер. Видим, что он показывает связь там, где она явно отсутствует. Требование получить одно и то же представление для точек из двух паттернов мешает автоэнкодеру нормально выучить эти паттерны — они получаются смазанными или даже неверными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.scatter(dots_restored[:, 0], dots_restored[:, 1], color=\"grey\", linewidth=4)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что наш автоэнкодер восстанавливает часть объектов в область, где ничего нет. Потому что у него нет возможности понять, что это две несвязные компоненты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что будет, если мы будем передавать в кодировщик и в декодировщик метку объекта?\n",
    "Тогда окажется, что наш автокодировщик работает в разы лучше:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConditionalEncoderDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat(\n",
    "            [x, y.view(-1, 1)], dim=1\n",
    "        )  # combine the labels with X, change the dimension of the labels\n",
    "        z = self.encoder(x)\n",
    "        x = torch.cat([z, y.view(-1, 1)], dim=1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    noised, labels, test_size=0.25, random_state=42\n",
    ")\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "encdec = SimpleConditionalEncoderDecoder()\n",
    "optimizer = optim.Adam(encdec.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=50)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(5000)):\n",
    "    optimizer.zero_grad()\n",
    "    x_restored = encdec(x_train, y_train)\n",
    "    loss = criterion(x_train, x_restored)\n",
    "    loss.backward()\n",
    "    if optimizer.param_groups[0][\"lr\"] < 10e-7:\n",
    "        print(epoch)\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_restored = encdec(x_test, y_test)\n",
    "        val_loss = criterion(x_test, x_restored)\n",
    "    scheduler.step(val_loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    X_restored = encdec(x_test, y_test)\n",
    "    dots_restored = X_restored.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dots1[:, 0], dots1[:, 1], color=\"red\", linewidth=4)\n",
    "plt.plot(dots2[:, 0], dots2[:, 1], color=\"yellow\", linewidth=4)\n",
    "plt.scatter(noised[:, 0], noised[:, 1], c=colors)\n",
    "plt.scatter(dots_restored[:, 0], dots_restored[:, 1], color=\"grey\", linewidth=4)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ситуация стала лучше. То, что мы применили, называется условными автоэнкодерами (Conditional AE). Конкретно — вместе с признаковым описанием объекта мы также передаем метки, которые указывают на то, что он относится к каким-то важным группам объектов, для которых, возможно, сети нужно учить отличное от других представление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Условные вариационные автоэнкодеры (CVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация вариационного автоэнкодера с условиями, CVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычные автокодировщики с условиями применяются редко, так как они по-прежнему не гарантируют нам связность представления в пределах одной метки.\n",
    "\n",
    "Однако добавление меток в вариационный автокодировщик часто помогает решать уже описанные задачи на хорошем уровне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/cvae_scheme.png\" alt=\"alttext\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как подмешивать метку к изображению, чтобы передать это полностью свёрточной нейронной сети, не очевидно и обычно не нужно. Часто достаточно передавать метку только декодеру. Энкодер имеет в распоряжении изначальный объект и при желании может предсказать его метку сам.\n",
    "\n",
    "Напишем код для CVAE. По сути надо поменять только декодер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dims = [512, 256, 128, 64, 32]\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=latent_dim + 10,  # add +10(num of labels) to latent space\n",
    "            out_features=hidden_dims[0],\n",
    "        )\n",
    "\n",
    "        modules = []\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        hidden_dims[i],\n",
    "                        hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    hidden_dims[-1],\n",
    "                    hidden_dims[-1],\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Conv2d(hidden_dims[-1], out_channels=1, kernel_size=7, padding=1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x, lab):\n",
    "        x = torch.cat([x, lab], dim=1)  # concatenate latent vector and label\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, 512, 1, 1)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_pass_handler(encoder, decoder, data, label, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    mu, log_var = vae_split(latent)\n",
    "    sample = vae_reparametrize(mu, log_var)\n",
    "    label = torch.nn.functional.one_hot(label, num_classes=10)  # labels to ohe\n",
    "    recon = decoder(sample, label)\n",
    "    return latent, recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "learning_rate = 1e-2\n",
    "encoder = VAEEncoder(latent_dim=latent_dim * 2)\n",
    "decoder = CDecoder(latent_dim=latent_dim)\n",
    "\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    chain(encoder.parameters(), decoder.parameters()), lr=learning_rate\n",
    ")\n",
    "\n",
    "for i in range(1, 6):\n",
    "    train(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        loader=train_loader,\n",
    "        epoch=i,\n",
    "        single_pass_handler=cvae_pass_handler,\n",
    "        loss_handler=vae_loss_handler,\n",
    "        log_interval=450,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, cvae_pass_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавив передачу метки в декодер, мы позволили автоэнкодеру отображать все цифры в \"одно место\". За счет этого ему легче стало учиться, loss стал чуть ниже.\n",
    "\n",
    "При этом, если нарисовать латентное представление для всех наших цифр разом, получится комок, сосредоточенный в области нормального распределения. Это не значит, что оно плохое. Просто наша картина не учитывает, что нейросеть различает цифры теперь по меткам, а не по латентному представлению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res[\"latent\"], run_res[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у каждой цифры \"свое\" нормальное распределение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res[\"latent\"][run_res[\"labels\"] == 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res[\"latent\"][run_res[\"labels\"] == 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация заданных цифр из латентного распределения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядит наше латентное представление, скажем, для четверок, которых мы до этого почти не видели (сливались с 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20\n",
    "space1 = torch.linspace(-2, 2, steps)\n",
    "space2 = torch.linspace(-2, 2, steps)\n",
    "grid = torch.cartesian_prod(space1, space2)\n",
    "label = torch.full((grid.shape[0],), 4)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)\n",
    "with torch.no_grad():\n",
    "    imgs = decoder(grid.to(device), label.to(device))\n",
    "    imgs = imgs.cpu().numpy().squeeze()\n",
    "\n",
    "plot_samples(\n",
    "    *[imgs[x : x + steps] for x in range(0, steps * steps, steps)], single_size=0.35\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, как у нас четверки плавно расположены по стилю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20\n",
    "space1 = torch.linspace(-2, 2, steps)\n",
    "space2 = torch.linspace(-2, 2, steps)\n",
    "grid = torch.cartesian_prod(space1, space2)\n",
    "label = torch.full((grid.shape[0],), 9)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)\n",
    "with torch.no_grad():\n",
    "    imgs = decoder(grid.to(device), label.to(device))\n",
    "    imgs = imgs.cpu().numpy().squeeze()\n",
    "\n",
    "plot_samples(\n",
    "    *[imgs[x : x + steps] for x in range(0, steps * steps, steps)], single_size=0.35\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При желании можно посмотреть на процесс того, как нейросеть учит такие латентные представления.\n",
    "\n",
    "Ниже показано, как нейросеть выполняет задачу генерации заданной цифры по мере обучения и как выглядит латентное представление объектов, относящихся к данной цифре (для примера взята семерка)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/gen_cvae_7.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/lat_cvae_7.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "<em>Source: <a href=\"https://habr.com/ru/articles/331664/\">Автоэнкодеры в Keras, Часть 4: Conditional VAE</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация заданных цифр с переносом стиля"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также успешно такая нейросеть справится в задаче, где мы используем латентное представление одной цифры  для того, чтобы сгенерировать цифру с таким же стилем написания.\n",
    "\n",
    "Чтобы сделать это, достаточно просто получить латентное представление для 7, а затем передать его в декодер с меткой 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/generation_indicated_digits_from_latent_space.png\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты переноса стилей для нескольких разных 7 представлены ниже.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L12/style_transfer_7.png\" alt=\"alttext\" width=\"500\">\n",
    "\n",
    "<em>Source: <a href=\"https://habr.com/ru/articles/331664/\">Автоэнкодеры в Keras, Часть 4: Conditional VAE</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем то же самое для наших двоек и пятерок.\n",
    "Выберем двойку и сгенерим несколько 5 с ее стилем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(test_loader))\n",
    "real = imgs[labels == 2][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "Image.fromarray(np.uint8(np.squeeze(real.numpy()) * 255)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "mu, log_var = vae_split(encoder(real.to(device)))\n",
    "sigma = torch.exp(0.5 * log_var)\n",
    "z = torch.randn(sample_size, mu.shape[1]).to(device)\n",
    "latent = z * sigma + mu\n",
    "\n",
    "label = torch.full((sample_size,), 5)\n",
    "label = torch.nn.functional.one_hot(label, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = decoder(latent.to(device), label.to(device))\n",
    "    imgs = np.squeeze(imgs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение (disentangling) стиля и метки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В CVAE мы полагаемся на то, что если декодер получает метку класса извне, то информация о метке не кодируется в латентном пространстве.\n",
    "\n",
    "Можно явно внести в CVAE дополнительное требование, чтобы информация о метке класса \"не утекала\" в латентное пространство. Для этого можно добавить дополнительный классификатор, который будет получать на вход вектор из латентного пространства и пытаться по нему предсказать класс объекта.\n",
    "\n",
    "Если **классификатор будет хорошо справляться с этой задачей**, то это значит, что информация о классе утекла в латентное пространство, и **за это мы должны штрафовать энкодер**.\n",
    "\n",
    "Таким образом, энкодер будет вынужден кодировать в латентное пространство другие абстрактные признаки, вроде стиля, но не метку класса, и тогда перенос стиля будет работать еще точнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L12/out/disentangling_style_and_label.png\" alt=\"alttext\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Полезные материалы</font>\n",
    "\n",
    "<font size = \"5\">Representation learning</font>\n",
    "\n",
    "1. [Representation Learning: A Review and New Perspectives (Yoshua Bengio et al., 2012)](https://arxiv.org/abs/1206.5538)\n",
    "\n",
    "2. [A Few Words on Representation Learning (Silva, 2021)](https://sthalles.github.io/a-few-words-on-representation-learning/)\n",
    "\n",
    "3. [Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "<font size = \"5\">Metric learning</font>\n",
    "\n",
    "1. [FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff et al., 2015)](https://arxiv.org/abs/1503.03832)\n",
    "\n",
    "2. [One-Shot Learning for Face Recognition](https://machinelearningmastery.com/one-shot-learning-with-siamese-networks-contrastive-and-triplet-loss-for-face-recognition/)\n",
    "\n",
    "3. [Habr: Ищем знакомые лица](https://habr.com/ru/post/317798/)\n",
    "\n",
    "4. [Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "5. [Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](https://www.researchgate.net/publication/4246277_Dimensionality_Reduction_by_Learning_an_Invariant_Mapping)\n",
    "\n",
    "\n",
    "<font size=\"5\">Про unsupervised learning при помощи нейросетей</font>\n",
    "\n",
    "Главы из учебника Гудфеллоу по теме:\n",
    "1. [Representation learning](https://www.deeplearningbook.org/contents/representation.html)\n",
    "2. [Генеративные модели](https://www.deeplearningbook.org/contents/generative_models.html)\n",
    "\n",
    "Про все увеличивающуюся роль unsupervised learning:\n",
    "[Unsupervised Deep Learning - Google DeepMind & Facebook Artificial Intelligence NeurIPS 2018](https://www.youtube.com/watch?v=rjZCjosEFpI)\n",
    "\n",
    "[Лекция по генеративным моделям](https://www.youtube.com/watch?v=5WoItGTWV54)\n",
    "\n",
    "[Проклятье размерности для классификации](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)\n",
    "\n",
    "<font size=\"5\">Автоэнкодеры</font>\n",
    "\n",
    "[Главы из учебника Гудфеллоу по теме](https://www.deeplearningbook.org/contents/autoencoders.html)\n",
    "\n",
    "\n",
    "\n",
    "[Более подробно про PCA и ссылка на его применение для MNIST](https://ryanwingate.com/intro-to-machine-learning/unsupervised/pca-on-mnist/)\n",
    "\n",
    "[Способы понижения размерности, PCA и разные типы автокодировщиков, лекция Техносферы](https://www.youtube.com/watch?v=W5JLSKcuaQo)\n",
    "\n",
    "[Eigenfaces](https://ieeexplore.ieee.org/document/139758)\n",
    "\n",
    "\n",
    "Удаление шума из\n",
    "1. [Изображений](https://debuggercafe.com/autoencoder-neural-network-application-to-image-denoising/)\n",
    "2. [Текста](https://debuggercafe.com/denoising-text-image-documents-using-autoencoders/)\n",
    "\n",
    "[Введение в автоэнкодеры на kaggle](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)\n",
    "\n",
    "<font size=\"5\">Вариационные автоэнкодеры</font>\n",
    "\n",
    "[Understanding Variational Autoencoders (VAEs)](https://machinelearningmastery.ru/understanding-variational-autoencoders-vaes-f70510919f73/#)\n",
    "\n",
    "Введение в автоэнкодеры на Хабре\n",
    "1. [Введение](https://habr.com/ru/post/331382/)\n",
    "2. [Manifold learning и скрытые (latent) переменные](https://habr.com/ru/post/331500/)\n",
    "3. [Вариационные автоэнкодеры (VAE)](https://habr.com/ru/post/331552/)\n",
    "4. [Conditional VAE](https://habr.com/ru/post/331664/)\n",
    "5. [GAN(Generative Adversarial Networks)](https://habr.com/ru/post/332000/)\n",
    "6. [GAN + VAE](https://habr.com/ru/post/332074/)\n",
    "\n",
    "\n",
    "[Оригинальная статья по VAE](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "[[video] 📺 Ali Ghodsi, Лекция по VAE](https://www.youtube.com/watch?v=uaaqyVS9-rM)\n",
    "\n",
    "[Jeremy Jordan, введение в автоэнкодеры](https://www.jeremyjordan.me/autoencoders/)\n",
    "\n",
    "[Jeremy Jordan, вариационные автоэнкодеры](https://www.jeremyjordan.me/variational-autoencoders/)\n",
    "\n",
    "[[arxiv] 🎓 Туториал по VAE](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "\n",
    "[Еще одно введение в вариационные автоэнкодеры](https://livebook.manning.com/book/deep-learning-with-python/chapter-8/)\n",
    "\n",
    "[Туториал по VAE от Google по tensorflow](https://www.tensorflow.org/tutorials/generative/cvae)\n",
    "\n",
    "[Векторная арифметика в VAE при генерации изображений](https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)\n",
    "\n",
    "[Генерация анимированных персонажей](https://mlexplained.wordpress.com/category/generative-models/vae/)\n",
    "\n",
    "[VAE на PyTorch с пояснениями](https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/)\n",
    "\n",
    "\n",
    "[Введение в условные вариационные автоэнкодеры](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)\n",
    "\n",
    "[[git] 🐾 Репозиторий с различными модификациями вариационных автоэнкодеров](https://github.com/AntixK/PyTorch-VAE)\n",
    "\n",
    "\n",
    "<font size=\"5\">KL-дивергенция</font>\n",
    "\n",
    "[Википедия по дивергенции Кульбака-Лейблера](https://ru.wikipedia.org/wiki/Расстояние_Кульбака_—_Лейблера)\n",
    "[Мотивация KL-дивергенции](https://math.stackexchange.com/questions/90537/what-is-the-motivation-of-the-kullback-leibler-divergence)\n",
    "\n",
    "Объяснение проблем и разницы между KL-дивергенцией, дивергенцией Йенсена-Шеннона и расстоянием Вассерштейна:\n",
    "1. [Часть 1, проблемы KL-дивергенции. Дивергенция Йенсена-Шеннона](https://www.youtube.com/watch?v=_z9bdayg8ZI) и\n",
    "2. [Часть 2, проблемы KL-дивергенции и дивергенции Йенсена-Шеннона. Расстояние Вассерштейна](https://www.youtube.com/watch?v=y8LGAhzCOxQ)\n",
    "\n",
    "\n",
    "<font size=\"5\">AAE</font>\n",
    "\n",
    "[[git] 🐾 Примеры AAE на mlxnet](https://github.com/nicklhy/AdversarialAutoEncoder)\n",
    "\n",
    "[Здесь в 6 и 8 лекции тоже можно найти примеры](https://github.com/che-shr-cat/deep-learning-for-biology-hse-2019-course)\n",
    "\n",
    "<font size=\"5\">Модификации автоэнкодеров</font>\n",
    "\n",
    "[Contractive Autoencoders](http://www.icml-2011.org/papers/455_icmlpaper.pdf) — автоэнкодеры, родственные шумоподавляющим автокодировщикам.\n",
    "\n",
    "[Variational losssy autoencoder](https://arxiv.org/pdf/1611.02731.pdf) — один из типов VAE, который пытается решить проблему того, что сильный декодер может игнорировать латентное представление.\n",
    "\n",
    "[$\\beta$- VAE](https://arxiv.org/pdf/1804.03599.pdf) — еще одно возможное улучшение VAE\n",
    "\n",
    "[Wassershtein autoencoders](https://arxiv.org/pdf/1711.01558.pdf)\n",
    "\n",
    "[Concrete autoencoders](https://arxiv.org/abs/1901.09346) — якобы позволяют выделять наиболее важные признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> Примеры практического применения </font>\n",
    "\n",
    "\n",
    "1. [Age Progression/Regression — предсказание того, как будет выглядеть человек в другом возрасте](https://arxiv.org/abs/1702.08423)\n",
    "\n",
    "2. [druGAN, генерация новых химических веществ](https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.7b00346)\n",
    "\n",
    "3. [Генерация лекарств, специфически меняющих активность генов человека](https://www.frontiersin.org/articles/10.3389/fphar.2020.00269/full)\n",
    "\n",
    "4. [Генерация ингибиторов определенного белка](https://www.nature.com/articles/s41587-019-0224-x)\n",
    "\n",
    "5. [Получение латентных представлений транскриптомов](https://academic.oup.com/nar/article/48/10/e56/5814052)\n",
    "\n",
    "6. [MethylNet](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3443-8) — Использование метилирования генома для обучения латентного представления, помогающего в предсказании возраста и т.д.\n",
    "\n",
    "7. [scVAE](https://academic.oup.com/bioinformatics/article-abstract/36/16/4415/5838187?redirectedFrom=fulltext) — получение данных об экспрессии генов из single cell данных\n",
    "\n",
    "8. [U-Net](https://arxiv.org/abs/1505.04597) — сегментация медицинских изображений\n",
    "9. [W-Net](https://arxiv.org/abs/1711.08506) — unsupervised сегментация медицинских изображений"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
