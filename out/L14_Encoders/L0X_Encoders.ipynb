{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоенкодеры (AE). Вариационные автоэнкодеры (VAE). Условные вариационные автоенкодеры (CVAE). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning \n",
    "\n",
    "*If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake.* — **Yann LeCun**\n",
    "\n",
    "Сегодня с вами мы разберем подход, который относится к задаче обучения без учителя (**unsupervised learning**), когда объекты известны, но им не сопоставлены метки, которые мы должны тем или иным способом предсказывать.\n",
    "\n",
    "Разницу между двумя задачами можно понять при помощи картинки, представленной ниже. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/uns_sup_diff.png\" alt=\"alttext\" style=\"width: 700px;\"/>\n",
    "\n",
    "В случае **supervised learning** для каждого объекта нам известна метка, что вот это — изображение яблок, это — изображение груш и т.д. Далее модель учится по изображению определять фрукт. \n",
    "\n",
    "В случае же **unsupervised learning** модель просматривает все изображения фруктов, не зная, где какой фрукт находится, и далее формирует представление, которое **неявно** делит фрукты по похожести. \n",
    "\n",
    "Зачем вообще изучать такой тип задачи? \n",
    "\n",
    "1. Иногда у нас слишком мало размеченных объектов, чтобы учить на них какую-либо задачу классификации и т. д. При этом у нас огромное количество неразмеченных данных. Мы можем **надеяться**, что если мы как-то обработаем наши данные, то они сами разделятся каким-то образом, согласующимся с метками. \n",
    "\n",
    "2. Человеческий мозг в основном учится в unsupervised манере. Возможно, для того чтобы решать задачи, которые легко по сравнению с компьютером решает человек, нам стоит и компьютер учить похожим образом.\n",
    "\n",
    "3. Часто обучение без учителя дает результаты, которые в дальнейшем позволяют быстро адаптироваться к новым задачам обучения с учителем и переключаться между ними. Причем делать это **эффективнее**, чем transfer learning. (*Stop learning tasks, start learning skills* — Satinder Singh)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation learning\n",
    "\n",
    "Одной из областей, тесно связанной с Unsupervised learning, является задача выучивания представления данных (**representation learning**).\n",
    "\n",
    "Выученное представление используется либо как шаг предобучения нейросети перед обучением ее уже на целевую задачу, либо как источник признаков для других, не обязательно нейросевых, алгоритмов. \n",
    "\n",
    "Задачу получения полезного представления данных можно решать при помощи supervised learning, однако, как правило, это куда менее эффективно. Почему?\n",
    "\n",
    "ImageNet содержит 1.28 миллионов изображений. Допустим даже мы передаем их в сетку, приводя все к разрешение 128x128. В этом случае они будует занимать $\\approx$ 500Гбит. \n",
    "\n",
    "А сколько будут занимать метки изображений? У нас 1000 возможных классов. Допустим, мы используем one-hot encoding в виде битового вектора. Получается $\\approx$ 12.8Мбит.\n",
    "\n",
    "Нейронная сеть для ImageNet может легко содержать 30М весов.\n",
    "\n",
    "Есть ли смысл нейросетке учить информации больше, чем закодировано в **предсказываемой величине**? — Нет. Потому, как правило, представление данных, полученное из полностью supervised нейросети, будет крайне бедным и заточенным на конкретную задачу. Могут возникнуть проблемы даже с применением этого представления для похожей задачи. \n",
    "\n",
    "А вот если мы воспользуемся unsupervised подходом, то потенциально можем принудить нейросетку пытаться выработать эффективное представление всех 500Гбит.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понижения размерности\n",
    "\n",
    "Еще одной областью, тесно связанной с предыдущими двумя, является задача снижения размерности — когда мы хотим данные из пространства высокой размерности преобразовать в пространство более низкой, с сохранением одного или нескольких свойств, например:\n",
    "\n",
    "* данные реконструируются обратно почти без ошибки;\n",
    "* расстояние между объектами сохраняется.\n",
    "\n",
    "Зачем это нужно? По многим причинам.\n",
    "\n",
    "Многие алгоритмы показывают себя плохо на простанствах большой размерности в принципе ([проклятье размерности](https://en.wikipedia.org/wiki/Curse_of_dimensionality)). \n",
    "\n",
    "Некоторые — просто будут значительно дольше работать, при этом качество их работы не изменится от уменьшения размерности. \n",
    "\n",
    "Понижение размерности позволяет использовать память более эффективно и подавать модели на обучение за один раз больше объектов. \n",
    "\n",
    "Также понижение размерности помогает избавится от шума, как мы обсудим дальше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X68mLzjZpaiV"
   },
   "source": [
    "## Автоэнкодер\n",
    "\n",
    "Autoencoder — архитектура нейросети, которая сначала с помощью нейросети-энкодера сжимает изображение в вектор небольшой размерности (он называется скрытым представлением), а затем восстанавливает этот вектор в исходную картинку с помощью нейросети-декодера. \n",
    "\n",
    "\n",
    "Практика показывает, что скрытое представление картинки позволяет делать очень интересные и красивые вещи — например, очищать изображение от шума, проводить гладкую интерполяцию между 2 написанными от руки цифрами, генерировать новую рукописную цифру со стилем от имеющейся. \n",
    "\n",
    "Откуда берутся эти свойства? Они являются следствием сжатия информации. Одна из форм сжатия — это классификация, которую мы уже делали. Если это цифры, то вместо изображения можно сохранить только какая это цифра. Это предельное сжатие информации, но при попытке перевести цифру в картинку мы уже не имеем достаточно информации, чтобы картинка получалось разной. Если не так сильно ограничивать информацию в точке максимального сжатия, то кроме класса цифры сохранится еще что-то и изображение удастся восстановить с большим количеством сохранённых деталей.\n",
    "\n",
    "\n",
    "![alt text](https://edunet.kea.su/repo/src/L0X_Encoders/img/better_enc_dec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сжатие информации и потери\n",
    "\n",
    "Автоэнкодер может быть без потерь и с потерями (lossless и lossy). В какой-то степени это альтернативно методам сжатия архиваторов и кодирования контента (zip, mp3, jpeg, flac, ...). Можно ли сделать сжатие на нейронных сетях с помощью автоэнкодеров? Да, это будет работать. Размер сети будет большим, но сжатие может превзойти другие алгоритмы. Исследования в этой области ведутся, но практически применямых примеров нет. \n",
    "\n",
    "![alt text](https://edunet.kea.su/repo/src/L0X_Encoders/img/lossy_encoding.png)\n",
    "\n",
    "Почему это может работь? Дело в том, что нейронная сеть может сформулировать набор правил, по которому на основе латентного представления приближенно или точно кодировать (за счет кодировщика), а затем восстанавливать исходный ообъект (при помощи декодировщика).\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/ae_principle.png\" alt=\"alttext\" style=\"width: 700px;\"/>\n",
    "А почему мы уверены, что такой набор правил будет существовать и мы вообще имеем право понижать размерность пространства?\n",
    "\n",
    "\n",
    "## Manifold assumption \n",
    "\n",
    "Мы предполагаем, что наши данные на самом деле лежат в пространстве меньшем, чем пространство исходных признаков.\n",
    "\n",
    "![alt text](https://edunet.kea.su/repo/src/L0X_Encoders/img/manifold1.png)\n",
    "\n",
    "В большинстве случаев это действительно правда. Например, лица людей даже на фотографиях 300x300, очевидно, лежат в пространстве меньшей размерности, нежели 90000. Ведь не каждая матрица 300 на 300, заполненная какими-то значениями от 0 до 1, даст нам изображение человека\n",
    "\n",
    "![alt text](https://edunet.kea.su/repo/src/L0X_Encoders/img/manifold2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод главных компонент (PCA). \n",
    "Метод главных компонент (Principal Component Analysis). Это метод отображения векторов свойств объектов (помним, что у нас объект всегда описывается вектором свойств, длина вектора — это количество свойств) в вектора производных свойств (**компонент**), меньшей длины с помощью линейной комбинации, чтобы обратной операцией можно было восстановить значения векторов свойств как можно ближе к исходным. То есть PCA тоже выполняет сжатие информации, он тоже работает для группы объектов (а нейронная сеть автоэнкодера учится под определённую группу объектов). \n",
    "\n",
    "Обычно PCA работает для центрированных переменных. Каждая следующая компонента проводится перпендикулярно предыдущим и так, чтобы объяснить наибольшую часть разницы, не объясненной предыдущими компонентами между объектами.\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/pca_motivation.png\" alt=\"alttext\" style=\"width: 700px;\"/>\n",
    "\n",
    "Графически можно представить PCA, как поиск подпространства, проекция точек, на которое минимально меняет координаты в исходном пространстве. Например, для объектов на плоскости PCA можно сделать в одномерное пространство — на прямую.\n",
    "![alt text](https://edunet.kea.su/repo/src/L0X_Encoders/img/pca_decomposition.png)\n",
    "Прямая определяется только вектором нормали, то есть линия проекции проходит через 0. \n",
    "\n",
    "\n",
    "Возвращаясь к примеру с лицами — долгое время для распознавания лиц размерности 128\\*128 использовалось представление, полученное при помощи PCA. Для хорошего качества восстановления хватает около 100 компонент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналогия AE и PCA\n",
    "\n",
    "Отличия PCA и AE в том, что PCA выполняет линейную комбинацию над компонентами исходного вектора свойств объекта, а AE — как правило, нелинейную. PCA вычисляется однозначно, а AE обучается без гарантии нахождения наилучшего положения. PCA гарантирует ортогональный базис для разложения сжатых свойств, а AE — нет.\n",
    "\n",
    "PCA будет частным случаем AE, если в нём сделать только один плотный слой (Dense) с количеством нейронов, равным требуемому числу компонент, сделать линейную функцию активации, сделать потери по среднему квадрату ошибки (mean squared error — MSE). Кроме этого, необходимо будет нормировать признаки перед подачей их на вход AE.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/pca_autoencoder.png\" alt=\"alttext\" style=\"width: 500px;\"/>\n",
    "\n",
    "Тогда PCA позволяет рассчитать веса для нейронов такого автоэнкодера. При этом гарантировав (в отличии от градиентного спуска) наилучшее решение задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очищение изображения от шумов\n",
    "Интересное применение Autoencoder'ов — очищение входной картинки от шумов. Такое принципиально возможно из-за того, что размерность латентного пространства очень мала по сравнению с размерностью входного пространства(в нашем случае — 32 и 784 соответственно) — в нём попросту нет места случайному шуму, но зато есть место для общих закономерностей из входного пространства.\n",
    "\n",
    "То есть мы подаём на обучении автоэнкодера такой незашумлённый датасет, что в нём на самом деле есть некое простраство свойств, которое его описывает. На выходе энкодера в изображении останутся именно эти свойства. Шум является внешним свойством и не сможет закодироваться.\n",
    "\n",
    "Иными словами — за счет кодировщика и декодировщика автоенкодер выучивается «проектировать» объекты на латентное пространство и восстанавливать их из него. Если шум небольшой, то автоенкодер спроецирует объект в нужное место в латентном пространстве и обратно восстановит его уже без шума.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/denoising_prop.png\" alt=\"alttext\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "При этом важно понимать, что если шум поместит наш объект так, что автоенкодеру придется выбирать между разными вариантами проекции, могут возникнуть артефакты. \n",
    "\n",
    "В случае, приведенном на рисунке, зашумленному x соответствуют две группы объектов из реального датасета. Если мы, к примеру, оптимизируем MSE, то автоенкодеру «экономнее» всего будет восстанавливать нечто между двумя группами. При этом этого «нечто» в природе не существует или оно очень маловероятно. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/bad_bimodal.png\" alt=\"alttext\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление шума к исходной выборке\n",
    "\n",
    "Также в случае отсутствия шума в изначальной выборке, ее малом размере и т.д. можно добавлять шум к самим исходным данным, получая из объекта $x$ объект $\\tilde{x}$, и требуя от энкодера восстановить на основе зашумленного объекта исходный. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/denoising_autoencoder.png\" alt=\"alttext\" style=\"height: 450px;\"/>\n",
    "\n",
    "Этот подход может работать и является примером *аугментации данных*. Он может дополнительно заставить автоенкодер выучивать полезные признаки, т.е. его можно использовать, даже если целью не является получение автоенкодера, избавляющего данные от шума.\n",
    "\n",
    "С ним, однако, надо быть очень аккуратным:\n",
    "\n",
    "1. Шум, который вы добавляете, не должен сильно менять исходный объект. Если это происходит, то либо аввтоенкодер легко будет находить места, где был добавлен шум и при этом делать ему это будет легче, чем учить сжатое представление данных. Либо автоенкодер выучит о ваших данных что-то такое, чего там и в помине быть не может. К примеру, если добавить к признакам, которые всегда целые, нормальный шум, ничего хорошего не выйдет. \n",
    "\n",
    "\n",
    "2. Шум должен соответствовать «натуральному шуму». Если реальный шум в данных отличается от того, на котором учился автоенкодер, есть вероятность, что он не будет очищать данные от исходного шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA для избавления от шума\n",
    "Давайте применим PCA, как простеший автоэнкодер, для очищения от шумов изображений базы MNIST. Нам потребуется база MNIST, numpy, библиотека отрисовки matplotlib и сам PCA, который есть в пакете sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from itertools import chain\n",
    "\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data'\n",
    "\n",
    "train_set = dset.MNIST(root=root, \n",
    "                       train=True, \n",
    "                       transform=torchvision.transforms.ToTensor(),\n",
    "                       download=True)\n",
    "test_set = dset.MNIST(root=root, \n",
    "                      train=False,\n",
    "                      transform=torchvision.transforms.ToTensor(),\n",
    "                      download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и приведём размерность к двумерной, чтобы это был набор векторов свойств."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_set.data.numpy()\n",
    "y_train = train_set.targets.numpy()\n",
    "x_test =  test_set.data.numpy()\n",
    "y_test = test_set.targets.numpy()\n",
    "\n",
    "x_train, x_test = x_train/255., x_test/255.\n",
    "xt_shape = x_train.shape\n",
    "print(\"Initial shape \", xt_shape)\n",
    "xt_flat = x_train.reshape(-1, xt_shape[1]*xt_shape[2])\n",
    "print(\"Reshaped to \", xt_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь заведём класс PCA и настроим его, чтобы сохранял 90% исходной картинки. Обучим его и посмотрим, сколько ему потребовалось свойств для описания каждой картинки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.90)\n",
    "xt_encoded = pca.fit_transform(xt_flat)\n",
    "print(\"Encoded features \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер (он же декодер, ведь это просто обратная матрица от энкодера PCA) обучен. Теперь можно проверить, как он закодирует и раскодирует тестовую выборку. Для этого проведём такие же преобразования размерности для неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_shape = x_test.shape\n",
    "xtest_flat = x_test.reshape(-1, xtest_shape[1]*xtest_shape[2])\n",
    "xtest_encoded = pca.transform(xtest_flat)\n",
    "xtest_decoded = pca.inverse_transform(xtest_encoded).reshape(xtest_shape)\n",
    "print(\"Decoded xtest_decoded shape is \", xtest_decoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно определить функцию для отрисовки изображений MNIST. Она будет выводить несколько изображений в ряд, поэтому будет принимать трёхмерный массив. Шкала не должна быть автоподстраиваемой, так как после обработки изображения выйдут за диапазон (0,1), в котором заданы исходные изображения. Мы зафиксируем шкалу в диапазоне (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, title):\n",
    "    fig=plt.figure(figsize=(16, 3))\n",
    "    columns = images.shape[0]\n",
    "    rows = 1\n",
    "    for i in range(columns):\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(images[i], cmap='gray_r', clim=(0,1))\n",
    "    fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем исходное и закодированное изображение для некоторых изображений, которые мы элегантно случайным образом выберем из всей тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.choice(x_test.shape[0], 6)\n",
    "samples_orig = x_test[sample_indices]\n",
    "samples_decoded = xtest_decoded[sample_indices]\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"PCA decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что `pca.n_components_` (87 для 90% PCA) достаточно для описания картинок MNIST вместо 784 исходных пикселей. Но при этом нужно хранить матрицу кодирования-декодирования, а также изображения получаются немного зашумлёнными. Мы получили способ сжатия с потерями для рукописных цифр, где изображение центрировано и отмасштабировано по рамке из 28х28 пикселей (подробней смотрите правила базы MNIST). \n",
    "\n",
    "Степень сжатия у нас условно 87/784 ~= 0.11. То есть сжатие в 9 раз. «Условно», так как сжатое изображение хранится во float, а исходное в uint8, который требует в 4 раза меньше байт. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим, как наш автоэнкодер без нейросетей справится с очисткой от зашумления. Для этого сделаем функцию добавления шумов к MNIST выборке и посмотрим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_add_noise(noise_factor, dataset):\n",
    "    return dataset + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=dataset.shape) \n",
    "\n",
    "x_test_noisy = mnist_add_noise(0.3, x_test)\n",
    "samples_noisy = x_test_noisy[sample_indices]\n",
    "plot_images(samples_noisy, \"x_test with added noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно провести ту же операцию PCA энкодера и декодера, что выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCArecode(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1]*dataset.shape[2])\n",
    "    return pca.inverse_transform(pca.transform(dataset_flat)).reshape(dataset.shape)\n",
    "\n",
    "x_filtered = PCArecode(x_test_noisy)\n",
    "samples_filtered = x_filtered[sample_indices]\n",
    "plot_images(samples_filtered, \"Noise filtered x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты сравнения напишем функцию, которая будет строить зашумленные и восстановленные образцы друг под другом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(*args, invert_colors=True, digit_size = 28, name=None):\n",
    "    args = [x.squeeze() for x in args]\n",
    "    n = min([x.shape[0] for x in args])\n",
    "    figure = np.zeros((digit_size * len(args), digit_size * n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(len(args)):\n",
    "            figure[j * digit_size: (j + 1) * digit_size,\n",
    "                   i * digit_size: (i + 1) * digit_size] = args[j][i].squeeze()\n",
    "\n",
    "    if invert_colors:\n",
    "        figure = 1-figure\n",
    "\n",
    "    plt.figure(figsize=(2*n, \n",
    "                        2*len(args))\n",
    "              )\n",
    "    \n",
    "    plt.imshow(figure,\n",
    "               cmap='Greys_r',\n",
    "               clim=(0,1))\n",
    "    \n",
    "    plt.grid(False)\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if name is not None:\n",
    "        plt.savefig(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(samples_noisy, samples_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, шумы стали значительно меньше, но и артефакты вокруг цифры усилились. Это неудивительно, ведь мы сжимали информацию линейным образом на целых 87 компоненты. Это не позволяет ни глубокого обучения, ни достаточного сжатия. Повышение уровня сжатия приведёт к еще большему количеству артефактов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Латентное представление для цифр после PCA \n",
    "\n",
    "Посмотрим теперь на то, как делятся наши картинки в латентном представлении. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_latent(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1]*dataset.shape[2])\n",
    "    return pca.transform(dataset_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_manifold(latent_r, labels, alpha=0.5):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(latent_r[:, 0], latent_r[:, 1], c=labels, cmap=\"tab10\", alpha=0.5)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_r = pca_latent(x_test)\n",
    "plot_manifold(latent_r, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что латентное представление слабо разделяет картинки по тому, какие цифры на них изображены. Только 1 расположены более-менее обособленно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация автоенкодера\n",
    "Итак, вспомним, что в автоэнкодере одна сеть переводит пространство свойств в пространство меньшей размерности, а другая сеть восстанавливает исходное изображение. Вместо вычисления коэффициентов сети мы будем её обучать. Для обучения нужно определить функцию потерь. Мы возьмём среднеквадратичное расстояние (MSE). То есть мы требуем, чтобы значения пикселей исходного изображения и восстановленного отличались несильно.\n",
    "![alt_text](https://edunet.kea.su/repo/src/L0X_Encoders/img/encoder_loss.png)\n",
    "Мы можем использовать любую сеть для энкодера и декодера: на плотных слоя или на свёрточных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно задать архитектуру модели. Мы будем использовать последовательную модель и свёрточную  архитектуру. В конце кодировщика должен быть вектор длины `latent_size`. И декодировщик должен принимать этот вектор и восстанавливать до целого изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=(3,3), \n",
    "                               padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=(3,3),\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,\n",
    "                               out_channels=64,\n",
    "                               kernel_size=(3,3), \n",
    "                               padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=(3,3), \n",
    "                               padding=1)\n",
    "        self.linear = nn.Linear(in_features=3136, \n",
    "                                out_features=latent_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        x = F.max_pool2d(x, kernel_size=(2,2))\n",
    "        x = self.conv3(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        x = self.conv4(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        x = F.max_pool2d(x, kernel_size=(2,2))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=latent_size, \n",
    "                                out_features=1024)\n",
    "        self.conv1 = nn.Conv2d(64, 64, (3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3,3), padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3,3), padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 1, (3,3), padding=1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, 64, 4, 4)\n",
    "        x = F.interpolate(x, size=(14, 14))\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        x = F.interpolate(x, size=(28, 28))\n",
    "        x = self.conv3(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем основную функцию для обучения нейросети. single_pass_handler и loss_handler бдут меняться в зависимости от сетки, которую мы обучаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(enc, \n",
    "          dec,\n",
    "          loader, \n",
    "          optimizer, \n",
    "          single_pass_handler, \n",
    "          loss_handler,\n",
    "          epoch, \n",
    "          log_interval=500):\n",
    "    for batch_idx, (data, lab) in enumerate(loader):\n",
    "        batch_size = data.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        if USE_CUDA:\n",
    "            data = data.cuda()\n",
    "            lab = lab.cuda()\n",
    "\n",
    "        latent, output = ae_pass_handler(encoder, decoder, data, lab)\n",
    "\n",
    "        loss = loss_handler(data, output, latent)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loader.dataset),\n",
    "                100. * batch_idx / len(loader), loss.item()))\n",
    "            \n",
    "def ae_pass_handler(encoder, decoder, data, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    recons = decoder(latent)\n",
    "    return latent, recons\n",
    "\n",
    "def ae_loss_handler(data, recons, *args, **kwargs):\n",
    "    return F.binary_cross_entropy(recons, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим загрузчики наших данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 2\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_size=latent_size)\n",
    "decoder = Decoder(latent_size=latent_size)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "optimizer = optim.Adam(chain(encoder.parameters(),\n",
    "                            decoder.parameters()\n",
    "                           ),\n",
    "                      lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    train(enc=encoder, \n",
    "      dec=decoder, \n",
    "      optimizer=optimizer,\n",
    "      loader=train_loader,\n",
    "      epoch=i, \n",
    "      single_pass_handler=ae_pass_handler,\n",
    "      loss_handler=ae_loss_handler,\n",
    "      log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, чтобы удобно прогонять датасет через обученную нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(encoder, \n",
    "             decoder, \n",
    "             loader, \n",
    "             single_pass_handler,\n",
    "             return_real=True, \n",
    "             return_recontr=True,\n",
    "             return_latent=True,\n",
    "             return_labels=True):\n",
    "  \n",
    "    if return_real:\n",
    "        real = []\n",
    "    if return_recontr:\n",
    "        reconstr = []\n",
    "    if return_latent:\n",
    "        latent = []\n",
    "    if return_labels:\n",
    "        labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, lab) in enumerate(loader):  \n",
    "            if return_labels:\n",
    "                labels.append(lab.numpy())\n",
    "            if return_real:\n",
    "                real.append(data.numpy())\n",
    "            if USE_CUDA:\n",
    "                data = data.cuda()\n",
    "                lab = lab.cuda()\n",
    "            rep, rec = single_pass_handler(encoder, decoder, data, lab)\n",
    "            if return_latent:\n",
    "                latent.append(rep.cpu().numpy())\n",
    "            if return_recontr:\n",
    "                reconstr.append(rec.cpu().numpy())\n",
    "    \n",
    "    result = {}\n",
    "    if return_real:\n",
    "        real = np.concatenate(real)\n",
    "        result['real'] = real.squeeze()\n",
    "    if return_latent:\n",
    "        latent = np.concatenate(latent)\n",
    "        result['latent'] = latent\n",
    "    if return_recontr:\n",
    "        reconstr = np.concatenate(reconstr)\n",
    "        result['reconstr'] = reconstr.squeeze()\n",
    "    if return_labels:\n",
    "        labels = np.concatenate(labels)\n",
    "        result['labels'] = labels\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала оценим то, как ведет себя наш автоенкодер работает в целом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(run_res['real'][0:9], run_res['reconstr'][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим, какое латетное представление он выучил. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_manifold(run_res['latent'], run_res['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь обучим автоенкодер с латентным слоем размера 24. И посмотрим, как он будет бороться с шумом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 24\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_size=latent_size)\n",
    "decoder = Decoder(latent_size=latent_size)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "optimizer = optim.Adam(chain(encoder.parameters(),\n",
    "                            decoder.parameters()\n",
    "                           ),\n",
    "                      lr=learning_rate)\n",
    "\n",
    "for i in range(1, 11):\n",
    "    train(enc=encoder, \n",
    "      dec=decoder, \n",
    "      optimizer=optimizer,\n",
    "      loader=train_loader,\n",
    "      epoch=i, \n",
    "      single_pass_handler=ae_pass_handler,\n",
    "      loss_handler=ae_loss_handler,\n",
    "      log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем dataloader, который добавляет в наш датасет шум автоматически"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noise_set = dset.MNIST(root=root, \n",
    "                      train=False,\n",
    "                      transform=torchvision.transforms.Compose([\n",
    "                          torchvision.transforms.ToTensor(),\n",
    "                          AddGaussianNoise(0., 0.1)\n",
    "                      ]),\n",
    "                      download=True)\n",
    "\n",
    "test_noised_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.Subset(test_noise_set, list(range(64))),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_dataloader, ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(run_res['real'][0:9], run_res['reconstr'][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сжатия мы оценили визуально выше. Если обратить внимание, то исходные картинки даже почистились от мелких шумов и странностей изображения и больше стали похожи на непрерывные линии. Размерность латентного пространства `latent_size` значительно меньше исходного количества свойств (784), поэтому мы получили неплохое сжатие изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженный автоенкодер\n",
    "\n",
    "До этого мы говорили о том, что построение эффективного автоенкодера возможно только в случае, когда размер латентного слоя сильно меньше входных слоев. \n",
    "\n",
    "Однако, сделать латентный слой «узким местом» сети можно и иначе. \n",
    "Это позволяет модели самой выбирать размер латентного представления и использовать для разных групп объектов латентные представления разного размера, что может быть полезно для части задач и позволять получить более полезное внутреннее представление. \n",
    "\n",
    "Для того, чтобы добиться этого введем следующее условие: \n",
    "**Для каждого объекта x в среднем на латентном слое должна активироваться лишь малая доля нейронов**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/sparse_autoencoder.png\" alt=\"alttext\" style=\"height: 450px;\"/>\n",
    "\n",
    "Добавить такое требование в модель можно двумя способами.\n",
    "\n",
    "### L1-регуляризации\n",
    "\n",
    "#### Напоминание:\n",
    "\n",
    "Квадрат от значения аргумента называют L2-регуляризацией или гребневой регрессией. У неё нет свойства отбирать одни признаки против других, так как по мере приближения к нулю параболическая зависимость начинает давать мало вклада в потери, а вклад в основном приходит от больших аргументов.\n",
    "\n",
    "$$L2 =  \\alpha \\sum_i w_i^2 $$\n",
    "\n",
    "Модуль от значения аргумента называют L1-регуляризацией или лассо-регрессией. Эта регуляризация обычно довольно агрессивная и в многомерном пространстве приводит к отбору свойств или признаков объекта, обращая малозначимые коэффициенты под регуляризацией в ноль. То есть если надо минимизировать сумму модулей, то лучше удержать какой-то параметр на величину 'a', но свести малозначимый параметр со значением 'a' до нуля (меньше потери по нему уже не сделать). Последний параметр перестанет действовать, но зато более значимый параметр при разумных значения коэффициента регуляризации не пострадает.\n",
    "\n",
    "$$L1 = \\beta\\sum_i |w_i| $$\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/losses.gif\" alt=\"alttext\" style=\"width: 600px;\"/>\n",
    "\n",
    "Для нашей задачи потому подойдет именно L1-регуляризация — нам важно, чтобы нейросеть именно **не использовала** часть весов, а не просто использовала много мелких весов, которые на следующих слоях может увеличить. \n",
    "\n",
    "$sparse\\_l1\\_loss = ||x - decoder(encoder(x))||^2 + \\beta \\sum_i |a_{i}^{latent}|$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим то, как должен выполнятся forward pass для нашего автоенкодера и как должен подсчитываться лосс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_ae_pass_handler(encoder, decoder, data, *args, **kwargs):\n",
    "    latent = encoder(data)\n",
    "    recons = decoder(latent)\n",
    "    return latent, recons\n",
    "\n",
    "def sparse_ae_loss_handler(data, recons, latent, beta=0.1, *args, **kwargs):\n",
    "    return F.binary_cross_entropy(recons, data) + F.l1_loss(latent, torch.zeros_like(latent)) * beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 16 * 16\n",
    "\n",
    "learning_rate = 1e-4\n",
    "encoder = Encoder(latent_size=latent_size)\n",
    "decoder = Decoder(latent_size=latent_size)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "optimizer = optim.Adam(chain(encoder.parameters(),\n",
    "                            decoder.parameters()\n",
    "                           ),\n",
    "                      lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "for i in range(1, 10):\n",
    "    train(enc=encoder, \n",
    "      dec=decoder, \n",
    "      optimizer=optimizer,\n",
    "      loader=train_loader,\n",
    "      epoch=i, \n",
    "      single_pass_handler=sparse_ae_pass_handler,\n",
    "      loss_handler=partial(sparse_ae_loss_handler, beta=0.01),\n",
    "      log_interval=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_noised_dataloader, sparse_ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(run_res['real'][0:9], run_res['reconstr'][0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что разреженный автоенкодер несмотря на большой размер латентного слоя все равно эффективно убирает шум из изображений.\n",
    "\n",
    "Посмотрим, как активируются нейроны латентного слоя для каждого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_res = run_eval(encoder, decoder, test_loader, sparse_ae_pass_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(nrows=2, ncols=5, figsize=(16,5))\n",
    "for label in range(0, 10):\n",
    "    figure = run_res['latent'][run_res['labels'] == label].mean(axis=0)\n",
    "    figure = figure.reshape(16, 16)\n",
    "    ax = axs[label % 2, label % 5]\n",
    "    ax.imshow(figure,\n",
    "            cmap='Greys_r',\n",
    "            clim=(0,1))\n",
    "    ax.grid(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что нейронов активируются в среднем крайне мало, между классами активирующиеся нейроны отличаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот подход очень просто реализуется, но в то же время не совсем очевидно, как с помощью него задать условие вида: пусть в среднем на латентном слое активируется 1% нейронов. Понятно, что это косвенно задается величиной коэффициента $\\beta$, но хотелось бы задавать это в явном виде. \n",
    "\n",
    "### Дивергенция Кульбака-Лейблера\n",
    "\n",
    "\n",
    "Для данной цели используется дивергенция Кульбака-Лейблера, которая считается по формуле: \n",
    "\n",
    "$$KL(P||Q) = \\int_X p(x)\\log \\dfrac {p(x)} {q(x)} dx$$\n",
    "\n",
    "В теории информации p считается целевым распределением, а q — тем, с которым мы его сравниваем. \n",
    "Важно понимать, что при этом KL не является мерой расстояния, а именно в общем случае $KL(P||Q) != KL(Q||P)$\n",
    "\n",
    "\n",
    "[Оказывается](https://math.stackexchange.com/questions/90537/what-is-the-motivation-of-the-kullback-leibler-divergence), в подобных задачах она как правило обеспечивают бОльшую сходимость к требуемому распределениею, нежели та же L1-регуляризация.\n",
    "\n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/kl_plot.png)\n",
    "\n",
    "Пусть мы хотим, чтобы на каждом слое для данного объекта активировалось в среднем p% нейронов.\n",
    "\n",
    "В нашем случае для каждой активации i нейрона латентного слоя $a_i^{latent}$ мы можем решить, активирован нейрон или нет. Например, в случае функции ReLU очевидно, что любое положительное значение активации отвечает активированному нейрону. После этого мы можем посчитать для каждого объекта, какая доля нейронов активировалась в его случае. Получим величину $\\hat{p}$\n",
    "\n",
    "Фактически мы сравниваем два бернулиевских распределения — то, которое хотим мы, с параметром p, и то, которое мы наблюдаем — с оценным параметром $\\hat{p}$. \n",
    "\n",
    "$$KL(P||Q) =  p(x) \\log \\dfrac {p(x)} {\\hat{p}(x)} + (1 - p(x)) \\log \\dfrac {(1 - p(x))} {1 - \\hat{p}(x)} $$\n",
    "\n",
    "Далее лишь остается просуммировать данный лосс по батчу.\n",
    "\n",
    "Можно делать и иначе — требовать, чтобы каждый нейрон в среднем активировался в p% случаев. В этом случае на первом шаге мы усредняем не по всему слою, а по батчам. А вот подсчитанный лосс усреднеяем по всем нейронам слоя. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодер, как генератор и его ограничения. Плавная интерполяция\n",
    "У нас уже была система с латентным пространством и возможностью строить по нему объекты — GAN. Значит в случае автоэнкодеров тоже можно подавать случайный вектор на декодер и получать новые объекты. До этого мы же только восстанавливали исходную картинку.\n",
    "![alt text](https://edunet.kea.su/repo/src/L0X_Encoders/img/encoder_as_generator.png)\n",
    "\n",
    "Какое значение вектора выбрать? Мы же никак не управляли латентным пространством. Непонятно какие числа подставлять. Поэтому мы можем выбрать промежуточные значения между двумя представлением двух исходных изображений в латентном пространстве и получить плавную интерполяцию между изображениями. Постепенно свойства одного изображения будут исчезать, а появляться свойства другого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "0yNPENXvZCUV",
    "outputId": "a14a25a3-d466-4c2c-fe24-c33882fb0044"
   },
   "outputs": [],
   "source": [
    "def interpolation(vec1, vec2, N_inter):\n",
    "    intermediate_values = np.zeros((0, vec1.shape[0]))\n",
    "    for i in range(N_inter):\n",
    "        intermediate = (1-float(i / N_inter))*vec1 + float(i / N_inter)*vec2\n",
    "        intermediate_values = np.append(intermediate_values, intermediate.reshape(1, -1), axis=0)\n",
    "    intermediate_values = np.append(intermediate_values, vec2.reshape(1, -1), axis=0)\n",
    "    return intermediate_values\n",
    "\n",
    "N_inter = 8\n",
    "\n",
    "# Take pairs of random images from the test set\n",
    "encodings = encoder.predict(np.expand_dims(samples_orig, axis = 3))\n",
    "for i in range(len(sample_indices) - 1):\n",
    "    vectors = interpolation(encodings[i], encodings[i+1], N_inter)\n",
    "    images = np.squeeze(decoder.predict(vectors), axis = 3)\n",
    "    plot_images(images, \"Interpolation of %i into %i\"%(y_test[sample_indices[i]], y_test[sample_indices[i+1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-08LyQu0te_a"
   },
   "source": [
    "Если этот файл запущен не на Google Colab, с помощью этого кода можно создать видео, на котором процесс превращения одной цифры в другую будет виден ещё более наглядно. Для этого можно использовать уже известный нам OpenCV. Он умеет делать видеофайлы из массивов чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSua0vyadurR"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "N_inter = 90\n",
    "resize_coeff = 10\n",
    "\n",
    "size = (images.shape[2]*resize_coeff, images.shape[1]*resize_coeff)\n",
    "out = cv2.VideoWriter('output/interpolation.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, size, 0)\n",
    "for i in range(len(sample_indices) - 1):\n",
    "    vectors = interpolation(encodings[i], encodings[i+1], N_inter)\n",
    "    images = np.squeeze(decoder.predict(vectors), axis = 3)\n",
    "    for i in range(len(images)):\n",
    "        img = images[i] / np.max(images[i])\n",
    "        img = (cv2.resize(255*img.reshape(28, 28), size, cv2.INTER_NEAREST))\n",
    "        out.write(img.astype(np.uint8))\n",
    "    \n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так себе интерполяция вышла. Старое изображение затухает, а новое изображение появляется. Хочется, чтобы в промежуточных кадрах не было каки-то непонятных очертаний, а изображение было чем-то промежуточным по смыслу между стартовым и конечным изображением. Причина в том, что в латентном пространстве действительно возникли зоны, которые умеют декодироваться в хорошие изображения. Но никто не сказал, что между этими зонами должно быть что-то адекватное (что мы видели из представления).\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/bad_latent_for_ae.png\" alt=\"alttext\" style=\"width: 600px;\"/>\n",
    "Представим это графически. Пусть наш очень умный, содержащий очень много коэффициентов, энкодер и декодер смог разложить все входные объекты на одной оси (размерность латентного пространства — 1). По сути он каждому входному изображению присвоил номер и по номеру может это изображение вспомнить. То есть автоэнкодер очень переобученный. Тогда если мы возьмём промежуточный номер (пытаемся интерполировать), то какое изображение мы собираемся получить?\n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/bad_latent_for_ae2.png)\n",
    "\n",
    "Если мы хотим, чтобы декодированные промежуточные латентные состояния имели черты близких к ним объектов, то надо притянуть латентные координаты похожих объектов. Например вот так:\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/good_latent_for_ae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариационный автоэнкодер\n",
    "\n",
    "\n",
    "### Мотивация \n",
    "\n",
    "Хотим вместо представления слева получить представление справа\n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/vae_motivation.png)\n",
    "\n",
    "При этом зоны пересечения должны действительно содержать переходные картины \n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/vae_ideal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u90px5yEajSf"
   },
   "source": [
    "## Неудачная попытка: регуляризация\n",
    "\n",
    "Можем попробовать заставить наши объекты «лежать» рядом — будем штрафовать латентные представления, которые далеко уходят от начала координат. \n",
    "\n",
    "Можем использовать как L1, так и L2 регуляризацию, так и их комбинацию — elastic loss.\n",
    "\n",
    "Однако, это приведет просто к масштабированию распределения. Нам надо одновременно получить связное латентное представление — чтобы у нас не возникало зон в латентном представлении, которым не соответствует ничего, и при этом представление, в котором цифры будут отделены друг от друга. \n",
    "\n",
    "\n",
    "![alt_text](https://edunet.kea.su/repo/src/L0X_Encoders/img/reconstruction_loss_only.png)\n",
    "\n",
    "Мы имеем ситуацию, как на картинке слева. Переход-интерполяция между объектами проходит через зону отсутствующих в обучении объектов, декодирование которых даст несуществующие в реальности объекты. Нам не удастся погенерировать новые картинки, преобразовывая случайную точку из латентного пространства в случайную картинку. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вероятностный автоэнкодер\n",
    "\n",
    "При этом постановка задачи с автоенкодером говорит нам, что есть некое пространство меньшей размерности $Z$, которое и обуславливает процесс генерации объектов из $X$. Все остальные различия — следствия случайности — один и тот же человек может по-разному нарисовать цифру 5. \n",
    "\n",
    "Будем искать латентное пространство Z, которое удовлетворяет следующему условию:\n",
    "\n",
    "$$p(x) = \\int p(x, z)dz $$\n",
    "\n",
    "Кроме того, пусть объекты из Z легко генерировать. \n",
    "\n",
    "По формуле совместной вероятности:\n",
    "\n",
    "$$p(x, z) = p(x|z)p(z) $$\n",
    "\n",
    "Ну, осталось только подобрать такие параметры, чтобы все работало)) \n",
    "\n",
    "К сожалению, сделать это в таком виде не получится. Пространство $X$ может быть высокоразмерным. \n",
    "\n",
    "Но - мы можем существенно сузить область поиска, ведь каждому $x$ из пространства $X$ соответствует лишь небольшая возможная область в $Z$.\n",
    "\n",
    "Для этого будем также учить отображение из пространства $X$ в пространство $Z$, т.е, пытаться выучить $p(z|x)$. Назовем функцию, которой будем его приближать $q(z|x)$. \n",
    "\n",
    "Что же в случае автоенкодера выполняет роль p(x|z) и q(z|x) ?\n",
    "Очевидно — кодировщик и декодировщик соответственно. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/vae_as_two_functions.png)\n",
    "\n",
    "\n",
    "Чтобы все получилось, нужно сделать с кодировщиком две вещи. Заметьте, что декодировщик мы оставим без изменений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первая модификация\n",
    "\n",
    "Пусть наш кодировщик генерирует на основе объекта X вектор средних и вектор стандартных отклонений.\n",
    "\n",
    "Этих двух векторов хватает нам для того, чтобы задать многомерное нормальное распределение с независимыми компонентами (чтобы матрица ковариаций была диагональной), соответствующее данному объекту.\n",
    "\n",
    "Чтобы получить латентное представление объекта, отличающегося от X только в силу случайности нам достаточно сгенерировать вектор из нормального распределения с такими параметрами. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/vae_structure.png)\n",
    "\n",
    "Далее мы можем требовать, чтобы из полученного латентного представления декодировшик восстанавливал объект, похожий на исходный. \n",
    "\n",
    "Здесь, однако, сразу возникает возникает проблема с тем, что граф вычислений, соответствующий предыдущей структуре не может пропускать градиент — как пропустить градиент через генератор случайного нормального числа? Если считать из определения, то даже малейшему изменению параметра могут соответствовать бесконечные изменения генерируемого числа (нормальное распределение определено на бесконечности). В общем, проблема. \n",
    "\n",
    "Но мы можем вспомнить замечательное свойство одномерного нормального распределения:\n",
    "\n",
    "$$N(\\mu,\\sigma^2) = N(0,1) * \\sigma + \\mu$$ \n",
    "\n",
    "Выполняется это и для многомерного случая. Потому сделаем следующее — будем генерировать значение из нормального распределения с средними 0 и дисперсиями 1, а затем домножать это на вектор стандартных отклонений и прибавлять вектор средних. Получится вот такое преобразование, которое называется reparametrization trick.\n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/reparametrization_trick.png)\n",
    "\n",
    "В отличии от левого случая, в правом мы спокойно можем пропустить градиент через детерминистичные ноды. \n",
    "\n",
    "\n",
    "Но если просто применить такой принцип, то он снова имеет проблему предыдущего детерминистического подхода, так как вероятностное распределение сможет свернуться в дельта-функцию — зачем нейросети мучаться с объектами немного отличающимися от тех, что есть в обучающей выборке и пытаться нормально их восстанавливать, если можно просто начать генерировать стандартные отклонения, близкие к нулю и тем самым получить $\\delta$-функцию, которая будет нашему объекту всегда сопоставлять одну точку в латентном представлении. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/Dirac_function_approximation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вторая модификация\n",
    "\n",
    "Поэтому нам надо ввести регуляризацию, требующую от каждого распределения быть близким к нормальному распределению вокруг нуля координат латентного пространства с дисперсией 1 (наше $P(z)$). \n",
    "\n",
    "В принципе, нам подойдёт любая адекватная мера расстояния между двумя распределениями. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Только KL-дивергенция\n",
    "\n",
    "Что произойдет, если мы будем использовать такой лосс при обучении модели?\n",
    "\n",
    "$$ Loss = - KL(Q(z|x)||P(z)) $$\n",
    "\n",
    "Видим, что мы забываем забываем про декодировщик — он может выдавать все, что угодно. Потому логично ожидать, что обучится только кодировщик и обучится он отражать наши точки в нормальное распределение со средним 0 и дисперсией 1. И все, большего ему в жизни и не надо. Можете проверить это. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/pure_kl_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совмещаем ошибку восстановления и KL-дивергению\n",
    "\n",
    "\n",
    "Поэтому мы должны сохранить исходный лосс — декодировщик штрафуется за то, что не может нормально реконструировать объект. \n",
    "\n",
    "Формально это записывается следующим образом: \n",
    "\n",
    "$$ vae\\_loss = E_{z \\sim Q(z|x)}[logP(x|z)] - KL[Q(z|x)||P(z)]$$\n",
    "\n",
    "Вторая компонента осталась без изменений, а первая — это красиво записанное требование корректно восстанавливать объекты из обучающей выборки и чтобы при этом объекты, полученные их небольшим изменением за счет случайности также восстанавливались в объекты, близкие к объектам из тренировочной выборки. И удовлетворять этой компоненте лосса мы можем за счет того же лосса, который использовали в обычном автоенкодере. \n",
    "\n",
    "Если подставить на место p(z) нормальное распределение, то получим следующее выражение:\n",
    "\n",
    "\n",
    "$$KL(N(\\mu, \\sigma) || N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}$$\n",
    "\n",
    "\n",
    "$$ vae\\_loss = ||x - \\tilde{x}||^2 + 0.5 * (1 + \\log\\sigma^2 - \\mu^2 - \\sigma^2)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Учет обеих компонент позволяет нам получить то, что мы хотели — непрерывное простанство, где нет «дыр» в представлении, и при этом близкие по смыслу объекты расположены рядом, а далекие — далеко. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/kl_repr_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Почему KL(Q||P)?\n",
    "\n",
    "Внимательный слушатель может заметить, что почему-то здесь мы используем на KL(P||Q), как в прошлый раз, а KL(Q||P). Одним из объяснений этого является то, что если Q — распределение, которым мы аппроксимируем реальное, то может минимизация KL(P||Q) и KL(Q||P) ведет к разному результирующему Q. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/KL_inclusive_exclusive.png)\n",
    "\n",
    "Оказывается, чаще всего нам предпочтительнее получать первое поведение, так как лучше не генерировать какую-то часть объектов, чем генерировать объекты, настоящая вероятность которых близка к 0 (обратите внимание на область второго графика, где реальное распределение P жмется к 0, а Q при этом наоборот, достигает максимума)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблемы  «ванильного» VAE\n",
    "\n",
    "Одна из проблем VAE, с которой можно столкнуться, состоит в том, что две компоненты лосса конфликтуют друг с другом. Если будет доминировать KL-loss, то мы получим представление, из которого наши объекты очень плохо восстанавливаются — они раскиданы по представлению, как угодно. \n",
    "\n",
    "Если же наоборот, будем доминировать reconstruction loss, то мы получим ситуацию, в которой объекты восстанавливаются нормально, но никакой непрерывностью и не пахло. \n",
    "\n",
    "Проблема возникает и с самой KL-дивергенцией, у которой есть ряд существенных недостатков. Есть другие способы оценки близости двух распределений, которые порой дают лучшие результаты. К ним относится дивергенция Йенсена — Шеннона, которую мы вскользь затронем далее и метрика Васерштейна (используется в Wasserstein autoencoders), изучение которой выходит за рамки курса. \n",
    "\n",
    "Кроме того, в случае, когда декодировщик содержит значительно больше параметров, нежели кодировщик, может возникать ситуация, при которой сгенерированное латентной представление игнорируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация VAE \n",
    "\n",
    "Давайте реализуем новый энкодер и декодер с ручной вставкой репараметризации и ручным циклом обучения. Начнём с 2 раза большего количества выходов у энкодера.\n",
    "\n",
    "Так как потребуется довольно много ручного кода, то будем двигаться по шагам. Сначала попробуем реализовать ручное обучение, используя только средние значения энкодера и не используя дисперсию и вообще какие-то распределения вероятностей. Для этого воспользуемся `tf.split` и возьмём половину выходов нейронов. Что будет, если взять другую половину?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getDecoder and getEncoder methods are already valid\n",
    "\n",
    "VAE_latent_size = 24\n",
    "\n",
    "VAE_encoder = getEncoder(VAE_latent_size * 2)\n",
    "VAE_decoder = getDecoder(VAE_latent_size)\n",
    "\n",
    "VAE_input = Input(shape=(28, 28, 1)) # Set shape to (28, 28, 1) if autoencoder is convolutional, (784, 1) otherwise\n",
    "VAE_encoder_output = VAE_encoder(VAE_input)\n",
    "VAE_fake_generate, _ = tf.split(VAE_encoder_output, num_or_size_splits=2, axis=1)\n",
    "VAE_fake_output = VAE_decoder(VAE_fake_generate)\n",
    "fake_VAE = Model(inputs=VAE_input, outputs=VAE_fake_output)\n",
    "fake_VAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструмент GradientTape\n",
    "Преобразуем нашу выборку в датасет tensorflow. Нам потребуется рассчитывать потери — введём для этого функцию, просто считающую потери по mse, как ранее для автоэнкодера, без какой-то регуляризации.\n",
    "\n",
    "Для ручного обучения в tensorflow есть инструмент GradientTape — лента записи событий, для которых можно просить выдать градиент по обучающимся параметрам. Он сам разбирается как такой градиент построить. Далее градиент уже передаётся оптимизатору. Тот сам разбирается, как ему применить градиент для решения задачи оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train etc are already imported, normalized and expanded for convolutional NN\n",
    "\n",
    "epochs = 10\n",
    "train_size = len(x_train)\n",
    "batch_size = 64\n",
    "test_size = len(x_test)\n",
    "\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(x_train)\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test)\n",
    "                .shuffle(test_size).batch(batch_size))\n",
    "\n",
    "def compute_loss(model, x):\n",
    "  mse = tf.keras.losses.MeanSquaredError()\n",
    "  return mse(model(x), x)\n",
    "\n",
    "#@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "  \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "  This function computes the loss and gradients, and uses the latter to\n",
    "  update the model's parameters.\n",
    "  \"\"\"\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = compute_loss(model, x)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы импортируем модель расчёта времени, чтобы знать сколько длится эпоха. Эпохи это реализуемый вручную цикл вызова расчёта потерь, градиента, оптимизации. В цикле мы будем брать батч за батчем, считать время на эпоху, дополнительно для проверки обучения будем считать потери на всех батчах тестовой выборки и выводить статистику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  start_time = time.time()\n",
    "  for train_x in train_dataset:\n",
    "    train_step(fake_VAE, train_x, tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "  end_time = time.time()\n",
    "\n",
    "  loss = tf.keras.metrics.Mean()\n",
    "  for test_x in test_dataset:\n",
    "    loss(compute_loss(fake_VAE, test_x))\n",
    "  print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {:.3}'\n",
    "        .format(epoch, loss.result(), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пришло время проверить, удалось ли нам вручную проводить обучение и обучить не хуже автоматических инструментов. Для этого сделаем проверку, как для автоэнкодера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_decoded = np.squeeze(fake_VAE.predict(x_test[sample_indices]), axis = 3)\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"Autoencoder decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы смогли получить то же, что и раньше с автоэнкодером. Теперь нам нужно добавить вариации и репараметризацию. Меняются 2 вещи: модель и функция потерь.\n",
    "\n",
    "Меняется модель VAE, так как теперь не половина выходов энкодера — это входы декодера, а все выходы — это параметры для генерации точки в латентном пространстве, а уже она — вход для декодера. Это сложно оформить моделью tensorflow. Для удобной передачи, как раньше в виде объекта, создадим объект VAE для обучения и использования. Этот объект позволит нам получать trainable_variables, которые нужны в обучении.\n",
    "\n",
    "Для начала покажем как энкодер передаёт сигнал декодеру на необученной сети. Для этого нам не нужна репараметризация, ведь мы не будем считать градиенты или обратное распространение ошибки.\n",
    "Цепочка расчёта у нас из 3 звеньев: энкодер, генерация точки в латентном пространстве, передача этой точки на декодер. В конце визуализация, что было на входе и что на выходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getDecoder, getEncoder methods are already valid, and VAE_latent_size already defined\n",
    "\n",
    "# Rebuild encoder and decoder to loose connection with previous model.\n",
    "# But they are essentially the same as before.\n",
    "VAE_encoder = getEncoder(VAE_latent_size * 2)\n",
    "VAE_decoder = getDecoder(VAE_latent_size)\n",
    "\n",
    "# Get probability density parameters (interpret them as such)\n",
    "mean, logvar = tf.split(VAE_encoder(samples_orig), num_or_size_splits=2, axis=1)\n",
    "# Instantiate encoded state statistically\n",
    "z = tf.random.normal(shape=mean.shape) * tf.exp(logvar * .5) + mean\n",
    "# Run decoder with dropping one dimension for visualization\n",
    "samples_decoded = np.squeeze(VAE_decoder(z), axis = 3)\n",
    "\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"Fresh VAE decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь то же самое в виде класса. Если есть способ из входа сделать выход, то назовём его recode(), а для потерь нужно только добавить MSE. Также добавим репараметризацию, как функцию, для наглядности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_larva(tf.keras.Model):\n",
    "    \"\"\"Variational autoencoder larva.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE_larva, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = getEncoder(2 * latent_dim)\n",
    "        self.decoder = getDecoder(latent_dim)\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def recode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decoder(z)\n",
    "        \n",
    "    def compute_loss(self, x):\n",
    "        return self.mse(x, self.recode(x))\n",
    "\n",
    "vael = VAE_larva(VAE_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цикл обучения почти не поменялся. Только вызываем мы теперь методы класса, а не внешние функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, x, optimizer):\n",
    "  \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "  This function computes the loss and gradients, and uses the latter to\n",
    "  update the model's parameters.\n",
    "  \"\"\"\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = model.compute_loss(x)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  start_time = time.time()\n",
    "  for train_x in train_dataset:\n",
    "    train_step(vael, train_x, tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "  end_time = time.time()\n",
    "\n",
    "  loss = tf.keras.metrics.Mean()\n",
    "  for test_x in test_dataset:\n",
    "    loss(vael.compute_loss(test_x))\n",
    "  print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, -loss.result(), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы выучили вариационный автоэнкодер, но без регуляризации. Поэтому пока никаких улучшений ожидать не стоит. Не добавлена еще дивергенция KL в функцию потерь. Но снова можно проверить, что он обучился не хуже предыдущих автоэнкодеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_decoded = np.squeeze(vael.recode(samples_orig), axis = 3)\n",
    "\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"VAE larva decoded x_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    \"\"\"Variational autoencoder finally.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = getEncoder(2 * latent_dim)\n",
    "        self.decoder = getDecoder(latent_dim)\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def recode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decoder(z)\n",
    "        \n",
    "    def compute_loss(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_out = self.decoder(z)\n",
    "        logpx_z = -self.mse(x, x_out)\n",
    "        logpz = self.log_normal_pdf(z, 0., 0.)\n",
    "        logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
    "        return -tf.reduce_mean(10*logpx_z + logpqz - logqz_x)\n",
    "    \n",
    "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
    "        log2pi = tf.math.log(2. * np.pi)\n",
    "        return tf.reduce_sum(\n",
    "            -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "            axis=raxis)\n",
    "\n",
    "    \n",
    "vae = VAE(VAE_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути наша новая функция потерь — это потери на некорректное восстановление изображения + потери на неточное попадание `z` в нормальное распределение с центром в нуле и единичной дисперсией — потери на неточное попадание `z` в нормальное распределение со средним и дисперсией, сгенерированных энкодером."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "  start_time = time.time()\n",
    "  for train_x in train_dataset:\n",
    "    train_step(vae, train_x, tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "  end_time = time.time()\n",
    "\n",
    "  loss = tf.keras.metrics.Mean()\n",
    "  for test_x in test_dataset:\n",
    "    loss(vae.compute_loss(test_x))\n",
    "  print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, -loss.result(), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, наконец, что вариационный автоэнкодер работает как автоэнкодер, визуализировав привычным способом перекодированные изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_decoded = np.squeeze(vae.recode(samples_orig), axis = 3)\n",
    "\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"VAE decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы используем размерность латентного пространства 2, то это позволит нам получать распределение классов цифр на плоскости, типа такого:\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/vae_sampling.png\" alt=\"alttext\" style=\"width: 600px;\"/>\n",
    "\n",
    "Это не просто интерполяция по двум направлениям. Тут именно все 10 цифр должны так занять место на плоскости, чтобы плавно перетекать друг в друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторная арифметика\n",
    "\n",
    "В принципе, можно даже в латентном пространстве брать разницу черт написания двух одинаковых цифр, прибавлять к другой цифре, получая в результате цифру, написанную немного по-другому. \n",
    "\n",
    "Подробнее: \n",
    "\n",
    "У нас есть 1, написанная без наклона и 1, написанная с наклоном. \n",
    "И у нас есть 9 без наклона.\n",
    "\n",
    "Вычитаем из латентного кода 1 с наклоном латентный код единицы без наклона и прибавляем к 9. Если все пройдет хорошо — получим девятку с наклоном. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/vector_arithm.png\" alt=\"alttext\" style=\"width: 700px;\"/>\n",
    "\n",
    "Такое можно делать и для других примеров — добавлять людям на изображении очки. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/vector_arithm3.png\" alt=\"alttext\" style=\"width: 400px;\"/>\n",
    "\n",
    "или получать нечто среднее между двумя объектами.\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/vector_arithm2.png\" alt=\"alttext\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Условные автоенкодеры (CAE)\n",
    "\n",
    "### Мотивация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как, используя обычный VAE, сгенерировать картинку с заданной меткой? \n",
    "\n",
    "На самом деле, задача нетривиальна. Как вариант, мы можем понять, в какую область латентного пространства VAE отображает все 0 и затем сэмплировать уже из этой области. \n",
    "\n",
    "Хорошо, а если мы хотим нарисовать 1 тем же почерком, которым нарисована данная нам тройка? В этом случае классический VAE вообще не получится использовать. \n",
    "\n",
    "На самом деле, есть еще одна проблема. Что, если распределение объектов действительно сильно зависит от какой-то дополнительной информации, например, того, какую цифру хотел изобразить человек? Тогда KL-loss будет пытаться «скрестить ежа с ужом» и в результате мы получим очень странное представление и опять же, на границах могут получаться несуществующие в реальности мутанты (если внимательно посмотрите на предыдущую картинку — так и получается). \n",
    "\n",
    "\n",
    "\n",
    "Продемонстрировать мы это можем на модельной задаче. Сгенерируем два несвязных набора точек в двумерном пространстве, каждый из которых представляет собой некий паттерн с добавленным шумом. \n",
    "\n",
    "### Несвязные компоненты и условный автокодировщик\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/unconnected_repr.png\" alt=\"alttext\" style=\"width: 600px;\"/>\n",
    "\n",
    "Теперь попробуем обучить обычный автоенкодер. \n",
    "И посмотрим, что выучил автоенкодер. Видим, что он показывает связь там, где она явно отсутствует. Требование получить одно и то же представление для точек из двух паттернов мешают автоенкодеру нормально выучить эти паттерны — они получаются смазанными или даже неверными.\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/bad_latent_unconnected.png\" alt=\"alttext\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что будет, если мы будем передавать в кодировщик и в декодировщик метку объекта? \n",
    "Тогда окажется, что наш автокодировщик работает в разы лучше:\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/good_latent_unconnected.png\" alt=\"alttext\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Условные вариационные автоенкодеры,   CVAE\n",
    "\n",
    "Обычные условные автокодировщики применяются редко — они по-прежнему не гарантируют нам связность представления в пределах одной метки. \n",
    "\n",
    "\n",
    "\n",
    "Однако, добавление меток в вариационный автокодировщик часто помогает решать уже описанные задачи на хорошем уровне. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/cvae.png\" alt=\"alttext\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Генерация заданных цифр из латентного распределения\n",
    "\n",
    "Ниже приведено то, как хорошо выполняют задачу генерации заданной цифры нейросеть по мере обучения и то, как выглядит латентное представление объектов, относящихся к данной цифре. \n",
    "\n",
    "Для 3:\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/gen_cvae_3.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/lat_cvae_3.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "Для 7:\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/gen_cvae_7.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "<td> <img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/lat_cvae_7.gif\" alt=\"alttext\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация заданных цифр из латентного распределения\n",
    "\n",
    "Также успешно такая нейросеть справится в задаче, где мы используем латентное представление одной цифры  для того, чтобы сгенерировать цифру с таким же стилем написания. \n",
    "\n",
    "Чтобы сделать это, достаточно просто получить латентное представление для 7, а затем передать его в декодер с меткой 3.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/style_transfer2.png\" alt=\"alttext\" style=\"width: 600px;\"/>\n",
    "\n",
    "Результаты переноса стилей для нескольких разных 7 представлены ниже.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/style_transfer.png\" alt=\"alttext\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Состязательные автокодировщики (AAE = AE + GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе курса вы уже познакомились с генеративными состязательными сетями. Возникает искушение как-то использовать принципы, лежащие в их основе, для VAE. Действительно, так делают. Такие нейросети называются **adversarial autoencoders**. \n",
    "\n",
    "Использовать мы будем дискриминатор. К какой части нейросети мы можем его «приделать»? На самом деле — к любой. Но самое распространенное — а давайте уберем наши мучения с KL-дивергенцией. Пусть теперь дискриминатор будет отличать латентное представление, которое мы сгенерировали от стандартного нормального распределения. Если может отличить хорошо — то штрафуем енкодер за это.\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/aae.png)\n",
    "\n",
    "Внезапно, это избавляет нас от необходимости даже делать какие-то дополнительные манипуляции с кодировщиком — нам не нужно теперь, чтобы он выдавал средние и дисперсии, а потом мы использовали их для генерации объектов из реального распределения. Нам достаточно того, что дискриминатор не может отличить латентное представление, которое мы получаем от нормального распределения. \n",
    "\n",
    "Более того, оказывается, что AAE может генерировать более «качественные» объекты, нежели ванильный VAE. Можно теоретически показать, что это является следствием того, что он минимизирует не KL-divergence, а более эффективную дивергенцию Йенсена-Шеннона. Однако подробный разбор этого выходит за рамки курса. \n",
    "\n",
    "Кроме того, для AAE легче добиться того, чтобы ваше латентное представление выглядело специфичным образом. Просто генерируем выборку из нужного распределения и говорим дискриминатору пытаться отличать сгенерированную выборку от объектов из латентного представления. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/different_latent_spaces.png)\n",
    "\n",
    "И все это без введения дополнительной лосс-функции! Фактически — GAN и есть наша loss-функция. Мы используем одну нейросеть в качестве лосс-функции для обучения другой нейросети. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение (disentangling) стиля и метки \n",
    "\n",
    "В CVAE мы полагались на то, что если мы передаем нашим нейросетям метку объекта, то на латентном слое эта метка убирается.\n",
    "Что ж, это существенное допущение. Если этого не произойдет, то в нашей сгенерированной 7 будет немного 5 и так далее.\n",
    "\n",
    "А давайте просто сделаем так, чтобы метка отделялась в латентном слое в явном виде. И будем за неверное предсказание метки нейросеть штрафовать. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/disentangle_aae.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что нам это дает? — теперь мы можем использовать стиль объекта отдельно и для простых ситуаций быть уверены, что в стиль объекта не замешалась метка. \n",
    "Для более сложных ситуаций нейросеть может нас и обмануть — мы никак не прописали явно, что в z не должно быть информации о метке. Ну что же, мы можем добавить это требование. А как? Добавим еще нейросеть, которая будет пытаться предсказать метку на основе только вектора z. \n",
    "\n",
    "Если эта нейросеть предсказывает хорошо, то енкодер не избавился от информации о метки и мы его за это штрафуем. \n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/disentagle_aae2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semisupervised AAE\n",
    "А еще мы можем кормить в нашу нейросетку неразмеченные объекты! Для них нейросетка сама будет пытаться определить label. Просто не будем штрафовать нейросеть за неверное предсказание метки (мы же ее и не знаем). А кроме них будем кормить и размеченные. Получаем semisupervised learning. \n",
    "\n",
    "\n",
    "Чтобы на неразмеченных объектах нейросеть не генерировала мусор, можем потребовать от нее, чтобы метка, которую она генерит приходила из категориального распределения. Как это сделать? Да опять же, добавим дискриминатор, который будет отличать получившееся распределение меток от реального ([категориального](https://en.wikipedia.org/wiki/Categorical_distribution)).\n",
    "\n",
    "![alttext](https://edunet.kea.su/repo/src/L0X_Encoders/img/semi_aae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные материалы \n",
    "\n",
    "### Про unsupervised learning при помощи нейросетей\n",
    "\n",
    "Главы из учебника Гудфеллоу по теме:\n",
    "1. [Representation learning](https://www.deeplearningbook.org/contents/representation.html)\n",
    "2. [Генеративные модели](https://www.deeplearningbook.org/contents/generative_models.html)\n",
    "\n",
    "Про все увеличивающуюся роль unsupervised learning: \n",
    "[Unsupervised Deep Learning - Google DeepMind & Facebook Artificial Intelligence NeurIPS 2018](https://www.youtube.com/watch?v=rjZCjosEFpI)\n",
    "\n",
    "[Лекция по генеративным моделям](https://www.youtube.com/watch?v=5WoItGTWV54)\n",
    "\n",
    "Про проклятье размерности: \n",
    "1. [В целом](https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2)\n",
    "2. [Для классификации](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)\n",
    "3. [Немного другой взгляд](https://towardsdatascience.com/the-curse-of-dimensionality-f07c66128fe1)\n",
    "\n",
    "### Автоенкодеры\n",
    "\n",
    "[Главы из учебника Гудфеллоу по теме](https://www.deeplearningbook.org/contents/autoencoders.html)\n",
    "\n",
    "\n",
    "\n",
    "[Более подробно про PCA и ссылка на его применение для MNIST](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n",
    "\n",
    "[Способы понижения размерности, PCA и разные типы автокодировщиков, лекция Техносферы](https://www.youtube.com/watch?v=W5JLSKcuaQo)\n",
    "\n",
    "[Eigenfaces](https://ieeexplore.ieee.org/document/139758)\n",
    "\n",
    "[Конспект от Andrew Ng по разреженным автокодировщикам](https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf)\n",
    "\n",
    "Получение разреженного автоенкодера при помощи:\n",
    "\n",
    "1. [L1-loss а](https://debuggercafe.com/sparse-autoencoders-using-l1-regularization-with-pytorch/)\n",
    "\n",
    "2. [KL-divergence](https://debuggercafe.com/sparse-autoencoders-using-kl-divergence-with-pytorch/)\n",
    "\n",
    "Удаление шума из\n",
    "1. [изображений](https://debuggercafe.com/autoencoder-neural-network-application-to-image-denoising/)\n",
    "2. [текста](https://debuggercafe.com/denoising-text-image-documents-using-autoencoders/)\n",
    "\n",
    "[Введение в автоенкодеры на kaggle](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)\n",
    "\n",
    "### Вариационные автоенкодеры \n",
    "\n",
    "[Введение в автоенкодеры, вариационные автоенкодеры, PCA](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)\n",
    "\n",
    "Введение в автоенкодеры на Хабрахабре\n",
    "1. [Введение](https://habr.com/ru/post/331382/)\n",
    "2. [Manifold learning и скрытые (latent) переменные](https://habr.com/ru/post/331500/)\n",
    "3. [Вариационные автоэнкодеры (VAE)](https://habr.com/ru/post/331552/)\n",
    "4. [Conditional VAE](https://habr.com/ru/post/331664/)\n",
    "5. [GAN(Generative Adversarial Networks)](https://habr.com/ru/post/332000/)\n",
    "6. [GAN + VAE](https://habr.com/ru/post/332074/)\n",
    "\n",
    "\n",
    "[Оригинальная статья по VAE](https://arxiv.org/abs/1312.6114)\n",
    "\n",
    "[Ali Ghodsi, Лекция по VAE](https://www.youtube.com/watch?v=uaaqyVS9-rM)\n",
    "[Irhum Shafkat, Введение в автоенкодеры, векторная арифметика](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)\n",
    "\n",
    "[Jeremy Jordan, введение в автоенкодеры](https://www.jeremyjordan.me/autoencoders/)\n",
    "[Jeremy Jordan, вариационные автоенкодеры](https://www.jeremyjordan.me/variational-autoencoders/)\n",
    "\n",
    "[Туториал по VAE с arxiv](https://arxiv.org/pdf/1606.05908.pdf)\n",
    "\n",
    "[Еще одно введение в вариационные автоенкодеры](https://livebook.manning.com/book/deep-learning-with-python/chapter-8/)\n",
    "\n",
    "[Туториал по VAE от Google по tensorflow](https://www.tensorflow.org/tutorials/generative/cvae)\n",
    "\n",
    "[Векторная арифметика в VAE при генерации изображений](https://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/)\n",
    "\n",
    "[Генерация анимированных персонажей](https://mlexplained.wordpress.com/category/generative-models/vae/)\n",
    "\n",
    "[Генерация лиц, можно менять пол, заставлять знаменитостей улыбаться](https://towardsdatascience.com/variational-autoencoders-vaes-for-dummies-step-by-step-tutorial-69e6d1c9d8e9)\n",
    "\n",
    "[VAE на pytorch с пояснениями](https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/)\n",
    "\n",
    "\n",
    "[Введение в условные вариационные автоенкодеры](https://ijdykeman.github.io/ml/2016/12/21/cvae.html)\n",
    "\n",
    "[Репозиторий с различными модификациями вариационных автоенкодеров](https://github.com/AntixK/PyTorch-VAE)\n",
    "\n",
    "\n",
    "Взгляд на VAE как на игру с двумя участниками:\n",
    "1. [часть 1](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b)\n",
    "2. [часть 2](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-ii-b80d48512f46)\n",
    "3. [часть 3](https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-iii-d8d56c301600)\n",
    "\n",
    "\n",
    "### KL-дивергенция \n",
    "\n",
    "[Википедия по дивергенции Кульбака-Лейблера](https://ru.wikipedia.org/wiki/Расстояние_Кульбака_—_Лейблера)\n",
    "[Мотивация KL-дивергенции](https://math.stackexchange.com/questions/90537/what-is-the-motivation-of-the-kullback-leibler-divergence)\n",
    "\n",
    "Объяснение проблем и разницы между KL-дивергенцией, дивергенцией Йенсена-Шеннона и расстоянием Вассерштейна:\n",
    "1. [часть 1, проблемы KL-дивергенции. Дивергенция Йенсена-Шеннона](https://www.youtube.com/watch?v=_z9bdayg8ZI) и \n",
    "2. [часть 2, проблемы KL-дивергенции и дивергенции Йенсена-Шеннона. Расстояние Вассерштейна](https://www.youtube.com/watch?v=y8LGAhzCOxQ)\n",
    "\n",
    "\n",
    "### AAE \n",
    "\n",
    "Цикл статей по AAE:\n",
    "1. [Автоенкодеры](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4)\n",
    "2. [Добавление дискриминатора](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9)\n",
    "3. [Разделение стиля и содержания при помощи AAE](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-3-disentanglement-of-style-and-content-89262973a4d7)\n",
    "4. [Semisupervised learning при помощи AAE](https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-4-classify-mnist-using-1000-labels-2ca08071f95)\n",
    "\n",
    "[Примеры AAE на tensorflow](https://github.com/nicklhy/AdversarialAutoEncoder)\n",
    "\n",
    "[Здесь в 6 и 8 лекции тоже можно найти примеры](https://github.com/che-shr-cat/deep-learning-for-biology-hse-2019-course)\n",
    "\n",
    "### Модификации автоенкодеров\n",
    "\n",
    "[Contractive Autoencoders](http://www.icml-2011.org/papers/455_icmlpaper.pdf) - автоенкодеры, родственные denoising autoencoders\n",
    "\n",
    "[Variational losssy autoencoder](https://arxiv.org/pdf/1611.02731.pdf) - один из типов VAE, который пытается решить проблему того, что сильные декодер может игнорировать латентное представление. \n",
    "\n",
    "[$\\beta$- VAE](https://arxiv.org/pdf/1804.03599.pdf) - еще одно возможное улучшение VAE\n",
    "\n",
    "[Wassershtein autoencoders](https://arxiv.org/pdf/1711.01558.pdf)\n",
    "\n",
    "[Concrete autoencoders](https://arxiv.org/abs/1901.09346) - якобы позволяют выделять наиболее важные признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры практического применения \n",
    "\n",
    "\n",
    "1. [Age Progression/Regression - преедсказание того, как будет выглядеть человек в другом возрасте](https:/arxiv/abs/1702.08423)\n",
    "\n",
    "2. [druGAN, генерация новых химических веществ](https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.7b00346)\n",
    "\n",
    "3. [Генерация лекарств, специфически меняющих активность генов человека](https://www.frontiersin.org/articles/10.3389/fphar.2020.00269/full)\n",
    "\n",
    "4. [Генерация ингибиторов определенного белка](https://www.nature.com/articles/s41587-019-0224-x)\n",
    "\n",
    "5. [Получение латентных представлений транскриптомов](https://academic.oup.com/nar/article/48/10/e56/5814052)\n",
    "\n",
    "6. [MethylNet](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-020-3443-8) - Использование метилирования генома для обучения латентного представления, помогающего в предсказании возраста и т.д\n",
    "\n",
    "7. [scVAE](https://academic.oup.com/bioinformatics/article-abstract/36/16/4415/5838187?redirectedFrom=fulltext) - получение данных об экспрессии генов из single cell данных"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Autoencoders lesson.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
