{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков, соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30 признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n",
    "\n",
    "Можно иметь сколь угодно хороший алгоритм для классификации - но до тех пор, пока данные на входе - мусор, на выходе из нашего чудесного классификатора мы тоже будем получать мусор (*garbage in - garbage out*). Давайте разберемся, что конкретно надо сделать, чтобы kNN реально заработал.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer() # load data\n",
    "X = cancer.data # features\n",
    "Y = cancer.target # labels(classes)\n",
    "print(f'X shape: {X.shape}, Y shape: {Y.shape}') \n",
    "print(f'X[0]: \\n {X[0]}') \n",
    "print(f'Y[0]: \\n {Y[0]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько данных в классе `0` и сколько данных в классе `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5)) # set fig size \n",
    "plt.bar(1,Y[Y==1].shape, label=cancer.target_names[0]) # 1 label \n",
    "plt.bar(0,Y[Y==0].shape, label=cancer.target_names[1]) # 0 label\n",
    "plt.title('Class balance') \n",
    "plt.ylabel('Num examples') \n",
    "plt.xticks(ticks=[1,0], labels=['1','0']) \n",
    "plt.legend(loc='upper left') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк в каждой из которой по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.boxenplot(data=pd.DataFrame(X), orient=\"h\", palette=\"Set2\")\n",
    "ax.set(xscale='log', xlim=(1e-4, 1e4), xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы адекватно сравнить данные между собой нам следует использовать нормализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Нормализация, выбор Scaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация — это преобразование данных к неким безразмерным единицам.\n",
    "Ключевая цель нормализации — приведение различных данных в самых разных единицах измерения и диапазонах значений к единому виду, который позволит сравнивать их между собой.\n",
    "\n",
    "Главное условие правильной нормализации — все признаки должны быть равны в возможностях своего влияния.\n",
    "\n",
    "Например, у нас есть данные по группе людей: *возраст* (в годах) и *размер дохода* (в рублях). Возраст может измениться в диапазоне от 18 до 70 ( интервал 70-18 = 52). А доход от 30 000 р до 500 000 р (интервал 500 000 - 30 000 = 470 000). В таком варианте разница в возрасте имеет меньшее влияние, чем разница в доходе. Получается, что доход становится более важным признаком, изменения в котором влияют больше при сравнении схожести двух людей.\n",
    "\n",
    "Должно быть так, чтобы максимальные изменения любого признака в «основной массе объектов» были одинаковы. Тогда потенциально все признаки будут равноценны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось определиться с выбором инструмента, часто используют следующие варианты: `MinMaxScaler`, `StandardScaler`, `RobustScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для признака `data[:,0]`. **Обратите внимание на ось X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # setting the initialization parameter for random values\n",
    "\n",
    "# generate random values from 1 to 255, shape (30,1)\n",
    "test = X[:,0].reshape(-1,1)\n",
    "\n",
    "plt.figure(1, figsize=(30, 5))  \n",
    "plt.subplot(141)  # set location\n",
    "plt.scatter(test, range(len(test)), c=Y)  \n",
    "plt.ylabel(\"Num examples\", fontsize=15)  \n",
    "plt.xticks(fontsize=15)  \n",
    "plt.yticks(fontsize=15)  \n",
    "plt.title(\"Non scaled data\", fontsize=18)  \n",
    "\n",
    "# scale data with MinMaxScaler\n",
    "test_scaled = MinMaxScaler().fit_transform(test)  \n",
    "plt.subplot(142)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"MinMaxScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with StandardScaler\n",
    "test_scaled = StandardScaler().fit_transform(test)  \n",
    "plt.subplot(143)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"StandardScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with RobustScaler\n",
    "test_scaled = RobustScaler().fit_transform(test)  \n",
    "plt.subplot(144)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"RobustScaler\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные в диапазоне от 0 до 1. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (e.g., масштабирование RGB пикселей)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$z=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "$X_{min}$ и $X_{max}$ задаются как минимальное и максимальное допустимое значение, по умолчанию:  $X_{min}=0$  и $X_{max}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение 0 и стандартное отклонение 1. Большинство значений будет в  диапозоне от -1 до 1. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-u}{s}$$\n",
    "\n",
    "$u$ — среднее значение (или 0 при `with_mean=False`) и $s$ — стандартное отклонение (или 0 при `with_std=False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И StandardScaler и MinMaxScaler очень чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. Процентиль — мера, в которой процентное значение общих значений равно этой мере или меньше ее. Например, 90 % значений данных находятся ниже 90-го процентиля, а 10 % значений данных находятся ниже 10-го процентиля. Соответственно, RobustScaler не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-X_{median}}{IQR}$$\n",
    "\n",
    "$X_{median}$ — значение медианы, $IQR$ — межквартильный диапазон равный разнице между 75-ым и 25-ым процентилями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нашей задачи по определению раковых опухолей обработаем наши 30 признаков с помощью StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = StandardScaler().fit_transform(X)  # scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что они стали намного более сравнимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_norm).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxenplot(data=pd.DataFrame(X_norm), \n",
    "                   orient=\"h\", \n",
    "                   palette=\"Set2\")\n",
    "ax.set(xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим kNN для общей выборки данных, при разном значении количества соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  # array of the number of neighbors\n",
    "\n",
    "quality = np.zeros(\n",
    "    n_nei_rng.shape[0]\n",
    ")  \n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  # for all elements\n",
    "    # create knn for all num neighbors \n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_nei_rng[ind]\n",
    "    )  \n",
    "    knn.fit(X_norm, Y)  \n",
    "    q = accuracy_score(y_pred=knn.predict(X_norm), y_true=Y)  # accuracy\n",
    "    quality[ind] = q  # fill quality\n",
    "\n",
    "plt.figure(figsize=(8, 5))  \n",
    "plt.title(\"KNN on train\", size=20)  \n",
    "plt.xlabel(\"Neighbors\", size=15)  \n",
    "plt.ylabel(\"Accuracy\", size=15)  \n",
    "plt.plot(n_nei_rng, quality)  \n",
    "plt.xticks(n_nei_rng) \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество на 1 соседе - самое лучшее. Но это и понятно - ближайшим соседом элемента из обучающей выборки будет сам объект. Мы просто **запомнили** все объекты.\n",
    "\n",
    "Если теперь мы попробуем взять какой-то новый образец опухоли и классифицировать его - у нас скорее всего ничего не получится. В таких случаях мы говорим, что наша модель не умеет обобщать (*generalization*).\n",
    "\n",
    "Для того, чтобы знать заранее обобщает ли наша модель или нет, мы можем разбить все имеющиеся у нас данныe на 2 части. Но одной части мы будем обучать классификатор (*train set*), а на другой тестировать насколько хорошо он работает (*test set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data to train/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train_norm = scaler.transform(X_train)  # scaling data\n",
    "X_test_norm = scaler.transform(X_test)  # scaling data\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  \n",
    "train_quality = np.zeros(n_nei_rng.shape[0])  # quality on train data\n",
    "test_quality = np.zeros(n_nei_rng.shape[0])  # quality on test data\n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=n_nei_rng[ind])  \n",
    "    knn.fit(X_train_norm, Y_train)  \n",
    "    \n",
    "    # accuracy on train data\n",
    "    trq = accuracy_score(y_pred=knn.predict(X_train_norm), y_true=Y_train)  \n",
    "    train_quality[ind] = trq  \n",
    "\n",
    "    # accuracy on test data\n",
    "    teq = accuracy_score(y_pred=knn.predict(X_test_norm), y_true=Y_test)  \n",
    "    test_quality[ind] = teq  \n",
    "\n",
    "# accuracy plot  on train and test data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот, теперь мы видим, что 1 сосед был \"ложной тревогой\". Такие случаи мы называем *переобучением*. Чтобы действительно предсказывать что-то полезное нам надо выбирать число соседей начиная минимум с 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (метод опорных векторов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Отличное видео про SVM от Stat Quest которое все объясняет](https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим одномерный пример. У нас есть данные по массе мышей. Часть из них определена как нормальные, а часть как мыши с ожирением. Чтобы их отделить друг от друга, нам достаточно одного критерия. Мы можем посмотреть на график, и визуально определить предельную массу, после которой мышки будут жирненькими"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    X = np.hstack([np.random.uniform(14,21, total_len//2), \n",
    "                   np.random.uniform(24,33, total_len//2)])\n",
    "    Y = np.hstack([np.zeros(total_len//2), \n",
    "                   np.ones(total_len//2)])\n",
    "    return X,Y\n",
    "\n",
    "def plot_data(X, Y, total_len=40, s=50, threshold=21.5):\n",
    "    ax = sns.scatterplot(x=X, y=np.zeros(len(X)), hue=Y, s=s)\n",
    "    ax.axvline(threshold, color='red', ls='dashed')\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пользуясь нашим простым критерием, попробуем классифицировать каких-то новых мышей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.random.uniform(14,30, 5)\n",
    "\n",
    "def classify(X, threshold=21.5):\n",
    "    y = np.zeros_like(X)\n",
    "    y[X > threshold] = 1\n",
    "    return y\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)\n",
    "ax = plot_data(X_test, classify(X_test), total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если наши мыши находятся тут?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([21.45, 22.5])\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)\n",
    "ax = plot_data(X_test, classify(X_test), total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения нашего классификатора - все четко. Больше порогового значения - значит перевес, меньше - значит нормальные. Но с точки зрения здравого смысла, логичнее было бы классифицировать обоих мышей как нормальных, так как они значительно ближе к нормальным, чем к ожиревшим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вооружившись этим новым знанием, попробуем классифицировать наших отъевшихся мышек по-умному. Возьмем крайние точки в каждом класстере. И в качестве порогового значения будем использовать среднее между ними"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data(total_len=total_len)\n",
    "normal_limit = X[Y==0].max() # extreme point for 'normal'\n",
    "obese_limit = X[Y==1].min() # extreme point for 'obese'\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit]) # separated with mean value \n",
    "\n",
    "X_test = np.array([21.5, 23])\n",
    "ax = plot_data(X, Y, total_len=total_len, threshold=threshold)\n",
    "ax = plot_data(X_test, classify(X_test, threshold=threshold), total_len=total_len, s=300, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать, насколько наша мышь близка к тому, чтобы оказаться в другом классе. Такое расстояние называется **margin**. И оно считается как $\\mathrm{margin} = |\\mathrm{threshold} - \\mathrm{observation}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = np.abs(X_test - threshold)\n",
    "print(margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если мы посчитаем margins для наших крайних точек `normal_limit` и `obese_limit`, мы найдем самое большое возможное значение margin для нашего классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_0 = np.abs(normal_limit - threshold)\n",
    "margin_1 = np.abs(obese_limit - threshold)\n",
    "print(margin_0, margin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой классификатор, мы называем **Maximum Margin Classifier**. Он хорошо работает в случае, когда все данные размечены аккуратно. Теперь рассмотрим более реалистичный пример, где что-то пошло не так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_data(total_len=40):\n",
    "    X = np.hstack([np.random.uniform(14,21, total_len//2), np.random.uniform(24,33, total_len//2)])\n",
    "    Y = np.hstack([np.zeros(total_len//2), np.ones(total_len//2)])\n",
    "    indx = np.where(X == X[Y==1].min())[0]\n",
    "    Y[indx] = 0\n",
    "    s = np.ones_like(X)*50\n",
    "    s[indx] = 300\n",
    "    return X,Y,s\n",
    "\n",
    "total_len = 40\n",
    "X,Y,s = generate_realistic_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len, s=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае, наш **Maximum Margin Classifier** работать не будет. Исходя из этого, мы можем прийти к выводу, что наш классификатор очень чувствителен к выбросам. Давайте подумаем можно ли это как-то исправить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы можем разрешить нашему классификатору ошибаться. Если мы будем использовать в качестве порогового значения не самое крайнее, а следующее за ним - мы промажем в классификации конкретно этой странной точки. Но в целом будем правы. Margin определенный таким образом, называется **Soft margin**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "X, Y, s = generate_realistic_data(total_len=total_len)\n",
    "\n",
    "normal_limit = np.sort(X[Y==0])[-2]\n",
    "obese_limit = np.sort(X[Y==1])[1]\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit])\n",
    "\n",
    "X_test = np.array([20.5, 25])\n",
    "ax = plot_data(X, Y, total_len=total_len, threshold=threshold)\n",
    "ax = plot_data(X_test, classify(X_test, threshold=threshold), total_len=total_len, s=300, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но почему мы решили взять именно следующее значение? Почему не через 2? Откуда мы знаем что это лучший из возможных вариантов? А ни откуда не знаем. Чтобы узнать какой из margins лучше, нам стоит численно это проверить и посчитать, сколько раз мы ошибемся, если возьмем в качестве порогового значения между каждой парой точек. Для этого мы вновь воспользуемся *кросс-валидацией*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "X, Y, s = generate_realistic_data(total_len=total_len)\n",
    "\n",
    "indxs = []\n",
    "accuracies = []\n",
    "thresholds = []\n",
    "\n",
    "X_test, Y_test = generate_data(total_len=total_len)\n",
    "\n",
    "for i in range(total_len//2):\n",
    "    for j in range(total_len//2-1):\n",
    "        normal_limit = np.sort(X[Y==0])[-i]\n",
    "        obese_limit = np.sort(X[Y==1])[j]\n",
    "\n",
    "        threshold = np.mean([normal_limit, obese_limit])\n",
    "        \n",
    "        Y_pred = classify(X_test, threshold=threshold)\n",
    "        accuracy = np.mean(Y_test == Y_pred)\n",
    "        indxs.append((-i,j))\n",
    "        accuracies.append(accuracy)\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "print(f'Accuracy = {np.max(accuracies)*100}%')\n",
    "print(f'Best indexes', indxs[np.argmax(accuracies)])\n",
    "\n",
    "best_treshold = thresholds[np.argmax(accuracies)]\n",
    "print('Best treshhold value %.2f'% best_treshold)\n",
    "\n",
    "ax = plot_data(X_test, classify(X_test, threshold=best_treshold), \n",
    "               total_len=total_len, \n",
    "               s=200, \n",
    "               threshold=best_treshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда для классификатора используется **Soft Margin** - такой классификатор называют **Soft Margin Classifier** или по другому - **Support Vector Classifier**. По сути это уже SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D класификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример, где мы измерили не только вес мышей, но и их длину от хвоста до носа. Мы можем вновь применить наш метод Support Vector Classifier, и теперь классы разделяет не одно пороговое значение (по сути, точка), а линия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import svm\n",
    "\n",
    "def generate_2d_data(total_len=40):\n",
    "    X, Y = make_blobs(n_samples=total_len, centers=2, random_state=42)\n",
    "    X[:,0] += 10 \n",
    "    X[:,1] += 20 \n",
    "    return X, Y\n",
    "\n",
    "def plot_data(X, Y, total_len=40, s=50, threshold=21.5):\n",
    "    ax = sns.scatterplot(x=X[:,0], y=X[:,1], hue=Y, s=s)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g', ylabel='Length, cm');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_2d_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)\n",
    "\n",
    "# Code for illustration, later we will understand how it works\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы добавим еще одно измерение - возраст, мы обнаружим, что наши данные стали трехмерными, а разделяет их теперь не линия, а плоскость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_data(total_len=40):\n",
    "    X, Y = make_blobs(n_samples=total_len, centers=2, random_state=42, n_features=3)\n",
    "    X[:,0] += 10 \n",
    "    X[:,1] += 20 \n",
    "    X[:,2] += 10\n",
    "    return X, Y\n",
    "\n",
    "def plot_data(X, Y, total_len=40, s=50, threshold=21.5):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(xs=X[:,0], ys=X[:,1], zs=X[:,2], c=Y, s=s, cmap='Set1')\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    ax.plot_surface(XX, YY, XX*YY*0.2, alpha=0.2)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g', ylabel='Length, cm', zlabel='Age, days');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_3d_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если бы у нас было 4 измерения и больше (например: вес, длинна, возраст, кровяное давление), то многомерная плоскость которая бы разделяла наши классы - называлась бы **гиперплоскость** (рисовать мы ее, конечно же, не будем). Чисто технически, и точка, и линия - тоже гиперплоскости. Но все же гиперплоскостью принято называть то, что нельзя нарисовать на бумаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные не всегда разделяются так хорошо как в случае нашего мышиного датасета. Например, рассмотрим следующее: у нас есть данные по дозировке лекарства и 2 класса - пациенты, которые поправились, и те, которым лучше не стало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patients_data(total_len=40):\n",
    "    X = np.random.uniform(0,50, total_len)\n",
    "    Y = np.zeros_like(X)\n",
    "    Y[(X > 15) & (X < 35)] = 1\n",
    "    return X, Y\n",
    "\n",
    "def plot_data(X, Y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=X, y=np.zeros(len(X)), hue=Y, s=s)\n",
    "\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Recover', 'Sick'])\n",
    "    ax.set(xlabel='dose, mg');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно мы не можем найти такое пороговое значение, которое будет разделять наши классы на больных и здоровых, а следовательно, и Support Vector Classifier работать тоже не будет.  Для начала, давайте преобразуем наши данные таким образом, что бы они стали 2-х мерными. В качестве значений по оси Y будем использовать дозу возведенную в квадрат (**доза**$^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, Y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=X[0,:], y=X[1,:], hue=Y, s=s)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Recover', 'Sick'])\n",
    "    ax.set(xlabel='Dose, mg');\n",
    "    ax.set(ylabel='Dose$^2$');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X_1, Y = generate_patients_data(total_len=total_len)\n",
    "X_2 = X_1**2\n",
    "X = np.vstack([X_1, X_2])\n",
    "\n",
    "plot_data(X, Y, total_len=40, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем вновь использовать Support Vector Classifier для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, Y, total_len=40, s=50)\n",
    "\n",
    "x = np.linspace(0,50,50)\n",
    "xs = [X[0,:][Y==1].min(), X[0,:][Y==1].max()]\n",
    "ys = [X[1,:][Y==1].min(), X[1,:][Y==1].max()]\n",
    "\n",
    "# Calculate the coefficients.\n",
    "coefficients = np.polyfit(xs, ys, 1)\n",
    "\n",
    "# Let's compute the values of the line...\n",
    "polynomial = np.poly1d(coefficients)\n",
    "y_axis = polynomial(x)\n",
    "\n",
    "# ...and plot the points and the line\n",
    "plt.plot(x, y_axis, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут возникает резонный вопрос - почему мы решили возвести в квадрат? Почему не в куб? Или наоборот не извлечь корень? Как нам решить какое преобразование использовать?\n",
    "\n",
    "И у нас есть **вторая проблема** - а если перейти надо в пространство очень большой размерности? В этом случае наши данные очень сильно увеличатся в размере.\n",
    "\n",
    "Комбинация двух проблем дает нам **много радости** - надо перебирать большое число возможных пространств большей размерности\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Однако основная фишка Support Vector Machine состоит в том, что внутри он работает на скалярных произведениях. И можно эти скалярные произведения считать, **не переходя в пространство большей размерности**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого SVM использует **Kernel Function**. \n",
    "\n",
    "Kernel Function может, например, быть полиномом (**Polynomial Kernel Function**), который имеет параметр $d$ - сколько размерностей выбрать. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L02/svm_kernel_function.png\" width=\"500\">\n",
    "\n",
    "Примеры ядер :\n",
    "\n",
    "* $k(x_i, x_j) = (<x_i, x_j> + c)^d, с, d \\in \\mathbb{R}$ - полиномиальное ядро, считает расстояние между объектами в пространстве размерности d\n",
    "\n",
    "* $k(x_i, x_j) = \\frac{1}{z} e^{-\\frac{h(x_i, x_j)^2}{h}}$ - радиальная базисная функция RBF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, в случае SVM можно легко перебрать много таких пространств на кроссвалидации и выбрать более удобное. \n",
    "\n",
    "Более того, SVM может проверять пространства признаков бесконечного размера. Если для такого пространства существует kernel function. Иногда такие пространства оказываются очень удобными для решения задач. Часто используют тот же RBF-пространство, приведенное выше. А оно как раз бесконечно-мерное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{blue}{\\text{*Не обязательное задание:}}$\n",
    "\n",
    "Поставлю дополнительно 20 баллов тому, кто графически оформит результаты этого эксперимента и напишет вывод о зависимости качества обучения от learning_rate и batch_size (возможно, нужно будет добавить варианты их сочетаний и/или увеличить количество эпох до 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "file_exists = os.path.exists(\"/content/cifar-10-batches-py\")\n",
    "if file_exists == False:\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !tar -xzf cifar-10-python.tar.gz   \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "X_train = np.zeros((0, 3072))\n",
    "Y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    X_train = np.append(X_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    Y_train = np.append(Y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "X_test = np.array(test[b\"data\"])\n",
    "Y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_eng = [\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_train shape: {X_test.shape}, Y_train shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LinearClassifier():\n",
    "    def __init__(self, labels, batch_size, random_state=42):\n",
    "        self.labels = labels  # classes names\n",
    "        self.classes_num = len(labels)  # num of classes\n",
    "\n",
    "        np.random.seed(\n",
    "            random_state\n",
    "        )  \n",
    "        self.W = (\n",
    "            np.random.randn(3073, self.classes_num) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias \n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, X_train, Y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # обнуляем loss\n",
    "        train_len = X_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)  \n",
    "\n",
    "        for i in range(\n",
    "            0, train_len, self.batch_size\n",
    "        ):  \n",
    "            idx = indexes[\n",
    "                i : i + self.batch_size\n",
    "            ]  # \n",
    "            X_batch = X_train[idx]  \n",
    "            Y_batch = Y_train[idx]  \n",
    "\n",
    "            X_batch = np.hstack(\n",
    "                [X_batch, np.ones((X_batch.shape[0], 1))]\n",
    "            )  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(X_batch, Y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        current_batch_size = X.shape[0]  # batch_size\n",
    "        loss = 0.0  \n",
    "        dW = np.zeros(self.W.shape) \n",
    "        for i in range(current_batch_size):  \n",
    "            scores = X[i].dot(\n",
    "                self.W\n",
    "            )  # vector of shape 10\n",
    "            correct_class_score = scores[\n",
    "                int(Y[i])\n",
    "            ]  \n",
    "            above_zero_loss_count = 0  \n",
    "            for j in range(self.classes_num): \n",
    "                if j == Y[i]:  # predict class\n",
    "                    continue\n",
    "                margin = scores[j] - correct_class_score + 1  # loss\n",
    "                if margin > 0:  \n",
    "                    above_zero_loss_count += (\n",
    "                        1  \n",
    "                    )\n",
    "                    loss += margin  # \n",
    "                    dW[:, j] += X[i]  # \n",
    "            dW[:, int(Y[i])] -= above_zero_loss_count * X[i]  \n",
    "        loss /= current_batch_size  \n",
    "        dW /= current_batch_size  \n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = np.append(X, 1)  # add 1 (bias)\n",
    "        scores = X.dot(self.W)  \n",
    "        return np.argmax(scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def validate(model, X_test, Y_test, noprint=False):\n",
    "    correct = 0  \n",
    "    for i, img in enumerate(X_test):  \n",
    "        index = model.forward(img)  \n",
    "        correct += (\n",
    "            1 if index == Y_test[i] else 0\n",
    "        )  \n",
    "        if noprint is False:  \n",
    "            if i > 0 and i % 1000 == 0:  \n",
    "                print(\n",
    "                    \"Accuracy {:.3f}\".format(correct / i)\n",
    "                )  \n",
    "    return correct / len(Y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How learning quality depends of speed:\")\n",
    "\n",
    "for lr in [1e-2, 1e-8]:\n",
    "    for bs in [256, 2048]:\n",
    "\n",
    "        print(\"-\" * 50, \"\\n\", \"learning_rate =\", lr, \"\\tbatch_size =\", bs)\n",
    "        print()\n",
    "        lc_model = LinearClassifier(labels_eng, batch_size=bs)\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(10):\n",
    "            loss = lc_model.fit(X_train, Y_train, learning_rate=lr)\n",
    "            accuracy = validate(lc_model, X_test, Y_test, noprint=True)\n",
    "            if best_accuracy < accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_epoch = epoch\n",
    "            print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть MSE-loss\n",
    "\n",
    "$$loss = (y - \\hat{y})^2 $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta (y - \\hat{y})^2 } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = 2(y-\\hat{y}) \\cdot -1 = 2 (\\hat{y} - y)$$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = 2 x \\cdot (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE-loss\n",
    "$$MSE = \\frac 1 N \\sum_i(y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "$y_i$ - константы\n",
    "$\\hat{y_i}$ - не являются функциями друг от друга \n",
    "$$\\hat{y} = wx_i + b $$\n",
    "\n",
    "$$\\frac {\\delta MSE} {\\delta w} = \\frac 1 N \\sum \\frac {\\delta (y_i - \\hat{y_i}) ^2} {\\delta \\hat{y_i}} \\frac {\\delta \\hat{y_i}} {\\delta w}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть MAE-Loss\n",
    "\n",
    "$$loss = |y - \\hat{y}| $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}}$$\n",
    "\n",
    "Строго говоря, у модуля не существует производной в 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X = [i for i in range(-5, 6)]\n",
    "Y = [abs(i) for i in range(-5, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X, Y, label=\"y = |x|\")\n",
    "plt.title(\"y = |x|\", size=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы можем сказать, что в этой точке производная равна 0.\n",
    "Если аргумент модуля меньше 0, то производная будет -1.\n",
    "\n",
    "Если больше +1\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} = - sign(y - \\hat{y}) =  sign(\\hat{y} - y)$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i for i in range(-5, 1, 1)]\n",
    "Y = [i * 0 - 1 for i in range(6)]\n",
    "X_1 = [i for i in range(0, 6)]\n",
    "Y_1 = [i * 0 + 1 for i in range(0, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X, Y, \"b\")\n",
    "plt.plot(X_1, Y_1, \"b\")\n",
    "plt.plot(0, 0, \"ro\")\n",
    "plt.plot(0, 1, \"bo\")\n",
    "plt.plot(0, -1, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Loss\n",
    "\n",
    "$$b = max(x, y)$$\n",
    "\n",
    "$$b = x~~if~~x > y~~else~~y$$\n",
    "\n",
    "\n",
    "$$\\frac {\\delta b} {\\delta x} =  \\frac {\\delta x} {\\delta x}~~if~~x > y~~else~~\\frac {\\delta y} {\\delta x} = 1~~if~~x > y~~else~~0$$\n",
    "\n",
    "Если $x > y$, то он оказал влияние на $b$. Иначе, его вклада в $b$ НЕ БЫЛО - градиент равен 0\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из: \n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем:\n",
    "\n",
    "$ \\nabla_W\tL(W) = {1 \\over N}\\sum_{i=1}^N \\nabla_W L_i(x_i, y_i, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Сложность модели__ (*model complexity*) &mdash; важный гиперпараметр. В частности, для линейной модели, сложность может быть представлена количеством параметров, для полиномиальных моделей &mdash; степенью полинома, для деревьев решений &mdash; глубиной дерева и т.д.\n",
    "\n",
    "Сложность модели тесно связана с __ошибкой обобщения__ (_generalization error_). Ошибка обобщения отличается от ошибки обучения, измеряемой на тренировочных данных, тем, что позволяет оценить обобщающую способность модели, приобретенную в процессе обучения, давать точные ответы на неизвестных ей объектах. Cлишком простой модели не будет хватать мощности для обобщения сложной закономерности в данных, что приводит к большой ошибке обобщения, с другой стороны слишком сложная модель также приводит к большой ошибке обобщения за счет того, что в силу своей сложности модель начинает пытаться искать закономерности в шуме, добиваясь большей точности на тренировочных данных, теряя при этом часть обобщающей способности.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L02/model_complexity.png\" width=\"500\"> \n",
    "\n",
    "Проиллюстрируем описанное явление на примере полиномиальной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2*np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "plt.scatter(x, y, s=50, facecolors='none', edgecolors='b', label='noisy data')\n",
    "\n",
    "x_true = np.linspace(0, 2*np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "plt.plot(x_true, y_true, c='lime', label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем аппроксимировать имеющуюся зависимость с помощью полиномиальной модели, используя шумные данные в качестве тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x_train = x.reshape(-1,1)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "for i, degree in enumerate([0,1,3,9]):\n",
    "\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1,1))\n",
    "\n",
    "    fig.add_subplot(2,2,i+1)\n",
    "    plt.plot(x_true, y_plot, c='red', label=f'M={degree}')\n",
    "    plt.scatter(x, y, s=50, facecolors='none', edgecolors='b')\n",
    "    plt.plot(x_true, y_true, c='lime')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель может переобучаться, подстраиваясь под тренировочную выборку. В полиноме степень, и как следствие количество весов &mdash; это гиперпараметр, который можно подбирать на кросс-валидации, однако, когда мы таким образом подбираем сложность модели мы налагаем довольно грубое ограничение на обобщающую способность модели в целом. Вместо этого более разумным было бы оставить модель сложной, но использвать некий ограничитель (__регуляризатор__), который будет заставлять модель отдавать предпочтение выбору более простого обобщения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(9), LinearRegression())\n",
    "model_ridge = make_pipeline(PolynomialFeatures(9), Ridge(alpha=0.1))\n",
    "\n",
    "model.fit(x_train, y)\n",
    "y_plot = model.predict(x_true.reshape(-1,1))\n",
    "\n",
    "model_ridge.fit(x_train, y)\n",
    "y_plot_ridge = model_ridge.predict(x_true.reshape(-1,1))\n",
    "\n",
    "plt.plot(x_true, y_plot, c='red', label=f'M={degree}')\n",
    "plt.plot(x_true, y_plot_ridge, c='black', label=f'M={degree}, alpha=0.1')\n",
    "plt.scatter(x, y, s=50, facecolors='none', edgecolors='b')\n",
    "plt.plot(x_true, y_true, c='lime', label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "poly_coef = model[1].coef_\n",
    "\n",
    "eq = f'y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x'\n",
    "for i in range(2, 10):\n",
    "    eq += f'+{round(poly_coef[i], 2)}*x^{i}'\n",
    "    \n",
    "print('Without regularization: ', eq)\n",
    "\n",
    "poly_coef = model_ridge[1].coef_\n",
    "\n",
    "eq = f'y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x'\n",
    "for i in range(2, 10):\n",
    "    eq += f'+{round(poly_coef[i], 2)}*x^{i}'\n",
    "\n",
    "print('With regularization: ', eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что одним из \"симптомов\" переобучения являются аномально большие веса. Модель Ridge Regression, показанная в примере выше, использует L2 регуляризуцию для борьбы с этим явлением.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/l2_regularization.jpg\" width=\"700\"> \n",
    "\n",
    "\n",
    "L2 Regularization = weights decay\n",
    "\n",
    "Идея состоит в том, что мы можем наложить некоторое требование на сами веса. Дело в том, что можно получить один и тот же выход модели при разных весах (выход модели соответствует умножению весов на $x$), при разных $w$ выход может быть идентичен.\n",
    "\n",
    "Эти параметры задают некоторую аппроксимацию нашей целевой функции, аппроксимировать функцию можно двумя способами:\n",
    "1. Использовать все имеющиеся данные и провести ее строго через все точки, которые нам известны; \n",
    "2. Использовать более простую функцию: (в данном случае линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым общим закономерностям, которые у них есть. \n",
    "\n",
    "Характерной чертой переобучения является второй сценарий, и сопровождается он, как правило, большими весами. Введение L2-регуляризации приводит к тому, что большие веса штрафуются и предпочтение отдается решениям, использующим малые значения весов. \n",
    "\n",
    "Модель может попробовать схитрить и по-другому - использовать все веса, все признаки, даже незначимые, но с маленькими коэффициентами. С этим L2-loss поможет хуже, так как он не сильнее штрафует мелкие веса. Результатов его применения - малые значения весов, которые использует модель\n",
    "\n",
    "В этом случае на помощь приходит L1-loss, который штрафует вес за сам факт отличия его от нуля. Но и штрафует он все веса одинаково. Результат его применения - малое число весов, которые использует модель в принципе. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/l1_and_l2_regularization.gif\" alt=\"alttext\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "Это лоссы можно комбинировать - получится Elastic Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$\\lambda=$ regularization strength (hyperparameter)\n",
    "\n",
    "$L(W)=\\underbrace{\\frac1N\\sum_{i=1}^NL_i(f(x_i,W),y_i)}_{\\textbf{Data loss} }+\\underbrace {\\lambda R(W)}_{\\textbf{Regularization}}$\n",
    "\n",
    "Берем сумму всех весов по всей матрице $w$, и добавляем ее к loss. Соответственно, чем больше будет эта сумма, тем больше будет суммарный loss. \n",
    "\n",
    "\n",
    "В дальнейшем проблема с переобучением будет вставать довольно часто. Методов регуляризации модели существует достаточно много. Этот — один из базовых, который будет использоваться практически во всех оптимизаторах, с которыми познакомимся позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графиках. Возьмем массив случайных логитов и применим к ним softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_logits = np.linspace(-1,1,50)\n",
    "fig,ax = plt.subplots(ncols=2)\n",
    "\n",
    "ax[0].plot(np.arange(50), rand_logits)\n",
    "ax[0].set_title('Logits')\n",
    "ax[1].plot(np.arange(50), softmax(rand_logits))\n",
    "ax[1].set_title('Softmax')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Начнем с простого примера, пусть у нас есть 10 точек со следующими значениями признака x: \n",
    "\n",
    "`x = [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]`\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L02/cross_entropy_ten_dots.png\">\n",
    "\n",
    "Пусть наши точки принадлежат двум классам: зеленый и красный:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L02/cross_entropy_two_classes.png\">\n",
    "\n",
    "Перед нами простая задача классификации: по признаку x предсказать класс наших точек. Мы можем переформулировать задачу как нахождение вероятности того, что точка зеленая или красная. В идеальной ситуации для зеленой точки вероятность того, что она зеленая равна 1, в то же время вероятность того, что красная точка &mdash; зеленая должна быть равна 0. \n",
    "\n",
    "Вероятности, которые выдает, обучаемая нами модель, зачастую далеки от идеальной ситуации. В нашем примере сравнить насколько сильно предсказанная вероятность класса отличается от вероятности, выдаваемой \"идеальной моделью\" можно за счет __бинарной кросс-энтропии__ частного случая кросс-энропии.\n",
    "\n",
    "$$H_p(q)=-\\frac{1}{N}\\sum^N_{i=1}y_i\\cdot log(p(y_i))+(1-y_i)\\cdot log(1-p(y_i))$$\n",
    "\n",
    "где $y$ &mdash; метка класса (1 для зеленого, 0 для красного), которую можно также интерпретировать как вероятность, предсказанную \"идеальной моделью\", $p(y)$ &mdash; вероятность того, что точка зеленая, предсказываемая оцениваемой моделью.\n",
    "\n",
    "Какое же отношение энтропия имеет к этой формуле? Давайте углубимся в детали.\n",
    "\n",
    "Поскольку $y$ представляет метку классов точек, то его распределение $q(y)$ выглядит следующим образом:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L02/cross_entropy_distribution.png\">\n",
    "\n",
    "__Энтропия__ &mdash; мера неуверенности, связанная с распределением $q(y)$. Какова была бы мера неуверенности распределения $q(y)$ если бы все точки были зелеными? Так как у нас бы не было сомнений насчет цвета точки (он всегда зеленый), значение энтропии было бы 0. Теперь представим другую ситуацию, пусть у нас поровну точек зеленого и красного цвета. Для нас это наихудшая ситуация, поскольку попытка определить цвет точки по сути представляет случайное угадывание. В этом случае энтропия вычисляется по формуле Хартли:\n",
    "\n",
    "$$H(q)=log(2)$$\n",
    "\n",
    "Мы рассмотрели два крайних случая, но как быть с промежуточными ситуациями? Для этих случаев мы можем использовать формулу Шеннона:\n",
    "\n",
    "$$H(q)=-\\sum^C_{c=1}q(y_c)\\cdot log(q(y_c))$$\n",
    "\n",
    "где C &mdash; количество классов. Нетрудно заметить, что формула Хартли является частным случаем формулы Шеннона.\n",
    "\n",
    "Таким образом, зная истинное распределение случайной величины, мы можем рассчитать его энтропию. А что будет если мы попытаемся аппроксимировать истинное распределение $q(y)$ некоторым другим распределением $p(y)$? Допустим, что наши цветные точки подчиняются этому распределению $p(y)$, также мы знаем, что исходят они из неизвестного нам истинного распределения $q(y)$, если мы посчитаем следующую энтропию, это и будет __кросс-энтропия__:\n",
    "\n",
    "$$H_p(q)=-\\sum^C_{c=1}q(y_c)\\cdot log(p(y_c))$$\n",
    "\n",
    "Если окажется, что распределения $p(y)$ и $q(y)$ совпадают, в этом случае энтропия $H(q)$ и кросс-энтропия $H_p(q)$ также будут совпадать. Однако в реальности такое случается редко и кросс-энтропия бывает больше энтропии истинного распределения\n",
    "\n",
    "$$H_p(q)-H(q)\\geq0$$\n",
    "\n",
    "Разница между кросс-энтропией и энтропией называется __дивергенцией Кульбака-Лейблера__, которая является мерой различия между двумя распределениями:\n",
    "\n",
    "$$D_{KL}(q||p)=H_p(q)-H(q)=\\sum^C_{c=1}q(y_c)\\cdot [log(q(y_c))-log(p(y_c))]$$\n",
    "\n",
    "Это значит, что чем ближе $p(y)$ к $q(y)$, тем меньше будет значение дивергенции Кульбака-Лейблера и, следовательно, меньше значение кросс-энтропии.\n",
    "\n",
    "Таким образом, мы хотим добиться, чтобы модель, которую мы оцениваем порождала $p(y)$ близкое к $q(y)$. Для этих целей мы стремимся __минимизировать кросс-энтропию__.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
