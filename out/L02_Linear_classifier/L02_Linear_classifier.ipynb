{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия L02_Linear_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kwmXvkr7D0Ec",
        "mfi58QI8D0E3",
        "APvC85bND0E_",
        "LnUuUq_OD0Ff",
        "UKKKN7-jD0Fg",
        "2-u6uSzjD0Ft",
        "lewl5XvUD0F5",
        "FjfEWxYVUmIp",
        "bG5SLMDoUmIp"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwmXvkr7D0Ec"
      },
      "source": [
        "## 1 Линейный классификатор\n",
        "\n",
        "Технологии компьютерного зрения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Swq869UmIX"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_Classifier-1.jpg\" width=\"600\">\n",
        "\n",
        "Как выяснилось (Из разбора домашнего задания, на cifar10 получили невысокую точность точность, необходимо ее повышать), лучше работает метрика L1. Она представляет из себя сумму абсолютных разностей между пикселями. Если сравнивать ее с L2 и визуализировать получившиеся границы классов, видно, что они немного различаются. L1 дает более параллельные границы классов осям координат, в то время как у L2 границы получаются более плавными. С этими метриками мы будем сталкиваться часто: и в качестве loss функции, и в качестве регуляризации, поэтому познакомиться с ними полезно. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-WjZzKlD0E0"
      },
      "source": [
        "### Алгоритм k-nearest neighbors\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-KNN.jpg\" width=\"900\">\n",
        "\n",
        "Метод ближайших соседей, в принципе, можно использовать, чтобы создать некоторую очень простую модель, к которой можно обращаться так же, как к сложной. Но чтобы было понятно, что внутри находится обычный питоновский код. В верхней части слайда показан пример кода, как можно все записать в одну строчку. Но проблема заключается в следующем: даже если бы этот код работал более точно, он все равно бы был очень медленным, потому что для определения принадлежности картинки к какому-то классу мы должны сравнить ее со всеми элементами обучающего набора, потому что мы его весь запомнили. Это будет занимать очень много времени, поэтому на практике для классификации такой подход неприемлем. Мы хотим, чтобы обученная модель работала быстро.\n",
        "\n",
        "Тем не менее, метод ближайшего соседа используется в других задачах, где без него обойтись сложно. Например, в задаче распознавания лиц, когда у нас есть большая база данных, где каждому лицу сопоставлен некий вектор признаков. Для того, чтобы понять, есть ли в нашей базе данных лицо человека, который прошел перед камерой, нужно сделать ровно то же самое, что мы делали с изображениями: нужно его сравнить со всеми элементами в этой базе данных. При этом там нет никакого класса, мы будем смотреть по принципу “похож-не похож”. А если фотографий в базе данных несколько, как на примере сверху, где есть фотографии актеров и некоторое количество векторов в виде точек. Если из семи соседей 5 - это фотографии Джеки Чана, то скорее всего, это его фотография. В таких случаях данный метод вполне полезен.\n",
        "\n",
        "Ссылки с примерами эффективной реализации метода:\n",
        "\n",
        "https://github.com/facebookresearch/faiss\n",
        "\n",
        "<a href=\"https://arxiv.org/abs/1603.09320\">HNSW</a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfi58QI8D0E3"
      },
      "source": [
        "## 2 Переобучение и кроссвалидация \n",
        "\n",
        "### 2.1 Переобучение\n",
        "\n",
        "В случае с методом K-Nearest neighbours мы уже столкнулись с тем, что оценивать качество алгоритма на основе выборки, на которой и учился - плохая идея.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPP2jlyhanmE"
      },
      "source": [
        "import sklearn.datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np \n",
        "\n",
        "\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAT1ZJkOfIlu"
      },
      "source": [
        "Загрузим датасет с образцами здоровой и раковой ткани. Надо отличать одни от других по набору признаков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMbrgJNweLX8"
      },
      "source": [
        "cancer = sklearn.datasets.load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3RZ79NwN8kr"
      },
      "source": [
        "cancer.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgmT5j6mbxLZ"
      },
      "source": [
        "X_norm = StandardScaler().fit_transform(X) # normalize data to make all features 'equal'\n",
        "\n",
        "n_nei_rng = np.arange(1, 30)                                                      # Количество соседей\n",
        "quality = np.zeros(n_nei_rng.shape[0])\n",
        "\n",
        "for ind in range(n_nei_rng.shape[0]):\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_nei_rng[ind])                        # Обучение для разного числа соседей\n",
        "    knn.fit(X_norm, y)\n",
        "    q = accuracy_score(y_pred=knn.predict(X_norm), y_true = y)\n",
        "    quality[ind] = q                                                              # Запоминаем результат обучения в список\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"KNN on train\", size=20)\n",
        "plt.plot(n_nei_rng, quality)\n",
        "plt.xticks(n_nei_rng)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqATp9a6dy1Z"
      },
      "source": [
        "Видим, что качество на 1 соседе - самое лучшее. Но это и понятно - ближайшим соседом элемента из обучающей выборки будет сам объект. Мы просто **запомнили** все объекты\n",
        "\n",
        "На новом объекте ошибка может быть катастрофической\n",
        "\n",
        "Поэтому выборку всегда делят на обучение и тест "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCodEP6ndyRb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=777)\n",
        "#scaler = StandardScaler()                                                        # Важность выбора метода скалирования !!!\n",
        "scaler = MinMaxScaler()\n",
        "#scaler = RobustScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_norm = scaler.transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "#X_train_norm = X_train                                                          # Важность нормирования и стандартизации !!!\n",
        "#X_test_norm = X_test\n",
        "\n",
        "n_nei_rng = np.arange(1, 30)\n",
        "train_quality = np.zeros(n_nei_rng.shape[0])\n",
        "test_quality = np.zeros(n_nei_rng.shape[0])\n",
        "\n",
        "for ind in range(n_nei_rng.shape[0]):\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_nei_rng[ind])\n",
        "    knn.fit(X_train_norm, y_train)\n",
        "  \n",
        "    trq = accuracy_score(y_pred=knn.predict(X_train_norm), y_true = y_train)\n",
        "    train_quality[ind] = trq\n",
        "\n",
        "    teq = accuracy_score(y_pred=knn.predict(X_test_norm), y_true = y_test)\n",
        "    test_quality[ind] = teq\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"KNN on train vs test\", size=20)\n",
        "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
        "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
        "plt.legend()\n",
        "plt.xticks(n_nei_rng)\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0dXxdymgnn8"
      },
      "source": [
        "Вот, теперь мы видим, что 1 сосед был \"ложной тревогой\", надо выбирать число соседей начиная минимум с 3 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyiIQ3LaaoWp"
      },
      "source": [
        "### 2.2. Кросс-валидация\n",
        "\n",
        "Каждая модель имеет как ряд **параметров**, которые она оценивает на основе обучающей выборки, так и ряд **гиперпараметров**, которые влияют на то, каким способом модель будет оценивать свои параметры по выборке. \n",
        "\n",
        "В случае KNN параметры, строго говоря, отсутствует - модель просто запоминает объекты обучающей выборки. Особо упорные могут считать их параметрами. \n",
        "\n",
        "А вот гиперпараметры есть, даже несколько групп. Какие? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBDVVIDThnrY"
      },
      "source": [
        "1. число соседей \n",
        "2. функция, которой считаем расстояние между объектами (L2=eucledian, L1=manhattan)\n",
        "3. веса, с которыми складываем метки ближайших соседей\n",
        "4. ...\n",
        "5. признаки! (но об этом с вами поговорим позже)\n",
        "6. сама модель - мы могли выбрать не KNN, а нагуглить что-нибудь другое"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDFV5IieiMpC"
      },
      "source": [
        "Посмотрим еще раз на график, который нарисовали на прошлом шаге. Какое число соседей считать оптимальным? Метрика явно скачет? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXa9STgYh1CE"
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"KNN on train vs test\", size=20)\n",
        "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
        "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
        "plt.legend()\n",
        "plt.xticks(n_nei_rng)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fCXqW8IhYAJ"
      },
      "source": [
        "\n",
        "Не понятно, насколько результат зависит от того, как нам повезло или не повезло с разбиением данных на обучение и тест. Может оказаться так, что для конкретного разбиения хорошо выбрать k=5, а для другого - 7. \n",
        "\n",
        "Кроме того, опять же - фактически, мы сами выступаем в роли модели, которая учит гиперпараметры (а не параметры) под видимую ей выборку. \n",
        "\n",
        "Представим себе, что у нас есть 10000 моделей, полученных подкручиванием разных гиперпараметров (в том числе, выборов просто разного типа модели). Представим, что все эти модели не работают. Вообще. Представим так же, что каждая модель угадывает класс в задаче разделения на два класса с вероятностью 0.5 (и будем считать, что классы у нас сбалансированны - то есть 50% одного класса и 50% другого). \n",
        "Опять же, понятно, что таким модели ничем не лучше подбрасывания монетки. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL2IfBXGlBL5"
      },
      "source": [
        "def guess_model(y_real):\n",
        "    guessed = np.random.choice([True, False], \n",
        "                               size=y_real.shape[0],\n",
        "                               replace=True)\n",
        "    y_predicted = np.zeros_like(y_real)\n",
        "    y_predicted[guessed] = y_real[guessed]\n",
        "    y_predicted[~guessed] = 1 - y_real[~guessed]\n",
        "    return y_predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYZh6D1RlveF"
      },
      "source": [
        "models_num = 10000\n",
        "best_quality = 0.5\n",
        "y_real = np.random.choice([0,1], size=250, replace=True)\n",
        "\n",
        "for i in range(models_num):\n",
        "    y_pred = guess_model(y_real)\n",
        "    q = accuracy_score(y_pred=y_pred, y_true=y_real)\n",
        "    if q > best_quality:\n",
        "        best_quality = q\n",
        "print(best_quality)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr6oVkV9mses"
      },
      "source": [
        "То есть мы перебором всех возможных моделей вполне можем получить для абсолютно бесполезной модели приемлемое качество"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEs3EXGZlFYM"
      },
      "source": [
        "\n",
        "То есть если подбирать гиперпараметры модели на тесте, то:\n",
        "1. можно переобучитьcя, просто на более \"высоком\" уровне. Особенно если гиперпараметров у нее много и все они разнообразны\n",
        "2. нельзя быть уверенным, что выбор параметров не зависит от разбиения на обучение и тест \n",
        "\n",
        "Поэтому мы:\n",
        "\n",
        "1) подбираем гиперпараметры моделей на отдельном датасете, называмым валидационным. Получаем мы его разбиением обучающего датасета на собственно обучающий и валидационный \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-cross-validation.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw0C9LCJD0E7"
      },
      "source": [
        "2) чаще всего делаем несколько таких разбиений по какой-то схеме, чтобы получить уверенность оценок качества для моделей с разными гиперпараметрами - кросс-валидация\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-cross-validation2.png\" width=\"500\">\n",
        "\n",
        "Часто применяется следующий подход, называемый K-Fold кросс-валидацией:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "\n",
        "\n",
        "Берется тренировочная часть датасета, разбивается на части - блоки. Дальше мы будем использовать для проверки первую часть (Fold 1), а на остальных учиться. И так последовательно для всех частей. В результате у нас будут информация о точности для разных фрагментов данных и уже на основании этого можно понять, насколько значение этого параметра, который мы проверяем, зависит или не зависит от данных. То есть если у нас от разбиения точность при одном и том же К меняться не будет, значит мы подобрали правильное К. Если она будет сильно меняться в зависимости от того, на каком куске данных мы проводим тестирование, значит, это просто такие данные.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US7Y3JLYorv3"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "# Рекомендую прочитать о GridSearchCV, KFold\n",
        "# можно в русскоявычной интернете"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6AzQ58Pooa2"
      },
      "source": [
        "model = GridSearchCV(KNeighborsClassifier(), \n",
        "                     cv = KFold(3, \n",
        "                                shuffle=True,\n",
        "                                random_state=555),\n",
        "                     param_grid={\"n_neighbors\": np.arange(1, 20), \n",
        "                                 \"metric\": [\"euclidean\", \"manhattan\"],\n",
        "                                 \"weights\": [\"uniform\", \"distance\"]},\n",
        "                     scoring=\"accuracy\",\n",
        "                     n_jobs=-1)\n",
        "model.fit(X_train_norm, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BLUo-7apxAK"
      },
      "source": [
        "model.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWPdH2x-p8U1"
      },
      "source": [
        "Вот и список оптимальных параметров с точки зрения кросс-валидации\n",
        "\n",
        "Объект GridSearchCV можно использовать как обычную модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hulj3towp4K4"
      },
      "source": [
        "y_pred = model.predict(X_test_norm)\n",
        "print(accuracy_score(y_pred=y_pred, y_true=y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwYYB4xlq3it"
      },
      "source": [
        "Но из него еще можно извлечь дополнительные данные о кроссвалидации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbpYB67Yp7oe"
      },
      "source": [
        "model.cv_results_['mean_test_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_alFEKHbrUNZ"
      },
      "source": [
        "model.cv_results_['std_test_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3VSvik8iNHl"
      },
      "source": [
        "model.cv_results_['params']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhyZioYes5v7"
      },
      "source": [
        "Построим, например, при фиксированных остальных параметрах (равных лучшим параметрам), качество модели на валидации в зависимости от числа соседей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJdNcvqyrY97"
      },
      "source": [
        "selected_means = []\n",
        "selected_std = []\n",
        "n_nei = []\n",
        "for ind, params in enumerate(model.cv_results_['params']):\n",
        "    if params['metric'] == model.best_params_['metric'] and\\\n",
        "       params['weights'] == model.best_params_['weights']:\n",
        "       n_nei.append(params['n_neighbors'])\n",
        "       selected_means.append(model.cv_results_['mean_test_score'][ind])\n",
        "       selected_std.append(model.cv_results_['std_test_score'][ind])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kFFTKNPsVjn"
      },
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.title(f\"KNN CV, {params['metric']}, {params['weights']}\", size=20)\n",
        "plt.errorbar(n_nei,  selected_means, yerr=selected_std, linestyle='None',fmt='-o')\n",
        "plt.xticks(n_nei)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbrWyyMuuQvW"
      },
      "source": [
        "Видим, что на самом деле большой разницы в числе соседей и нет. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MjGsHTKukLS"
      },
      "source": [
        "### Можно ли делать только кросс-валидацию (без теста)\n",
        "\n",
        "Нет, нельзя. Кросс-валидация не до конца спасает от подгона параметров модели под выборку, на которой она проводится.\n",
        "\n",
        "Оценка конечного качества модели должно производиться на отложенной тестовой выборке. \n",
        "\n",
        "Если у вас очень мало данных, можно рассмотреть [вложенную кросс-валидацию](https://weina.me/nested-cross-validation/)\n",
        "\n",
        "Но даже в этом случае надо тогда анализировать поведение вашей модели и тд, чтобы показать, что она учит что-то разумное.\n",
        "\n",
        "Кстати, вложенную кросс-валидацию можно использовать чтобы просто получить более устойчивю оценку поведения модели на тесте"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APvC85bND0E_"
      },
      "source": [
        "## 3. Линейный классификатор"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLpMX45iD0FB"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-04.jpg\" width=\"800\">\n",
        "\n",
        "Давайте подумаем, как избавиться от проблемой со скоростью K-Nearest Neighbors. Реализуя метод ближайшего соседа, мы сравнивали одно изображение со всеми, при этом мы предполагали, что изображения из одного класса будут чем-то похожи друг на друга, и поэтому разница будет меньше. Метод как-то работал. Но в целом нам важно понять, для изображения какого класса похоже наше. \n",
        "\n",
        "Чтобы ускорить этот процесс, мы можем взять весь блок изображений (справа), который относится, например, к машинам, каким-то образом сольем в один массив (усредним). Таким образом, получим некоторый шаблон для класса “автомобиль”. Возможно, это будет работать не слишком здорово, но зато вместо тысяч изображений (в данном случае 5000) появится одно. Возможно, это поможет сильно сэкономить время."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2HSg4S1D0FE"
      },
      "source": [
        "### 3.1 Переход к сравнению с шаблоном\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-05.jpg\" width=\"450\">\n",
        "\n",
        "То есть, идея в следующем: вместо того, чтобы сравнивать каждое изображение со всеми остальными по очереди, будем сравнивать изображения с шаблоном для каждого класса. Их будет всего 10 для данного датасета. Этот подход позволит радикально увеличить скорость. \n",
        "\n",
        "Проверим, что получится из этой идеи. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atPmap2HD0FF"
      },
      "source": [
        "скачиваем CIFAR10 в архиве"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-_OIrAZD0FG"
      },
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xzf cifar-10-python.tar.gz\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh5hzFJSD0FI"
      },
      "source": [
        "Загрузили архив в пямять целиком, используя для этого фрагмент кода с сайта CIFAR10. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUg4UWvID0FL"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "x_train = np.zeros((0,3072))\n",
        "y_train = np.array([])\n",
        "for i in range(1,6):\n",
        "  raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
        "  x_train = np.append(x_train,np.array(raw[b'data']),axis=0)\n",
        "  y_train = np.append(y_train,np.array(raw[b'labels']),axis=0)\n",
        "\n",
        "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
        "x_test = np.array(test[b'data'])\n",
        "y_test = np.array(test[b'labels'])\n",
        "\n",
        "# Load label names. For for convenience only.\n",
        "labels_ru = ['Самолет', 'Автомобиль', 'Птица', 'Кошка', 'Олень', 'Собака', 'Лягушка', 'Лошадь', 'Корабль', 'Грузовик']\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KOfdDyZD0FN"
      },
      "source": [
        "Код классификатора. Идея полностью аналогична NN, но сравнение идет с шаблоном, который генерируется во время обучения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VojZVn9kD0FN"
      },
      "source": [
        "class TemplateBasedClassifier:\n",
        "  def __init__(self):\n",
        "    self.templates = []\n",
        "    meta = unpickle(\"/content/cifar-10-batches-py/batches.meta\")\n",
        "    self.labels= meta[b'label_names']\n",
        "\n",
        "  def fit(self, x_train, y_train):\n",
        "    self.templates = []\n",
        "    for label_num in range(len(self.labels)):\n",
        "      indexes = np.where(y_train == label_num)\n",
        "      mask = np.zeros(len(y_train), dtype=bool)\n",
        "      mask[indexes,] = True\n",
        "      images = x_train[mask]\n",
        "      mn = np.mean(images,0)\n",
        "      self.templates.append(mn)\n",
        "      \n",
        "  def forward(self,x):\n",
        "    distances = np.sum(np.abs(self.templates - x),axis =1)\n",
        "    return np.argmin(distances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t80kWUukD0FP"
      },
      "source": [
        "Проверим, насколько поменялся результат."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szTzoBEVD0FQ"
      },
      "source": [
        "import time\n",
        "def validate(model,x_test,y_test,noprint=False):\n",
        "  correct = 0\n",
        "  for i,img in enumerate(x_test):\n",
        "    index = model.forward(img)\n",
        "    correct += 1 if index == y_test[i] else 0\n",
        "    if noprint is False:\n",
        "      if i > 0 and i % 1000 == 0:\n",
        "        print (\"Accuracy {:.3f}\".format(correct/i))\n",
        "  return correct/len(y_test)  \n",
        "\n",
        "model = TemplateBasedClassifier()\n",
        "start = time.perf_counter()\n",
        "model.fit(x_train,y_train)\n",
        "print(\"Train time\",time.perf_counter() - start)\n",
        "start = time.perf_counter()\n",
        "accuracy = validate(model,x_test,y_test)\n",
        "tm = time.perf_counter() - start\n",
        "print(\"Accuracy {:.2f} Train {:d} /test {:d} in {:.1f} sec. speed {:.2f} samples per second.\".format(accuracy,len(x_train),len(x_test),tm,len(x_test)/tm) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pl83jJbD0FS"
      },
      "source": [
        "Точность упала незначительно — 0.27. Зато скорость возросла более чем в 600 раз!\n",
        "\n",
        "Визуализируем шаблоны:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5_mWcqCD0FT"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (20.0, 20.0)\n",
        "\n",
        "def show_templates(model):\n",
        "    for i, template in enumerate(model.templates):\n",
        "      img = (template/max(template))*255\n",
        "      img = template.reshape(3,32,32).transpose(1,2,0).astype(int)\n",
        "      plt.subplot(1, len(model.labels), i+1)\n",
        "      plt.title(model.labels[i])\n",
        "      imshow(img)\n",
        "\n",
        "show_templates(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whrdsFcUD0FW"
      },
      "source": [
        "Итак, мы придумали, как намного быстрее и эффективнее проводить обучение - для этого мы изготовили специальные шаблоны. В процессе обучения модель стала извлекать из данных какие-то общие черты.\n",
        "Сейчас мы визуализировали получившиеся шаблоны.\n",
        "\n",
        "Теперь у нас есть 10 шаблонов, с которыми мы будем сравнивать изображение и на основании этого делать предсказание. На этих шаблонах можно увидеть очертания объектов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhArsgUvQY7o"
      },
      "source": [
        "### 3.2 Переход к весам\n",
        "----\n",
        "Умножение вместо вычитания - для чего?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1Z-BlVWD0FX"
      },
      "source": [
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-06.jpg\" width=\"450\">\n",
        "\n",
        "Теперь давайте сделаем следующий, пока ничем не обоснованный, ход: оставим всё, как было, но заменим вычитание умножением. Логика в том, что не все пиксели одинаково важны. Вероятно, если на изображении совпадут какие-то пиксели, которые отвечают, например, за глаза кошки, это будет намного важнее, чем фон, который может быть точно таким же у собаки. Здесь будут какие-то важные особенности, которым можно придать больший вес.\n",
        "\n",
        "Если мы распишем в виде формул наложение шаблона на картинку таким образом с умножением, то получается, что мы скалярно перемножаем два вектора. Если один вектор – это картинка, а второй – изображение, если мы их вытянем, у нас получается <a href=\"https://ru.wikipedia.org/wiki/Скалярное_произведение\">скалярное произведение</a> векторов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9WthSbhD0FY"
      },
      "source": [
        "### 3.3 Математическая запись"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL1QSrCbD0FZ"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-08.jpg\" width=\"700\">\n",
        "\n",
        "\n",
        "\n",
        "Обозначим входное изображение как x, а шаблон для первого из классов как $w_0$\n",
        "Элементы пронумеруем подряд 1,2,3 … $n$\n",
        "То есть развернем матрицу пикселей изображения в вектор. \n",
        "\n",
        "На картинке одноканальное изображение.\n",
        "Но если каналов несколько, то принципиально это ничего не изменит: мы можем добавлять их в наш вектор один за одним. \n",
        "И его длина(n) будет  H*W*C где С - количество каналов.\n",
        "\n",
        "Для CIFAR10 n =  32*32*3\n",
        "\n",
        "Тогда результат сравнения изображения с этим шаблоном будет вычисляться по формуле: $x[0]*w0[0] + x[1]*w0[1] + … x[n-1]*w0[n-1]$ \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Yd3u3AD0Fa"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-skalyar.png\" width=\"800\">\n",
        "\n",
        "\n",
        "Этот подход используется очень широко. Во-первых, ровно так работает искусственный нейрон: у него есть набор входов, которые перемножаются на веса и дальше суммируются. Эта простая модель лежит в основе практически всех сложных, которые мы будем рассматривать дальше. То есть внутри мы будем пользоваться скалярным произведением.\n",
        "\n",
        "Сама свертка работает очень похоже: мы тоже накладываем шаблон на некоторую матрицу и перемножаем элементы, затем складываем. Единственное отличие – обычно ядро свертки меньше, чем размер самого изображения. Этот же принцип использует линейный классификатор, который вы, возможно, использовали раньше, может быть просто не для классификации изображений. С помощью аналогичного выражения задаются гиперплоскости, которые разделяют данные. Простейшим его применением является линейная регрессия, где для случая одной переменной можно увидеть все на координатной плоскости.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN4LoyUaD0F6"
      },
      "source": [
        "### 3.4 Пример простой линейной регрессии"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOLANAavD0F6"
      },
      "source": [
        "В этой задаче регрессии мы спрогнозируем успеваемость студента, в зависимости от количества часов, которые он учил материал. Это простая задача линейной регрессии, поскольку она включает всего две переменные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmDuzZCD0F6"
      },
      "source": [
        "!wget http://edunet.kea.su/repo/src/L02_Linear_classifier/datasets/student_scores.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-59wGTQRD0F6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zpgT3vOD0F7"
      },
      "source": [
        "dataset = pd.read_csv('/content/student_scores.csv')\n",
        "print(dataset.shape)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGKq3-pRD0F7"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVgZIvpoirN7"
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-UsD1oxD0F7"
      },
      "source": [
        "%matplotlib inline\n",
        "dataset.plot(x='Hours', y='Scores', style='o')\n",
        "plt.title('Hours vs Percentage')\n",
        "plt.xlabel('Hours Studied')\n",
        "plt.ylabel('Percentage Score')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8tY43T7D0F8"
      },
      "source": [
        "# Preparation data\n",
        "# Remove labels from dataset\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, 1].values\n",
        "\n",
        "# Separate data to train and test parts\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pj20rK-iyBa"
      },
      "source": [
        "type(X), type(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQt2rEulD0F8"
      },
      "source": [
        "# Train LinearRegression from sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0HU-UmqD0F9"
      },
      "source": [
        "%matplotlib inline\n",
        "x_points = np.linspace(min(X_train), max(X_train), 100)\n",
        "y_pred = regressor.predict(x_points)\n",
        "\n",
        "dataset.plot(x='Hours', y='Scores', style='o')\n",
        "plt.plot(x_points, y_pred, label = \"Fit line\")\n",
        "plt.title('Hours vs Percentage')\n",
        "plt.xlabel('Hours Studied')\n",
        "plt.ylabel('Percentage Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHkSnQCsD0F9"
      },
      "source": [
        "# Linear equetion is y = coef * x + intercept\n",
        "print(\"coef =\", regressor.coef_[0])\n",
        "print(\"intercept =\", regressor.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aRXFyaKD0F-"
      },
      "source": [
        "# Make predictions for test part\n",
        "y_pred = regressor.predict(X_test)\n",
        "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "\n",
        "# Show Actual value comparison with Predicted value\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR03CcvEihaY"
      },
      "source": [
        "type(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGjnoDGfD0F-"
      },
      "source": [
        "# Evaluate model\n",
        "from sklearn import metrics\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pprx6hyxD0Fb"
      },
      "source": [
        "### 3.5 Геометрическая интерпретация "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4goW8vkGD0Fd"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-09.jpg\" width=\"750\">\n",
        "\n",
        "\n",
        "http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/\n",
        "\n",
        "\n",
        "Прямая может не проходить через 0, данные могут быть распределены так, что они будут далеко от нуля. Чтобы модель линейного классификатора работала хорошо, мы должны к нашим данным добавить смещение. Это смещение позволит проводить разделяющие плоскости или гиперплоскости в n-мерном пространстве уже не через 0, а некоторым произвольным образом, в зависимости от того, какие будут данные, на которых они обучились. Если у нас есть несколько классов (несколько шаблонов), мы для каждого получаем уравнение kx+b, соответственно, если пространство n-мерное, k тоже будет n-мерным и для каждого объекта этот вектор будет свой. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Мы их можем собрать в матрицу, тогда получится следующее: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvVhELr7D0Fd"
      },
      "source": [
        "### 3.6 Добавление смещения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr9LXZfdD0Fd"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-10.jpg\" width=\"800\">\n",
        "\n",
        "У нас есть матрица коэффициентов, которые мы каким-то образом подобрали, пока ещё не понятно как. Есть вектор, соответствующий изображению. Мы умножаем вектор на матрицу, получаем нашу гиперплоскость для четырехмерного пространства в данном случае. Чтобы оно не лежало в 0, мы должны добавить смещение. И мы можем сделать это после, но можно взять и этот вектор смещения (вектор **b**) просто приписать к матрице **W**.\n",
        "\n",
        "Что будет выходом такой конструкции? Мы умножили матрицу весов на наш вектор, соответствующий изображению, получили некоторый отклик. По этому отклику мы так же, как и при реализации метода ближайшего соседа можем судить: если он больше остальных, то мы предполагаем, что это кошка.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGkNTwIsD0Fa"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-07.jpg\" width=\"800\">\n",
        "\n",
        "\n",
        "Собирая его вместе, получаем какое-то компактное представление, что у нас есть некоторая функция, на вход которой мы подаем изображение, и у него есть параметры (веса). Пока происходит просто умножение вектора на матрицу, в дальнейшем это может быть что-то более сложное, функция будет представлять какую-то более сложную модель. А на выходе ( для классификатора) мы получаем числа, которые интерпретируют уверенность модели в том, что изображение принадлежит к определенному классу.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoOjZbf2D0Fe"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-11.jpg\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "То есть, ничего не поменялось по сравнению с тем, что мы просто сравнивали шаблоны, кроме способа того, как мы их накладываем, то есть менялся знак, добавилось ещё смещение в виде одного числа. На каждый шаблон мы должны ещё запомнить одно число, соответствующее смещению. В целом логика осталась такой же.\n",
        "\n",
        "Соответственно, эти коэффициенты, которые являются весами модели, надо каким-то образом подбирать. Но прежде, чем подбирать коэффициенты, давайте определимся со следующим: как мы будем понимать, что модель работает хорошо или плохо? Ведь она возвращает достатотчно абстрактные числа, которые нужно уметь интерпретировать.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnUuUq_OD0Ff"
      },
      "source": [
        "## 4. Оценка результата"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ownsy-VkO-Vx"
      },
      "source": [
        "https://ru.wikipedia.org/wiki/Метод_опорных_векторов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKKKN7-jD0Fg"
      },
      "source": [
        "### 4.1 Функция потерь SVM-loss\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-12.jpg\" width=\"550\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l9Wae5gD0Fh"
      },
      "source": [
        "По аналогии с тем, что мы уже делали, мы можем сравнивать отклик на ключевой класс  (про который нам известно, что он на изображении, так как у нас есть метка этого класса) с остальными. Соответственно, мы подали изображение кошки и получили на выход вектор. Чем больше уверенность, тем больше вероятность того, что, по мнению модели, на изображении этот класс. Для кошки в данном случае это значение 2.9. Хорошо это или плохо? Нельзя сказать, пока мы не проанализировали остальную часть вектора. Если мы двинемся по нему, мы увидим, что здесь есть значения больше, то есть в данном случае модель считает, что это собака, а не кошка, потому что для собаки значение максимально. \n",
        "\n",
        "На этом можно построить некоторую оценку. Давайте смотреть разницу правильного класса с неправильными. В зависимости от того, насколько уверенность в кошке будет больше остальных, настолько будет наша модель хорошей.\n",
        "\n",
        "Но поскольку нам важна не работа модели на конкретном изображении, а важно оценить ее работу в целом, то эту операцию нужно проделать либо для всего датасета, либо для некоторого пакета, который мы подаем на вход и подсчитываем средний показатель. Этот показатель того, как хорошо работает модель, называется функцией потерь, или loss функцией. Называется она так потому, что она показывает не то, насколько хорошо работает модель, а то, насколько плохо. \n",
        "\n",
        "Дальше будет понятно, почему так удобнее (разница только в знаке).\n",
        "Как это посчитать для всего датасета?\n",
        "\n",
        "Мы каким-то образом считаем ее для конкретного изображения, потом усредняем по всем изображениям."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_djJTSMqD0Fh"
      },
      "source": [
        "Дано: 3 учебных примера, 3 класса. При некотором W баллы f (a, W) = Wx равны:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-13_1.jpg\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvr86y3yD0Fj"
      },
      "source": [
        "Функция потерь показывает, насколько хорош наш текущий классификатор.\n",
        "\n",
        "Дан датасет примеров:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-13_2.png\" width=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH91NrorD0Fl"
      },
      "source": [
        "Где xᵢs изображение и yᵢs метка (число).\n",
        "\n",
        "Потери по набору данных — это среднее значение потерь для примеров:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-13_3.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thbVs-i0D0Fm"
      },
      "source": [
        "Снова возьмём наши обучающие примеры **$x_i$** и метки классов **$y_i$**. Представим прогнозы классификатора (оценки) в виде векторов: **$s = f(xi, W)$**. Для каждого отдельного примера просуммируем значения оценок всех **$y$**, кроме истинной метки **$yi$**. То есть, мы получим сумму значений всех неправильных категорий. \n",
        "\n",
        "Теперь посчитаем разницу между ложными прогнозами и истинной оценкой: **$sj − syi$**; **$j≠yi$**. Если истинная оценка больше, чем сумма всех неправильных прогнозов плюс некоторый дополнительный «зазор» (установим его равным 1) — значит, полученный балл для правильной категории намного превосходит любую ошибочную оценку. Это и будет наша функция потерь.\n",
        "\n",
        "В символьном виде это выглядит так:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM1.png\" width=\"350\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTy708COUmIj"
      },
      "source": [
        "### 4.2 Вычисление SVM loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zutAiRxoD0Fm"
      },
      "source": [
        " Не самая мощная, но достаточно интуитивно понятная loss функция –  SVM loss. Она пришла из классического машинного обучения, из метода опорных векторов. \n",
        " \n",
        "Логика такая: если у нас уверенность модели в правильном классе большая, то модель работает хорошо и loss для данного конкретного примера должен быть равен нулю. Если есть класс, в котором модель уверена больше, чем в правильном, то loss должен быть не равен нулю, а отображать какую-то разницу, поскольку модель сильно ошиблась. При этом есть ещё одно соображение: что будет, если на выходе у правильного и ошибочного класса будут примерно равные веса? То есть, например, у кошки было бы 3.2, а у машины не 5.2, а 3.1. В этом случае ошибки нет, но понятно, что при небольшом изменении в данных (просто шум) скорее всего она появится.\n",
        " \n",
        "\n",
        "\n",
        "То есть модель плохо отличает эти классы. Поэтому можно ввести некоторый зазор, который должен быть между правильным и неправильным ответом. \n",
        "\n",
        "<br>\n",
        "<img src =\"https://kodomo.fbb.msu.ru/FBB/year_20/svm_decision_boundary.png\" width=\"550\">\n",
        "\n",
        "И тоже учитывать его в loss функции: то есть сравнивать его результат для правильного класса не с чистым выходом для другого, а добавить к нему некоторую дельту (в данном случае – 1(единица)). Смотрим: если разница больше 0, то модель работает хорошо и loss равно нулю. Если нет, то мы возвращаем эту разницу и loss будет складываться из этих индивидуальных разниц.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM1.png\" width=\"350\">\n",
        "\n",
        "Ниже пример того, как считается loss.\n",
        "\n",
        "\n",
        "Считаем функцию потерь для 1го изображения:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM1.jpg\" width=\"650\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSxHOlrhD0Fo"
      },
      "source": [
        "Также считаем потери для 2го и 3его изображения:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM2_1.jpg\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRZ4PQ6qD0Fo"
      },
      "source": [
        "Значения losses получились следующие:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM4.png\" width=\"450\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5EXqMc4D0Fo"
      },
      "source": [
        "Считаем среднее значение loss для всего датасета:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM5.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-dHiMMvD0Fs"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-16.jpg\" width=\"450\">\n",
        "\n",
        "Как записать это в коде:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsX3IrCQHtQW"
      },
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "x_train = np.zeros((0,3072))\n",
        "y_train = np.array([])\n",
        "for i in range(1,6):\n",
        "  raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
        "  x_train = np.append(x_train,np.array(raw[b'data']),axis=0)\n",
        "  y_train = np.append(y_train,np.array(raw[b'labels']),axis=0)\n",
        "\n",
        "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
        "x_test = np.array(test[b'data'])\n",
        "y_test = np.array(test[b'labels'])\n",
        "\n",
        "# Load label names. For for convenience only.\n",
        "labels_ru = ['Самолет', 'Автомобиль', 'Птица', 'Кошка', 'Олень', 'Собака', 'Лягушка', 'Лошадь', 'Корабль', 'Грузовик']\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpEaW_81D0Fs"
      },
      "source": [
        "W = np.random.randn(10, 3072) * 0.0001  # For CIFAR10\n",
        "\n",
        "def Li_svm(x,y,W):\n",
        "  y = int(y) # for use as index\n",
        "  scores = W.dot(x)\n",
        "  margins = np.maximum(0,scores - scores[y] + 1 ) # delta = 1\n",
        "  margins[y] = 0 # Что бы не писать лишнее условие\n",
        "  return np.sum(margins)\n",
        "\n",
        "L0 = Li_svm(x_train[0],y_train[0],W)\n",
        "print(\"Loss on first image\", L0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm-wdp2WD0Fq"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-15.jpg\" width=\"900\">\n",
        "\n",
        "Собираем все вместе: считаем loss для всего датасета, усредняя ее.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0mOon7GD0Fp"
      },
      "source": [
        "Как будет выглядеть график этой SVM-loss?\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/hinge_loss.gif\" width=\"400\">\n",
        "\n",
        "\n",
        "Функция потерь для ситуации, когда зазор большой, будет равна 0. Она будет меняться только тогда, когда у нас есть ошибка. Причем будет меняться линейно.\n",
        "\n",
        "Как мы можем использовать эту loss функцию? Допустим, мы знаем, что у нас модель будет работать не очень хорошо (выдает большую ошибку). Можно попытаться найти минимум этой loss функции, то есть подобрать такое W, когда loss функция обратится в 0  (в данном случае), а в общем случае (если какое-то количество данных нельзя обратить в 0), мы можем попытаться найти ее минимум.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-u6uSzjD0Ft"
      },
      "source": [
        "## 5. Обновления весов методом градиентного спуска"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wKUW4qoD0Fv"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-18.png\" width=\"750\">\n",
        "\n",
        "Если переходить на случай n-мерного (в данном случае трехмерного) пространства, здесь достаточно очевидна аналогия с поверхностью: у нас есть поверхность земли, котораяя описывается координатами x, y, z. Если мы движемся по этой поверхности, высота (z) будет зависеть от x, y. \n",
        "\n",
        "x, y - это координаты, мы можем записать их как вектор. Наша функция будет работать с вектором координат и будет выдавать скаляр, третью координату. Если в качестве координат мы будем использовать веса нашей модели, а в качестве z - loss, то аналогия станет полной. Наша задача сведется к тому, чтобы найти такой набор весов, при котором значение функции будет минимально. То есть мы окажемся в каком-то минимуме, где ошибка будет минимально возможна для этих данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "028RIqtOD0Ft"
      },
      "source": [
        "Для того, чтобы предположить, где он может находиться, надо понимать, в какую сторону функция растет, а в какую - убывает. Для этого существует производная.\n",
        "\n",
        "Поскольку у нас функция от нескольких переменных, если мы будем брать от нее производную, у нас получится вектор частных производных, то есть градиент этой функции, который будет показывать, как она меняется в каждом направлении."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uGC_ZtUD0Fw"
      },
      "source": [
        "### Градиент loss функции\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYGPejbaD0Fw"
      },
      "source": [
        "Градиент — вектор, своим направлением указывающий направление наибольшего возрастания некоторой величины **φ**, значение которой меняется от одной точки пространства к другой (скалярного поля), а по величине (модулю) равный скорости роста этой величины в этом направлении.\n",
        "\n",
        "Например, если взять в качестве **φ**  высоту поверхности земли над уровнем моря, то её градиент в каждой точке поверхности будет показывать «направление самого крутого подъёма», и своей величиной характеризовать крутизну склона.\n",
        "\n",
        "Другими словами, градиент — это производная по пространству, но в отличие от производной по одномерному времени, градиент является не скаляром, а векторной величиной."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6slm54uD0Fz"
      },
      "source": [
        "Для случая трёхмерного пространства градиентом скалярной функции **φ=φ(x,y,z)** координат **(x,y,z)** называется векторная функция с компонентами\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/gradient.svg\">\n",
        "\n",
        "\n",
        "Если **φ**  — функция **n** переменных **X1...xn**, то её градиентом называется **n**-мерный вектор\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/gradient4.svg\">\n",
        "\n",
        "компоненты которого равны частным производным **φ** по всем её аргументам.\n",
        "\n",
        "Размерность вектора градиента определяется, таким образом, размерностью пространства (или многообразия), на котором задано скалярное поле, о градиенте которого идёт речь.\n",
        "Оператором градиента называется оператор, действие которого на скалярную функцию (поле) даёт её градиент. Этот оператор иногда коротко называют просто «градиентом»."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSi3eUhmD0F1"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-gradient.png\" width=\"650\">\n",
        "\n",
        "Наша задача будет сводиться к тому, что мы будем искать градиент loss функции по весам, которые будут состоять из производной по каждому направлению. Поскольку у нас здесь числа, можно считать этот градиент численно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfdagLdmD0F2"
      },
      "source": [
        "### 5.1 Численный расчет производной\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-gradient_spusk.png\" width=\"650\">\n",
        "\n",
        "\n",
        "\n",
        "Посчитаем градиент приближенно, воспользовавшись определением (в формуле аргумент обозначен как х у нас же аргументом будет W):\n",
        "На нулевом шаге у нас есть $W_0$ найдем $L_0 = Loss(f(W0,x))$\n",
        "Прибавим к первому элементу $W0[0]$ небольшую величину  $h$ = 0.0001 и получим новую матрицу весов $W_1$ отличающуюся от $W_0$ на один единственный элемент\n",
        "Найдем Loss от $W_1$ : $L_1 = Loss(f(W_1,x))$\n",
        "По определению производной $dL/W_0 = ( L_1 - L_0 ) /  h$\n",
        "Повторяя этот процесс для каждого элемента из W найдем вектор частных производных то есть градиент $dL/dW$.\n",
        "\n",
        "Плюсы:\n",
        "Это просто. \n",
        "\n",
        "Проблемы:\n",
        "\n",
        "1. Это очень долго, нам придется заново искать значение loss функции для каждого $W_i$.\n",
        "\n",
        "2. Это неточно, так как по определению приращение $h$ бесконечно мало, а мы используем конкретное пусть и небольшое число. И если мы сделаем его слишком маленьким то столкнемся с ошибками связанными с округлением в памяти компьютера.\n",
        "\n",
        "\n",
        "Поэтому данный метод может использоваться как проверочный. \n",
        "А на практике вместо него используется **аналитический расчет градиента**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LQIGh5cn4SN"
      },
      "source": [
        "### 5.2 Аналитический расчет производной от SVM Loss\n",
        "\n",
        "#### 5.2.1 Простые производные \n",
        "\n",
        "$$x' = \\frac {\\delta x} {\\delta x} = 1$$ \n",
        "\n",
        "$$(x^2)' = \\frac {\\delta x^2} {\\delta x} = 2x$$\n",
        "\n",
        "$$(\\log x)'  = \\frac {\\delta \\log x} {\\delta x} = \\frac 1 x $$\n",
        "\n",
        "$$(e^x)'  = \\frac {\\delta e^x} {\\delta x} = e^x $$\n",
        "\n",
        "$$\\frac {\\delta cf(x)} {\\delta x}= c \\cdot \\frac {\\delta f(x)} {\\delta x}$$\n",
        "\n",
        "$$\\frac {\\delta f(x) + c} {\\delta x}= \\frac {\\delta f(x)} {\\delta x}$$\n",
        "\n",
        "c - константа, не зависящая от x\n",
        "\n",
        "$$ \\frac {\\delta [f(x) + g(x)]} {\\delta x} = \\frac {\\delta f(x)} {\\delta x}  + \\frac {\\delta g(x)} {\\delta x} $$ \n",
        "\n",
        "\n",
        "$$\\frac {\\delta (x^2 + y^3)} {\\delta x} = 2x  $$\n",
        "так как y по отношению к x - константа и мы меняем только x\n",
        "\n",
        "$$\\frac {\\delta (x^2 + y^3)} {\\delta y} = 3y^2  $$\n",
        "так как x по отношению к y - константа и мы меняем только y\n",
        "\n",
        "$$(e^y)'  = \\frac {\\delta e^y} {\\delta x} = 0 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VY3A2_-n9Jq"
      },
      "source": [
        "#### 5.2.2 Chain-rule\n",
        "\n",
        "Производная функции $f(g)$:\n",
        "\n",
        "$$\\frac {\\delta f} {\\delta g}$$\n",
        "\n",
        "Пусть g  на самом деле не просто переменная, а зависит от h. Тогда производная от f по g **не меняется**, а производная f по h запишется следующим образом:\n",
        "\n",
        "\n",
        "$$\\frac {\\delta f(g(h))} {\\delta h} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h}$$\n",
        "\n",
        "Пусть теперь h зависит от x. Все аналогично\n",
        "\n",
        "\n",
        "$$\\frac {\\delta f(g(h(x)))} {\\delta x} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h} \\frac {\\delta h} {\\delta x}$$\n",
        "\n",
        "Так можно делать до бесконечности, находя производную сколь угодно сложной функции. И, что важно - мы можем считать градиенты частями - посчитать сначала f по g, потом g по h,....\n",
        "\n",
        "$$\\frac {\\delta log(x^2 + 5)} {\\delta x}$$\n",
        "\n",
        "$$h = x^2 + 5$$\n",
        "\n",
        "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac {\\delta log(h)} {\\delta h} \\frac {\\delta h} {\\delta x}$$ \n",
        "\n",
        "$$\\frac {\\delta log(h)} {\\delta h} = \\frac 1 h$$\n",
        "\n",
        "$$\\frac  {\\delta h} {\\delta x} = 2x $$\n",
        "\n",
        "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac 1 {x^2 + 5} \\cdot 2x = \\frac {2x} {x^2 + 5}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f_mRchyn_3a"
      },
      "source": [
        "#### 5.2.3 Часть MSE-loss\n",
        "\n",
        "$$loss = (y - \\hat{y})^2 $$\n",
        "\n",
        "$$\\hat{y} = wx + b $$\n",
        "\n",
        "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
        "\n",
        "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta (y - \\hat{y})^2 } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = 2(y-\\hat{y}) \\cdot -1 = 2 (\\hat{y} - y)$$\n",
        "\n",
        "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
        "\n",
        "$$ \\frac {\\delta loss} {\\delta w} = 2 x \\cdot (\\hat{y} - y) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSsbZ93dD0F3"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### 5.2.4 MSE-loss\n",
        "$$MSE = \\frac 1 N \\sum_i(y_i - \\hat{y_i})^2 $$\n",
        "\n",
        "$y_i$ - константы\n",
        "$\\hat{y_i}$ - не являются функциями друг от друга \n",
        "$$\\hat{y} = wx_i + b $$\n",
        "\n",
        "$$\\frac {\\delta MSE} {\\delta w} = \\frac 1 N \\sum \\frac {\\delta (y_i - \\hat{y_i}) ^2} {\\delta \\hat{y_i}} \\frac {\\delta \\hat{y_i}} {\\delta w}$$\n",
        "\n",
        "\n",
        "#### 5.2.4 Часть MAE-Loss\n",
        "\n",
        "$$loss = |y - \\hat{y}| $$\n",
        "\n",
        "$$\\hat{y} = wx + b $$\n",
        "\n",
        "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
        "\n",
        "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
        "\n",
        "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}}$$\n",
        "\n",
        "Строго говоря, у модуля не существует производной в 0. \n",
        "\n",
        "![altext](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Absolute_value.svg/1200px-Absolute_value.svg.png)\n",
        "\n",
        "Но мы можем сказать, что в этой точке производная равна 0.\n",
        "Если аргумент модуля меньше 0, то производная будет -1. Если больше - +1\n",
        "\n",
        "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} = - sign(y - \\hat{y}) =  sign(\\hat{y} - y)$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src =\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Signum_function.svg/1200px-Signum_function.svg.png\" width=\"500\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jplvZ8bMpZLE"
      },
      "source": [
        "#### 5.2.5 2-Max-Loss\n",
        "\n",
        "$$b = max(x, y)$$\n",
        "\n",
        "$$b = x~~if~~x > y~~else~~y$$\n",
        "\n",
        "\n",
        "$$\\frac {\\delta b} {\\delta x} =  \\frac {\\delta x} {\\delta x}~~if~~x > y~~else~~\\frac {\\delta y} {\\delta x} = 1~~if~~x > y~~else~~0$$\n",
        "\n",
        "Если x > y, то он оказал влияние на b. Иначе его вклада в b НЕ БЫЛО - градиент равен 0\n",
        "\n",
        "\n",
        "#### 5.2.6 k-Max-Loss\n",
        "\n",
        "Аналогично. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgyEDTOJpgPN"
      },
      "source": [
        "#### 5.2.7 SVM-Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIO83d88D0F3"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g1.png\" width=\"750\"> \n",
        "\n",
        "\n",
        "Распишем формулу для $L_i$ через скалярное произведение.\n",
        "Затем раскроем сумму.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJGNCSzOUmIm"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g2.png\" width=\"750\"> \n",
        "\n",
        "Считаем производную по первому элементу матрицы **W**. \n",
        "Если у нас правильный класс - не первый элемент, то он больше нигде не встретится, кроме как в первой сумме. Поэтому все остальные суммы при подсчете этой частной производной будут константами. Поэтому этот градиент будет равен либо $x_i$,  либо 0 (если эта сумма меньше 0). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkyljGmPUmIn"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g3.png\" width=\"750\"> \n",
        "\n",
        "Аналогично для  $w_i$ частные производные будут равны $x_i$ либо нулю, когда выражение больше меньше 0, \n",
        "кроме случая: $j == yi$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN1WsFP5UmIn"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g4.png\" width=\"750\"> \n",
        "\n",
        "\n",
        "Эта ситуация для “правильного изображения”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beMBCLQuUmIn"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g5.png\" width=\"750\">  \n",
        "\n",
        "Вот финальный результат.\n",
        "Общие потери считаются по пакету изображений.\n",
        "“1” в формулах  — это “if”.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6THqjMLD0F4"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class LinearClassifier:\n",
        "  def __init__(self, labels, batch_size, random_state=777):\n",
        "    self.labels = labels\n",
        "    self.classes_num = len(labels)\n",
        "    # Generate a random weight matrix of small numbers\n",
        "    # Number of weights changed from 3072 to 3073 for implement bias trick\n",
        "    np.random.seed(random_state)\n",
        "    self.W = np.random.randn(3073, self.classes_num) * 0.0001\n",
        "    self.batch_size = batch_size #256\n",
        "    \n",
        "\n",
        "  def fit(self, x_train, y_train, learning_rate = 1e-8):\n",
        "    loss = 0.0\n",
        "    train_len = x_train.shape[0]\n",
        "    indexes = list(range(train_len))\n",
        "    random.shuffle(indexes)\n",
        "\n",
        "    for i in range(0, train_len, self.batch_size):\n",
        "      # Split data into batches \n",
        "      idx = indexes[i:i+self.batch_size]\n",
        "      x_batch = x_train[idx]\n",
        "      y_batch = y_train[idx]\n",
        "\n",
        "      # Bias trick\n",
        "      x_batch = np.hstack([x_batch, np.ones((x_batch.shape[0], 1))])\n",
        "\n",
        "      # Weights update\n",
        "      loss_val, grad = self.loss(x_batch, y_batch)\n",
        "      self.W -= learning_rate*grad\n",
        "\n",
        "      loss += loss_val\n",
        "    return loss/(train_len)\n",
        "\n",
        "  def loss(self,x, y):\n",
        "    current_batch_size = x.shape[0]\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(self.W.shape) # Grad\n",
        "    for i in range(current_batch_size):\n",
        "      scores = x[i].dot(self.W)\n",
        "      correct_class_score = scores[int(y[i])]\n",
        "      above_zero_loss_count = 0\n",
        "      for j in range(self.classes_num):\n",
        "        if j == y[i]:\n",
        "          continue\n",
        "        margin = scores[j] - correct_class_score + 1 # note delta = 1\n",
        "        if margin > 0:\n",
        "          above_zero_loss_count +=1\n",
        "          loss += margin\n",
        "          dW[:,j] += x[i] # We summarize it because grad computed over a batch and will de divided by number of examples in this batch\n",
        "      dW[:,int(y[i])] -= above_zero_loss_count*x[i]\n",
        "    loss /= current_batch_size\n",
        "    dW /= current_batch_size\n",
        "    return loss, dW\n",
        "      \n",
        "  def forward(self,x):\n",
        "    x = np.append(x, 1)\n",
        "    scores = x.dot(self.W)\n",
        "    return np.argmax(scores) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wIcTw0Wb3QD"
      },
      "source": [
        "#### Не обязательное задание:\n",
        "----\n",
        "Поставлю дополнительно 20 баллов тому, кто графически оформит результаты этого эксперимента и напишет вывод о зависимости качества обучения от learning_rate и batch_size (возможно, нужно будет добавить варианты их сочетаний и/или увеличить количествоэпох до 20)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFUO0_AtD0F4"
      },
      "source": [
        "print('Исследуем зависимость качества обучения от скорости:')\n",
        "\n",
        "for lr in [1e-2, 1e-8]:\n",
        "  for bs in [256, 2048]:\n",
        "\n",
        "    print('-'*50, '\\n', 'learning_rate =', lr, '\\tbatch_size =', bs)\n",
        "    print()\n",
        "    lc_model = LinearClassifier(labels_ru, batch_size = bs)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(10):\n",
        "      loss = lc_model.fit(x_train, y_train, learning_rate = lr) # исследуем зависимость качества обучения от скорости\n",
        "      accuracy = validate(lc_model,x_test,y_test,noprint=True)\n",
        "      if best_accuracy < accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_epoch = epoch\n",
        "      print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
        "\n",
        "    print()\n",
        "    print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zemRk7zED0F5"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-change_weight.png\" width=\"450\"> \n",
        "\n",
        "https://docs.google.com/file/d/0Byvt-AfX75o1ZWxMRkxrUFJ2ZUE/preview\n",
        "\n",
        "Идея в следующем: у нас есть некоторая поверхность, на которой будет точка минимума функции. Мы должны обновлять веса таким образом, чтобы смещаться в сторону этой точки. Loss показывает, как возрастает функция потерь. Так как потери - это плохо, мы должны их минимизировать, то есть мы должны уменьшать веса в сторону, обратную росту этой функции."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgu-syMzUmIo"
      },
      "source": [
        "### 5.3 Выбор шага обучения\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-learning_rate.png\" > \n",
        "\n",
        "Шаг обучения - некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. \n",
        "У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который позволит ее проскочить, но в то же время чтобы тот же процесс не шел слишком медленно (как на графике слева). \n",
        "\n",
        "Пока изменения loss функции достаточно большие, сам по себе градиент тоже большой. За счет этого можно двигаться быстро. Когда он будет уменьшаться, мы должны оказаться рядом с этим минимумом, и шаг, который мы выбрали, не должен мешать этому процессу. \n",
        "\n",
        "Бывают ситуации, когда шаг можно менять в самом процессе обучения, когда в начале обучения модели шаг большой, потом, по мере того, как она сходится, чтобы более четко найти минимум, шаг можно изменить вручную. Но изначально его нужно каким-то образом подобрать, и это зависит от данных и от самой модели. Это тоже гиперпараметр, связанный с обучением."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNuQg6uMNrW-"
      },
      "source": [
        "### 5.4 Выбор размера пакета"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9UL1OoGUmIo"
      },
      "source": [
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-gd.png\" width=\"700\"> \n",
        "\n",
        "Мы с самого начала говорили о пакетах. При этом было не очень понятно, чем они мотивированны. Точнее, мы мотивировали это тем, что у нас много данных и мы не сможем их обработать все, и это правда. Даже если мы сможем загрузить все данные в память, нам нужно будет загрузить их и использовать при рассчете в том числе градиента. Это ещё более затратно. \n",
        "\n",
        "Зачем он нужен при рассчете loss? Если мы посчитаем loss по одному изображению, скорее всего он будет очень специфичен, и это движение, которое произойдет, будет направлено в сторону минимума, потому что по этому конкретному изображению мы улучшим показатели.\n",
        "\n",
        "Нас в первую очередь интересует более общее направление, которые будут работать на большинстве наших данных. Поэтому если мы будем оптимизировать модели, исходя из одного элемента данных, путь будет витиеватый, и процесс будет происходить достаточно долго. Если же данные не помещаются в память, то **можно использовать батч того размера, который у нас есть**. Можно загрузить в память и считать по этим мини-батчам градиент. В этом случае спуск будет более плавным, чем по одному изображению. \n",
        "\n",
        "Также при использовании всего датасета тоже есть свои минусы. **Не всегда загрузка всего датасета приводит к увеличению точности**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLSG33OtwZ0P"
      },
      "source": [
        "#### Важно !!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK88ZRfOvlku"
      },
      "source": [
        "Подробнее: \n",
        "* https://habr.com/ru/post/318970/\n",
        "* https://alxmamaev.medium.com/deep-learning-на-пальцах-часть-1-341cfe021ef9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0aQc7adUmIo"
      },
      "source": [
        "### 5.5 Регуляризация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzCnojSmUmIo"
      },
      "source": [
        "Наша модель может переобучаться - она может просто поднастроить веса по тренировочную выборку. Особенно это характерно для больших моделей, когда число весов приближается к числу параметров. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO1_L1msUmIo"
      },
      "source": [
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_regularisation.jpg\" width=\"700\"> \n",
        "\n",
        "\n",
        "Регуляризация Тихонова = L2 Regularization = weights decay\n",
        "\n",
        "Идея состоит в том, что мы можем наложить некоторое требование на сами веса. Дело в том, что можно получить один и тот же выход модели при разных весах (выход модели соответствует умножению весов на $x$), при разных $w$ выход может быть идентичен. При этом если мы говорим о том, что эти параметры задают некоторую аппроксимацию нашей целевой функции, то аппроксимировать функцию можно двумя способами:\n",
        "1. Использовать все имеющиеся данные и провести ее строго через все точки, которые нам известны; \n",
        "2. Использовать более простую функцию: (в данном случае линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым общим закономерностям, которые у них есть. \n",
        "\n",
        "Характерной чертой переобучения является второй сценарий, и сопровождается он, как правило большими весами. Введение L2-регуляризации приводит к тому, что большие веса штрафуются и предпочтение отдается решениям, использующим малые значения весов. \n",
        "\n",
        "Модель может попробовать схитрить и по-другому - использовать все веса, все признаки, даже незначимые, но с маленькими коэффициентами. С этим L2-loss поможет хуже, так как он не сильное штрафует мелкие веса. Результатов его применения - малые значения весов, которые использует модель\n",
        "\n",
        "В этом случае на помощь приходит L1-loss, который штрафует вес за сам факт отличия его от нуля. Но и штрафует он все веса одинаково. Результат его применения - малое число весов, которые использует модель в принципе. \n",
        "\n",
        "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/losses.gif\" alt=\"alttext\" style=\"width: 500px;\"/>\n",
        "\n",
        "\n",
        "Это лоссы можно комбинировать - получится Elastic Net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM2JMZ0-UmIo"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-regularisation2.png\" width=\"700\"> \n",
        "\n",
        "\n",
        "Берем сумму всех весов по всей матрице $w$, и добавляем ее к loss. Соответственно, чем больше будет эта сумма, тем больше будет суммарный loss. \n",
        "\n",
        "\n",
        "В дальнейшем проблема с переобучением будет вставать довольно часто. Методов регуляризации модели существует достаточно много. Этот — один из базовых, который будет использоваться практически во всех отпимизаторах. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lewl5XvUD0F5"
      },
      "source": [
        "## 6. Cross-entropy Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUIHvNLvUmIp"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-12.jpg\" width=\"650\">\n",
        "\n",
        "На этой части построены практически все модели, про которые будет идти речь.\n",
        "\n",
        "Это оценка результата и обновление весов с использованием градиента. Для более глубоких моделей есть детали, как протолкнуть этот градиент в слои, которые находятся внутри. У нас один слой, поэтому мы достаточно легко посчитали от него градиент. Если слоев будет много, сделать это будет сложней.\n",
        "\n",
        "Рассмотрим ещё одну loss-функцию. \n",
        "\n",
        "SVM loss хороша тем, что интуитивно понятно, как ее посчитать. Но проблема состоит в том, что модель выдает некоторые числа, которые сами по себе невозможно интерпретировать. Но сами по себе выходы модели мало что означают. Было бы неплохо, если бы мы сразу, глядя на выход модели для кошки, могли бы их как-то интерпретировать, не просматривая остальные веса. Хорошо бы, чтобы модель выдавала не какую-то абстрактную уверенность, а **вероятность** того, что по ее мнению, на картинке изображена кошка.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjfEWxYVUmIp"
      },
      "source": [
        "### 6.1 Переход к вероятностям\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_softmax.jpg\" width=\"750\">\n",
        "\n",
        "К этому можно придти, проведя с весами некоторые не очень сложные математические преобразования. \n",
        "\n",
        "На слайде выше показано, почему выходы модели часто называют logit’ами. Если предположить, что у нас есть некая вероятность, от которой мы берем такую функцию (logit), то она может принимать значения от нуля до бесконечности. Мы можем считать, что выходы модели - это logit’ы. \n",
        "\n",
        "Logit: https://en.wikipedia.org/wiki/Logit\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7NgGGF3UmIp"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_softmax2.jpg\" width=\"550\">\n",
        "\n",
        "А мы бы хотели получить не logit’ы, а настоящую вероятность на выходе модели. Для этого можно применить к сырым выходам несколько преобразований, поскольку вероятность должна быть неотрицательной, нам нужно сделать так, чтобы выходы модели тоже стали неотрицательными. Можно было бы их сдвинуть на на некоторое смещение, а можно сделать сложнее: провести над ними операцию экспоненцирования, то есть число Эйлера **возвести в степень**, соответствующую этому выходу. В результате мы получим вектор, уже гарантировано неотрицательных чисел (потому что мы ввели неположительное число в степень, пускай даже отрицательную. То есть выходы могут быть маленькие, но всегда положительные). \n",
        "\n",
        "Дальше, чтобы можно было интерпретировать эти числа как вероятности, их сумма должна быть равно нулю. Мы должны их нормализовать, то есть **поделить на сумму**. Это преобразование называется **Softmax функцией**.\n",
        "\n",
        "**Получаются вероятности**, то есть числа, которые можно интерпретировать таким образом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0y8me6xUmIp"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_cross-entropy.png\" width=\"900\">\n",
        "\n",
        "\n",
        "Теперь, с использованием такого вектора можно определить другую loss функцию. Если не вникать в детали, можно **взять от нее логарифм**. Тогда loss от такого выхода будет выглядеть вот таким образом (формула на слайде выше), как нормализованный выход от верного класса на сумму остальных, к которому добавили минус. \n",
        "\n",
        "Получится график loss (слева). Его плюс заключается в том, что у него нет участка с плато, практически по всей длине функция получается гладкой, с хорошими мощными производными. Когда loss большой, производная тоже большая, и за счет этого можно быстро обучать модель. Для кусочно-линейной функции потерь она же равна константе.\n",
        "\n",
        "До тех пор, пока модель не будет работать с точностью 100%, то есть пока все остальные выходы не станут нулевыми, мы можем продолжить обучение, даже если у нас нет явной регуляризации.\n",
        "\n",
        "Осталось выяснить, почему такая простая на вид функция **называется так сложно** - Кросс-энтропия. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG5SLMDoUmIp"
      },
      "source": [
        "### 6.2 Кросс-энтропия\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_cross-entropy2.png\" >\n",
        "\n",
        "Если мы рассматриваем выходы нашей модели как вероятности, то мы можем их сравнивать. У нас есть номер правильного класса. Соответственно, можно сказать, что вероятность этого класса - единица, а всех остальных 0, и получить таким образом вероятностное распределение. Выход модели тоже можно интерпретировать как вероятности (тоже можно получить вероятностное распределение). И для работы с этими распределением есть некоторый математический аппарат, который основан на понятии энтропии, который ввел Клод Шеннон.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4cMx40MUmIp"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_sun.png\" >\n",
        "\n",
        "\n",
        "Идея в следующем: у нас есть некоторые данные, которые мы передаем по каналу связи. Например, у нас есть метеостанция, которая сообщает прогноз погоды. Допустим, она может передавать 8 вариантов прогноза. Мы в каждый момент времени получаем от нее сообщение. Потребуется 3 бит, чтобы передавать это сообщение. \n",
        "\n",
        "Допустим, мы знаем некую вероятность, с которой в нашем регионе может наступить хорошая либо плохая погода. То есть мы знаем вероятностное распределение, по которыму у нас, допустим, солнечное место, где почти не бывает бури. Если мы знаем об этом, мы можем сэкономить на передаче информации. Мы можем закодировать наиболее вероятные сообщения двумя битами, причем придумать такие коды, чтобы они почти не пересекались. Они будут начинаться с нуля и пердавать 2 бита вместо трех. Таким образом можно сэкономить на передачи информации. То есть идея такая: **если мы знаем, что события маловероятны, мы можем кодировать их более длинной цепочкой, а более вероятные события - более короткими цепочками**. В этом случае в среднем количество информации, которое можно передать, сократиться.\n",
        "\n",
        "**Формула Шеннона позволяет посчитать, насколько сильно мы можем сэкономить для конкретного вероятностного распределения**. То есть если подставить эти данные в формулу, то получим 2,23.\n",
        "\n",
        "На практике реализовать это, используя биты, возможно, не выйдет, но это свойство данного вероятностного распределения можно посчитать. Оно несёт в себе некоторое количество полезной информации, соответствующее этой энтропии.\n",
        "\n",
        "Что же такое кросс-энтропия?\n",
        "\n",
        "Давайте представим, что мы каким-то образом начали кодировать эту информацию, прошили погодную станцию так, что она может эту кодированную информацию передавать, а пптотм перенесли в другой регион, где ситуация противоположная. И мы кодируем события, которые стали частыми, длинными цепочками, а маловероятные - короткой, и у нас в этом случае явно передается больше информации, чем мы могли бы, если бы сделаем все банально. \n",
        "\n",
        "Кросс-энтропия позволяет посчитать, насколько большая будет потеря в данном случае. То есть это некий **способ сравнить между собой вероятностные распределения**.\n",
        "\n",
        "Что, собственно говоря, отображено в формуле, потому что мы считаем кросс-энтропию по нашему вектору P и Q. Вектор P состоит из нулей и единиц, соответственно, во всех случаях, кроме одного, вот это выражение будет нулевым, кроме одного случая с единицей. Случай с единицей соответствует нашему правильному классу. Поэтому сумма исчезает, остаются логарифм и вектор, а вероятность для правильного класса мы считаем как Soft max. Отсюда название кросс-энтропия.  \n",
        "\n",
        "\n",
        "(В теории информации кросс-энтропия между двумя распределениями вероятностей p {\\displaystyle p} и q {\\displaystyle q} по одному и тому же базовому набору событий измеряет среднее число битов, необходимых для идентификации события , взятого из набора, если схема кодирования, используемая для набора, оптимизирована для оценочного распределения вероятностей q {\\displaystyle q}, а не для истинного распределения p {\\displaystyle p}.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc0-zYaRD0F3"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_cross-entropy3.png\" width=\"800\">\n",
        "\n",
        "Аналогичное обоснование: есть понятие дивергенция, которая позволяет оценить расхождение между вероятностными распределениями, которая нам здесь и нужна. Есть дивергенция **Кульбака-Лейблера**, которая выражается через кросс-энтропию, а энтропия от нашего вектора нулевая. Поэтому фактически здесь кросс-энтропия равна дивергенции, которая показывает, насколько непохожи два распределения. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O-vQxZzUmIq"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_softmax3.jpg\" width=\"800\">\n",
        "\n",
        "\n",
        "Эта функция очень популярна, она используется при обучении реальных моделей на практике. Поскольку внутри находится Softmax, часто ее называют Softmax-классификатором, что, строго говоря, не совсем верно. Здесь функция потерь - кросс-энтропия, Softmax же просто используется внутри нее.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UerWVtQWUmIq"
      },
      "source": [
        "**Градиент Cross Entropy Loss**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNaNEcFsUmIq"
      },
      "source": [
        "http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKzGOgUuNotq"
      },
      "source": [
        "n_classes = 10\n",
        "y_class = 5\n",
        "\n",
        "one_hot = np.zeros(n_classes)\n",
        "one_hot[y_class] = 1\n",
        "yi = one_hot\n",
        "yi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7iLIj2RUmIq"
      },
      "source": [
        "$$ -L = \\sum_i y_i \\log p_i = \\sum_i y_i \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
        "\n",
        "$$s_{y_i} = w_i x$$\n",
        "\n",
        "$$ \\dfrac {\\delta L} {\\delta w_i} = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} $$\n",
        "\n",
        "$$\\dfrac {\\delta s_{y_i}} {\\delta w_i} = x$$\n",
        "\n",
        "Только один $y_k = 1$\n",
        "\n",
        "\n",
        "$$ -L = y_k \\log p_i = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
        "\n",
        "i = k\n",
        "\n",
        "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_i}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_i} - \\log  \\sum_j e^{s_{y_j}}$$\n",
        "\n",
        "$$\\dfrac {\\delta -L} {\\delta s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\delta {\\sum_j e^{s_{y_j}}}} {\\delta s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\delta e^{s_{y_i}}} {\\delta s_{y_i}} = 1 - \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = 1 - p_i$$\n",
        "\n",
        "$$\\dfrac {\\delta L} {\\delta s_{y_j}} = p_i - 1 $$\n",
        "\n",
        "$$ \\dfrac {\\delta L_i} {\\delta w_i}  = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} = (p_i - 1) x $$\n",
        "\n",
        "i != k\n",
        "\n",
        "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_k}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_k} - \\log  \\sum_j e^{s_{y_j}}$$\n",
        "\n",
        "$$\\dfrac {\\delta -L} {\\delta s_{y_i}} = - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\delta {\\sum_j e^{s_{y_j}}}} {\\delta s_{y_i}} =  \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\delta e^{s_{y_i}}} {\\delta s_{y_i}} = \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = - p_i$$\n",
        "\n",
        "$$\\dfrac {\\delta L} {\\delta s_{y_j}} = p_i $$\n",
        "$$ \\dfrac {\\delta L_i} {\\delta w_i}  = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} = p_i x $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLo8L7dnUmIq"
      },
      "source": [
        "**Практическое вычисление SoftMax**\n",
        "\n",
        "Когда мы возводим число в степень, может получиться очень большое число и произойти переполнение. Неизвестно, какие будут у модели выходы. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOF_0gIBObNF"
      },
      "source": [
        "f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\n",
        "p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sew6jeeomRaL"
      },
      "source": [
        "Поэтому мы можем вычесть из каждого $s_i$ пооложительную константу, чтобы уменьшить значения экспонент. В качестве константы можно выбрать максимальный элемент этого вектора, тогда у нас гарантированно не будет очень больших чисел и такой будет работать более стабильно.\n",
        "\n",
        "$$M = \\max_j s_{y_{j}}$$\n",
        "$$s^{new}_{y_{i}}  = s_{y_{i}} - M $$\n",
        "\n",
        "$$ \\dfrac {e^{s^{new}_{y_{i}}}} {\\sum_j e^{s^{new}_{y_{j}}}}  = \\dfrac {e^{s_{y_{i}} - M }} {\\sum_j e^{s_{y_{j}} - M }} = \\dfrac {e^{s_{y_{i}}}e ^ {-M}} {\\sum_j e^{s_{y_{j}}} e ^ {-M}} = \\dfrac {e ^ {-M} e^{s_{y_{i}}}} {e ^ {-M} \\sum_j e^{s_{y_{j}}} } = \\dfrac { e^{s_{y_{i}}}} { \\sum_j e^{s_{y_{j}}} }$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S2RdAELOmvC"
      },
      "source": [
        "# instead: first shift the values of f so that the highest number is 0\n",
        "f = np.array([123, 456, 789])\n",
        "f -= f.max()\n",
        "p = np.exp(f) / np.sum(np.exp(f))\n",
        "f, p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gUCwV2eUmIq"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_practice_softmax_code.png\" width=\"800\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgy6eBWlUmIq"
      },
      "source": [
        "**Итог**\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_itog.png\" width=\"800\">"
      ]
    }
  ]
}