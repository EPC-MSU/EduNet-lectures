{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой лекции мы поговорим о ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритм k-nearest neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продолжим с того места, где мы остановились в предыдущей лекции - классификация методом ближайших соседей (kNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метрики расстояния**\n",
    "\n",
    "В практических заданиях EX01 мы использовали kNN для классификации изображений CIFAR10 (набор фотографий разделенных на 10 классов). Мы выяснили что точность метода kNN с использованем расстояний L1 (*Manhatten distance* - сумма абсолютных разностей между пикселями) оставляет желать лучшего, и попробовали применить L2 (*Euclidian distance*) в надежде эту точность классификации повысить. С этими метриками мы будем сталкиваться часто: и в качестве loss функции, и в качестве регуляризации, поэтому познакомиться с ними полезно. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/L02_Linear_Classifier-1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/L02_Linear_classifier-KNN.jpg\" width=\"450\">\n",
    "\n",
    "На практике, метод ближайших соседей для классификации используется крайне редко. Давайте разберемся почему.\n",
    "Проблема заключается в следующем: предположим, что точность классификации нас устраивает. Теперь давайте применим kNN для по больших данных (e.g. миллион картинок). Для определения класса каждой из картинок, нам нужно сравнить ее со всеми другими картинками в базе данных, такие расчеты, даже в супер оптимизированном виде, занимают много времени. Очень много времени.  Мы хотим, чтобы обученная модель работала быстро.\n",
    "\n",
    "Тем не менее, метод ближайших соседей используется в других задачах, где без него обойтись сложно. Например, в задаче распознавания лиц. Представим, что у нас у нас есть большая база данных с фотографиями лиц (например по 5 разных фотографий всех сотрудников, которые работают в офисном здании, как на примере выше) и есть камера установленная на входе в это здание. Мы хотим узнать кто и во сколько пришел на работу. Для того, что бы понять кто прошел перед камерой нам нужно зафиксировать лицо этого человека и сравнить его со всем фотографиями лиц в базе. В такой формулировке мы не пытаемся определить конкретный класс фотографии, а всего лишь определяем “похож-не похож”. Мы смотрим на k ближайших соседей и, например, если из k соседей 5 - это фотографии Джеки Чана, то скорее всего, под камерой прошел именно он. В таких случаях kNN метод вполне полезен. Похожим образом работает и поиск дупликатов в базах данных.\n",
    "\n",
    "Примеры эффективной реализации метода на основе kNN:\n",
    "* [Facebook AI Research Similarity Search](https://github.com/facebookresearch/faiss) – разработка команды Facebook AI Research для быстрого поиска ближайших соседей и кластеризации в векторном пространстве. Высокая скорость поиска позволяет работать с очень большими данными – до нескольких миллиардов векторов.\n",
    "* Алгоритм поиска ближайших соседей [Hierarchical Navigable Small World](https://arxiv.org/abs/1603.09320). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практические аспекты работы с классификаторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков, соотвествует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30 признакам обучить модель определять злокачественная или доброкачественная ткань.\n",
    "\n",
    "Можно иметь сколь угодно хороший алгоритм для классификации - но до тех пор, пока данные на входе - мусор, на выходе из нашего чудесного классификатора мы тоже будем получать мусор (*garbage in - garbage out*). Давайте разберемся что конкретно надо сделать, что бы kNN реально заработал.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем нужные библиотеки\n",
    "import sklearn.datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = sklearn.datasets.load_breast_cancer() # Загружаем датасет\n",
    "X = cancer.data # Здесь будем хранить признаки\n",
    "y = cancer.target # Здесь храним лейблы(классы)\n",
    "print(f'Размерность X {X.shape}, Размерность y {y.shape}') # Посмотри на размерность датасета\n",
    "print(f'Пример X \\n {X[0]}') # Посмотрим как выглядит набор признаков для одного примера\n",
    "print(f'Пример y \\n {y[0]}') # Посмотрим как выглядит пример лейбла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько данных в классе `0` и сколько данных в классе `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('default') # Установим сетку для отрисовки графика\n",
    "plt.figure(figsize=(8,5)) # Установим размер графика\n",
    "plt.bar(1,y[y==1].shape, label=cancer.target_names[0]) # Строим первый столбец \n",
    "plt.bar(0,y[y==0].shape, label=cancer.target_names[1]) # Строим второй столбец\n",
    "plt.title('Баланс классов') # Название графика\n",
    "plt.ylabel('Количество в каждом классе') # Название оси Y\n",
    "plt.xticks(ticks=[1,0], labels=['1','0']) # Название значение по оси X\n",
    "plt.legend(loc='upper left') # Отображение легенды, в левом верхнем угле\n",
    "plt.show() # Отображаем график"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк в каждой из которой по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxenplot(data=pd.DataFrame(X), orient=\"h\", palette=\"Set2\")\n",
    "ax.set(xscale='log', xlim=(1e-4, 1e4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы адекватно сравнить данные между собой нам следует использовать нормализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Нормализация, выбор Scaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация — это преобразование данных к неким безразмерным единицам.\n",
    "Ключевая цель нормализации — приведение различных данных в самых разных единицах измерения и диапазонах значений к единому виду, который позволит сравнивать их между собой или использовать для расчёта схожести объектов.\n",
    "Итого, главное условие правильной нормализации — все признаки должны быть равны в возможностях своего влияния.\n",
    "\n",
    "Например, у нас данные по группе людей возраст (в годах) и размер дохода (в рублях). Возраст может измениться в диапозоне от 18 до 70 ( интервал 70-18 = 52). А доход от 30 000 р до 500 000 р (интервал 500 000 - 30 000 = 470 000). В таком варианте разница в возрасте имеет меньшее влияние, чем разница в доходе. Получается, что доход становится более важным признаком, изменения в котором влияют больше при сравнении схожести двух людей.\n",
    "\n",
    "Должно быть так, чтобы максимальные изменения любого признака в «основной массе объектов» были одинаковы. Тогда потенциально все признаки будут равноценны.\n",
    "Точно степень влияния признака должно определить только обучение модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось определиться с выбором инструмента, часто используют следующие варианты: `MinMaxScaler`, `StandardScaler`, `RobustScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные в диапозоне от 0 до 1. Может быть полезно, если вы хотите выполнить преобразование, где отрицательные значения не допускаются (e.g., масштабирование RGB пикселей)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$z=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "$X_{min}$ и $X_{max}$ задаются как минимальное и максимальное допустимое значение, по умолчанию:  $X_{min}=0$  и $X_{max}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение 0 и стандартное отклонение 1. Большинство значений будет в  диапозоне от -1 до 1. Это стандартная трансформация, и она применима во многих ситуациях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-u}{s}$$\n",
    "\n",
    "$u$ — среднее значение (или 0 при `with_mean=False`) и $s$ — стандартное отклонение (или 0 при `with_std=False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И StandardScaler и MinMaxScaler очень чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях* (процентиль — мера, в которой процентное значение общих значений равно этой мере или меньше ее. Например, 90 % значений данных находятся ниже 90-го процентиля, а 10 % значений данных находятся ниже 10-го процентиля) и поэтому не зависит от небольшого числа очень больших предельных выбросов (*outliers*). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-X_{median}}{IQR}$$\n",
    "\n",
    "$X_{median}$ — значение медианы, $IQR$ — межквартильный диапазон равный разнице между 75-ым и 25-ым процентилями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для признака data[:,0]. Обратите внимание на ось X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # задаем параметр инициализации случайных значений\n",
    "\n",
    "# генерируем случайные значения от 1 до 255 размерностью (30,1)\n",
    "test = X[:,0].reshape(-1,1)\n",
    "\n",
    "plt.figure(1, figsize=(30, 5))  # задаем размер графика\n",
    "plt.subplot(141)  # указываем расположение графика\n",
    "plt.scatter(test, range(len(test)), c=y)  # строим график точек scatter\n",
    "plt.ylabel(\"Количество точек\", fontsize=15)  # указываем название оси Y\n",
    "plt.xticks(fontsize=15)  # указываем размер шрифта по оси Х для значений\n",
    "plt.yticks(fontsize=15)  # указываем размер шрифта по оси Y для значений\n",
    "plt.title(\"Необработанные данные\", fontsize=18)  # указываем название графика\n",
    "\n",
    "# масштабируем значения с помощью MinMaxScaler\n",
    "test_scaled = MinMaxScaler().fit_transform(test)  \n",
    "plt.subplot(142)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"После MinMaxScaler\", fontsize=18)\n",
    "\n",
    "# масштабируем значения с помощью StandardScaler\n",
    "test_scaled = StandardScaler().fit_transform(test)  \n",
    "plt.subplot(143)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"После StandardScaler\", fontsize=18)\n",
    "\n",
    "# масштабируем значения с помощью RobustScaler\n",
    "test_scaled = RobustScaler().fit_transform(test)  \n",
    "plt.subplot(144)\n",
    "plt.scatter(test_scaled, range(len(test)), c=y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"После RobustScaler\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нашей задачи по определению раковых опухолей обработаем наши 30 признаков с помощью StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = StandardScaler().fit_transform(X)  # масштабирум данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что они стали намного более сравнимы между собой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_norm).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxenplot(data=pd.DataFrame(X_norm), orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим KNN для общей выборки данных, при разном значении количества соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nei_rng = np.arange(1, 31)  # создаем массив из количества соседей\n",
    "\n",
    "# создаем массив из нулей размерностью как массив количества соседей\n",
    "quality = np.zeros(\n",
    "    n_nei_rng.shape[0]\n",
    ")  \n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  # проходимся по по индексам массивов\n",
    "    # создаем knn для разного количества соседей\n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_nei_rng[ind]\n",
    "    )  \n",
    "    knn.fit(X_norm, y)  # обучаем knn\n",
    "    q = accuracy_score(y_pred=knn.predict(X_norm), y_true=y)  # считаем точность\n",
    "    quality[ind] = q  # заполняем массив значениями точности\n",
    "\n",
    "plt.figure(figsize=(8, 5))  # задаем размеры графика\n",
    "plt.title(\"KNN on train\", size=20)  # задаем название графика\n",
    "plt.xlabel(\"Neighbors\", size=15)  # задаем название оси X\n",
    "plt.ylabel(\"Accuracy\", size=15)  # задаем название оси Y\n",
    "plt.plot(n_nei_rng, quality)  # строим график\n",
    "plt.xticks(n_nei_rng)  # указываем метки для оси X\n",
    "plt.show()  # отображаем график"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество на 1 соседе - самое лучшее. Но это и понятно - ближайшим соседом элемента из обучающей выборки будет сам объект. Мы просто **запомнили** все объекты.\n",
    "\n",
    "Если теперь мы попробуем взять какой-то новый образец опухоли и классифицировать его - у нас скореее всего ничего не получится. В таких случаях мы говорим что наша модель не умеет обобщать (*generalization*).\n",
    "\n",
    "Для того, чтобы знать заранее обобщает ли наша модель или нет, мы можем разбить все имеющиеся у нас данныe на 2 части. Но одной части мы будем обучать классификатор (*train set*), а на другой тестировать насколько хорошо он работает (*test set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# делим выборки на обучающую и поверочную\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")  \n",
    "scaler = StandardScaler()  # создаем scaler\n",
    "scaler.fit(X_train)  # обучаем scaler\n",
    "X_train_norm = scaler.transform(X_train)  # масштабируем данные\n",
    "X_test_norm = scaler.transform(X_test)  # масштабируем данные\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  # создаем массив количества соседей от 1 до 30\n",
    "train_quality = np.zeros(n_nei_rng.shape[0])  # качество модели на обучающей выборке\n",
    "test_quality = np.zeros(n_nei_rng.shape[0])  # качество модели на поверочной выборке\n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  # проходимся по всем индексам массива\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_nei_rng[ind])  # создаем KNN\n",
    "    knn.fit(X_train_norm, y_train)  # обучаем KNN\n",
    "    \n",
    "    # считаем точность на обучающей выборке\n",
    "    trq = accuracy_score(\n",
    "        y_pred=knn.predict(X_train_norm), y_true=y_train\n",
    "    )  \n",
    "    train_quality[ind] = trq  # добавляем в массив\n",
    "\n",
    "    # считаем точность на поверочной выборке\n",
    "    teq = accuracy_score(\n",
    "        y_pred=knn.predict(X_test_norm), y_true=y_test\n",
    "    )  \n",
    "    test_quality[ind] = teq  # добавляем в массив\n",
    "\n",
    "# строим график сравнения работы модели на обучающей и поверочной выборке\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот, теперь мы видим, что 1 сосед был \"ложной тревогой\". Такие случаи мы называем *переобучением*. Что бы действительно предсказывать что-то полезное нам надо выбирать число соседей начиная минимум с 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-валидация\n",
    "\n",
    "Каждая модель имеет как ряд **параметров**, которые она меняет в процессе обучения (например, веса модели), так и ряд **гиперпараметров**, которые влияют на то, каким способом модель менять параметры в процессе обучения. \n",
    "\n",
    "В случае kNN параметры, строго говоря, отсутствует - модель просто запоминает объекты обучающей выборки. Особо упорные могут считать их параметрами. \n",
    "\n",
    "А вот гиперпараметры есть, даже несколько групп. Какие? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. число соседей \n",
    "2. функция, которой считаем расстояние между объектами (L2=eucledian, L1=manhattan)\n",
    "3. веса, с которыми складываем метки ближайших соседей\n",
    "4. признаки! (но об этом с вами поговорим позже)\n",
    "5. сама модель - мы могли выбрать не kNN, а нагуглить что-нибудь другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим еще раз на график, который нарисовали на прошлом шаге. Какое число соседей считать оптимальным? Метрика явно скачет? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Не понятно, насколько результат зависит от того, как нам повезло или не повезло с разбиением данных на обучение и тест. Может оказаться так, что для конкретного разбиения хорошо выбрать k=5, а для другого - k=7. \n",
    "\n",
    "Кроме того, опять же - фактически, мы сами выступаем в роли модели, которая учит гиперпараметры (а не параметры) под видимую ей выборку. \n",
    "\n",
    "Представим себе, что у нас есть 10000 моделей, полученных подкручиванием разных гиперпараметров (в том числе, выборов просто разного типа модели). Представим, что все эти модели не работают. Вообще. Представим так же, что каждая модель угадывает класс в задаче разделения на два класса с вероятностью 0.5 (и будем считать, что классы у нас сбалансированны - то есть 50% одного класса и 50% другого). \n",
    "Опять же, понятно, что классификация такими моделями ничем не лучше подбрасывания монетки. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_model(y_real):\n",
    "    # создаем массив из значений True/False, для случайного выбора значений по маске\n",
    "    guessed = np.random.choice(\n",
    "        [\n",
    "            True,\n",
    "            False,\n",
    "        ],  \n",
    "        size=y_real.shape[0],\n",
    "        replace=True,\n",
    "    )\n",
    "    y_predicted = np.zeros_like(y_real)  # создаем массив из нулей размером y_real\n",
    "    \n",
    "    # с помощью маски guessed присваиваем значения y_predicted\n",
    "    y_predicted[guessed] = y_real[guessed]  \n",
    "    y_predicted[~guessed] = 1 - y_real[~guessed]  # меняем значения по обратной маске\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_num = 10000  # количество экспериментов\n",
    "best_quality = 0.5  # порог качества по точности\n",
    "\n",
    "# создаем случайный массив из значений от 0 до 1.\n",
    "y_real = np.random.choice(\n",
    "    [0, 1], size=250, replace=True\n",
    ")  \n",
    "\n",
    "for i in range(models_num):  # запускаем в цикле количество экспериментов\n",
    "    y_pred = guess_model(y_real)  # создаем с помощью функции \"предсказанные\" значения\n",
    "    q = accuracy_score(y_pred=y_pred, y_true=y_real)  # считаем точность\n",
    "    if q > best_quality:  # если точность больше порога\n",
    "        best_quality = q  # то порог становится текущим максимальным значением точности\n",
    "print(f\"Лучший результат {best_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы перебором всех возможных моделей вполне можем получить для абсолютно бесполезной модели приемлемое качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Получается, что если подбирать гиперпараметры модели на *train set*, то:\n",
    "1. можно переобучитьcя, просто на более \"высоком\" уровне. Особенно если гиперпараметров у модели много и все они разнообразны\n",
    "2. нельзя быть уверенным, что выбор параметров не зависит от разбиения на обучение и тест \n",
    "\n",
    "Поэтому мы:\n",
    "\n",
    "1) подбираем гиперпараметры моделей на отдельном датасете, называмым валидационным. Получаем мы его разбиением обучающего датасета на собственно обучающий и валидационный \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-cross-validation.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) чаще всего делаем несколько таких разбиений по какой-то схеме, чтобы получить уверенность оценок качества для моделей с разными гиперпараметрами - кросс-валидация\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/L02_Linear_classifier-cross-validation2.png\" width=\"500\">\n",
    "\n",
    "Часто применяется следующий подход, называемый [K-Fold кросс-валидацией](https://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "\n",
    "Берется тренировочная часть датасета, разбивается на части - блоки. Дальше мы будем использовать для проверки первую часть (Fold 1), а на остальных учиться. И так последовательно для всех частей. В результате у нас будут информация о точности для разных фрагментов данных и уже на основании этого можно понять, насколько значение этого параметра, который мы проверяем, зависит или не зависит от данных. То есть если у нас от разбиения точность при одном и том же К меняться не будет, значит мы подобрали правильное К. Если она будет сильно меняться в зависимости от того, на каком куске данных мы проводим тестирование, значит, надо попробовать другое К и если ни при каком не получилось - то это такие данные.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем параметры для модели с помощью GridSearchCV\n",
    "\n",
    "GridSearchCV – это инструмент для автоматического подбирания параметров для моделей машинного обучения. GridSearchCV находит наилучшие параметры, путем обычного перебора: он создает модель для каждой возможной комбинации параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "\"\"\"\n",
    "Параметры для GridSearchCV:\n",
    "\n",
    "estimator — модель которую хотим обучать (алгоритм)\n",
    "\n",
    "cv — сколько разрезов кросс-валидации мы ходим сделать\n",
    "\n",
    "param_grid — передаем какие параметры хотим подбирать, GridSearchCV на всех параметрах сделает обучение\n",
    "\n",
    "scoring — выбор метрики ошибки (для разных задач можно выбрать разные функции ошибки)\n",
    "\n",
    "n_jobs - количество процессов выполняемых параллельно, значение -1, задействовать все возможные\n",
    "\"\"\"\n",
    "model = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    cv=KFold(3, shuffle=True, random_state=42),\n",
    "    param_grid={\n",
    "        \"n_neighbors\": np.arange(1, 31),\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем лучшие гиперпараметры для модели, которые подобрали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Метрика:\", model.best_params_[\"metric\"])\n",
    "print(\"Количество соседей:\", model.best_params_[\"n_neighbors\"])\n",
    "print(\"Веса:\", model.best_params_[\"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Объект GridSearchCV можно использовать как обычную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_norm)\n",
    "print(f\"Процент правильных ответов {np.round(accuracy_score(y_pred=y_pred, y_true=y_test)*100,2)} %\")\n",
    "print(f\"Процент правильных ответов по отношению к классам {np.round(balanced_accuracy_score(y_pred=y_pred, y_true=y_test)*100,2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем извлечь дополнительные данные о кроссвалидации и по ключу обратиться к результатам всех моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем для примера mean_test_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(model.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"mean_test_score\", size=20)\n",
    "plt.xlabel(\"Количество экспериметов\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим, например, при фиксированных остальных параметрах (равных лучшим параметрам), качество модели на валидации в зависимости от числа соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_means = []\n",
    "selected_std = []\n",
    "n_nei = []\n",
    "for ind, params in enumerate(model.cv_results_[\"params\"]):\n",
    "    if (\n",
    "        params[\"metric\"] == model.best_params_[\"metric\"]\n",
    "        and params[\"weights\"] == model.best_params_[\"weights\"]\n",
    "    ):\n",
    "        n_nei.append(params[\"n_neighbors\"])\n",
    "        selected_means.append(model.cv_results_[\"mean_test_score\"][ind])\n",
    "        selected_std.append(model.cv_results_[\"std_test_score\"][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим error bar, для сравнения разброса ошибки при разном количестве соседей Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"KNN CV, {params['metric']}, {params['weights']}\", size=20)\n",
    "plt.errorbar(n_nei, selected_means, yerr=selected_std, linestyle=\"None\", fmt=\"-o\")\n",
    "plt.xticks(n_nei)\n",
    "plt.ylabel(\"Mean_test_score\", size=15)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на самом деле большой разницы в числе соседей и нет. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Можно ли делать только кросс-валидацию (без теста)?**\n",
    "\n",
    "\n",
    "Нет, нельзя. Кросс-валидация не до конца спасает от подгона параметров модели под выборку, на которой она проводится. Оценка конечного качества модели должно производиться на отложенной тестовой выборке. Если у вас очень мало данных, можно рассмотреть [вложенную кросс-валидацию](https://weina.me/nested-cross-validation/), но об этом позже в последующих лекциях. Но даже в этом случае придется анализировать поведение модели, чтобы показать, что она учит что-то разумное. Кстати, вложенную кросс-валидацию можно использовать чтобы просто получить более устойчивю оценку поведения модели на тесте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-04.jpg\" width=\"800\">\n",
    "\n",
    "Давайте подумаем, как избавиться от проблемой со скоростью K-Nearest Neighbors. Реализуя метод ближайшего соседа, мы сравнивали одно изображение со всеми, при этом мы предполагали, что изображения из одного класса будут чем-то похожи друг на друга, и поэтому разница будет меньше. Метод как-то работал. \n",
    "\n",
    "Чтобы ускорить этот процесс, мы можем взять весь блок изображений (справа), который относится, например, к машинам и усредним. Таким образом, получим некоторый шаблон для класса “автомобиль”. Скорее всего, работать такой подход будет не слишком здорово, но зато вместо тысяч изображений (в данном случае 50000) появится одно. Возможно, это поможет сильно сэкономить время."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к сравнению с шаблоном\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-05.jpg\" width=\"450\">\n",
    "\n",
    "Идея в следующем: вместо того, чтобы сравнивать каждое изображение со всеми остальными по очереди, будем сравнивать изображения с шаблоном для каждого класса. Их будет всего 10 для данного датасета. Этот подход позволит радикально увеличить скорость. \n",
    "\n",
    "Проверим, что получится из этой идеи. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "скачиваем CIFAR10 в архиве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xzf cifar-10-python.tar.gz\n",
    "!ls -l\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузили архив в пямять целиком, используя для этого фрагмент кода с сайта CIFAR10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "path = '/content/cifar-10-batches-py/'\n",
    "\n",
    "x_train = np.zeros((0, 3072))\n",
    "y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"{path}/data_batch_{i}\")\n",
    "    x_train = np.append(x_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    y_train = np.append(y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(f\"{path}/test_batch\")\n",
    "x_test = np.array(test[b\"data\"])\n",
    "y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_ru = [\n",
    "    \"Самолет\",\n",
    "    \"Автомобиль\",\n",
    "    \"Птица\",\n",
    "    \"Кошка\",\n",
    "    \"Олень\",\n",
    "    \"Собака\",\n",
    "    \"Лягушка\",\n",
    "    \"Лошадь\",\n",
    "    \"Корабль\",\n",
    "    \"Грузовик\",\n",
    "]\n",
    "\n",
    "print(f\"Размерность обучающей выборки X: {x_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"Размерность тестовой выборки X: {x_test.shape}, y: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "pylab.rcParams[\"figure.figsize\"] = (20.0, 20.0)  # задаем размеры графика\n",
    "for i in range(10):  # проходим по классам от 0 до 9\n",
    "    # получаем список из индексов положений класса i в y_train\n",
    "    label_indexes = np.where(y_train == i)[0]  \n",
    "    index = np.random.choice(label_indexes)  # случайным образом выбираем из списка индекс\n",
    "    # Выбираем из x_train нужное изображение\n",
    "    img = x_train[index].reshape(\n",
    "        32, 32, 3, order=\"F\"\n",
    "    )  \n",
    "    img = np.rot90(img, k=3)  # поворачиваем изображение\n",
    "    plt.subplot(1, 10, i + 1)  # строим 10 изображений\n",
    "    plt.title(labels_ru[i])  # указываем название каждого изображения по классу\n",
    "    plt.axis(\"off\")  # отключаем отображение осей\n",
    "    imshow(img.astype(\"uint8\"))  # отрисовываем изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала создадим шаблоны и визуализируем их.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = []  # здесь будем хранить шаблоны\n",
    "meta = unpickle(f\"{path}/batches.meta\")  # загружаем данные\n",
    "labels = meta[b\"label_names\"]  # берем названия лейблов\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    indexes = np.where(y_train == i)  # получаем индекс для каждого класса\n",
    "    mask = np.zeros(len(y_train), dtype=bool)  # создаем маску из значений False\n",
    "    mask[indexes, ] = True  # меняем значения на True по индексам\n",
    "    images = x_train[mask]  # выбираем все изображения одного класса\n",
    "    mn = np.mean(images, 0)  # считаем среднее\n",
    "    templates.append(mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20.0, 2.0))\n",
    "\n",
    "def show_templates(templates, labels):\n",
    "    for i, template in enumerate(templates):\n",
    "        img = template.reshape(3, 32, 32).transpose(1, 2, 0).astype(int)\n",
    "        plt.subplot(1, len(labels), i + 1)\n",
    "        plt.title(labels_ru[i])\n",
    "        plt.axis(\"off\")\n",
    "        imshow(img)\n",
    "\n",
    "show_templates(templates, labels)  # отрисовывем получившиеся шаблоны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем классификатор, который будет сравнивать изображение с шаблоном, который генерируется во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateBasedClassifier:\n",
    "    def __init__(self):\n",
    "        self.templates = []  # здесь будем хранить шаблоны\n",
    "        meta = unpickle(f\"{path}/batches.meta\")  # загружаем данные\n",
    "        self.labels = meta[b\"label_names\"]  # берем названия лейблов\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.templates = []\n",
    "        for label_num in range(len(self.labels)):  # проходимся по всем классам\n",
    "            # получаем индекс для каждого класса\n",
    "            indexes = np.where(y_train == label_num)  \n",
    "            mask = np.zeros(len(y_train), dtype=bool)  # создаем маску из значений False\n",
    "            mask[indexes, ] = True  # меняем значения на True по индексам\n",
    "            images = x_train[mask]  # выбираем все изображения одного класса\n",
    "            mn = np.mean(images, 0)  # считаем среднее значение пикселей для каждого изображения класса\n",
    "            self.templates.append(mn)  # добавляем в шаблоны\n",
    "\n",
    "    def forward(self, x):\n",
    "        distances = np.sum(\n",
    "            np.abs(self.templates - x), axis=1)  # отнимаем от шаблонов значения изображения, считаем разницу\n",
    "        return np.argmin(distances)  # возвращаем индекс минимального значения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть 10 шаблонов, с которыми мы будем сравнивать изображение и на основании этого делать предсказание. На этих шаблонах можно увидеть очертания объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим сравнение на конкретном примере с кошкой. Построим изображения картинки, шаблона, а затем посчитаем расстояние L1 между ними. Чем желтее цвет - тем больше изображени похоже на шаблон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3, figsize=(10,5))\n",
    "\n",
    "indexes = np.where(y_train == 3)  # получаем индекс для каждого класса\n",
    "mask = np.zeros(len(y_train), dtype=bool)  # создаем маску из значений False\n",
    "mask[indexes, ] = True  # меняем значения на True по индексам\n",
    "images = x_train[mask]\n",
    "\n",
    "img = images[0].reshape(3, 32, 32).transpose(1, 2, 0).astype(int)\n",
    "template = templates[3].reshape(3, 32, 32).transpose(1, 2, 0).astype(int)\n",
    "residual = np.mean(np.abs(img-template),-1)\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(template)\n",
    "r_plot = ax[2].imshow(residual, cmap='inferno_r')\n",
    "\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(r_plot, cax=cax, orientation='vertical', label='L1')\n",
    "\n",
    "ax[0].set_title('Изображение')\n",
    "ax[1].set_title('Шаблон')\n",
    "ax[2].set_title('D = Изображение - Шаблон')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказание и замерим время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemplateBasedClassifier()  # создаем наш классификатор\n",
    "model.fit(x_train, y_train)  # запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def validate(model, x_test, y_test, noprint=False):\n",
    "    correct = 0  # здесь храним количество верных предсказаний\n",
    "    for i, img in enumerate(x_test):  # проходимся по всем изображениям в выборке\n",
    "        index = model.forward(img)  # определяем класс изображения\n",
    "        correct += (\n",
    "            1 if index == y_test[i] else 0\n",
    "        )  # если класс верный то увеличиваем correct на 1\n",
    "        if noprint is False:  # если стоит флаг False\n",
    "            if i > 0 and i % 1000 == 0:  # каждые 1000 значений\n",
    "                print(\n",
    "                    \"Accuracy {:.3f}\".format(correct / i)\n",
    "                )  # выводим значения точност, количество верных ответов/количество попыток\n",
    "    return correct / len(y_test)  # возвращаем общее значение точности\n",
    "\n",
    "start = time.perf_counter()  # засекаем время\n",
    "accuracy= validate(model, x_test, y_test)  # считаем точность\n",
    "tm = time.perf_counter() - start  # считаем, сколько заняло времени\n",
    "print(\n",
    "    \"Accuracy {:.2f} Train {:d} /test {:d} in {:.1f} sec. speed {:.2f} samples per second.\".format(\n",
    "        accuracy, len(x_train), len(x_test), tm, len(x_test) / tm\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если бы мы обучали KNN на таком количестве данных, это заняло бы у нас 2 часа.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Переход к весам\n",
    "----\n",
    "Умножение вместо вычитания - для чего?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-06.jpg\" width=\"450\">\n",
    "\n",
    "В сранении с шаблонами, мы вычитали нашу картинку из шаблона и таким образом смотрели насколько они похожи (L1). Теперь давайте сделаем следующий, пока ничем не обоснованный, ход: оставим всё, как было, но заменим вычитание умножением. Логика в том, что не все пиксели одинаково важны. Вероятно, если на изображении совпадут какие-то пиксели, которые отвечают, например, за глаза кошки, это будет намного важнее, чем фон, который может быть точно таким же у собаки. Здесь будут какие-то важные особенности, которым можно придать больший вес.\n",
    "\n",
    "Если мы распишем в виде формул наложение шаблона на картинку таким образом с умножением, то получается, что мы скалярно перемножаем два вектора. Подробнее <a href=\"https://ru.wikipedia.org/wiki/Скалярное_произведение\">скалярное произведение</a> векторов. Для того, чтобы получить веткор из изображения размерностью 32х32х3, достаточно \"выпрямить\" его, получим вектор размерностью 1х3072.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опять рассмотри сравнение на конкретном примере с кошкой. Построим изображения картинки, шаблона, а затем посчитаем расстояние между ними используя наш новый метод перемножения. Чем желтее цвет - тем больше изображени похоже на шаблон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3, figsize=(10,5))\n",
    "\n",
    "indexes = np.where(y_train == 3)  # получаем индекс для каждого класса\n",
    "mask = np.zeros(len(y_train), dtype=bool)  # создаем маску из значений False\n",
    "mask[indexes, ] = True  # меняем значения на True по индексам\n",
    "images = x_train[mask]\n",
    "\n",
    "img = images[0].reshape(3, 32, 32).transpose(1, 2, 0).astype(int)\n",
    "template = templates[3].reshape(3, 32, 32).transpose(1, 2, 0).astype(int)\n",
    "residual = np.mean(np.abs(img*template),-1)\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(template)\n",
    "r_plot = ax[2].imshow(residual, cmap='inferno_r')\n",
    "\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(r_plot, cax=cax, orientation='vertical', label='Расстояние')\n",
    "\n",
    "ax[0].set_title('Изображение')\n",
    "ax[1].set_title('Шаблон')\n",
    "ax[2].set_title('D = Изображение - Шаблон')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Математическая запись"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-08.jpg\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "Обозначим входное изображение как $x$, а шаблон для первого из классов как $w_0$\n",
    "Элементы пронумеруем подряд 1,2,3 … $n$\n",
    "То есть развернем матрицу пикселей изображения в вектор. \n",
    "\n",
    "\n",
    "\n",
    "Тогда результат сравнения изображения с этим шаблоном будет вычисляться по формуле: $x[0]*w0[0] + x[1]*w0[1] + … x[n-1]*w0[n-1]$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-skalyar.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Эта простая модель лежит в основе практически всех сложных, которые мы будем рассматривать дальше. Внутри мы будем также пользоваться скалярным произведением.\n",
    "\n",
    "В дальнейшем мы будем проходить сверточные сети, они работают очень похоже:\n",
    "мы тоже накладываем шаблон на некоторую матрицу и перемножаем элементы, затем складываем. Единственное отличие – обычно ядро свертки меньше, чем размер самого изображения. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (метод опорных векторов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Отличное видео про SVM от Stat Quest которое все объясняет](https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим одномерный пример. У нас есть данные по массе мышей. Часть из них определена как нормальные, а часть как мыши с ожирением. Что бы их отдельить друг от друга, нам достаточно одного критерия. Мы можем посмотреть на график, и визуально определить предельную массу, после которой мышки будут жирненькими"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    X = np.hstack([np.random.uniform(14,21, total_len//2), np.random.uniform(24,33, total_len//2)])\n",
    "    y = np.hstack([np.zeros(total_len//2), np.ones(total_len//2)])\n",
    "    return X,y\n",
    "\n",
    "def plot_data(X, y, total_len=40, s=50, threshold=21.5):\n",
    "    ax = sns.scatterplot(x=X, y=np.zeros(len(X)), hue=y, s=s)\n",
    "    ax.axvline(threshold, color='red', ls='dashed')\n",
    "\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Нормальные', 'С ожирением'])\n",
    "    ax.set(xlabel='Масса, г');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X,y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, y, total_len=total_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пользуясь нашим простым критерием, попробуем классифицировать каких-то новых мышей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.random.uniform(14,30, 5)\n",
    "\n",
    "def classify(X, threshold=21.5):\n",
    "    y = np.zeros_like(X)\n",
    "    y[X > threshold] = 1\n",
    "    return y\n",
    "\n",
    "total_len = 40\n",
    "X,y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, y, total_len=total_len)\n",
    "ax = plot_data(X_test, classify(X_test), total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если наши мыши, находятся тут?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([21.45, 22.5])\n",
    "\n",
    "total_len = 40\n",
    "X,y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, y, total_len=total_len)\n",
    "ax = plot_data(X_test, classify(X_test), total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения нашего классификатора - все четко. Больше порогового значения - значит перевес, меньше - значит нормальные. Но с точки зрения здравого смысла, логичнее было бы классифицировать обоих мышей как нормальных, так как они значительно ближе к нормальным, чем к ожиревшим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вооружившись этим новым знанием, попробуем классифицировать наших отъевшихся мышек по умному. Возьмум крайние точки в каждом класстере. И в качестве порогового значения будем использовать среднее между ними"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = generate_data(total_len=total_len)\n",
    "most_obeese_normal = X[y==0].max()\n",
    "most_normal_obeese = X[y==1].min()\n",
    "\n",
    "threshold = np.mean([most_obeese_normal, most_normal_obeese])\n",
    "\n",
    "X_test = np.array([21.5, 23])\n",
    "ax = plot_data(X, y, total_len=total_len, threshold=threshold)\n",
    "ax = plot_data(X_test, classify(X_test, threshold=threshold), total_len=total_len, s=300, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать насколько наша мышь близка к тому что бы оказаться в другом классе. Такое расстояние называется **margin**. И оно считается как $\\mathrm{margin} = |\\mathrm{threshold} - \\mathrm{observation}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = np.abs(X_test - threshold)\n",
    "print(margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если мы посчитаем margins для наших крайних точек `most_obeese_normal` и `most_normal_obeese`, мы найдем самое большое возможное значение margin для нашего классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_0 = np.abs(most_obeese_normal - threshold)\n",
    "margin_1 = np.abs(most_normal_obeese - threshold)\n",
    "print(margin_0, margin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой классификатор, мы называем **Maximum Margin Classifier**. Он хорошо работает в случае, когда все данные размечены аккуратно. Теперь рассмотрим более реалистичный пример, где что-то пошло не так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_data(total_len=40):\n",
    "    X = np.hstack([np.random.uniform(14,21, total_len//2), np.random.uniform(24,33, total_len//2)])\n",
    "    y = np.hstack([np.zeros(total_len//2), np.ones(total_len//2)])\n",
    "    indx = np.where(X == X[y==1].min())[0]\n",
    "    y[indx] = 0\n",
    "    s = np.ones_like(X)*50\n",
    "    s[indx] = 300\n",
    "    return X,y,s\n",
    "\n",
    "total_len = 40\n",
    "X,y,s = generate_realistic_data(total_len=total_len)\n",
    "ax = plot_data(X, y, total_len=total_len, s=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае, наш **Maximum Margin Classifier** работать не будет. Исходя из этого, мы можем придти к выводу, что наш классификатор очень чувствителен к выбросам. Давайте подумаем можно ли это как-то исправить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы можем разрешить нашему классификатору ошибаться. Если мы будем использовать в качестве порогового значения не самое крайнее, а следующее за ним - мы промажем в классификации конкретно этой странной точки. Но в целом будем правы. Margin определенный таким образом называется **Soft margin**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "X,y,s = generate_realistic_data(total_len=total_len)\n",
    "\n",
    "most_obeese_normal = np.sort(X[y==0])[-2]\n",
    "most_normal_obeese = X[y==1].min()\n",
    "\n",
    "threshold = np.mean([most_obeese_normal, most_normal_obeese])\n",
    "\n",
    "X_test = np.array([20.5, 25])\n",
    "ax = plot_data(X, y, total_len=total_len, threshold=threshold)\n",
    "ax = plot_data(X_test, classify(X_test, threshold=threshold), total_len=total_len, s=300, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И тут то мы и подобрались к проблеме, из-за которой страдает практически все машинное обучение - **bias/variance trade-off**. Другими словами, перед тем, как мы позволили нашему классификатору ошибаться - у нас был маленький bias (предубеждение, смещение), и он плохо работал с новыми данными (high variance). После того как мы разрешили классификатору ошибаться - у классификатора теперь значительный bias, но при этом он лучше определяет класс - low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но почему мы решили взять именно следующее значение? Почему не через 2? Откуда мы знаем что это лучший из возможных вариантов? А ни откуда не знаем. Что бы узнать какой из margins лучше, нам стоит численно это проверить и посчитать сколько раз мы ошибемся, если возьмем в качесве порогового значения между каждой парой точек. Для этого мы вновь воспользуемся *кросс-валидацией*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "X,y,s = generate_realistic_data(total_len=total_len)\n",
    "\n",
    "indxs = []\n",
    "accuracies = []\n",
    "thresholds = []\n",
    "\n",
    "X_test, y_test = generate_data(total_len=total_len)\n",
    "\n",
    "for i in range(total_len//2):\n",
    "    for j in range(total_len//2-1):\n",
    "        most_obeese_normal = np.sort(X[y==0])[-i]\n",
    "        most_normal_obeese = np.sort(X[y==1])[j]\n",
    "\n",
    "        threshold = np.mean([most_obeese_normal, most_normal_obeese])\n",
    "        \n",
    "        y_pred = classify(X_test, threshold=threshold)\n",
    "        accuracy = np.mean(y_test == y_pred)\n",
    "        indxs.append((-i,j))\n",
    "        accuracies.append(accuracy)\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "print(f'Точность = {np.max(accuracies)*100}%')\n",
    "print(f'Индексы, которые лучше всего подходят', indxs[np.argmax(accuracies)])\n",
    "\n",
    "best_treshold = thresholds[np.argmax(accuracies)]\n",
    "print('Лучшее пороговое значение %.2f'% best_treshold)\n",
    "\n",
    "ax = plot_data(X_test, classify(X_test, threshold=best_treshold), \n",
    "               total_len=total_len, \n",
    "               s=200, \n",
    "               threshold=best_treshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда для классификатора используется **Soft Margin** - такой классификатор называют **Soft Margin Classifier** или по другому - **Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D класификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример, где мы измерили не только вес мышей, но и их длинну от хвоста до носа. Мы можем вновь применить наш метод Support Vector Classifier, и теперь классы разделяет не одно пороговое значение (по сути точка), а линия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import svm\n",
    "\n",
    "def generate_2d_data(total_len=40):\n",
    "    X, y = make_blobs(n_samples=total_len, centers=2, random_state=6)\n",
    "    X[:,0] += 10 \n",
    "    X[:,1] += 20 \n",
    "    return X,y\n",
    "\n",
    "def plot_data(X, y, total_len=40, s=50, threshold=21.5):\n",
    "    ax = sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, s=s)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Нормальные', 'С ожирением'])\n",
    "    ax.set(xlabel='Масса, г', ylabel='Длинна, см');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X,y = generate_2d_data(total_len=total_len)\n",
    "ax = plot_data(X, y, total_len=total_len)\n",
    "\n",
    "# Код для иллюстрации, разберемся что тут происходит позднее в лекции \n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы добивим еще одно измерение - возраст, мы обнаружим что наши данные стали трехмерными, а разделяет их теперь не линия, а плоскость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def generate_3d_data(total_len=40):\n",
    "    X, y = make_blobs(n_samples=total_len, centers=2, random_state=6, n_features=3)\n",
    "    X[:,0] += 10 \n",
    "    X[:,1] += 20 \n",
    "    X[:,2] += 10\n",
    "    return X,y\n",
    "\n",
    "def plot_data(X, y, total_len=40, s=50, threshold=21.5):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(xs=X[:,0], ys=X[:,1], zs=X[:,2], c=y, s=s, cmap='Set1')\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    ax.plot_surface(XX, YY, XX*YY*0.2, alpha=0.2)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Нормальные', 'С ожирением'])\n",
    "    ax.set(xlabel='Масса, г', ylabel='Длинна, см', zlabel='Возраст, дни');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X,y = generate_3d_data(total_len=total_len)\n",
    "ax = plot_data(X, y, total_len=total_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если бы у нас было 4 измерения и больше (например вес, длинна, возраст, кровяное давление), то многомерная плоскость которая бы разделяла наши классы - называлась бы **гиперплоскость** (рисовать мы ее конечно же не будем). Чисто технически, и точка, и линия - тоже гиперплоскости. Но все же гиперплоскостью принято называть то, что нельзя нарисовать на бумаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но данные не всегда разделяются так хорошо как в случае нашего мышиного датасета. Напрмиер рассмотрим такой пример: у нас есть данные по дозировке лекарства и 2 класса - пациенты которые поправились, и те, скажем так что бы никого не расстраивать, которым лучше не стало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patients_data(total_len=40):\n",
    "    X = np.random.uniform(0,50, total_len)\n",
    "    y = np.zeros_like(X)\n",
    "    y[(X > 15) & (X < 35)] = 1\n",
    "    return X,y\n",
    "\n",
    "def plot_data(X, y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=X, y=np.zeros(len(X)), hue=y, s=s)\n",
    "\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Поправились', 'Болеют'])\n",
    "    ax.set(xlabel='Доза, мг');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X,y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(X, y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно мы не можем найти такое пороговое значение, которое будет разделять наши классы на больных и здоровых, а следовательно и Support Vector Classifier работать тоже не будет. И вот тут-то мы наконец-то добрались до **Support Vector Machines**. Для начала, давайте преобразуем наши данные таким образом, что бы они стали 2х мерными. В качестве значений по оси Y будем использовать дозу возведенную в квадрат (**доза**$^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=X[0,:], y=X[1,:], hue=y, s=s)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Поправились', 'Болеют'])\n",
    "    ax.set(xlabel='Доза, мг');\n",
    "    ax.set(ylabel='Доза$^2$');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X_1,y = generate_patients_data(total_len=total_len)\n",
    "X2 = X_1**2\n",
    "X = np.vstack([X_1, X2])\n",
    "\n",
    "plot_data(X, y, total_len=40, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем вновь использовать Support Vector Classifier для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y, total_len=40, s=50)\n",
    "\n",
    "x = np.linspace(0,50,50)\n",
    "xs = [X[0,:][y==1].min(), X[0,:][y==1].max()]\n",
    "ys = [X[1,:][y==1].min(), X[1,:][y==1].max()]\n",
    "\n",
    "# Calculate the coefficients. This line answers the initial question. \n",
    "coefficients = np.polyfit(xs, ys, 1)\n",
    "\n",
    "# Let's compute the values of the line...\n",
    "polynomial = np.poly1d(coefficients)\n",
    "y_axis = polynomial(x)\n",
    "\n",
    "# ...and plot the points and the line\n",
    "plt.plot(x, y_axis, 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внимательный слушатель, возможно уже догадался к чему все идет. Основная идея Support Vector Machine такая:\n",
    "* Начинаем в низкоразмерном пространстве (в случае с дозами - одномерном)\n",
    "* Превращаем наши данные в многомерные\n",
    "* Применяем Support Vector Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут возникает резонный вопрос - почему мы решили возвести в квадрат? Почему не в куб? Или наоборт не извлечь корень? Как нам решить какое преобразование использовать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM определяет способ, с помощью которого будет преобразовывать данные с помощью некой функции, которая называется **Kernel Function**. Kernel Function может, например быть полиномом (**Polynomial Kernel Function**), который имеет параметр $d$ - сколько размерностей выбрать. \n",
    "Для тех кто забыл, полином от степени n записывается в виде:\n",
    "\n",
    "$a_dx^d + a_{d-1}x^{d-1} + a_{d-2}x^{d-2} + ... + a_0$\n",
    "\n",
    "где $a_d$ - какое-то натуральное число.\n",
    "\n",
    "Например полином степени d=1 можно записать в виде:\n",
    "\n",
    "$a_1x^1 + a_0$\n",
    "\n",
    "Полином степени d=2:\n",
    "\n",
    "$a_2x^2 + a_1x^1 + a_0$\n",
    "\n",
    "И так далее.\n",
    "\n",
    "SVM перебирает значения $d$ по очереди в заданых пользователем пределах (например от 1ого до 3х) и с помощью кросс-валидации считает **какая ошибка классификатора при заданном $d$**. То $d$ при котором ошибка меньше всего фиксируется и используется для классификации новых точек."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Kernel Function** можно переписать в следующем виде: $(a \\times b + r)^d$, где $a$ и $b$ точки (вектора), которые мы сравниваем, $d$ - степерь полинома, а $c$ - свободный параметр, позволяющий компенсировать влияние членов высшего порядка на члены низшего порядка в полиноме. Когда c = 0, ядро называется однородным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим конкретный пример, возьмем наши дозировки лекарств. И посчитаем однородную Polynomial Kernel Function для $d$ от 1 до 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patients_data(total_len=40):\n",
    "    X = np.random.uniform(0,50, total_len)\n",
    "    y = np.zeros_like(X)\n",
    "    y[(X > 15) & (X < 35)] = 1\n",
    "    return X,y\n",
    "\n",
    "def plot_data(X, y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=X, y=np.zeros(len(X)), hue=y, s=s)\n",
    "\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Поправились', 'Болеют'])\n",
    "    ax.set(xlabel='Доза, мг');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X,y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(X, y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще часто используют **Radial Kernel Function**, или по другому ее еще называют **Radial Basis Function Kernel (RBF)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример простой линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ненадолго отвлечемся от SVM и рассмотрим другую задачу. В этой задаче мы будем прогнозировать успеваемость студента, в зависимости от количества часов, которые он учил материал. Это простая задача линейной регрессии, поскольку она включает всего две переменные.\n",
    "\n",
    "Регрессия - это статистический метод, используемый в финансах, инвестировании и других дисциплинах, который пытается определить силу и характер связи между одной зависимой переменной (обычно обозначаемой Y) и рядом других переменных (известных как независимые переменные). В задачах регрессии, как и раньше, мы будем пытаться минимизировать ошибку между предсказанием и истинными данными. \n",
    "\n",
    "\"Регрессия\" происходит от слова \"регресс\", которое, в свою очередь, происходит от латинского \"regressus\" - возвращаться (к чему-либо). В этом смысле регрессия - это техника, которая позволяет \"вернуться назад\" от беспорядочных, трудно интерпретируемых данных к более четкой и осмысленной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "!wget http://edunet.kea.su/repo/src/L02_Linear_classifier/datasets/student_scores.csv\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что там в нем. Видим что у нас есть два признака - часы и результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/content/student_scores.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построем график зависимости одного от другого, а так же отобразим распрпеделения каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.jointplot(data=dataset, x=\"Scores\", y=\"Hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.iloc[:, :-1].values # значения столбца Hours\n",
    "y = dataset.iloc[:, 1].values # значения столбца Score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель для линейной регрессии. Что бы не писать с нуля, воспользуемся готовой из библиотееки `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression() # создаем модель линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train) # обучаем на наших данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points = np.linspace(\n",
    "    min(X_train), max(X_train), 100\n",
    ")  # создаем 100 точек в пределах min/max X_train\n",
    "y_pred = regressor.predict(x_points)  # предсказываем результат для этих точек\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X_train, y_train, \"o\", label=\"Scores\")\n",
    "plt.plot(x_points, y_pred, label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_ ))\n",
    "plt.title(\"Hours vs Percentage\", size=15)\n",
    "plt.xlabel(\"Hours Studied\", size=15)\n",
    "plt.ylabel(\"Percentage Score\", size=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем предсказание для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)  # делаем предсказание тестовой выборки\n",
    "\n",
    "x_points = np.linspace(\n",
    "    min(X_test), max(X_test), 100\n",
    ")  # создаем 100 точек в пределах min/max X_train\n",
    "y_pred = regressor.predict(x_points)  # предсказываем результат для этих точек\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X_test, y_test, \"o\", label=\"Scores\")\n",
    "plt.plot(x_points, y_pred, label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_ ))\n",
    "plt.title(\"Hours vs Percentage\", size=15)\n",
    "plt.xlabel(\"Hours Studied\", size=15)\n",
    "plt.ylabel(\"Percentage Score\", size=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит не плохо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики для наших значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "print(\"Mean Absolute Error: %9.2f\" % metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(\"Mean Squared Error: %10.2f\" % metrics.mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: %5.2f\" % np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Геометрическая интерпретация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрались с тем что такое регрессия и с чем ее едят, вернемся к нашим картинкам. Как можно применить регрессию для классификации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим у нас есть только 2 класса. Как можно использовать регрессию для того что бы определить относится ли изображение к классу 0 или к классу 1? В упрощенном варианте, задача будет состоять в том, что бы провести разделяющую плоскость (прямую) между 2-мя классами. Например мы можем провести прямую через 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/2lecture_classifier_2.png\" width=\"270\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другую ситуацию, в этом случае, мы не можем просто провести прямую через 0. Но можем отступить от 0 на какое то расстояние и провести ее там. Вспомнил что уравнение прямой это $y=wx+b$, где $b$ - это смещение (*bias*). Соответственно если b != 0, то прямая через 0 проходить не будет, а будет проходить через значение b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/2lecture_classifier.png\" width=\"270\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Classification Loss Visualization\n",
    "](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)\n",
    "\n",
    "\n",
    "Если у нас есть несколько классов (несколько шаблонов), мы можем для каждого из них посчитать уравнение $y_{i} = w_{i}x_{i}+b_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-09.jpg\" width=\"400\">\n",
    "\n",
    "[Linear Classification Loss Visualization\n",
    "](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На картинке нас интересуют 3 класса. Соответвенно мы можем записать систему линейных уравненией:\n",
    "\n",
    "\\begin{aligned}\n",
    "y_{0} = w_{0}x_{0} + b_{0} \\\\\n",
    "y_{1} = w_{1}x_{1} + b_{1} \\\\\n",
    "y_{2} = w_{2}x_{2} + b_{2} \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы их можем собрать в матрицу, тогда получится следующее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-10.jpg\" width=\"750\">\n",
    "\n",
    "У нас есть матрица коэффициентов, которые мы каким-то образом подобрали, пока ещё не понятно как. Есть вектор $x$, соответствующий изображению. \n",
    "\n",
    "Мы умножаем вектор на матрицу, получаем нашу гиперплоскость для четырехмерного пространства в данном случае. Чтобы оно не лежало в 0, мы должны добавить смещение. И мы можем сделать это после, но можно взять и этот вектор смещения (вектор **b**) просто приписать к матрице **W**.\n",
    "\n",
    "Что будет выходом такой конструкции? Мы умножили матрицу весов на наш вектор, соответствующий изображению, получили некоторый отклик. По этому отклику мы так же, как и при реализации метода ближайшего соседа можем судить: если он больше остальных, то мы предполагаем, что это кошка.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img = np.array([56, 231, 24, 2])\n",
    "w_cat = np.array([0.2, -0.5, 0.1, 2.0])\n",
    "print(\"Изображение \", img)\n",
    "print(\"Вектор весов \", w_cat)\n",
    "print(\"Умножаем вектор изображения и весов \", img * w_cat)\n",
    "print(\"Считаем сумму \", (img * w_cat).sum())\n",
    "print(\"Добавляем смещение \", (img * w_cat).sum() + 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-07.jpg\" width=\"600\">\n",
    "\n",
    "\n",
    "Собирая все вместе, получаем какое-то компактное представление, что у нас есть некоторая функция, на вход которой мы подаем изображение, и у нее есть параметры (веса). Пока происходит просто умножение вектора на матрицу, в дальнейшем это может быть что-то более сложное, функция будет представлять какую-то более сложную модель. А на выходе ( для классификатора) мы получаем числа, которые интерпретируют уверенность модели в том, что изображение принадлежит к определенному классу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-11.jpg\" width=\"450\">\n",
    "\n",
    "\n",
    "\n",
    "Соответственно, эти коэффициенты, которые являются весами модели, надо каким-то образом подбирать. Но прежде, чем подбирать коэффициенты, давайте определимся со следующим: как мы будем понимать, что модель работает хорошо или плохо? Ведь она возвращает достатотчно абстрактные числа, которые нужно уметь интерпретировать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка результата"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь SVM-loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к Support Vector Machine. Мы разобрали, как в принципе работает метод опорных векторов (SVM) — набор схожих алгоритмов обучения с учителем, использующихся для задач классификации и регрессионного анализа. Особым свойством метода опорных векторов является непрерывное уменьшение ошибки классификации и увеличение зазора. \n",
    "\n",
    "Теперь разберемся как SVM работает на практике. Вновь вернемся к нашему датасету CIFAR10. Мы же уже утверждали, что SVM работает с векторами (в частном случае с точками), а значит и с векторами размером (H,W,C) SVM работать тоже будет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-12.jpg\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По аналогии с тем, что мы уже делали, мы можем сравнивать отклик на ключевой класс  (про который нам известно, что он на изображении, так как у нас есть метка этого класса) с остальными. Соответственно, мы подали изображение кошки и получили на выход вектор. Чем больше значение, тем больше вероятность того, что, по мнению модели, на изображении этот класс. Для кошки в данном случае это значение 2.9. Хорошо это или плохо? Нельзя сказать, пока мы не проанализировали остальную часть вектора. Если бы мы могли посмотреть на все значения в векторе, мы бы увидили, что есть значения больше, то есть в данном случае модель считает, что это собака, а не кошка, потому что для собаки значение максимально. \n",
    "\n",
    "На основании этого можно построить некоторую оценку. Давайте смотреть на разницу правильного класса с неправильными. В зависимости от того, насколько уверенность в кошке будет больше остальных, настолько будет наша модель хорошей.\n",
    "\n",
    "Но поскольку нам важна не работа модели на конкретном изображении, а важно оценить ее работу в целом, то эту операцию нужно проделать либо для всего датасета, либо для некоторой выборки, которую мы подаем на вход и подсчитываем средний показатель. Этот показатель (насколько хорошо работает модель), называется функцией потерь, или **loss функцией**. Называется она так потому, что она показывает не то, насколько хорошо работает модель, а то, насколько плохо. \n",
    "\n",
    "Дальше будет понятно, почему так удобнее (разница только в знаке). Как это посчитать для всего датасета?\n",
    "\n",
    "Мы каким-то образом считаем loss для конкретного изображения, потом усредняем по всем изображениям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано: 3 учебных примера, 3 класса. При некотором W баллы f (a, W) = Wx равны:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-13_1.jpg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь показывает, насколько хорош наш текущий классификатор.\n",
    "\n",
    "Дан датасет примеров:\n",
    "\n",
    "$\\begin{Bmatrix} (x_i,y_i)  \\end{Bmatrix}_{i=1}^N \t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где **$x_i$** изображение и **$y_i$** метка (число).\n",
    "\n",
    "Потери по набору данных — это среднее значение потерь для примеров:\n",
    "\n",
    "$ L = {1 \\over N}\\sum_iL_i(f(x_i,W),y_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова возьмём наши обучающие примеры **$x_i$** и метки классов **$y_i$**. Представим прогнозы классификатора (оценки) в виде векторов: **$s = f(x_i, W)$**. Для каждого отдельного примера просуммируем значения оценок всех **$y$**, кроме истинной метки **$y_i$**. То есть, мы получим сумму значений всех неправильных категорий. \n",
    "\n",
    "Теперь посчитаем разницу между ложными прогнозами и истинной оценкой: **$s_j − s_yi$** ; **$j≠y_i$**. Если истинная оценка больше, чем сумма всех неправильных прогнозов плюс некоторый дополнительный «зазор» (установим его равным 1) — значит, полученный балл для правильной категории намного превосходит любую ошибочную оценку. Это и будет наша функция потерь.\n",
    "\n",
    "В символьном виде это выглядит так:\n",
    "\n",
    "\n",
    "$f(n) = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}$\n",
    "\n",
    "$=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление SVM loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Не самая мощная, но достаточно интуитивно понятная loss функция –  SVM loss. \n",
    " \n",
    "Логика такая: если у нас уверенность модели в правильном классе большая, то модель работает хорошо и loss для данного конкретного примера должен быть равен нулю. Если есть класс, в котором модель уверена больше, чем в правильном, то loss должен быть не равен нулю, а отображать какую-то разницу, поскольку модель сильно ошиблась. При этом есть ещё одно соображение: что будет, если на выходе у правильного и ошибочного класса будут примерно равные веса? То есть, например, у кошки было бы 3.2, а у машины не 5.2, а 3.1. В этом случае ошибки нет, но понятно, что при небольшом изменении в данных (просто шум) скорее всего она появится.\n",
    " \n",
    "\n",
    "\n",
    "То есть модель плохо отличает эти классы. Поэтому мы и вводили некоторый зазор, который должен быть между правильным и неправильным ответом. \n",
    "\n",
    "Посмотрим на изображение снизу, у нас есть два класса, фиолетовые треугольники и синие квадраты разделенные зазором. Также можем увидеть желтые треугольники и квадраты, это ошибочно распознанные классы.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/svm_decision_boundary.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И тоже учитывать его в loss функции: то есть сравнивать его результат для правильного класса не с чистым выходом для другого, а добавить к нему некоторую дельту (в данном случае – 1(единица)). Смотрим: если разница больше 0, то модель работает хорошо и loss равно нулю. Если нет, то мы возвращаем эту разницу и loss будет складываться из этих индивидуальных разниц.\n",
    "\n",
    "$f(n) = \\sum_{j\\neq y_i}\\begin{cases}\n",
    "  0,  & \\mbox{если } s_{y_i}\\geq s_j+1\\mbox{} \\\\\n",
    "  s_j-s_{y_i}+1, & \\mbox{если наоборот, то} \\mbox{}\n",
    "\\end{cases}$\n",
    "\n",
    "$=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "Ниже пример того, как считается loss.\n",
    "\n",
    "\n",
    "Считаем функцию потерь для 1го изображения:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM1.jpg\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также считаем потери для 2го и 3его изображения:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM2_1.jpg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения losses получились следующие:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-SVM4.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем среднее значение loss для всего датасета:\n",
    "\n",
    "$ L = {1 \\over N}\\sum_{i=1}^N L_i$\n",
    "\n",
    "$L={2.9 + 0 + 12.9 \\over 3} = 5.27$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM loss\n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$\n",
    "\n",
    "\n",
    "Как записать это в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "x_train = np.zeros((0, 3072))\n",
    "y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    x_train = np.append(x_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    y_train = np.append(y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "x_test = np.array(test[b\"data\"])\n",
    "y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_ru = [\n",
    "    \"Самолет\",\n",
    "    \"Автомобиль\",\n",
    "    \"Птица\",\n",
    "    \"Кошка\",\n",
    "    \"Олень\",\n",
    "    \"Собака\",\n",
    "    \"Лягушка\",\n",
    "    \"Лошадь\",\n",
    "    \"Корабль\",\n",
    "    \"Грузовик\",\n",
    "]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(10, 3072) * 0.0001 # создаем случайные веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assa = np.maximum(0, W.dot(x_train[0]) - W.dot(x_train[0])[6] + 1)\n",
    "assa[6] = 0\n",
    "np.sum(assa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(10, 3072) * 0.0001  # создаем случайные веса\n",
    "\n",
    "\n",
    "def Li_svm(x, y, W):\n",
    "    y = int(y)  # меняем тип данных, чтобы использовать как индекс\n",
    "    scores = W.dot(x)  # умножаем веса на изображение\n",
    "    margins = np.maximum(0, scores - scores[y] + 1)  # считаем loss\n",
    "    margins[y] = 0  # не учитываем текущий классс равнвый 1\n",
    "    return np.sum(margins)  # возвращаем сумму значений\n",
    "\n",
    "\n",
    "L0 = Li_svm(x_train[0], y_train[0], W)\n",
    "print(\"Loss on first image\", L0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-15.jpg\" width=\"900\">\n",
    "\n",
    "Собираем все вместе: считаем loss для всего датасета, усредняя ее.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как будет выглядеть график этой SVM-loss?\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/hinge_loss.gif\" width=\"400\">\n",
    "\n",
    "\n",
    "Функция потерь для ситуации, когда зазор большой, будет равна 0. Она будет меняться только тогда, когда у нас есть ошибка. Причем будет меняться линейно.\n",
    "\n",
    "Как мы можем использовать эту loss функцию? Допустим, мы знаем, что у нас модель будет работать не очень хорошо (выдает большую ошибку). Можно попытаться найти минимум этой loss функции, то есть подобрать такое W, когда loss функция обратится в 0  (в данном случае), а в общем случае (если какое-то количество данных нельзя обратить в 0), мы можем попытаться найти ее минимум.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обновления весов методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-18.png\" width=\"750\">\n",
    "\n",
    "Идея в следующем: у нас есть некоторая поверхность, на которой будет точка минимума функции. Мы должны обновлять веса таким образом, чтобы смещаться в сторону этой точки. Loss показывает, как возрастает функция потерь. Так как потери - это плохо, мы должны их минимизировать, то есть мы должны уменьшать веса в сторону, обратную росту этой функции.\n",
    "\n",
    "Если переходить на случай n-мерного (в данном случае трехмерного) пространства, здесь достаточно очевидна аналогия с поверхностью: у нас есть поверхность земли, которая описывается координатами $x, y, z$. Если мы движемся по этой поверхности, высота $(z)$ будет зависеть от $x, y$. \n",
    "\n",
    "$x, y$ - это координаты, мы можем записать их как вектор. Наша функция будет работать с вектором координат и будет выдавать скаляр, третью координату. Если в качестве координат мы будем использовать веса нашей модели, а в качестве $z$ - loss, то аналогия станет полной. Наша задача сведется к тому, чтобы найти такой набор весов, при котором значение функции будет минимально. То есть мы окажемся в каком-то минимуме, где ошибка будет минимально возможна для этих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы предположить, где он может находиться, надо понимать, в какую сторону функция растет, а в какую - убывает. Для этого существует производная.\n",
    "\n",
    "Поскольку у нас функция от нескольких переменных, если мы будем брать от нее производную, у нас получится вектор частных производных, то есть градиент этой функции, который будет показывать, как она меняется в каждом направлении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент loss функции\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент — вектор, своим направлением указывающий направление наибольшего возрастания некоторой величины **φ**, значение которой меняется от одной точки пространства к другой (скалярного поля), а по величине (модулю) равный скорости роста этой величины в этом направлении.\n",
    "\n",
    "Например, если взять в качестве **φ**  высоту поверхности земли над уровнем моря, то её градиент в каждой точке поверхности будет показывать «направление самого крутого подъёма», и своей величиной характеризовать крутизну склона.\n",
    "\n",
    "Другими словами, градиент — это производная по пространству, но в отличие от производной по одномерному времени, градиент является не скаляром, а векторной величиной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случая трёхмерного пространства градиентом скалярной функции **φ=φ(x,y,z)** координат **(x,y,z)** называется векторная функция с компонентами\n",
    "$$\\frac{\\partial\\varphi}{\\partial x}, \\frac{\\partial\\varphi}{\\partial y}, \\frac{\\partial\\varphi}{\\partial z}$$.\n",
    "\n",
    "Если **φ**  — функция **n** переменных **X1...xn**, то её градиентом называется **n**-мерный вектор\n",
    "$$\\frac{\\partial\\varphi}{\\partial x_1},...,\\frac{\\partial\\varphi}{\\partial x_n}$$\n",
    "\n",
    "\n",
    "\n",
    "компоненты которого равны частным производным **φ** по всем её аргументам.\n",
    "\n",
    "Размерность вектора градиента определяется, таким образом, размерностью пространства (или многообразия), на котором задано скалярное поле, о градиенте которого идёт речь.\n",
    "Оператором градиента называется оператор, действие которого на скалярную функцию (поле) даёт её градиент. Этот оператор иногда коротко называют просто «градиентом»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$W$ - матрица(вектор) весов\n",
    "\n",
    "$L$ - функция потерь\n",
    "\n",
    "$\\partial W = W_2 - W_1$\n",
    "\n",
    "$\\partial L = L_2 - L_1$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W}=\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial W_1} \\\\\n",
    "\\frac{\\partial L}{\\partial W_2} \\\\\n",
    "... \\\\\n",
    "\\frac{\\partial L}{\\partial W_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Наша задача будет сводиться к тому, что мы будем искать градиент loss функции по весам, которые будут состоять из производной по каждому направлению. Поскольку у нас здесь числа, можно считать этот градиент численно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Численный расчет производной\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-gradient_spusk.png\" width=\"650\">\n",
    "\n",
    "\n",
    "\n",
    "Посчитаем градиент приближенно, воспользовавшись определением (в формуле аргумент обозначен как $x$, у нас же аргументом будет $W$):\n",
    "На нулевом шаге у нас есть $W_0$ найдем $L_0 = Loss(f(W_0,x))$\n",
    "Прибавим к первому элементу $W_0$ небольшую величину  $h$ = 0.0001 и получим новую матрицу весов $W_1$ отличающуюся от $W_0$ на один единственный элемент.\n",
    "\n",
    "Найдем Loss от $\\frac {W_1}  {L_1} = Loss(f(W_1,x))$\n",
    "По определению производной $\\frac {dL}{W_0} = \\frac{( L_1 - L_0 )}   {h}$\n",
    "Повторяя этот процесс для каждого элемента из $W$ найдем вектор частных производных то есть градиент $\\frac{dL}{dW}$.\n",
    "\n",
    "Плюсы:\n",
    "Это просто. \n",
    "\n",
    "Проблемы:\n",
    "\n",
    "1. Это очень долго, нам придется заново искать значение loss функции для каждого $W_i$.\n",
    "\n",
    "2. Это неточно, так как по определению приращение $h$ бесконечно мало, а мы используем конкретное пусть и небольшое число. И если мы сделаем его слишком маленьким то столкнемся с ошибками связанными с округлением в памяти компьютера.\n",
    "\n",
    "\n",
    "Поэтому данный метод может использоваться как проверочный. \n",
    "А на практике вместо него используется **аналитический расчет градиента**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналитический расчет производной от SVM Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые производные \n",
    "\n",
    "$$x' = \\frac {\\delta x} {\\delta x} = 1$$ \n",
    "\n",
    "$$(x^2)' = \\frac {\\delta x^2} {\\delta x} = 2x$$\n",
    "\n",
    "$$(\\log x)'  = \\frac {\\delta \\log x} {\\delta x} = \\frac 1 x $$\n",
    "\n",
    "$$(e^x)'  = \\frac {\\delta e^x} {\\delta x} = e^x $$\n",
    "\n",
    "$$\\frac {\\delta cf(x)} {\\delta x}= c \\cdot \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$$\\frac {\\delta f(x) + c} {\\delta x}= \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$c$ - константа, не зависящая от $x$\n",
    "\n",
    "$$ \\frac {\\delta [f(x) + g(x)]} {\\delta x} = \\frac {\\delta f(x)} {\\delta x}  + \\frac {\\delta g(x)} {\\delta x} $$ \n",
    "\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta x} = 2x  $$\n",
    "так как $y$ по отношению к $x$ - константа и мы меняем только $x$\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta y} = 3y^2  $$\n",
    "так как $x$ по отношению к $y$ - константа и мы меняем только $y$\n",
    "\n",
    "$$(e^y)'  = \\frac {\\delta e^y} {\\delta x} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-rule\n",
    "\n",
    "Производная функции $f(g)$:\n",
    "\n",
    "$$\\frac {\\delta f} {\\delta g}$$\n",
    "\n",
    "Пусть $g$  на самом деле не просто переменная, а зависит от $h$. Тогда производная от $f$ по $g$ **не меняется**, а производная $f$ по $h$ запишется следующим образом:\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h))} {\\delta h} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h}$$\n",
    "\n",
    "Пусть теперь $h$ зависит от $x$. Все аналогично\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h(x)))} {\\delta x} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h} \\frac {\\delta h} {\\delta x}$$\n",
    "\n",
    "Так можно делать до бесконечности, находя производную сколь угодно сложной функции. И, что важно - мы можем считать градиенты частями - посчитать сначала $f$ по $g$, потом $g$ по $h$....\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x}$$\n",
    "\n",
    "$$h = x^2 + 5$$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac {\\delta log(h)} {\\delta h} \\frac {\\delta h} {\\delta x}$$ \n",
    "\n",
    "$$\\frac {\\delta log(h)} {\\delta h} = \\frac 1 h$$\n",
    "\n",
    "$$\\frac  {\\delta h} {\\delta x} = 2x $$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac 1 {x^2 + 5} \\cdot 2x = \\frac {2x} {x^2 + 5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть MSE-loss\n",
    "\n",
    "$$loss = (y - \\hat{y})^2 $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta (y - \\hat{y})^2 } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = 2(y-\\hat{y}) \\cdot -1 = 2 (\\hat{y} - y)$$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = 2 x \\cdot (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE-loss\n",
    "$$MSE = \\frac 1 N \\sum_i(y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "$y_i$ - константы\n",
    "$\\hat{y_i}$ - не являются функциями друг от друга \n",
    "$$\\hat{y} = wx_i + b $$\n",
    "\n",
    "$$\\frac {\\delta MSE} {\\delta w} = \\frac 1 N \\sum \\frac {\\delta (y_i - \\hat{y_i}) ^2} {\\delta \\hat{y_i}} \\frac {\\delta \\hat{y_i}} {\\delta w}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть MAE-Loss\n",
    "\n",
    "$$loss = |y - \\hat{y}| $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}}$$\n",
    "\n",
    "Строго говоря, у модуля не существует производной в 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "x = [i for i in range(-5, 6)]\n",
    "y = [abs(i) for i in range(-5, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, label=\"y = |x|\")\n",
    "plt.title(\"y = |x|\", size=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы можем сказать, что в этой точке производная равна 0.\n",
    "Если аргумент модуля меньше 0, то производная будет -1.\n",
    "\n",
    "Если больше +1\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} = - sign(y - \\hat{y}) =  sign(\\hat{y} - y)$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(-5, 1, 1)]\n",
    "y = [i * 0 - 1 for i in range(6)]\n",
    "x_1 = [i for i in range(0, 6)]\n",
    "y_1 = [i * 0 + 1 for i in range(0, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, \"b\")\n",
    "plt.plot(x_1, y_1, \"b\")\n",
    "plt.plot(0, 0, \"ro\")\n",
    "plt.plot(0, 1, \"bo\")\n",
    "plt.plot(0, -1, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Loss\n",
    "\n",
    "$$b = max(x, y)$$\n",
    "\n",
    "$$b = x~~if~~x > y~~else~~y$$\n",
    "\n",
    "\n",
    "$$\\frac {\\delta b} {\\delta x} =  \\frac {\\delta x} {\\delta x}~~if~~x > y~~else~~\\frac {\\delta y} {\\delta x} = 1~~if~~x > y~~else~~0$$\n",
    "\n",
    "Если $x > y$, то он оказал влияние на $b$. Иначе, его вклада в $b$ НЕ БЫЛО - градиент равен 0\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из: \n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем:\n",
    "\n",
    "$ \\nabla_W\tL(W) = {1 \\over N}\\sum_{i=1}^N \\nabla_W L_i(x_i, y_i, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Дополнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g1.png\" width=\"750\"> \n",
    "\n",
    "\n",
    "Распишем формулу для $L_i$ через скалярное произведение.\n",
    "Затем раскроем сумму.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g2.png\" width=\"750\"> \n",
    "\n",
    "Считаем производную по первому элементу матрицы $W$. \n",
    "Если у нас правильный класс - не первый элемент, то он больше нигде не встретится, кроме как в первой сумме. Поэтому все остальные суммы при подсчете этой частной производной будут константами. Поэтому этот градиент будет равен либо $x_i$,  либо 0 (если эта сумма меньше 0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g3.png\" width=\"750\"> \n",
    "\n",
    "Аналогично для  $w_i$ частные производные будут равны $x_i$ либо нулю, когда выражение больше меньше 0, \n",
    "кроме случая: $j == yi$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g4.png\" width=\"750\"> \n",
    "\n",
    "\n",
    "Эта ситуация для “правильного изображения”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_an_r_g5.png\" width=\"750\">  \n",
    "\n",
    "Вот финальный результат.\n",
    "Общие потери считаются по пакету изображений.\n",
    "“1” в формулах  — это “if”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Не обязательное задание:\n",
    "\n",
    "Поставлю дополнительно 20 баллов тому, кто графически оформит результаты этого эксперимента и напишет вывод о зависимости качества обучения от learning_rate и batch_size (возможно, нужно будет добавить варианты их сочетаний и/или увеличить количество эпох до 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xzf cifar-10-python.tar.gz\n",
    "!ls -l\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "x_train = np.zeros((0, 3072))\n",
    "y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    x_train = np.append(x_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    y_train = np.append(y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "x_test = np.array(test[b\"data\"])\n",
    "y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_ru = [\n",
    "    \"Самолет\",\n",
    "    \"Автомобиль\",\n",
    "    \"Птица\",\n",
    "    \"Кошка\",\n",
    "    \"Олень\",\n",
    "    \"Собака\",\n",
    "    \"Лягушка\",\n",
    "    \"Лошадь\",\n",
    "    \"Корабль\",\n",
    "    \"Грузовик\",\n",
    "]\n",
    "\n",
    "print(f\"Размерность обучающей выборки X: {x_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"Размерность тестовой выборки X: {x_test.shape}, y: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class LinearClassifier:\n",
    "    def __init__(self, labels, batch_size, random_state=42):\n",
    "        self.labels = labels  # названия классов\n",
    "        self.classes_num = len(labels)  # количество классов\n",
    "\n",
    "        np.random.seed(\n",
    "            random_state\n",
    "        )  # устанаваливаем randim.seed чтобы можно было повторить случайные значения\n",
    "        self.W = (\n",
    "            np.random.randn(3073, self.classes_num) * 0.0001\n",
    "        )  # создаем случайные веса, меняем размерность для добавления bias\n",
    "        self.batch_size = batch_size  # значение batch_size\n",
    "\n",
    "    def fit(self, x_train, y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # обнуляем loss\n",
    "        train_len = x_train.shape[0]  # сколько примеров\n",
    "        indexes = list(range(train_len))  # список индексов для train_len\n",
    "        random.shuffle(indexes)  # перемешиваем порядок индексов\n",
    "\n",
    "        for i in range(\n",
    "            0, train_len, self.batch_size\n",
    "        ):  # для все выборки, с шагом batch_size\n",
    "            idx = indexes[\n",
    "                i : i + self.batch_size\n",
    "            ]  # собираем индексы изображений для кажддого батча\n",
    "            x_batch = x_train[idx]  # батч для X\n",
    "            y_batch = y_train[idx]  # батч для Y\n",
    "\n",
    "            x_batch = np.hstack(\n",
    "                [x_batch, np.ones((x_batch.shape[0], 1))]\n",
    "            )  # добавляем bias\n",
    "\n",
    "            loss_val, grad = self.loss(x_batch, y_batch)  # считаем loss и градиент\n",
    "            self.W -= learning_rate * grad  # обновляем веса\n",
    "\n",
    "            loss += loss_val  # суммируем loss\n",
    "        return loss / (train_len)  # высчитываем среднее значение loss\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        current_batch_size = x.shape[0]  # текущий batch_size\n",
    "        loss = 0.0  # обнуляем loss\n",
    "        dW = np.zeros(self.W.shape)  # массив нулей для градиента\n",
    "        for i in range(current_batch_size):  # по каждому изображению в batch_size\n",
    "            scores = x[i].dot(\n",
    "                self.W\n",
    "            )  # умножаем значения изображения на веса, получим вектор размером 10\n",
    "            correct_class_score = scores[\n",
    "                int(y[i])\n",
    "            ]  # получем текущее значения для верного класса\n",
    "            above_zero_loss_count = 0  # счетчик для loss > 0\n",
    "            for j in range(self.classes_num):  # проходимся по всем классам\n",
    "                if j == y[i]:  # определяем текущий класс\n",
    "                    continue\n",
    "                margin = scores[j] - correct_class_score + 1  # считаем значение loss\n",
    "                if margin > 0:  # если больше 0\n",
    "                    above_zero_loss_count += (\n",
    "                        1  # считаем сколько раз loss был больше нуля\n",
    "                    )\n",
    "                    loss += margin  # суммируем loss\n",
    "                    dW[:, j] += x[i]  # задаем значения градиента\n",
    "            dW[:, int(y[i])] -= above_zero_loss_count * x[i]  # корректируем\n",
    "        loss /= current_batch_size  # считаем loss по текущему batch_size\n",
    "        dW /= current_batch_size  # считаем градиент по текущему  batch_size\n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.append(x, 1)  # добавляем единицу в конец массива для bias\n",
    "        scores = x.dot(self.W)  # получаем значения оценки\n",
    "        return np.argmax(scores)  # возвращаем индекс максимального значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def validate(model, x_test, y_test, noprint=False):\n",
    "    correct = 0  # здесь храним количество верных предсказаний\n",
    "    for i, img in enumerate(x_test):  # проходимся по всем изображениям в выборке\n",
    "        index = model.forward(img)  # определяем класс изображения\n",
    "        correct += (\n",
    "            1 if index == y_test[i] else 0\n",
    "        )  # если класс верный то увеличиваем correct на 1\n",
    "        if noprint is False:  # если стоит флаг False\n",
    "            if i > 0 and i % 1000 == 0:  # каждые 1000 значений\n",
    "                print(\n",
    "                    \"Accuracy {:.3f}\".format(correct / i)\n",
    "                )  # выводим значения точност, количество верных ответов/количество попыток\n",
    "    return correct / len(y_test)  # возвращаем общее значение точности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Исследуем зависимость качества обучения от скорости:\")\n",
    "\n",
    "for lr in [1e-2, 1e-8]:\n",
    "    for bs in [256, 2048]:\n",
    "\n",
    "        print(\"-\" * 50, \"\\n\", \"learning_rate =\", lr, \"\\tbatch_size =\", bs)\n",
    "        print()\n",
    "        lc_model = LinearClassifier(labels_ru, batch_size=bs)\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(10):\n",
    "            loss = lc_model.fit(x_train, y_train, learning_rate=lr)\n",
    "            accuracy = validate(lc_model, x_test, y_test, noprint=True)\n",
    "            if best_accuracy < accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_epoch = epoch\n",
    "            print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор шага обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-change_weight.png\" width=\"450\"> \n",
    "\n",
    "<!-- [Визуализация](https://docs.google.com/file/d/0Byvt-AfX75o1ZWxMRkxrUFJ2ZUE/preview)\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг обучения - некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время чтобы тот же процесс не шел слишком медленно (как на графике слева)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier-learning_rate.png\" > \n",
    "\n",
    " \n",
    "\n",
    "Пока изменения loss функции достаточно большие, сам по себе градиент тоже большой. За счет этого можно двигаться быстро. Когда он будет уменьшаться, мы должны оказаться рядом с этим минимумом, и шаг, который мы выбрали, не должен мешать этому процессу. \n",
    "\n",
    "Бывают ситуации, когда шаг можно менять в самом процессе обучения, когда в начале обучения модели шаг большой, потом, по мере того, как она сходится, чтобы более четко найти минимум, шаг можно изменить вручную. Но изначально его нужно каким-то образом подобрать, и это зависит от данных и от самой модели. Это тоже гиперпараметр, связанный с обучением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор размера батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/L02_Linear_classifier-gd.png\" width=\"500\"> \n",
    "\n",
    "Мы с самого начала говорили о выборках, некоторого количества примеров. В дальнейшим мы будем называть их батчами. Батч (англ. *batch*) это некоторое подмножество обучающей выборки фиксированного размера.\n",
    "\n",
    "При этом было не очень понятно, чем они мотивированны. Точнее, мы мотивировали это тем, что у нас много данных и мы не сможем их обработать все, и это правда. Даже если мы сможем загрузить все данные в память, нам нужно будет загрузить их и использовать при рассчете в том числе градиента. Это ещё более затратно. \n",
    "\n",
    "Зачем батчи нужны при рассчете loss? Если мы посчитаем loss по одному изображению, скорее всего он будет очень специфичен, и это движение, которое произойдет, будет направлено в сторону минимума, потому что по этому конкретному изображению мы улучшим показатели,что не отражает обобщения всех данных, поэтому мы используем батчи.\n",
    "\n",
    "Нас в первую очередь интересует более общее направление, которые будут работать на большинстве наших данных. Поэтому если мы будем оптимизировать модели, исходя из одного элемента данных, путь будет витиеватый, и процесс будет происходить достаточно долго. Если же данные не помещаются в память, то **можно использовать батч того размера, который у нас есть**. Можно загрузить в память и считать по батчам градиент. В этом случае спуск будет более плавным, чем по одному изображению. \n",
    "\n",
    "Также при использовании всего датасета тоже есть свои минусы. **Не всегда загрузка всего датасета приводит к увеличению точности**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее: \n",
    "* [Методы оптимизации нейронных сетей\n",
    "](https://habr.com/ru/post/318970/)\n",
    "* [Обучение нейронной сети](https://alxmamaev.medium.com/deep-learning-на-пальцах-часть-1-341cfe021ef9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель может переобучаться - она может просто поднастроить веса по тренировочную выборку. Особенно это характерно для больших моделей, когда число весов приближается к числу параметров. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_regularisation.jpg\" width=\"700\"> \n",
    "\n",
    "\n",
    "L2 Regularization = weights decay\n",
    "\n",
    "Идея состоит в том, что мы можем наложить некоторое требование на сами веса. Дело в том, что можно получить один и тот же выход модели при разных весах (выход модели соответствует умножению весов на $x$), при разных $w$ выход может быть идентичен.\n",
    "\n",
    "Эти параметры задают некоторую аппроксимацию нашей целевой функции, аппроксимировать функцию можно двумя способами:\n",
    "1. Использовать все имеющиеся данные и провести ее строго через все точки, которые нам известны; \n",
    "2. Использовать более простую функцию: (в данном случае линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым общим закономерностям, которые у них есть. \n",
    "\n",
    "Характерной чертой переобучения является второй сценарий, и сопровождается он, как правило большими весами. Введение L2-регуляризации приводит к тому, что большие веса штрафуются и предпочтение отдается решениям, использующим малые значения весов. \n",
    "\n",
    "Модель может попробовать схитрить и по-другому - использовать все веса, все признаки, даже незначимые, но с маленькими коэффициентами. С этим L2-loss поможет хуже, так как он не сильное штрафует мелкие веса. Результатов его применения - малые значения весов, которые использует модель\n",
    "\n",
    "В этом случае на помощь приходит L1-loss, который штрафует вес за сам факт отличия его от нуля. Но и штрафует он все веса одинаково. Результат его применения - малое число весов, которые использует модель в принципе. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/losses.gif\" alt=\"alttext\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "Это лоссы можно комбинировать - получится Elastic Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$\\lambda=$ regularization strength (hyperparameter)\n",
    "\n",
    "$L(W)=\\underbrace{\\frac1N\\sum_{i=1}^NL_i(f(x_i,W),y_i)}_{\\textbf{Data loss} }+\\underbrace {\\lambda R(W)}_{\\textbf{Regularization}}$\n",
    "\n",
    "Берем сумму всех весов по всей матрице $w$, и добавляем ее к loss. Соответственно, чем больше будет эта сумма, тем больше будет суммарный loss. \n",
    "\n",
    "\n",
    "В дальнейшем проблема с переобучением будет вставать довольно часто. Методов регуляризации модели существует достаточно много. Этот — один из базовых, который будет использоваться практически во всех оптимизаторах, с которыми познакомимся позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy Loss\n",
    "\n",
    "[Отличное видео от Statquest про энтропию](https://www.youtube.com/watch?v=YtebGVx-Fxw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "На Cross-entropy Loss построены практически все модели, про которые будет идти речь.\n",
    "\n",
    "Это оценка результата и обновление весов с использованием градиента. У нас один слой, поэтому мы достаточно легко посчитали от него градиент. Если слоев будет много, сделать это будет сложней.\n",
    "\n",
    "Рассмотрим ещё одну loss-функцию. \n",
    "\n",
    "SVM loss хороша тем, что интуитивно понятно, как ее посчитать. Но проблема состоит в том, что модель выдает некоторые числа, которые сами по себе невозможно интерпретировать. Сами по себе выходы модели мало что означают. Было бы неплохо, если бы мы сразу, глядя на выход модели для кошки, могли бы их как-то интерпретировать, не просматривая остальные веса. Хорошо бы, чтобы модель выдавала не какую-то абстрактную уверенность, а **вероятность** того, что по ее мнению, на картинке изображена кошка.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к вероятностям\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "[Видео от StatQuest, которое объясняет Softmax](https://www.youtube.com/watch?v=KpKog-L9veg)\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/L02_Linear_classifier_softmax.png\" width=\"750\">\n",
    "\n",
    "Перейти к вероятностям мы сможем, проведя с весами некоторые не очень сложные математические преобразования. \n",
    "\n",
    "На слайде выше показано, почему выходы модели часто называют [logit’ами](https://en.wikipedia.org/wiki/Logit). Если предположить, что у нас есть некая вероятность, от которой мы берем такую функцию (logit), то она может принимать значения от нуля до бесконечности. Мы можем считать, что выходы модели - это logit’ы. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например мы могли бы просто взять индекс массива, в котором значение (logit) максимально. Предположим, что наша сеть выдала следующие значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "logits = [\n",
    "          5.1, # кошка\n",
    "          3.2, # машина\n",
    "          -1.7, # лягушка\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, что бы узнать какой класс наша сеть предсказала, мы могли бы просто взять `argmax` от наших `logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Предсказанный класс = %i (кошка)' % (np.argmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но от argmax нельзя посчитать градиент, так как производная от константы равна 0. Соответствеенно, если бы мы вставили производную от argmax в градиентный спуск, мы бы получили везде нули, и соответственно, наша сеть бы вообще ничему не научилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(3), [1,0,0], color='red', s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А мы бы хотели получить не logit’ы, а настоящую вероятность на выходе модели. Да еще и таким образом, что бы от наших вероятностей можно было бы посчитать градиент. Для этого мы можем применить к нашим логитам функцию **Softmax**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/L02_Linear_classifier_softmax2.jpg\" width=\"1300\">\n",
    "\n",
    "Мы можем провести над логитами операцию экспоненцирования, то есть число Эйлера (2.71828) **возвести в степень**, соответствующую этому выходу. В результате мы получим вектор, гарантировано неотрицательных чисел (потому что мы ввели неположительное число в степень, пускай даже отрицательную. То есть выходы могут быть маленькие, но всегда положительные). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше, чтобы можно было интерпретировать эти числа как вероятности, их сумма должна быть равно единице. Мы должны их нормализовать, то есть **поделить на сумму**. Это преобразование называется **Softmax функцией**.\n",
    "\n",
    "**Получаются вероятности**, то есть числа, которые можно интерпретировать таким образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Softmax}_\\text{кошка} = \\frac{e^{5.1}}{e^{5.1} + e^{3.2} + e^{-1.7}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return np.exp(logits)/np.sum(np.exp(logits))\n",
    "\n",
    "print(softmax(logits))\n",
    "print('Сумма = %.2f' % np.sum(softmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что Softmax, никоим образом не поменял порядок значений. Самому большому logit'у соотвеетствует самая большая вероятность, а самому маленькому, соответственно самая маленькая"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графиках. Возьмем массив случайных логитов и применим к ним softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_logits = np.linspace(-1,1,50)\n",
    "fig,ax = plt.subplots(ncols=2)\n",
    "\n",
    "ax[0].plot(np.arange(50), rand_logits)\n",
    "ax[0].set_title('Логиты')\n",
    "ax[1].plot(np.arange(50), softmax(rand_logits))\n",
    "ax[1].set_title('Softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-entropy / log loss**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_cross-entropy.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Теперь, с использованием такого вектора можно определить другую loss функцию. Если не вникать в детали, можно **взять от нее логарифм**. Тогда loss от такого выхода будет выглядеть вот таким образом:\n",
    "\n",
    "$ L_i =  -log(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}})$\n",
    "\n",
    "как нормализованный выход от верного класса на сумму остальных, к которому добавили минус. \n",
    "\n",
    "Получится график loss (слева). Его плюс заключается в том, что у него нет участка с плато, практически по всей длине функция получается гладкой, с хорошими мощными производными. Когда loss большой, производная тоже большая, и за счет этого можно быстро обучать модель. Для кусочно-линейной функции потерь она же равна константе.\n",
    "\n",
    "До тех пор, пока модель не будет работать с точностью 100%, то есть пока все остальные выходы не станут нулевыми, мы можем продолжить обучение, даже если у нас нет явной регуляризации.\n",
    "\n",
    "Осталось выяснить, почему такая простая на вид функция **называется так сложно** - Кросс-энтропия. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-энтропия\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_cross-entropy2.png\" >\n",
    "\n",
    "Если мы рассматриваем выходы нашей модели как вероятности, то мы можем их сравнивать. У нас есть номер правильного класса. Соответственно, можно сказать, что вероятность этого класса - единица, а всех остальных 0, и получить таким образом вероятностное распределение. Выход модели тоже можно интерпретировать как вероятности (тоже можно получить вероятностное распределение). И для работы с этими распределением есть некоторый математический аппарат, который основан на понятии энтропии, который ввел Клод Шеннон.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_sun.png\" >\n",
    "\n",
    "\n",
    "Идея в следующем: у нас есть некоторые данные, которые мы передаем по каналу связи. Например, у нас есть метеостанция, которая сообщает прогноз погоды. Допустим, она может передавать 8 вариантов прогноза. Мы в каждый момент времени получаем от нее сообщение. Предположим, что потребуется 3 бит, чтобы передавать это сообщение. \n",
    "\n",
    "Допустим, мы знаем некую вероятность, с которой в нашем регионе может наступить хорошая либо плохая погода. То есть мы знаем вероятностное распределение, по которыму у нас, допустим, солнечное место, где почти не бывает бури. Если мы знаем об этом, мы можем сэкономить на передаче информации. Мы можем закодировать наиболее вероятные сообщения двумя битами, причем придумать такие коды, чтобы они почти не пересекались. Они будут начинаться с нуля и передавать два бита вместо трех. Таким образом можно сэкономить на передачи информации. То есть идея такая: **если мы знаем, что события маловероятны, мы можем кодировать их более длинной цепочкой, а более вероятные события - более короткими цепочками**. В этом случае в среднем количество информации, которое можно передать, сократиться.\n",
    "\n",
    "**Формула Шеннона позволяет посчитать, насколько сильно мы можем сэкономить для конкретного вероятностного распределения**. То есть если подставить эти данные в формулу, то получим 2,23.\n",
    "\n",
    "На практике реализовать это, используя биты, возможно, не выйдет, но это свойство данного вероятностного распределения можно посчитать. Оно несёт в себе некоторое количество полезной информации, соответствующее этой энтропии.\n",
    "\n",
    "**Что же такое кросс-энтропия?**\n",
    "\n",
    "Давайте представим, что мы каким-то образом начали кодировать эту информацию, прошили погодную станцию так, что она может эту кодированную информацию передавать, а потом перенесли в другой регион, где ситуация противоположная. И мы кодируем события, которые стали частыми, длинными цепочками, а маловероятные - короткой, и у нас в этом случае явно передается больше информации, чем мы могли бы, если бы сделали все банально. \n",
    "\n",
    "Кросс-энтропия позволяет посчитать, насколько большая будет потеря в данном случае. То есть это некий **способ сравнить между собой вероятностные распределения**.\n",
    "\n",
    "Что, собственно говоря, отображено в формуле, потому что мы считаем кросс-энтропию по нашему вектору $P$ и $Q$. Вектор $P$ состоит из нулей и единиц, соответственно, во всех случаях, кроме одного, это выражение будет нулевым, кроме одного случая с единицей. Случай с единицей соответствует нашему правильному классу. Поэтому сумма исчезает, остаются логарифм и вектор, а вероятность для правильного класса мы считаем как Soft max. Отсюда название кросс-энтропия.  \n",
    "\n",
    "\n",
    "(В теории информации кросс-энтропия между двумя распределениями вероятностей $p$ и $q$  по одному и тому же базовому набору событий измеряет среднее число битов, необходимых для идентификации события , взятого из набора, если схема кодирования, используемая для набора, оптимизирована для оценочного распределения вероятностей $q$, а не для истинного распределения $p$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img_license/L02_Linear_classifier_cross-entropy3.png\" width=\"900\">\n",
    "\n",
    "Аналогичное обоснование: есть понятие дивергенция, которая позволяет оценить расхождение между вероятностными распределениями, которая нам здесь и нужна. Есть дивергенция **Кульбака-Лейблера**, которая выражается через кросс-энтропию, а энтропия от нашего вектора нулевая. Поэтому фактически здесь кросс-энтропия равна дивергенции, которая показывает, насколько непохожи два распределения. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_softmax3.jpg\" width=\"800\">\n",
    "\n",
    "\n",
    "Эта функция очень популярна, она используется при обучении реальных моделей на практике. Поскольку внутри находится Softmax, часто ее называют Softmax-классификатором, что, строго говоря, не совсем верно. Здесь функция потерь - кросс-энтропия, Softmax же просто используется внутри нее.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент Cross Entropy Loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cross Entropy Loss](http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подавляющее большинство методов классификации и регрессии сформулированы в терминах евклидовых или метрических пространств, то есть подразумевают представление данных в виде векторов одинаковой размерности. В реальности мы имеем дело с категориальными данными, которые нужно представить в виде вектора. One Hot Encoding подразумевает создание вектора, размером равным количеству классов, все из которых равны нулю за исключением одного. На позицию, соответствующую значению класса мы помещаем 1.\n",
    "\n",
    "Например, если у нас 10 классов, тогда для каждого класса соотвентственно \n",
    "\n",
    "Класс 0 = [**1**,0,0,0,0,0,0,0,0,0]\\\n",
    "Класс 1 = [0,**1**,0,0,0,0,0,0,0,0]\\\n",
    "...\\\n",
    "Класс 9 = [0,0,0,0,0,0,0,0,0,**1**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_class = 5\n",
    "\n",
    "one_hot = np.zeros(n_classes)\n",
    "one_hot[y_class] = 1\n",
    "yi = one_hot\n",
    "print(yi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Дополнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -L = \\sum_i y_i \\log p_i = \\sum_i y_i \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
    "\n",
    "$$s_{y_i} = w_i x$$\n",
    "\n",
    "$$ \\dfrac {\\delta L} {\\delta w_i} = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} $$\n",
    "\n",
    "$$\\dfrac {\\delta s_{y_i}} {\\delta w_i} = x$$\n",
    "\n",
    "Только один $y_k = 1$\n",
    "\n",
    "\n",
    "$$ -L = y_k \\log p_i = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}})$$\n",
    "\n",
    "i = k\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_i}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_i} - \\log  \\sum_j e^{s_{y_j}}$$\n",
    "\n",
    "$$\\dfrac {\\delta -L} {\\delta s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\delta {\\sum_j e^{s_{y_j}}}} {\\delta s_{y_i}} = 1 - \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\delta e^{s_{y_i}}} {\\delta s_{y_i}} = 1 - \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = 1 - p_i$$\n",
    "\n",
    "$$\\dfrac {\\delta L} {\\delta s_{y_j}} = p_i - 1 $$\n",
    "\n",
    "$$ \\dfrac {\\delta L_i} {\\delta w_i}  = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} = (p_i - 1) x $$\n",
    "\n",
    "i != k\n",
    "\n",
    "$$ -L = \\log(\\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}}) = \\log e^{s_{y_k}} - \\log  \\sum_j e^{s_{y_j}}  = s_{y_k} - \\log  \\sum_j e^{s_{y_j}}$$\n",
    "\n",
    "$$\\dfrac {\\delta -L} {\\delta s_{y_i}} = - \\dfrac 1 {\\sum_j e^{s_{y_j}}} \\cdot \\dfrac {\\delta {\\sum_j e^{s_{y_j}}}} {\\delta s_{y_i}} =  \\dfrac 1 {\\sum_j e^{s_{y_i}}} \\cdot \\dfrac {\\delta e^{s_{y_i}}} {\\delta s_{y_i}} = \\dfrac {e^{s_{y_i}}} {\\sum_j e^{s_{y_j}}} = - p_i$$\n",
    "\n",
    "$$\\dfrac {\\delta L} {\\delta s_{y_j}} = p_i $$\n",
    "$$ \\dfrac {\\delta L_i} {\\delta w_i}  = \\dfrac {\\delta L} {\\delta s_{y_i}} \\dfrac {\\delta s_{y_i}} {\\delta w_i} = p_i x $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое вычисление SoftMax\n",
    "\n",
    "Когда мы возводим число в степень, может получиться очень большое число и произойти переполнение памяти. Неизвестно, какие будут у модели выходы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([123, 456, 789])\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы можем вычесть из каждого $s_i$ пооложительную константу, чтобы уменьшить значения экспонент. В качестве константы можно выбрать максимальный элемент этого вектора, тогда у нас гарантированно не будет очень больших чисел и такой способ будет работать более стабильно.\n",
    "\n",
    "$$M = \\max_j s_{y_{j}}$$\n",
    "$$s^{new}_{y_{i}}  = s_{y_{i}} - M $$\n",
    "\n",
    "$$ \\dfrac {e^{s^{new}_{y_{i}}}} {\\sum_j e^{s^{new}_{y_{j}}}}  = \\dfrac {e^{s_{y_{i}} - M }} {\\sum_j e^{s_{y_{j}} - M }} = \\dfrac {e^{s_{y_{i}}}e ^ {-M}} {\\sum_j e^{s_{y_{j}}} e ^ {-M}} = \\dfrac {e ^ {-M} e^{s_{y_{i}}}} {e ^ {-M} \\sum_j e^{s_{y_{j}}} } = \\dfrac { e^{s_{y_{i}}}} { \\sum_j e^{s_{y_{j}}} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([123, 456, 789])\n",
    "f -= f.max()\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "print(f, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Дополнительная информация}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итог**\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L02_Linear_classifier/img/L02_Linear_classifier_itog.png\" width=\"800\">"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
