{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Генеративные модели</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Классические\" генеративные алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача генерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом курсе мы в основном работали с **размеченными** данными. Мы научили нейронные сети решать задачи классификации, сегментации и т.д. \n",
    "\n",
    "В этой лекции мы разберемся, как научить нейросеть создавать что-то новое.\n",
    "\n",
    "\n",
    "Как подойти к такой задаче с помощью нейронных сетей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Постановка задачи генерации**\n",
    "\n",
    "**Дано**: неразмеченные данные\n",
    "\n",
    "**Выход**: новые данные, которые будут удовлетворять следующим условиям:\n",
    "* Новые данные должны быть **похожи** на исходные.\n",
    "* Но **не повторять** их в точности (или повторять, при случайном стечении обстоятельств).\n",
    "* Чтобы результат генерации был различным при разных запусках, нам нужен **элемент случайности**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простейший пример: генерация объектов из нормального распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация объектов из произвольного известного распределения, алгоритм MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе https://twiecki.io/blog/2015/11/10/mcmc-sampling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генеративные алгоритмы, основанные на глубоком обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение  в генеративно-состязательные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Эволюция в генерации изображений лиц:**\n",
    "\n",
    "[Множество примеров различных генераторов](https://thisxdoesnotexist.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/faces_generation_quality_progress.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся с **элементом случайности**. В нейронных сетях мы привыкли к **воспроизводимости** результата: в режиме валидации мы можем несколько раз подать на вход один и тот же объект и получить один и тот результат.  Возникает два вопроса:\n",
    "- что подавать на **вход** сети для генерации?\n",
    "- как реализовать **случайность**?\n",
    "\n",
    "Ответ на оба вопроса: подавать в качестве **входа** вектор **случайного шума**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/generator_model_pipeline.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему именно **вектор**? Почему не одно **случайное число**? \n",
    "\n",
    "**Ответ**: входной вектор можно рассматривать как **признаки** генерируемого объекта. Каждый такой признак — **независимая случайная величина**. Если мы будем передавать только одно случайное число, то генерация будет однообразной. Чем больше признаков (степеней свободы) у входного вектора, тем разнообразнее будет результат генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть случайный шум **большей размерности** даёт нам **больше вариабельности**  для генерации. Это называется **input latent space** — входное латентное пространство.\n",
    "\n",
    "**Note:** *из-за неустоявшейся терминологии случайное распределение на входе генератора называется латентным пространством так же, как и скрытое пространство в автоэнкодерах. Поэтому в этой лекции будем называть его **входным** латентным пространством. Также в статьях встречается вариант: predefined latent space.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждую компоненту латентного пространства можно рассматривать как отдельную шкалу, вдоль которой изменяются определенные свойства генерируемых объектов. Например, можно выбрать четыре латентных вектора и посмотреть, как генерируемые объекты плавно изменяются при переходе от одного вектора к другому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/input_latent_space_lin_interpol.png\" width=\"600\">\n",
    "\n",
    "*Линейные интерполяции между четырьмя изображениями в латентном пространстве.*\n",
    "\n",
    "[M. Pieters, M. Wiering](https://arxiv.org/abs/1803.09093)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Размерность входного латентного пространства"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В выборе размерности входного латентного пространства важно соблюсти  баланс.\n",
    "- при **низкой размерности** возникнет проблема **низкой вариабельности**. \n",
    "\n",
    "Пример: генератор лиц с входным вектором длины 1. Результатом работы генератора будет всего одна шкала, вдоль которой будут расположены генерируемые изображения. Скорее всего, генератор выучит наиболее простую и \"очевидную\" шкалу: от молодой женщины блондинки к пожилому мужчине брюнету. У такой сети будет низкая вариабельность — она не сможет сгенерировать, например, рыжего ребенка в очках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- при **большой размерности** латентное пространство может быть слишком **разреженным**. \n",
    "\n",
    "При обучении модели количество точек в этом латентном пространстве будет настолько мало, что в основном пространство будет состоять из пустот. Тогда модель будет крайне некачественно генерировать объекты в точках латентного пространства, далеких от точек обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший способ выбрать длину вектора — это найти публикацию с похожей задачей и взять значение из нее.\n",
    "\n",
    "Если такой информации нет, то придется экспериментировать. Лучше начинать с низкой размерности латентного пространства, чтобы наладить работу всей сети, пусть и с низким разнообразием, а затем проводить эксперименты по поиску оптимальной размерности.\n",
    "\n",
    "Можно использовать собственные знания в предметной области: спросите себя, сколькими вещественными числами можно описать важную информацию об объекте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распределение входных латентных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы знаем из лекции про обучение сети, инициализация весов и нормализация входных данных вносят существенный вклад в работу модели. Поэтому принято использовать **многомерное нормальное распределение** для input latent space. Оно лучше взаимодействует с весами модели и улучшает сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/binomial_distribution.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em><a href=\"https://matplotlib.org/3.1.0/gallery/lines_bars_and_markers/scatter_hist.html\">Двумерное нормальное распределение</a></p> </em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный подход в решении задачи генерации\n",
    "(как делать на практике НЕ нужно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем создать генератор точек на параболе (cамым тривиальным решением кажется подача случайного шума на вход сети) и будем ожидать на выходе точки параболы. Проверим, как это будет работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x * x\n",
    "    return np.hstack(\n",
    "        (x.reshape(-1, 1), y.reshape(-1, 1))\n",
    "    )  # Create num of correct dots(x,y) on parabola\n",
    "\n",
    "\n",
    "pairs = gen_pair(100)\n",
    "plt.scatter(pairs[:, 0], pairs[:, 1])\n",
    "plt.title(\"Random dots on parabola,\\nwhich will use like a dataset.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём размерность входного латентного пространства $ls = 1$ и объединим шум с точками в датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define input parameters\n",
    "n_batches = 10\n",
    "batch_size = 128\n",
    "ls = 1  # latent space\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(size=(n_batches * batch_size, ls), dtype=torch.float)\n",
    "print(f\"NN Input: noise.shape: {noise.shape}\")\n",
    "\n",
    "# Generates dots on parabola\n",
    "xy_pair = gen_pair(num=(n_batches * batch_size))\n",
    "xy_pair = torch.tensor(xy_pair, dtype=torch.float)\n",
    "print(f\"NN Output: xy_pair.shape: {xy_pair.shape}\")\n",
    "\n",
    "dataset = TensorDataset(noise, xy_pair)  # model inputs, model outputs\n",
    "trainset, testset = train_test_split(\n",
    "    dataset, train_size=0.8\n",
    ")  # split dataset for train and test\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простую модель, которая будет ожидать шум на вход и генерировать точки на выходе. (Обратите внимание, что функция активации на последнем слое отсутствует, поскольку мы не ограничиваем наш генератор в каком-то диапазоне.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GenModel(nn.Module):\n",
    "    def __init__(self, latent_space):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2),\n",
    "        )  # x,y\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки loss.\n",
    "\n",
    "Так как мы не знаем, в каком месте параболы генератор создаст новую точку, то непонятно, с каким элементом из датасета ее сравнивать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №1**\n",
    "\n",
    "Для сгенерированного $x$ аналитически вычислять $y_{target}=x*x$ и считать разницу между $y$ сгенерированным моделью и $y_{target}$ вычисленным аналитически:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pair, label):\n",
    "    # All inputs are batches\n",
    "    x_fake = pair[:, 0]\n",
    "    y_fake = pair[:, 1]\n",
    "    return torch.abs(x_fake * x_fake - y_fake).mean()  # average by batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это будет работать. \n",
    "\n",
    "Однако если мы знаем способ точно предсказать выход по входу, то задача уже решена и нейронная сеть не нужна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №2**\n",
    "\n",
    "Найти в датасете точку  $ target = (x_{target},y_{target})$, наиболее близкую к созданной генератором $ generated = (x,y)$, и использовать расстояние между этими точками в качестве loss. \n",
    "\n",
    "$$ Loss = min(dist(target_{i},generated))$$\n",
    "\n",
    "В пространстве высокой размерности такой поиск будет весьма ресурсозатратным, но в нашем учебном примере работать будет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, targets):\n",
    "        super().__init__()\n",
    "        self.targets = targets  # Remember all real samples, impossible in real world\n",
    "\n",
    "    def forward(self, input, dummy_target=None):\n",
    "        dist = torch.cdist(input, self.targets)  # claculate pairwise distances (euc.)\n",
    "        min_dist, index = torch.min(dist, dim=1)  # take the best\n",
    "        return min_dist.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательный код для вывода loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_test_loss(model, loader):\n",
    "    test_data = next(iter(loader))\n",
    "    test_loss = Loss(test_data[1])\n",
    "    outputs = model(samples.to(device))\n",
    "    return test_loss(outputs.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной код обучения.\n",
    "\n",
    "Целевые точки из датасета запоминаются в loss, затем идет обычный цикл обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_epochs = 600\n",
    "model = GenModel(latent_space=ls)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "all_train_targets = next(iter(train_loader))[1]\n",
    "criterion = Loss(all_train_targets.to(device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_epoch = 0\n",
    "    for samples, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples.to(device))\n",
    "        loss = criterion(outputs.to(device), labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    loss_test = get_test_loss(model, test_loader)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch={epoch} train_loss={loss_epoch/len(train_loader):.4} test_loss={loss_test:.4}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим результаты генерации на шуме:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(model, pairs, ls=1):\n",
    "    model.eval().to(\"cpu\")\n",
    "    noise = torch.tensor(np.random.normal(size=(1000, ls)), dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "        xy_pair_gen = model(noise)\n",
    "\n",
    "    xy_pair_gen = xy_pair_gen.detach().numpy()\n",
    "    plt.scatter(pairs[:, 0], pairs[:, 1], color=\"red\", label=\"real\")\n",
    "    plt.scatter(xy_pair_gen[:, 0], xy_pair_gen[:, 1], color=\"blue\", label=\"generated\")\n",
    "    plt.axis([-1, 1, 0, 1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "test_image(model, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель генерирует точки, лежащие на параболе, при этом все они лежат в довольно узком интервале по оси х. \n",
    "\n",
    "\n",
    "Это неудивительно: в loss function мы прописали, что сгенерированная точка должна лежать на параболе, и модель обучилась. А информацию о том, в каких частях кривой должны оказаться точки, мы в loss никак не кодировали.\n",
    "\n",
    "Более того, модель может научиться хорошо генерировать одну единственную точку, и при этом loss может стать нулевым.\n",
    "\n",
    "\n",
    "Итак, надо решить две проблемы:\n",
    "\n",
    "\n",
    "1.   Закодировать в loss условие о том, что точки должны быть различными.\n",
    "2.   Придумать способ проверки, не требующий перебора всего датасета.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем наказывать нейронную сеть не напрямую стандартной loss function, а второй **сетью**, которая определяет, лежит ли сгенерированная точка на параболе. \n",
    "\n",
    "Создадим сеть-классификатор точек (лежит/не лежит на параболе), которую назовём **дискриминатор** или критик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisModel(nn.Module):\n",
    "    def __init__(self, n_points):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15, 1),  # real/fake\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 2 * n_points)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае задача дискриминатора — определять, принадлежит ли объект к распределению обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итого** мы имеем: \n",
    "- **генератор**, выдающий точки, которые могут принадлежать параболе, а могут не принадлежать ей;\n",
    "- **дискриминатор**, который будет их различать.\n",
    "\n",
    "Мы будем подавать в **дискриминатор** **правильные точки** (чтобы он знал, как это должно выглядеть) и **точки, которые выдаёт генератор**, считая их подделкой.\n",
    "\n",
    "Таким образом, **генератор** будет учиться **подражать** реальным данным, а дискриминатор будет учиться **отличать** реальные точки, от подделок. \n",
    "\n",
    "Мы пришли к идее **генеративно-состязательных** нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generative adversarial network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2014 Generative Adversarial Networks (Goodfellow et al., 2014)](https://arxiv.org/abs/1406.2661) (**Cited by 33430!!!**)\n",
    "\n",
    "[Видео разбор оригинальной статьи](https://youtu.be/eyxmSmjmNS0)\n",
    "\n",
    "[Видео лекции Иана Гудфеллоу](https://www.youtube.com/watch?v=HGYYEUSm-0Q)\n",
    "\n",
    "**Генеративно-состязательную** сеть описал Иан Гудфеллоу из компании Google (на тот момент) в 2014 году. Сейчас он возглавляет подразделение машинного обучения в Apple. Принцип состязательности в сети **GAN** нередко описывается через метафоры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/generative_adversarial_network_scheme.png\" width=\"700\"></center>\n",
    "    <center><em>Схематичное представление архитектуры GAN </em></center>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор — фальшивомонетчик!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще со времен **AlexNet** мы знаем, что если мы что-то и умеем делать с нейросетями, так это **классификаторы**. В классическом GAN **дискриминатор** выполняет простейшую из задач классификации — **бинарную классификацию** (либо *real*, либо *fake*). А вот задача **генерации** каким-то прямым образом на тот момент решена не была.\n",
    "\n",
    "Как использовать всю мощь классификатора для создания генератора?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что есть фальшивомонетчик $G$ (generator) и банкир с прибором для проверки подлинности купюр $D$ (discriminator).\n",
    "\n",
    "Фальшивомонетчик черпает вдохновение из генератора случайных чисел в виде случайного шума $z$ и создает подделки $G(z)$.\n",
    "\n",
    "Банкир $D$ получает на вход пачку купюр $x$, проверяет их подлинность и сообщает вектор $D(x)$, состоящий из чисел от нуля до единицы — свою уверенность (вероятность) по каждой купюре в том, что она настоящая. Его цель — выдавать нули для подделок $D(G(z))$ и единицы для настоящих денег $D(x)$. Задачу можно записать как максимизацию произведения $D(x)(1-D(G(z)))$, а произведение, в свою очередь, можно представить как сумму через логарифм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, задача банкира — максимизировать $log(D(x))+log(1-D(G(z)))$.\n",
    "\n",
    "Цель фальшивомонетчика прямо противоположна — максимизировать $D(G(z))$, то есть убедить банкира в том, что подделки настоящие.\n",
    "\n",
    "Продолжая аналогию, обучение генератора можно представить так: фальшивомонетчик не просто генерирует подделки наудачу. Он добывает прибор для распознавания подделок, разбирает его, смотрит, как тот работает, и затем создает подделки, которые смогут обмануть этот прибор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математически это **[игра](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%B8%D0%B3%D1%80) двух игроков**:\n",
    "\n",
    "$$\\min\\limits_{\\theta_g}  \\max\\limits_{\\theta_d} [\\mathbb{E}_{x _\\sim p(data)} log(D_{\\theta_d}(x)]+\\mathbb{E}_{z _\\sim p(z)} \n",
    "[log(1-D_{\\theta_d}(G_{\\theta_g}(z))]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дискриминатор** \n",
    "- обучается при **фиксированном генераторе** ${G}_{\\theta_{g}}$,\n",
    "- **максимизирует** функцию выше относительно $\\theta_d$ (**градиентный подъем**),\n",
    "- решает задачу **бинарной классификации**: старается присвоить $1$ точкам данных из обучающего набора $E_{x∼p_{data}}$ и 0 сгенерированным выборкам $E_{z∼p(z)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Генератор**\n",
    "- обучается при **фиксированном дискриминаторе** $D_{θ_d}$,\n",
    "- получает градиенты весов за счет backpropagation через дискриминатор,\n",
    "- **минимизирует** функцию выше относительно $\\theta_d$ (**градиентный спуск**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посредством **чередования** градиентного **подъема** и **спуска** сеть можно обучить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **подъем** на **дискриминаторе**:\n",
    "\n",
    "\n",
    "$$\\max\\limits_{\\theta_d} [\\mathbb{E}_{x _\\tilde{}p(data)} log(D_{\\theta_d}(x)+\\mathbb{E}_{z _\\tilde{}p(z)} log(1-D_{\\theta_d}(G_{\\theta_g}(z)))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **спуск** на **генераторе**:\n",
    "\n",
    "\n",
    "$$\\min\\limits_{\\theta_g} \\mathbb{E}_{z _\\tilde{}p(z)} log(1-D_{\\theta_d}(G_{\\theta_d}(z)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **спуск** на **генераторе** эквивалентен градиентному **подъему**\n",
    "\n",
    "$$\\max\\limits_{\\theta_g} \\mathbb{E}_{z _\\tilde{}p(z)} log(D_{\\theta_d}(G_{\\theta_d}(z)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе совместного конкурентного обучения, если система достаточно сбалансирована, достигается **минимаксное состояние равновесия**, в котором обе сети эффективно учатся.\n",
    "\n",
    "Сгенерированные удачно обученной нейросетью изображения практически неотличимы от настоящих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хорошенько подумать, то можно прийти к выводу, что **loss function** в **GAN** — это не какая-то функция, заданная людьми, а еще одна **нейросеть**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества GAN**\n",
    "* Теоретические **гарантии сходимости**\n",
    "* Можно обучать обычным **SGD/Adam**\n",
    "* Решает в явном виде задачу **generative modeling**, но неявным образом (**нейросети**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Недостатки GAN**\n",
    "* **Нестабильное обучение**\n",
    "* Очень **долгая сходимость**\n",
    "* **Mode-collapsing** (модель выдает одно и то же изображение или один и тот же класс и т.д., независимо от того, какие входные данные ей подаются)\n",
    "* **Исчезновение градиента**: дискриминатор настолько хорошо научился отличать сгенерированные образцы от реальных, что градиент весов генератора становится равным 0: в какую сторону бы генератор не изменил свои веса, дискриминатор все равно идеально распознает фальшивки\n",
    "* Поиск оптимальных параметров — **pure luck**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Практический пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим наши **генератор** и **дискриминатор**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_space, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "        )  # x,y\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),  # real/fake\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим **входные параметры**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10  # latent space\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что у нас так же, как и в первом примере, есть переменная **latent space**. Это тот шум, из которого мы будем генерировать наши точки. Закон сохранения масс в действии — нельзя создать что-то из ничего!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим все необходимое для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "criterion = nn.BCELoss()  # Loss\n",
    "gen = Generator(latent_space=latent_dim, hidden_dim=50).to(device)\n",
    "disc = Discriminator(hidden_dim=50).to(device)\n",
    "\n",
    "# 2 optimizers for Discriminator and Generator\n",
    "optimizerD = torch.optim.Adam(disc.parameters(), lr=3e-4)\n",
    "optimizerG = torch.optim.Adam(gen.parameters(), lr=3e-4)\n",
    "\n",
    "# Fix noise to compare\n",
    "fixed_noise = torch.randn(128, latent_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы используем `BCELoss` (**Binary Cross Entropy**). Давайте разберемся почему:\n",
    "\n",
    "- **Дискриминатор** решает задачу **бинарной классификации**. Для этой задачи хорошо подходит **BCE**.\n",
    "- Требование к генератору может быть сформулировано как \"объектам, сгенерированным **генератором**, **дискриминатором** должна быть присвоена **высокая вероятность**\". Для **\"идеального\" генератора**, который всегда генерирует фотореалистичные результаты, значения **$D(G(z))$** всегда должны быть **близки к 1**. Для этой задачи хорошо подходит **BCE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которая создает точки на параболе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x * x\n",
    "    return torch.tensor(\n",
    "        np.hstack((x.reshape(-1, 1), y.reshape(-1, 1))), dtype=torch.float\n",
    "    )  # Create num of correct dots(x,y) on parabola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что сейчас будет происходить? $$$$\n",
    "\n",
    "* Обучение дискриминатора\n",
    "    * обнулим градиенты **дискриминатора**\n",
    "    * real точки\n",
    "        * создадим набор **real точек**, которые лежат на параболе\n",
    "        * посчитаем значение функции потерь дискриминатора на **real точках** и **real метках** $\\text{loss D}_\\text{real}$\n",
    "        * посчитаем градиенты для **дискриминатора**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gan_training_algorithm_1.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Обучение дискриминатора (продолжение) \n",
    "    * fake точки\n",
    "        * сгенерируем случайный шум $z$\n",
    "        * возьмем наш не обученный **генератор** и создадим с его помощью **fake точки** из $z$\n",
    "        * посчитаем значение функции потерь дискриминатора на **fake точках** и **fake метках** $\\text{loss D}_\\text{fake}$\n",
    "        * посчитаем градиенты для **дискриминатора** (они сложатся с уже посчитанными ранее)\n",
    "    * обновление весов \n",
    "        * сделаем шаг обучения **дискриминатора** (обновим его веса)\n",
    "        * **генератор** не обучается\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gan_training_algorithm_2.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Обучение генератора\n",
    "    * обнулим градиенты **генератора**\n",
    "    * сгенерируем случайный шум $z$\n",
    "    * создадим с помощью **генератора** набор **fake точек** из $z$\n",
    "    * посчитаем значение функции потерь дискриминатора на **fake точках** и **real метках** $\\text{loss G}$ (подмена меток)\n",
    "    * посчитаем градиенты для **генератора**\n",
    "    * сделаем шаг обучения **генератора** (обновим его веса)\n",
    "    * **дискриминатор** не обучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gan_training_algorithm_3.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрите код внимательно, чтобы понять, о чем речь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Main Training Loop\n",
    "print(\"Training...\")\n",
    "print(device)\n",
    "\n",
    "x = []\n",
    "y_D = []\n",
    "y_G = []\n",
    "for epoch in range(num_epochs):\n",
    "    # max log(D(x)) + log(1 - D(G(z)))\n",
    "    # train on real points\n",
    "    disc.zero_grad()\n",
    "\n",
    "    # Define real points\n",
    "    real_points = gen_pair(num=batch_size).to(device)\n",
    "    label = torch.full(\n",
    "        (batch_size,), real_label, dtype=torch.float, device=device\n",
    "    ).view(-1)\n",
    "\n",
    "    # Train disc on real_points\n",
    "    output = disc(real_points).view(-1)\n",
    "    errD_real = criterion(output, label)\n",
    "    errD_real.backward()\n",
    "\n",
    "    # Define fake points\n",
    "    # This dots generated by generator transform from latent space\n",
    "    noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "    fake_points = gen(noise)\n",
    "    label.fill_(fake_label)\n",
    "\n",
    "    # Train disc on fake_points\n",
    "    output = disc(fake_points.detach()).view(-1)\n",
    "    errD_fake = criterion(output, label)\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # Discriminator loss(real+fake)\n",
    "    errD = errD_real + errD_fake\n",
    "\n",
    "    optimizerD.step()\n",
    "\n",
    "    # max log(D(G(z)))\n",
    "    # Now, train generator\n",
    "    gen.zero_grad()\n",
    "\n",
    "    # Let's tell the discriminator that our generator creates real points\n",
    "    label.fill_(real_label)\n",
    "\n",
    "    output = disc(fake_points).view(-1)\n",
    "\n",
    "    errG = criterion(output, label)\n",
    "\n",
    "    errG.backward()\n",
    "\n",
    "    optimizerG.step()\n",
    "\n",
    "    # Plotting every N epoch\n",
    "    x.append(epoch)\n",
    "    y_D.append(errD.item() / 2)\n",
    "    y_G.append(errG.item())\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        fig, ax = plt.subplots(nrows=2, figsize=(9, 6))\n",
    "        ax[0].plot(x, y_D, color=\"red\", lw=1, label=\"D\")\n",
    "        ax[0].plot(x, y_G, color=\"green\", lw=1, label=\"G\")\n",
    "\n",
    "        # Generates dots from fixed_noise\n",
    "        fake_points = gen(fixed_noise)\n",
    "        ax[1].scatter(\n",
    "            fake_points.detach().to(\"cpu\")[:, 0],\n",
    "            fake_points.detach().to(\"cpu\")[:, 1],\n",
    "            color=\"green\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax[1].scatter(\n",
    "            real_points.detach().to(\"cpu\")[:, 0],\n",
    "            real_points.detach().to(\"cpu\")[:, 1],\n",
    "            color=\"red\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax[1].set_xlim(-1, 1)\n",
    "        ax[1].set_ylim(0, 1)\n",
    "        clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Класс!** У нас получилось (если вдруг не сошлось за 10000 эпох, перезапустите заново, к сожалению, **фиксация seed еще не гарантирует стабильность GAN**). Особенно круто смотреть, как красиво loss **дискриминатора** и **генератора** сходятся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN — Генерация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью **GAN** можно, разумеется, генерировать не только точки на параболе. Можно генерировать, например, изображения. Но появляются закономерные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как из шума на входе сети получить изображение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым простым ответом будет: взять шум, пропустить его через **полносвязные слои** и сделать **reshape** до нужного разрешения. В целом, это будет работать.\n",
    "\n",
    "\n",
    "Однако **DCGAN (Deep Convolutional GAN)** использует **сверточные** и **сверточно-транспонированные** (*convolutional* и *convolutional-transpose*) слои в дискриминаторе и генераторе соответственно. Впервые метод **DCGAN** был описан в статье [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford et al., 2015)](https://arxiv.org/abs/1511.06434)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/deep_convolutional_gan_scheme.png\" width=\"700\"></center>\n",
    "<center><em>Схема работы DCGAN (Radford et al., 2015).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже видна разница в генерации при помощи исключительно **полносвязных слоёв** и при помощи **обратных свёрток**. Очевидно, результат **DCGAN** лучше, чем **GAN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/gan_dcgan_mnist_examples.png\" width=\"600\"></center>\n",
    "<center><em>Сравнение результатов на MNIST (Radford et al., 2015)</em></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/1511.06434.pdf\">\tUnsupervised representation learning with deep convolutional generative adversarial networks</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход генератора подают шум для создания разнообразных объектов. Этот шум представляет собой вектор в многомерном пространстве. Один вектор — один сгенерированный объект. Задача дискриминатора — преобразовать вектор в изображение.\n",
    "\n",
    "Такое преобразование возможно при помощи транспонированных сверточных (convolution-transpose, иногда называют fractionally strided convolution) слоев. Как и обычные сверточные слои, эти слои используют сверточные ядра, но перед вычислением сверток они увеличивают размер исходного изображения, \"раздвигая\" пиксели и заполняя образующиеся промежутки между пикселями нулями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/dcgan_architecture.png\" width=\"700\"></center>\n",
    "<center><em>Зеркальная архитектура DCGAN </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте кратко вспомним, что делают TC слои.\n",
    "\n",
    "Transposed convolution проходит по всем пикселям входа и умножает их на обучаемое ядро свертки. При этом каждый одиночный пиксель превращается в фрагмент. Там, где фрагменты накладываются друг на друга, значения попиксельно суммируются.\n",
    "\n",
    "Используя Transposed convolution с параметром `stride = 2`, можно повышать размер карты признаков приблизительно в два раза, добавляя на нее мелкие детали."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/transposed_convolution_explained.png\" width=\"1024\"></center>\n",
    "\n",
    "<center><em>Transposed convolution</em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10)) * 255  # one 3-channel image with 10x10 size\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convT = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "y = convT(x)\n",
    "print(y.shape)  # One 3-chanells image with 12x12 size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученное изображение не похоже на входное, потому что были применены свёрточные ядра со случайными коэффициентами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "ax[0].imshow(x[0].permute(1, 2, 0).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow(y[0].permute(1, 2, 0).detach().numpy().astype(np.uint8))\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"After ConvTranspose\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Другие способы повышения разрешения — Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо **обратных свёрток** существуют другие методы повышения разрешения из низкой размерности.\n",
    "\n",
    "Самый простой способ — выполнить повышение разрешения с помощью **интерполяции**. Давайте вспомним, что в PyTorch это осуществляется слоем [Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10))  # one 3-channal image with 10x10 size\n",
    "print(\"Input shape:\", x.shape)\n",
    "\n",
    "upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "y = upsample(x)\n",
    "\n",
    "print(\"Output shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "ax[0].imshow((x[0].permute(1, 2, 0) * 256).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow((y[0].permute(1, 2, 0).detach().numpy() * 256).astype(np.uint8))\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"After Upsample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обученного DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на пример обученного **DCGAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "model = torch.hub.load(\n",
    "    \"facebookresearch/pytorch_GAN_zoo:hub\", \"DCGAN\", pretrained=True, useGPU=use_gpu\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "num_images = 16\n",
    "noise, _ = model.buildNoiseData(num_images)\n",
    "with torch.no_grad():\n",
    "    generated_images = model.test(noise)\n",
    "    generated_images = (\n",
    "        generated_images.clamp(-1, 1) + 1\n",
    "    ) / 2.0  # normalization to 0..1 range\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16 * 3, 2 * 3))\n",
    "ax.imshow(\n",
    "    torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy(),\n",
    "    interpolation=\"nearest\",\n",
    "    aspect=\"equal\",\n",
    ")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практический пример DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем сами написать свой **DCGAN** и обучить его на датасете **FashionMNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2  # Num of epochs\n",
    "batch_size = 64  # batch size\n",
    "lr = 2e-4  # Learning rate\n",
    "b1 = 0.5  # Adam: decay of first order momentum of gradient\n",
    "b2 = 0.999  # Adam: decay of first order momentum of gradient\n",
    "num_cpu = 8  # Num of cpu threads to generate batch\n",
    "latent_dim = 100  # latent space\n",
    "img_size = 32  # images size\n",
    "channels = 1  # Num of channels\n",
    "sample_interval = 450  # interval between image sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно мы **инициализируем веса** случайным образом, но ничто не мешает нам инициализировать их так, как мы хотим. В [оригинальной статье](https://arxiv.org/pdf/1511.06434.pdf) про **DCGAN** предложено инициализировать веса нормальным распределением с центром в нуле и стандартным отклонением 0,02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, как преобразуется **шум** в **генераторе**:\n",
    "* Сначала с помощью **полносвязного слоя** он преобразуется в **первичные фичи**\n",
    "* Потом с помощью функции **view** **ресэмплится** в картинку низкого разрешения\n",
    "* Потом при прохождении через **conv_blocks** к нему поочерёдно применяются **Upsample** и **ОБЫЧНЫЕ** свёртки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size**2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [\n",
    "                nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "            ]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2**4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size**2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Initialize weight\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для отображения изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def show_gen_img(model, latent_dim=100):\n",
    "    z = Tensor(np.random.normal(0, 1, (9, latent_dim)))  # define latent dim\n",
    "\n",
    "    # Generate noise from latent dim\n",
    "    sample_images = generator(z)\n",
    "    sample_images = sample_images.cpu().detach()\n",
    "\n",
    "    # Plotting images\n",
    "    grid = (\n",
    "        make_grid(sample_images, nrow=3, ncols=3, normalize=True)\n",
    "        .permute(1, 2, 0)\n",
    "        .numpy()\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(grid)\n",
    "    plt.axis(\"off\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим данные и загрузим и их в Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "        # Adversarial ground truths\n",
    "        valid = Tensor(imgs.shape[0], 1).fill_(1.0)\n",
    "        fake = Tensor(imgs.shape[0], 1).fill_(0.0)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(Tensor)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        batches_done = epoch * len(data_loader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, num_epochs, i, len(data_loader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "            fig = show_gen_img(generator)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы картинки обрели приличный вид, хватает 2 эпох. Чтобы стали выглядеть хорошо — 5 эпох."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cGAN — GAN с условием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cGAN** расшифровывается как **Conditional Generative Adversarial Net** — это **GAN** с условием. Условие может быть любым, например, генерация конкретной цифры. В этом случае нам нужен уже размеченный датасет для того, чтобы обучить дискриминатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/conditional_gan_scheme.png\" width=\"800\"></center>\n",
    "<center><em>Схема работы cGAN. Label Y добавляется к случайному шуму, тем самым мы говорим генератору генерировать случайное изображение нужного класса. Также он подаётся в дискриминатор в качестве входа, чтобы дискриминатор знал, какое изображение классифицировать как реальное, а какое — как вымышленное.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение в данном случае будет аналогичным обучению **GAN**: мы будем обучать сети, чередуя реальные данные и сгенерированные, добавив `label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/cGANS_results_20_and_50_epochs_mnist.png\" width=\"600\"></center>\n",
    "<center><em>Сравнение результатов cGAN и cDCGAN.</em></center>\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/1511.06434.pdf\">Unsupervised Representation Learning with Deep Convolutional Generative Adversial Networks</a></p> </em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как закодировать метки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку подавать в сеть числа от 0 до 9 (в случае **MNSIT**) нет смысла, то нужно придумать, как подавать их в нейронную сеть. На помощь приходят **Embeddings**. Мы можем представить каждую метку в виде вектора с десятью элементами.\n",
    "\n",
    "[Документация nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = next(iter(data_loader))\n",
    "\n",
    "label_emb = nn.Embedding(10, 10)\n",
    "\n",
    "e = label_emb(labels)\n",
    "\n",
    "print(f\"Label: {labels[0]}\")\n",
    "print(f\"Embedding for this label: {e[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого **эмбединги** меток обычно склеиваются с входами сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Почему нельзя подать просто число?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы будем подавать просто число, например 0.1 для единицы, и 0.5 для пяти, то вход у нас будет непрерывным, что довольно нелогично: тогда при небольшом изменении входа мы будем генерировать другую цифру. А также сети будет сложнее выучить небольшие расхождения в этом небольшом интервале. В случае с векторным представлением мы избегаем этих проблем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модификации cGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метки классов можно подавать не только способом, описанным выше. Можно вместо подачи их в дискриминатор сделать так, чтобы он их предсказывал — **Semi-Supervised GAN**.\n",
    "\n",
    "Или же не подавать label в дискриминатор, но ждать от него классификации в соответствии с классом, который мы хотим получить от генератора — это **InfoGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна модификация cGAN — это **AC-GAN** (auxiliary classifier), в которой единственное различие заключается в том, что дискриминатор должен помимо распознавания реальных и фейковых изображений ещё и классифицировать их. Он имеет эффект стабилизации процесса обучения и позволяет генерировать большие высококачественные изображения, изучая представление в скрытом пространстве, которое не зависит от метки класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gans_zoo_schemes.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тонкости обучения GANов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья — детальный разбор тонкностей и советов](https://beckham.nz/2021/06/28/training-gans.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частые/простые ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Убедитесь, что сгенерированые сэмплы в том же диапазоне, что и реальные данные.** Например, реальные данные `[-1,1]`, при этом генерируются данные `[0,1]`. Это нехорошо, так как это подсказка для дискриминатора. \n",
    "* **Убедитесь, что сгенерированные сэмплы того же размера, что и реальные данные.** Например, размер картинок в MNIST `(28,28)`, а генератор выдает `(32,32)`. В таком случае нужно либо изменить архитектуру генератора, чтобы получать на выходе размер `(28,28)`, либо сделать ресайз реальных данных до `(32,32)`.\n",
    "* **Старайтесь не использовать `BatchNorm`**. Проблема `BN` в том, что во время обучения его внутренняя статистика считается по минибатчу, а во время инференса она вычисляется как *moving average*, что в свою очередь может повлечь непредсказуемые результаты. Если архитектура GAN предполагает нормализацию, то лучше использовать **`InstanceNorm`**.\n",
    "* **Визуализируйте свои лоссы в процессе обучения**. Для этого существует множество прекрасных библиотек (например, TensorBoard). Следить за бегущими по экрану цифрами от двух соревнующихся между собой лоссов бессмысленно.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем давать преимущество дискриминатору"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ваша версия **GAN** работает не так хорошо, как вам хотелось бы, попробуйте дать своему **дискриминатору** преимущество, обучив его на относительно большее число итераций, чем **генератор**. Другими словами, чем лучше **дискриминатор** различает настоящие и фальшивые данные, тем лучше сигнал, который **генератор** может извлечь из него. Обратите внимание, что эта логика не имела смысла во времена \"до WGAN\", поскольку слишком хорошая работа дискриминатора вредила обучению.\n",
    "\n",
    "Например:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def train_on_batch(x, iter_, n_gen=5):\n",
    "     Generator:\n",
    "    ...\n",
    "    ...\n",
    "    if iter_ % n_gen == 0:\n",
    "        g_loss.backward()\n",
    "        opt_g.step()\n",
    "        \n",
    "     Discriminator:\n",
    "    ...\n",
    "    ...\n",
    "    d_loss.backward() \n",
    "    d_loss.step()\n",
    "```\n",
    "\n",
    "Где `iter_` — текущая итерация шага градиента, а `n_gen` определяет интервал между обновлениями генератора. В данном случае, поскольку он равен 5, мы можем считать, что это означает, что дискриминатор обновляется в 5 раз чаще, чем генератор.\n",
    "\n",
    "Естественно, работает не всегда и не везде. Но попробовать стоит\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование оптимизатора ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что почти во всех статьях по **GAN** используется **ADAM**. Сложно сказать, почему так получается, но он работает, и работает очень хорошо. Если качество вашего **GAN** оставляет желать лучшего, скорее всего оптимизатор тут не при чем. Ищите ошибку где-то еще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `epsilon` **ADAM** по умолчанию в PyTorch равен `1e-8`, что может вызвать проблемы после длительного периода обучения, например, значения функции потерь периодически взрываются или увеличиваются. Подробнее об этом на [StackOverflow](https://stackoverflow.com/questions/42327543/adam-optimizer-goes-haywire-after-200k-batches-training-loss-grows) и в комментарии на [Reddit](https://www.reddit.com/r/reinforcementlearning/comments/j9rflf/intuitive_explanation_for_adams_epsilon_parameter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье [Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples (Sinha et al., 2020)](https://arxiv.org/abs/2002.06224) утверждается, что если просто **обнулить вклад градиента от сэмплов, которые дискриминатор считает поддельными**, то генератор обучается значительно лучше, достигая нового **SOTA** (реализуется в одну строчку)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "на основе https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/course_UvA-DL/09-normalizing-flows.ipynb#scrollTo=bd028bb1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://github.com/Lightning-AI/lightning-tutorials/raw/main/course_UvA-DL/09-normalizing-flows/normalizing_flow_layout.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Диффузионные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "на основе https://arxiv.org/abs/2006.11239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прямой диффузный процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы имеем некоторый объект $\\mathbf{x}_0$ из заданного имеющимся датасетом распределения $\\mathbf{x}_0 \\sim q(\\mathbf{x})$. Определим так называемый прямой диффузный процесс, в ходе которого мы будем последовательно добавлять небольшое количество Гауссового шума последовательно $T$ раз, создавая из нашего объекта последовательность $\\mathbf{x}_1, \\dots, \\mathbf{x}_T$ постепенно всё более зашумленных версий нашего исходного объекта. Параметры добавляемого шума зависят от номера шага зашумления и их набор $\\{\\beta_t \\in (0, 1)\\}_{t=1}^T$ задан заранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/DDPM.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходное распределение данных из датасета будет последовательно преобразовано в Гауссов шум:\n",
    "\n",
    "$$q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\quad\n",
    "q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})$$\n",
    "\n",
    "Важным свойством описанного выше процесса является то, что он допускает аналитическое вычисление шума на любом из шагов процесса в явном виде. Это связано с тем, что сумма нескольких нормально распределенных случайных величин также является случайной величиной с нормальным распределением. Введём обозначения из оригинальной статьи $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$ и $\\alpha_t = 1 - \\beta_t$, тогда:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{x}_t \n",
    "&= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} & \\text{ ;где } \\boldsymbol{\\epsilon}_{t-1}, \\boldsymbol{\\epsilon}_{t-2}, \\dots \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\\\\n",
    "&= \\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\boldsymbol{\\epsilon}}_{t-2} & \\text{ ;где } \\bar{\\boldsymbol{\\epsilon}}_{t-2} \\text{ новая нормально распределенная величина (*).} \\\\\n",
    "&= \\dots \\\\\n",
    "&= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon} \\\\\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_0) &= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\n",
    "\\end{aligned} $$\n",
    "\n",
    "(*)  Напомним, что когда мы складываем две нормально распределенные величины с разной дисперсией и нулевым средним $\\mathcal{N}(\\mathbf{0}, \\sigma_1^2\\mathbf{I})$ и $\\mathcal{N}(\\mathbf{0}, \\sigma_2^2\\mathbf{I})$,то получаем новую нормально распределенную случайную величину $\\mathcal{N}(\\mathbf{0}, (\\sigma_1^2 + \\sigma_2^2)\\mathbf{I})$, что в нашей параметризации означает $\\sqrt{(1 - \\alpha_t) + \\alpha_t (1-\\alpha_{t-1})} = \\sqrt{1 - \\alpha_t\\alpha_{t-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратный диффузный процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod^T_{t=1} p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) \\quad\n",
    "p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/diffusion-example.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение функции потерь*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация прямого и обратного диффузного процесса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128, 32)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256, 16)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256, 8)\n",
    "\n",
    "        self.bot1 = DoubleConv(256, 512)\n",
    "        self.bot2 = DoubleConv(512, 512)\n",
    "        self.bot3 = DoubleConv(512, 256)\n",
    "\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128, 16)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64, 32)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.sa6 = SelfAttention(64, 64)\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение диффузных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
    "        if self.step < step_start_ema:\n",
    "            self.reset_parameters(ema_model, model)\n",
    "            self.step += 1\n",
    "            return\n",
    "        self.update_model_average(ema_model, model)\n",
    "        self.step += 1\n",
    "\n",
    "    def reset_parameters(self, ema_model, model):\n",
    "        ema_model.load_state_dict(model.state_dict())\n",
    "\n",
    "def train(args):\n",
    "    setup_logging(args.run_name)\n",
    "    device = args.device\n",
    "    dataloader = get_data(args)\n",
    "    model = UNet_conditional(num_classes=args.num_classes).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    mse = nn.MSELoss()\n",
    "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
    "    l = len(dataloader)\n",
    "    ema = EMA(0.995)\n",
    "    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        logging.info(f\"Starting epoch {epoch}:\")\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            if np.random.random() < 0.1:\n",
    "                labels = None\n",
    "            predicted_noise = model(x_t, t, labels)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ema.step_ema(ema_model, model)\n",
    "\n",
    "            pbar.set_postfix(MSE=loss.item())\n",
    "            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "from modules import UNet\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    setup_logging(args.run_name)\n",
    "    device = args.device\n",
    "    dataloader = get_data(args)\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    mse = nn.MSELoss()\n",
    "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
    "    l = len(dataloader)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        logging.info(f\"Starting epoch {epoch}:\")\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, (images, _) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            predicted_noise = model(x_t, t)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(MSE=loss.item())\n",
    "            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
    "\n",
    "        sampled_images = diffusion.sample(model, n=images.shape[0])\n",
    "        save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))\n",
    "        torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))\n",
    "\n",
    "\n",
    "def launch():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args()\n",
    "    args.run_name = \"DDPM_Uncondtional\"\n",
    "    args.epochs = 500\n",
    "    args.batch_size = 12\n",
    "    args.image_size = 64\n",
    "    args.dataset_path = r\"C:\\Users\\dome\\datasets\\landscape_img_folder\"\n",
    "    args.device = \"cuda\"\n",
    "    args.lr = 3e-4\n",
    "    train(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    launch()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "from modules import UNet_conditional, EMA\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=256, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, labels, cfg_scale=3):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t, labels)\n",
    "                if cfg_scale > 0:\n",
    "                    uncond_predicted_noise = model(x, t, None)\n",
    "                    predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    setup_logging(args.run_name)\n",
    "    device = args.device\n",
    "    dataloader = get_data(args)\n",
    "    model = UNet_conditional(num_classes=args.num_classes).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    mse = nn.MSELoss()\n",
    "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
    "    l = len(dataloader)\n",
    "    ema = EMA(0.995)\n",
    "    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        logging.info(f\"Starting epoch {epoch}:\")\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            if np.random.random() < 0.1:\n",
    "                labels = None\n",
    "            predicted_noise = model(x_t, t, labels)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ema.step_ema(ema_model, model)\n",
    "\n",
    "            pbar.set_postfix(MSE=loss.item())\n",
    "            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            labels = torch.arange(10).long().to(device)\n",
    "            sampled_images = diffusion.sample(model, n=len(labels), labels=labels)\n",
    "            ema_sampled_images = diffusion.sample(ema_model, n=len(labels), labels=labels)\n",
    "            plot_images(sampled_images)\n",
    "            save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))\n",
    "            save_images(ema_sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}_ema.jpg\"))\n",
    "            torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))\n",
    "            torch.save(ema_model.state_dict(), os.path.join(\"models\", args.run_name, f\"ema_ckpt.pt\"))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(\"models\", args.run_name, f\"optim.pt\"))\n",
    "\n",
    "\n",
    "def launch():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args()\n",
    "    args.run_name = \"DDPM_conditional\"\n",
    "    args.epochs = 300\n",
    "    args.batch_size = 14\n",
    "    args.image_size = 64\n",
    "    args.num_classes = 10\n",
    "    args.dataset_path = r\"C:\\Users\\dome\\datasets\\cifar10\\cifar10-64\\train\"\n",
    "    args.device = \"cuda\"\n",
    "    args.lr = 3e-4\n",
    "    train(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def plot_images(images):\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    plt.imshow(torch.cat([\n",
    "        torch.cat([i for i in images.cpu()], dim=-1),\n",
    "    ], dim=-2).permute(1, 2, 0).cpu())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_images(images, path, **kwargs):\n",
    "    grid = torchvision.utils.make_grid(images, **kwargs)\n",
    "    ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)\n",
    "\n",
    "\n",
    "def get_data(args):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(80),  # args.image_size + 1/4 *args.image_size\n",
    "        torchvision.transforms.RandomResizedCrop(args.image_size, scale=(0.8, 1.0)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.ImageFolder(args.dataset_path, transform=transforms)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def setup_logging(run_name):\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"models\", run_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"results\", run_name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Условные диффузные модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Использованная литература</font>\n",
    "\n",
    "<font size=\"5\">GAN</font>\n",
    "\n",
    "[Книга по генеративным сетям](https://habr.com/ru/company/piter/blog/504956/)\n",
    "\n",
    "[Generative Adversarial Networks (Goodfellow et al., 2014)](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "[Видео разбор оригинальной статьи GAN](https://youtu.be/eyxmSmjmNS0)\n",
    "\n",
    "[Видео лекции Иана Гудфеллоу](https://www.youtube.com/watch?v=HGYYEUSm-0Q)\n",
    "\n",
    "[Generative adversarial networks](https://deepgenerativemodels.github.io/notes/gan/)\n",
    "\n",
    "[Самые современные генеративные модели](https://paperswithcode.com/methods/category/generative-models)\n",
    "\n",
    "[exactly how the NVIDIA GauGAN neural network works](https://sudonull.com/post/29972-Pictures-from-rough-sketches-how-exactly-the-NVIDIA-GauGAN-neural-network-works-ITSumma-Blog)\n",
    "\n",
    "<font size=\"5\">DCGAN</font>\n",
    "\n",
    "[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford et al., 2015)](https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "[DCGAN TUTORIAL](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "\n",
    "<font size=\"5\">Wassserstein GAN</font>\n",
    "\n",
    "[Wasserstein GAN (Arjovsky et. al., 2017)](https://arxiv.org/abs/1701.07875)\n",
    "\n",
    "[Блог пост о Wasserstein GAN](https://vincentherrmann.github.io/blog/wasserstein/)\n",
    "\n",
    "[Improved Training of Wasserstein GANs (Gulrajani et al., 2017)](https://arxiv.org/abs/1704.00028)\n",
    "\n",
    "[Spectral Normalization for Generative Adversarial Networks (Miyato et al., 2018)](https://arxiv.org/abs/1802.05957).\n",
    "\n",
    "<font size=\"5\">ProGAN -> StyleGAN -> StyleGAN2 -> Alias-Free GAN</font>\n",
    "\n",
    "[Progressive Growing of GANs for Improved Quality, Stability, and Variation (ProGAN) [Karras et al., 2017]](https://arxiv.org/abs/1710.10196)\n",
    "\n",
    "[A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN) [Karras et al., 2018]](https://arxiv.org/abs/1812.04948)\n",
    "\n",
    "[Analyzing and Improving the Image Quality of StyleGAN (StyleGAN2) [Karras et al., 2019]](https://arxiv.org/abs/1912.04958)\n",
    "\n",
    "[Alias-Free Generative Adversarial Networks (Alias-Free GAN) [Karras et al., 2021]](https://arxiv.org/abs/2106.12423)\n",
    "\n",
    "<font size=\"5\">Тонкости обучения GAN</font>\n",
    "\n",
    "[Статья - детальный разбор тонкостей и советов](https://beckham.nz/2021/06/28/training-gans.html)\n",
    "\n",
    "[Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples (Sinha et al., 2020)](https://arxiv.org/abs/2002.06224)\n",
    "\n",
    "<font size=\"5\">GAN Zoo:</font>\n",
    "\n",
    "<font size=\"5\">BigGAN</font>\n",
    "\n",
    "[Large Scale GAN Training for High Fidelity Natural Image Synthesis (Brock et al., 2018)](https://arxiv.org/abs/1809.11096)\n",
    "\n",
    "<font size=\"5\">StackGAN</font>\n",
    "\n",
    "[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al., 2016)](https://arxiv.org/abs/1612.03242)\n",
    "\n",
    "[StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al., 2017)](https://arxiv.org/abs/1710.10916)\n",
    "\n",
    "[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (Ledig et al., 2016)](https://arxiv.org/abs/1609.04802)\n",
    "\n",
    "[Let’s Read Science! “StackGAN: Text to Photo-Realistic Image Synthesis”](https://medium.com/@rangerscience/lets-read-science-stackgan-text-to-photo-realistic-image-synthesis-4562b2b14059)\n",
    "\n",
    "[Deep Learning Generative Models for Image Synthesis and Image Translation](https://www.rulit.me/data/programs/resources/pdf/Generative-Adversarial-Networks-with-Python_RuLit_Me_610886.pdf)\n",
    "\n",
    "[youtube [StackGAN++] Realistic Image Synthesis with Stacked Generative Adversarial Networks | AISC](https://www.youtube.com/watch?v=PXWIaLE7_NU)\n",
    "\n",
    "[youtube Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://www.youtube.com/watch?v=crI5K4RCZws)\n",
    "\n",
    "<font size=\"5\">ControlGAN</font>\n",
    "\n",
    "[Controllable Generative Adversarial Network](https://arxiv.org/pdf/1708.00598.pdf)\n",
    "\n",
    "[Controllable Text-to-Image Generation](https://arxiv.org/pdf/1909.07083.pdf)\n",
    "\n",
    "[Image Generation and Recognition (Emotions)](https://arxiv.org/pdf/1910.05774.pdf)\n",
    "\n",
    "[Natural Language & Text-to-Image 2019](https://meta-guide.com/data/data-processing/text-to-image-systems/natural-language-text-to-image-2019)\n",
    "\n",
    "<font size=\"5\">AC-GAN</font>\n",
    "\n",
    "[How to Develop an Auxiliary Classifier GAN (AC-GAN) From Scratch with Keras](https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/)\n",
    "\n",
    "[Understanding ACGANs with code[PyTorch]](https://towardsdatascience.com/understanding-acgans-with-code-pytorch-2de35e05d3e4)\n",
    "\n",
    "[An Auxiliary Classifier Generative Adversarial Framework for Relation Extraction](https://arxiv.org/pdf/1909.05370.pdf)\n",
    "\n",
    "[A Multi-Class Hinge Loss for Conditional GANs](https://openaccess.thecvf.com/content/WACV2021/papers/Kavalerov_A_Multi-Class_Hinge_Loss_for_Conditional_GANs_WACV_2021_paper.pdf)\n",
    "\n",
    "<font size=\"5\">Domain Transfer Network</font>\n",
    "\n",
    "[Unsupervised Cross-Domain Image Generation (Taigma et al., 2016)](https://arxiv.org/abs/1611.02200)\n",
    "\n",
    "<font size=\"5\">Pix2Pix</font>\n",
    "\n",
    "[Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2016)](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "<font size=\"5\">Семантическая генерация</font>\n",
    "\n",
    "[Learning to Generate Chairs, Tables and Cars\n",
    "with Convolutional Networks (Dosovitskiy et al., 2017)](https://arxiv.org/abs/1411.5928)\n",
    "\n",
    "<font size=\"5\">Text to image</font>\n",
    "\n",
    "[Text-to-Image Generation with Attention Based Recurrent Neural Networks (Zia et al., 2020)](https://arxiv.org/abs/2001.06658)\n",
    "\n",
    "<font size=\"5\">Image-to-Image</font>\n",
    "\n",
    "[GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (Chong et al., 2021)](https://arxiv.org/abs/2106.06561)\n",
    "\n",
    "[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (Zhu et al., 2017)](https://arxiv.org/abs/1703.10593)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Ссылки</font>\n",
    "\n",
    "[GitHub MNIST CelebA cGAN cDCGAN](https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN)\n",
    "\n",
    "[GitHub Text-to-Photo realistic Image Synthesis with Stacked Generative Adversarial Networks](https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks)\n",
    "\n",
    "[GitHub ControlGAN](https://github.com/mrlibw/ControlGAN)\n",
    "\n",
    "[GitHub ControlGAN-Tensorflow](https://github.com/taki0112/ControlGAN-Tensorflow)\n",
    "\n",
    "[GitHub Keras-ACGan](https://github.com/lukedeo/keras-acgan)\n",
    "\n",
    "[Множество примеров различных генераторов](https://thisxdoesnotexist.com)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
