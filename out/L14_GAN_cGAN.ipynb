{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Генеративные модели</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Классические\" генеративные алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача генерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом курсе мы в основном работали с **размеченными** данными. Мы научили нейронные сети решать задачи классификации, сегментации и т.д.\n",
    "\n",
    "В этой лекции мы разберемся, как научить нейросеть создавать что-то новое.\n",
    "\n",
    "\n",
    "Как подойти к такой задаче с помощью нейронных сетей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Постановка задачи генерации**\n",
    "\n",
    "**Дано**: неразмеченные данные\n",
    "\n",
    "**Выход**: новые данные, которые будут удовлетворять следующим условиям:\n",
    "* Новые данные должны быть **похожи** на исходные.\n",
    "* Но **не повторять** их в точности (или повторять, при случайном стечении обстоятельств).\n",
    "* Чтобы результат генерации был различным при разных запусках, нам нужен **элемент случайности**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простейший пример: генерация объектов из нормального распределения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, у нас под рукой есть генератор случайных чисел, который позволяет нам легко получить случайные числа в диапазоне $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "uniform_noise = np.random.uniform(size=1000)\n",
    "plt.hist(uniform_noise)\n",
    "plt.title(\"uniform distribution U\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $U_i$ — точка, которую даёт нам генератор равномерно распределенных случайных чисел. Если мы возьмём пару таких таких точек $U_1$, $U_2$ и произведём с ними так называемое [преобразование Бокса-Мюллера](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform), то получим пару новых точек $X$ и $Y$ на вещественной прямой:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "X=\\sqrt{-2 \\log U_{1}} \\cos \\left(2 \\pi U_{2}\\right) \\\\\n",
    "Y=\\sqrt{-2 \\log U_{1}} \\sin \\left(2 \\pi U_{2}\\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторив такую процедуру для большого числа точек $U_i$, можно заметить, что распределение для объектов $X$ и $Y$ становится похожим на стандартное нормальное:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U1 = np.random.uniform(size=1000)\n",
    "U2 = np.random.uniform(size=1000)\n",
    "R = np.sqrt(-2 * np.log(U1))\n",
    "Theta = 2 * np.pi * U2\n",
    "X = R * np.cos(Theta)\n",
    "Y = R * np.sin(Theta)\n",
    "\n",
    "plt.hist(X)\n",
    "plt.title(\"Box–Muller transform of U\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_noise = np.random.normal(size=1000)\n",
    "plt.hist(gaussian_noise)\n",
    "plt.title(\"Gaussian distribution N\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле мы можем в точности доказать, что преобразование Бокса-Мюллера преобразовало исходное равномерное распределение в стандартное нормальное:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "X=\\sqrt{-2 \\log U_{1}} \\cos \\left(2 \\pi U_{2}\\right) \\\\\n",
    "Y=\\sqrt{-2 \\log U_{1}} \\sin \\left(2 \\pi U_{2}\\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решим уравнение относительно переменных $U_i$:\n",
    "$$ \\large\n",
    "\\begin{array}{l}\n",
    "U_{1}=e^{-\\left(X^{2}+Y^{2}\\right) / 2} \\\\\n",
    "U_{2}=\\frac{1}{2 \\pi} \\arctan \\left(\\frac{X}{Y}\\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица Якоби для такого преобразования:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large\n",
    "J(X,Y) = \\begin{bmatrix}\n",
    "\\frac{\\partial U_1}{\\partial X} & \\frac{\\partial U_1}{\\partial Y}\\\\\n",
    "\\frac{\\partial U_2}{\\partial X} & \\frac{\\partial U_2}{\\partial Y}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "-X e^{\\frac{-(X^2+Y^2)}{2}}  & -Y e^{\\frac{-(X^2+Y^2)}{2}}\\\\\n",
    "\\frac{1}{2\\pi} \\frac{Y}{X^2 + Y^2} & \\frac{1}{2\\pi} \\frac{-X}{X^2 + Y^2}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсюда имеем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large\n",
    "\\begin{aligned}\n",
    "f_{X,Y}(x,y)&=f_{U_1,U_2}(e^{-\\left(X^{2}+Y^{2}\\right) / 2}, \\frac{1}{2 \\pi} \\arctan \\left(\\frac{X}{Y}\\right)) |\\det(J)|\\\\\n",
    "&= |\\det(J)|\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{X^2}{2}} \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{Y^2}{2}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы получили пару независимых случайных чисел $X$ и $Y$, каждое из которых лежит в нормальном распределении:\n",
    "$$\n",
    "X\\perp Y, \\quad X \\sim \\mathcal{N}(0,1),\\quad Y\\sim \\mathcal{N}(0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать, если нам хочется сгенерировать объекты не в стандартном нормальном распределении, а в каком-то более сложном? В общем случае нам необходимо подобрать некоторую функцию, которая будет отображать объекты из известного \"простого\" распределения в \"целевое\". Осуществляющие такое преобразование функции будем называть **генеративными моделями**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если целевое распределение $\\mathcal{F}[\\vec {x}]$ известно аналитически, то для подбора такой функции можно воспользоваться различными модификациями алгоритма [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/generative_models.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генеративные алгоритмы, основанные на глубоком обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/deep_generative_models.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение  в генеративно-состязательные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Эволюция в генерации изображений лиц:**\n",
    "\n",
    "[Множество примеров различных генераторов](https://thisxdoesnotexist.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/faces_generation_quality_progress.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся с **элементом случайности**. В нейронных сетях мы привыкли к **воспроизводимости** результата: в режиме валидации мы можем несколько раз подать на вход один и тот же объект и получить один и тот результат.  Возникает два вопроса:\n",
    "- что подавать на **вход** сети для генерации?\n",
    "- как реализовать **случайность**?\n",
    "\n",
    "Ответ на оба вопроса: подавать в качестве **входа** вектор **случайного шума**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/generator_model_pipeline.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему именно **вектор**? Почему не одно **случайное число**?\n",
    "\n",
    "**Ответ**: входной вектор можно рассматривать как **признаки** генерируемого объекта. Каждый такой признак — **независимая случайная величина**. Если мы будем передавать только одно случайное число, то генерация будет однообразной. Чем больше признаков (степеней свободы) у входного вектора, тем разнообразнее будет результат генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть случайный шум **большей размерности** даёт нам **больше вариабельности**  для генерации. Это называется **input latent space** — входное латентное пространство.\n",
    "\n",
    "**Note:** *из-за неустоявшейся терминологии случайное распределение на входе генератора называется латентным пространством так же, как и скрытое пространство в автоэнкодерах. Поэтому в этой лекции будем называть его **входным** латентным пространством. Также в статьях встречается вариант: predefined latent space.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждую компоненту латентного пространства можно рассматривать как отдельную шкалу, вдоль которой изменяются определенные свойства генерируемых объектов. Например, можно выбрать четыре латентных вектора и посмотреть, как генерируемые объекты плавно изменяются при переходе от одного вектора к другому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/input_latent_space_lin_interpol.png\" width=\"600\">\n",
    "\n",
    "<center><em>Линейные интерполяции между четырьмя изображениями в латентном пространстве</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/1803.09093\">Comparing Generative Adversarial Network Techniques for Image Creation and Modification(M. Pieters, M. Wiering, 2018)</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Размерность входного латентного пространства"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В выборе размерности входного латентного пространства важно соблюсти  баланс.\n",
    "- при **низкой размерности** возникнет проблема **низкой вариабельности**.\n",
    "\n",
    "Пример: генератор лиц с входным вектором длины 1. Результатом работы генератора будет всего одна шкала, вдоль которой будут расположены генерируемые изображения. Скорее всего, генератор выучит наиболее простую и \"очевидную\" шкалу: от молодой женщины блондинки к пожилому мужчине брюнету. У такой сети будет низкая вариабельность — она не сможет сгенерировать, например, рыжего ребенка в очках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- при **большой размерности** латентное пространство может быть слишком **разреженным**.\n",
    "\n",
    "При обучении модели количество точек в этом латентном пространстве будет настолько мало, что в основном пространство будет состоять из пустот. Тогда модель будет крайне некачественно генерировать объекты в точках латентного пространства, далеких от точек обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший способ выбрать длину вектора — это найти публикацию с похожей задачей и взять значение из нее.\n",
    "\n",
    "Если такой информации нет, то придется экспериментировать. Лучше начинать с низкой размерности латентного пространства, чтобы наладить работу всей сети, пусть и с низким разнообразием, а затем проводить эксперименты по поиску оптимальной размерности.\n",
    "\n",
    "Можно использовать собственные знания в предметной области: спросите себя, сколькими вещественными числами можно описать важную информацию об объекте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Распределение входных латентных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы знаем из лекции про обучение сети, инициализация весов и нормализация входных данных вносят существенный вклад в работу модели. Поэтому принято использовать **многомерное нормальное распределение** для input latent space. Оно лучше взаимодействует с весами модели и улучшает сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/binomial_distribution.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://matplotlib.org/3.1.0/gallery/lines_bars_and_markers/scatter_hist.html\">Двумерное нормальное распределение</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный подход в решении задачи генерации\n",
    "(как делать на практике НЕ нужно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем создать генератор точек на параболе (cамым тривиальным решением кажется подача случайного шума на вход сети) и будем ожидать на выходе точки параболы. Проверим, как это будет работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x * x\n",
    "    return np.hstack(\n",
    "        (x.reshape(-1, 1), y.reshape(-1, 1))\n",
    "    )  # Create num of correct dots(x,y) on parabola\n",
    "\n",
    "\n",
    "pairs = gen_pair(100)\n",
    "plt.scatter(pairs[:, 0], pairs[:, 1])\n",
    "plt.title(\"Random dots on parabola,\\nwhich will use like a dataset.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём размерность входного латентного пространства $ls = 1$ и объединим шум с точками в датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define input parameters\n",
    "n_batches = 10\n",
    "batch_size = 128\n",
    "ls = 1  # latent space\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(size=(n_batches * batch_size, ls), dtype=torch.float)\n",
    "print(f\"NN Input: noise.shape: {noise.shape}\")\n",
    "\n",
    "# Generates dots on parabola\n",
    "xy_pair = gen_pair(num=(n_batches * batch_size))\n",
    "xy_pair = torch.tensor(xy_pair, dtype=torch.float)\n",
    "print(f\"NN Output: xy_pair.shape: {xy_pair.shape}\")\n",
    "\n",
    "dataset = TensorDataset(noise, xy_pair)  # model inputs, model outputs\n",
    "trainset, testset = train_test_split(\n",
    "    dataset, train_size=0.8\n",
    ")  # split dataset for train and test\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простую модель, которая будет ожидать шум на вход и генерировать точки на выходе. (Обратите внимание, что функция активации на последнем слое отсутствует, поскольку мы не ограничиваем наш генератор в каком-то диапазоне.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GenModel(nn.Module):\n",
    "    def __init__(self, latent_space):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2),\n",
    "        )  # x,y\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки loss.\n",
    "\n",
    "Так как мы не знаем, в каком месте параболы генератор создаст новую точку, то непонятно, с каким элементом из датасета ее сравнивать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №1**\n",
    "\n",
    "Для сгенерированного $x$ аналитически вычислять $y_{target}=x*x$ и считать разницу между $y$, сгенерированным моделью, и $y_{target}$, вычисленным аналитически:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pair, label):\n",
    "    # All inputs are batches\n",
    "    x_fake = pair[:, 0]\n",
    "    y_fake = pair[:, 1]\n",
    "    return torch.abs(x_fake * x_fake - y_fake).mean()  # average by batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это будет работать.\n",
    "\n",
    "Однако если мы знаем способ точно предсказать выход по входу, то задача уже решена и нейронная сеть не нужна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №2**\n",
    "\n",
    "Найти в датасете точку  $ target = (x_{target},y_{target})$, наиболее близкую к созданной генератором $ generated = (x,y)$, и использовать расстояние между этими точками в качестве loss.\n",
    "\n",
    "$$\\large Loss = min(dist(target_{i},generated))$$\n",
    "\n",
    "В пространстве высокой размерности такой поиск будет весьма ресурсозатратным, но в нашем учебном примере работать будет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, targets):\n",
    "        super().__init__()\n",
    "        self.targets = targets  # Remember all real samples, impossible in real world\n",
    "\n",
    "    def forward(self, input, dummy_target=None):\n",
    "        dist = torch.cdist(input, self.targets)  # claculate pairwise distances (euc.)\n",
    "        min_dist, index = torch.min(dist, dim=1)  # take the best\n",
    "        return min_dist.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательный код для вывода loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_test_loss(model, loader):\n",
    "    test_data = next(iter(loader))\n",
    "    test_loss = Loss(test_data[1])\n",
    "    outputs = model(samples.to(device))\n",
    "    return test_loss(outputs.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной код обучения.\n",
    "\n",
    "Целевые точки из датасета запоминаются в loss, затем идет обычный цикл обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_epochs = 600\n",
    "model = GenModel(latent_space=ls)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "all_train_targets = next(iter(train_loader))[1]\n",
    "criterion = Loss(all_train_targets.to(device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_epoch = 0\n",
    "    for samples, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples.to(device))\n",
    "        loss = criterion(outputs.to(device), labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "\n",
    "    loss_test = get_test_loss(model, test_loader)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch={epoch} train_loss={loss_epoch/len(train_loader):.4} test_loss={loss_test:.4}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим результаты генерации на шуме:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(model, pairs, ls=1):\n",
    "    model.eval().to(\"cpu\")\n",
    "    noise = torch.tensor(np.random.normal(size=(1000, ls)), dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "        xy_pair_gen = model(noise)\n",
    "\n",
    "    xy_pair_gen = xy_pair_gen.detach().numpy()\n",
    "    plt.scatter(pairs[:, 0], pairs[:, 1], color=\"red\", label=\"real\")\n",
    "    plt.scatter(xy_pair_gen[:, 0], xy_pair_gen[:, 1], color=\"blue\", label=\"generated\")\n",
    "    plt.axis([-1, 1, 0, 1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "test_image(model, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель генерирует точки, лежащие на параболе, при этом все они лежат в довольно узком интервале по оси х.\n",
    "\n",
    "\n",
    "Это неудивительно: в loss function мы прописали, что сгенерированная точка должна лежать на параболе, и модель обучилась. А информацию о том, в каких частях кривой должны оказаться точки, мы в loss никак не кодировали.\n",
    "\n",
    "Более того, модель может научиться хорошо генерировать одну единственную точку, и при этом loss может стать нулевым.\n",
    "\n",
    "\n",
    "Итак, надо решить две проблемы:\n",
    "\n",
    "\n",
    "1.   Закодировать в loss условие о том, что точки должны быть различными.\n",
    "2.   Придумать способ проверки, не требующий перебора всего датасета.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем наказывать нейронную сеть не напрямую стандартной loss function, а второй **сетью**, которая определяет, лежит ли сгенерированная точка на параболе.\n",
    "\n",
    "Создадим сеть-классификатор точек (лежит/не лежит на параболе), которую назовём **дискриминатор** или критик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisModel(nn.Module):\n",
    "    def __init__(self, n_points):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15, 1),  # real/fake\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 2 * n_points)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае задача дискриминатора — определять, принадлежит ли объект к распределению обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итого** мы имеем:\n",
    "- **генератор**, выдающий точки, которые могут принадлежать параболе, а могут не принадлежать ей;\n",
    "- **дискриминатор**, который будет их различать.\n",
    "\n",
    "Мы будем подавать в **дискриминатор** **правильные точки** (чтобы он знал, как это должно выглядеть) и **точки, которые выдаёт генератор**, считая их подделкой.\n",
    "\n",
    "Таким образом, **генератор** будет учиться **подражать** реальным данным, а дискриминатор будет учиться **отличать** реальные точки, от подделок.\n",
    "\n",
    "Мы пришли к идее **генеративно-состязательных** нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generative adversarial network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2014 Generative Adversarial Networks (Goodfellow et al., 2014)](https://arxiv.org/abs/1406.2661) (**Cited by 33430!!!**)\n",
    "\n",
    "[Видео разбор оригинальной статьи](https://youtu.be/eyxmSmjmNS0)\n",
    "\n",
    "[Видео лекции Иана Гудфеллоу](https://www.youtube.com/watch?v=HGYYEUSm-0Q)\n",
    "\n",
    "**Генеративно-состязательную** сеть описал Иан Гудфеллоу из компании Google (на тот момент) в 2014 году. Сейчас он возглавляет подразделение машинного обучения в Apple. Принцип состязательности в сети **GAN** нередко описывается через метафоры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/generative_adversarial_network_scheme.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Схематичное представление архитектуры GAN </em></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор — фальшивомонетчик!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще со времен **AlexNet** мы знаем, что если мы что-то и умеем делать с нейросетями, то это **классификаторы**. В классическом GAN **дискриминатор** выполняет простейшую из задач классификации — **бинарную классификацию** (либо *real*, либо *fake*). А вот задача **генерации** каким-то прямым образом на тот момент решена не была.\n",
    "\n",
    "Как использовать всю мощь классификатора для создания генератора?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что есть фальшивомонетчик $G$ (generator) и банкир с прибором для проверки подлинности купюр $D$ (discriminator).\n",
    "\n",
    "Фальшивомонетчик черпает вдохновение из генератора случайных чисел в виде случайного шума $z$ и создает подделки $G(z)$.\n",
    "\n",
    "Банкир $D$ получает на вход пачку купюр $x$, проверяет их подлинность и сообщает вектор $D(x)$, состоящий из чисел от нуля до единицы — свою уверенность (вероятность) по каждой купюре в том, что она настоящая. Его цель — выдавать нули для подделок $D(G(z))$ и единицы для настоящих денег $D(x)$. Задачу можно записать как максимизацию произведения $D(x)(1-D(G(z)))$, а произведение, в свою очередь, можно представить как сумму через логарифм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, задача банкира — максимизировать $log(D(x))+log(1-D(G(z)))$.\n",
    "\n",
    "Цель фальшивомонетчика прямо противоположна — максимизировать $D(G(z))$, то есть убедить банкира в том, что подделки настоящие.\n",
    "\n",
    "Продолжая аналогию, обучение генератора можно представить так: фальшивомонетчик не просто генерирует подделки наудачу. Он добывает прибор для распознавания подделок, разбирает его, смотрит, как тот работает, и затем создает подделки, которые смогут обмануть этот прибор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математически это **[игра](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%B8%D0%B3%D1%80) двух игроков**:\n",
    "\n",
    "$$\\large \\min\\limits_{\\theta_g}  \\max\\limits_{\\theta_d} [\\mathbb{E}_{x _\\sim p(data)} log(D_{\\theta_d}(x)]+\\mathbb{E}_{z _\\sim p(z)}\n",
    "[log(1-D_{\\theta_d}(G_{\\theta_g}(z))]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дискриминатор**\n",
    "- обучается при **фиксированном генераторе** ${G}_{\\theta_{g}}$,\n",
    "- **максимизирует** функцию выше относительно $\\theta_d$ (**градиентный подъем**),\n",
    "- решает задачу **бинарной классификации**: старается присвоить $1$ точкам данных из обучающего набора $E_{x∼p_{data}}$ и 0 сгенерированным выборкам $E_{z∼p(z)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Генератор**\n",
    "- обучается при **фиксированном дискриминаторе** $D_{θ_d}$,\n",
    "- получает градиенты весов за счет backpropagation через дискриминатор,\n",
    "- **минимизирует** функцию выше относительно $\\theta_d$ (**градиентный спуск**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посредством **чередования** градиентного **подъема** и **спуска** сеть можно обучить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **подъем** на **дискриминаторе**:\n",
    "\n",
    "\n",
    "$$\\large \\max\\limits_{\\theta_d} [\\mathbb{E}_{x _\\tilde{}p(data)} log(D_{\\theta_d}(x)+\\mathbb{E}_{z _\\tilde{}p(z)} log(1-D_{\\theta_d}(G_{\\theta_g}(z)))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **спуск** на **генераторе**:\n",
    "\n",
    "\n",
    "$$\\large \\min\\limits_{\\theta_g} \\mathbb{E}_{z _\\tilde{}p(z)} log(1-D_{\\theta_d}(G_{\\theta_d}(z)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **спуск** на **генераторе** эквивалентен градиентному **подъему**\n",
    "\n",
    "$$\\large \\max\\limits_{\\theta_g} \\mathbb{E}_{z _\\tilde{}p(z)} log(D_{\\theta_d}(G_{\\theta_d}(z)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе совместного конкурентного обучения, если система достаточно сбалансирована, достигается **минимаксное состояние равновесия**, в котором обе сети эффективно учатся.\n",
    "\n",
    "Сгенерированные удачно обученной нейросетью изображения практически неотличимы от настоящих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хорошенько подумать, то можно прийти к выводу, что **loss function** в **GAN** — это не какая-то функция, заданная людьми, а еще одна **нейросеть**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества GAN**\n",
    "* Теоретические **гарантии сходимости**\n",
    "* Можно обучать обычным **SGD/Adam**\n",
    "* Решает в явном виде задачу **generative modeling**, но неявным образом (**нейросети**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Недостатки GAN**\n",
    "* **Нестабильное обучение**\n",
    "* Очень **долгая сходимость**\n",
    "* **Mode-collapsing** (модель выдает одно и то же изображение или один и тот же класс и т.д., независимо от того, какие входные данные ей подаются)\n",
    "* **Исчезновение градиента**: дискриминатор настолько хорошо научился отличать сгенерированные образцы от реальных, что градиент весов генератора становится равным 0: в какую сторону бы генератор не изменил свои веса, дискриминатор все равно идеально распознает фальшивки\n",
    "* Поиск оптимальных параметров — **pure luck**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Практический пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим наши **генератор** и **дискриминатор**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_space, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "        )  # x,y\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),  # real/fake\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим **входные параметры**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10  # latent space\n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что у нас так же, как и в первом примере, есть переменная **latent space**. Это тот шум, из которого мы будем генерировать наши точки. Закон сохранения масс в действии — нельзя создать что-то из ничего!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим все необходимое для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "criterion = nn.BCELoss()  # Loss\n",
    "gen = Generator(latent_space=latent_dim, hidden_dim=50).to(device)\n",
    "disc = Discriminator(hidden_dim=50).to(device)\n",
    "\n",
    "# 2 optimizers for Discriminator and Generator\n",
    "optimizerD = torch.optim.Adam(disc.parameters(), lr=3e-4)\n",
    "optimizerG = torch.optim.Adam(gen.parameters(), lr=3e-4)\n",
    "\n",
    "# Fix noise to compare\n",
    "fixed_noise = torch.randn(128, latent_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы используем `BCELoss` (**Binary Cross Entropy**). Давайте разберемся почему:\n",
    "\n",
    "- **Дискриминатор** решает задачу **бинарной классификации**. Для этой задачи хорошо подходит **BCE**.\n",
    "- Требование к генератору может быть сформулировано как \"объектам, сгенерированным **генератором**, **дискриминатором** должна быть присвоена **высокая вероятность**\". Для **\"идеального\" генератора**, который всегда генерирует фотореалистичные результаты, значения **$D(G(z))$** всегда должны быть **близки к 1**. Для этой задачи хорошо подходит **BCE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которая создает точки на параболе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x * x\n",
    "    return torch.tensor(\n",
    "        np.hstack((x.reshape(-1, 1), y.reshape(-1, 1))), dtype=torch.float\n",
    "    )  # Create num of correct dots(x,y) on parabola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что сейчас будет происходить? $$$$\n",
    "\n",
    "* Обучение дискриминатора\n",
    "    * обнулим градиенты **дискриминатора**\n",
    "    * real точки\n",
    "        * создадим набор **real точек**, которые лежат на параболе\n",
    "        * посчитаем значение функции потерь дискриминатора на **real точках** и **real метках** $\\text{loss D}_\\text{real}$\n",
    "        * посчитаем градиенты для **дискриминатора**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/gan_training_algorithm_1.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Обучение дискриминатора (продолжение)\n",
    " * fake точки\n",
    "   * сгенерируем случайный шум $z$\n",
    "   * возьмем наш не обученный **генератор** и создадим с его помощью **fake точки** из $z$\n",
    "   * посчитаем значение функции потерь дискриминатора на **fake точках** и **fake метках** $\\text{loss D}_\\text{fake}$\n",
    "   * посчитаем градиенты для **дискриминатора** (они сложатся с уже посчитанными ранее)\n",
    " * обновление весов\n",
    "   * сделаем шаг обучения **дискриминатора** (обновим его веса)\n",
    "   * **генератор** не обучается\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/gan_training_algorithm_2.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Обучение генератора\n",
    " * обнулим градиенты **генератора**\n",
    " * сгенерируем случайный шум $z$\n",
    " * создадим с помощью **генератора** набор **fake точек** из $z$\n",
    " * посчитаем значение функции потерь дискриминатора на **fake точках** и **real метках** $\\text{loss G}$ (подмена меток)\n",
    " * посчитаем градиенты для **генератора**\n",
    " * сделаем шаг обучения **генератора** (обновим его веса)\n",
    " * **дискриминатор** не обучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/gan_training_algorithm_3.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Main Training Loop\n",
    "print(\"Training...\")\n",
    "print(device)\n",
    "\n",
    "x = []\n",
    "y_D = []\n",
    "y_G = []\n",
    "for epoch in range(num_epochs):\n",
    "    # max log(D(x)) + log(1 - D(G(z)))\n",
    "    # train on real points\n",
    "    disc.zero_grad()\n",
    "\n",
    "    # Define real points\n",
    "    real_points = gen_pair(num=batch_size).to(device)\n",
    "    label = torch.full(\n",
    "        (batch_size,), real_label, dtype=torch.float, device=device\n",
    "    ).view(-1)\n",
    "\n",
    "    # Train disc on real_points\n",
    "    output = disc(real_points).view(-1)\n",
    "    errD_real = criterion(output, label)\n",
    "    errD_real.backward()\n",
    "\n",
    "    # Define fake points\n",
    "    # This dots generated by generator transform from latent space\n",
    "    noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "    fake_points = gen(noise)\n",
    "    label.fill_(fake_label)\n",
    "\n",
    "    # Train disc on fake_points\n",
    "    output = disc(fake_points.detach()).view(-1)\n",
    "    errD_fake = criterion(output, label)\n",
    "    errD_fake.backward()\n",
    "\n",
    "    # Discriminator loss(real+fake)\n",
    "    errD = errD_real + errD_fake\n",
    "\n",
    "    optimizerD.step()\n",
    "\n",
    "    # max log(D(G(z)))\n",
    "    # Now, train generator\n",
    "    gen.zero_grad()\n",
    "\n",
    "    # Let's tell the discriminator that our generator creates real points\n",
    "    label.fill_(real_label)\n",
    "\n",
    "    output = disc(fake_points).view(-1)\n",
    "\n",
    "    errG = criterion(output, label)\n",
    "\n",
    "    errG.backward()\n",
    "\n",
    "    optimizerG.step()\n",
    "\n",
    "    # Plotting every N epoch\n",
    "    x.append(epoch)\n",
    "    y_D.append(errD.item() / 2)\n",
    "    y_G.append(errG.item())\n",
    "\n",
    "    if epoch % 250 == 0:\n",
    "        fig, ax = plt.subplots(nrows=2, figsize=(9, 6))\n",
    "        ax[0].plot(x, y_D, color=\"red\", lw=1, label=\"D\")\n",
    "        ax[0].plot(x, y_G, color=\"green\", lw=1, label=\"G\")\n",
    "\n",
    "        # Generates dots from fixed_noise\n",
    "        fake_points = gen(fixed_noise)\n",
    "        ax[1].scatter(\n",
    "            fake_points.detach().to(\"cpu\")[:, 0],\n",
    "            fake_points.detach().to(\"cpu\")[:, 1],\n",
    "            color=\"green\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax[1].scatter(\n",
    "            real_points.detach().to(\"cpu\")[:, 0],\n",
    "            real_points.detach().to(\"cpu\")[:, 1],\n",
    "            color=\"red\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax[1].set_xlim(-1, 1)\n",
    "        ax[1].set_ylim(0, 1)\n",
    "        clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Класс!** У нас получилось (если вдруг не сошлось за 10000 эпох, перезапустите заново, к сожалению, **фиксация seed еще не гарантирует стабильность GAN**). Особенно круто смотреть, как красиво loss **дискриминатора** и **генератора** сходятся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN — Генерация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью **GAN** можно, разумеется, генерировать не только точки на параболе. Можно генерировать, например, изображения. Но появляются закономерные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как из шума на входе сети получить изображение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым простым ответом будет: взять шум, пропустить его через **полносвязные слои** и сделать **reshape** до нужного разрешения. В целом, это будет работать.\n",
    "\n",
    "\n",
    "Однако **DCGAN (Deep Convolutional GAN)** использует **сверточные** и **сверточно-транспонированные** (*convolutional* и *convolutional-transpose*) слои в дискриминаторе и генераторе соответственно. Впервые метод **DCGAN** был описан в статье [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford et al., 2015)](https://arxiv.org/abs/1511.06434)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/deep_convolutional_gan_scheme.png\" width=\"700\"></center>\n",
    "<center><em>Схема работы DCGAN (Radford et al., 2015).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже видна разница в генерации при помощи исключительно **полносвязных слоёв** и при помощи **обратных свёрток**. Очевидно, результат **DCGAN** лучше, чем **GAN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/gan_dcgan_mnist_examples.png\" width=\"600\"></center>\n",
    "<center><em>Сравнение результатов на MNIST (Radford et al., 2015)</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1511.06434.pdf\">\tUnsupervised representation learning with deep convolutional generative adversarial networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход генератора подают шум для создания разнообразных объектов. Этот шум представляет собой вектор в многомерном пространстве. Один вектор — один сгенерированный объект. Задача дискриминатора — преобразовать вектор в изображение.\n",
    "\n",
    "Такое преобразование возможно при помощи транспонированных сверточных (convolution-transpose, иногда называют fractionally strided convolution) слоев. Как и обычные сверточные слои, эти слои используют сверточные ядра, но перед вычислением сверток они увеличивают размер исходного изображения, \"раздвигая\" пиксели и заполняя образующиеся промежутки между пикселями нулями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/dcgan_architecture.png\" width=\"700\"></center>\n",
    "<center><em>Зеркальная архитектура DCGAN </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте кратко вспомним, что делают TC слои.\n",
    "\n",
    "Transposed convolution проходит по всем пикселям входа и умножает их на обучаемое ядро свертки. При этом каждый одиночный пиксель превращается в фрагмент. Там, где фрагменты накладываются друг на друга, значения попиксельно суммируются.\n",
    "\n",
    "Используя Transposed convolution с параметром `stride = 2`, можно повышать размер карты признаков приблизительно в два раза, добавляя на нее мелкие детали."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/transposed_convolution_explained.png\" width=\"1024\"></center>\n",
    "\n",
    "<center><em>Transposed convolution</em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10)) * 255  # one 3-channel image with 10x10 size\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convT = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "y = convT(x)\n",
    "print(y.shape)  # One 3-chanells image with 12x12 size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученное изображение не похоже на входное, потому что были применены свёрточные ядра со случайными коэффициентами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "ax[0].imshow(x[0].permute(1, 2, 0).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow(y[0].permute(1, 2, 0).detach().numpy().astype(np.uint8))\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"After ConvTranspose\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Другие способы повышения разрешения — Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо **обратных свёрток** существуют другие методы повышения разрешения из низкой размерности.\n",
    "\n",
    "Самый простой способ — выполнить повышение разрешения с помощью **интерполяции**. Давайте вспомним, что в PyTorch это осуществляется слоем [Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10))  # one 3-channal image with 10x10 size\n",
    "print(\"Input shape:\", x.shape)\n",
    "\n",
    "upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "y = upsample(x)\n",
    "\n",
    "print(\"Output shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "ax[0].imshow((x[0].permute(1, 2, 0) * 256).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow((y[0].permute(1, 2, 0).detach().numpy() * 256).astype(np.uint8))\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].set_title(\"After Upsample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обученного DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на пример обученного **DCGAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "model = torch.hub.load(\n",
    "    \"facebookresearch/pytorch_GAN_zoo:hub\", \"DCGAN\", pretrained=True, useGPU=use_gpu\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "num_images = 16\n",
    "noise, _ = model.buildNoiseData(num_images)\n",
    "with torch.no_grad():\n",
    "    generated_images = model.test(noise)\n",
    "    generated_images = (\n",
    "        generated_images.clamp(-1, 1) + 1\n",
    "    ) / 2.0  # normalization to 0..1 range\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16 * 3, 2 * 3))\n",
    "ax.imshow(\n",
    "    torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy(),\n",
    "    interpolation=\"nearest\",\n",
    "    aspect=\"equal\",\n",
    ")\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практический пример DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем сами написать свой **DCGAN** и обучить его на датасете **FashionMNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2  # Num of epochs\n",
    "batch_size = 64  # batch size\n",
    "lr = 2e-4  # Learning rate\n",
    "b1 = 0.5  # Adam: decay of first order momentum of gradient\n",
    "b2 = 0.999  # Adam: decay of first order momentum of gradient\n",
    "num_cpu = 8  # Num of cpu threads to generate batch\n",
    "latent_dim = 100  # latent space\n",
    "img_size = 32  # images size\n",
    "channels = 1  # Num of channels\n",
    "sample_interval = 450  # interval between image sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно мы **инициализируем веса** случайным образом, но ничто не мешает нам инициализировать их так, как мы хотим. В [оригинальной статье](https://arxiv.org/pdf/1511.06434.pdf) про **DCGAN** предложено инициализировать веса нормальным распределением с центром в нуле и стандартным отклонением 0,02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, как преобразуется **шум** в **генераторе**:\n",
    "* Сначала с помощью **полносвязного слоя** он преобразуется в **первичные фичи**\n",
    "* Потом с помощью функции **view** **ресэмплится** в картинку низкого разрешения\n",
    "* Потом при прохождении через **conv_blocks** к нему поочерёдно применяются **Upsample** и **ОБЫЧНЫЕ** свёртки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size**2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [\n",
    "                nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "            ]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2**4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size**2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Initialize weight\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для отображения изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "def show_gen_img(model, latent_dim=100):\n",
    "    z = Tensor(np.random.normal(0, 1, (9, latent_dim)))  # define latent dim\n",
    "\n",
    "    # Generate noise from latent dim\n",
    "    sample_images = generator(z)\n",
    "    sample_images = sample_images.cpu().detach()\n",
    "\n",
    "    # Plotting images\n",
    "    grid = (\n",
    "        make_grid(sample_images, nrow=3, ncols=3, normalize=True)\n",
    "        .permute(1, 2, 0)\n",
    "        .numpy()\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(grid)\n",
    "    plt.axis(\"off\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим данные и загрузим и их в Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "        # Adversarial ground truths\n",
    "        valid = Tensor(imgs.shape[0], 1).fill_(1.0)\n",
    "        fake = Tensor(imgs.shape[0], 1).fill_(0.0)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(Tensor)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        batches_done = epoch * len(data_loader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, num_epochs, i, len(data_loader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "            fig = show_gen_img(generator)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы картинки обрели приличный вид, хватает 2 эпох. Чтобы стали выглядеть хорошо — 5 эпох."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cGAN — GAN с условием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cGAN** расшифровывается как **Conditional Generative Adversarial Net** — это **GAN** с условием. Условие может быть любым, например, генерация конкретной цифры. В этом случае нам нужен уже размеченный датасет для того, чтобы обучить дискриминатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/conditional_gan_scheme.png\" width=\"800\"></center>\n",
    "<center><em>Схема работы cGAN. Label Y добавляется к случайному шуму, тем самым мы говорим генератору генерировать случайное изображение нужного класса. Также он подаётся в дискриминатор в качестве входа, чтобы дискриминатор знал, какое изображение классифицировать как реальное, а какое — как вымышленное.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение в данном случае будет аналогичным обучению **GAN**: мы будем обучать сети, чередуя реальные данные и сгенерированные, добавив `label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/cGANS_results_20_and_50_epochs_mnist.png\" width=\"700\"></center>\n",
    "<center><em>Сравнение результатов cGAN и cDCGAN</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN\">PyTorch implementation of conditional Generative Adversarial Networks</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как закодировать метки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку подавать в сеть числа от 0 до 9 (в случае **MNSIT**) нет смысла, то нужно придумать, как подавать их в нейронную сеть. На помощь приходят **Embeddings**. Мы можем представить каждую метку в виде вектора с десятью элементами.\n",
    "\n",
    "[Документация nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = next(iter(data_loader))\n",
    "\n",
    "label_emb = nn.Embedding(10, 10)\n",
    "\n",
    "e = label_emb(labels)\n",
    "\n",
    "print(f\"Label: {labels[0]}\")\n",
    "print(f\"Embedding for this label: {e[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого **эмбеддинги** меток обычно склеиваются с входами сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Почему нельзя подать просто число?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы будем подавать просто число, например 0.1 для единицы и 0.5 для пяти, то вход у нас будет непрерывным, что довольно нелогично: тогда при небольшом изменении входа мы будем генерировать другую цифру. А также сети будет сложнее выучить небольшие расхождения в этом небольшом интервале. В случае с векторным представлением мы избегаем этих проблем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модификации cGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метки классов можно подавать не только способом, описанным выше. Можно вместо подачи их в дискриминатор сделать так, чтобы он их предсказывал — **Semi-Supervised GAN**.\n",
    "\n",
    "Или же не подавать label в дискриминатор, но ждать от него классификации в соответствии с классом, который мы хотим получить от генератора — это **InfoGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна модификация cGAN — это **AC-GAN** (auxiliary classifier), в которой единственное различие заключается в том, что дискриминатор должен помимо распознавания реальных и фейковых изображений ещё и классифицировать их. Он имеет эффект стабилизации процесса обучения и позволяет генерировать большие высококачественные изображения, изучая представление в скрытом пространстве, которое не зависит от метки класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/gans_zoo_schemes.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тонкости обучения GANов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья — детальный разбор тонкностей и советов](https://beckham.nz/2021/06/28/training-gans.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частые/простые ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Убедитесь, что сгенерированые сэмплы находятся в том же диапазоне, что и реальные данные.** Например, реальные данные `[-1,1]`, при этом генерируются данные `[0,1]`. Это нехорошо, так как это подсказка для дискриминатора.\n",
    "* **Убедитесь, что сгенерированные сэмплы того же размера, что и реальные данные.** Например, размер картинок в MNIST `(28,28)`, а генератор выдает `(32,32)`. В таком случае нужно либо изменить архитектуру генератора, чтобы получать на выходе размер `(28,28)`, либо сделать ресайз реальных данных до `(32,32)`.\n",
    "* **Старайтесь не использовать `BatchNorm`**. Проблема `BN` в том, что во время обучения его внутренняя статистика считается по минибатчу, а во время инференса она вычисляется как *moving average*, что в свою очередь может повлечь непредсказуемые результаты. Если архитектура GAN предполагает нормализацию, то лучше использовать **`InstanceNorm`**.\n",
    "* **Визуализируйте свои лоссы в процессе обучения**. Для этого существует множество прекрасных библиотек (например, TensorBoard). Следить за бегущими по экрану цифрами от двух соревнующихся между собой лоссов бессмысленно.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Зачем давать преимущество дискриминатору"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ваша версия **GAN** работает не так хорошо, как вам хотелось бы, попробуйте дать своему **дискриминатору** преимущество, обучив его на относительно большее число итераций, чем **генератор**. Другими словами, чем лучше **дискриминатор** различает настоящие и фальшивые данные, тем лучше сигнал, который **генератор** может извлечь из него. Обратите внимание, что эта логика не имела смысла во времена \"до WGAN\", поскольку слишком хорошая работа дискриминатора вредила обучению.\n",
    "\n",
    "Например:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def train_on_batch(x, iter_, n_gen=5):\n",
    "     Generator:\n",
    "    ...\n",
    "    ...\n",
    "    if iter_ % n_gen == 0:\n",
    "        g_loss.backward()\n",
    "        opt_g.step()\n",
    "\n",
    "     Discriminator:\n",
    "    ...\n",
    "    ...\n",
    "    d_loss.backward()\n",
    "    d_loss.step()\n",
    "```\n",
    "\n",
    "Где `iter_` — текущая итерация шага градиента, а `n_gen` определяет интервал между обновлениями генератора. В данном случае, поскольку он равен 5, мы можем считать, что это означает, что дискриминатор обновляется в 5 раз чаще, чем генератор.\n",
    "\n",
    "Естественно, работает не всегда и не везде. Но попробовать стоит\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование оптимизатора ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что почти во всех статьях по **GAN** используется **ADAM**. Сложно сказать, почему так получается, но он работает, и работает очень хорошо. Если качество вашего **GAN** оставляет желать лучшего, скорее всего оптимизатор тут не при чем. Ищите ошибку где-то еще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `epsilon` **ADAM** по умолчанию в PyTorch равен `1e-8`, что может вызвать проблемы после длительного периода обучения, например, значения функции потерь периодически взрываются или увеличиваются. Подробнее об этом на [StackOverflow](https://stackoverflow.com/questions/42327543/adam-optimizer-goes-haywire-after-200k-batches-training-loss-grows) и в комментарии на [Reddit](https://www.reddit.com/r/reinforcementlearning/comments/j9rflf/intuitive_explanation_for_adams_epsilon_parameter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Диффузионные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая [публикация](https://arxiv.org/abs/2006.11239) по теме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прямой диффузный процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы имеем некоторый объект $\\mathbf{x}_0$ из заданного имеющимся датасетом распределения $\\mathbf{x}_0 \\sim q(\\mathbf{x})$. Определим так называемый прямой диффузный процесс, в ходе которого мы будем последовательно добавлять небольшое количество Гауссового шума последовательно $T$ раз, создавая из нашего объекта последовательность $\\mathbf{x}_1, \\dots, \\mathbf{x}_T$ постепенно всё более зашумленных версий нашего исходного объекта. Параметры добавляемого шума зависят от номера шага зашумления и их набор $\\{\\beta_t \\in (0, 1)\\}_{t=1}^T$ задан заранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/diffusion_process.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходное распределение данных из датасета будет последовательно преобразовано в Гауссов шум:\n",
    "\n",
    "$$\\large q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}, \\beta_t\\mathbf{I}) \\quad\n",
    "q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важным свойством описанного выше процесса является то, что он допускает аналитическое вычисление шума на любом из шагов процесса в явном виде. Это связано с тем, что сумма нескольких нормально распределенных случайных величин также является случайной величиной с нормальным распределением. Введём обозначения из оригинальной статьи $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$ и $\\alpha_t = 1 - \\beta_t$. Тогда:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large\n",
    "\\begin{aligned}\n",
    "\\mathbf{x}_t\n",
    "&= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} \\\\\n",
    "&= \\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\boldsymbol{\\epsilon}}_{t-2} \\\\\n",
    "&= \\dots \\\\\n",
    "&= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon} \\\\\n",
    "\\end{aligned} $$\n",
    "\n",
    "$$ \\large\n",
    "\\begin{aligned}\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_0) &= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\n",
    "\\end{aligned}; $$\n",
    "\n",
    "$\\large \\text{где } \\boldsymbol{\\epsilon}_{t-1}, \\boldsymbol{\\epsilon}_{t-2}, \\dots \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}),$\n",
    "\n",
    "$\\large \\qquad \\bar{\\boldsymbol{\\epsilon}}_{t-2} \\text{ – новая нормально распределенная величина (*)}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*)  Напомним, что когда мы складываем две нормально распределенные величины с разной дисперсией и нулевым средним $\\mathcal{N}(\\mathbf{0}, \\sigma_1^2\\mathbf{I})$ и $\\mathcal{N}(\\mathbf{0}, \\sigma_2^2\\mathbf{I})$, то получаем новую нормально распределенную случайную величину $\\mathcal{N}(\\mathbf{0}, (\\sigma_1^2 + \\sigma_2^2)\\mathbf{I})$, что в нашей параметризации означает $\\sqrt{(1 - \\alpha_t) + \\alpha_t (1-\\alpha_{t-1})} = \\sqrt{1 - \\alpha_t\\alpha_{t-1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обратный диффузный процесс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large p_\\theta(\\mathbf{x}_{0:T}) = p(\\mathbf{x}_T) \\prod^T_{t=1} p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) \\quad\n",
    "p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/diffusion_example.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models\">Lilian Weng: What are Diffusion Models?</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/resnet_block.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, mid_features=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_features:\n",
    "            mid_features = out_features\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_features, mid_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_features),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_features, out_features, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.conv_stack(x))\n",
    "        else:\n",
    "            return self.conv_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блок уменьшения размера:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/down_block.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Блок увеличения размера:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/upblock.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ResizeBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, emb_dim):\n",
    "        super().__init__()\n",
    "        # defines non-linear map from time embedding features to conv features\n",
    "        self.emb_projection = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb_dim, out_features),\n",
    "        )\n",
    "\n",
    "    def add_emb(self, x, t_vector):\n",
    "        # [batch_size, time_embedding_dim] -> [batch_size, out_features]\n",
    "        emb = self.emb_projection(t_vector)\n",
    "        # [batch_size, out_features] - > [batch_size, out_features, H, W]\n",
    "        emb = emb[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Down(ResizeBlock):\n",
    "    def __init__(self, in_features, out_features, emb_dim=256):\n",
    "        super().__init__(in_features, out_features, emb_dim)\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            ResNetBlock(in_features, in_features, residual=True),\n",
    "            ResNetBlock(in_features, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        x = self.add_emb(x, t)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(ResizeBlock):\n",
    "    def __init__(self, in_features, out_features, emb_dim=256):\n",
    "        super().__init__(in_features, out_features, emb_dim)\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            ResNetBlock(in_features, in_features, residual=True),\n",
    "            ResNetBlock(in_features, out_features, in_features // 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.add_emb(x, t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L14/out/se_block.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, num_heads=4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, C, H, W] -> [batch_size, H*W, C]\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        # K, Q, V in Self attention are equal\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        # skip connection\n",
    "        attention_value = attention_value + x\n",
    "        # simple multilayer perceptron and second skip connection\n",
    "        attention_value = self.mlp(attention_value) + attention_value\n",
    "        # back to [batch_size, C, H, W]\n",
    "        return attention_value.swapaxes(2, 1).view(\n",
    "            -1, self.channels, self.size, self.size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Denoising U-Net model implementation based on arXiv:2006.11239 [cs.LG]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels=3, img_size=64, time_enbed_dim=256):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.time_dim = time_enbed_dim\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Downsample and enlarge feature dim\n",
    "        self.inc = ResNetBlock(num_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128, img_size // 2)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256, img_size // 4)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256, img_size // 8)\n",
    "\n",
    "        # Keep spatial dim constant\n",
    "        self.conv_bottleneck = nn.Sequential(\n",
    "            ResNetBlock(256, 512),\n",
    "            SelfAttention(512, img_size // 8),\n",
    "            ResNetBlock(512, 256),\n",
    "        )\n",
    "\n",
    "        # Upsample and reduce feature dim\n",
    "        # 512=256+256 from conv_bottleneck and sa3\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128, img_size // 4)\n",
    "        # 256=128+128 from sa4 and sa2\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64, img_size // 2)\n",
    "        # 128=64+64 from sa5 and sa1\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.sa6 = SelfAttention(64, img_size)\n",
    "        self.outc = nn.Conv2d(64, num_channels, kernel_size=1)\n",
    "\n",
    "    def pos_encoding(self, t):\n",
    "        r\"\"\"\n",
    "        Returns embedding vector for given integer time index.\n",
    "\n",
    "        We adopt 1d Positional Encoding form arXiv:1706.03762 [cs.CL]\n",
    "        see 3.5 for more details.\n",
    "\n",
    "        PE(x,2i) = sin(x/10000^(2i/D))\n",
    "        PE(x,2i+1) = cos(x/10000^(2i/D))\n",
    "\n",
    "        Where:\n",
    "        x is a point in 1d space\n",
    "        i is an integer in [0, D/2), where D is the size of the feature dimension\n",
    "\n",
    "        Args:\n",
    "            t: Tensor, shape ``[batch_size, 1]``\n",
    "        Returns:\n",
    "            pe: Tensor, shape ``[batch_size, time_embedding_dim]``\n",
    "        \"\"\"\n",
    "\n",
    "        # placeholder for diffusion time encoding vector\n",
    "        pe = torch.zeros(t.shape[0], self.time_dim).to(t)\n",
    "\n",
    "        # factor 1/10000^(2i/D)\n",
    "        div_factors = torch.exp(\n",
    "            torch.arange(0, self.time_dim, 2)\n",
    "            * (-torch.log(torch.as_tensor(10000.0)) / self.time_dim)\n",
    "        ).to(t)\n",
    "\n",
    "        # repeat t index for each feature\n",
    "        x = t.repeat(1, self.time_dim // 2)\n",
    "\n",
    "        # sin(x/10000^(2i/D)) for even features\n",
    "        pe[:, 0::2] = torch.sin(x * div_factors)\n",
    "        # cos(x/10000^(2i/D)) for odd features\n",
    "        pe[:, 1::2] = torch.cos(x * div_factors)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float).to(x)\n",
    "        t = self.pos_encoding(t)\n",
    "\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "\n",
    "        x4 = self.conv_bottleneck(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация прямого и обратного диффузного процесса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "class DiffusionGenerativeModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        model_name=\"unconditioned_baseline\",\n",
    "        num_noise_steps=1000,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        img_size=64,\n",
    "        denoising_model_class=UNet,\n",
    "        batch_size=2,\n",
    "        lr=0.001,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.num_noise_steps = num_noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "\n",
    "        # diffusion process linear noise schedule\n",
    "        self.beta = self._get_noise_schedule().to(self.device)\n",
    "        self.alpha = 1.0 - self.beta\n",
    "        # \\hat{\\alpha}_{i-1} = \\prod_{j=0}^{i-1} \\alpha_j\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        # init denoising model\n",
    "        self.denoising_model = denoising_model_class(img_size=img_size).to(self.device)\n",
    "\n",
    "        # init dataset and dataloader\n",
    "        self.dataset = dataset\n",
    "        self.dl = torch.utils.data.DataLoader(\n",
    "            self.dataset, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "\n",
    "        # init optimizer and loss for training\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = AdamW(self.denoising_model.parameters(), lr=self.lr)\n",
    "\n",
    "    def _load_pretrained_model(self, path=None):\n",
    "        if path is not None:\n",
    "            self.denoising_model.load_state_dict(\n",
    "                torch.load(path, map_location=self.device)\n",
    "            )\n",
    "        else:\n",
    "            self.denoising_model.load_state_dict(\n",
    "                torch.load(f\"{self.model_name}.pt\", map_location=self.device)\n",
    "            )\n",
    "\n",
    "    def _get_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.num_noise_steps)\n",
    "\n",
    "    def _noise_images_batch(self, x, t):\n",
    "        # \\mu_i = \\sqrt{\\hat{\\alpha}_i}\n",
    "        mu = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        # \\sigma_i = \\sqrt{1 - \\hat{\\alpha}_i}\n",
    "        sigma = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "\n",
    "        standard_normal_noise = torch.randn_like(x)\n",
    "        noised_image_batch = mu * x + sigma * standard_normal_noise\n",
    "        return noised_image_batch, standard_normal_noise\n",
    "\n",
    "    def _get_timestams_batch(self):\n",
    "        return torch.randint(low=1, high=self.num_noise_steps, size=(self.batch_size,))\n",
    "\n",
    "    def generate_images_batch(self, use_best_model=False):\n",
    "        if use_best_model:\n",
    "            self.denoising_model.load_state_dict(\n",
    "                torch.load(f\"{self.model_name}.pt\", map_location=self.device)\n",
    "            )\n",
    "\n",
    "        # stop tracking normalisation stats\n",
    "        self.denoising_model.eval()\n",
    "\n",
    "        # avoid training on generated samples\n",
    "        with torch.no_grad():\n",
    "            # start from pure noise batch\n",
    "            x = torch.randn((self.batch_size, 3, self.img_size, self.img_size)).to(\n",
    "                self.device\n",
    "            )\n",
    "            # and apply self.num_noise_steps denoising steps with model\n",
    "            for t_i in tqdm(reversed(range(1, self.num_noise_steps)), position=0):\n",
    "                # Build tensor with timestamp index. Same for each element in batch\n",
    "                t = torch.full((self.batch_size,), t_i).long().to(self.device)\n",
    "\n",
    "                # predict noise on current timestamp\n",
    "                pred_noise = self.denoising_model(x, t)\n",
    "\n",
    "                # restore noise parametrs on current timestamp\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "\n",
    "                # partialy denoise batch of images\n",
    "                x = x - (1.0 - alpha) / (1 - alpha_hat).sqrt() * pred_noise\n",
    "                x = (1 / alpha.sqrt()) * x\n",
    "\n",
    "                # add appropriate amount of noise for next step if any\n",
    "                if t_i > 0:\n",
    "                    z = torch.randn_like(x).to(self.device)\n",
    "                    x = x + beta.sqrt() * z\n",
    "\n",
    "        # clip x to valid 0..255 image range\n",
    "        x = x.clamp(0, 1)\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "\n",
    "        # reenable tracking normalisation stats\n",
    "        self.denoising_model.train()\n",
    "        return x\n",
    "\n",
    "    def _train_step(self, batch):\n",
    "        # unpack data\n",
    "        images, labels = batch\n",
    "        t = self._get_timestams_batch()\n",
    "\n",
    "        # send tensors to device\n",
    "        images = images.to(self.device)\n",
    "        t = t.to(self.device)\n",
    "\n",
    "        # prep batch of noised images\n",
    "        noised_images, target_noise = self._noise_images_batch(images, t)\n",
    "\n",
    "        # estimate noise with U-Net\n",
    "        predicted_noise = self.denoising_model(noised_images, t)\n",
    "\n",
    "        # optimize model to fit target noise\n",
    "        loss = self.criterion(predicted_noise, target_noise)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def fit(self, n_epochs=20, save_images=True):\n",
    "        best_loss = float(\"inf\")\n",
    "        for epoch in tqdm(range(n_epochs), position=1):\n",
    "            ep_loss = 0.0\n",
    "            with tqdm(total=len(self.dl), position=2) as progress_bar:\n",
    "                for batch in self.dl:\n",
    "                    loss_step = self._train_step(batch)\n",
    "                    # loss per single object\n",
    "                    loss_step = loss_step / len(self.dl) * self.batch_size\n",
    "                    ep_loss += loss_step\n",
    "\n",
    "                    progress_bar.update(1)\n",
    "            log_string = f\"Loss at epoch {epoch + 1}: {ep_loss:.3f}\"\n",
    "\n",
    "            if best_loss > ep_loss:\n",
    "                best_loss = ep_loss\n",
    "                torch.save(self.denoising_model.state_dict(), f\"{self.model_name}.pt\")\n",
    "                log_string += \" || Best model updated\"\n",
    "\n",
    "                # Save images generated at this epoch\n",
    "                if save_images:\n",
    "                    generated_images = self.generate_images_batch()\n",
    "                    image_grid = make_grid(generated_images)\n",
    "                    # convert to numpy\n",
    "                    ndarr = image_grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
    "                    # and save\n",
    "                    im = Image.fromarray(ndarr)\n",
    "                    im.save(f\"{self.model_name}_{epoch}.jpg\")\n",
    "\n",
    "            print(log_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение диффузных моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install astronn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context  # ignore url error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример использования опеределенной выше диффузной модели генерации на примере датасета Galaxy10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L14/galaxy10sdss_example.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Примеры каждого класса изображений из датасета Galaxy10</a></em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/henrysky/Galaxy10\">Galaxy10 DECals Dataset</a></em></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroNN.datasets import load_galaxy10\n",
    "\n",
    "\n",
    "images, labels = load_galaxy10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class GalaxyDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform):\n",
    "        super().__init__()\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        image = self.images[indx]\n",
    "        label = self.labels[indx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(64, antialias=True),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomVerticalFlip(0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = GalaxyDataset(images, labels, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm = DiffusionGenerativeModel(dataset, device=\"cuda\", batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение сети осуществляется следующим образом. Пропустим этот этап, так как процесс занимает продолжительное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddpm.fit(n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на изображения, сгенерированные необученной моделью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = ddpm.generate_images_batch()\n",
    "image_grid = make_grid(generated_images)\n",
    "# convert to numpy\n",
    "ndarr = image_grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
    "# and show\n",
    "Image.fromarray(ndarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что сгенерированные изображения представляют собой шум.\n",
    "\n",
    "Загрузим веса предобученной модели:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-content/L14/weights/unconditioned_baseline.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm._load_pretrained_model(\"unconditioned_baseline.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = ddpm.generate_images_batch()\n",
    "image_grid = make_grid(generated_images)\n",
    "# convert to numpy\n",
    "ndarr = image_grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
    "# and show\n",
    "Image.fromarray(ndarr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Использованная литература</font>\n",
    "\n",
    "<font size=\"5\">GAN</font>\n",
    "\n",
    "[Книга по генеративным сетям](https://habr.com/ru/company/piter/blog/504956/)\n",
    "\n",
    "[Generative Adversarial Networks (Goodfellow et al., 2014)](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "[Видео разбор оригинальной статьи GAN](https://youtu.be/eyxmSmjmNS0)\n",
    "\n",
    "[Видео лекции Иана Гудфеллоу](https://www.youtube.com/watch?v=HGYYEUSm-0Q)\n",
    "\n",
    "[Generative adversarial networks](https://deepgenerativemodels.github.io/notes/gan/)\n",
    "\n",
    "[Самые современные генеративные модели](https://paperswithcode.com/methods/category/generative-models)\n",
    "\n",
    "[exactly how the NVIDIA GauGAN neural network works](https://sudonull.com/post/29972-Pictures-from-rough-sketches-how-exactly-the-NVIDIA-GauGAN-neural-network-works-ITSumma-Blog)\n",
    "\n",
    "<font size=\"5\">DCGAN</font>\n",
    "\n",
    "[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford et al., 2015)](https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "[DCGAN TUTORIAL](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "\n",
    "<font size=\"5\">Wassserstein GAN</font>\n",
    "\n",
    "[Wasserstein GAN (Arjovsky et. al., 2017)](https://arxiv.org/abs/1701.07875)\n",
    "\n",
    "[Блог пост о Wasserstein GAN](https://vincentherrmann.github.io/blog/wasserstein/)\n",
    "\n",
    "[Improved Training of Wasserstein GANs (Gulrajani et al., 2017)](https://arxiv.org/abs/1704.00028)\n",
    "\n",
    "[Spectral Normalization for Generative Adversarial Networks (Miyato et al., 2018)](https://arxiv.org/abs/1802.05957).\n",
    "\n",
    "<font size=\"5\">ProGAN -> StyleGAN -> StyleGAN2 -> Alias-Free GAN</font>\n",
    "\n",
    "[Progressive Growing of GANs for Improved Quality, Stability, and Variation (ProGAN) [Karras et al., 2017]](https://arxiv.org/abs/1710.10196)\n",
    "\n",
    "[A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN) [Karras et al., 2018]](https://arxiv.org/abs/1812.04948)\n",
    "\n",
    "[Analyzing and Improving the Image Quality of StyleGAN (StyleGAN2) [Karras et al., 2019]](https://arxiv.org/abs/1912.04958)\n",
    "\n",
    "[Alias-Free Generative Adversarial Networks (Alias-Free GAN) [Karras et al., 2021]](https://arxiv.org/abs/2106.12423)\n",
    "\n",
    "<font size=\"5\">Тонкости обучения GAN</font>\n",
    "\n",
    "[Статья - детальный разбор тонкостей и советов](https://beckham.nz/2021/06/28/training-gans.html)\n",
    "\n",
    "[Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples (Sinha et al., 2020)](https://arxiv.org/abs/2002.06224)\n",
    "\n",
    "<font size=\"5\">GAN Zoo:</font>\n",
    "\n",
    "<font size=\"5\">BigGAN</font>\n",
    "\n",
    "[Large Scale GAN Training for High Fidelity Natural Image Synthesis (Brock et al., 2018)](https://arxiv.org/abs/1809.11096)\n",
    "\n",
    "<font size=\"5\">StackGAN</font>\n",
    "\n",
    "[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al., 2016)](https://arxiv.org/abs/1612.03242).\n",
    "\n",
    "Дополнительно: [репозиторий к статье с воспроизведением результатов](https://github.com/hanzhanggit/StackGAN).\n",
    "\n",
    "[StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al., 2017)](https://arxiv.org/abs/1710.10916)\n",
    "\n",
    "[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (Ledig et al., 2016)](https://arxiv.org/abs/1609.04802)\n",
    "\n",
    "[Deep Learning Generative Models for Image Synthesis and Image Translation](https://www.rulit.me/data/programs/resources/pdf/Generative-Adversarial-Networks-with-Python_RuLit_Me_610886.pdf)\n",
    "\n",
    "[youtube [StackGAN++] Realistic Image Synthesis with Stacked Generative Adversarial Networks | AISC](https://www.youtube.com/watch?v=PXWIaLE7_NU)\n",
    "\n",
    "[youtube Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://www.youtube.com/watch?v=crI5K4RCZws)\n",
    "\n",
    "<font size=\"5\">ControlGAN</font>\n",
    "\n",
    "[Controllable Generative Adversarial Network](https://arxiv.org/pdf/1708.00598.pdf)\n",
    "\n",
    "[Controllable Text-to-Image Generation](https://arxiv.org/pdf/1909.07083.pdf)\n",
    "\n",
    "[Image Generation and Recognition (Emotions)](https://arxiv.org/pdf/1910.05774.pdf)\n",
    "\n",
    "[Natural Language & Text-to-Image 2019](https://meta-guide.com/data/data-processing/text-to-image-systems/natural-language-text-to-image-2019)\n",
    "\n",
    "<font size=\"5\">AC-GAN</font>\n",
    "\n",
    "[How to Develop an Auxiliary Classifier GAN (AC-GAN) From Scratch with Keras](https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/)\n",
    "\n",
    "Conditional Image Synthesis with Auxiliary Classifier GANs: [статья](https://arxiv.org/pdf/1610.09585.pdf), [репозиторий](https://github.com/clvrai/ACGAN-PyTorch)\n",
    "\n",
    "[An Auxiliary Classifier Generative Adversarial Framework for Relation Extraction](https://arxiv.org/pdf/1909.05370.pdf)\n",
    "\n",
    "[A Multi-Class Hinge Loss for Conditional GANs](https://openaccess.thecvf.com/content/WACV2021/papers/Kavalerov_A_Multi-Class_Hinge_Loss_for_Conditional_GANs_WACV_2021_paper.pdf)\n",
    "\n",
    "<font size=\"5\">Domain Transfer Network</font>\n",
    "\n",
    "[Unsupervised Cross-Domain Image Generation (Taigma et al., 2016)](https://arxiv.org/abs/1611.02200)\n",
    "\n",
    "<font size=\"5\">Pix2Pix</font>\n",
    "\n",
    "[Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2016)](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "<font size=\"5\">Семантическая генерация</font>\n",
    "\n",
    "[Learning to Generate Chairs, Tables and Cars\n",
    "with Convolutional Networks (Dosovitskiy et al., 2017)](https://arxiv.org/abs/1411.5928)\n",
    "\n",
    "<font size=\"5\">Text to image</font>\n",
    "\n",
    "[Text-to-Image Generation with Attention Based Recurrent Neural Networks (Zia et al., 2020)](https://arxiv.org/abs/2001.06658)\n",
    "\n",
    "<font size=\"5\">Image-to-Image</font>\n",
    "\n",
    "[GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (Chong et al., 2021)](https://arxiv.org/abs/2106.06561)\n",
    "\n",
    "[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (Zhu et al., 2017)](https://arxiv.org/abs/1703.10593)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Ссылки</font>\n",
    "\n",
    "[GitHub MNIST CelebA cGAN cDCGAN](https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN)\n",
    "\n",
    "[GitHub Text-to-Photo realistic Image Synthesis with Stacked Generative Adversarial Networks](https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks)\n",
    "\n",
    "[GitHub ControlGAN](https://github.com/mrlibw/ControlGAN)\n",
    "\n",
    "[GitHub ControlGAN-Tensorflow](https://github.com/taki0112/ControlGAN-Tensorflow)\n",
    "\n",
    "[GitHub Keras-ACGan](https://github.com/lukedeo/keras-acgan)\n",
    "\n",
    "[Множество примеров различных генераторов](https://thisxdoesnotexist.com)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
