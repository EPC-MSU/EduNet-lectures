{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "L07_Batch_normalization_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nW14JAZZDjL7",
        "krgpPIXMDjL9",
        "Zcfy7Y7GDjMA",
        "YTqWE7d9DjMA",
        "P-cbNGAYDjMF",
        "Qh7OLrRLDjMF",
        "0vHrDTpIDjMG",
        "Bp05jlcdDjMG",
        "S78gLn_zDjMH",
        "giRXlNUkDjMK",
        "E0bqxngCDjMK",
        "SHpSvwm8DjMK"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JugzuzsYDjLs"
      },
      "source": [
        "# Улучшение сходимости нейросетей и борьба с переобучением."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYL0uPfjwdtH"
      },
      "source": [
        "##Блок вспомогательного кода\n",
        "\n",
        "В этом блоке определенна нейросеть и код для ее обучения который будет использоваться для демонстрации в ходе семинара. Так же он потребуется при выполнении практических заданий."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6mduu2PDjLv"
      },
      "source": [
        "### Подготовка Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cflc78wqMHx8"
      },
      "source": [
        "# Вспомогательный метод для запуска Tensorboard в Colab\n",
        "\n",
        "# Fix: https://stackoverflow.com/questions/60730544/tensorboard-colab-tensorflow-api-v1-io-gfile-has-no-attribute-get-filesystem\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Запуск Tensorboard в Colab\n",
        "def reinit_tensorboard(clear_log = True):\n",
        "  # Лог-файлы читаются из этого каталога: \n",
        "  logs_base_dir = \"runs\"\n",
        "  if clear_log:\n",
        "    # Очистка логов\n",
        "    #!rm -rfv {logs_base_dir}/*\n",
        "    shutil.rmtree(logs_base_dir, ignore_errors = True)\n",
        "    os.makedirs(logs_base_dir, exist_ok=True)\n",
        "  # Магия Colab\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJVyc2AtMQMt"
      },
      "source": [
        "#reinit_tensorboard(clear_log=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDfhGA8DjLw"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpy45cURxzJC"
      },
      "source": [
        "### Загрузка датасета MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJsSOQ9pJDIr"
      },
      "source": [
        "%%bash\n",
        "if test -f ./datasets/MNIST.tar.gz; then\n",
        "    echo \"Already downloaded\"; \n",
        "  else \n",
        "    wget -P ./datasets www.di.ens.fr/~lelarge/MNIST.tar.gz;\n",
        "    tar -zxvf ./datasets/MNIST.tar.gz -C ./datasets; \n",
        "fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4XemxDjDjLw"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))\n",
        "     ])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./datasets', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./datasets', train=False,\n",
        "                                       download=True, transform=transform)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYqEH68mx6SE"
      },
      "source": [
        "### Класс для создания полносвязанной сети\n",
        "\n",
        "Конструктор класса получает на воход два параметра количество слоев и функцию активации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xjmMIVPDjLx"
      },
      "source": [
        "class SimpleMNIST_NN(nn.Module):\n",
        "    def __init__(self, n_layers, activation=nn.Sigmoid):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.activation = activation()\n",
        "        layers = [nn.Linear(28 * 28, 100), self.activation] # first layer\n",
        "        for _ in range(0, n_layers - 1):\n",
        "            layers.append(nn.Linear(100, 100)) # All intermediate has the same size\n",
        "            layers.append(self.activation)\n",
        "        layers.append(nn.Linear(100, 10))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x, predict_proba=False):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.layers(x)\n",
        "        if predict_proba:\n",
        "          x = F.softmax(x)\n",
        "        return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTXH5RRwyX7g"
      },
      "source": [
        "### Вспомогательные методы для обучения.\n",
        "\n",
        "Используйте их при выполнении практической работы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B4vYf5JDjLy"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset,\n",
        "                                           batch_size=32,\n",
        "                                           shuffle=True, \n",
        "                                           num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset,\n",
        "                                         batch_size=32,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hs1k1F-NngL"
      },
      "source": [
        "class Steper():\n",
        "  def __init__(self):\n",
        "    self._global_step = 1\n",
        "  @property\n",
        "  def global_step(self):\n",
        "    return self._global_step\n",
        "\n",
        "  def step(self):\n",
        "    self._global_step += 1\n",
        "\n",
        "import os \n",
        "def get_summary_writer(name, runs_dir=\"runs/\"):\n",
        "    path = os.path.join(runs_dir, name)\n",
        "    return SummaryWriter(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZD7D-BJELjt"
      },
      "source": [
        "def train_epoch(model, \n",
        "                optimizer,\n",
        "                criterion, \n",
        "                train_loader, \n",
        "                model_name,\n",
        "                steper, \n",
        "                writer):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        X, y = batch\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, y)\n",
        "        writer.add_scalar(f\"{model_name}_train_loss\", \n",
        "                          loss.cpu().detach().numpy(),\n",
        "                          global_step= steper.global_step)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steper.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D60g-LI3EzMU"
      },
      "source": [
        "def validate(model, \n",
        "             validation_loader,\n",
        "             model_name,\n",
        "             steper,\n",
        "             writer):\n",
        "    cumloss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_loader:\n",
        "            X, y = batch\n",
        "            X, y = X.cuda(), y.cuda()\n",
        "            pred = model(X)\n",
        "            loss = criterion(pred, y)\n",
        "            writer.add_scalar(f\"{model_name}_test_loss\", \n",
        "                          loss.cpu().detach().numpy(),\n",
        "                          global_step= steper.global_step)\n",
        "            steper.step()\n",
        "            cumloss += loss\n",
        "    return cumloss / len(validation_loader)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAFgOQ8qDjL9"
      },
      "source": [
        "## Нормализация входных данных\n",
        "В 5й лекции мы убедились что нормализация данных оказывает заметное влияние на сходимость модели. Давайте разберемся почему так происходит.\n",
        "\n",
        "\n",
        "1. Масштаб. Для табличных данных это было очевидно. И хотя пикселы изображения ближе по абсолютной величине друг к другу для них масштабирование тоже может быть актуально.\n",
        "\n",
        "2. Стабильность. Малые изменения весов меньше влияют на результат. \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gan/norm_stab.png\" width=\"700\">\n",
        "\n",
        "3. Скорость обучения\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gan/norm_speed.png\" width=\"500\">\n",
        "\n",
        "4. Активации\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gan/norm_activations.png\" width=\"500\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwdbqXT-DjL9"
      },
      "source": [
        "Представим себе, что данные, которые мы подаем в нейросеть, распределены следующщим образом\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/no_normalization.png\" width=\"300\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSLYEqxwDjL9"
      },
      "source": [
        "Фактически нейросети работают со скалярными произведениями. В этом плане две вектора, изображенных на рисунке, не сильно отличаются. Так же и точки нашего датасета слабо разделимы. Чтобы с этим работать, нейросеть сначала должна подобрать удобное преобразование, а затем только сравнивать наши объекты. Понятно, это усложняют задачу. \n",
        "\n",
        "Для того, чтобы облегчить нейросети задачу, входные признаки часто нормируют:\n",
        "\n",
        "$$x1' = \\dfrac {x1 - \\mu_{x1}} {\\sigma_{x1}}$$\n",
        "$$x2' = \\dfrac {x2 - \\mu_{x2}} {\\sigma_{x2}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4NsENyKDjL9"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/normalization.png\" width=\"300\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESFFIeA9DjL9"
      },
      "source": [
        " Такое преобразование действительно помогает нейросети \n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2V0UvRvLi3A"
      },
      "source": [
        "###В чем состоит нормализация:\n",
        "\n",
        "Вычитаем среднее, делеим на стандартное отклонение.\n",
        "\n",
        "Для табличных данных иногда применяют PCA а затем снова нормализуют. Но для изображений такая техника не применяется (Почему?)\n",
        "\n",
        " <img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gan/norm_pca.png\" width=\"700\">\n",
        "\n",
        "Нормализация изображений в Pytorch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcvy9CNmMguL"
      },
      "source": [
        "torchvision.transforms.ToTensor() # x /= 255 -> [0 .. 1]\n",
        "# per channel mean, std\n",
        "torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],[0.2470, 0.2434, 0.2615]) \n",
        "                                            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRAHeL7dDjLz"
      },
      "source": [
        "## Инициализация весов\n",
        "\n",
        "То что нормализация входных данных полезна мы  убедились при работе на неглубоких моделях.\n",
        "\n",
        "Но что будет если в модели несколько слоев?\n",
        "\n",
        "Входом каждого следующего слоя буде выход предшествующего.\n",
        "\n",
        "А его значения зависят как от входа так и от весов.\n",
        "\n",
        " <img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gan/weights_nn.png\" width=\"700\">\n",
        "\n",
        " Давайте посмотрим на распределение выходов внутренних словев."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt6IFfD6-Lvq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,12)\n",
        "\n",
        "def show_act(ax,x):\n",
        "  ax.hist(x, bins=500, range=(-3,3))\n",
        "  std = x.std()\n",
        "  ax.title.set_text(f\"std {std:.9f}\")\n",
        "\n",
        "# Model from forth lecture with added third layer\n",
        "class NeuralNet:\n",
        "  def __init__(self):\n",
        "    self.W1 = np.random.randn(3072, 500) * 0.001 \n",
        "    self.W2 = np.random.randn(500, 100) * 0.1  # 0.1 10\n",
        "    self.W3 = np.random.randn(100, 10) * 0.5 # 0.5 100\n",
        "   \n",
        "\n",
        "  def predict(self,x):\n",
        "\n",
        "    fig, axs = plt.subplots(4, 1)\n",
        "    fig.tight_layout()\n",
        "    show_act(axs[0],x)\n",
        "\n",
        "    # Layer 1\n",
        "    scores1 = x.dot(self.W1) # Linear / Fully connected layer1\n",
        "    activations1 = np.tanh(scores1) # ReLU activation\n",
        "    show_act(axs[1],activations1)\n",
        "    \n",
        "    # Layer 2\n",
        "    scores2 = activations1.dot(self.W2)\n",
        "    activations2 = np.tanh(scores2) # ReLU activation\n",
        "    show_act(axs[2],activations2)\n",
        "    \n",
        "    # Layer 3\n",
        "    scores3 = activations2.dot(self.W3)\n",
        "    show_act(axs[3],scores3)\n",
        "\n",
        "    return scores3\n",
        "\n",
        "x = np.random.randn(3072) # input image\n",
        "simple_net = NeuralNet()\n",
        "scores = simple_net.predict(x)\n",
        "#print(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnF2KBWvW7UO"
      },
      "source": [
        "Выходы схлопаваются.\n",
        "\n",
        "\n",
        "Очевидно это связанно с весами. Кстати почему мы вообще так их инициализируем.\n",
        "\n",
        "Почему бы не использовать ноль ..\n",
        "\n",
        "или 1- цу\n",
        "...\n",
        "\n",
        "или сделать их побольше, мы же все время на них умножаем, а выходы и так маленикие.\n",
        "\n",
        ".. \n",
        "\n",
        "похоже что это работает если на каждом слое масштабировать веса, то коллапса можно избежать...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoF1IolMDjLz"
      },
      "source": [
        "Хотим задать изначальные веса в нейросети. \n",
        "Как это можно сделать? \n",
        "\n",
        "Делать инициализацию 0 - плохая идея. В этом случае нейросеть получим, что градиент по всем таким весам будет одинаков - и их обновление, следовательно, тоже. Ничего не выучим. \n",
        "\n",
        "Можно инициализировать веса нормальным шумом с матожиданием 0 и маленькой дисперсией. Маленькая дисперсия нужна затем, чтобы не получить огромные градиенты за большие изначальные ошибки в предсказании. \n",
        "\n",
        "Но почему бы не взять другое распределение? Может, веса, инициализированные экспоненциальным распределениям дают какие-нибудь классные гарантии? Или какой-нибудь другой, не приходящий сразу в голову способ инициализации. Давайте это обсудим. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IViWQunry9Iz"
      },
      "source": [
        "### Эксперимент: 2Layer vs 3Layer\n",
        "(Сигмоида затухает и теоретически и практически)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opjKL1xrDjLy"
      },
      "source": [
        "model = SimpleMNIST_NN(n_layers=2).cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "writer = get_summary_writer(\"n_layers2_sigmoid\")\n",
        "\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdaiTTQbFJk_"
      },
      "source": [
        "\n",
        "for epoch in range(10):\n",
        "    train_epoch(model,\n",
        "              optimizer, \n",
        "              criterion,\n",
        "              train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-dFi42ZPixI"
      },
      "source": [
        "from torch import nn\n",
        "model = SimpleMNIST_NN(n_layers=3).cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "writer = get_summary_writer(\"n_3layers_sigmoid\")\n",
        "\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvYNy2j0PfOM"
      },
      "source": [
        "for epoch in range(10):\n",
        "    train_epoch(model,\n",
        "              optimizer, \n",
        "              criterion,\n",
        "              train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lob46DrPPn0H"
      },
      "source": [
        "reinit_tensorboard(clear_log=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RSBUSTSUaGW"
      },
      "source": [
        "Нейросеть с тремя слоями вообще не учится. Почему? Можем попробовать разобраться\n",
        "\n",
        "Для этого напишем функции, которые будут следить за распределением градиентов и активаций на наших слоях.\n",
        "\n",
        "И скажем при помощи метода register_backward_hook  пайторчу исполнять эти функции при каждом пропускании градиента"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qypDWDGKVz-e"
      },
      "source": [
        "def get_forward_hook(writer, tag):\n",
        "    ind = 0\n",
        "    def forward_hook(self, input_, output):\n",
        "        nonlocal ind\n",
        "        ind += 1\n",
        "        writer.add_scalar(tag,\n",
        "                          input_[0].abs().mean(),\n",
        "                          ind)\n",
        "    return forward_hook\n",
        "\n",
        "def get_backward_hook(writer, tag):\n",
        "    ind = 0\n",
        "    def backward_hook(grad): # for tensors \n",
        "        nonlocal ind\n",
        "        ind += 1\n",
        "        writer.add_scalar(tag, \n",
        "                          grad.abs().mean(), \n",
        "                          ind)\n",
        "    return backward_hook\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upS4wiTcaFjD"
      },
      "source": [
        "def register_model_hooks(model, writer, max_ind = 4):\n",
        "  cur_ind = 0\n",
        "  for child in model.layers.children():\n",
        "      if isinstance(child, nn.Linear):\n",
        "          cur_ind += 1\n",
        "          forward_hook = get_forward_hook(writer, f\"activation_{cur_ind}\")\n",
        "          child.register_forward_hook(forward_hook)\n",
        "\n",
        "          backward_hook = get_backward_hook(writer, f\"gradient_{max_ind - cur_ind + 1}\")\n",
        "          child.weight.register_hook(backward_hook)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdCOn_4L76Cy"
      },
      "source": [
        "model = SimpleMNIST_NN(n_layers=3).cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "writer = get_summary_writer(\"n3_layers_sigmoid2\")\n",
        "\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP0QJpkrp1l_"
      },
      "source": [
        "register_model_hooks(model, writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvpYTiiMUStJ"
      },
      "source": [
        "for epoch in range(10):\n",
        "    train_epoch(model,\n",
        "              optimizer, \n",
        "              criterion,\n",
        "              train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwnQkvlG7EMZ"
      },
      "source": [
        "reinit_tensorboard(clear_log=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evrETBaabzBR"
      },
      "source": [
        "Мы видим, что градиент нашей модели стремительно затухает. Первые слои (до которых градиент доходит последним), получают значения градиента, мало отличимые от нуля\n",
        "\n",
        "Причем, это будет верно с самых первых шагов обучения нашей модели\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXqWA9HXgPOq"
      },
      "source": [
        "Это явление получило название **паралич сети**\n",
        "\n",
        "Оказывается, иногда чтобы побороть такое поведение нейросети достаточно лишь правильно инициализировать веса "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glYUeJeCDjLz"
      },
      "source": [
        "### Инициализация Ксавьера (Xavier, Glorot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BldC-l0xDjLz"
      },
      "source": [
        "Рассмотрим в качестве активации нечетную функцию с единичной производной в нуле .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfNMdtvHDjL0"
      },
      "source": [
        "Например, нам подойдет гиперболический тангенс (tanh)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkojUfCcWQGp"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(-10, 10.1, 0.1)\n",
        "plt.plot(x, np.tanh(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciHKAXJtDjL0"
      },
      "source": [
        "Мы хотим начать из линейного региона этой функции, чтобы избежать\n",
        "затухающих градиентов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf2Vw6zxDjL0"
      },
      "source": [
        "Как у нас зависят активации на текущем слое от активаций на предыдущем?\n",
        "\n",
        "$$z^{i+1} = f(z^iW^i)$$\n",
        "\n",
        "Тогда, так как мы вначале хотим находиться в районе линейности нашей функции, то\n",
        "\n",
        "$$z^{i+1} \\approx z^i W^i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4odAaG_yDjL0"
      },
      "source": [
        "Ответить на вопрос, а как будут связаны дисперсии этих активаций сложнее \n",
        "\n",
        "Сначала распишем:\n",
        "1. чему равна дисперсия суммы двух независимых величин\n",
        "\n",
        "$$D(\\eta + \\gamma) = D\\eta + D\\gamma$$\n",
        "\n",
        "2. чему равна дисперсия произведений двух независимых величин\n",
        "\n",
        "\n",
        "$$D\\eta\\gamma = E(\\eta\\gamma)^2 - (E\\eta\\gamma)^2 = E\\eta^2E\\gamma^2 - (E\\eta)^2(E\\gamma)^2$$ \n",
        "\n",
        "Далее распишем для одного веса текущего слоя \n",
        "\n",
        "$$z^{i+1}_{k} = \\sum_t z^i_t w_{kt}$$\n",
        "\n",
        "$$D(z^{i+1}_{k}) = D(\\sum_t z^i_t w_{kt}) = \\sum_t D(z^i_t w_{kt})$$\n",
        "\n",
        "Предполагая, что дисперсии весов и активаций одинаковые (а нам бы так хотелось)\n",
        "\n",
        "$$D(z^{i+1}_{k}) = n D(z^i_0 w_{k0})$$\n",
        "\n",
        "Далее применяем нашу формулу и получаем:\n",
        "\n",
        "$$D(z^{i+1}_{k}) = n [E(z^i_0)^2E(w_{k0})^2 - (Ez^i_0)^2(Ew_{k0})^2]$$\n",
        "\n",
        "Хотим, чтобы что матожидание наших активаций 0, и мы можем этого добиться, делая  матожидание весов равным 0. \n",
        "\n",
        "$$D(z^{i+1}_{k}) =   n E(z^i_0)^2E(w_{k0})^2 $$\n",
        "\n",
        "Заметим, что так как матожидание активация и весов равны 0, то матожидания их квадратов равны дисперсии активаций и весов соответственно\n",
        "\n",
        "$$Dz = E(z^{i+1}_{k})^2 - (Ez^{i+1}_{k})^2 = E(z^{i+1}_{k})^2$$\n",
        "\n",
        "$$D(z^{i+1}_{k}) = n Dz^i_0Dw_{k0}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GynCmJmODjL1"
      },
      "source": [
        "Отсюда можно вывести формулу для зависимости активаций любого слоя от весов предуыдущих слоев и дисперсии исходных данных\n",
        "\n",
        "$$Dz^i = Dx \\prod_{p=0}^{i-1}n_pDW^p $$\n",
        "\n",
        "Где n_p - размерность выхода слоя p-го слоя\n",
        "\n",
        "Аналогично можно вывести формулу для градиентов по активациям\n",
        "\n",
        "\n",
        "$$D(\\dfrac {\\delta L} {\\delta z^i}) = D(\\dfrac {\\delta L} {\\delta z^d} ) \\prod_{p=i}^{d}n_{p+1}DW^p $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgPxYZRcDjL1"
      },
      "source": [
        "Для хорошей инициализации нам бы хотелось, чтобы дисперсии активаций на всех слоях были одинаковые. \n",
        "А еще хотелось бы, чтобы дисперсии градиентов тоже были одинаковые. \n",
        "\n",
        "Тогда у нас не происходит резких скачков в распределении активаций, а градиент не затухает и не взрывается \n",
        "\n",
        "\n",
        "$$Dz^i = Dz^j$$\n",
        "$$D\\dfrac {\\delta L} {\\delta z^i} = D\\dfrac {\\delta L} {\\delta z^j}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjUeQRW9DjL1"
      },
      "source": [
        "Учитывая предыдущее, это эквивалентно тому, что мы требуем\n",
        "\n",
        "$$n_iDW^i = 1$$\n",
        "$$n_{i+1}DW^i = 1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKt2Yv62DjL1"
      },
      "source": [
        "Одновременно так сделать не получится $$n_i \\ne n_{i+1}$$\n",
        "\n",
        "Потому делаем компромисс - среднее гармонческое решений первого и второго уравнения\n",
        "\n",
        "$$DW^i = \\dfrac 2 {n_i + n_{i+1}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmuepCJWDjL1"
      },
      "source": [
        "Надо выбрать распределение.\n",
        "\n",
        "$$ EW^i = 0 $$\n",
        "\n",
        "$$DW^i = \\dfrac 2 {n_i + n_{i+1}}$$\n",
        "\n",
        "Можно было бы взять нормальное распределение с такими параметрами.\n",
        "\n",
        "$$W_i \\sim N(0, sd=\\sqrt{\\dfrac 2 {n_i + n_{i+1}}}) $$\n",
        "\n",
        "А можно равномерное:\n",
        "\n",
        "$$D(U[a, b]) = \\dfrac 1 {12} (b -a)^2$$\n",
        "\n",
        "$$W_i \\sim U[-\\dfrac {\\sqrt{6}} {n_i + n_{i + 1}}, \\dfrac {\\sqrt{6}} {n_i + n_{i + 1}} ]$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXZhl5O7iZmP"
      },
      "source": [
        "import math\n",
        "class X_NeuralNet(NeuralNet):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.W1 = np.random.randn(3072, 500) * (1 / np.sqrt(3072))  \n",
        "    self.W2 = np.random.randn(500, 100) * (1 / np.sqrt(500)) \n",
        "    self.W3 = np.random.randn(100, 10) * (1  / np.sqrt(100))\n",
        "\n",
        "    # Rule\n",
        "    # W = np.random.randn(Din,Dout) / np.sqrt(Din)\n",
        "\n",
        "simple_net = X_NeuralNet()\n",
        "scores = simple_net.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAJi9DzGDjL2"
      },
      "source": [
        "### He-инициализация (Kaiming)\n",
        "\n",
        "Но многие люди используют ReLU. Она в 0 не имеет производной, потому применять инициализацию, указанную выше, проблематично. \n",
        "\n",
        "Тогда получатся похожие условия \n",
        "\n",
        "$$Dz^i = Dx \\prod_{p=0}^{i-1}\\dfrac 1 2 n_pDW^p $$\n",
        "\n",
        "$$D(\\dfrac {\\delta L} {\\delta z^i}) = D(\\dfrac {\\delta L} {\\delta z^d} ) \\prod_{p=i}^{d}\\dfrac 1 2 n_{p+1}DW^p $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRLlWIYyDjL2"
      },
      "source": [
        "И для них решения будут тоже похожими:\n",
        "\n",
        "$$\\frac 2 {n_k}$$\n",
        "\n",
        "и\n",
        "\n",
        "$$\\frac 2 {n_{k+1}}$$\n",
        "\n",
        "Можно опять взять среднее гармоническое. Но на практике, особенно в случае сверточных нейронных сетей, просто берут либо $ \\frac 2 {n_i}$ либо $\\frac 2 {n_i + 1}$\n",
        "\n",
        "$$W^i \\sim N(0, sd=\\sqrt{\\frac 2 n_i})$$\n",
        "\n",
        "$$W^i \\sim N(0, sd=\\sqrt{\\frac 2 {n_i + 1}})$$\n",
        "\n",
        "Опять же, можно использовать и равномерное распределение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciqUqqeCoDZS"
      },
      "source": [
        "# Rule\n",
        "# W = np.random.randn(Din,Dout) / np.sqrt(2/Din)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8bChm1-DjL2"
      },
      "source": [
        "### Важность соответствия функции активации выбранной инициализации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi9CusQ2DjL2"
      },
      "source": [
        "От правильной активации может зависеть очень многое:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMwU3vijDjL2"
      },
      "source": [
        "1. Нейросеть может сойтись значительно быстрее\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-07.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvW_kKURDjL3"
      },
      "source": [
        "2. В зависимости от выбранной активации сеть вообще может сойтись или не сойтись\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-08.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEWFjf-tDjL3"
      },
      "source": [
        "### Обобщение инициализаций Ксавьера и He-инициализации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhSBzXMODjL3"
      },
      "source": [
        "Вообще говоря, коэффициенты в инициализациях (числитель в формуле для дисперсии), зависит от конкретной выбранной функции активации.\n",
        "В pytorch есть функции для вычисления этих коэффициентов\n",
        "https://pytorch.org/docs/stable/nn.init.html\n",
        "\n",
        "\n",
        "https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#Conv2d\n",
        "\n",
        "https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
        "\n",
        "\n",
        "```\n",
        "init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z31kyx6DjL4"
      },
      "source": [
        "### Ортогональная инициализация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyA3d5yGDjL4"
      },
      "source": [
        "Также иногда используется так называемая ортогональная инициализация весов. \n",
        "Для каждого слоя мы убеждаемся, что изначальная матрица весов является ортогональной. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQLwUUkRDjL4"
      },
      "source": [
        "Выберем ортогональную матрицу весов \n",
        "$$W: WW^T = 1$$. \n",
        "\n",
        "Тогда:\n",
        "1.  норма активации сохраняется (опять же, активации между слоями остаются в одном масштабе)\n",
        "$$||s_{i+1}|| = ||W_{i}s_i|| = ||s_i||$$\n",
        "\n",
        "2.  все нейроны делают «разные» преобразования\n",
        "$$ ⟨W_i, W_j⟩ = 0~i \\ne j$$\n",
        "$$ ⟨W_i, W_j⟩ = 1~i = j$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1t9EOaDDjL4"
      },
      "source": [
        "Иногда такая инициализация обеспечивает значительно лучшую сходимость https://datascience.stackexchange.com/questions/64899/why-is-orthogonal-weights-initialization-so-important-for-ppo\n",
        "\n",
        "https://smerity.com/articles/2016/orthogonal_init.html\n",
        "\n",
        "https://arxiv.org/abs/2001.05992"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCFiRkhyDjMC"
      },
      "source": [
        "А градиент по весам уменьшается в a раз\n",
        "\n",
        "$$\\dfrac {\\delta BN((aW)u)} {\\delta aW} = \\dfrac 1 a \\dfrac {\\delta BN(Wu)} {\\delta W} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4KWNKVDjMC"
      },
      "source": [
        "Таким образом нейросеть автоматически не дает большим весам расти"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1M4hDUcDjMC"
      },
      "source": [
        "Допустим, мы решили увеличить веса в $a$ раз"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvRbBn98DjMC"
      },
      "source": [
        "Так как мы шкалируем, то домножение весов W на константу выходных значений слоя не меняет\n",
        "\n",
        "$$BN((aW)u) = BN(Wu)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2B9HCTWDjMC"
      },
      "source": [
        "Градиент слоя по входу не меняется\n",
        "\n",
        "$$\\dfrac {\\delta BN((aW)u)} {\\delta u} = \\dfrac {\\delta BN(Wu)} {\\delta u}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFvVtWnZiOxv"
      },
      "source": [
        "### Инициализация весов в Pytorch\n",
        "\n",
        "Для инициализации весов Pytorch используется модуль torch.nn.init\n",
        "\n",
        "В нем определены разные функции для инициализации весов.\n",
        "\n",
        "Нюанс состоит в том, что обычно для слоев разного типа может требоваться разная регуляриация. Потому в функции, которая инициализирует слов вашей нейронное сети желательно прописывать разное поведение для разных слоев.\n",
        "\n",
        "Попробуем, например, добавить в нашу нейросеть инициализацию. Нам нужна регуляризация Xavier, так как у нас Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHgS70etiN4s"
      },
      "source": [
        "class SimpleMNIST_NN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 n_layers,\n",
        "                 activation=nn.Sigmoid, \n",
        "                 init_form=\"normal\"):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.activation = activation()\n",
        "        layers = [nn.Linear(28 * 28, 100), self.activation]\n",
        "        for _ in range(0, n_layers - 1):\n",
        "            layers.append(nn.Linear(100, 100))\n",
        "            layers.append(self.activation)\n",
        "        layers.append(nn.Linear(100, 10))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.init_form = init_form\n",
        "        if self.init_form is not None:\n",
        "            self.init()\n",
        "        if not isinstance(self.activation, nn.Sigmoid):\n",
        "            raise NotImplementedError()\n",
        "        \n",
        "    def forward(self, x, predict_proba=False):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.layers(x)\n",
        "        if predict_proba:\n",
        "          x = F.softmax(x)\n",
        "        return x \n",
        "\n",
        "    def init(self):\n",
        "        sigmoid_gain = torch.nn.init.calculate_gain(\"sigmoid\")\n",
        "        for child in self.layers.children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                if self.init_form == \"normal\":\n",
        "                  torch.nn.init.xavier_normal_(child.weight, \n",
        "                                                gain=sigmoid_gain)\n",
        "                  if child.bias is not None:\n",
        "                      torch.nn.init.zeros_(child.bias)\n",
        "                elif self.init_form == \"uniform\":\n",
        "                    torch.nn.init.xavier_uniform_(child.weight, \n",
        "                                                gain=sigmoid_gain)\n",
        "                    if child.bias is not None:\n",
        "                        torch.nn.init.zeros_(child.bias)\n",
        "                else:\n",
        "                    raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyg48bKbkXLe"
      },
      "source": [
        "model = SimpleMNIST_NN(n_layers=3).cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "writer = get_summary_writer(\"n3_layers_sigmoid_havier\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wad4s2GOp4PB"
      },
      "source": [
        "plt.hist(list(model.layers.children())[0].weight.cpu().detach().numpy().reshape(-1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRLOD1nFk13Y"
      },
      "source": [
        "register_model_hooks(model, writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmnOm5oZEsFy"
      },
      "source": [
        "train_steper = Steper()\n",
        "test_steper = Steper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO2N3Ns3mWJy"
      },
      "source": [
        "for epoch in range(10):\n",
        "    train_epoch(model,\n",
        "              optimizer, \n",
        "              criterion,\n",
        "              train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH5QOSLqybem"
      },
      "source": [
        "reinit_tensorboard(clear_log = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv76kID3xZAv"
      },
      "source": [
        "Видим, что нейросеть стала хоть как-то учиться"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd76wnC9vwxJ"
      },
      "source": [
        "## Затухание(Vanishing)/Взрыв(Exploding) градиента"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEMqGT7tP5Fb"
      },
      "source": [
        "### Причины? \n",
        "\n",
        "Посмотрим на обычную сигмоиду\n",
        "\n",
        "$$\\sigma(z) = \\dfrac 1 {1 + e^{-z}}$$\n",
        "\n",
        "Ее производная, как вы уже выводили, равна\n",
        "\n",
        "$$\\dfrac {\\delta \\sigma(z)} {\\delta z} = \\sigma(z) (1 - \\sigma(z))$$\n",
        "\n",
        "Какое максимальное значение у такой функции?\n",
        "\n",
        "Учтем, что сигмоида находится в пределах от 0 до 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cznk-KoSP2cn"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "x = np.arange(0, 1.001, 0.001)\n",
        "plt.plot(x, x - x**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZarilkExRZah"
      },
      "source": [
        "Получается, что максимальное значение производной по сигмоиде - 1/4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdXBHilPRf5C"
      },
      "source": [
        "Теперь возьмем простую нейронную сеть \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/simple_nn.png\" width=\"700\">\n",
        "\n",
        "Посчитаем у нее градиент\n",
        "\n",
        "$$\\dfrac {\\delta L} {\\delta z_4} = \\dfrac {\\delta L} {\\delta y} \\dfrac {\\delta y} {\\delta z_4} = \\dfrac {\\delta L} {\\delta y} \\dfrac {\\delta \\sigma(w_5z)} {\\delta z} w_5 \\le \\dfrac 1 4 \\dfrac {\\delta L} {\\delta y}  w_5 $$\n",
        "\n",
        "Аналогично можно посчитать градиент для $z_3$\n",
        "\n",
        "$$\\dfrac {\\delta L} {\\delta z_3} = \\dfrac {\\delta L} {\\delta z_4} \\dfrac {\\delta z_4} {\\delta z_3} \\le \\dfrac {\\delta L} {\\delta y} \\dfrac {\\delta \\sigma(w_5z)} {\\delta z} w_5 \\le {\\dfrac 1 4}^2 \\dfrac {\\delta L} {\\delta y}  w_5 w_4$$\n",
        "\n",
        "И так далее\n",
        "\n",
        "$$\\dfrac {\\delta L} {\\delta x}  \\le {\\dfrac 1 4}^5 \\dfrac {\\delta L} {\\delta y}  w_5 w_4 w_3 w_2 w_1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRMSGYFOTmfU"
      },
      "source": [
        "Таким образом, градиент начинает экспоненциально затухать, если веса маленькие.\n",
        "\n",
        "Если веса большие - то градиент наоборот начнет экспоненциально возрастать - все взорвется\n",
        "\n",
        "Для некоторых функций активации картина будет не столь катастрофична, но тоже неприятна.\n",
        "\n",
        "В семинарском задании вы посмотрите, например, как ведет себя функция ReLU в этом случае\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJPv7fAwvi2-"
      },
      "source": [
        "## Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnijRsrFDjL-"
      },
      "source": [
        "### Плохой вариант борьбы с этим"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hn-8mQJDjL-"
      },
      "source": [
        "Давайте на каждом слое просто нормировать каждый признак, используя среднее и дисперсию по батчу\n",
        "\n",
        "$$\\tilde{x}_i^j = \\dfrac {x_i^j - Ex^j} {\\sqrt{Dx^j}}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TCLWW0nDjL-"
      },
      "source": [
        "Проблема в том, что таким образом мы можем попасть в область линейной составляющей нашей функции. Например, в случае сигмоиды \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lin_sigmoid.png\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9W2z1ihDjL_"
      },
      "source": [
        "Получаем набор линейных слоев фактически без функций активации -> все вырождается в однослойную сеть. Не то что нам надо"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMqmDyDg0N23"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gan/bn_relu.png\" width=\"400\">\n",
        "\n",
        "В случае с ReLU подобной функцией активации, проблемы не исчезат. Если нормировать активации в интервал [0,1] то на каждом слое мы будем всегда обнулять половину активаций. \n",
        "\n",
        "Хотя опыт принудительной нормализации выходов слоя известен: в AlexNet было два слоя нормализации.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gan/bn_loss.png\" width=\"600\">\n",
        "\n",
        "Обратите внимание на то что в начале обучения Loss сильно меняется. В связи с этим происходят большие обновления весов в результате чего значтельная часть активаций может занулиться или уйти в область где Relu линейна.\n",
        "\n",
        "На этом этапе нормализация могла бы помочь. Но по мере обучения сети изменения весов становятся более осмысленными.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgSf-z7mDjL_"
      },
      "source": [
        "Нам надо дать нейросетке возможность перемещать распределение слоя из области 0 и самой подбирать дисперсию \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/nn_example.png\" width=\"250\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBcGTx2UDjL_"
      },
      "source": [
        "$$\\tilde{x}_i^j = \\dfrac {x_i^j - Ex^j} {\\sqrt{Dx^j}}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd2AUsbXDjL_"
      },
      "source": [
        "$$\\hat{x_i^j} = \\gamma^j\\tilde{x}_i^j + \\beta^j$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY1KTTgaDjL_"
      },
      "source": [
        "Фактически, теперь нейросеть даже может отменить нормализацию, если считает ее ненужной"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDwESr11DjL_"
      },
      "source": [
        "#### Скользящее среднее"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLbLMgzwDjL_"
      },
      "source": [
        "Во время предсказания батча у нас уже нет - откуда брать матожидание и дисперсию? \n",
        "\n",
        "1. Можно посчитать по всей обучающей выборке - не всегда удобно, долго\n",
        "2. Считать скользящее среднее во время обучения\n",
        "\n",
        "$$E^j_{k+1} = (1 - \\alpha)E^j_k + \\alpha E^j_{batch}$$\n",
        "\n",
        "Используем обычно $\\alpha = 0.1$\n",
        "\n",
        "Аналогично считаем дисперсию "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zcfy7Y7GDjMA"
      },
      "source": [
        "#### Защита от нулей в знаменателе\n",
        "\n",
        "Чтобы у нас не мог возникнуть 0 в знаменателе, добавляем маленькое чиссло - $\\epsilon$. Например, равное 1e-5\n",
        "\n",
        "\n",
        "$$\\tilde{x}_i^j = \\dfrac {x_i^j - Ex^j} {\\sqrt{Dx^j + \\epsilon}}$$\n",
        "\n",
        "$$\\hat{x_i^j} = \\gamma^j\\tilde{x}_i^j + \\beta^j$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTqWE7d9DjMA"
      },
      "source": [
        "#### Линейный слои и конволюции\n",
        "\n",
        "Слой конволюции можно свести к линейному слою с очень жесткими ограничениями на веса. Поэтому неудивительно, что BatchNorm можно применять и для линейных слоев и для конволюций. \n",
        "\n",
        "С конволюциями есть единственный нюанс - у нас \"одним признаком\" считается вся получаемая **feature map**. \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/feature_map.png\" width=\"450\">\n",
        "\n",
        "И нормализация идет по всей такой feature map (по всему каналу) для всех объектов. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4XkSkufDjMA"
      },
      "source": [
        "#### Пример работы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mlFBW1HDjMA"
      },
      "source": [
        "Этот метод действительно работает. \n",
        "Видим, что нейросети с батчнормализацией:\n",
        "\n",
        "1. Сходятся быстрее, чем нейросети без \n",
        "2. Могут работать с более высоким начальным learning rate, причем это позволяет достигать лучших результатов\n",
        "3. BatchNorm позволяет глубокой нейросетке работать даже с функцией активации в виде сигмоиды. Без батчнорма такая сеть не обучилась бы вовсе. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ivkHy5DjMA"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/batchnorm_work.png\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QeFG6jUDjMB"
      },
      "source": [
        "#### Градиент"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkzpzY3VDjMB"
      },
      "source": [
        "Вычисление градиента batchnorm - интересное упражнение на понимание того, как работает backpropagation. В лекции мы это опускаем, можете ознакомиться самостоятельно\n",
        "\n",
        "[Вывод градиентов для весов слоя BatchNorm](https://kevinzakka.github.io/2016/09/14/batch_normalization/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DpecpRPDjMB"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-20.png\" width=\"700\">\n",
        "\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gan/bn_graph.png\" width=\"700\">\n",
        "\n",
        "https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tRWA1tvDjL9"
      },
      "source": [
        "### Covariate shift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB-IVSZ9DjL-"
      },
      "source": [
        "Covariate shift - явление, когда признаки тренировочной выборки и тестовой по-разному распределены.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/covariate_shift.png\" width=\"300\">\n",
        "\n",
        "В такой ситуации модель не в состоянии делать адекватные предсказания на тесте, так как не видела области пространства, в которой расположены тестовые объекты\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/covariate_shift2.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ObZHk5bDjL-"
      },
      "source": [
        "### Internal covariate shift\n",
        "\n",
        "Похожее явление может иметь место уже внутри нейросети\n",
        "\n",
        "Пусть у нас iй слой переводит выдачу i-1 в новое пространство. \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/internal_covariate_shift.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF4bDZ4cDjL-"
      },
      "source": [
        "В конце нейросеть делает предсказание, считается лосс, делается обратное распространение ошибки и обновляются веса. \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/internal_covariate_shift2.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LZk6Hr_DjL-"
      },
      "source": [
        "После этого возникает нехорошая ситуация - распределение выходов i-1 слоя поменялось, а i-й слой изменял веса, думая, что распределение выходов не изменилось\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/internal_covariate_shift3.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq2kYk6NDjMD"
      },
      "source": [
        "### Internal covariate shift?\n",
        "\n",
        "Согласно некоторым исследованиям ([например](https://arxiv.org/abs/1805.11604)), успех BatchNormalization не заключается в исправлении covariate shift. \n",
        "\n",
        "BatchNormalization работает как-то иначе, улучшая гладкость пространства решений и облегчает поиск в нем минимума.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/batchnorm_smooth.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3buSY3pHDjMD"
      },
      "source": [
        "### Tips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14s4o2GYDjMD"
      },
      "source": [
        "Стоит помнить, что с батч-нормализацией:\n",
        "\n",
        "* **Крайне важно** перемешивать батчи между эпохами. Единицей обучения параметров $\\beta$ и $\\gamma$ являются батчи. Если их не перемешивать, то из 6400 объектов в тренировочном датасете получим лишь 100 объектов для обучения $\\beta$ и $\\gamma$\n",
        "\n",
        "* В слое, после которого поставили BatchNormalization, надо убрать смещения (параметр $\\beta$ в BatchNormalization берет эту роль сам по себе)\n",
        "\n",
        "\n",
        "* Другое расписание learning rate: бОльшее значение в начале обучения и быстрое уменьшение в процессе обучения\n",
        "\n",
        "* Если используем BatchNormalization, то надо уменьшить силу Dropout и L2-регуляризации\n",
        "\n",
        "* Чем меньше размер батча в обучении, тем хуже будет работать BatchNormalization\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/batchnorm_batch.png\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pJbhPqw3Bjv"
      },
      "source": [
        "### Используем BatchNormalization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJpLIAll2_28"
      },
      "source": [
        "class SimpleMNIST_NN_Init_Batchnorm(nn.Module):\n",
        "    def __init__(self, n_layers):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        layers = [nn.Linear(28 * 28, 100, bias=False),\n",
        "                  nn.BatchNorm1d(100),\n",
        "                  nn.Sigmoid()]\n",
        "        for _ in range(0, n_layers - 1):\n",
        "            layers.append(nn.Linear(100, 100, bias=False))\n",
        "            layers.append(nn.BatchNorm1d(100))\n",
        "            layers.append(nn.Sigmoid())\n",
        "        layers.append(nn.Linear(100, 10))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.init()\n",
        "        \n",
        "    def forward(self, x, predict_proba=False):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.layers(x)\n",
        "        if predict_proba:\n",
        "            x = F.softmax(x)\n",
        "        return x \n",
        "\n",
        "    def init(self):\n",
        "        sigmoid_gain = torch.nn.init.calculate_gain(\"sigmoid\")\n",
        "        for child in self.layers.children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                torch.nn.init.xavier_normal_(child.weight, \n",
        "                                              gain=sigmoid_gain)\n",
        "                if child.bias is not None:\n",
        "                    torch.nn.init.zeros_(child.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8tsLnkzFAe6"
      },
      "source": [
        "model = SimpleMNIST_NN_Init_Batchnorm(n_layers=3).cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "writer = get_summary_writer(\"batchnorm2\")\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgwiXxidqe7B"
      },
      "source": [
        "register_model_hooks(model, writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulX6nb5rFLl0"
      },
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    train_epoch(model,\n",
        "              optimizer, \n",
        "              criterion,\n",
        "              train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    model.eval()\n",
        "    validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HnXYpz3HtW7"
      },
      "source": [
        "И попробуем, согласно советам, увеличить learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pou0ocuyHrtb"
      },
      "source": [
        "model = SimpleMNIST_NN_Init_Batchnorm(n_layers=3).cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "writer = get_summary_writer(\"batchnorm_increased_lr\")\n",
        "\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUHOeFqK33Rl"
      },
      "source": [
        "register_model_hooks(model, writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2jFUCf8HsFq"
      },
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    train_epoch(model,\n",
        "              optimizer, \n",
        "              criterion,\n",
        "              train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    model.eval()\n",
        "    validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDgFt9pt5is_"
      },
      "source": [
        "reinit_tensorboard(clear_log=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdqILnUeDjMD"
      },
      "source": [
        "### Другие виды Нормализаций\n",
        "\n",
        "[Множество их](https://paperswithcode.com/methods/category/normalization)\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/normalization_methods.png\" width=\"900\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HT6l0uaDjME"
      },
      "source": [
        "Чаще всего нормализации тестируют на изображениях в том числе или только на изображениях. Потому их графически объясняют при помощи следующей схемы:\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/notation_3d_tensor_viz.png\" width=\"350\">\n",
        "\n",
        "По одной оси - каналы (feature maps), по второй - объекты из батча, а по третье feature map, reshapeнутого в одномерный вектор \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTO67OBiDjME"
      },
      "source": [
        "В этой терминологии BatchNorm выглядит следующим образом\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/batch_norm_3d.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M21KUChSDjME"
      },
      "source": [
        "#### Layer Norm\n",
        "\n",
        "Нормализуем активации не вдоль объектов, а вдоль признаков, для каждого объекта индивидуально. Хорошо работает для рекуррентных нейронных сетей\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/layer_norm_3d.png\" width=\"350\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz7Vq3pnDjME"
      },
      "source": [
        "#### Instance Norm\n",
        "\n",
        "При пакетной нормализации происходит усреднение параметров по всему пакету. Например, в случае задачи переноса стилей картин, это вносит много шума. При усреднении теряются индивидуальные характеристики объектов. Поэтому используется более тонкая нормализация — индивидуальная нормализация (англ. instance normalization) (по каналу, но только для данного объекта)\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/instance_norm_3d.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ysOZA9pDjME"
      },
      "source": [
        "#### GroupNorm\n",
        "\n",
        "Нормализуем активации по группам каналов (**feature_map**), как и в BatchNorm, но только для данного объекта.\n",
        "\n",
        "Эффективен в случае, когда у нас батчи маленького размера (именно его мы сравнивали с обычной батч-нормализацией выше)\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/group_normalization_3d.png\" width=\"350\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ke3K7xJH7mj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp3kC_i4DjMF"
      },
      "source": [
        "### Weights standartization 3d\n",
        "\n",
        "Будем нормализовать **веса** каждого выходного слоя отдельно. \n",
        "Это в теории улучшает ландшафт, в котором мы ищем минимум функции потерь.\n",
        "Обратите внимание, что здесь оси другие\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/weight_standartization3d.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLt3dg8nDjMF"
      },
      "source": [
        "За счет замены BatchNorm в популярной архитектуре ResNet на GroupNorm и Weights standartization была получена нейросеть, которую легко можно было адаптировать к разным задачам \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/resnet_modified.gif\" width=\"350\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4irPygCtDjMF"
      },
      "source": [
        "### Ставить BatchNormalization до или после активации?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-cbNGAYDjMF"
      },
      "source": [
        "#### До\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/bn_act.png\" width=\"350\">\n",
        "\n",
        "\n",
        "* Рекомендуется авторами статьи, где предложили Batch Normalization\n",
        "* Для сигмоиды, BN, поставленная после активации, не решает проблем сигмоиды\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIynSDmpDjMF"
      },
      "source": [
        "#### После\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/act_bn.png\" width=\"350\">\n",
        "\n",
        "* Аргументация авторов статьи не до конца обоснованна\n",
        "* Обычно, сигмоиду не используют в современных нейронных сетях\n",
        "* Для популярной ReLU, BN, поставленная до активации может приводить к “умирающей ReLU”, когда большая часть ее входов меньше 0 и потому для них градиент не проходит\n",
        "* На многих задачах BN после функции активации работает лучше или не хуже поставленной до"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ7GwlwvDjMF"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/bn_relu.png\" width=\"500\">\n",
        "\n",
        "[ссылка](https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKFXvzVCDjMF"
      },
      "source": [
        "### Ставить BatchNormalization до или после Dropout?..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh7OLrRLDjMF"
      },
      "source": [
        "#### До\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/bn_before.png\" width=\"350\">\n",
        "\n",
        "* Меньше влияние (covariate shift) Dropout на Batchnorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vHrDTpIDjMG"
      },
      "source": [
        "#### После\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/bn_after.png\" width=\"350\">\n",
        "\n",
        "* Информация о зануленных активациях не просачивается через среднее и дисперсию батча"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp05jlcdDjMG"
      },
      "source": [
        "#### Ставить только что-то одно \n",
        "\n",
        "* Dropout может отрицательно влиять на качество нейросети с BatchNorm за счет разного поведения на train и test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqS6PGbqDjMG"
      },
      "source": [
        "\n",
        "#### Строго говоря\n",
        "\n",
        "* Оптимальный порядок следования слоев зависит от задачи и архитектуры сети\n",
        "* Возможно, стоит применять модифицированные версии BatchNorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxBX8ZIWDjMB"
      },
      "source": [
        "### Batchnorm как регуляризация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Wb7WDEDjMB"
      },
      "source": [
        "Почему для нейросети с батчнормализацией можно использовать более высокие learning rate? \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSl1YhoADjMC"
      },
      "source": [
        "Оказывается, батчнормализация делает неявную регуляризацию на веса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9jNSAkiDjL4"
      },
      "source": [
        "## Регуляризация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yehA8U2DjL4"
      },
      "source": [
        "Второй способ улучшения сходимости нейросетей и борьбы с переобучением - введение регуляризации. Ее можно вводить несколькими способами. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZPrlHt3DjL5"
      },
      "source": [
        "### L1, L2 регуляризации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ7kPq5lDjL5"
      },
      "source": [
        "Самый простой способ мы уже разобрали - давайте просто добавим в лосс штраф к весам. На сходимость нейросети, это, правда, повлияет слабо"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRs16o7YDjL5"
      },
      "source": [
        "$$Loss\\_reg = loss + \\lambda \\cdot reg$$\n",
        "\n",
        "$$ reg_{L1} = \\lambda \\sum |w_i| $$\n",
        "\n",
        "$$ reg_{L2} = \\lambda \\sum w_i^2 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh9bUDn0DjL5"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/src/L0X_Encoders/img/losses.gif\" alt=\"alttext\" width=\"500px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCpkNtqjDjL5"
      },
      "source": [
        "Иногда уже его хватает, чтобы решить все проблемы. Напомним, что L2 лосс приводит к большому числу маленьких ненулевых весов в сети. А L1 лосс - к маленькому числу ненулевых весов (разреженной нейросети)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0kOBa4KDjL5"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU9eQzaaDjL5"
      },
      "source": [
        "Одним из распространненных именно в нейросетях методом регуляризации является Dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbrtMwv2DjL6"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-12.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6a35YRCDjL6"
      },
      "source": [
        "Состоит этот метод в следующем:\n",
        "\n",
        "1. Во время обучения мы с вероятностью *p* зануляем выход нейронов слоя (например, *p* = 0.5)\n",
        "2. Зануленные нейроны не участвуют в данном forward, и градиент потому к ним при backward не идет. \n",
        "\n",
        "3. Сила регуляризации определяется вероятностью p, чем она больше - тем сильнее регуляризация. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl0ApkTeDjL6"
      },
      "source": [
        "### Мотивация Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vkzebWpDjL6"
      },
      "source": [
        "#### Борьба с коадаптацией \n",
        "\n",
        "Одной из  проблем при работе с глубокими сетями является совместная адаптация нейронов. В такой сети, если все весовые коэффициенты изучаются вместе, обычно некоторые соединения будут иметь больше возможностей прогнозирования, чем другие.\n",
        "\n",
        "Часть нейронов делает основную работу - предсказывает, а остальные могут вообще не вносить никакого вклада в итоговое предсказание. Или же другая картина - один нейрон делает кривоватое предсказание, другие его правят и в итоге первый нейрон своей ошибки не исправляет. \n",
        "\n",
        "Это явление называется со-адаптацией. Этого нельзя было предотвратить с помощью традиционной регуляризации, такой как L1 и L2. А вот Dropout с этим хорошо борется"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaLKs0DYDjL6"
      },
      "source": [
        "Отмена способности некоторых нейронов учиться во время тренировки на самом деле оказывается направлена на то, чтобы получить более обученные нейроны и сократить переобучение, гарантирует, что выучиваемые индивидуальными нейронами функции хорошо работают со случайно выбранными подмножествами функций, выученных другими нейронами. \n",
        "Таким образом,  Dropout приводит к появлению более требовательных нейронов, которые пытаются обойти сложные, индивидуальные особенности, которые склонны плохо обобщать, и сохраняют больше полезной информации самостоятельно. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOvzlEQYDjL6"
      },
      "source": [
        "На следующем рисунке (извлеченном из статьи Dropout: A Simple Way to Prevent Neural Networks from Overfitting) мы находим сравнение признаков, изученных в наборе данных MNIST с одним автоэнкодером скрытого слоя, имеющим 256 выпрямленных линейных единиц без отсева (слева), и признаков, изученных той же структурой с использованием отсева в ее скрытом слое с p=0,5 (справа).\n",
        "\n",
        "В то время как первый показывает неструктурированные, беспорядочные паттерны, которые невозможно интерпретировать, второй явно демонстрирует целенаправленное распределение веса, которое обнаруживает штрихи, края и пятна самостоятельно, нарушая их взаимозависимость с другими нейронами для выполнения этой работы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-mrqyiwDjL7"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-13.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW14JAZZDjL7"
      },
      "source": [
        "#### Dropout как регуляризация \n",
        "\n",
        "Фактически, Dropout штрафует слишком сложные, неустойчивые решения. Добавляя в нейросеть Dropout мы сообщаем ей о том, что решение, которое мы ожидаем, должно устойчиво к шуму"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoNiGO7MDjL7"
      },
      "source": [
        "#### Dropout как ансамбль \n",
        "\n",
        "Можно рассматривать Dropout как ансамбль нейросетей со схожими параметрами, которые мы учим одновременно, вместо того, чтобы учить каждую в отдельности, а затем результат их предсказания усредняем, замораживая Dropout (http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)\n",
        "\n",
        "Фактически, возникает аналогия со случайным лесом - каждая из наших нейросетей легко выучивает выборку и переобучается - имеет низкий bias, но высокий variance. При этом за счет временного отключения активаций, каждая нейросеть видит не все объекты, а только часть. Усредняя все эти предсказания, мы уменьшаем variance. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eltX6KPiixHu"
      },
      "source": [
        "#### Dropout помогает бороться с переобучением\n",
        "\n",
        "Dropout, в силу указанного выше, может хорошо помогать бороться с переобучением\n",
        "\n",
        "И в случае линейных слоев \n",
        "\n",
        "<img src =\"https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/index_files/index_22_0.png\" width=\"600\">\n",
        "\n",
        "\n",
        "И в случае конволюций \n",
        "\n",
        "\n",
        "<img src =\"https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/index_files/index_36_0.png\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcvgEUinivzF"
      },
      "source": [
        "\n",
        "#### Confidence interval от Dropout\n",
        "Можно используя нейросеть с дропаутом, получить доверительный интервал для вашего предсказания. Просто не \"замораживаем\" dropout-слои во время предсказания, а делаем предсказания с активными dropout. \n",
        "\n",
        "И делаем forward через такую нейросеть для одного объекта 1000 раз. \n",
        "Сделав это 1000 раз вы получаете распределение предсказаний, на основе которого можно делать confidence интервалы и как раз ловить те объекты, на которых нейросетей вообще не понимает, что ей делать и потому предсказывает метку или еще что-то с сильной дисперсией. \n",
        "\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/confidence_nn.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw9lD1VGykdV"
      },
      "source": [
        "###  Простая реализация Dropout выглядит следующим образом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Htrmi8I4DjL7"
      },
      "source": [
        "class BadDropout(nn.Module):\n",
        "    def __init__(self, p: float = 0.5):\n",
        "        super().__init__()\n",
        "        if p < 0 or p > 1:\n",
        "            raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.training:\n",
        "            keep = torch.rand(X.size()) > self.p\n",
        "            if X.is_cuda:\n",
        "                keep = keep.cuda()\n",
        "            return X * keep\n",
        "        return weights * (1 - self.p)  # in test time, expectation is calculated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEC2WR9DjL7"
      },
      "source": [
        "Нежелательное свойство представленной выше схемы состоит в том, что мы должны масштабировать активации во время прогнозирования. Поскольку производительность тестирования критически важна, лучше всего использовать инвертированный dropout, который выполняет масштабирование во время обучения. Кроме того, если мы захотим убрать dropout из кода, функция прогнозирования останется без изменений."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQkEPg1qDjL8"
      },
      "source": [
        "class Dropout(nn.Module):\n",
        "    def __init__(self, p: float = 0.5):\n",
        "        super().__init__()\n",
        "        if p < 0 or p > 1:\n",
        "            raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.training:\n",
        "            keep = torch.rand(X.size()) > self.p\n",
        "            if X.is_cuda:\n",
        "                keep = keep.cuda()\n",
        "            return X * keep / (1 - self.p)\n",
        "        return X # in test time, expectation is calculated intrinsically - we just not divide weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ5_HDDIzoaM"
      },
      "source": [
        "Попробуем засунуть Dropout в нашу нейросеть"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wu9cwlbzm65"
      },
      "source": [
        "class SimpleMNIST_NN_Dropout(nn.Module):\n",
        "    def __init__(self, \n",
        "                 n_layers,\n",
        "                 activation=nn.Sigmoid, \n",
        "                 init_form=\"normal\"):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.activation = activation()\n",
        "        layers = [nn.Linear(28 * 28, 100), self.activation]\n",
        "        for _ in range(0, n_layers - 1):\n",
        "            layers.append(nn.Linear(100, 100))\n",
        "            layers.append(Dropout())\n",
        "            layers.append(self.activation)\n",
        "        layers.append(nn.Linear(100, 10))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.init_form = init_form\n",
        "        if self.init_form is not None:\n",
        "            self.init()\n",
        "        if not isinstance(self.activation, nn.Sigmoid):\n",
        "            raise NotImplementedError()\n",
        "        \n",
        "    def forward(self, x, predict_proba=False):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.layers(x)\n",
        "        if predict_proba:\n",
        "          x = F.softmax(x)\n",
        "        return x \n",
        "\n",
        "    def init(self):\n",
        "        sigmoid_gain = torch.nn.init.calculate_gain(\"sigmoid\")\n",
        "        for child in self.layers.children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                if self.init_form == \"normal\":\n",
        "                  torch.nn.init.xavier_normal_(child.weight, \n",
        "                                                gain=sigmoid_gain)\n",
        "                  if child.bias is not None:\n",
        "                      torch.nn.init.zeros_(child.bias)\n",
        "                elif self.init_form == \"uniform\":\n",
        "                    torch.nn.init.xavier_uniform_(child.weight, \n",
        "                                                gain=sigmoid_gain)\n",
        "                    if child.bias is not None:\n",
        "                        torch.nn.init.zeros_(child.bias)\n",
        "                else:\n",
        "                    raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgYo0LBX0pqT"
      },
      "source": [
        "model = SimpleMNIST_NN_Dropout(n_layers=3).cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "writer = get_summary_writer(\"nn3_dropout\")\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJXswpxG1BTV"
      },
      "source": [
        "Так как наша модель из-за Dropout ведет себя по-разному во время обучения и во время тестирования, то мы должны прямо ей сообщать, обучается она сейчас или нет. Делается это при помощи функций model.train и model.eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cVP_T8B0fOc"
      },
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    train_epoch(model,\n",
        "                  optimizer, \n",
        "                  criterion,\n",
        "                  train_loader,\n",
        "                  \"cross_entropy\",\n",
        "                  train_steper,\n",
        "                  writer)\n",
        "    model.eval()\n",
        "    validate(model, \n",
        "             test_loader, \n",
        "             \"cross_entropy\", \n",
        "             test_steper,\n",
        "             writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdAGAaXI3o0T"
      },
      "source": [
        "В данном случае выигрыша мы не получили. Возможно, если учить нейросеть больше эпох, эффект бы заметили"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viYK_cLWICkx"
      },
      "source": [
        "#### Пример борьбы с переобучением при помощи Dropout\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTyVaGNVfZNk"
      },
      "source": [
        "Чтобы увидеть эффект, и при этом не учить долго нейросеть 100+ эпох, сделаем искуственный пример.\n",
        "\n",
        "Просто добавим к линейной зависимости шум и попробуем выучить ее нейронной сетью "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIdeniXeX4Ev"
      },
      "source": [
        "#https://towardsdatascience.com/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd\n",
        "N = 50 #number of data points\n",
        "noise = 0.3\n",
        "\n",
        "#generate the train data\n",
        "X_train = torch.unsqueeze(torch.linspace(-1, 1, N),1)\n",
        "Y_train = X_train + noise * torch.normal(torch.zeros(N,1), torch.ones(N,1))\n",
        "\n",
        "#generate the test data\n",
        "X_test = torch.unsqueeze(torch.linspace(-1,1,N),1)\n",
        "Y_test = X_test + noise * torch.normal(torch.zeros(N,1), torch.ones(N,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dys5piAVYTJo"
      },
      "source": [
        "plt.scatter(X_train.data.numpy(), Y_train.data.numpy(), c='purple', alpha=0.5, label='train')\n",
        "plt.scatter(X_test.data.numpy(), Y_test.data.numpy(), c='yellow', alpha=0.5, label='test')\n",
        "\n",
        "x_real = np.arange(-1, 1, 0.01)\n",
        "y_real = x_real\n",
        "plt.plot(x_real, y_real, c=\"green\", label='true')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N46j3A39h7N-"
      },
      "source": [
        "Модель без дропаут"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1i-h7S1h5G4"
      },
      "source": [
        "N_h = 100\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(1, N_h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(N_h, N_h),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(N_h, 1),\n",
        ")\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lLEoBkph98d"
      },
      "source": [
        "Модель с дропаут"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6O7LgQgX8cx"
      },
      "source": [
        "N_h = 100\n",
        "\n",
        "model_dropout = nn.Sequential(\n",
        "    nn.Linear(1, N_h),\n",
        "    nn.Dropout(0.5), #50 % probability \n",
        "    nn.ReLU(),\n",
        "    torch.nn.Linear(N_h, N_h),\n",
        "    nn.Dropout(0.2), #20% probability\n",
        "    nn.ReLU(),\n",
        "    torch.nn.Linear(N_h, 1),\n",
        ")\n",
        "opt_dropout = torch.optim.Adam(model_dropout.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJzgixN0YzFz"
      },
      "source": [
        "max_epochs = 1500\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    \n",
        "    pred = model(X_train) # look at the entire data in a single shot\n",
        "    loss = loss_fn(pred, Y_train)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    \n",
        "    pred_dropout = model_dropout(X_train)\n",
        "    loss_dropout = loss_fn(pred_dropout, Y_train)\n",
        "    opt_dropout.zero_grad()\n",
        "    loss_dropout.backward()\n",
        "    opt_dropout.step()\n",
        "    \n",
        "    \n",
        "    if epoch % 100 == 0:\n",
        "        \n",
        "        model.eval()\n",
        "        model_dropout.eval()\n",
        "        \n",
        "        test_pred = model(X_test)\n",
        "        test_loss = loss_fn(test_pred, Y_test)\n",
        "        \n",
        "        test_pred_dropout = model_dropout(X_test)\n",
        "        test_loss_dropout = loss_fn(test_pred_dropout, Y_test)\n",
        "        \n",
        "        plt.scatter(X_train.data.numpy(), Y_train.data.numpy(), c='purple', alpha=0.5, label='train')\n",
        "        plt.scatter(X_test.data.numpy(), Y_test.data.numpy(), c='yellow', alpha=0.5, label='test')\n",
        "        plt.plot(X_test.data.numpy(), test_pred.data.numpy(), 'r-', lw=3, label='normal')\n",
        "        plt.plot(X_test.data.numpy(), test_pred_dropout.data.numpy(), 'b--', lw=3,  label='dropout')\n",
        "        \n",
        "        plt.title('Epoch %d, Loss = %0.4f, Loss with dropout = %0.4f' % (epoch, test_loss, test_loss_dropout))\n",
        "        \n",
        "        plt.legend()\n",
        "\n",
        "        model.train()\n",
        "        model_dropout.train()\n",
        "        \n",
        "        plt.pause(0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWm5UYtfihwf"
      },
      "source": [
        "Видим, что нейросеть без dropout сильно переобучилась"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS51YdTJDjL8"
      },
      "source": [
        "### Dropconnect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WawTXM7DjL8"
      },
      "source": [
        "Будем занулять не нейроны, а веса. Фактически - для каждого батча обрубаем часть связей между нейронами"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9iPSOJvDjL8"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/dropconnect.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njzjYiMpDjL8"
      },
      "source": [
        "Drop Connect случайным образом отбрасывая веса, а не активации с вероятностью *p*.\n",
        "\n",
        "DropConnect похож на Dropout, поскольку он вводит динамическую разреженность в модель , но отличается тем, что разреженность зависит от весов *W*, а не от выходных векторов слоя. Другими словами, полностью связанный слой с DropConnect становится разреженно связанным слоем, в котором соединения выбираются случайным образом на этапе обучения. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXsoS5S4DjL8"
      },
      "source": [
        "В принципе, вариантов зануления чего-то в нейросетке можно предложить великое множество, в разных ситуациях будут работать разные ([в этом списке](https://paperswithcode.com/methods/category/regularization)  много Drop...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krgpPIXMDjL9"
      },
      "source": [
        "### DropBlock\n",
        "\n",
        "Например, можно убирать для каждого батча из нейросети случайные блоки из слоев. И это будет работать!\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/dropblock.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76F27CUGDjMG"
      },
      "source": [
        "## Оптимизация весов нейросетей\n",
        "\n",
        "Методов тоже много, расскажем о популярных ([неполный список](https://paperswithcode.com/methods/category/stochastic-optimization))\n",
        "\n",
        "Эти методы реализованы в модуле torch.optim\n",
        "При этом, что важно, подсчет градиентов лосс функции по весам никак не зависит от оптимизаторов из пакета. На прошлых занятиях мы с вами видели, что pytorch вычисляет градиенты автоматически, на основе оптимизаций"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZJE7_pcDjMG"
      },
      "source": [
        "### SGD\n",
        "\n",
        "Обычный стохастичный градиентный спуск. Вы его уже реализовывали. \n",
        "Просто обновляем веса в соответствии с текущем градиентом по ним, домножая градиент на постоянный коэффициент $\\eta$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "grad = evaluate_gradient(loss_fun,x,weights)\n",
        "weights = weights - learning_rate * grad\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/gradient_descent.gif\" width=\"500\">\n",
        "\n",
        "До сих пор является достаточно популярным методом обучения нейросетей, ибо простой, не требует дополнительных параметров кроме $\\eta$, и сам по себе обычно дает не плохие результаты. \n",
        "\n",
        "Если же важна каждая сотая в качестве, то нужно либо использовать его вкупе с другими технологиями (о них - ниже), либо использовать другие способы. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tuAi3QDDjMG"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNVLMf1NDjMG"
      },
      "source": [
        "parameters = torch.randn(10, requires_grad=True)\n",
        "optimizer = optim.SGD([parameters], lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baz3zOr6DjMG"
      },
      "source": [
        "Минусы SGD:\n",
        "\n",
        " 1. Если функция ошибки быстро меняется в одном направлении и медленно - в другом, то это приводит к резким изменениям направления  градиентов и замедляет процесс обучения\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/no-momentum.gif\" width=\"500\">\n",
        "\n",
        "\n",
        " 2. Может застревать в локальных минимумах или седловых точках (точках, где все производные равны 0, но не являющихся минимума/ максимумами). В них градиент равен 0, веса не ообновляются - конец оптимизации. \n",
        "\n",
        "Пример таких точек:\n",
        "\n",
        "\n",
        "Точка 0 у функции $x^3$, не имеющей минимума или максимума вовсе\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/saddle_pointx3.png\" width=\"400\">\n",
        "\n",
        "Или точка 0, 0 у функции y = $x^2 - y^2$\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/saddle_point_x2y2.png\" width=\"400\">\n",
        "\n",
        " 3. Так как мы оцениваем градиенты по малой части выборки, они могут плохо отображать градиент по всей выборке и являться шумными. ВВ результате часть шагов градиентного спуска делаются впустую или во вред. \n",
        " \n",
        " 4. Мы применяем один и тот же learning rate ко всем параметрам, что не всегда разумно. Параметр, отвечающий реедкому классу, будет обучаться медленнее остальных. \n",
        " \n",
        " 5. Просто медленнее сходится"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEkv44pbDjMH"
      },
      "source": [
        "### Momentum\n",
        "\n",
        "Чтобы избежать проблем 1-3, можно использовать momentum - фактически, мы добавляем нашему движению инерции. Если представить наши текущие веса как координаты шарика, и мы этот шарик пытаемся загнать в наиболее глубокую дырку, то у шарика теперь появился вес.\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/ball_1.gif\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "vx = 0 # velocity\n",
        "while True:\n",
        "  dx = evaluate_gradient(loss_fun,x,weights)\n",
        "  vx = vx*rho + dx # rho 0.9 .. 0.99 'friction'\n",
        "  x -= lr *dx\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Сначала пытаемся поменять направление движения шарик с прежнего направления с учетом текущего градиента\n",
        "$$v_{t+1} = \\rho v_t + \\nabla_wL(x, y, W)$$\n",
        "\n",
        "Вычисляем, куда он покатится\n",
        "$$w_{t+1} = w_t - \\eta v_{t+1}$$\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/momentum.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xRhUbbKDjMH"
      },
      "source": [
        "parameters = torch.randn(10, requires_grad=True)\n",
        "optimizer = optim.SGD([parameters], \n",
        "                      momentum=0.9,\n",
        "                      lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR8YR707QC7F"
      },
      "source": [
        "Теперь мы быстрее достигаем локального минимуума и можем выкатываться из совсем неглубоких. Градиентн стал менее подвержен шум, меньше асциллирует\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/no-momentum.gif\" width=\"400\">\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/momentum.gif\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVSG3HNiDjMH"
      },
      "source": [
        "У этого подхода есть одна опасность - мы можем выкатиться за пределы минимума, к которому стремимся, а потом какое-то время к нему возвращаться. \n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/increasing_momentum.gif\" width=\"700\">\n",
        "\n",
        "\n",
        "Чтобы с этим бороться, предложен другой способ подсчета момента инерции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kk7lJtYDjMH"
      },
      "source": [
        "### NAG (Nesterov momentum)\n",
        "\n",
        "Будем сначала смещаться в сторону, куда привел бы нас наш накопленный градиент, там считать новый градиент и смещаться по нему. \n",
        "В результате перескоки через минимум будут менее значительными и мы будем быстрее сходиться\n",
        "\n",
        "$$v_{t+1} = \\rho v_t +  \\nabla_w L(w + \\rho v_t )$$\n",
        "\n",
        "$$w_{t} = w_{t-1} - \\eta v_{t} $$\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/nesterov_momentum.png\" width=\"800\">\n",
        "\n",
        "На практике эту формулу все равно можно записать так, чтобы задача вычисления градиента ложилась не на сам оптимизатор.\n",
        "([реализация в pytorch](https://github.com/pytorch/pytorch/blob/master/torch/optim/_functional.py))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pHBlfRaDjMH"
      },
      "source": [
        "parameters = torch.randn(10, requires_grad=True)\n",
        "optimizer = optim.SGD([parameters],\n",
        "                      momentum=0.9,\n",
        "                      nesterov=True,\n",
        "                      lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S78gLn_zDjMH"
      },
      "source": [
        "### Adaptive Learning Rate\n",
        "\n",
        "Оба озвученных улучшения никак не борются с тем, что некоторые параметры нашей нейросети могут получать информацию для обновления реже, чем остальные. Например, фильтр, отличающий сиамских кошек от остальных, будет работать только тогда, когда в выборке попадутся конкретно сиамские кошки. А их может быть и не очень много, по батчам мы их не размажем (либо батчей поолучится очень много, если нарошно балансировать)\n",
        "\n",
        "Другая ситуация - учим нейросеть на словах из русского языка. Есть веса, отвечающие за редкие слова, например \"молвить\". Вы часто в языке встречаете слово молвить? Молвят устами, а они тоже встречаются не часто. В результате, если learning rate постояннен, то мы выучим параметры для слова молвить плохо. \n",
        "\n",
        "Единственный путь - завести для каждого параметра индивидуальный learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U8o7eU4DjMH"
      },
      "source": [
        "### Adagrad\n",
        "\n",
        "Будем хранить для каждого параметра сумму квадратов его градиентов. \n",
        "\n",
        "И будем добавлять наш градиент с коэффициентом, обратно пропорциональным корню из это суммы (фактически - некая аппроксимация средней амплитуды градиента нашего параметра). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3bKC0w6DjMI"
      },
      "source": [
        "$$ w = w - \\eta \\frac{l}{\\sqrt{G} + e} \\odot (\\nabla_w L(x,y,W)) $$\n",
        "\n",
        "$$ G = \\sum_{t=1}^T \\nabla_w L(x,y,w_t)^2 $$\n",
        "\n",
        "\n",
        "```\n",
        "grad_squared = 0 # it's tensor !\n",
        "while True:\n",
        "  dx = evaluate_gradient(loss_fun,x,weights)\n",
        "  grad_squared += dx*dx\n",
        "  x = lr*dx / (sqrt(grad_squared) + 1e-7)\n",
        "```  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WckRwSnyE0hy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VovcfpXDjMI"
      },
      "source": [
        "В результате, если градиент у нашего веса часто большой - коэффициент будет уменьшаться. \n",
        "\n",
        "Единственная проблема - при такой формуле наш learning rate неминуемо в конце концов затухает (так как сумма квадратов не убывает)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cYnILepDjMI"
      },
      "source": [
        "parameters = torch.randn(10, requires_grad=True)\n",
        "optimizer = optim.Adagrad([parameters])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkJ8nkxaDjMI"
      },
      "source": [
        "### RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UazgOLFDjMI"
      },
      "source": [
        "Давайте устроим \"забывание\" предыдущих квадратов градиентов. Просто будем домножать их на некий коэффициент меньше 0\n",
        "\n",
        "\n",
        "$$v_t = \\alpha v_{t-1} + (1-\\alpha) (\\nabla_w L(x,y,w_t))^2$$\n",
        "\n",
        "$$w = w - \\frac{\\eta}{\\sqrt{v_t }+ e} \\odot \\nabla_w L(x,y,W)$$\n",
        "\n",
        "\n",
        "```\n",
        "grad_squared = 0\n",
        "decay_rate =0.9\n",
        "while True:\n",
        "  dx = evaluate_gradient(loss_fun,x,weights)\n",
        "  grad_squared += decay_rate*grad_squared + (1-decay_rate)dx*dx\n",
        "  x = lr*dx / (sqrt(grad_squared) + 1e-7)\n",
        "```  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9MYUTUKDjMI"
      },
      "source": [
        "parameters = torch.randn(10, requires_grad=True)\n",
        "optimizer = optim.RMSprop([parameters], alpha=0.99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NgCSsZ6DjMI"
      },
      "source": [
        "### Adam\n",
        "\n",
        "Один из самых популярных адаптивных оптимизаторов является Adam. \n",
        "Получается он за счет объединения идеи с инерцией и идеи с суммой квадратов. \n",
        "\n",
        "$$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) (\\nabla_w L(x,y,w_t)) $$\n",
        "$$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla_w L(x,y,w_t)^2) $$\n",
        "$$ w = w - \\eta \\cdot \\frac{m_t}{\\sqrt{v_t} + e} $$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTNu_0q5DjMI"
      },
      "source": [
        "Чтобы в начале у нас получались очень большие шаги, будем дополнительно модицифировать инерцию и сумму квадратов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7AzNWehDjMJ"
      },
      "source": [
        "$$ m_t = \\frac{m_t}{1-\\beta_1^t} $$\n",
        "\n",
        "$$ v_t = \\frac{v_t}{1-\\beta_2^t} $$\n",
        "\n",
        "Псевдокод:\n",
        "\n",
        "```\n",
        "beta1 = 0.9\n",
        "beta2 =0.9\n",
        "\n",
        "first_momentum = 0\n",
        "second_momentum = 0\n",
        "while True:\n",
        "  dx = evaluate_gradient(loss_fun,x,weights)\n",
        "  first_momentum = first_momentum * beta1 +  (1-beta1)*dx\n",
        "  second_momentum = second_momentum*beta2 + \n",
        "  (1-beta2)*dx*dx\n",
        "  x = lr*first_momentum / (sqrt(second_momentum) + 1e-7)\n",
        "```  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51RQcUYHIEin"
      },
      "source": [
        "Реальный Pytorch код"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXgBTFnADjMJ"
      },
      "source": [
        "parameters = torch.randn(10, requires_grad=True)\n",
        "optimizer = optim.Adam([parameters], \n",
        "                       betas=(0.9, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iYnN8K1NCG9"
      },
      "source": [
        "model = SimpleMNIST_NN_Init_Batchnorm(n_layers=3).cuda()\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "writer = get_summary_writer(\"adam\")\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgizuiZ9NJj-"
      },
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    train_epoch(model,\n",
        "              optimizer, \n",
        "              criterion,\n",
        "              train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    model.eval()\n",
        "    validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_uMDrqSDjMJ"
      },
      "source": [
        "### L2-loss\n",
        "\n",
        "Все оптимизаторы так же поддерживают возможность добавления к ним напрямую L2-loss, коэффициент перед этим лоссом -  $\\textrm{weight_decay}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpauEvnMDjMJ"
      },
      "source": [
        "parameters = torch.randn(10, \n",
        "                         requires_grad=True)\n",
        "optimizer = optim.RMSprop([parameters], \n",
        "                          alpha=0.99, \n",
        "                          weight_decay=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDODMDSKDjMJ"
      },
      "source": [
        "Нюанс в том, что в Adam L2-loss учитывается не совсем верно. Потому есть поправленная версия Adam - AdamW. Но не факт, что она всегда лучше работает"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXPPVqr6DjMJ"
      },
      "source": [
        "### Сравнение оптимизаторов \n",
        "\n",
        "У каждого из предложенных оптимизаторов есть минусы и плюсы \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-8k0D0sDjMJ"
      },
      "source": [
        "#### Методы с инерцией сходятся к решению более плавно, но могут \"перелетать\"\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/opt_smooth.gif\" width=\"250\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giRXlNUkDjMK"
      },
      "source": [
        "#### Методы с адаптивным learning rate быстрее сходятся, более стабильны и меньше случайно блуждают\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/adaptive_stability.gif\" width=\"250\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0bqxngCDjMK"
      },
      "source": [
        "####  Алгоритмы без адаптивного learning rate сложнее выбираются из локальных минимумом\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/local_minima.gif\" width=\"450\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHpSvwm8DjMK"
      },
      "source": [
        "#### Алгоритмы с инерцией осцилируют в седловых точках прежде чем найти верный путь\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/saddle_point.gif\" width=\"450\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0KdfvmtDjMK"
      },
      "source": [
        "## Режимы обучения\n",
        "\n",
        "Нам не обязательно поддерживать один и тот же learning rate в течении всего обучения. Более того, для того же SGD есть гарантии, что если правильно подобрать схему уменьшения learning rate, он сойдется к глобальномуу оптимуму.\n",
        "\n",
        "\n",
        "Мы можем менять learning rate по некоторым правилам.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuoyNpboDjMK"
      },
      "source": [
        "### Ранняя остановка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFfuRePxDjMK"
      },
      "source": [
        "Можем использовать критерий ранней остановки - когда лосс на валидационной выборке не улучшается patience эпох, умножаем learning rate на некий factor\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/early_stopping.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It0c2OpsDjMK"
      },
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiS1gN0gDjML"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, \n",
        "                              'min', \n",
        "                              factor=0.1,\n",
        "                              patience = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QXtZl55Odv5"
      },
      "source": [
        "Применим к нашей модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VoNmqylNwb5"
      },
      "source": [
        "model = SimpleMNIST_NN_Init_Batchnorm(n_layers=3).cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "writer = get_summary_writer(\"reduce_lr_on_plateu\")\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()\n",
        "scheduler = ReduceLROnPlateau(optimizer, \n",
        "                              'min', \n",
        "                              factor=0.1,\n",
        "                              patience = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oDp3p2lN5B1"
      },
      "source": [
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    train_epoch(model,\n",
        "              optimizer, \n",
        "              criterion,\n",
        "              train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    model.eval()\n",
        "    val_loss = validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)\n",
        "    scheduler.step(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4fKdujIDjML"
      },
      "source": [
        "### Домножать learning rate на alpha каждую эпоху"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk9eJfAiDjML"
      },
      "source": [
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
        "                                            step_size=2,\n",
        "                                            gamma=0.1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGjFCyEsDjML"
      },
      "source": [
        "### Cyclical learning schedule\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8w0FVh6DjML"
      },
      "source": [
        "Мы можем все время понижать learning rate, а делать это циклически - то понижать, то повышать. Делать это можно по-разному:\n",
        "\n",
        "\n",
        "1. Постоянно оставлять одни и те же границы\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/cycle_lr1.png\" width=\"350\">\n",
        "\n",
        "2. Уменьшать верхнюю границу в сколько-то раз\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/cycle_lr2.png\" width=\"350\">\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/cycle_lr3.png\" width=\"350\">\n",
        "\n",
        "\n",
        "Нюанс - здесь мы ОБЯЗАТЕЛЬНО должны хранить модель с лучшим качеством. Такая оптимизация не гарантирует, что в конце модель будет лучше, чем на каком-то промежуточном шаге"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN6znykKDjML"
      },
      "source": [
        "#### Подбираем границы learning rate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JbQrwTWDjML"
      },
      "source": [
        "1. просто берем и либо делаем для каждого learning rate минимизацию одну эпоху (чтобы всю выборку увидеть), нейросетку для каждого значения learning rate инициализируем заново\n",
        "\n",
        "Это долго\n",
        "\n",
        "Потому  делаем \"магию\":\n",
        "\n",
        "2. На каждое значение learning rate у нас будет лишь один батч, нейросетку для каждого learning rate не меняем. И это часто работает, так как нам нужна просто грубая прикидка\n",
        "\n",
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lr_selection.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInpvbGADjML"
      },
      "source": [
        "Полученные значения (оптимальные границы) и используем в качестве высокого и низкого порога на learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb-hODw8RADV"
      },
      "source": [
        "start_lr = 1e-8\n",
        "end_lr = 10\n",
        "lr_find_epochs = 2\n",
        "steps = lr_find_epochs * len(train_loader)\n",
        "smoothing = 0.05"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PnurLHNPjIU"
      },
      "source": [
        "model = SimpleMNIST_NN_Init_Batchnorm(n_layers=3).cuda()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebBp8CksRQp8"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1yjCzt1O3PD"
      },
      "source": [
        "lrs = []\n",
        "losses = []\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-8)\n",
        "lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / (steps))\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "for epoch in range(lr_find_epochs):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        X, y = batch\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        loss = loss.detach().cpu().numpy()\n",
        "        if len(losses) > 1:\n",
        "          loss = smoothing  * loss + (1 - smoothing) * losses[-1]\n",
        "        losses.append(loss)\n",
        "        lr_step = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "        lrs.append(lr_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMbYAd8Rusx"
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.title(\"LR range selection\")\n",
        "plt.plot(np.log10(lrs), losses)\n",
        "plt.xlabel(\"Learning rate (log10)\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnZS1v0_UjeT"
      },
      "source": [
        "На этом графике нам нужен минимум. Это в районе 1e-1. Делим это число на 10. \n",
        "Нижняя граница, стало быть - 1e-2.\n",
        "\n",
        "Снижение лосса начинается с learning rate в районе 1e-4. Его делим на 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT-S65y9QjTO"
      },
      "source": [
        "min_lr = 1e-4 / 6\n",
        "max_lr = 1e-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOSUkCs733Rv"
      },
      "source": [
        "Этот scheduler надоо применять после каждого батча. Потому перепишем train_epoch_sh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wei_lN13V1yn"
      },
      "source": [
        "def train_epoch_sh(model, \n",
        "                optimizer,\n",
        "                scheduler,\n",
        "                criterion, \n",
        "                train_loader, \n",
        "                model_name,\n",
        "                steper, \n",
        "                writer):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        X, y = batch\n",
        "        X, y = X.cuda(), y.cuda()\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, y)\n",
        "        writer.add_scalar(f\"{model_name}_train_loss\", \n",
        "                      loss.cpu().detach().numpy(),\n",
        "                      global_step= steper.global_step)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steper.step()\n",
        "        scheduler.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUlJ-OhIDjMM"
      },
      "source": [
        "#### Запускаем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfj_0Q3GDjMM"
      },
      "source": [
        "model = SimpleMNIST_NN_Init_Batchnorm(n_layers=3).cuda()\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "train_steper = Steper()\n",
        "test_steper = Steper()\n",
        "writer = get_summary_writer(\"sgd_cycle_lr\")\n",
        "optimizer = optim.SGD(model.parameters(), \n",
        "                      lr=min_lr)\n",
        "\n",
        "scheduler = optim.lr_scheduler.CyclicLR(optimizer,\n",
        "                                        base_lr=min_lr, \n",
        "                                        max_lr=max_lr,\n",
        "                                        mode=\"triangular\") # first case\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z7cuEqGXe_2"
      },
      "source": [
        "По хорошемуу, в коде ниже мы на каждой эпохе сохраняем лучшую модель. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t714T3MjVTSh"
      },
      "source": [
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    train_epoch_sh(model,\n",
        "              optimizer, \n",
        "              scheduler,\n",
        "              criterion,\n",
        "               train_loader,\n",
        "              \"cross_entropy\",\n",
        "              train_steper,\n",
        "              writer)\n",
        "    model.eval()\n",
        "    val_loss = validate(model, \n",
        "           test_loader, \n",
        "           \"cross_entropy\", \n",
        "           test_steper,\n",
        "           writer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHEzTCSrDjMM"
      },
      "source": [
        "## Neural Network WarmUp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmrzqCArDjMM"
      },
      "source": [
        "Также, для достаточно больших нейронных практикуют следующую схему (**gradual warmup**, [изначальная статья](https://arxiv.org/pdf/1706.02677.pdf) ):\n",
        "\n",
        "Поставить изначальный learning rate значительно ниже того, с которого мы обычно начинаем обучение. За несколько эпох, например, 5, довести learning rate от этого значения до требуемого. За счет наша нейросет лучше \"адаптируется\" к нашим данным. \n",
        "\n",
        "Также такой learning schedule позволяет адаптивным оптимизаторам лучше оценить значения learning rate для разных параметров"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1EeOGhIDjMM"
      },
      "source": [
        "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/warmup.png\" width=\"1000\">\n",
        "\n",
        "kn на картинке - это размер одного батча"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej-O846JDjMM"
      },
      "source": [
        "### Взаимодействие learning schedule и адаптивного изменения learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMt_cvEADjMM"
      },
      "source": [
        "И то, и другое меняет learning rate, learning scheduler - глобально, а адаптивные оптимизаторы - для каждого веса отдельно \n",
        "\n",
        "Часто их применяют вместе, особенно в случае критерия ранней остановки и WarmUp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPQ14h2NDjMN"
      },
      "source": [
        "Однако в случае циклического режима обучения так делают не всегда. Дело в том, что одна из его задач - как раз избежать использования адаптивных оптимизаторов, требующих бОльше памяти и дополнительных вычислений. \n",
        "\n",
        "Однако никаких препятствий к использованию того же Adam в компании вместе с циклическим режимом обучения нет. В [исходной статье](https://arxiv.org/pdf/2004.02401.pdf) так делают. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCODmjwfDjMN"
      },
      "source": [
        "# Ссылки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq13IBLHDjMN"
      },
      "source": [
        "[Оптимизаторы1](https://theaisummer.com/optimization/)\n",
        "\n",
        "[Оптимизаторы2](https://mlfromscratch.com/optimizers-explained/#adam)\n",
        "\n",
        "[Батч-нормализация](https://theaisummer.com/normalization/)\n",
        "\n",
        "[Циклический learning rate](https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee)\n",
        "\n",
        "[Разные функции активации, затухающие и взрывающщиеся градиенты и т.д](https://mlfromscratch.com/activation-functions-explained/#/)\n",
        "\n",
        "[Визуализация разных оптимизаторов в ipynb, но на tensorflow](https://nbviewer.jupyter.org/github/ilguyi/optimizers.numpy/blob/master/optimizer.tf.all.opt.plot.ipynb)"
      ]
    }
  ]
}