{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение. Инициализация, оптимизаторов, batch-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение сходимости\n",
    "\n",
    "Ускорение сходимости\n",
    "\n",
    "\n",
    "* Инициализация (Xavier, He)\n",
    "\n",
    "* Нормализация (Batch Normalization, Layer normalization)\n",
    "\n",
    "Борьба с переобучением\n",
    "\n",
    "\n",
    "* Регуляризация (Dropout, DropConnect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier (Glorot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим нечетную функцию с единичной производной в нуле в\n",
    "качестве активации (напр. tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотим начать из линейного региона, чтобы избежать\n",
    "затухающих градиентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-04.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-05.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим ReLU в качестве активации:\n",
    "* Функция не симметрична\n",
    "* Не дифференцируема в нуле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU или Rectified Linear Unit стала довольно популярной в последние годы. Она вычисляет функцию f(x) = max(0,x), то есть просто выдаёт значения «ноль» и «не ноль». Это решает проблему обнуления градиента для положительных чисел. Кроме того, ReLU очень просто вычисляется: примерно в шесть раз быстрее сигмоиды и тангенса. Однако, в ней снова отсутствует нулевое центрирование.\n",
    "\n",
    "Другой очевидный недостаток — градиент по-прежнему «умирает» при отрицательных входных данных. Это может привести к тому, что половина нейронов будет неактивна и не сможет обновляться. \n",
    "\n",
    "Проблему можно попробовать решить, задав более низкую скорость обучения и подобрав другие весовые коэффициенты. Или использовать модификации ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-06.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier против He для ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-07.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда было обучение более глубоких сети, в которых использовался ReLU, He et. и др. было обнаружено, что 30-слойный CNN, использующий инициализацию Xavier, полностью застопорился и не научился вообще Однако, когда одна и та же сеть была инициализирована в соответствии с трехэтапной процедурой, описанной выше, она имела значительно большую конвергенцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-08.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сходимость 30-слойCNN благодаря Kaiming init. Кредит\n",
    "\n",
    "ссылка https://arxiv.org/pdf/1502.01852.pdf\n",
    "\n",
    "Мораль этой истории для нас заключается в том, что любая сеть, которую мы обучаем с нуля, особенно для приложений компьютерного зрения, почти наверняка будет содержать функции активации ReLU и иметь несколько уровней. В таких случаях Kaiming должен быть нашей стратегией инициации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот тут есть вопрос-ответ с кодом, может понадобится\n",
    "\n",
    "**Ручной инициализатор Xavier: какие значения для lrelu и relu**\n",
    "\n",
    "В качестве продолжения ответа (не выбранного) в How to do Xavier initialization on TensorFlow: у кого-нибудь есть идея, какие значения использовать в relu и особенно leaky relu?\n",
    "\n",
    "https://coderoad.ru/39231032/%D0%A0%D1%83%D1%87%D0%BD%D0%BE%D0%B9-%D0%B8%D0%BD%D0%B8%D1%86%D0%B8%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%82%D0%BE%D1%80-Xavier-%D0%BA%D0%B0%D0%BA%D0%B8%D0%B5-%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F-%D0%B4%D0%BB%D1%8F-lrelu-%D0%B8-relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ортогональная инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Инициализатор, который генерирует случайную ортогональную матрицу*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем ортогональную матрицу весов W: WWT = I. Тогда:\n",
    "* ∥*Wix*∥ = ∥*x*∥ — норма сохраняется\n",
    "* ⟨*Wi, Wj*⟩ = *δij* — все нейроны делают «разные» преобразования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать для сверточных слоев?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-10.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризация используется для упрощения модели, чтобы избежать переобучения и заставить её корректно работать на новых данных. Но чаще всего в машинном обучении применяется эффективный и простой алгоритм Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-12.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый раз, выполняя прямой проход по слою, мы устанавливаем некоторые значения активаций нулевыми (нейроны выбираются случайно). В результате мы получим несколько «подсетей» и будем поочерёдно оценивать их эффективность, что делает dropout похожим на ансамбль методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* С вероятностью *p* занулим выход нейрона (например, *p* = 0.5) \n",
    "* В test-time домножаем веса на вероятность сохранения\n",
    "* Не стоит выкидывать нейроны последнего слоя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Самая простая реализация для трёхслойной нейросети выглядит следующим образом:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Vanilla Dropout: не рекомендуется использовать на практике \"\"\"\n",
    "\n",
    "p = 0.5 # вероятность оставить нейрон активным. Чем выше, тем меньше dropout\n",
    "\n",
    "def train_step(X):\n",
    "\n",
    "  \"\"\" X - данные \"\"\"\n",
    "\n",
    "  # прямой проход по трёхслойной нейросети\n",
    "\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "\n",
    "  U1 = np.random.rand(*H1.shape) < p # первый dropout\n",
    "\n",
    "  H1 *= U1 # drop!\n",
    "\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "\n",
    "  U2 = np.random.rand(*H2.shape) < p # второй dropout\n",
    "\n",
    "  H2 *= U2 # drop!\n",
    "\n",
    "  out = np.dot(W3, H2) + b3\n",
    "\n",
    "  # обратный проход: вычисление градиентов... (не показано)\n",
    "\n",
    "  # обновление параметров... (не показано)\n",
    "\n",
    "def predict(X):\n",
    "\n",
    "  # ансамблевый прямой проход\n",
    "\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # масштабируем активации!\n",
    "\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # масштабируем активации!\n",
    "\n",
    "  out = np.dot(W3, H2) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нежелательное свойство представленной выше схемы состоит в том, что мы должны масштабировать активации во время прогнозирования. Поскольку производительность тестирования критически важна, лучше всего использовать инвертированный dropout, который выполняет масштабирование во время обучения. Кроме того, если мы захотим убрать dropout из кода, функция прогнозирования останется без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "Inverted Dropout: Рекомендуемая реализация\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "p = 0.5 # вероятность оставить нейрон активным. Чем выше, тем меньше dropout\n",
    "\n",
    "def train_step(X):\n",
    "\n",
    "  # прямой проход по трёхслойной нейросети\n",
    "\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "\n",
    "  U1 = (np.random.rand(*H1.shape) < p) / p # первый dropout. Заметьте /p!\n",
    "\n",
    "  H1 *= U1 # drop!\n",
    "\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "\n",
    "  U2 = (np.random.rand(*H2.shape) < p) / p # второй dropout. Заметьте /p!\n",
    "\n",
    "  H2 *= U2 # drop!\n",
    "\n",
    "  out = np.dot(W3, H2) + b3\n",
    "\n",
    "  # обратный проход: вычисление градиентов... (не показано)\n",
    "\n",
    "  # обновление параметров... (не показано)\n",
    "\n",
    "def predict(X):\n",
    "\n",
    "  # ансамблевый прямой проход\n",
    "\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) # масштабирование не нужно\n",
    "\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "\n",
    "  out = np.dot(W3, H2) + b3\n",
    "\n",
    "Dropout очень полезен тем, что с его помощью можно менять силу регуляризации, просто варьируя значение параметра p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout, мотивация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout изменил концепцию обучения всех весов вместе, чтобы узнать долю весов в сети на каждой итерации обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной из основных проблем в изучении больших сетей является совместная адаптация. В такой сети, если все весовые коэффициенты изучаются вместе, обычно некоторые соединения будут иметь больше возможностей прогнозирования, чем другие.\n",
    "\n",
    "В таком сценарии, поскольку сеть обучается итеративно, эти мощные соединения изучаются больше, в то время как более слабые игнорируются. За многие итерации обучается только часть соединений узлов. А остальные перестают участвовать.\n",
    "\n",
    "Это явление называется со-адаптацией. Этого нельзя было предотвратить с помощью традиционной регуляризации, такой как L1 и L2. Причина в том, что они также упорядочены на основе прогнозирующих возможностей соединений. Благодаря этому они становятся близки к детерминистическим в выборе и отклонении весов. И, таким образом, снова сильный становится сильнее, а слабый становится слабее.\n",
    "\n",
    "Основные последствия этого были:расширение размера нейронной сети не поможет, Следовательно, размер нейронных сетей и, следовательно, точность стали ограниченными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем пришел Dropout. Новый подход к регуляризации. Это решило совместную адаптацию. Теперь мы могли бы строить более глубокие и широкие сети. И используйте силу предсказания всего этого.\n",
    "\n",
    "На этом фоне давайте погрузимся в математику отсева. Вы можете перейти непосредственно кОтсев, эквивалентный регуляризованной сетираздел для выводов,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Борьба с соадаптацией – нейроны больше не могут рассчитывать на наличие соседей\n",
    "* Биология: не все гены родителей будут присутсвовать у\n",
    "потомков\n",
    "* Усреднение большого (2^*n*) числа моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-13.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure:** Выученные признаки на MNIST (автокодировщик с одним скрытым слоем и ReLU в качестве активации). \n",
    "\n",
    "Слева: без Dropout, справа – с Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Зачем мне калечить (cripple) нейронную сеть?**\n",
    "\n",
    "\n",
    "Как бы странно это ни звучало, отмена способности некоторых нейронов учиться во время тренировки на самом деле направлена на то, чтобы получить более обученные нейроны и сократить переобучение.\n",
    "\n",
    "Таким образом, мы получаем приблизительный результат усреднения более простых обученных моделей, которые в противном случае заняли бы гораздо больше времени и вычислительной мощности, чтобы обучаться по очереди. Но это не единственная причина. На самом деле, тренировка сетей таким образом не только помогает им адаптироваться, уравновешивая их слабые и сильные стороны, но и гарантирует, что инкапсулируемые ими функции хорошо работают со случайно выбранными подмножествами усвоенных функций других.\n",
    "\n",
    "Это приводит к появлению более требовательных нейронов, которые пытаются обойти сложные, индивидуальные особенности, которые склонны плохо обобщать, и сохраняют больше полезной информации самостоятельно. На следующем рисунке (извлеченном из статьи Dropout: A Simple Way to Prevent Neural Networks from Overfitting) мы находим сравнение признаков, изученных в наборе данных MNIST с одним автоэнкодером скрытого слоя, имеющим 256 выпрямленных линейных единиц без отсева (слева), и признаков, изученных той же структурой с использованием отсева в ее скрытом слое с p=0,5 (справа).\n",
    "\n",
    "В то время как первый показывает неструктурированные, беспорядочные паттерны, которые невозможно интерпретировать, второй явно демонстрирует целенаправленное распределение веса, которое обнаруживает штрихи, края и пятна самостоятельно, нарушая их взаимозависимость с другими нейронами для выполнения этой работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник https://quantdare.com/dropout-in-feed-forward-neural-networks/\n",
    "\n",
    "перевод яндекс переводчика, при необходимости перепишу человеческим текстом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropconnect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-14.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Зануляем не выходы нейронов, а каждый вес по отдельности**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Connect обобщает Drop out, случайным образом отбрасывая веса, а не активации с вероятностью *1 — p*. DropConnect похож на Dropout, поскольку он вводит динамическую разреженность в модель , но отличается тем, что разреженность зависит от весов *W*, а не от выходных векторов слоя. Другими словами, полностью связанный слой с DropConnect становится разреженно связанным слоем, в котором соединения выбираются случайным образом на этапе обучения. Обратите внимание, что это не эквивалентно *W* lkz установкb фиксированной разреженной матрицы во время обучения.\n",
    "\n",
    "Для слоя Drop Connect выходные данные задаются следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-14-1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где *r* выход слоя, *u* это вход в слой, *W* это весовые параметры и *M* это двоичная матрица, кодирующая информацию о соединении где *M(ij) ~ Bernoulli(p)*. Каждый элемент матрицы *M* создается независимо для каждого примера во время обучения, по существу создавая различные связи для каждого увиденного примера. Кроме того, предубеждения также маскируются во время обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник https://paperswithcode.com/method/dropconnect\n",
    "\n",
    "перевод яндекс переводчика, при необходимости перепишу человеческим текстом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим классическую нейронную сеть с несколькими слоями. Каждый слой имеет множество входов и множество выходов. Сеть обучается методом обратного распространения ошибки, по батчам, то есть ошибка считается по какому-то подмножестве обучающей выборки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартный способ нормировки — для каждого k рассмотрим распределение элементов батча. Вычтем среднее и поделим на дисперсию выборки, получив распределение с центром в 0 и дисперсией 1. Такое распределение позволит сети быстрее обучатся, т.к. все числа получатся одного порядка. Но ещё лучше ввести две переменные для каждого признака, обобщив нормализацию следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входы: Значения x в мини-батче B = {xi}\n",
    "m\n",
    "i=1;\n",
    "\n",
    "Параметры: γ, β\n",
    "Выход: {yi = BNγ,β(xi)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-19.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим среднее, дисперсию. Эти параметры будут входить в алгоритм обратного распространения ошибки.\n",
    "Тем самым получаем batch normalization слой с 2*k параметрами, который и будем добавлять в архитектуру предложенной сети для распознавания лиц.\n",
    "\n",
    "\n",
    "На вход в моей задаче подаётся черно-белое изображение лица человека размером 50x50 пикселей. На выходе имеем 14000 вероятностей классов. Класс с максимальной вероятностью считается результатом предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мотивация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Обычно наблюдается более быстрая сходимость при\n",
    "декорелированных входах\n",
    "* Whitening: xˆ = Cov[x]−1/2(x − E[x])\n",
    "\n",
    "* Нормализация: xˆ\n",
    "(k) =\n",
    "x\n",
    "(k)−E[x\n",
    "(k)\n",
    "√\n",
    "]\n",
    "V ar[x(k)]\n",
    "для каждой размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Батч-нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Covariate shift: изменение распределения входов во время\n",
    "обучения\n",
    "* Цель — уменьшить covariate shift скрытых слоев\n",
    "* Нормализуем входы в каждый слой xˆ\n",
    "(k) =\n",
    "x\n",
    "(k)−E[x\n",
    "(k)\n",
    "√\n",
    "]\n",
    "D[x(k)]\n",
    "* Статистики Ex и Dx оценим для каждого мини-батча\n",
    "\n",
    "\n",
    "\n",
    "**? Почему этот метод плох для сетей с сигмоидами?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Сигмоиды становятся почти линейными ⇒ линейная модель :(\n",
    "* Доп. параметры: y\n",
    "^(k) = γ\n",
    "^(k)xˆ\n",
    "^(k) + β\n",
    "^(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вычислить градиент при помощи chain rule\n",
    "\n",
    "Важно помнить, что *μB* и *σ\n",
    "2\n",
    "B*\n",
    "не являются константами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-20.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во время предсказания батч-нормализация является линейным\n",
    "слоем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-21.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E[*x*] и D[*x*] вычисляются по всему обучающему множеству.\n",
    "На практике статистики вычисляются во время обучения\n",
    "экспоненциальным средним: *Ei+1* = (1 − *α*)*Ei* + *αEB*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchnorm как регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-22.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При увеличении весов в *a* раз, градиент выхода слоя по входу не меняется, а градиент по весам уменьшается в *a* раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит помнить, что с батч-нормализацией:\n",
    "* Надо убрать смещения\n",
    "* Другое расписание learning rate: большее значение в начале обучения и быстрое уменьшение в процессе обучения\n",
    "* Уменьшить силу Dropout и *L2* регуляризации\n",
    "* Перемешивать обучающую выборку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lля изображений: нормализация каждого канала (одинаковые среднее и дисперсия вдоль пространственных размерностей)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-24.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого момента мы рассматривали способы навигации по поверхности потерь нейронной сети с использованием импульса и адаптивных скоростей обучения. Мы также рассмотрели несколько методов инициализации параметров, чтобы минимизироватьаприориуклоны внутри сети. В этом разделе мы рассмотрим, как мы можем манипулировать самими данными, чтобы помочь нашей оптимизации модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При пакетной нормализации происходит усреднение параметров по всему пакету. Например, в случае задачи переноса стилей картин, это вносит много шума. При усреднении теряются индивидуальные характеристики объектов. Поэтому используется более тонкая нормализация — индивидуальная нормализация (англ. instance normalization). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L07_Batch_normalization/img/lecture-25.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Типы нормализации. Ось N — по объектам в пакете, ось C — по картам признаков (channels), оставшаяся ось — по пространственным измерениям объектов, например, ширине и высоте картинки.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница заключается в том, что нормализация происходит по каждому отдельному объекту, а не по всему пакету. Для примера, усреднение происходит по пикселям картины, но не по всем картинам в пакете, как видно на рисунке."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
