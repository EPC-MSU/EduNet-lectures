{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как избежать \"подводных камней\" машинного обучения: руководство для академических исследователей\n",
    "\n",
    "[How to avoid machine learning pitfalls: a guide for academic researchers (Lones, 2021)](https://arxiv.org/abs/2108.02497)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В этой лекции дается краткое описание некоторых распространенных ошибок, возникающих при использовании методов машинного обучения, и того, что можно сделать, чтобы их избежать. Лекция предназначена в первую очередь как руководство для студентов-исследователей и посвящена вопросам, которые особенно актуальны в академических исследованиях, например, необходимости проводить строгие сравнения и делать обоснованные выводы. Однако большинство уроков применимо и к более широкому использованию ML.\n",
    "\n",
    "Мы рассмотрим пять этапов процесса машинного обучения:\n",
    "* что нужно сделать до построения модели\n",
    "* как надежно строить модели\n",
    "* как надежно оценивать модели\n",
    "* как справедливо сравнивать модели\n",
    "* как сообщать о результатах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прежде чем приступить к созданию моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это нормально - испытывать непреодолимое желание начать обучение и оценку моделей, но важно найти время, чтобы подумать о целях проекта, полностью понять данные, которые будут использоваться для достижения этих целей, рассмотреть любые ограничения данных, которые необходимо устранить, и понять, что уже было сделано в вашей области. Если этого не сделать, есть риск получить результаты, которые трудно опубликовать, или модели, которые не подходят для решения поставленной задачи.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разберитесь с тем, какую задачу/проблему вы хотите решить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При оценке того, является ли задача решаемой, стоит сначала спросить себя: а нужно ли ее решать? И если да, то нужно ли ее решать с помощью машинного обучения?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучите литературу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы, вероятно, не первый, кто использует ML для решения конкретной проблемной области, поэтому важно понимать, что было сделано и что не было сделано ранее. Другие люди работали над той же проблемой - это не плохо; академический прогресс обычно представляет собой итерационный процесс, когда каждое исследование предоставляет информацию, которая может послужить основой для следующего. Может быть неприятно обнаружить, что кто-то уже исследовал вашу замечательную идею, но, скорее всего, они оставили открытыми многие пути исследования, и их предыдущая работа может быть использована в качестве обоснования вашей работы. Игнорировать предыдущие исследования - значит потенциально упустить ценную информацию. Например, возможно, кто-то уже пробовал предложенный вами подход и нашел фундаментальные причины, по которым он не сработает (и тем самым избавил вас от нескольких лет разочарования), или же частично решил проблему таким образом, что вы можете от нее оттолкнуться. Поэтому важно провести обзор литературы до начала работы; если вы сделаете это слишком поздно, это может означать, что вам придется объяснять, почему вы повторяете ту же самую тему или не опираетесь на существующие знания, когда вы сядете писать статью.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Потратьте время на понимание своих данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конечном итоге вы захотите опубликовать свою работу. Это гораздо легче сделать, если ваши данные получены из надежного источника, собраны с использованием надежной методологии и имеют хорошее качество. Например, если вы используете данные, собранные на интернет-ресурсе, убедитесь, что вы знаете, откуда они взяты. Описаны ли они в статье? Если да, посмотрите на документ; убедитесь, что он был опубликован в авторитетном месте, и проверьте, упоминают ли авторы какие-либо ограничения данных. Не думайте, что если набор данных использовался в ряде работ, то он хорошего качества - иногда данные используются только потому, что их легко достать, а некоторые широко используемые наборы данных, как известно, имеют существенные ограничения (см. [Paullada et al., 2020]((https://arxiv.org/abs/2012.05345))). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, при исследовании категорий `faces` в ImageNet [Deng et al., 2009](https://ieeexplore.ieee.org/document/5206848), [Crawford & Paglen, 2019](https://excavating.ai/) обнаружили миллионы изображений людей, которые были помечены оскорбительными категориями, включая расовые оскорбления и унизительные фразы. В ответ на эту работу, большая часть набора данных ImageNet была удалена [Yang et al., 2020](https://dl.acm.org/doi/abs/10.1145/3351095.3375709). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы обучаете свою модель на плохих данных, то, скорее всего, вы создадите плохую модель: процесс, известный как **garbage in garbage out** (чушь на входе - чушь на выходе). Поэтому всегда начинайте с того, что убедитесь, что ваши данные имеют смысл. Проведите **эксплораторный анализ данных** (см. [Cox, 2017](https://www.oreilly.com/library/view/translating-statistics-to/9781484222560/A426308_1_En_3_Chapter.html)). Ищите недостающие или непоследовательные записи. Гораздо проще сделать это сейчас, до обучения модели, чем потом, когда вы будете пытаться объяснить рецензентам, почему вы использовали плохие данные. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ИДЕИ ДЛЯ ПРИМЕРА? \n",
    "\n",
    "Мои варианты: разобрать датасет с кучей дубликатов и прочей фигни в разных форматах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это важно сделать независимо от того, используете ли вы существующие наборы данных или генерируете новые данные в рамках своего исследования. Если вы генерируете собственные данные, также учитывайте, что исследовательский анализ может быть ценен сам по себе, а результаты этого анализа могут стать важной частью вашей работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не смотрите на *все* свои данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда вы изучаете данные, вполне вероятно, что вы заметите закономерности и сделаете выводы, которые будут направлять ваши усилия по построению модели. Это еще одна веская причина для изучения данных. Однако важно, чтобы вы не делали **непроверяемых** предположений, которые впоследствии будут использованы в вашей модели. Вообще делать предположения - это нормально, но они должны использоваться только для обучения модели, а не для тестирования. \n",
    "\n",
    "Вы должны избегать пристального изучения любых тестовых данных на начальном этапе исследовательского анализа. В противном случае вы можете сознательно или бессознательно сделать предположения, которые ограничат общность вашей модели не поддающимся тестированию способом. К этой теме мы будем возвращаться еще не раз, поскольку утечка информации из тестового набора в процесс обучения является распространенной причиной плохой генерализации ML моделей.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_size = 100\n",
    "n_features = 2\n",
    "\n",
    "# Создадим рандомный датасет\n",
    "X = np.random.normal(size=(dataset_size,n_features))\n",
    "y = np.random.normal(size=(dataset_size,)) \n",
    "print('Размерность X', X.shape)\n",
    "print('Размерность y', y.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И разделим его на `train` и `test`. Если вы применяете случайное разбиение (как в следующем блоке кода), не забудьте зафиксировать random state, что бы в следующий раз вы получили ровно такую же тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Размерность X_train', X_train.shape)\n",
    "print('Размерность X_test', X_test.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующий раз вам стоит воспользоваться тестом только уже при проверке вашей обученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Правильно делайте разбиения\n",
    "\n",
    "Разбиение на обуучение, валидацию и тест (как и разбиение при кроссвалидации) - это не так просто, как может показаться. \n",
    ">\n",
    "Случайно разбиение может легко привести к утечке данных\n",
    "\n",
    "МЕСТО ДЛЯ ПРИМЕРА \n",
    "\n",
    "Разные разбиения (даже осмысленные) могут давать разную оценку качества модели\n",
    "\n",
    "МЕСТО ДЛЯ ПРИМЕРА \n",
    "\n",
    "Поэтому подбирайте модель на разбиении, которое наиболее точно описывает вашу задачу. Если вам нужно быстро находить похожие на ваш случай - это одно разбиение. Если вам нужно предсказывать что-то новое, до сих пор не виданное - другое. \n",
    "\n",
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не допускайте утечки тестовых данных в процесс обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень важно иметь данные, которые можно использовать для измерения того, насколько хорошо ваша модель обобщает. Распространенная проблема заключается в том, что информация об этих данных просачивается в конфигурацию, обучение или выбор моделей. Когда это происходит, данные перестают быть надежной мерой обобщения, что в свою очередь ведет к тому, что опубликованные модели ML часто не демонстрируют хороших результатов на реальных данных. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Существует несколько способов утечки информации из тестового набора. Некоторые из них кажутся вполне безобидными. Например, при подготовке данных используется информация о средних и диапазонах переменных во всем наборе данных для масштабирования переменных - чтобы предотвратить утечку информации, такие вещи следует делать только с обучающими данными. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Другие распространенные примеры утечки информации: выбор признаков до разбиения данных. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Лучшее, что вы можете сделать для предотвращения этих проблем, это разделить подмножество ваших данных в самом начале проекта, и использовать этот независимый тестовый набор только один раз для измерения генерализации одной модели в конце проекта. См. [Cawley, 2010](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf) и [Kaufman et al., 2012](https://dl.acm.org/doi/10.1145/2382577.2382579) для более широкого обсуждения этого вопроса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Убедитесь, что у вас достаточно данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас недостаточно данных, то обучение хорошей обобщающей модели может оказаться невозможным. Понять так ли это - сложно пока вы не начнете строить модели: все зависит от отношения сигнал/шум в наборе данных. Если сигнал сильный, то можно обойтись меньшим количеством данных; если слабый, то нужно больше данных. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перекрестная валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Если вы не можете получить больше данных - то вы можете лучше использовать имеющиеся данные, используя перекрестную валидацию. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА про cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Вы также можете использовать методы **аугментации** данных (например, см. [Wong et al., 2016](https://arxiv.org/abs/1609.08764), [Shorten et al., 2019](https://link.springer.com/article/10.1186/s40537-019-0197-0?code=a6ae644c-3bfc-43d9-b292-82d77d5890d5)), и они могут быть весьма эффективны для расширения небольших датасетов. \n",
    "\n",
    "Так же в работе [Fort et al., 2021](https://arxiv.org/abs/2105.13343) было показано, что если для каждого сэмпла делать не одну аугментацию, а несколько (в рамках одного и того же батча) то увеличивается точность на тесте. При этом этот метод работает как при обучении на малых, так и на больших батчах, несмотря на уменьшение количества уникальных обучающих примеров в каждом мини-батче.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import albumentations as A\n",
    "import cv2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)\n",
    "])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!wget https://wallpaperaccess.com/full/3842851.jpg"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "image = cv2.imread(\"3842851.jpg\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transformed = transform(image=image)\n",
    "transformed_image = transformed[\"image\"]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(transformed_image)\n",
    "\n",
    "ax[0].set_title('Original')\n",
    "ax[1].set_title('Augmented')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Несколько аугментаций лучше чем одна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До 2021 года была принято проводить аугментации следующим образом: берем сэмпле, применяем к нему набор аугментаций с некоторой вероятностью (например p=0.5) и отправляем этот сэмпл в модель на обучение. В статье [Fort et al., 2021](https://arxiv.org/abs/2105.13343) предлагается способ, который работает лучше.\n",
    "\n",
    "Берем сэмпл, дублируем его N раз и к каждому из дублей применяем свою аугментацию. Казалось бы, в каждом мини-батче становится меньше уникальных примеров, что должно бы негативно сказаться на способности к обобщению, но нет. Точность на тесте только увеличивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим так ли это. ЕЩЕ ПЕРЕСОБЕРУ В НОРМАЛЬНУЮ ФУНКЦИЮ С ПАРАМЕТРАМИ. НО РАБОТАЕТ +)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import os\n",
    "# Helper method to run Tensorboard in Colab\n",
    "def reinit_tensorboard(clear_log = True):\n",
    "  logs_base_dir = \"/content/runs\" # Directory for log files\n",
    "  if clear_log:    \n",
    "    shutil.rmtree(logs_base_dir, ignore_errors = True)\n",
    "    os.makedirs(logs_base_dir, exist_ok=True)\n",
    "  # Colab magic\n",
    "  %reload_ext tensorboard\n",
    "  %tensorboard --logdir {logs_base_dir}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "class CIFAR10_with_multiplicity(torchvision.datasets.CIFAR10):\n",
    "    def __init__(\n",
    "            self,\n",
    "            multiplicity = 1,\n",
    "            *args,\n",
    "            **kwargs\n",
    "            ):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.multiplicity = multiplicity\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img_pil = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img_with_duplicates = []\n",
    "            if self.train == True:        \n",
    "                for i in range(self.multiplicity):\n",
    "                    # Duplicate\n",
    "                    img_with_duplicates.append(self.transform(img_pil)) \n",
    "                target = torch.tensor([target]).repeat(self.multiplicity)\n",
    "                return img_with_duplicates, target\n",
    "            \n",
    "            else:\n",
    "                img = self.transform(img_pil)\n",
    "                return img, target\n",
    "\n",
    "def collate_fn(data):\n",
    "    img = torch.vstack([torch.stack(x[0]) for x in data])\n",
    "    labels = torch.vstack([x[1] for x in data]).flatten()\n",
    "    # print(img.shape)\n",
    "    return img, labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(p=0.5),\n",
    "     transforms.RandomVerticalFlip(p=0.5),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "multiplicity = 2\n",
    "train_set = CIFAR10_with_multiplicity(multiplicity = multiplicity, root='./data', train=True,\n",
    "                                 download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size//multiplicity,\n",
    "                                          shuffle=False, num_workers=2, collate_fn=collate_fn,\n",
    "                                           worker_init_fn=lambda x: np.random.seed())\n",
    "\n",
    "test_set = CIFAR10_with_multiplicity(multiplicity = multiplicity, root='./data', train=False,\n",
    "                                 download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "model = SimpleNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def validate(model,testloader,device):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "  \n",
    "  return correct / total  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reinit_tensorboard(clear_log=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for img_batch, labels_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(img_batch.to(device))\n",
    "        loss = criterion(output, labels_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    writer.add_scalar('Loss/train',loss.cpu().item(),epoch)\n",
    "    accuracy = validate(model,test_loader,device)\n",
    "    writer.add_scalar('Accuracy',accuracy,epoch)\n",
    "    print(\"Epoch {} Loss {:.2f} Accuracy {:.2f}\".format(epoch,loss.item(),accuracy))\n",
    "    writer.flush()\n",
    "writer.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Увеличение данных также полезно в ситуациях, когда у вас ограниченные данные в определенных частях датасета, например, в задачах классификации, когда у вас меньше образцов в некоторых классах, чем в других - ситуация, известная как **дисбаланс классов** (см. [Haixiang et al., 2017](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175) для обзора методов решения этой проблемы). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решить эту проблему частично можно при помощи transfer-learning - использовать модели, обученные на близких к вашей задачах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако если у вас ограниченные данные, то, скорее всего, вам также придется ограничить сложность используемых моделей ML, поскольку модели с большим количеством параметров, например, глубокие нейронные сети, могут легко переобучаться на небольших датасетах. В любом случае, важно выявить эту проблему на ранней стадии и разработать подходящую (и обоснованную) стратегию для ее решения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обращайтесь к экспертам в данной области"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперты в области вашего исследования могут быть очень ценными. Они помогут вам понять, какие проблемы полезно решать, как выбрать наиболее подходящий набор данных и модель ML, а также как определить в каком журнале публиковать статью (аудитория какого журнала будет наиболее подходящей). Если не учитывать мнение экспертов, вы рискуете тем, что ваш проект не будут решать полезные проблемы или будет решать полезные проблемы неподходящими способами (использование непрозрачной ML-модели для решения проблемы, где прозрачность критически важна, например, при принятии медицинских или финансовых решений, см. [Rudin, 2019](https://www.nature.com/articles/s42256-019-0048-x)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Просматривайте критику научных статей из вашей области"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто даже большая доля экспертов из вашей области могут ошибаться при построении моделей и обработке данных. Полезно искать критический разбор таких работ от ученых из этой и смежных областей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подумайте о том, как (и где) будет развернута ваша модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему вы хотите построить модель ML? Это важный вопрос, и ответ на него должен повлиять на процесс, который вы используете для разработки модели. Многие академические исследования по своей сути не предназначены для создания моделей, которые будут использоваться в реальном мире. Это вполне справедливо, поскольку процесс построения и анализа моделей сам по себе может дать очень полезное понимание проблемы. Однако, у статей, в которых описываются модели, которые можно применять в реальных ситуациях, обычно, значительнно выше цитирование. Так что, стоит заранее подумать о том, как и где ваша модель будет применяться. \n",
    "\n",
    "Например, если модель будет развернута в среде с ограниченными ресурсами, например, на датчике или роботе, это может наложить ограничения на сложность модели. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Если существуют ограничения по времени, например, классификация сигнала должна быть выполнена в течение миллисекунд, то это также необходимо учитывать при выборе модели. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА\n",
    "\n",
    "*Например отслеживание тенисного мяча во время турнира. Идеи по проще есть?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одним моментом является то, как модель будет связана с более широкой программной системой, в которой она будет развернута. Эта процедура зачастую далеко не проста (см. [Sculley et al., 2015](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)). Однако новые подходы, такие как [**ML Ops**](https://ml-ops.org/), направлены на решение некоторых трудностей; см. [Tamburri et al., 2020](https://research.tue.nl/en/publications/sustainable-mlops-trends-and-challenges)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как надежно строить модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение моделей - одна из самых приятных частей ML. С современными ML-фреймворками легко использовать всевозможные подходы к данным и смотреть, что к ним подходит. Однако это может закончится неорганизованном беспорядком экспериментов, которые потом сложно обосновать и о которых сложно писать статью. Поэтому важно подходить к построению моделей организованно, убеждаясь, что вы правильно используете данные, и уделяя должное внимание выбору моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Какие есть доступные ресурсы для обучения на GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU - крайне эффективный и ооооочень дорогой способ оубчать модели. К счастью для вас - вы относитесь к привелигированому классу исследователей, которым за GPU обычно платить не надо. Если вы можете обосновать зачем вам GPU - многие корпорации с удовольствием предоставят его бесплатно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab - в базовой версии - бесплатный способ обучать модели на GPU, но про него вы уже все знаете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle](https://www.kaggle.com/code) - похожим на Colab образом работают и Kaggle Kernels. Бесплатно и без особенных бумажек (GPU goes brrrrrrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо бесплатных Colab и Kaggle, есть возможность получить доступ к серьезным ресурсам (несколько тысяч долларов на вычисления). У каждой програмы немного свои правила, но в целом схема такая: пишете заявку, где рассказываете о своих исследованиях и объясняете зачем вам GPU. Затем отправляете заявку с официального университетского/институтского аккаунта и в течение месяца получаете свои кредиты. \n",
    "\n",
    "Основные программы: \n",
    "* [Google Research Credits](https://edu.google.com/programs/credits/research/?modal_active=none)\n",
    "* [Nvidia Research Grants](https://mynvidia.force.com/HardwareGrant/s/Application?ncid=so-twit-867980-vt16#cid=ix01_so-twit_en-us)\n",
    "* [Yandex Research Grants](https://cloud.yandex.ru/promo/datasphere-education)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Советы](https://mobile.twitter.com/artsiom_s/status/1434125597018632193?s=28) по использованию GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `DataLoader` по умолчанию имеет плохие настройки. Поменяйте `num_workers > 0` (например в Colab используйте 2 workers, в ColabPro с большим ОЗУ - 4)\n",
    "* Используйте `torch.backends.cudnn.benchmark = True` для тюнинга cuda kernels\n",
    "* Если применение позваляет - максимизируйтее размер батча для максимальной нагрузки на GPU\n",
    "* Перед слоями нормализации не забывайте отключать bias (`bias=False`) - в этом случае, это лишний параметр, который только раздувает размер модели\n",
    "* Если вы работаете на нескольких GPU, используйте `DistributedDataParallel` даже если на самом деле ваши GPU не distributed\n",
    "* Обучайте с `float16`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуйте различные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще говоря, не существует такой вещи, как единственная лучшая модель ML. Cуществует даже доказательство этого в виде теоремы **No Free Lunch**, которая показывает, что ни один подход ML не лучше другого, если рассматривать все возможные проблемы [Wolpert, 2002](https://link.springer.com/chapter/10.1007/978-1-4471-0123-9_3). \n",
    "\n",
    "Таким образом, ваша задача - найти модель ML, которая хорошо работает для вашей конкретной проблемы. Возможно, у вас есть некоторые *априорные* знания о том, какая модель подойдет лучше всего, в виде качественных исследований по схожим темам, но большую часть времени вам придется работаете в темноте. К счастью, современные библиотеки ML в Python и т.д. позволяют вам опробовать несколько моделей с небольшими изменениями в коде, поэтому нет причин не опробовать несколько моделей и не выяснить для себя, какая из них работает лучше. В свете принципа \"бесплатного обеда\" важно избегать синдрома \"не здесь придумано\", т.е. использовать только те модели, которые были придуманы в вашем институте, поскольку это может привести к тому, что вы упустите лучшую модель для конкретной проблемы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не используйте неподходящие модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снижая барьер для имплементации, современные библиотеки ML также облегчают применение неподходящих моделей к вашим данным. В качестве примера можно привести применение моделей, предполагающих категориальные признаки, к набору данных, состоящему из числовых признаков, или попытку применить модель, предполагающую отсутствие зависимостей между переменными, к данным временного ряда. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это особенно важно учитывать в свете публикации, поскольку представление результатов, полученных с помощью несоответствующих моделей, создаст у рецензентов плохое впечатление о вашей работе. Другой пример - использование неоправданно сложной модели. Например, глубокая нейронная сеть - не лучший выбор, если у вас мало данных, если знания в области предполагают, что лежащая в основе модель довольно проста, или если модель должна быть интерпретируемой. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "И, наконец, не используйте \"современность\" модели в качестве оправдания для выбора модели: старые, проверенные модели часто работают лучше, чем новые."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизируйте гиперпараметры вашей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие модели имеют **гиперпараметры** - то есть числа или параметры, которые влияют на конфигурацию модели. Примерами могут служить функция ядра, используемая в SVM, количество деревьев в случайном лесу и архитектура нейронной сети. Многие из этих гиперпараметров существенно влияют на производительность модели, и, как правило, универсальных параметров не существует. То есть, чтобы получить максимальную отдачу от модели, их нужно подбирать под конкретный набор данных. Хотя может возникнуть соблазн возиться с гиперпараметрами до тех пор, пока вы не найдете что-то подходящее, такой подход, скорее всего, не будет оптимальным. Гораздо лучше использовать какую-то стратегию **оптимизации гиперпараметров**, и это гораздо легче обосновать при написании работы. Базовые стратегии включают случайный поиск и поиск по сетке, но они не очень хорошо масштабируются для большого количества гиперпараметров или для моделей, которые дорого обучать, поэтому стоит использовать инструменты, которые ищут оптимальные конфигурации более интеллектуальным способом (исследовано в [Yang et al., 2020](https://arxiv.org/abs/2007.15745)). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Библиотеки для оптимизации гиперпараметров**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует довольно много библиотек для оптимизации гиперпараметров. Ключевыми являются [Ray-tune](https://docs.ray.io/en/latest/tune/index.html), [Optuna](https://optuna.readthedocs.io/en/stable/) и [Hyperopt](https://github.com/hyperopt/hyperopt). В целом они друг от друга принципиально не отличаются, так что скорее это вопрос вкуса.\n",
    "\n",
    "*Не уверен, что нам вообще тут надо много примеров показывать, в целом мы это уже в лекции объясняли, разве что напомнить*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Ray-Tune**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install ray[tune] tune-sklearn"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 14)\n",
    "\n",
    "# Example parameters to tune from SGDClassifier\n",
    "parameter_grid = {\"alpha\": [1e-4, 1e-1, 1], \"epsilon\": [0.01, 0.1]}\n",
    "\n",
    "tune_search = TuneGridSearchCV(\n",
    "    SGDClassifier(),\n",
    "    parameter_grid,\n",
    "    early_stopping=True,\n",
    "    max_iters=10)\n",
    "\n",
    "tune_search.fit(x_train, y_train)\n",
    "\n",
    "#best set of perameter\n",
    "print(tune_search.best_params_)\n",
    "\n",
    "#best score with best set of perameters\n",
    "print(tune_search.best_score)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Также можно использовать методы **AutoML** для оптимизации выбора модели и ее гиперпараметров в дополнение к другим частям пайплайна добычи данных - см. обзор в [He et al., 2021](https://arxiv.org/abs/1908.00709)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будьте осторожны при оптимизации гиперпараметров и выборе признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим распространенным этапом обучения модели является проведение **feature selection** (рассматривается в [Cai et al., 2018](https://www.sciencedirect.com/science/article/pii/S0925231218302911)). Однако при проведении оптимизации гиперпараметров и выборе признаков важно рассматривать их как часть обучения модели, а не как нечто более общее, что делается перед обучением модели. Особенно распространенной ошибкой является отбор признаков на всем наборе данных до начала обучения модели. Эта ошибка приведет к утечке информации из тестового набора в процесс обучения. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Поэтому, если вы оптимизируете гиперпараметры или признаки, используемые моделью, надо использовать для отбора признаков отдельный, валидационный датасет, или же кросс-валидацию. Вы НЕ должны подбирать модель или ее гиперпараметры на тесте\n",
    "\n",
    "Для бОльшей надежности можно исползьзовать метод **вложенной перекрестной валидации** (также известный как двойная кросс-валидация), которая включает в себя оптимизацию гиперпараметров и выбор признаков в качестве дополнительного цикла внутри основного цикла кросс-валидации. См. [Cawley, 2010](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf) и [Wainer et al., 2021](https://research-portal.uea.ac.uk/en/publications/nested-cross-validation-when-selecting-classifiers-is-overzealous) для более широкого обсуждения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используйте систему для контроля за своими экспериментами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая все сказанное выше, лего представить какой хаос будет творится в процессе ваших эксперементов. Что бы этого избежать имеет смысл каким-то образом систематизировать эксперементальную фазу.\n",
    "\n",
    "В простейшем виде можно завести **git** репозиторий. Каждый раз когда вы меняете что-то в модели, делайте commit. Таким образом если что-то пойдет не так - у вас будет возможность откатить все назад.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Отличной идеей будет использовать инструменты контроля типа [Tensorboard](https://www.tensorflow.org/tensorboard),  [MLFlow](https://mlflow.org/) или [Weights&Biases](https://wandb.ai/site), которые помимо прочего позволяют визуально сравнивать модели и их результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как надежно оценить модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы способствовать прогрессу в своей области, необходимо иметь достоверные результаты, на основании которых можно сделать надежные выводы. К сожалению, очень легко несправедливо оценивать модели ML и тем самым мутить воду академического прогресса. Поэтому тщательно продумайте, как вы собираетесь использовать данные в своих экспериментах, как вы собираетесь измерять истинную производительность ваших моделей и как вы собираетесь сообщать об этой производительности в содержательной и информативной форме.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используйте соответствующий тестовый датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде всего, всегда используйте тестовый набор для измерения обобщающей способности модели ML. То, насколько хорошо модель работает на обучающем множестве, на практике не имеет никакого значения, и достаточно сложная модель может полностью переобучится на *train set*, но так и не научится обобщать тестовые данные. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Также важно убедиться, что данные в тестовом наборе на самом деле подходят. То есть, они не должны пересекаться с обучающим набором и в тоже время достаточно репрезентативными для реальных данных. Например, рассмотрим датасет фотографий объектов, где изображения в обучающем и тестовом наборе были получены на открытом воздухе в солнечный день. Наличие одинаковых погодных условий означает, что тестовое множество не будет независимым, а поскольку оно не охватывает более широкий спектр погодных условий, оно также не будет репрезентативным. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичные ситуации могут возникать, когда для сбора обучающих и тестовых данных используется одно оборудование. Если модель не учитывает характеристики оборудования, она, скорее всего, не будет обобщаться на другие единицы оборудования, и это не будет обнаружено при оценке на тестовом наборе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используйте набор для проверки (*validation set*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нередко обучают несколько моделей последовательно, используя знания, полученные о работе каждой модели, для настройки следующей. При этом важно не использовать тестовый набор в этом процессе. Вместо этого для измерения производительности следует использовать отдельный набор для проверки (*validation set*). Он содержит набор данных, которые не используются непосредственно в обучении, но используются для руководства обучением. Если вы будете использовать тестовое множество для этой цели, то тестовое множество станет неявной частью процесса обучения и больше не сможет служить независимой мерой способности к генерализации, т.е. ваши модели будут постепенно подгонять тестовое множество [Cawley, 2010](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf). \n",
    "\n",
    "\n",
    "#### ЭТОТ РАЗДЕЛ СИЛЬНО ПЕРЕКЛИКАЕТСЯ С ПОДБОРОМ ГИПЕРПАРАМЕТРОВ, НАДО ПЕРЕДЕЛАТЬ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Еще одним преимуществом наличия валидационного набора является то, что вы можете сделать **раннюю остановку** (*early stopping*), когда во время обучения одной модели, модель оценивается по валидационному набору на каждой итерации процесса обучения. Обучение прекращается, когда результат валидации начинает снижаться, так как это указывает на то, что модель начинает переобучаться на *train set*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценивайте модель несколько раз"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие модели ML нестабильны. То есть, если вы обучаете их несколько раз, или если вы вносите небольшие изменения в обучающие данные, то их производительность значительно изменяется. С этим можно и нужно бороться путем фиксации всех *random seeds* и переводом модели на детерменистические операции. Такая внимательность к деталям так же позволит другим людям *на самом деле* воспроизвести ваши результаты.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако на практике это означает, что одна оценка модели может быть ненадежной и может либо недооценить, либо переоценить истинный потенциал модели. По этой причине обычно проводится несколько оценок. Существует множество способов сделать это, и большинство из них предполагает многократное обучение модели с использованием различных подмножеств обучающих данных. **Кросс-валидация** (CV) особенно популярна и имеет множество разновидностей [Arlot et al., 2010](https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full). Десятикратная CV, когда обучение повторяется десять раз, является стандартом, но вы можете добавить больше строгости, используя повторную CV, когда весь процесс CV повторяется несколько раз с различными разбиениями данных. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если некоторые классы данных малы, важно провести **стратификацию**, которая гарантирует, что каждый класс будет адекватно представлен в каждой складке. Обычно сообщается среднее значение и стандартное отклонение многократных оценок, но также рекомендуется вести учет индивидуальных оценок на случай, если вы позже будете использовать статистический тест для сравнения моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохраните некоторые данные для оценки окончательного варианта модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ТУТ КАКАЯ-ТО БРЕДЯТИНА (В ОРИГИНАЛЕ). КРОСС-ВАЛИДАЦИЯ НЕ ВЫДАСТ 10 разных моделей, она выдаст ПАРАМЕТРЫ, КОТОРЫЕ НАДО УСТАНОВИТЬ ПРИ ОБУЧЕНИИ НА ВСЕЙ ВАЛИДАЦИИ (СМОТРИМ SKLEARN тот же)\n",
    "\n",
    "Мы использовали термин *модель* довольно свободно, но есть важное различие между оценкой потенциала общей модели (например, насколько хорошо нейронная сеть может решить вашу проблему) и производительностью конкретного варианта модели (например, конкретной нейронной сети, созданной одним запуском обратного распространения). Перекрестная проверка хороша в первом случае, но она менее полезна во втором. \n",
    "\n",
    "Допустим, например, вы провели десятикратную перекрестную валидацию. В результате будет получено десять экземпляров модели. Допустим, вы выбираете вариант с наивысшей оценкой тестовой складки в качестве модели, которую вы будете использовать на практике. Как вы сообщите о ее производительности? Можно подумать, что результат тестового сложения является надежным показателем ее эффективности, но это, скорее всего, не так. Во-первых, количество данных в одном фолде относительно невелико. Во-вторых, экземпляр с самым высоким баллом вполне может оказаться экземпляром с самой простой тестовой складкой, поэтому содержащиеся в нем данные оценки могут быть нерепрезентативными. Следовательно, единственным способом получить достоверную оценку универсальности экземпляра модели может быть использование другого тестового набора. Поэтому, если у вас достаточно данных, лучше отложить некоторые из них в сторону и использовать только один раз, чтобы получить несмещенную оценку окончательно выбранного экземпляра модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не используйте точность в несбалансированных наборах данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ЛУЧШЕ ЗАМЕНИТЬ НА ИСПОЛЬЗУЙТЕ МЕТРИКИ, ПОДХОДЯЩИЕ ЗАДАЧЕ. КОМУ-ТО и ACCURACY СОЙДЕТ НА НЕСБАЛАНСИРОВАННЫХ ДАННЫХ\n",
    "\n",
    "\n",
    "Подробрнее - читаем [ДЬЯКОНОВА](https://dyakonov.org/2021/05/27/imbalance/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, будьте внимательны к тому, какие метрики вы используете для оценки моделей ML. Например, в случае моделей классификации наиболее часто используемой метрикой является точность (*accuracy*), которая представляет собой долю образцов в наборе данных, которые были правильно классифицированы моделью. Это хорошо работает, если ваши классы сбалансированы, т.е. если каждый класс представлен одинаковым количеством образцов в наборе данных. Но многие наборы данных не сбалансированны, и в этом случае точность может быть очень обманчивой метрикой. \n",
    "\n",
    "Рассмотрим, например, набор данных, в котором 90\\% образцов представляют один класс, а 10\\% образцов - другой класс. Бинарный классификатор, который всегда выдает первый класс, независимо от его входа, будет иметь точность 90\\%, несмотря на то, что он совершенно бесполезен. В такой ситуации предпочтительнее использовать такие метрики, как коэффициент каппа Коэна ($\\kappa$) или коэффициент корреляции Мэтьюса (MCC), которые относительно нечувствительны к дисбалансу размеров классов. Более широкий обзор методов работы с несбалансированными данными см. в [Haixiang et al., 2017](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исследуйте артефакты набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты, выявляющие артефакты данных, или *абляции набора данных*, как их называет [Heinzerling, 2019](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/), включают простые или бессмысленные *baselines*, такие как:\n",
    "\n",
    "1) обучение моделей на неполных входных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) обучение модели на бесмысленных признаках \n",
    "полученных перемешиванием исходных признаков "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) сравнение итоговой модели с предсказанием среднего/медианы/..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это очень важная часть публикации. В современном мире машинного обучения, опубликовать статью без абляций - крайне затруднительно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретируйте предсказания ваших моделей\n",
    "\n",
    "На данный момент известно множество подходов для интерпретации предсказаний моделей, в том числе сложных моделей типа нейронных сетей. \n",
    "К таким относятся [SHAP](https://github.com/slundberg/shap), [LIME](https://github.com/marcotcr/lime), [Grad-Cam](https://github.com/jacobgil/pytorch-grad-cam) и многие другие. Существуют и методы, дающие более точную интерпретацию, но для конкретных моделей. \n",
    "\n",
    "Если вы можете взглянуть на то, на что обращает внимание ваши модели - вы сможете:\n",
    "\n",
    "1) понять, какая модель обращает внимание на более разумные с точки зрения области признаки \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "МЕСТО ДЛЯ ПРИМЕРА \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2) насколько устойчиво то, на какие признаки обращают внимание модели \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) найти уже упомянутые артефакты в данных \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как справедливо сравнивать модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение моделей является основой академических исследований, но сделать это правильно удивительно сложно. Если вы проведете сравнение нечестно и опубликуете его, то впоследствии другие исследователи могут быть введены в заблуждение. Поэтому убедитесь, что вы оцениваете различные модели в одном и том же контексте, изучите несколько точек зрения и правильно используйте статистические тесты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не думайте, что большее число означает лучшую модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не редкость, когда в статье говорится что-то вроде \"В предыдущих исследованиях сообщалось о точности до 94\\%. Наша модель достигла 95\\%, и поэтому она лучше.\" Существуют различные причины, по которым более высокий показатель не всегда означает лучшую модель. Например, если модели обучались или оценивались на разных разделах одного и того же набора данных, то небольшие различия в производительности могут быть вызваны этим. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если они использовали совершенно разные наборы данных, то это может объяснять даже большие различия в производительности. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой причиной несправедливых сравнений является неспособность провести одинаковый объем оптимизации гиперпараметров при сравнении моделей; например, если одна модель имеет настройки по умолчанию, а другая была оптимизирована, то сравнение не будет справедливым. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Например** в статье [Csordás et al., 2021](https://arxiv.org/abs/2108.12284) авторы демонстрируют, что, просто пересмотрев такие базовые конфигурации модели, как масштабирование embeddings, early stopping, relative positional embeddings и универсальные варианты трансформеров, можно значительно улучшить обобщающую способность трансформеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этим и другим причинам к сравнениям, основанным на опубликованных цифрах, всегда следует относиться с осторожностью. Чтобы действительно быть уверенным в справедливости сравнения двух подходов, вам следует заново реализовать все сравниваемые модели, оптимизировать каждую из них в одинаковой степени, провести множественные оценки, а затем использовать статистические тесты, чтобы определить, являются ли различия в производительности значительными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используйте статистические тесты при сравнении моделей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы хотите убедить людей в том, что ваша модель лучше, чем чья-то другая, то статистический тест - очень полезный инструмент. В широком смысле существует две категории тестов для сравнения отдельных моделей ML. Первый используется для сравнения отдельных экземпляров модели, например, двух обученных деревьев решений. Например,  *McNemar's test* является довольно распространенным выбором для сравнения двух классификаторов, и работает путем сравнения выходных меток классификаторов для каждой выборки в тестовом наборе (поэтому не забудьте записать их). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Вторая категория тестов используется для более общего сравнения двух моделей, например, для определения того, какое дерево решений или нейронная сеть лучше подходит для данных. Они требуют множественных оценок каждой модели, которые можно получить с помощью кросс-валидации или повторной выборки (или, если алгоритм обучения стохастический, многократных повторений на одних и тех же данных). Затем тест сравнивает два результирующих распределения. *T-тест Стьюдента* является распространенным выбором для такого сравнения, но он надежен только в случае нормального распределения, что часто бывает не так. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ФИГНЯ. ВЫБОРКИ, ПОЛУЧЕННЫЕ ВЫШЕ - НЕ НЕЗАВИСИМЫ (ЕСТЬ СТАТЬИ ПРО ЭФФЕКТ ЭТОГО ), ПОТОМУ Манн-Уитни ОСОБО ЛУЧШЕ НЕ СДЕЛАЕТ. ДА И ДЛЯ БОЛЬШОГО ЧИСЛА ОБЪЕКТОВ МНОГИЕ МЕТРИКИ РАСПРЕДЕЛЕНЫ НОРМАЛЬНО.  \n",
    "\n",
    "Более надежным является *U-тест Манна-Уитни*, поскольку он не предполагает, что распределения нормальны. Для получения дополнительной информации смотрите [Raschka, 2020](https://arxiv.org/abs/1811.12808)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сделайте поправку на множественное сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все становится немного сложнее, когда вы хотите использовать статистические тесты для сравнения более чем двух моделей, поскольку проведение нескольких парных тестов сродни многократному использованию тестового набора - это может привести к слишком оптимистичным интерпретациям значимости. В принципе, каждый раз, когда вы проводите сравнение двух моделей с помощью статистического теста, существует вероятность того, что он обнаружит значимые различия там, где их нет. Это отраженно в уровне доверия теста, который обычно устанавливается на уровне 95\\%: это означает, что в 1 случае из 20 тест даст ложный результат. Для единичного сравнения это может быть уровнем неопределенности, с которым можно смириться. Однако она накапливается. То есть, если вы проведете 20 парных тестов с уровнем достоверности 95\\%, один из них, скорее всего, даст вам неверный ответ. Это известно как **эффект множественности**, и является примером более широкой проблемы в науке о данных, известной как **выкапывание данных** (*data dredging*) или **p-хакинг** ([Head et a., 2015](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106)). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Чтобы решить эту проблему, можно применить поправку на множественные тесты. Наиболее распространенным подходом является поправка Бонферрони, очень простой метод, который снижает порог значимости в зависимости от количества проводимых тестов - см. [Salzberg, 1997](https://link.springer.com/article/10.1023/A:1009752403260). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Однако существует множество других подходов, и также ведутся споры о том, когда и где следует применять эти поправки; доступный обзор см. в [Streiner et al., 2015](https://academic.oup.com/ajcn/article/102/4/721/4564678)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Избегайте ошибок работы со статистикой \n",
    "\n",
    "[Makin et al., 2019](https://elifesciences.org/articles/48175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "О необходимости повышения воспроизводимости исследований написано много, и неоднократно звучали призывы к улучшению обучения методам статистического анализа. Давайте обсудим несколько статистических ошибок, которые часто встречаются в научной литературе. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Использование маленьких сэмплов при сравнении моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Эта часть](https://elifesciences.org/articles/48175#:~:text=Use%20of-,small,-samples). МОЖЕТ БЫТЬ ПОДХОДИТ. РАСПИСАТЬ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P-Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Эта часть](https://elifesciences.org/articles/48175#:~:text=Flexibility%20of%20analysis%3A%20p-hacking,-The%20problem). МОЖЕТ БЫТЬ ПОДХОДИТ. РАСПИСАТЬ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не доверяйте результатам community benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых проблемных областях стало обычным делом использовать эталонные наборы данных (*benchmarks*) для оценки новых моделей ML. Идея заключается в том, что, поскольку все используют одни и те же данные для обучения и тестирования своих моделей, сравнения будут более прозрачными. К сожалению, этот подход имеет ряд существенных недостатков. Во-первых, если доступ к тестовому набору неограничен, то стоит исходить из предположения, что люди использовали его в процессе обучения, что приводит к data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более тонкая проблема заключается в том, что даже если каждый, кто использует данные, использует тестовый набор только один раз, в совокупности тестовый набор используется сообществом много раз. В результате, сравнивая множество моделей на одном и том же тестовом наборе, становится все более вероятным, что лучшая модель просто слишком хорошо подходит к тестовому набору и не обязательно обобщает лучше, чем другие модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этой и другим причинам следует внимательно относиться к результатам, полученным с помощью эталонного набора данных, и не считать, что небольшое увеличение производительности является значительным. См. [Paullada et al., 2020](https://arxiv.org/abs/2012.05345) для более широкого обсуждения вопросов, связанных с использованием общих наборов данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассмотрите комбинации моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя этот раздел посвящен сравнению моделей, полезно знать, что ML не всегда заключается в выборе между различными моделями. Часто имеет смысл использовать комбинации моделей. Различные модели ML исследуют различные компромиссы; комбинируя их, вы можете иногда компенсировать слабые стороны одной модели за счет использования сильных сторон другой модели, и наоборот. Такие комбинированные модели известны как **ансамбли**, а процесс их создания известен как **обучение ансамблей**. Существует множество подходов к обучению ансамблей (см. обзор в [Dong et al., 2019](https://link.springer.com/article/10.1007/s11704-019-8208-z)). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ВОДА ВОДЯНАЯ. Править\n",
    "К первой категории относятся многие классические подходы, такие как **bagging** и **boosting**. Ансамбли могут быть либо сформированы из существующих обученных моделей, либо базовые модели могут быть обучены как часть процесса, обычно с целью создания разнообразного набора моделей, которые делают ошибки на разных частях пространства данных. \n",
    "\n",
    "Общее соображение в обучении ансамблей заключается в том, как объединить различные базовые модели; подходы к этому варьируются от очень простых методов, таких как голосование или взвешенное голосование, до более сложных подходов, которые используют другую модель ML для объединения выходов базовых моделей. Этот последний подход часто называют **стекинг**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как сообщать о результатах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель академического исследования - не самовозвеличивание, а возможность внести свой вклад в развитие знаний. Для того чтобы внести эффективный вклад в знания, вам необходимо предоставить полную картину вашей работы, охватывающую как то, что получилось, так и то, что не получилось. ML часто связан с компромиссами - очень редко бывает, что одна модель лучше другой во всех отношениях, и вы должны постараться отразить это, используя тонкий и взвешенный подход к изложению результатов и выводов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будьте прозрачны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде всего, всегда старайтесь быть прозрачным в отношении того, что вы сделали и что обнаружили, так как это облегчит другим людям использование ваших наработок. В частности, хорошая практика - делиться своими моделями в доступной форме. Например, если вы использовали скрипт для проведения всех экспериментов, опубликуйте этот скрипт после публикации результатов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошим примером воспроизводимых исследований может являться [ColabFold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb) - блокнот в Colab сопровождающий статью [Highly accurate protein structure prediction with AlphaFold (Jumper et al., 2021)](https://www.nature.com/articles/s41586-021-03819-2). В этом блокноте одновременно присутствуюет несколько вещей - во-первых, для того что бы им пользоваться не нужно ничего устанавливать; во-вторых, он снабжен более менее приличным графичееским интерфейсом, что сильно упрощает использование для не-програмистов; ну и в третьих, при желании, код можно посмотреть \"неотходя от кассы\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно, что бы другие люди могли легко повторить ваши эксперименты. Воспроизводимость придает убедительности вашей работе. Это также значительно облегчает людям сравнение моделей, поскольку им больше не придется все реализовывать с нуля, чтобы обеспечить справедливое сравнение. Знание того, что вы будете делиться своей работой, также побуждает вас быть более осторожным, хорошо документировать свои эксперименты и писать чистый код, что приносит пользу как вам, так и другим. \n",
    "\n",
    "Стоит также отметить, что вопросы воспроизводимости становятся все более заметными в сообществе ML, поэтому в будущем вы, возможно, не сможете опубликовать работу, если ваш рабочий процесс не будет надлежащим образом задокументирован и распространен - например, см. [Pineau et al., 2020](https://arxiv.org/abs/2003.12206).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сообщайте о результатах работы различными способами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из способов достижения большей строгости при оценке и сравнении моделей является использование нескольких наборов данных. Это помогает преодолеть любые недостатки, связанные с отдельными наборами данных, и позволяет представить более полную картину работы вашей модели. Также хорошей практикой является представление нескольких метрик для каждого набора данных, поскольку разные метрики могут представить различные точки зрения на результаты и повысить прозрачность вашей работы. Например, если вы используете точность, то нелишним будет включить метрики, которые менее чувствительны к дисбалансу классов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы используете частичную метрику, такую как precision, recall, sensitivity или specificity, также включите метрику, которая дает более полное представление об уровне ошибок вашей модели. Убедитесь, что понятно, какие метрики вы используете. Например, если вы сообщаете F-score, уточните, идет ли речь о F1 или о каком-то другом балансе между precision и recall. Если вы сообщаете AUC, укажите, является ли это площадью под кривой ROC или кривой PR. Для более широкого обсуждения смотрите [Blagec et al., 2020](https://arxiv.org/abs/2008.02577).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не делайте обобщений, выходящих за рамки данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно не представлять недействительные выводы, поскольку это может сбить с пути других исследователей. Распространенная ошибка - делать общие заявления, которые не подтверждаются данными, использованными для обучения и оценки моделей. Например, если ваша модель действительно хорошо работает на одном наборе данных, это не означает, что она будет хорошо работать и на других наборах данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя вы можете получить более надежные выводы, используя несколько наборов данных, всегда существует ограничение на то, что вы можете заключить из любого экспериментального исследования. Этому есть множество причин (см. [Paullada et al., 2020](https://arxiv.org/abs/2012.05345)), многие из которых связаны с тем, как составлены наборы данных. Одной из распространенных проблем является предвзятость, или **ошибка выборки**: данные недостаточно репрезентативны для реального мира. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другая проблема - дублирование: несколько наборов данных могут не быть независимыми и иметь схожие предубеждения (*biases*). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует также проблема качества: и это особенно актуально для наборов данных глубокого обучения, где необходимость в количестве данных ограничивает объем проверки качества, которую можно провести. Короче говоря, не преувеличивайте свои результаты и помните об их ограничениях.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будьте осторожны, сообщая о статистической значимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже обсуждали статистические тесты, и как их можно использовать для определения различий между ML-моделями. Однако статистические тесты не совершенны. Некоторые из них консервативны и склонны занижать значимость, другие либеральны и склонны завышать значимость. Это означает, что положительный тест не всегда означает, что что-то является значимым, а отрицательный тест не обязательно означает, что что-то не является значимым. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Кроме того, существует проблема использования порога для определения значимости; например, 95\\% доверительный порог (т.е. когда p-значение $<$ 0,05) означает, что 1 из 20 случаев разница, отмеченная как значимая, не будет значимой. На самом деле, статистики все чаще утверждают, что лучше не использовать пороговые значения, а вместо этого просто сообщать p-значения и предоставить читателю самому интерпретировать их. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо статистической значимости, необходимо также учитывать, действительно ли разница между двумя моделями важна. Если у вас достаточно данных, вы всегда можете найти значительные различия, даже если фактическая разница в производительности ничтожна. Чтобы лучше понять, является ли что-то важным, можно измерить **размер эффекта** (*effect size*). Для этого используется целый ряд подходов: $d$ статистика Коэна, вероятно, самая распространенная, но более надежные подходы, такие как Колмогорова-Смирнова, предпочтительнее. Подробнее об этом см. в [Betensky, 2019](https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1529624)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрите на свои модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обученные модели содержат много полезной информации. К сожалению, многие авторы просто сообщают показатели эффективности обученной модели, не давая никакого представления о том, чему она на самом деле научилась. Помните, что цель исследования не в том, чтобы добиться чуть более высокой точности, чем все остальные. Скорее, она заключается в том, чтобы генерировать знания и понимание и делиться ими с исследовательским сообществом. Если вы сможете это сделать, то вероятность того, что ваша работа получит достойную публикацию, значительно возрастет. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как рекомендовалось ранее при выборе моделей - опять же, загляните внутрь своей итоговой модели. \n",
    "\n",
    " Для относительно простых моделей, таких как деревья решений, также может быть полезно предоставить визуализацию ваших моделей, и в большинстве библиотек есть функции, которые сделают это за вас. Для сложных моделей, таких как глубокие нейронные сети, рассмотрите возможность использования методов **объяснимого ИИ** (Explainable AI) для извлечения знаний (рассмотрено в [Belle et al., 2021](https://www.frontiersin.org/articles/10.3389/fdata.2021.688969/full)); они вряд ли расскажут вам, что именно делает модель, но могут дать вам некоторые полезные идеи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МЕСТО ДЛЯ ПРИМЕРА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие советы по написанию статей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Полезные инструменты для написания статей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Английский язык, для многих из нас не является родным (и даже не тем языком, на котором мы свободно разговариваем). Конечно прекрастно, если в ваших соавторах есть носитель языка, или коллега, который много лет работал в Европе или США, но чаще всего - доступа к тамим коллегам у нас нет. \n",
    "\n",
    "**DISCLAIMER**. Всем, о чем пойдет речь дальше, следует пользоваться с большой осторожностью и не пренибрегать дополнительными проверками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хороший способ перевести текст с русского на английский - воспользоваться переводчиком [DeepL](https://www.deepl.com/translator), он довольно не плохо справляется со сложными научными и техническими текстами + в ннего встроено редактирование \"на ходу\" (то есть, на каждое слово - можно посмотреть несколько вариантов перевода)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже написанные на английском тексты, часто требуется перефразировать, например, когда вы описываете результаты исследований других людей или просто считаете, что написаное не доносит тот смысл, который вы хотели донести. Тут на помощь приходит инструмент [Wordtuna](https://www.wordtune.com/), который позволяет перефразировать любое предложение другими словами. Опять таки, часто в предложеных вариантах встречается билеберда, так что пользуйтесь внимательно и аккуратно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто, когда мы пишем на английском, у нас страдает граматика и орфография (да что уж на английском, на русском тоже). С помощью [Grammarly](https://app.grammarly.com/) можно, как минимум, часть ошибок обнаружить и поправить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один инструмент, [GPT-J](https://6b.eleuther.ai/), вообще не стоит рассматривать в серьез как инструмент для научной комуникации. Тем не менее, им можно пользоваться для вдохновения. Например, часто мы начинаем писать предложение и еще не уверены, *что сказать. Поэтому мы записываем слова одно за другим, в произвольном порядке. Затем мы смотрим на записанные слова. Затем мы начинаем думать о том, как связать эти слова друг с другом. Затем мы снова записываем предложение. Иногда мы вносим изменения. Иногда мы начинаем сначала. Иногда мы думаем, что это никуда не годится, и выбрасываем предложения.* (*курсив* - дополнено GPT-J).\n",
    "\n",
    "Вообщем вы поняли идею."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction, Main Part, Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Советы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cover letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Советы https://www.nature.com/articles/s41561-021-00824-y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист для публикации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ]  Пункт 1 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Список литературы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [How to avoid machine learning pitfalls: a guide for academic researchers (Lones, 2021)](https://arxiv.org/abs/2108.02497)\n",
    "\n",
    "* [Data and its (dis)contents: A survey of dataset development and use in machine learning research (Paullada et al., 2020)](https://arxiv.org/abs/2012.05345)\n",
    "\n",
    "* [ImageNet: A large-scale hierarchical image database (Deng et al., 2009)](https://ieeexplore.ieee.org/document/5206848)\n",
    "\n",
    "* [Excavating AI. The Politics of Images in Machine Learning Training Sets (Crawford & Paglen, 2019)](https://excavating.ai/)\n",
    "\n",
    "* [Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy (Yang et al., 2020)](https://dl.acm.org/doi/abs/10.1145/3351095.3375709)\n",
    "\n",
    "* [Translating Statistics to Make Decisions: A Guide for the Non-Statistician (Cox, 2017)](https://www.oreilly.com/library/view/translating-statistics-to/9781484222560/A426308_1_En_3_Chapter.html)\n",
    "\n",
    "* [Understanding data augmentation for classification: when to warp? (Wong et al., 2016)](https://arxiv.org/abs/1609.08764)\n",
    "\n",
    "* [A survey on Image Data Augmentation for Deep Learning (Shorten et al., 2019)](https://link.springer.com/article/10.1186/s40537-019-0197-0?code=a6ae644c-3bfc-43d9-b292-82d77d5890d5)\n",
    "\n",
    "* [Learning from class-imbalanced data: Review of methods and applications (Haixiang et al., 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175)\n",
    "\n",
    "* [NLP's Clever Hans Moment has Arrived (Heinzerling, 2019)](https://thegradient.pub/nlps-clever-hans-moment-has-arrived/)\n",
    "\n",
    "* [The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers (Csordás et al., 2021)](https://arxiv.org/abs/2108.12284)\n",
    "\n",
    "* [Drawing Multiple Augmentation Samples Per Image During Training Efficiently Decreases Test Error (Fort et al., 2021)](https://arxiv.org/abs/2105.13343)\n",
    "\n",
    "* [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead (Rudin, 2019)](https://www.nature.com/articles/s42256-019-0048-x)\n",
    "\n",
    "* [Hidden Technical Debt in Machine Learning Systems (Sculley et al., 2015)](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)\n",
    "\n",
    "* [Sustainable MLOps - Trends and Challenges (Tamburri et al., 2020)](https://research.tue.nl/en/publications/sustainable-mlops-trends-and-challenges)\n",
    "\n",
    "* [On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation (Cawley, 2010)](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)\n",
    "\n",
    "* [Leakage in data mining: Formulation, detection, and avoidance (Kaufman et al., 2012)](https://dl.acm.org/doi/10.1145/2382577.2382579)\n",
    "\n",
    "* [The Supervised Learning No-Free-Lunch Theorems (Wolpert, 2002)](https://link.springer.com/chapter/10.1007/978-1-4471-0123-9_3)\n",
    "\n",
    "* [Feature selection in machine learning: A new perspective (Cai et al., 2018)](https://www.sciencedirect.com/science/article/pii/S0925231218302911)\n",
    "\n",
    "* [On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice (Yang et al., 2020)](https://arxiv.org/abs/2007.15745)\n",
    "\n",
    "* [AutoML: A Survey of the State-of-the-Art (He et al., 2021)](https://arxiv.org/abs/1908.00709)\n",
    "\n",
    "* [A survey of cross-validation procedures for model selection (Arlot et al., 2010)](https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full)\n",
    "\n",
    "* [Learning from class-imbalanced data: Review of methods and applications (Haixiang et al., 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175)\n",
    "\n",
    "* [Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning (Raschka, 2020)](https://arxiv.org/abs/1811.12808)\n",
    "\n",
    "* [The Extent and Consequences of P-Hacking in Science (Head et a., 2015)](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106)\n",
    "\n",
    "* [On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach (Salzberg, 1997)](https://link.springer.com/article/10.1023/A:1009752403260)\n",
    "\n",
    "* [Best (but oft-forgotten) practices: the multiple problems of multiplicity—whether and how to correct for many statistical tests (Streiner et al., 2015)](https://academic.oup.com/ajcn/article/102/4/721/4564678)\n",
    "\n",
    "* [A survey on ensemble learning (Dong et al., 2019)](https://link.springer.com/article/10.1007/s11704-019-8208-z)\n",
    "\n",
    "* [Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program) (Pineau et al., 2020)](https://arxiv.org/abs/2003.12206)\n",
    "\n",
    "* [A critical analysis of metrics used for measuring progress in artificial intelligence (Blagec et al., 2020)](https://arxiv.org/abs/2008.02577)\n",
    "\n",
    "* [The p-Value Requires Context, Not a Threshold (Betensky, 2019)](https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1529624)\n",
    "\n",
    "* [Principles and Practice of Explainable Machine Learning (Belle et al., 2021)](https://www.frontiersin.org/articles/10.3389/fdata.2021.688969/full)\n",
    "\n",
    "* [Science Forum: Ten common statistical mistakes to watch out for when writing or reviewing a manuscript (Makin et al., 2019)](https://elifesciences.org/articles/48175)\n",
    "\n",
    "* [Highly accurate protein structure prediction with AlphaFold (Jumper et al., 2021)](https://www.nature.com/articles/s41586-021-03819-2). "
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {}
}
