{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как избежать \"подводных камней\" машинного обучения: руководство для академических исследователе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2108.02497.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "В этой лекции дается краткое описание некоторых распространенных ошибок, возникающих при использовании методов машинного обучения, и того, что можно сделать, чтобы их избежать. Лекция предназначена в первую очередь как руководство для студентов-исследователей и посвящена вопросам, которые особенно актуальны в академических исследованиях, например, необходимости проводить строгие сравнения и делать обоснованные выводы. Однако большинство уроков применимо и к более широкому использованию ML, и эту лекцию можно использовать в качестве вводного пособия для тех, кто начинает работать в этой области.\n",
    "\n",
    "Мы рассмотрим пять этапов процесса машинного обучения:\n",
    "* что нужно сделать до построения модели\n",
    "* как надежно строить модели\n",
    "* как надежно оценивать модели\n",
    "* как справедливо сравнивать модели\n",
    "* как сообщать о результатах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прежде чем приступить к созданию моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это нормально - спешить начать обучение и оценку моделей, но важно найти время, чтобы подумать о целях проекта, полностью понять данные, которые будут использоваться для достижения этих целей, рассмотреть любые ограничения данных, которые необходимо устранить, и понять, что уже было сделано в вашей области. Если вы не сделаете этого, то в итоге вы можете получить результаты, которые трудно опубликовать, или модели, которые не подходят для намеченной цели.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Потратьте время на понимание своих данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конечном итоге вы захотите опубликовать свою работу. Это гораздо легче сделать, если ваши данные получены из надежного источника, собраны с использованием надежной методологии и имеют хорошее качество. Например, если вы используете данные, собранные на интернет-ресурсе, убедитесь, что вы знаете, откуда они взяты. Описаны ли они в статье? Если да, посмотрите на документ; убедитесь, что он был опубликован в авторитетном месте, и проверьте, упоминают ли авторы какие-либо ограничения данных. Не думайте, что если набор данных использовался в ряде работ, то он хорошего качества - иногда данные используются только потому, что их легко достать, а некоторые широко используемые наборы данных, как известно, имеют существенные ограничения (см. [Paullada et al., 2020]((https://arxiv.org/abs/2012.05345))). Если вы обучаете свою модель на плохих данных, то, скорее всего, вы создадите плохую модель: процесс, известный как **garbage in garbage out** (чушь на входе - чушь на выходе). Поэтому всегда начинайте с того, что убедитесь, что ваши данные имеют смысл. Проведите **эксплораторный анализ данных** (см. [Cox, 2017](https://www.oreilly.com/library/view/translating-statistics-to/9781484222560/A426308_1_En_3_Chapter.html)). Ищите недостающие или непоследовательные записи. Гораздо проще сделать это сейчас, до обучения модели, чем потом, когда вы будете пытаться объяснить рецензентам, почему вы использовали плохие данные. \n",
    "\n",
    "Это важно сделать независимо от того, используете ли вы существующие наборы данных или генерируете новые данные в рамках своего исследования. Если вы генерируете собственные данные, также учитывайте, что исследовательский анализ может быть ценен сам по себе, а результаты этого анализа могут стать важной частью вашей работы.\n",
    "\n",
    "[Data and its (dis)contents: A survey of dataset development and use in machine learning research (Paullada et al., 2020)](https://arxiv.org/abs/2012.05345)\n",
    "\n",
    "[Translating Statistics to Make Decisions: A Guide for the Non-Statistician (Cox, 2017)](https://www.oreilly.com/library/view/translating-statistics-to/9781484222560/A426308_1_En_3_Chapter.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не смотрите на *все* свои данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда вы изучаете данные, вполне вероятно, что вы заметите закономерности и сделаете выводы, которые будут направлять ваши усилия по построению модели. Это еще одна веская причина для изучения данных. Однако важно, чтобы вы не делали **непроверяемых** предположений, которые впоследствии будут использованы в вашей модели. Здесь важна частица \"непроверяемые\"; делать предположения - это нормально, но они должны использоваться только для обучения модели, а не для тестирования. Поэтому, чтобы убедиться в этом, вы должны избегать пристального изучения любых тестовых данных на начальном этапе исследовательского анализа. В противном случае вы можете сознательно или бессознательно сделать предположения, которые ограничат общность вашей модели не поддающимся тестированию способом. К этой теме мы будем возвращаться еще не раз, поскольку утечка информации из тестового набора в процесс обучения является распространенной причиной того, что модели ML не справляются с генерализацией.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Убедитесь, что у вас достаточно данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас недостаточно данных, то обучение обобщающей модели может оказаться невозможным. Определить, так ли это, может быть непросто, и это может быть не очевидно, пока вы не начнете строить модели: все зависит от соотношения сигнала и шума в наборе данных. Если сигнал сильный, то можно обойтись меньшим количеством данных; если слабый, то нужно больше данных. Если вы не можете получить больше данных - а это распространенная проблема во многих областях исследований - то вы можете лучше использовать имеющиеся данные, используя перекрестную валидацию. Вы также можете использовать методы расширения данных (например, см. [Wong et al., 2016](https://arxiv.org/abs/1609.08764), [Shorten et al., 2019](https://link.springer.com/article/10.1186/s40537-019-0197-0?code=a6ae644c-3bfc-43d9-b292-82d77d5890d5)), и они могут быть весьма эффективны для расширения небольших наборов данных. Увеличение данных также полезно в ситуациях, когда у вас ограниченные данные в определенных частях датасета, например, в задачах классификации, когда у вас меньше образцов в некоторых классах, чем в других - ситуация, известная как **дисбаланс классов** (см. [Haixiang et al., 2017](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175) для обзора методов решения этой проблемы). Однако если у вас ограниченные данные, то, скорее всего, вам также придется ограничить сложность используемых моделей ML, поскольку модели с большим количеством параметров, например, глубокие нейронные сети, могут легко переоптимизировать небольшие наборы данных. В любом случае, важно выявить эту проблему на ранней стадии и разработать подходящую (и обоснованную) стратегию для ее решения.\n",
    "\n",
    "[Understanding data augmentation for classification: when to warp? (Wong et al., 2016)](https://arxiv.org/abs/1609.08764)\n",
    "\n",
    "[A survey on Image Data Augmentation for Deep Learning (Shorten et al., 2019)](https://link.springer.com/article/10.1186/s40537-019-0197-0?code=a6ae644c-3bfc-43d9-b292-82d77d5890d5)\n",
    "\n",
    "[Learning from class-imbalanced data: Review of methods and applications (Haixiang et al., 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обращайтесь к экспертам в данной области"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доменные эксперты могут быть очень ценными. Они помогут вам понять, какие проблемы полезно решать, помогут выбрать наиболее подходящий набор функций и модель ML, а также помогут опубликовать информацию для наиболее подходящей аудитории. Если не учитывать мнение экспертов, это может привести к тому, что проекты не будут решать полезные проблемы или будут решать полезные проблемы неподходящими способами. Примером последнего может служить использование непрозрачной ML-модели для решения проблемы, когда необходимо понять, как модель достигает результата, например, при принятии медицинских или финансовых решений (см. [Rudin, 2019](https://www.nature.com/articles/s42256-019-0048-x)). В начале проекта эксперты в области ваших исследований могут помочь вам понять данные и указать на признаки (фитчи), которые, вероятно, будут полезными. В конце проекта они могут помочь вам опубликоваться в журналах, посвященных конкретной области, и тем самым привлечь аудиторию, которая, скорее всего, получит пользу от ваших исследований.\n",
    "\n",
    "[Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead (Rudin, 2019)](https://www.nature.com/articles/s42256-019-0048-x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучите литературу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы, вероятно, не первый, кто использует ML для решения конкретной проблемной области, поэтому важно понимать, что было сделано и что не было сделано ранее. Другие люди работали над той же проблемой - это не плохо; академический прогресс обычно представляет собой итерационный процесс, когда каждое исследование предоставляет информацию, которая может послужить основой для следующего. Может быть неприятно обнаружить, что кто-то уже исследовал вашу замечательную идею, но, скорее всего, они оставили открытыми многие пути исследования, и их предыдущая работа может быть использована в качестве обоснования вашей работы. Игнорировать предыдущие исследования - значит потенциально упустить ценную информацию. Например, возможно, кто-то уже пробовал предложенный вами подход и нашел фундаментальные причины, по которым он не сработает (и тем самым избавил вас от нескольких лет разочарования), или же частично решил проблему таким образом, что вы можете от нее оттолкнуться. Поэтому важно провести обзор литературы до начала работы; если вы сделаете это слишком поздно, это может означать, что вам придется объяснять, почему вы повторяете ту же самую тему или не опираетесь на существующие знания, когда вы сядете писать статью.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подумайте о том, как (и где) будет развернута ваша модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему вы хотите построить модель ML? Это важный вопрос, и ответ на него должен повлиять на процесс, который вы используете для разработки модели. Многие академические исследования являются именно этим - исследованиями - и не предназначены для создания моделей, которые будут использоваться в реальном мире. Это вполне справедливо, поскольку процесс построения и анализа моделей сам по себе может дать очень полезное понимание проблемы. Однако для многих академических исследований конечной целью является создание модели ML, которая может быть использована в реальной ситуации. Если это так, то стоит заранее подумать о том, как она будет применяться. Например, если она будет развернута в среде с ограниченными ресурсами, например, на датчике или роботе, это может наложить ограничения на сложность модели. Если существуют ограничения по времени, например, классификация сигнала должна быть выполнена в течение миллисекунд, то это также необходимо учитывать при выборе модели. Еще одним моментом является то, как модель будет связана с более широкой программной системой, в которой она будет развернута. Эта процедура зачастую далеко не проста (см. [Sculley et al., 2015](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)). Однако новые подходы, такие как **ML Ops**, направлены на решение некоторых трудностей; см. [Tamburri et al., 2020](https://research.tue.nl/en/publications/sustainable-mlops-trends-and-challenges).\n",
    "\n",
    "[Hidden Technical Debt in Machine Learning Systems (Sculley et al., 2015)](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)\n",
    "\n",
    "[Sustainable MLOps - Trends and Challenges (Tamburri et al., 2020)](https://research.tue.nl/en/publications/sustainable-mlops-trends-and-challenges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to reliably build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building models is one of the more enjoyable parts of ML. With modern ML frameworks, it's easy to throw all manner of approaches at your data and see what sticks. However, this can lead to a disorganised mess of experiments that's hard to justify and hard to write up. So, it's important to approach model building in an organised manner, making sure you use data correctly, and putting adequate consideration into the choice of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't allow test data to leak into the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's essential to have data that you can use to measure how well your model generalises. A common problem is allowing information about this data to leak into the configuration, training or selection of models. When this happens, the data no longer provides a reliable measure of generality, and this is a common reason why published ML models often fail to generalise to real world data. There are a number of ways that information can leak from a test set. Some of these seem quite innocuous. For instance, during data preparation, using information about the means and ranges of variables within the whole data set to carry out variable scaling --- in order to prevent information leakage, this kind of thing should only be done with the training data. Other common examples of information leakage are carrying out feature selection before partitioning the data (see \\nameref{feature}), and using the same test data to evaluate the generality of multiple models (see \\nameref{validation} and \\nameref{community}). The best thing you can do to prevent these issues is to partition off a subset of your data right at the start of your project, and only use this independent test set once to measure the generality of a single model at the end of the project (see \\nameref{savedata}). See \\citep{cawley2010over} and \\citep{kaufman2012leakage} for a broader discussion of this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do try out a range of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, there's no such thing as a single best ML model. In fact, there's a proof of this, in the form of the No Free Lunch theorem, which shows that no ML approach is any better than any other when considered over every possible problem \\citep{wolpert2002supervised}. So, your job is to find the ML model that works well for your particular problem. There may be some \\textit{a priori} knowledge of this, in the form of good quality research on closely related problems, but most of the time you're operating in the dark. Fortunately, modern ML libraries in Python (e.g.\\ scikit-learn \\citep{varoquaux2015scikit}), R (e.g.\\ caret \\citep{kuhn2015short}), Julia (e.g.\\ MLJ \\citep{blaom2020mlj}) etc. allow you to try out multiple models with only small changes to your code, so there's no reason not to try out multiple models and find out for yourself which one works best. In the light of No Free Lunch, it's important to avoid ``not invented here syndrome'', i.e.\\ only using models that have been invented at your own institution, since this may cause you to omit the best model for a particular problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't use inappropriate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By lowering the barrier to implementation, modern ML libraries also make it easy to apply inappropriate models to your data. Examples of this include applying models that expect categorical features to a data set comprised of numeric features, or attempting to apply a model that assumes no dependencies between variables to time series data. This is particularly something to consider in the light of publication, since reporting results from inappropriate models will give reviewers a bad impression of your work. Another example is using a model that is unnecessarily complex. For instance, a deep neural network is not a good choice if you have limited data, if domain knowledge suggests the underlying pattern is quite simple, or if the model needs to be interpretable. Finally, don't use recency as a justification for choosing a model: old, established, models often work better than new ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do optimise your model's hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models have \\textbf{hyperparameters} --- that is, numbers or settings that affect the configuration of the model. Examples include the kernel function used in an SVM, the number of trees in a random forest, and the architecture of a neural network. Many of these hyperparameters significantly effect the performance of the model, and there is generally no one-size-fits-all. That is, they need to be fitted to your particular data set in order to get the most out of the model. Whilst it may be tempting to fiddle around with hyperparameters until you find something that works, this is not likely to be an optimal approach. It's much better to use some kind of \\textbf{hyperparameter optimisation} strategy, and this is much easier to justify when you write it up. Basic strategies include random search and grid search, but these don't scale well to large numbers of hyperparameters or to models that are expensive to train, so it's worth using tools that search for optimal configurations in a more intelligent manner (surveyed in \\citep{yang2020hpo}). It is also possible to use \\textbf{AutoML} techniques to optimise both the choice of model and its hyperparameters, in addition to other parts of the data mining pipeline --- see \\cite{he2021automl} for a review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do be careful where you optimise hyperparameters and select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common stage of training a model is to carry out \\textbf{feature selection} (surveyed by \\cite{cai2018feature}). However, when carrying out both hyperparameter optimisation and feature selection, it is important to treat them as part of model training, and not something more general that you do before model training. A particularly common error is to do feature selection on the whole data set before model training begins, but this will result in information leaking from the test set into the training process. So, if you optimise the hyperparameters or features used by a model, you should ideally use exactly the same data that you use to train the model. A common technique for doing this is \\textbf{nested cross-validation} (also known as double cross-validation), which involves doing hyperparameter optimisation and feature selection as an extra loop inside the main cross-validation loop. See \\cite{cawley2010over} for a broader discussion, and \\nameref{multiple} for more information about cross-validation.% --- see \\cite{wainer2021nested} for a broader discussion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to robustly evaluate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to contribute to progress in your field, you need to have valid results that you can draw reliable conclusions from. Unfortunately it's really easy to evaluate ML models unfairly, and, by doing so, muddy the waters of academic progress. So, think carefully about how you are going to use data in your experiments, how you are going to measure the true performance of your models, and how you are going to report this performance in a meaningful and informative way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do use an appropriate test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, always use a test set to measure the generality of an ML model. How well a model performs on the training set is almost meaningless, and a sufficiently complex model can entirely learn a training set yet capture no generalisable knowledge. It's also important to make sure the data in the test set is appropriate. That is, it should not overlap with the training set and it should be representative of the wider population. For example, consider a photographic data set of objects where the images in the training and test set were collected outdoors on a sunny day. The presence of the same weather conditions mean that the test set will not be independent, and by not capturing a broader variety of weather conditions, it will also not be representative. Similar situations can occur when a single piece of equipment is used to collect both the training and test data. If the model overlearns characteristics of the equipment, it will likely not generalise to other pieces of equipment, and this will not be detectable by evaluating it on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do use a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not unusual to train multiple models in succession, using knowledge gained about each model's performance to guide the configuration of the next. When doing this, it's important not to use the test set within this process. Rather, a separate validation set should be used to measure performance. This contains a set of samples that are not directly used in training, but which are used to guide training. If you use the test set for this purpose, then the test set will become an implicit part of the training process, and will no longer be able to serve as an independent measure of generality, i.e.\\ your models will progressively overfit the test set \\citep{cawley2010over}. Another benefit of having a validation set is that you can do \\textbf{early stopping}, where, during the training of a single model, the model is measured against the validation set at each iteration of the training process. Training is then stopped when the validation score starts to fall, since this indicates that the model is starting to overfit the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do evaluate a model multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many ML models are unstable. That is, if you train them multiple times, or if you make small changes to the training data, then their performance varies significantly. This means that a single evaluation of a model can be unreliable, and may either underestimate or overestimate the model's true potential. For this reason, it is common to carry out multiple evaluations. There are numerous ways of doing this, and most involve training the model multiple times using different subsets of the training data. \\textbf{Cross-validation} (CV) is particularly popular, and comes in numerous varieties \\citep{arlot2010survey}. Ten-fold CV, where training is repeated ten times, is arguably the standard, but you can add more rigour by using repeated CV, where the whole CV process is repeated multiple times with different partitionings of the data. If some of your data classes are small, it's important to do \\textbf{stratification}, which ensures each class is adequately represented in each fold. It is common to report the mean and standard deviation of the multiple evaluations, but it is also advisable to keep a record of the individual scores in case you later use a statistical test to compare models (see \\nameref{statistical}).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do save some data to evaluate your final model instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've used the term \\textit{model} quite loosely, but there is an important distinction between evaluating the potential of a general model (e.g.\\ how well a neural network can solve your problem), and the performance of a particular model instance (e.g.\\ a specific neural network produced by one run of back-propagation). Cross-validation is good at the former, but it's less useful for the latter.  Say, for instance, that you carried out ten-fold cross-validation. This would result in ten model instances. Say you then select the instance with the highest test fold score as the model which you will use in practice. How do you report its performance? Well, you might think that its test fold score is a reliable measure of its performance, but it probably isn't. First, the amount of data in a single fold is relatively small. Second, the instance with the highest score could well be the one with the easiest test fold, so the evaluation data it contains may not be representative. Consequently, the only way of getting a reliable estimate of a model instance's generality may be to use another test set. So, if you have enough data, it's better to keep some aside and only use it once to provide an unbiased estimate of the final selected model instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't use accuracy with imbalanced data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, be careful which metrics you use to evaluate your ML models. For instance, in the case of classification models, the most commonly used metric is accuracy, which is the proportion of samples in the data set that were correctly classified by the model. This works fine if your classes are balanced, i.e.\\ if each class is represented by a similar number of samples within the data set. But many data sets are not balanced, and in this case accuracy can be a very misleading metric. Consider, for example, a data set in which 90\\% of the samples represent one class, and 10\\% of the samples represent another class. A binary classifier which always outputs the first class, regardless of its input, would have an accuracy of 90\\%, despite being completely useless. In this kind of situation, it would be preferable to use a metric such as Cohen's kappa coefficient ($\\kappa$) or Matthews Correlation Coefficient (MCC), both of which are relatively insensitive to class size imbalance (also see \\nameref{measure}). For a broader review of methods for dealing with imbalanced data, see \\cite{haixiang2017learning}.% Alternatively you could use data augmentation to boost the smaller classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compare models fairly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing models is the basis of academic research, but it's surprisingly difficult to get it right. If you carry out a comparison unfairly, and publish it, then other researchers may subsequently be led astray. So, do make sure that you evaluate different models within the same context, do explore multiple perspectives, and do use make correct use of statistical tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't assume a bigger number means a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not uncommon for a paper to state something like ``In previous research, accuracies of up to 94\\% were reported. Our model achieved 95\\%, and is therefore better.'' There are various reasons why a higher figure does not imply a better model. For instance, if the models were trained or evaluated on different partitions of the same data set, then small differences in performance may be due to this. If they used different data sets entirely, then this may account for even large differences in performance. Another reason for unfair comparisons is the failure to carry out the same amount of hyperparameter optimisation (see \\nameref{hyperparameters}) when comparing models; for instance, if one model has default settings and the other has been optimised, then the comparison won't be fair. For these reasons, and others, comparisons based on published figures should always be treated with caution. To really be sure of a fair comparison between two approaches, you should freshly implement all the models you're comparing, optimise each one to the same degree, carry out multiple evaluations (see \\nameref{multiple}), and then use statistical tests (see \\nameref{statistical}) to determine whether the differences in performance are significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do use statistical tests when comparing models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to convince people that your model is better than someone else's, then a statistical test is a very useful tool. Broadly speaking, there are two categories of tests for comparing individual ML models. The first is used to compare individual model instances, e.g.\\ two trained decision trees. For example, McNemar's test is a fairly common choice for comparing two classifiers, and works by comparing the classifiers' output labels for each sample in the test set (so do remember to record these). The second category of tests are used to compare two models more generally, e.g.\\ whether a decision tree or a neural network is a better fit for the data. These require multiple evaluations of each model, which you can get by using cross-validation or repeated resampling (or, if your training algorithm is stochastic, multiple repeats using the same data). The test then compares the two resulting distributions. Student's T test is a common choice for this kind of comparison, but it's only reliable when the distributions are normally distributed, which is often not the case. A safer bet is Mann-Whitney's U test, since this does not assume that the distributions are normal. For more information, see \\citep{raschka2020model} and \\citep{carrasco2020recent}. Also see \\nameref{multcomparisons} and \\nameref{significance}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do correct for multiple comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things get a bit more complicated when you want to use statistical tests to compare more than two models, since doing multiple pairwise tests is a bit like using the test set multiple times --- it can lead to overly-optimistic interpretations of significance. Basically, each time you carry out a comparison between two models using a statistical test, there's a probability that it will discover significant differences where there aren't any. This is represented by the confidence level of the test, usually set at 95\\%: meaning that 1 in 20 times it will give you a false positive. For a single comparison, this may be a level of uncertainty you can live with. However, it accumulates. That is, if you do 20 pairwise tests with a confidence level of 95\\%, one of them is likely to give you the wrong answer. This is known as the \\textbf{multiplicity effect}, and is an example of a broader issue in data science known as \\textbf{data dredging} or \\textbf{p-hacking} --- see \\citep{head2015extent}. To address this problem, you can apply a correction for multiple tests. The most common approach is the Bonferroni correction, a very simple method that lowers the significance threshold based on the number of tests that are being carried out --- see \\citep{salzberg1997comparing} for a gentle introduction. However, there are numerous other approaches, and there is also some debate about when and where these corrections should be applied; for an accessible overview, see \\citep{streiner2015best}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't always believe results from community benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain problem domains, it has become commonplace to use benchmark data sets to evaluate new ML models. The idea is that, because everyone is using the same data to train and test their models, then comparisons will be more transparent. Unfortunately this approach has some major drawbacks. First, if access to the test set is unrestricted, then you can't assume that people haven't used it as part of the training process. This is known as ``developing to the test set'', and leads to results that are heavily over-optimistic. A more subtle problem is that, even if everyone who uses the data only uses the test set once, collectively the test set is being used many times by the community. In effect, by comparing lots of models on the same test set, it becomes increasingly likely that the best model just happens to over-fit the test set, and doesn't necessarily generalise any better than the other models (see \\nameref{multcomparisons}). For these, and other reasons, you should be careful how much you read into results from a benchmark data set, and don't assume that a small increase in performance is significant. See  \\citep{paullada2020data} for a wider discussion of issues surrounding the use of shared datasets. Also see \\nameref{measure}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do consider combinations of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst this section focuses on comparing models, it's good to be aware that ML is not always about choosing between different models. Often it makes sense to use combinations of models. Different ML models explore different trade-offs; by combing them, you can sometimes compensate for the weaknesses of one model by using the strengths of another model, and vice versa. Such composite models are known as \\textbf{ensembles}, and the process of generating them is known as \\textbf{ensemble learning}. There are lots of ensemble learning approaches --- see \\citep{dong2020survey} for a review. However, they can be roughly divided into those that form ensembles out of the same base model type, e.g.\\ an ensemble of decision trees, and those that combine different kinds of ML models, e.g.\\ a combination of a decision tree, an SVM, and a deep neural network. The first category includes many classic approaches, such as bagging and boosting. Ensembles can either be formed from existing trained models, or the base models can be trained as part of the process, typically with the aim of creating a diverse selection of models that make mistakes on different parts of the data space. A general consideration in ensemble learning is how to combine the different base models; approaches to this vary from very simple methods such as voting, to more complex approaches that use another ML model to aggregate the outputs of the base models. This latter approach is often referred to as \\textbf{stacking} or \\textbf{stacked generalisation}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to report your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of academic research is not self-aggrandisement, but rather an opportunity to contribute to knowledge. In order to effectively contribute to knowledge, you need to provide a complete picture of your work, covering both what worked and what didn't. ML is often about trade-offs --- it's very rare that one model is better than another in every way that matters --- and you should try to reflect this with a nuanced and considered approach to reporting results and conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do be transparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First of all, always try to be transparent about what you've done, and what you've discovered, since this will make it easier for other people to build upon your work. In particular, it's good practice to share your models in an accessible way. For instance, if you used a script to implement all your experiments, then share the script when you publish the results. This means that other people can easily repeat your experiments, which adds confidence to your work. It also makes it a lot easier for people to compare models, since they no longer have to reimplement everything from scratch in order to ensure a fair comparison. Knowing that you will be sharing your work also encourages you to be more careful, document your experiments well, and write clean code, which benefits you as much as anyone else. It's also worth noting that issues surrounding reproducibility are gaining prominence in the ML community, so in the future you may not be able to publish work unless your workflow is adequately documented and shared --- for example, see \\citep{pineau2020improving}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do report performance in multiple ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to achieve better rigour when evaluating and comparing models is to use multiple data sets. This helps to overcome any deficiencies associated with individual data sets (see \\nameref{community}) and allows you to present a more complete picture of your model's performance. It's also good practice to report multiple metrics for each data set, since different metrics can present different perspectives on the results, and increase the transparency of your work. For example, if you use accuracy, it's also a good idea to include metrics that are less sensitive to class imbalances (see \\nameref{accuracy}). If you use a partial metric like precision, recall, sensitivity or specificity, also include a metric that gives a more complete picture of your model's error rates. And make sure it's clear which metrics you are using. For instance, if you report F-scores, be clear whether this is F1, or some other balance between precision and recall. If you report AUC, indicate whether this is the area under the ROC curve or the PR curve. For a broader discussion, see \\citep{blagec2020critical}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't generalise beyond the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important not to present invalid conclusions, since this can lead other researchers astray. A common mistake is to make general statements that are not supported by the data used to train and evaluate models. For instance, if your model does really well on one data set, this does not mean that it will do well on other data sets. Whilst you can get more robust insights by using multiple data sets (see \\nameref{measure}), there will always be a limit to what you can infer from any experimental study. There are numerous reasons for this (see \\citep{paullada2020data}), many of which are to do with how datasets are curated. One common issue is bias, or \\textbf{sampling error}: that the data is not sufficiently representative of the real world. Another is overlap: multiple data sets may not be independent, and may have similar biases. There's also the issue of quality: and this is a particular issue in deep learning datasets, where the need for quantity of data limits the amount of quality checking that can be done. So, in short, don't overplay your findings, and be aware of their limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do be careful when reporting statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I've already discussed statistical tests (see \\nameref{statistical}), and how they can be used to determine differences between ML models. However, statistical tests are not perfect. Some are conservative, and tend to under-estimate significance; others are liberal, and tend to over-estimate significance. This means that a positive test doesn't always indicate that something is significant, and a negative test doesn't necessarily mean that something isn't significant. Then there's the issue of using a threshold to determine significance; for instance, a 95\\% confidence threshold (i.e.\\ when the p-value $<$ 0.05) means that 1 in 20 times a difference flagged as significant won't be significant. In fact, statisticians are increasingly arguing that it is better not to use thresholds, and instead just report p-values and leave it to the reader to interpret these. Beyond statistical significance, another thing to consider is whether the difference between two models is actually important. If you have enough samples, you can always find significant differences, even when the actual difference in performance is miniscule. To give a better indication of whether something is important, you can measure \\textbf{effect size}. There are a range of approaches used for this: Cohen's $d$ statistic is probably the most common, but more robust approaches, such as Kolmogorov-Smirnov, are preferable. For more on this, see \\citep{betensky2019p}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do look at your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained models contain a lot of useful information. Unfortunately many authors just report the performance metrics of a trained model, without giving any insight into what it actually learnt. Remember that the aim of research is not to get a slightly higher accuracy than everyone else. Rather, it's to generate knowledge and understanding and share this with the research community. If you can do this, then you're much more likely to get a decent publication out of your work. So, do look inside your models and do try to understand how they reach a decision. For relatively simple models like decision trees, it can also be beneficial to provide visualisations of your models, and most libraries have functions that will do this for you. For complex models, like deep neural networks, consider using \\textbf{explainable AI} (XAI) techniques to extract knowledge (surveyed in \\cite{li2020survey}); they're unlikely to tell you exactly what the model is doing, but they may give you some useful insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document doesn't tell you everything you need to know, the lessons sometimes have no firm conclusions, and some of the things I've told you might be wrong, or at least debateable. This, I'm afraid, is the nature of research. The theory of how to do ML almost always lags behind the practice, academics will always disagree about the best ways of doing things, and what we think is correct today may not be correct tomorrow. Therefore, you have to approach ML in much the same way you would any other aspect of research: with an open mind, a willingness to keep up with recent developments, and the humility to accept you don't know everything.\n"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {}
}
