{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Обучение с подкреплением</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение методом проб и ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Традиционные подходы к обучению машин не отражают полностью процесс, присущий интеллектуальным существам. К полноценному обучению, свойственному не только человеку, но и живым организмам в целом, задачи классического машинного обучения имеют лишь косвенное отношение. Поэтому требуется иная формализация понятия «задачи, требующей интеллектуального решения», где обучение будет осуществляться не на основе заранее подготовленного набора данных.\n",
    "\n",
    "Термин \"подкрепление\" (**reinforcement**) пришел из поведенческой психологии и обозначает награду или наказание за определенный результат, зависящий не только от принятых решений, но и от внешних, не обязательно подконтрольных факторов.\n",
    "\n",
    "В этом контексте обучение понимается как поиск способов достижения желаемого результата методом проб и ошибок (trial and error), то есть попыток решить задачу и использования накопленного опыта для совершенствования своей стратегии в будущем.\n",
    "\n",
    "**Пример обучения методом проб и ошибок**:\n",
    " Вы учитесь готовить новое блюдо без рецепта. В первый раз вы добавляете слишком много соли, и блюдо получается слишком соленым. Во второй раз вы уменьшаете количество соли и блюдо получается вкусным. Используя метод проб и ошибок, вы постепенно находите оптимальное количество соли для этого рецепта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мотивация использования обучения с подкреплением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства и непротиворечивости определения данной парадигмы рассмотрим ключевые особенности уже известного нам подхода обучения с учителем и введем ключевую мотивацию использования методов обучения с подкреплением.\n",
    "\n",
    "Для успешного обучения с учителем необходимо выполнение трех ключевых условий:\n",
    "1. Исчерпывающий датасет, который покрывает все возможные вариации данных, чтобы модель могла учиться на различных примерах.\n",
    "\n",
    "2. Необходимы ответы или разметка данных, которая предоставляет правильные решения для каждого примера в датасете. Это позволяет модели корректировать свои предсказания в процессе обучения.\n",
    "\n",
    "3. Требуется дифференцируемая функция потерь, которая измеряет, насколько точны предсказания модели по сравнению с реальными ответами, и направляет процесс оптимизации параметров модели.\n",
    "\n",
    "*Пример задачи обучения с учителем* — решение задачи классификации рукописных цифр в датасете MNIST при помощи CNN.\n",
    "\n",
    "\n",
    "Однако, в ряде задач выполнение этих трех пунктов (или части из них) невозможно. Именно в таких ситуациях используется обучение с подкреплением. Этот подход не требует заранее размеченного датасета и дифференцируемой функции потерь. Вместо этого агент обучается на основе взаимодействия со средой, получая награды за свои действия. Это позволяет применять обучение с подкреплением в задачах, где заранее определить все возможные состояния и действия невозможно, а разметка данных затруднительна или невозможна.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/sl_rl_comp.png\" alt=\"Drawing\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Обучение с учителем vs обучение с подкреплением</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, разрабатывая автопилот для автомобиля, мы вряд ли сможем составить обучающую выборку, в которой будут перечислены все возможные дорожные ситуации в условиях плотного городского трафика, для которых к тому же будет известна разметка \"ground truth\" целевой переменной о предпочтительном решении автопилота в них.\n",
    "\n",
    "Невозможность собрать исчерпывающий датасет во многом может быть связана с тем, как в примере с автопилотом автомобиля, что **результат работы алгоритма может существенно изменить состояние окружающей его среды**, которая и является источником данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " При этом, не имея всей картины возможных действий и их последствий, мы можем предоставить нашему алгоритму возможность взаимодействовать с источником данных (**средой**) и постфактум оценить действия нашего алгоритма (**агента**), и, используя эту оценку, подстроить алгоритм так, чтобы он чаще совершал желательные действия, и реже — нежелательные. В этом случае обучение строится таким образом, чтобы алгоритм (**агент**) стремился максимизировать получаемое вознаграждение (**reward**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы можем резюмировать особенности парадигмы обучения с подкреплением.\n",
    "\n",
    "**Обучение с подкреплением**:\n",
    "- Существует произвольный источник данных — среда.\n",
    "- Среда **не обязана** быть определена в виде исчерпывающего и стационарного датасета.\n",
    "- Среда **может** изменяться после каждого предсказания алгоритма (решения агента).\n",
    "- Существует некоторый алгоритм (агент), который строит свое предсказание на основе данных, полученных из среды.\n",
    "- Можно сформулировать количественную оценку характера взаимодействия агента со средой (reward, функцию награды).\n",
    "- Функция награды **не обязана** быть дифференцируемой по весам модели машинного обучения (агента).\n",
    "- Функция награды **может** быть определена для последовательности решений агента.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/rl_def.png\" width=\"750\"></center>\n",
    "\n",
    "<center><em>Обучение с подкреплением<em/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Устоявшаяся терминология"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Введем основные определения и опишем формальную постановку задачи.\n",
    "\n",
    "* Интеллектуальную сущность (систему/робота/алгоритм), принимающую решения, будем называть агентом (**agent**).\n",
    "\n",
    "* Агент взаимодействует с миром или средой (**environment**), которая задается зависящим от времени состоянием (**state**). Агенту в каждый момент времени доступно лишь некоторое наблюдение (**observation**) текущего состояния мира.\n",
    "\n",
    "* Сам агент задает процедуру выбора действия (**action**) по доступным наблюдениям, эту процедуру будем называть стратегией или политикой (**policy**).\n",
    "\n",
    "* Под желаемым результатом мы будем понимать максимизацию некоторой скалярной величины, называемой наградой (**reward**).\n",
    "\n",
    "* Процесс взаимодействия агента и среды задается динамикой среды, определяющей правила изменения состояний среды во времени и генерации награды.\n",
    "\n",
    "* **Эпизод** — серия взаимодействий агента со средой (например, партия в игре).\n",
    "\n",
    "\n",
    "\n",
    "Буквы $s, a, r$ зарезервируем для обозначения состояний, действий и наград соответственно, буквой $t$ будем обозначать время в процессе взаимодействия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/rl_msuai.png\" alt=\"Drawing\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример, где агентом является ребенок (**agent**), который учится ездить на велосипеде:\n",
    "\n",
    "Среда (**environment**) включает в себя велосипед и окружающий мир, в котором он ездит.\n",
    "\n",
    "В каждый момент времени состояние (**state**) может содеражать, например, текущую скорость, угол наклона велосипеда и положение ребенка.\n",
    "\n",
    "Действие (**action**) представляет собой управление рулем и педалями в каждый момент времени.\n",
    "\n",
    " Награда (**reward**) - это положительное подкрепление в виде похвалы от родителей или радости от езды без падений и отрицательное подкрепление в виде боли от падений.\n",
    "\n",
    " Политика (**policy**) - это стратегия ребенка, например, как балансировать и крутить педали, которую он улучшает с опытом, стремясь минимизировать падения и получать больше положительных эмоций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры задач, решаемых с использованием RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Управление беспилотным транспортом, задачи математической теории оптимального управления**\n",
    "\n",
    "[[arxiv] 🎓 Deep Reinforcement Learning for Autonomous Driving: A Survey](https://arxiv.org/abs/2002.00444)<br>\n",
    "[[arxiv] 🎓 Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review](https://arxiv.org/abs/1805.00909)<br>\n",
    "[[git] 🐾 Lecture notes and exercises for control theory course](https://github.com/DPritykin/Control-Theory-Course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/cooling_cs.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Управление системой охлаждения датацентра при помощи методов обучения с подкреплением</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://deepmind.google/discover/blog/safety-first-ai-for-autonomous-data-centre-cooling-and-industrial-control/\">Safety-first AI for autonomous data centre cooling and industrial control</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В настоящее время большое число исследовательских групп занято разработкой решений для создания беспилотного транспорта. Алгоритм принятия решений в таких решениях часто основан на алгоритмах обучения с подкреплением ([Deep reinforcement learning on-board an autonomous car ✏️[blog]](https://wayve.ai/thinking/learning-to-drive-in-a-day/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Source: https://wayve.ai/wp-content/uploads/2022/06/ezgif.com-gif-maker1.mp4\n",
    "!wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/rl_control_system.mp4\n",
    "\n",
    "mp4 = open(\"rl_control_system.mp4\", \"rb\").read()\n",
    "data_url = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "HTML(f\"<video width=1000 controls><source src={data_url} type='video/mp4'></video>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Оптимизация недифференцируемых функционалов**\n",
    "\n",
    "[[article] 🎓 Reinforcement learning for combinatorial optimization: A survey](https://doi.org/10.1016/j.cor.2021.105400)<br>\n",
    "[[arxiv] 🎓 Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Создание игровых агентов**\n",
    "\n",
    "[[arxiv] 🎓 Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) <br>\n",
    "[[blog] ✏️ AlphaGo](https://deepmind.google/technologies/alphago/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/alpha_zero.png\" width=\"750\"></center>\n",
    "\n",
    "<center><em>The trained network is used to guide a search algorithm – known as Monte-Carlo Tree Search (MCTS) — to select the most promising moves in games. For each move, AlphaZero searches only a small fraction of the positions considered by traditional chess engines. In Chess, for example, it searches only 60 thousand positions per second, compared to roughly 60 million for Stockfish.</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/\">AlphaZero: Shedding new light on chess, shogi, and Go</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Обработка естественного языка**\n",
    "\n",
    "[[book] 📚 Reinforcement Learning for NLP](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf)<br>\n",
    "[[arxiv] 🎓 Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/rlhf.png\" width=\"900\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://huyenchip.com/2023/05/02/rlhf.html\">RLHF: Reinforcement Learning from Human Feedback</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Прочие приложения:**\n",
    "  \n",
    "  - Рекомендательные системы\n",
    "  - Трейдинг, принятие решений в условиях неопределенности, управление инвестициями\n",
    "  - Drug discovery\n",
    "  - (Neural Architecture Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры работ с использованием обучения с подкреплением у выпускников курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В работе **Управление движением сферического робота с помощью нейронной сети** решалась задача подбора управляющего сигнала для робота, позволяющего ему двигаться по определенной заранее траектории на плоскости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/sphere_robot.png\" width=\"350\"></center>\n",
    "\n",
    "<center><em>Сферический робот<em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/EduNetArchive/Nor_RL_sphere_robot\">EduNet-archive: RL sphere robot</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/sphere_robot_trajectories.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Сравнение целевых траекторий движения с полученными вследствие управления роботом при помощи RL-алгоритма</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/EduNetArchive/Nor_RL_sphere_robot\">EduNet-archive: RL sphere robot</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее:\n",
    "\n",
    "* [[video] 📺 Видеозапись выступления](https://www.youtube.com/watch?v=X_Wywj8lVHM)\n",
    "* [[slides] 📊 Презентация](https://docs.google.com/presentation/d/1qBizkCjiv2UsAnfdvN7yyOXY7QD0Ni7i/edit#slide=id.p1)\n",
    "* [[git] 🐾 Код](https://github.com/EduNetArchive/Nor_RL_sphere_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В работе **Самоюстирующиеся оптические системы на основе машинного обучения с подкреплением** был разработан управляющий алгоритм, позволяющий автоматизировать и ускорить процесс настройки сложной оптической экспериментальной установки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/optical_system.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Схема оптической установки, генерирующей рентгеновские лучи</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/EduNetArchive/Mareev_X_ray_AI\">EduNet-archive: NN for controlling z-axis</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/optical_system_results.png\" width=\"500\"></center>\n",
    "\n",
    "<em><center>Результаты работы алгоритма стабилизации выходного сигнала на основе RL-алгоритма</em></center>\n",
    "\n",
    "<em><center>Source: <a href=\"https://github.com/EduNetArchive/Mareev_X_ray_AI\">EduNet-archive: NN for controlling z-axis</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее:\n",
    "\n",
    "* [[video] 📺 Видеозапись выступления](https://www.youtube.com/watch?v=mYKIvb9Y2z8)\n",
    "* [[slides] 📊 Презентация](https://docs.google.com/presentation/d/1gfFgQY_gm6eQ3yrL5p2b_CjbFE2xvzAL/edit#slide=id.p1)\n",
    "* [[git] 🐾 Код](https://github.com/EduNetArchive/Mareev_X_ray_AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateless environment in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим простейший пример среды для задачи обучения с подкреплением:\n",
    "- среда стационарна, т.е. не изменяется во времени\n",
    "- среда не изменяется после действий агента\n",
    "\n",
    "Фактически можно сказать, что такая среда всегда находится в одном состоянии или вообще *не имеет состояния*. Для нас будет важно, что выдаваемый средой reward не обязан быть дифференцируемым, и потому работа агента по взаимодействию с такой вырожденной средой не может быть корректно решена в рамках парадигмы обучения с учителем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача о многоруких бандитах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примером stateless-среды может послужить классическая [задача о многоруких бандитах 📚[wiki]](https://en.wikipedia.org/wiki/Multi-armed_bandit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/mab.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласно условиям данной задачи мы имеем:\n",
    "- $K$ игровых автоматов, которые мы можем дернуть за ручку в надежде получить выигрыш,\n",
    "- $k$-тый игровой автомат возвращает выигрыш с вероятностью $p_k$,\n",
    "- вероятности выигрыша $\\{p_k\\}$ не изменяются во времени и вследствие решений игрока,\n",
    "- вероятности выигрыша неизвестны игроку.\n",
    "\n",
    "**Задача:** игроку необходимо придумать такую стратегию нажатия на рычаги многорукого бандита, которая бы обеспечивала максимум возможного выигрыша.\n",
    "\n",
    "Для того, чтобы приступить к решению этой задачи, нам необходимо создать модель описанной среды. Воспользуемся библиотекой Gymnasium, которую часто используют при решении задач методами обучения с подкреплением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как приступить к созданию интересующей нас среды, разберемся, что такое Gymnasium и какие возможности он предоставляет для обучения с подкреплением.\n",
    "\n",
    "**Gymnasium** (Gym) — это набор инструментов для разработки и сравнения алгоритмов RL, позволяющий стандартизировать взаимодействие между разными алгоритмами RL и средами. Также он предоставляет набор стандартных сред, которые могут, в том числе, использоваться для бенчмаркинга.\n",
    "\n",
    "* [[git] 🐾 Gymnasium](https://github.com/Farama-Foundation/Gymnasium)\n",
    "* [[doc] 🛠️ Документация](https://gymnasium.farama.org/)\n",
    "* [[colab] 🥨 Colab demonstration](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала рассмотрим устройство среды Gym в целом.\n",
    "__Среда__ — это некоторая модель мира, отвечающая за предоставление наблюдений и вознаграждений, в которой существует __агент__. Состояние среды будет изменяться в зависимости от действий агента.\n",
    "\n",
    "Рассмотрим для начала стандартную среду MountainCar, в которой стоит задача — довести машину до вершины горы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/mountain-car-v0.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym as openaigym\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Gym среды представлены классом `gym.Env`, который является унифицированным интерфейсом среды со следующими атрибутами и методами:\n",
    "* `action_space`: описание действий, допустимых в данной среде;\n",
    "* `observation_space`: структура и допустимые значения наблюдений состояния среды;\n",
    "* `reset()`: сбрасывает среду и возвращает случайное исходное состояние;\n",
    "* `step(action)`: метод, продвигающий развитие окружающей среды на одно действие и возвращающий информацию о результате этого действия, а именно:\n",
    "    * observation — следующее наблюдение;\n",
    "    * reward — локальное вознаграждение;\n",
    "    * done — флаг конца эпизода.\n",
    "\n",
    "Также в классе `gym.Env` есть несколько вспомогательных методов, например, `render()`, позволяющий представить наблюдение в понятной человеку форме, но мы не будем их касаться в нашей лекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пространства действий и наблюдений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действия агента могут быть дискретными, непрерывными и комбинированными. Дискретные действия представлены фиксированным набором и взаимно исключают друг друга, например, нажатие/отпускание клавиши. Непрерывным действиям соответствуют значения в некотором диапазоне, например, поворот руля от -720 до 720 градусов. В среде мы не ограничены каким-то одним действием, и вполне допустимо одновременное нажатие нескольких кнопок и одновременный поворот руля. Аналогичным образом наблюдения могут быть дискретными (лампочка включена/выключена) или непрерывными (тензоры, соответствующие цветным изображениям).\n",
    "\n",
    "Давайте посмотрим на то, как выглядят пространства действий и наблюдений в среде MountainCar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# Action and observation space\n",
    "action_space = env.action_space\n",
    "obs_space = env.observation_space\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"The action space: {action_space}\")\n",
    "print(f\"The observation space: {obs_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что пространство наблюдений и действий представлено некоторыми классами `Box` и `Discrete` соответственно. Что же это за классы? Для объединения нескольких пространств действий (например, непрерывных и дискретных) в одно действие, в Gym существует специальный класс контейнеров.\n",
    "\n",
    "* Класс `Discrete` представляет набор $n$ взаимоисключающих элементов. Например, `Discrete(n=4)` может быть использован для пространства действий с 4 направлениями движения ($\\leftarrow \\downarrow \\rightarrow \\uparrow$).\n",
    "* Класс `Box` представляет n-мерный тензор рациональных чисел в некотором диапазоне `[low, high]`. Например, нажатие педали газа от минимального значения 0 до максимального — 1, можно закодировать как `с`, аналогично наблюдение экрана игры, можно закодировать как `Box(low=0, high=255, shape=(100, 50, 3), dtype=np.float32)`.\n",
    "\n",
    "Давайте посмотрим на диапазон допустимых значений пространства наблюдений в среде MountainCar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upper Bound for Env Observation\", env.observation_space.high)\n",
    "print(\"Lower Bound for Env Observation\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обе структуры данных происходят от класса `gym.Space`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(env.action_space))\n",
    "print(type(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также стоит упомянуть еще один дочерний класс  `gym.Space` &mdash; `Tuple`, позволяющий объединять несколько экземпляров класса `gym.Space` вместе. Благодаря этому классу мы можем создавать пространства действий и наблюдений любой сложности.\n",
    "\n",
    "В классе `gym.Space` реализованы методы, позволяющие взаимодействовать с пространствами действий и наблюдений:\n",
    "* `sample()`: возвращает случайный пример из пространства наблюдений,\n",
    "* `contains(x)`: проверяет принадлежность аргумента к пространству наблюдений.\n",
    "\n",
    "Давайте возьмем случайное действие, доступное нам в исходном состоянии среды MountainCar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_action = env.action_space.sample()  # random number from 0 to 2\n",
    "print(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Взаимодействие со средой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем совершить случайное действие, которое мы выбрали выше, для этого перезапустим среду, чтобы вернуться в изначальное состояние, и сделаем шаг с помощью метода `step()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and see the initial observation\n",
    "obs, info = env.reset()\n",
    "print(f\"The initial observation is {obs}\")\n",
    "\n",
    "# Take the action and get the new observation space\n",
    "new_obs, reward, done, truncated, info = env.step(random_action)\n",
    "print(f\"The new observation is {new_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание своей среды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из простых задач обучения с подкреплением — задача о многоруких бандитах. В этой задаче состояние среды не меняется, а у агента есть фиксированный набор действий, которые он может выполнять.\n",
    "\n",
    "Эту задачу можно представить таким образом: вы игрок в казино с игровыми автоматами, в которых с различной вероятностью возможно выиграть приз различной величины. Ваша задача как агента — выиграть как можно больше денег, бросая монетки в автоматы на ваш выбор.\n",
    "\n",
    "Допустим, в казино находятся 4 автомата, лишь для одного из которых средний выигрыш будет > 1.\n",
    "\n",
    "Давайте напишем класс, который будет описывать эту ситуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from gym import Env, spaces\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class MultiArmedBanditEnv(Env):\n",
    "    def __init__(self, n=4):\n",
    "        \"\"\"\n",
    "        n - number of arms in the bandit\n",
    "        \"\"\"\n",
    "        self.num_bandits = n\n",
    "        self.action_space = spaces.Discrete(self.num_bandits)\n",
    "        self.observation_space = spaces.Discrete(1)  # the reward of the last action\n",
    "        self.bandit_success_prob = np.array(\n",
    "            [0.5, 0.1, 0.9, 0.2]\n",
    "        )  # success probabilities\n",
    "        self.bandit_reward = np.array([2, 20, 1, 3])\n",
    "\n",
    "    def step(self, action):\n",
    "        done = True\n",
    "        result_prob = np.random.random()\n",
    "        if result_prob < self.bandit_success_prob[action]:\n",
    "            reward = self.bandit_reward[action]\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск оптимальной стратегии решения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайный агент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем казино для того, чтобы принять участие в игре, нужно заплатить 1 монету. После оплаты игрок может нажать на любой рычаг игрового автомата и получить выигрыш с соответствующей вероятностью.\n",
    "\n",
    "Как именно в таком случае мы можем определить, за какой рычаг нам стоит дергать, чтобы максимизировать выигрыш? Ведь мы как игрок не знаем вероятности выигрыша для разных рычагов.\n",
    "\n",
    "Для начала реализауем случайную стратегию, наш агент будет равновероятно выбирать автомат для игры на каждом шаге:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Our agent initializes reward estimates as zeros.\n",
    "        This estimates will be updated incrementally after each\n",
    "        interaction with the environment.\n",
    "        \"\"\"\n",
    "        self.reward_estimates = np.zeros(4)\n",
    "        self.action_count = np.zeros(4)\n",
    "        self.cache = 1000  # initial amount of coins agent possesses\n",
    "\n",
    "    def get_action(self):\n",
    "        # pay 1 coin for the action\n",
    "        self.cache -= 1\n",
    "\n",
    "        # select a random action\n",
    "        action = np.random.choice(len(self.reward_estimates))\n",
    "\n",
    "        # add 1 to action selected in the action count\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_estimates(self, reward, action):\n",
    "        # update amount of cache by reward from our previous action\n",
    "        self.cache += reward\n",
    "\n",
    "        # compute the difference between the received rewards vs the reward estimates\n",
    "        error = reward - self.reward_estimates[action]\n",
    "\n",
    "        # update the reward estimate incrementally\n",
    "        n = self.action_count[action]\n",
    "        self.reward_estimates[action] += (1 / n) * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = MultiArmedBanditEnv()\n",
    "agent = RandomAgent()\n",
    "\n",
    "mean_reward_random = []\n",
    "rewards = []\n",
    "\n",
    "while (agent.cache > 0) and (len(mean_reward_random) != 10000):\n",
    "    act = agent.get_action()\n",
    "    reward, done = env.step(act)\n",
    "    agent.update_estimates(reward, act)\n",
    "    rewards.append(reward)\n",
    "    mean_reward_random.append(sum(rewards) / (len(rewards)))\n",
    "\n",
    "print(f\"Action counts: {agent.action_count}\")\n",
    "print(f\"Reward estimates: {agent.reward_estimates}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(mean_reward_random, label='Random Agent')\n",
    "ax[0].set_title('Average Reward over Time')\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "actions = ['Arm 1 (p=0.5, r=2)',\n",
    "           'Arm 2 (p=0.1, r=20)',\n",
    "           'Arm 3 (p=0.9, r=1)',\n",
    "           'Arm 4 (p=0.2, r=3)']\n",
    "ax[1].bar(actions, agent.action_count, label='Random Agent')\n",
    "ax[1].set_title('Action Selection Distribution')\n",
    "ax[1].set_ylabel('Count')\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=15)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Жадная стратегия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Попробуем сделать реализацию жадным способом: на каждом шаге будем совершать действие, имеющее максимальную оценку, и, в зависимости от результата, будем обновлять эту оценку.\n",
    "\n",
    "Давайте разберемся, что она представляет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценить ценность того или иного действия можно по среднему выигрышу, к которому оно приведет.\n",
    "\n",
    "Пусть $R_n$ — награда, полученная за выполнение $n$-го действия. Тогда оценкой ценности действия будет $\\displaystyle Q_n=\\frac{R_1+R_2+...+R_n}{n}$. Величина $Q_n$ по сути представляет собой простое скользящее среднее (_simple moving average_), которое может вычисляться постепенно:\n",
    "\n",
    "$\\displaystyle Q_n = \\frac{1}{n}(R_n+(n-1)Q_{n-1}) = \\frac{1}{n}(R_n+nQ_{n-1}-Q_{n-1}) = Q_{n-1}+\\frac{1}{n}(R_n-Q_{n-1})$\n",
    "\n",
    "В целом, подобная схема обновления параметров в обучении с подкреплением, представимая как `новая оценка:=старая оценка + шаг[цель — старая оценка]`, встречается довольно часто и называется _incremental update rule_.\n",
    "\n",
    "Для получения оценок действий жадным способом нам понадобится функция `argmax`, которая будет выбирать действие с наибольшей оценкой, а в случае равенства максимальной оценки будет выбирать случайное действие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(x):\n",
    "    return np.random.choice(np.flatnonzero(x == x.max()))\n",
    "\n",
    "\n",
    "class GreedyAgent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Our agent initializes reward estimates as zeros.\n",
    "        This estimates will be updated incrementally after each\n",
    "        interaction with the environment.\n",
    "        \"\"\"\n",
    "        self.reward_estimates = np.zeros(4)\n",
    "        self.action_count = np.zeros(4)\n",
    "        self.cache = 1000  # initial amount of coins agent posesses\n",
    "\n",
    "    def get_action(self):\n",
    "        # Pay 1 coin for the action\n",
    "        self.cache -= 1\n",
    "\n",
    "        # Select greedy action\n",
    "        action = argmax(self.reward_estimates)\n",
    "\n",
    "        # Add a 1 to action selected in the action count\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_estimates(self, reward, action):\n",
    "        # Update amount of cache by reward from our previuos action\n",
    "        self.cache += reward\n",
    "\n",
    "        # Compute the difference between the received rewards vs the reward estimates\n",
    "        error = reward - self.reward_estimates[action]\n",
    "\n",
    "        # Update the reward estimate incementally\n",
    "        n = self.action_count[action]\n",
    "        self.reward_estimates[action] += (1 / n) * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = MultiArmedBanditEnv()\n",
    "agent = GreedyAgent()\n",
    "\n",
    "mean_reward_greedy = []\n",
    "rewards = []\n",
    "\n",
    "while (agent.cache > 0) and (len(mean_reward_greedy) != 10000):\n",
    "    act = agent.get_action()\n",
    "    reward, done = env.step(act)\n",
    "    agent.update_estimates(reward, act)\n",
    "    rewards.append(reward)\n",
    "    mean_reward_greedy.append(sum(rewards) / (len(rewards)))\n",
    "\n",
    "print(f\"Action counts: {agent.action_count}\")\n",
    "print(f\"Reward estimates: {agent.reward_estimates}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(mean_reward_greedy, label='Greedy Agent')\n",
    "ax[0].plot(mean_reward_random, label='Random Agent')\n",
    "ax[0].set_title('Average Reward over Time')\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "actions = ['Arm 1 (p=0.5, r=2)',\n",
    "           'Arm 2 (p=0.1, r=20)',\n",
    "           'Arm 3 (p=0.9, r=1)',\n",
    "           'Arm 4 (p=0.2, r=3)']\n",
    "ax[1].bar(actions, agent.action_count, label='Greedy Agent')\n",
    "ax[1].set_title('Action Selection Distribution')\n",
    "ax[1].set_ylabel('Count')\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=15)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В жадной стратегии выбор действия детерминирован. Агент всегда выбирает действие с наивысшей оценкой, что означает отсутствие распределения вероятностей для действий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-жадная стратегия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что жадная стратегия не позволяет нам добиться оптимального принятия решений, поскольку оптимальную стратегию легко пропустить, если в начале агенту не повезет.\n",
    "\n",
    "Вместо жадной стратегии будем использовать $\\varepsilon$-жадную стратегию, которую можно описать как \"оптимизм при неопределенности\". Суть $\\varepsilon$-жадной стратегии заключается в следующем: с некоторой вероятностью $\\varepsilon$ совершать случайное действие, а с вероятностью $1-\\varepsilon$ вести себя жадно.\n",
    "\n",
    "$\\varepsilon$ — важный параметр алгоритма. Как правило, в начале используют большие значения $\\varepsilon$, тогда агент действует почти случайно и \"исследует мир\", а затем их уменьшают, и действия агента становятся близкими к оптимальным.\n",
    "\n",
    "Выбор величины $\\varepsilon$ связан с так называемым __exploration-exploitation trade-off__. Он может быть легко проиллюстрирован примером, когда вы не можете решить, пойти ли сегодня в любимый ресторан или попробовать сходить в новый? Новый ресторан может оказаться лучшего привычного, но это решение связано с риском разочарования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/exploration_vs_exploitation.png\" alt=\"Drawing\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(GreedyAgent):\n",
    "    def __init__(self, epsilon):\n",
    "        GreedyAgent.__init__(self)\n",
    "        # Store the epsilon value\n",
    "        assert epsilon >= 0 and epsilon <= 1\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_action(self):\n",
    "        # We need to redefine this function so that it takes an exploratory action with epsilon probability\n",
    "\n",
    "        # Pay 1 coin for the action\n",
    "        self.cache -= 1\n",
    "        # One hot encoding: 0 if exploratory, 1 otherwise\n",
    "        action_type = int(np.random.random() > self.epsilon)\n",
    "        # Generate both types of actions for every experiment\n",
    "        exploratory_action = np.random.randint(4)\n",
    "        greedy_action = argmax(self.reward_estimates)\n",
    "        # Use the one hot encoding to mask the actions for each experiment\n",
    "        action = greedy_action * action_type + exploratory_action * (1 - action_type)\n",
    "\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = EpsilonGreedyAgent(epsilon=0.25)\n",
    "\n",
    "mean_reward_e_greedy = []\n",
    "rewards = []\n",
    "\n",
    "while (agent.cache > 0) and (len(mean_reward_e_greedy) != 10000):\n",
    "    act = agent.get_action()\n",
    "    reward, done = env.step(act)\n",
    "    agent.update_estimates(reward, act)\n",
    "    rewards.append(reward)\n",
    "    mean_reward_e_greedy.append(sum(rewards) / len(rewards))\n",
    "    # Decrease epsilon with time until we reach 0.01 chanse to perform exploratory action\n",
    "    if agent.epsilon > 0.01:\n",
    "        agent.epsilon -= 0.0001\n",
    "\n",
    "print(f\"Action counts: {agent.action_count}\")\n",
    "print(f\"Reward estimates: {agent.reward_estimates}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(mean_reward_e_greedy, label='e-Greedy Agent')\n",
    "ax[0].plot(mean_reward_greedy, label='Greedy Agent')\n",
    "ax[0].plot(mean_reward_random, label='Random Agent')\n",
    "ax[0].set_title('Average Reward over Time')\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "actions = ['Arm 1 (p=0.5, r=2)',\n",
    "           'Arm 2 (p=0.1, r=20)',\n",
    "           'Arm 3 (p=0.9, r=1)',\n",
    "           'Arm 4 (p=0.2, r=3)']\n",
    "ax[1].bar(actions, agent.action_count, label='e-Greedy Agent')\n",
    "ax[1].set_title('Action Selection Distribution')\n",
    "ax[1].set_ylabel('Count')\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=15)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " В эпсилон-жадной стратегии, агент действует жадно большую часть времени, но с вероятностью эпсилон ($ε$) выбирает действие случайно, что добавляет элемент случайности и частичное распределение вероятностей, но только в те моменты, когда срабатывает эпсилон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax-стратегия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование $\\epsilon$-жадной стратегии позволило нам иногда совершать \"разведывательные\" решения, которые могли быть не оптимальны относительно буквальной интерпретации накопленного нами ранее опыта. Аналогичного эффекта мы бы могли достигнуть, если бы определили нашу стратегию принятия решения вероятностной.\n",
    "\n",
    "Пусть мы находимся в состоянии $s$, для которого возможны $K$ действий $\\{a_i\\}$. Определим величину $\\pi(a_i|s)$:\n",
    "- $\\pi(a_i| s)$ — вероятность принять решение $a_i$, если агент находится в состоянии $s$.\n",
    "\n",
    "Данное распределение вероятности принято называть **политикой (policy)**.\n",
    "\n",
    "Для \"игры\" с использованием такой политики нам нужно:\n",
    "1. Вычислить распределение вероятности $\\pi(a_i| s)$.\n",
    "2. Сгенерировать случайное решение $a$, удовлетворяющее данному распределению: $a \\sim \\pi(a_i| s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "class SoftmaxAgent(GreedyAgent):\n",
    "    def __init__(self):\n",
    "        GreedyAgent.__init__(self)\n",
    "\n",
    "    def get_action(self):\n",
    "        # We need to redefine this function so that it takes an action with probability pi(a,s)\n",
    "\n",
    "        # Pay 1 coin for the action\n",
    "        self.cache -= 1\n",
    "\n",
    "        # get \\pi(a_i|s):\n",
    "        pi = softmax(self.reward_estimates)\n",
    "        # sample action\n",
    "        action = choice(np.arange(0, 4, 1), 1, p=pi)[0]\n",
    "\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SoftmaxAgent()\n",
    "\n",
    "mean_reward_softmax_agent = []\n",
    "rewards = []\n",
    "\n",
    "while (agent.cache > 0) and (len(mean_reward_softmax_agent) != 10000):\n",
    "    act = agent.get_action()\n",
    "    reward, done = env.step(act)\n",
    "    agent.update_estimates(reward, act)\n",
    "    rewards.append(reward)\n",
    "    mean_reward_softmax_agent.append(sum(rewards) / len(rewards))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"Action counts: {agent.action_count}\")\n",
    "print(f\"Reward estimates: {agent.reward_estimates}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(mean_reward_softmax_agent, label='Softmax Agent')\n",
    "ax[0].plot(mean_reward_e_greedy, label='e-Greedy Agent')\n",
    "ax[0].plot(mean_reward_greedy, label='Greedy Agent')\n",
    "ax[0].plot(mean_reward_random, label='Random Agent')\n",
    "ax[0].set_title('Average Reward over Time')\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "actions = ['Arm 1 (p=0.5, r=2)',\n",
    "           'Arm 2 (p=0.1, r=20)',\n",
    "           'Arm 3 (p=0.9, r=1)',\n",
    "           'Arm 4 (p=0.2, r=3)']\n",
    "ax[1].bar(actions, agent.action_count, label='Softmax Agent')\n",
    "ax[1].set_title('Action Selection Distribution')\n",
    "ax[1].set_ylabel('Count')\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=15)\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В softmax-стратегии  агент всегда выбирает действие на основе распределения вероятностей, которое пропорционально экспонентам оценок вознаграждений. Это означает, что действия всегда сэмплируются из распределения вероятностей, что обеспечивает баланс между исследованием и использованием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мета-эвристики в обучении с подкреплением\n",
    "\n",
    "Мета-эвристики представляют собой обобщенные алгоритмы для поиска решений в сложных оптимизационных задачах, где мы не можем использовать градиентные методы оптимизации.\n",
    "\n",
    "Мы рассматриваем их как решение задачи black-box оптимизации. Мы можем поместить агента в среду реализовать выбранную стратегию, оценить результат и сделать вывод о том хорошую стратегию мы подобрали или нет.\n",
    "\n",
    "Задача алгоритма оптимизации состоит в том, чтобы на основе лишь этой информации предлагать, какие стратегии следует попробовать следующими. Рассмотрим один из таких подходов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy method (CEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Кросс-энтропийный метод (CEM) — это мета-эвристический алгоритм оптимизации, который используется для поиска оптимальных решений путем итеративного улучшения выборки решений. Он включает следующие шаги:\n",
    "\n",
    "1. **Инициализация распределения:**\n",
    "   - Начните с инициализации параметров вероятностного распределения $( P(\\theta)$), например, нормального распределения с некоторыми начальными средними и дисперсиями.\n",
    "\n",
    "2. **Генерация выборки:**\n",
    "   - Сгенерируйте выборку кандидатов $( \\{\\theta_1, \\theta_2, ..., \\theta_N\\} $) из текущего распределения $( P(\\theta) $).\n",
    "\n",
    "3. **Оценка кандидатов:**\n",
    "   - Для каждого кандидата $( \\theta_i $) оцените его качество или производительность с помощью функции награды $( S(\\theta_i) $).\n",
    "\n",
    "4. **Отбор лучших кандидатов:**\n",
    "   - Отберите некоторое количество $( k $) лучших кандидатов из выборки на основе их оценок. Эти лучшие кандидаты составляют элитный набор.\n",
    "\n",
    "5. **Обновление распределения:**\n",
    "   - Обновите параметры распределения $( P(\\theta) $) на основе элитного набора. Например, если распределение является нормальным, обновите его средние и дисперсии, используя значения элитных кандидатов.\n",
    "\n",
    "6. **Проверка условия остановки:**\n",
    "   - Проверьте, выполнено ли условие остановки (например, достижение определенного уровня производительности или истечение максимального числа итераций). Если да, завершите алгоритм. Если нет, вернитесь к шагу 2. В конце алгоритма выберите лучшие параметры $( \\theta $), найденные в процессе итераций, как решение оптимизационной задачи.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/schematic_view_cross_entropy.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример CEM в Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся готовой средой [MountainCarContinuous 🛠️[doc]](https://mgoulao.github.io/gym-docs/environments/classic_control/mountain_car_continuous/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/mountain-car-v0.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env.seed(42)\n",
    "clear_output()\n",
    "\n",
    "print(\"observation space:\", env.observation_space)\n",
    "print(\"action space:\", env.action_space)\n",
    "print(\"  - low:\", env.action_space.low)\n",
    "print(\"  - high:\", env.action_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = env.action_space.shape[0]\n",
    "        # define layers (we used 2 layers)\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        s_size = self.s_size\n",
    "        h_size = self.h_size\n",
    "        a_size = self.a_size\n",
    "        # separate the weights for each layer\n",
    "        fc1_end = (s_size * h_size) + h_size\n",
    "        fc1_W = torch.from_numpy(weights[: s_size * h_size].reshape(s_size, h_size))\n",
    "        fc1_b = torch.from_numpy(weights[s_size * h_size : fc1_end])\n",
    "        fc2_W = torch.from_numpy(\n",
    "            weights[fc1_end : fc1_end + (h_size * a_size)].reshape(h_size, a_size)\n",
    "        )\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end + (h_size * a_size) :])\n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "\n",
    "    def get_weights_dim(self):\n",
    "        return (self.s_size + 1) * self.h_size + (self.h_size + 1) * self.a_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x.cpu().data\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            action = self.forward(state)\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем Cross-Entropy Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def cem(\n",
    "    agent,\n",
    "    n_iterations=5,\n",
    "    max_t=1000,\n",
    "    gamma=1.0,\n",
    "    print_every=10,\n",
    "    pop_size=50,\n",
    "    elite_frac=0.2,\n",
    "    sigma=0.5,\n",
    "):\n",
    "    \"\"\"PyTorch implementation of the cross-entropy method.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        Agent (object): agent instance\n",
    "        n_iterations (int): maximum number of training iterations\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        pop_size (int): size of population at each iteration\n",
    "        elite_frac (float): percentage of top performers to use in update\n",
    "        sigma (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    n_elite = int(pop_size * elite_frac)\n",
    "\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Initialize the weight with random noise\n",
    "    best_weight = sigma * np.random.randn(agent.get_weights_dim())\n",
    "\n",
    "    for i_iteration in tqdm(range(1, n_iterations + 1)):\n",
    "        # Define the cadidates and get the reward of each candidate\n",
    "        weights_pop = [\n",
    "            best_weight + (sigma * np.random.randn(agent.get_weights_dim()))\n",
    "            for i in range(pop_size)\n",
    "        ]\n",
    "        rewards = np.array(\n",
    "            [agent.evaluate(weights, gamma, max_t) for weights in weights_pop]\n",
    "        )\n",
    "\n",
    "        # Select best candidates from collected rewards\n",
    "        elite_idxs = rewards.argsort()[-n_elite:]\n",
    "        elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "\n",
    "        reward = agent.evaluate(best_weight, gamma=1.0)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "\n",
    "        torch.save(agent.state_dict(), \"checkpoint.pth\")\n",
    "\n",
    "        if i_iteration % print_every == 0:\n",
    "            print(\n",
    "                \"Episode {}\\tAverage Score: {:.2f}\".format(\n",
    "                    i_iteration, np.mean(scores_deque)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if np.mean(scores_deque) >= 90.0:\n",
    "            print(\n",
    "                \"\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}\".format(\n",
    "                    i_iteration - 100, np.mean(scores_deque)\n",
    "                )\n",
    "            )\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "agent = Agent(env).to(device)\n",
    "scores = cem(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import base64, io\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "\n",
    "\n",
    "def show_video(env_name):\n",
    "    mp4list = glob.glob(\"video/*.mp4\")\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = \"video/{}.mp4\".format(env_name)\n",
    "        video = io.open(mp4, \"r+b\").read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(\n",
    "            HTML(\n",
    "                data=\"\"\"<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>\"\"\".format(\n",
    "                    encoded.decode(\"ascii\")\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name)\n",
    "    vid = video_recorder.VideoRecorder(env, path=\"video/{}.mp4\".format(env_name))\n",
    "    agent.load_state_dict(torch.load(\"checkpoint.pth\"))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        vid.capture_frame()\n",
    "\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env).to(device)\n",
    "show_video_of_model(agent, \"MountainCarContinuous-v0\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "print(\"Необученная модель:\")\n",
    "show_video(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим веса обученной в течение 500 эпизодов модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qN https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/weights/checkpoint.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env).to(device)\n",
    "show_video_of_model(agent, \"MountainCarContinuous-v0\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "print(\"Обученная модель:\")\n",
    "show_video(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Марковский процесс принятия решений (Markov decision process, MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/mdp.png\" alt=\"Drawing\" width=\"600\">\n",
    "\n",
    "<center><em> Марковский процесс принятия решений<em><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы разобрались с самой простой ситуацией, многорукими бандитами, в которой среда постоянно находится в одном и том же состоянии. Однако в реальности чаще всего состояние среды, из которой агент делает новый ход, будет изменяться. Например, при игре в шахматы агент должен учитывать то, что после его хода и хода оппонента позиция на доске изменится.\n",
    "\n",
    "Нам нужно определить некий процесс, в котором агент последовательно переходит из состояния в состояние в зависимости от своих действий, в некоторых состояниях получая награды."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем также важное предположение о природе нашего процесса: **он полностью описывается своим текущим состоянием**. Все, что произойдет в будущем, не зависит от информации из прошлого, кроме той, что мы уже наблюдаем в настоящем. Такой процесс называется __марковским__. Заметим, что описанный нами процесс в примере с многорукими бандитами также является марковским.\n",
    "\n",
    "Приведем другие классические примеры марковского процесса:\n",
    "\n",
    "1. Игральный кубик. Мы знаем, что на нем выпадет любая из граней с некоторой фиксированной вероятностью. На это никак не влияет то, что до этого на кубике выпало 6 шестерок подряд. Это может повлиять на нашу оценку вероятностей выпадения той или иной грани, но не на реальную вероятность.\n",
    "\n",
    "2. Шахматы. \"Текущая позиция на доске + чей ход\" однозначно описывает игру.\n",
    "\n",
    "\n",
    "Однако, несмотря на это свойство, мы сами можем регулировать описание текущего состояния. Например, включив больше информации в определение текущего состояния, мы можем обеспечить, чтобы процесс соответствовал марковскому свойству, даже если изначально это не было так. Таким образом, правильное определение и описание текущего состояния играют ключевую роль."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Состояние $S_{t}$ является Марковским тогда и только тогда, когда:\n",
    "$$ \\large\n",
    "p\\left(r_{t}, s_{t+1} \\mid s_{0}, a_{0}, r_{0}, \\ldots, s_{t}, a_{t}\\right)=p\\left(r_{t}, s_{t+1} \\mid s_{t}, a_{t}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Марковский процесс описывает **полностью наблюдаемые (fully observable)** среды. Можно описать их так:\n",
    "\n",
    "\n",
    "- Что происходит в будущем, зависит только от текущего состояния.\n",
    "\n",
    "- Информации, которую получает агент в момент времени t, достаточно, чтобы принять оптимальное решение для момента времени t.\n",
    "\n",
    "- Состояние среды — достаточная статистика для принятия решения.\n",
    "\n",
    "- Вероятность перехода в новое состояние зависит только от текущего состояния и текущего действия.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим себе, что студент живет вот по такой схеме. Заметим, что влиять в такой схеме на свои решения он не может: все решается подкидыванием кубика."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/markov_process.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Марковский процесс (цепь) — это кортеж $(S, P)$, где\n",
    "- $S$ — принимает дискретные (конечные) значения,\n",
    "- $P$ — матрица переходов (transition matrix):\n",
    "$$\\large\n",
    "P_{s s^{\\prime}}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s\\right)\n",
    "$$\n",
    "\n",
    "Строго говоря, необходимо еще распределение начальных состояний (но мы предполагаем, что оно вырождено, т.е. мы знаем, где начинаем, с вероятностью 1).\n",
    "\n",
    "Марковский процесс — основа для RL. Мы будем постепенно усложнять эту модель, добавляя rewards и actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция награды (reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши состояния, очевидно, неравноценны. Давайте к каждому состоянию привяжем награду $R_s$. Награды могут быть положительными, нулевыми или отрицательными и назначаться в различных временных точках, что определяет, как агент учится и принимает решения.\n",
    "\n",
    "* **Положительные награды.** Положительные награды сигнализируют агенту о желательных действиях или достижении определенных целей. Они способствуют усилению тех действий, которые привели к получению этой награды.\n",
    "\n",
    "* **Нулевые награды.** Нулевые награды сигнализируют агенту, что его действия не привели к каким-либо значимым результатам. Такие награды часто используются для поддержания текущего состояния или для действий, которые не оказывают непосредственного влияния на итоговый результат.\n",
    "\n",
    "* **Отрицательные награды.** Отрицательные награды или штрафы используются для того, чтобы агент избегал нежелательных действий или состояний. Они помогают корректировать поведение агента, указывая на ошибки или неэффективные действия.\n",
    "\n",
    "Награды могут назначаться в различных временных точках:\n",
    "\n",
    "\n",
    "* **Награды в конце эпизода.** В некоторых задачах награды назначаются только в конце эпизода, когда достигается конечное состояние или завершается определенная последовательность действий.\n",
    "Пример: В шахматах агент получает награду только в конце игры, когда определяется победитель, проигравший или фиксируется ничья. Все промежуточные действия могут не вознаграждаться отдельно, но влияют на итоговый результат.\n",
    "\n",
    "* **Награды на каждом шаге времени (t).** В других задачах награды могут назначаться на каждом шаге времени, чтобы агент получал обратную связь после каждого действия.\n",
    "Пример: В задаче управления дроном агент получает награды на каждом шаге времени за удержание стабильной высоты, избегание препятствий и достижение определенных целей. Эти награды помогают агенту корректировать свое поведение в реальном времени.\n",
    "\n",
    "Дизайн функции награды это отдельная нетривиальная задача, которая зависит от многих факторов.\n",
    "\n",
    "\n",
    "Схема для студента в этом случае модифицируется следующим образом. Заметьте, награды тут расставляются с точки зрения актора — студента:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/markov_reward.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Марковский процесс принятия решений\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого у нас получалось не совсем адекватное представление процесса — студент не мог ни на что повлиять. На самом же деле студент может решать, куда ему надо: в аудиторию, домой спать или в бар. Однако наше действие не всегда определяет состояние, в которое мы перейдем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочет наш студент пойти на лекцию, но по пути встречает товарища, и идут они в бар. Хочет он пойти спать, а по пути к выходу из университета встречает лектора с хорошей памятью и идет на лекцию.\n",
    "\n",
    "\n",
    "Как это отразить на схеме так, чтобы это можно было прочитать? Введем промежуточные состояния, куда нас переводят действия студента. А уже из этих промежуточных состояний случайно будем переходить в состояния среды.\n",
    "\n",
    "Процесс, когда в среде агент выбирает действия называют Markov decision process (MDP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формальное описание MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/markov_decision_process_return_random.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP — это кортеж $(S, A, R, P, \\gamma)$, где:\n",
    "- $S$ — состояния (дискретное пространство),\n",
    "- $A$ — действия (дискретное пространство),\n",
    "- $R$ — функция reward: $R_{s,s'}^{a}$ (мгновенное вознаграждение за переход из состояния $s$ в $s'$ при выборе действия $a$)\n",
    "- $P$ — матрица переходов (transition matrix): $P_{s s^{\\prime}}^{a}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s, A_{t}=a\\right)$,\n",
    "- $\\gamma$ — коэффициент дисконтирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\gamma$-коэффициент дисконтирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Цель агента заключается в том, чтобы найти стратегию, максимизирующую среднюю суммарную награду:\n",
    "\n",
    "\n",
    "\n",
    "$$\\text{Return} = \\sum_i {R_i}$$\n",
    "\n",
    "\n",
    "\n",
    "Суммарная награда — это просто сумма всех наград, которые агент получает за последовательность действий от начального до конечного состояния.\n",
    "\n",
    "\n",
    "Использование суммарной награды имеет несколько важных недостатков:\n",
    "\n",
    "1. **Отсроченные награды.** Действия сделанные сейчас влияют на события не на следующем шаге, а через несколько шагов вперед. Определить какое действие привело к награде через неопределенное количество шагов сильно усложняет задачу. Если дейсвтий много, то и сопоставить дейсвтие с наградой затруднительно.\n",
    "\n",
    "2. **Бесконечные циклы.** В нашей задаче могут быть бесконечно большие награды, поэтому нам важно понимать как можно было бы их сравнить.\n",
    "\n",
    "3. **Стабильность обучения.** Без учета временного аспекта, агент может переоценивать будущие награды, что приводит к нестабильному и неоптимальному поведению.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому при оценке кумулятивной награды на шаге $t$ $(G_t)$ используется коэффициент дисконтирования $\\gamma$, который показывает, насколько ценными являются будущие награды на текущий момент (см. [Markov Decision Processes (David Silver Lectures) 📚[book]](https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf)):\n",
    "\n",
    "$$\\large G_t = R_{t+1} + \\gamma R_{t+2} + ... =\\sum^{\\infty}_{k=0} \\gamma ^ kR_{t+k+1},\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $R_{t+1}, R_{t+1}, \\dots$ — вознаграждения по рёбрам переходов,\n",
    "- дисконтирование $\\gamma \\in[0,1]$ — это текущая стоимость будущих вознаграждений,\n",
    "- $\\gamma^{k} R$ — ценность получения награды $R$ после $k+1$ шагов.\n",
    "\n",
    "При этом:\n",
    "- немедленное вознаграждение ценится выше отложенного вознаграждения,\n",
    "- $\\gamma$ близко к 0 приводит к «близорукости»,\n",
    "- $\\gamma$ близко к 1 приводит к «дальновидной» оценке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Снижение влияния неопределенности будущего:**\n",
    "В реальных задачах будущее часто неопределенно. Дисконтирование уменьшает влияние этой неопределенности, так как делает дальнейшие награды менее значимыми для текущего состояния и действия.\n",
    "\n",
    "* **Стабильность обучения:**\n",
    "Дисконтирование помогает стабилизировать процесс обучения, так как уменьшает влияние долгосрочных и часто неопределенных наград на текущие решения. Это упрощает оптимизацию стратегии агента.\n",
    "\n",
    "* **Баланс между краткосрочными и долгосрочными целями:**\n",
    "Использование коэффициента дисконтирования позволяет агенту находить баланс между краткосрочными выгодами и долгосрочными целями, что способствует более стратегическому поведению.\n",
    "\n",
    "* **Предотвращение бесконечного накопления наград:**\n",
    "В средах, где награды могут накапливаться бесконечно, дисконтирование предотвращает ситуацию, когда агент стремится к достижению бесконечных будущих наград.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Какое значение $\\gamma$ выбрать?\n",
    "\n",
    "\n",
    "**Можно ли его выбрать равным 1?**\n",
    "\n",
    "Да, получим суммарную награду\n",
    "\n",
    "**Можно ли его выбрать равным 0?**\n",
    "\n",
    " Тоже да. В этом случае у нас получится \"жадный\" алгоритм: мы всегда выбираем решение, которое дает максимальную награду сейчас, нас не волнуют будущие награды.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обычно $0 \\le \\gamma \\le 1$**\n",
    "\n",
    "Обычно $\\gamma$ ставят равной чему-то между двумя этими крайностями.\n",
    "\n",
    "Близость $\\gamma$ к 0 отражает нашу \"нетерпеливость\" — насколько важно получить награду именно сейчас.\n",
    "\n",
    "Наличие такого $\\gamma$ позволяет нам не делать различия между моделями с ограниченным числом шагов и неограниченным: теперь в любом случае return будет конечным числом, т.к. выражение для return ограничено сверху суммой бесконечно убывающей геометрической прогрессии.\n",
    "\n",
    "Пусть $R_{\\max} = \\max R_i$ — максимальная награда, которую мы в принципе можем получить в каком-то состоянии.\n",
    "\n",
    "$\\displaystyle G_t = R_{t+1} + \\gamma \\cdot R_{t+2} + \\gamma^2 \\cdot R_{t+3} + ... \\le R_{\\max} + \\gamma \\cdot R_{\\max} + \\gamma^2 \\cdot R_{\\max} + ... = R_{\\max} (1 + \\gamma + \\gamma^2 + ... ) \\le R_{\\max} \\cdot \\frac{1}{1-\\gamma} = const $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discounting makes sums finite**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/discounting_makes_sums_finite.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G_{0}=\\sum_{k=0}^{\\infty} \\gamma^{k}=\\frac{1}{1-\\gamma}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно понимать, что выбор $\\gamma$ меняет задачу, которую мы решаем, и, соответственно, меняет решение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим наш MDP в виде среды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# After this setup we can import mdp package\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
    "!touch .setup_complete\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import MDP\n",
    "\n",
    "transition_probs = {\n",
    "    \"s0\": {\"a0\": {\"s1\": 1}},\n",
    "    \"s1\": {\"a0\": {\"s4\": 1}, \"a1\": {\"s2\": 0.6, \"s4\": 0.2, \"s3\": 0.2}, \"a2\": {\"s2\": 0.1, \"s3\": 0.9}},\n",
    "    \"s2\": {\"a0\": {\"s4\": 0.2, \"s3\":0.8}},\n",
    "    \"s3\": {\"a0\": {\"s4\": 1}},\n",
    "    \"s4\": {\"a0\": {\"s4\": 1}}\n",
    "}\n",
    "rewards = {\"s0\": {\"a0\": {\"s1\": -2}},\n",
    "           \"s1\": {\"a0\": {\"s4\": 0}, \"a1\": {\"s2\": -4, \"s4\": 0, \"s3\": 10}, \"a2\": {\"s2\":-4, \"s3\":10}},\n",
    "           \"s2\": {\"a0\": {\"s4\": 0, \"s3\": 10}}\n",
    "           }\n",
    "\n",
    "mdp = MDP(transition_probs, rewards, initial_state=\"s0\")\n",
    "\n",
    "# We can now use MDP just as any other gym environment:\n",
    "print(\"initial state =\", mdp.reset())\n",
    "next_state, reward, done, info = mdp.step(\"a0\")\n",
    "print(\"next_state = %s, reward = %s, done = %s\" % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP methods\n",
    "\n",
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions(\"s1\"))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states(\"s1\", \"a0\"))\n",
    "\n",
    "# state, action, next_state\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward(\"s1\", \"a0\", \"s0\"))\n",
    "\n",
    "# get_transition_prob(self, state, action, next_state)\n",
    "print(\n",
    "    \"mdp.get_transition_prob('s1', 'a1', 's2') = \",\n",
    "    mdp.get_transition_prob(\"s1\", \"a1\", \"s2\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_jpeg\n",
    "import mdp as MDP\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "display_jpeg(MDP.plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема поиска оптимальной политики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно найти последовательность переходов, которой будет соответствовать максимальная награда ($G_t$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Политика (policy) $\\pi$ — это функция, которая для текущего состояния $s$ дает распределение вероятностей на множестве действий $A$.\n",
    "\n",
    "$$ \\large\n",
    "\\pi(a|s)=\\mathbb{P}\\left[A_{t}=a \\mid S_{t}=s\\right]\n",
    "$$\n",
    "- Политика полностью определяет поведение агента.\n",
    "- Политики MDP зависят от текущего состояния среды (а не от прошлых состояний).\n",
    "- Т. е. политики являются стационарными (не зависящими от времени):\n",
    "\n",
    "$$ \\large\n",
    "A_{t} \\sim \\pi\\left(\\cdot \\mid S_{t}\\right), \\forall t>0\n",
    "$$\n",
    "\n",
    "Для нашего примера со студентом:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/markov_policy_example.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-value function $v_{\\pi}(s)$ (V-функция)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $v_\\pi(s)$ — измеряет ценность каждого состояния, а именно какое ожидаемое вознаграждение можно получить, если начать двигаться из состояния $s$ в течение всего оставшегося времени, придерживаясь политики $\\pi$.\n",
    "\n",
    "Так, например, агент может следовать политике, заключающейся в переходе между состояниями с высокой ценностью. Однако функция ценности состояния удалена от принятия решений агентом, поскольку агент зачастую не может сам выбрать следующее состояние, в которое перейдет, поскольку на него будет оказываться влияние случайности (например, броски кубиков) или внешних сил (ход оппонента). Поэтому при выборе оптимальной политики рассматривают ценность не состояний, а действий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/chess.png\" alt=\"Drawing\" width=\"400\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формально **V-функция** $v_\\pi(s)$ **вводится в качестве математического ожидания** (по политике $\\pi$ и по MDP среды $P_{s s^{\\prime}}^{a}$) от будущей дисконтированной награды $G_t$, в случае если мы начинаем свои действия в состоянии среды $s$ и будем действовать строго согласно нашей политики $\\pi$:\n",
    "\n",
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action-value function $q_{\\pi}(s, a)$ (Q-функция)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $q_\\pi(s,a)$ — аналогично функции ценности состояния, но при условии, что в состоянии $s (s_0 = s)$ было выбрано действие $a$ (мы фиксируем действие). То есть в $q$ мы, в отличие от $v$, фиксируем наше первое действие (не обязательно соответствующее политике), а в $v$ мы первое действие выбираем согласно политике $\\pi$.\n",
    "\n",
    "Формально Q-функция вводится в качестве математического ожидания от будущей дисконтированной награды $G_t$, в случае если мы находясь в состоянии $s$ **выбрали конкретное действие $a$**, а все последующие действия совершали строго согласно политике $\\pi$:\n",
    "\n",
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s, a) &=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s \\mid A_{t}=a\\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "Сразу отметим, что Q и V функции, очевидно, связаны. Если мы воспользуемся нашей политикой $\\pi$ для выбора действия $a$, то сразу же придём к определению V-функции (то есть усредним значения Q-функции согласно вероятностям принятия решений, которые задаёт политика):\n",
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\sum_a \\pi(a|s) q_{\\pi}(s, a) &=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s \\right]\\\\\n",
    "&=v_{\\pi}(s).\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "то есть V-функцию легко записать через Q функцию и политику $\\pi$. Обратное соотношение, позволяющее выразить Q функцию через V функцию мы рассмотрим ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уравнение Беллмана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Беллман 📚[wiki]](https://en.wikipedia.org/wiki/Richard_E._Bellman) показал, что задача динамической оптимизации в дискретном времени может быть сформулирована в рекурсивной пошаговой форме, путем записи связи между **функцией ценности в текущий период и функцией ценности в следующем периоде**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma\\left(R_{t+2}+\\gamma R_{t+3}+\\ldots\\right) \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right]\\\\\n",
    "&=\\mathbb{E}_{\\pi}\\left[R_{t+1}\\right]+\\mathbb{E}_{\\pi}\\left[\\gamma G_{t+1} \\mid S_{t}=s\\right],\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "где последнее слагаемое можно записать по определению V-функции, учитывая что вероятность перейти из состояния $s$ в возможное состояние $s'$ в момент времени $t$ определяется значением политики $\\pi(a|s)$ и MDP средой $P_{s s^{\\prime}}^{a}$:\n",
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{\\pi}\\left[\\gamma G_{t+1} \\mid S_{t}=s\\right] = \\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}P_{s s^{\\prime}}^{a}\\left[{\\mathbb{E}_{\\pi}\\left[\\gamma G_{t+1} \\mid S_{t+1}=s'\\right]}\\right] = \\\\\\gamma\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}P_{s s^{\\prime}}^{a}\\left[v_{\\pi}(s')\\right],\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "что теперь позволяет записать окончательно так называемое уравнение Беллмана:\n",
    "$$\\Large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[R_{t+1}\\right] + \\gamma\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}P_{s s^{\\prime}}^{a}\\left[v_{\\pi}(s')\\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "***Уравнение Беллмана - рекуррентное соотношение, позволяющее связать значение V-функции для двух последовательных состояний $s$ и $s'$, если нам известна политика $\\pi$ и матрица перехода в MDP $P$.*** Данное уравнение является ключевым результатом для построения всех последующих RL-алгоритмов.\n",
    "\n",
    "Представленный выше вывод можно повторить для Q-функции и записать аналогичное рекуррентное соотношение:\n",
    "$$\\Large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left[R_{t+1}\\right] + \\gamma\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}P_{s s^{\\prime}}^{a}\\left[\\sum_{a'}\\pi(a'|s')q_{\\pi}(s', a')\\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "Функцию ценности состояния можно представить в виде [резервной диаграммы 🎓[article]](https://openreview.net/pdf/45363cfb2c1b1123fc49b52916f8f3a451e09bbd.pdf) (_backup diagram_):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/bellman_backup_diagram.png\" width=\"600\"/></center>\n",
    "\n",
    "<center></em>Backup Diagram for State Value $v_\\pi(s)$ and Action Value $q_\\pi(s,a)$ with all components</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf {s} $ — **начальное состояние**, из него исходят стрелки, отображающие **вероятность** совершить то или иное **действие** в соответствии с **политикой** $ \\mathbf{\\pi} $, **действия** $\\mathbf{a}$ обозначаются переходом из кружка в чёрную точку, черные точки обозначают $\\mathbf{q} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если раскрыть знак матожидания для награды $R_t$ получим запись системы уравнений наиболее широко используемую в литературе:\n",
    "\n",
    "$$\n",
    "\\large v_{\\pi}(s) = \\sum_a\\pi(a|s)\\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma\\mathbb{E}_{\\pi}\\left[G_{t+1}\\mid S_{t+1}=s'\\right]) = \\sum_a\\pi(a|s)\\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma v_{\\pi}(s')).\n",
    "$$\n",
    "\n",
    "Аналогично для функции ценности действия:\n",
    "\n",
    "$$\n",
    "\\large q_{\\pi}(s, a) = \\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma\\sum_{a'}\\pi(a'|s') q_{\\pi}(s', a')) = \\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma \\large v_{\\pi}(s')).\n",
    "$$\n",
    "\n",
    "Важно отметить, что описанные выше рекуррентные соотношения выполняются в том числе и для оптимального выбора политики $\\pi$, к поиску которого мы и стремимся. Для этого воспользуемся ещё одной идеей, которая лежит в определении V и Q функций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Принцип оптимальности Беллмана**\n",
    "\n",
    "Беллманом был сформулирован принцип, позволяющий из анализа выписанных выше рекуррентных соотношений определить оптимальную политику принятия решений в MDP.\n",
    "\n",
    "*Оптимальная политика обладает тем свойством, что какими бы ни были начальное состояние и начальное действие, остальные действие должны представлять собой оптимальную политику в отношении состояния, полученного в результате первого действия.*\n",
    " (Bellman, 1957, Chap. III.3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/backup_diagram.png\" alt=\"Drawing\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ценность (Value) $v$ в состоянии $s$ — это $v_{\\pi}(s)$\n",
    "\n",
    "2. Из состояния $s$ агент может сделать 3 действия: $(a_1, a_2, a_3)$\n",
    "\n",
    "3. Ценность действия (Action Value) — это $q_{\\pi}(s,a)$, где $a = \\{a_1, a_2, a_3\\}$\n",
    "\n",
    "4. Агент предпринял действие $a_3$. А так он может перейти в состояния $s’_1$, $s’_2$ или $s’_3$ с вероятностью перехода p1, p2 или p3 соответственно\n",
    "\n",
    "5. Полученная награда — $r_1$, $r_2$ или $r_3$ в зависимости от текущего состояния"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понять это несколько громоздкое определение можно последовательно задав следующие вопросы:\n",
    "\n",
    "1. **Что вообще можно назвать оптимальной политикой?**\n",
    "\n",
    "Оптимальным решением для любого узла графа MDP, очевидно, будет **такой выбор стратегии принятия решений $a$, при котором матожидание будущей дисконтированной награды будет максимально**. Будущую дисконтированную награду измеряет функция V. То есть $\\pi^*$ - оптимальная политика, если среди всех допустимых уравнением Беллмана V-функций $v_{\\pi^*}$ такая, что для любого $s$ она принимает максимально возможное значение ($v_{\\pi^{*}}(s) \\rightarrow max$)\n",
    "\n",
    "2. **Как мы можем оценить значение V-функции для известной политики?**\n",
    "\n",
    "Вместо того, чтобы рассчитывать **ценность состояния** $ \\mathbf{s}$ **на основе ценности всех состояний** $ \\mathbf{s'}$, в которые мы можем перейти, мы можем рассчитать ценность состояния, учитывая ценность всех действий $\\mathbf{a}$, которые мы можем совершить, находясь в состоянии $\\mathbf{s}$. Получаем таким образом выражение одной функции через другую:\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\sum_a\\pi(a|s)q_{\\pi}(s,a).$$\n",
    "\n",
    "3. **Как оценивать ценность выбора действия для определенного состояния $s$ в MDP?**\n",
    "\n",
    "Для этого мы ввели Q-функцию. В случае нахождения оптимальной политики $q_{\\pi^{*}}$ лучшим с точки зрения матожидания награды будет выбор действия $\\text{argmax}_a q_{\\pi^{*}}(s,a)$. Отсюда сразу следует, что в случае оптимальной игры максимальное значение V функции достигается на максимальном значении Q функции:\n",
    "$$\\large v_{\\pi^*}(s) = \\max_{a(s)}q_{\\pi^*}(s,a) = \\max_{a(s)}\\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma v_{\\pi^*}(s')).$$\n",
    "\n",
    "\n",
    "**Отсюда сразу следует основной результат для Беллман-оптимальной политики** $\\pi^*$:\n",
    "$$\\large \\pi^*(s) = \\text{argmax}_aq_*(s,a)$$\n",
    "\n",
    "*Примечание: $\\text{argmax}$ может быть заменен на $\\text{softmax}$, если матрица перехода MDP явно зависит от $a$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск оптимальной политики Беллмана для MDP (решение \"MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы динамического программирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём рассмотренные выше уравнения Беллмана и определим с их помощью  оператор $\\mathcal{T}$ по его действию на V и Q функции:\n",
    "\n",
    "$$\\large \\mathcal{T}[\\color{red}{v}](s) = \\sum_a\\pi(a|s)\\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma \\color{red}{v(s')}),$$\n",
    "$$\\large \\mathcal{T}[\\color{red}{q}](s,a) = \\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma\\sum_{a'}\\pi(a'|s') \\color{red}{q(s', a')}).$$\n",
    "\n",
    "Как подробно показано в [[book] 📚 Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) такой оператор $\\mathcal{T}$ является *сжимающим*. То есть если взять любую пару функций $V_1$ и $V_2$, найдётся такое неотрицательное число $C$, что:\n",
    "\n",
    "$$\\large || \\mathcal{T}[V_1] - \\mathcal{T}[V_2]|| \\leq C||V_1 - V_2||,$$\n",
    "что означает что после применения оператора $\\mathcal{T}$ функции $V_1$ и $V_2$ становятся ближе друг к другу.\n",
    "\n",
    "[[wiki] 📚 По теореме о неподвижной точке](https://en.wikipedia.org/wiki/Banach_fixed-point_theorem) это означает, что существует как минимум одна функция, которая не изменяется оператором $\\mathcal{T}$. Так как мы построили наш оператор как правую часть уравнения Беллмана, то эта неподвижная точка - искомое нами решение. Отсюда появляется идея итеративного поиска решения для функций Q или V:\n",
    "\n",
    "1. Инициализируем произвольную $V_0$.\n",
    "2. Действуем на неё оператором $\\mathcal{T}[V_0] = V_1$.\n",
    "3. Повторяем $\\mathcal{T}[V_k] = V_{k+1}$ пока не  $||V_{k} - V_{k+1}|| \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь процесс поиска оптимальной политики можно разделить на два этапа:\n",
    "\n",
    "1. **Policy evaluation**\n",
    "\n",
    "Фиксируем произвольную политику $\\pi$ и V-функцию $V_0$. Проведём итерационную процедуру с оператором $\\large \\mathcal{T}[\\color{red}{v}](s)$, пока не сойдёмся к неподвижной точке.\n",
    "\n",
    "2. **Policy improvement**\n",
    "\n",
    "Теперь мы знаем правило $v$ для нашей политики. Как нам его улучшить?\n",
    "Будем в каждом состоянии менять нашу политику таким образом, чтобы мы шли в состояние с лучшим $q$. Мы могли бы улучшить это, действуя жадно $\\mathrm{q}(\\mathrm{s}, \\mathrm{a}) !$\n",
    "$$\n",
    "\\large\n",
    "\\pi^{\\prime}(s) \\leftarrow \\underset{a}{\\arg \\max } \\overbrace{\\sum_{r, s^{\\prime}} P_{ss'}^a\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]}^{q_{\\pi}(s, a)}\n",
    "$$\n",
    "\n",
    "Эта процедура гарантированно приведет к улучшению политики!\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\text { если } \\quad q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) \\geq v_{\\pi}(s) \\quad \\text { для всех состояний, } \\\\\n",
    "&\\text { тогда } \\quad v_{\\pi^{\\prime}}(s) \\geq v_{\\pi}(s) \\text { означает, } \\\\\n",
    "&\\text { что } \\qquad \\pi^{\\prime} \\geq \\pi .\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/policy_iteration.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод действительно сходится:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/convergence_of_method.png\" alt=\"Drawing\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/random_vs_greedy_policy.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, этап policy evaluation от policy iteration  может быть сокращен различными способами без потери гарантий сходимости policy iteration.\n",
    "\n",
    "Один важный особый случай &mdash; это когда **оценка политики останавливается** сразу **после одного цикла** (одно обновление каждого состояния). Этот алгоритм **называется value iteration**. Его можно записать как простую операцию обновления, которая сочетает в себе этапы улучшения политики и усеченной policy evaluation (один шаг).\n",
    "\n",
    "Это позволяет, в частности, вообще не записывать политику в явном виде, т.к. она однозначно определяется Q-функцией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration (VI) vs. Policy iteration (PI):\n",
    "\n",
    "- VI быстрее за одну итерацию $-\\mathrm{O}\\left(|\\mathrm{A} \\| \\mathrm{S}|^{2}\\right)$\n",
    "- VI требуется много итераций\n",
    "-PI медленнее за одну итерацию $-\\mathrm{O}\\left(|\\mathrm{A} \\| \\mathrm{S}|^{2}+|\\mathrm{S}|^{3}\\right)$\n",
    "- PI требуется мало итераций\n",
    "\n",
    "Проведем эксперимент с каким-то количеством шагов для нахождения лучшей политики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм**\n",
    "\n",
    "1. Инициализация:\n",
    "\n",
    "- создаем массив V с количеством элементов, равным количеству состояний,\n",
    "- заполняем его нулями.\n",
    "\n",
    "2. Оценка политики (Policy evaluation):\n",
    "\n",
    "- для всех состояний считаем Q(s,a),\n",
    "- обновляем массив V[s] $\\rightarrow$ max(Q(s,a)).\n",
    "\n",
    "*В отличие от Policy iteration, сама политика в памяти не хранится, она генерируется при помощи Q-функции.\n",
    "\n",
    "3. Обновление политики (Policy improvement):\n",
    "\n",
    "- для каждого состояния считаем Q(s,a) для всех a,\n",
    "- выбираем a, для которого Q(s,a) максимально,\n",
    "- если политика изменилась, переходим к шагу 2, иначе останавливаемся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте построим что-нибудь, чтобы решить эту MDP.\n",
    "\n",
    "Запишем решения для этого MDP. Самый простой алгоритм — это  __V__alue __I__teration.\n",
    "\n",
    "Псевдо код для VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    "\n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P_{ss'}^a \\cdot [ R_{ss'}^a + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "$R_{ss'}^a$ — награда, соответствующая выбору действия a в s,\n",
    "\n",
    "$s$ — исходное состояние,\n",
    "\n",
    "$s'$ — новое состояние."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для вычисления функции значения состояния-действия $Q^{\\pi}$ определяется следующим образом:\n",
    "\n",
    "$$\\large Q_i(s, a) = \\sum_{s'} P_{ss'}^a \\cdot [ R_{ss'}^a + \\gamma V_{i}(s')],$$\n",
    "\n",
    "$s'$ — зависит от вероятности перехода.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\"\n",
    "    Computes Q(s,a) as in formula above\n",
    "\n",
    "    mdp : MDP object\n",
    "    state_values : dictionayry of { state_i : V_i }\n",
    "    state: string id of current state\n",
    "    gamma: float discount coeff\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    next_states = mdp.get_next_states(state, action)\n",
    "\n",
    "    Q = 0.0\n",
    "\n",
    "    for next_state in next_states.keys():\n",
    "        # alternatively p = mdp.get_transition_prob(state, action, next_state)\n",
    "        p = next_states[next_state]\n",
    "        Q += p * (\n",
    "            mdp.get_reward(state, action, next_state) + gamma * state_values[next_state]\n",
    "        )\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя $Q(s,a)$, мы можем определить \"следующее\" V(s) для VI:\n",
    "\n",
    "$$\\large V_{(i+1)}(s) = \\max_a \\sum_{s'} P_{ss'}^a \\cdot [ R_{ss'}^a + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\"Computes next V(s) as in formula above. Please do not change state_values in process.\"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0  # Game over\n",
    "\n",
    "    q_max = float(\"-inf\")\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "        q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "        q_max = max(q_max, q)\n",
    "    return q_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, объединим все, что мы написали, в алгоритм итерации рабочего значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9  # discount for MDP\n",
    "num_iter = 100  # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "display_jpeg(MDP.plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь нет никакого \"текущего состояния\"! Добавим матрицу состояний и построим новый граф:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iter):\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "\n",
    "    new_state_values = {}\n",
    "    for s in state_values.keys():\n",
    "        new_state_values[s] = get_new_state_value(mdp, state_values, s, gamma)\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print(\"   \".join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_jpeg(MDP.plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь воспользуемся этим $V^{*}(s)$, чтобы найти оптимальные действия в каждом состоянии:\n",
    "\n",
    " $$\\pi^*(s) = \\text{argmax}_a \\sum_{s'} P_{ss'}^a \\cdot [ R_{ss'}^a + \\gamma V_{i}(s')] = \\text{argmax}_a Q_i(s,a)$$\n",
    "\n",
    "Единственное отличие от $V(s)$ в том, что здесь мы берем не $\\max$, а $\\text{argmax}$: найти действие с максимальным $Q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\"Finds optimal action using formula above.\"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "\n",
    "    best_action = None\n",
    "    q_max = float(\"-inf\")\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "        q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "        if q > q_max:\n",
    "            best_action = a\n",
    "            q_max = q\n",
    "\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_jpeg(\n",
    "    MDP.plot_graph_optimal_strategy_and_state_values(\n",
    "        mdp, state_values, get_action_value\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference (TD)-обучение (TD-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, в тех случаях, когда известна вероятность $P_{ss'}^a$, задача обучения может быть решена с помощью методов динамического программирования, как было рассмотрено выше. В реальных же задачах динамика среды зачастую неизвестна. Два главных подхода, позволяющие обходить незнание динамики среды, включают __метод Монте Карло__ (MC) и  __temporal difference (TD)-обучение__ (TD-learning). Оба эти подхода основываются на симулировании опыта взаимодействия со средой в виде __траекторий__ (MDP trajectory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDP trajectory**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/q_learning_scheme.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Траектория представляет из себя последовательность\n",
    "\n",
    "• states ($s$)\n",
    "• actions ($a$)\n",
    "• rewards ($r$)\n",
    "\n",
    "При этом MC оперирует полными траекториями, оканчивающимися в терминальном состоянии, а TD-learning позволяет учиться на неполных эпизодах, не дожидаясь конечного состояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/TD_MC_DP_backups_.png\" alt=\"Drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим второй подход подробнее.\n",
    "\n",
    "**TD-learning** — один из наиболее мощных подходов, используемых во многих алгоритмах RL. Смысл в том, чтобы **обновлять оценки ценности** (состояния или действия) на **основе уже обученных оценок** для последующих состояний. Эту процедуру можно представить как \"подтягивание\" значения функции ценности того состояния, в котором мы находились на предыдущем шаге, к значению функции ценности того состояния, в котором мы находимся сейчас. Каким образом это можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый алгоритм TD-обучения — $TD(0)$ заключается в следующем:\n",
    "\n",
    "Инициализируем функцию ценности, например, $V(s)$, и стратегию $\\pi$. Пусть, у нас есть текущее состояние $s$ из которого мы:\n",
    "- выбираем действие $a$ по стратегии $\\pi$,\n",
    "- совершаем действие $a$, наблюдаем награду $r$ и следующее состояние $s'$,\n",
    "- обновляем оценку $V(s)$:\n",
    "\n",
    "$$V(s) := V(s) + \\alpha(\\underbrace{\\overbrace{r+\\gamma V(s')}^{\\text{TD target}} - \\overbrace{V(s)}^{\\text{old estimate}}}_{\\text{TD error}}),$$\n",
    "\n",
    "- переходим к следующему шагу $s:=s'$\n",
    "\n",
    "При оптимальной политике $\\pi$:\n",
    "$$\\overbrace{r+\\gamma V(s')}^{\\text{TD target}} = \\overbrace{V(s)}^{\\text{old estimate}}$$\n",
    "\n",
    "Cоответственно, их разницу, называемую _TD-ошибкой_ (TD-error), мы и будем пытаться минимизировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему это работает? Ведь мы обучаем случайно инициализированную оценку $V(s)$ на основе других оценок $V(s')$, которые также были инициализированы случайно. Дело в том, что состояния, которые предшествуют получению реальных наград (например, за победу) будут обучаться первыми и затем распространять свою ценность на состояния, предшествующие им.\n",
    "\n",
    "Стоит отметить также, что TD-обучение не обязательно всегда будет смотреть на один шаг вперед. Алгоритм, называемый $TD(\\lambda)$, будет обновлять оценки функции ценности сразу на несколько шагов назад.\n",
    "\n",
    "Разновидностями TD-обучения являются алгоритмы __Q-learning__ и __SARSA__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица перехода $P_{ss'}^{a}$ может быть нам неизвестна явно, что затруднит прямое применение алгоритма динамического программирования для нахождения Q или V функций в общем случае:\n",
    "$$\n",
    "\\large q_{\\pi}(s, a) = \\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma\\sum_{a'}\\pi(a'|s') q_{\\pi}(s', a')) = \\mathbb{E}(R_{ss'}^a +\\gamma \\mathbb{E}_{\\pi} q_{\\pi}(s', a')).\n",
    "$$\n",
    "\n",
    "Мы можем обойти это ограничение, заменив \"честное\" матожидание по MDP, на выборочное среднее от серии наблюдений вдоль различных траекторий процесса:\n",
    "\n",
    "$$\n",
    "\\large q_{\\pi}(s, a) =  \\frac{1}{N}\\sum_n(R_{ss'}^a +\\gamma \\mathbb{E}_{\\pi} q_{\\pi}(s', a')).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, мы ходим по полю размером 12×4. Наша цель — попасть в ячейку (3,11).\n",
    "**Максимизируя сумму будущих наград**, мы также находим **самый быстрый путь** к цели, поэтому наша цель сейчас — найти способ сделать это!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/q_learning_field.png\" alt=\"Drawing\" width=\"700\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/443240/\">Понимание Q-learning, проблема «Прогулка по скале»</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с построения таблицы, которая измеряет, насколько хорошо будет выполнить определенное действие в любом состоянии (чем больше значение, тем лучше действие).\n",
    "\n",
    "* Таблица будет иметь одну строку для каждого состояния и один столбец для каждого действия. Сетка имеет 48 (4 по Y на 12 по X) состояний, и разрешены 4 действия ($\\leftarrow \\downarrow \\rightarrow \\uparrow$), поэтому таблица будет 48×4.\n",
    "* **Значения**, хранящиеся **в этой таблице**, называются **«Q-values»**.\n",
    "* Это оценки суммы будущих наград. Другими словами, они оценивают, сколько еще вознаграждения мы можем получить до конца игры, находясь в состоянии S и выполняя действие A.\n",
    "* Мы инициализируем таблицу случайными значениями (или некоторой константой, например, всеми нулями)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимальная «Q-table» имеет значения, которые позволяют нам предпринимать лучшие действия в каждом состоянии, давая нам в итоге лучший путь к победе.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/q_table.png\" alt=\"Drawing\" width=\"300\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/443240/\">Понимание Q-learning, проблема «Прогулка по скале»</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-learning** — это алгоритм, который «изучает» эти значения. На каждом шагу мы получаем больше информации о мире. Эта информация используется для обновления значений в таблице.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/q_learning.gif\" alt=\"Drawing\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/443240/\">Понимание Q-learning, проблема «Прогулка по скале»</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы не знаем вероятности переходов. Мы можем только генерировать траектории и учиться на них.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/q_learning_scheme.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возникает вопрос, какую из функций ценности нам лучше пытаться учить: $V(s)$ или $Q(s,a)$? Поскольку $V(s)$ не позволит определять политику без знания $P_{ss'}^a$, будет выгоднее учить $Q(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/q_learning_possible_actions.png\" alt=\"Drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализировать таблицу $Q(s, a)$ нулями.\n",
    "\n",
    "* Цикл:\n",
    "   * Семплировать $<s, a, r, s^{'}>$ из среды\n",
    "   * Вычислить $\\hat{Q}(s, a) = r(s,a) + \\gamma Q(s^{'}, a_{i})$\n",
    "   * Обновить $Q(s,a) \\longleftarrow \\alpha \\hat{Q}(s, a) + (1-\\alpha)Q(s,a)$\n",
    "\n",
    "$\\color{red}\\triangle$ Мы хотим **несмещённую оценку с меньшей дисперисей**, поэтому используем **скользящее среднее**.\n",
    "\n",
    "$\\color{red}\\triangle$ **Пересчёт** функции (поле в таблице) выполняется **после перехода**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{Q}$ — расчетное значение, $Q$ — то, что было в таблице, $\\alpha$ — шаг обучения.\n",
    "\n",
    "Данный алгоритм называется __Q-learning__. Он напрямую аппроксимирует функцию $q_*(s,a)$, которая фактически представляет оптимальную политику $\\pi_*$. В данном алгоритме оценка $Q(s, a)$ обновляется в соответствии с правилами TD-обучения:\n",
    "\n",
    "$$\\large Q(s,a) := Q(s,a) + \\alpha(R_{ss'}^a+\\gamma\\max_{a'}Q(s',a') -Q(s,a))$$\n",
    "\n",
    "Видно, что за счет максимума Q-learning будет аппроксимировать именно $q_*(s,a)$, независимо от того, какой политики мы придерживаемся (которая может быть, например $\\varepsilon$-жадной). Наш вариант Q-learning не учитывает того, что мы хотим иногда \"рандомить\". В совокупности группа методов, при которых мы можем придерживаться абсолютно любой стратегии, а обучаться все равно будут правильные оптимальные значения политики, называется __off-policy__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть задачи, где количество состояний и переходов огромно, и поддерживать таблицу значений не представляется возможным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/large_num_of_states.png\" width=\"350\"/><center>\n",
    "\n",
    "<center><em>Иллюстрация игры Space Invaders</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество состояний для $8$-мибитной цветной ($3$ RGB-канала) игры Space Invaders с размером игрового поля $210 \\times 160$ пикселей:\n",
    "\n",
    "$$\\large |S| = 210\\times160\\times2^{8\\times3}$$\n",
    "\n",
    "В таких ситуациях Q-функцию не считают в явном виде, а аппроксимируют нейросетью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/approximately_q_function_by_network.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы считать градиент, приходится принимать значение Q-функции фиксированным и не зависящим от параметров нашей нейросети. Что, конечно, не так.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/deep_q_learning_loss.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-values:\n",
    "$$\n",
    "\\large \\hat{Q}\\left(s_{t}, a_{t}\\right)=r+\\gamma \\cdot \\max _{a^{\\prime}} Q\\left(s_{t+1}, a^{\\prime}\\right)\n",
    "$$\n",
    "Objective:\n",
    "$$\n",
    "\\large L=\\left(Q\\left(s_{t}, a_{t}\\right)-\\left[r+\\gamma \\cdot \\max _{a^{\\prime}} Q\\left(s_{t+1}, a^{\\prime}\\right)\\right]\\right)^{2}\n",
    "$$\n",
    "Gradient step:\n",
    "$$\n",
    "\\large w_{t+1}=w_{t}-\\alpha \\cdot \\frac{\\delta L}{\\delta w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic deep Q-learning**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/basic_deep_q_learning_scheme.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\varepsilon -greedy$ нужна для исследования среды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проблема**:\n",
    "\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/information_about_states_is_unevenly_distributed.png\" alt=\"Drawing\" width=\"550\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве окружений информация, получаемая агентом, распределена не независимо. Т.е. последовательные наблюдения агента сильно коррелированы между собой (что понятно из интуитивных соображений, т.к. большинство окружений, в которых применяется RL, предполагают, что все изменения в них последовательны).\n",
    "\n",
    "Мы можем столкнуться с такой проблемой при обучении нейросети, если в игре есть большое количество стандартных персонажей и редко встречающиеся \"боссы\". После повторения большого количества типовых действий нейросеть будет забывать, как проходить боссов.\n",
    "\n",
    "Корреляция примеров ухудшает сходимость стохастического градиентного спуска. Таким образом, нам нужен способ, который позволяет улучшить распределение примеров для обучения (устранить или снизить корреляцию между ними)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/experience_replay_scheme.png\" alt=\"Drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Идея**: сохранять несколько предыдущих взаимодействий $<s,a,r,s^{'}>$.\n",
    "\n",
    "Обучать на случайных подвыборках.\n",
    "\n",
    "**Процесс обучения**:\n",
    "* Сыграть 1 шаг и записать его\n",
    "* Выбрать $N$ случайных записей для обучения\n",
    "\n",
    "**Profit**: не требуется заново возвращаться в те же пары *состояние-действие* $(s,a)$, чтобы выучить их.\n",
    "* Можно получить хорошую стратегию, сделав меньше взаимодействий со средой\n",
    "* Боремся с катастрофическим забыванием данных\n",
    "\n",
    "$\\color{red}\\triangle$ **Работает только с алгоритмами без политик.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно используется метод **проигрывания опыта (experience replay)**. Суть этого метода в том, что мы сохраняем некоторое количество примеров (состояние, действия, вознаграждение) в специальном буфере и для обучения выбираем случайные мини-батчи из этого буфера.\n",
    "\n",
    "Так же **experience replay** позволяет агенту эффективнее использовать свой прошлый опыт."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/DQN-Loss_.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В DQN для эффективного обучения должны использоваться две сети: одна непосредственно обучается, другая определяет цель (TD target). Почему мы не можем пользоваться одной и той же сетью для получения оценки текущего $Q(s', a')$ и предыдущего $Q(s, a)$ состояний?\n",
    "\n",
    "Напомним, что в процессе TD-обучения мы \"подтягиваем\" значение функции ценности того состояния, в котором мы находились на предыдущем шаге, к значению функции ценности того состояния, в котором мы находимся сейчас, а в DQN мы делаем это с помощью нейронной сети, и если использовать одну и ту же сеть для оценки $Q(s', a')$ и $Q(s, a)$, то этот процесс может превратиться в нескончаемую погоню за целью, которую невозможно достичь, поскольку с каждым обновлением весов, приближающим нас к цели, одновременно сама цель будет удаляться от нас (как на рисунке ниже):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/At_first_everything_look.png\" alt=\"Drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К счастью, эта проблема решаема: нужно сделать так, чтобы сеть обучалась некоторое время аппроксимировать оценку $\\max_{a'} Q(s', a')$, полученную без использования обновленных весов Q-сети. Для этого используют __целевую сеть__ (target network) с зафиксированными весами, обновляемыми раз в несколько эпизодов обучения (например, эпох или игровых эпизодов, если обучение происходит после каждого игрового эпизода)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/Suppose_we_freeze.png\" alt=\"Drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, параллельно будут существовать две сети, одна из которых будет отвечать за выбор действия, другая — за целевую функцию, одна из которых будет обучаться с отставанием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример c CartPole DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть Deep Q = DQN (Deep Q-Network).\n",
    "\n",
    "CartPole — перевернутый маятник с центром тяжести над своей точкой поворота. Он нестабилен, но его можно контролировать, перемещая точку поворота под центром масс. Цель состоит в том, чтобы сохранить равновесие, прикладывая соответствующие усилия к точке поворота.\n",
    "\n",
    "Другой env можно использовать без каких-либо изменений кода. Пространство состояний должно быть единым вектором, действия должны быть дискретными.\n",
    "\n",
    "CartPole — самый простой. На ее решение должно уйти несколько минут.\n",
    "\n",
    "Для LunarLander может потребоваться 1–2 часа, чтобы получить 200 баллов (хороший балл) в Colab, и прогресс в обучении не выглядит информативным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "################################################\n",
    "# For CartPole\n",
    "################################################\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/atari_wrappers.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/utils.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/replay_buffer.py\n",
    "\n",
    "!pip install -q swig\n",
    "!pip install -q gym[box2d]\n",
    "\n",
    "!touch .setup_complete\n",
    "\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ[\"DISPLAY\"] = \":1\"\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\", new_step_api=True).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "env_img = np.squeeze(env.render())\n",
    "clear_output()\n",
    "plt.imshow(env_img)\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно построить нейронную сеть, которая может сопоставлять наблюдения с состоянием q-значений.\n",
    "\n",
    "Модель не должна быть слишком сложной: 1–2 скрытых слоя с < 200 нейронами и активацией ReLU, вероятно, будет достаточно.\n",
    "\n",
    "Batch Normalization и Dropout могут все испортить, поэтому их не используем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print(state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "\n",
    "        # Define NN\n",
    "        ##############################################\n",
    "        hidden_size = 150\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        ##############################################\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "\n",
    "        ##############################################\n",
    "        qvalues = self._nn(state_t)\n",
    "        ##############################################\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2\n",
    "            and qvalues.shape[0] == state_t.shape[0]\n",
    "            and qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy.\"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward.\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = (\n",
    "                qvalues.argmax(axis=-1)[0]\n",
    "                if greedy\n",
    "                else agent.sample_actions(qvalues)[0]\n",
    "            )\n",
    "            s, r, done, _, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer and Target Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Интерфейс довольно прост:**\n",
    "\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` — сохраняет (s,a,r,s',done) кортеж в буфер?\n",
    "* `exp_replay.sample(batch_size)` — возвращает observations, actions, rewards, next_observations и is_done для `batch_size` random samples,\n",
    "* `len(exp_replay)` — возвращает количество элементов, хранящихся в replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "\n",
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим ошибку TD Q-learning:\n",
    "\n",
    "$$ \\large L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_\\text{reference}(s,a) ] ^2 $$\n",
    "\n",
    "С Q-reference, определенным как:\n",
    "\n",
    "$$ \\large Q_\\text{reference}(s,a) = r(s,a) + \\gamma \\cdot \\max_{a'} Q_\\text{target}(s', a'), $$\n",
    "\n",
    "где:\n",
    "* $Q_\\text{target}(s',a')$ обозначает $Q$-значение следующего предсказанного состояния и следующего действия __target_network__,\n",
    "* $s, a, r, s'$ — текущее состояние, действие, вознаграждение и следующее состояние соответственно,\n",
    "* $\\gamma$ является коэффициентом дисконтирования, определенным двумя ячейками выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(\n",
    "    states,\n",
    "    actions,\n",
    "    rewards,\n",
    "    next_states,\n",
    "    is_done,\n",
    "    agent,\n",
    "    target_network,\n",
    "    gamma=0.99,\n",
    "    check_shapes=False,\n",
    "    device=device,\n",
    "):\n",
    "    \"\"\"Compute td loss using torch operations only. Use the formulae above.\"\"\"\n",
    "    states = torch.tensor(\n",
    "        states, device=device, dtype=torch.float32\n",
    "    )  # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(\n",
    "        actions, device=device, dtype=torch.int64\n",
    "    )  # shape: [batch_size]\n",
    "    rewards = torch.tensor(\n",
    "        rewards, device=device, dtype=torch.float32\n",
    "    )  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype(\"float32\"),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # with torch.no_grad():\n",
    "    predicted_next_qvalues = target_network(\n",
    "        next_states\n",
    "    )  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[\n",
    "        range(len(actions)), actions\n",
    "    ]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    ##############################################\n",
    "    next_state_values = predicted_next_qvalues.max(axis=-1)[0]\n",
    "    ##############################################\n",
    "\n",
    "    assert (\n",
    "        next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0]\n",
    "    ), \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    ###############################################\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * is_not_done\n",
    "    ##############################################\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean(\n",
    "        (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
    "    )\n",
    "\n",
    "    if check_shapes:\n",
    "        assert (\n",
    "            predicted_next_qvalues.data.dim() == 2\n",
    "        ), \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert (\n",
    "            next_state_values.data.dim() == 1\n",
    "        ), \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert (\n",
    "            target_qvalues_for_actions.data.dim() == 1\n",
    "        ), \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = make_env()\n",
    "env.reset(seed=seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем экспоненциальное скользящее среднее для сглаживания графика TD loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import fftconvolve, gaussian\n",
    "\n",
    "\n",
    "# Exponention Moving Average for plot smoothing\n",
    "def smoothen(values):\n",
    "    kernel = gaussian(100, std=100)\n",
    "    # kernel = np.concatenate([np.arange(100), np.arange(99, -1, -1)])\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return fftconvolve(values, kernel, \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один агент для игры, второй для обучения. Периодически веса обученного агента копируются в \"игрока\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "\n",
    "    hint: use agent.sample.actions\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for _ in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "\n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        # action = action.argmax(axis=-1)[0]\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        sum_rewards += reward\n",
    "\n",
    "        exp_replay.add(s, action, reward, state, done)\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "\n",
    "        s = state\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\n",
    "            \"\"\"\n",
    "            Less than 100 Mb RAM available.\n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "        )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0\n",
    "\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "state = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print(\"less that 100 Mb RAM available, freezing\")\n",
    "            print(\"make sure everything is ok and use KeyboardInterrupt to continue\")\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(\n",
    "            init_epsilon, final_epsilon, step, decay_steps\n",
    "        )\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        # sample batch_size of data from experience replay\n",
    "        s, a, r, next_s, is_done = exp_replay.sample(batch_size)\n",
    "        # loss = compute TD loss\n",
    "        loss = compute_td_loss(s, a, r, next_s, is_done, agent, target_network)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm.data.cpu().item())\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(\n",
    "                evaluate(make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues([make_env(seed=step).reset()])\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(make_env(), agent, n_games=30, greedy=True, t_max=1000)\n",
    "print(\"final score:\", final_score)\n",
    "if final_score > 300:\n",
    "    print(\"Well done\")\n",
    "else:\n",
    "    print(\"not good enough for DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Литература</font>\n",
    "\n",
    "* [[arxiv] 🎓 Deep Reinforcement Learning for Autonomous Driving: A Survey](https://arxiv.org/abs/2002.00444)<br>\n",
    "* [[arxiv] 🎓 Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review](https://arxiv.org/abs/1805.00909)<br>\n",
    "* [[git] 🐾 Lecture notes and exercises for control theory course](https://github.com/DPritykin/Control-Theory-Course)\n",
    "* [[article] 🎓 Reinforcement learning for combinatorial optimization: A survey](https://doi.org/10.1016/j.cor.2021.105400)<br>\n",
    "* [[arxiv] 🎓 Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940)\n",
    "* [[arxiv] 🎓 Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) <br>\n",
    "* [[blog] ✏️ AlphaGo](https://deepmind.google/technologies/alphago/)\n",
    "* [[book] 📚 Reinforcement Learning for NLP](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf)<br>\n",
    "* [[arxiv] 🎓 Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n",
    "* [[doc] 🛠️ Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "<font size=\"5\">Дополнительно:</font>\n",
    "* [[git] 🐾 A course in reinforcement learning in the wild](https://github.com/yandexdataschool/Practical_RL/tree/master)\n",
    "* [[book] 📚 8.1. Обучение с подкреплением, Учебник по машинному обучению ШАД](https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem)\n",
    "* [[course] 📚 UC Berkeley CS188 Intro to AI](http://ai.berkeley.edu/lecture_slides.html)\n",
    "* [[course] 📚 Deep Reinforcement Learning](https://rail.eecs.berkeley.edu/deeprlcourse/)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
