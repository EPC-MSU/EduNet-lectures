{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–º –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–±—É—á–µ–Ω–∏—é –º–∞—à–∏–Ω –Ω–µ –æ—Ç—Ä–∞–∂–∞—é—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ—Ü–µ—Å—Å, –ø—Ä–∏—Å—É—â–∏–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —Å—É—â–µ—Å—Ç–≤–∞–º. –ö –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é, —Å–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ–º—É –Ω–µ —Ç–æ–ª—å–∫–æ —á–µ–ª–æ–≤–µ–∫—É, –Ω–æ –∏ –∂–∏–≤—ã–º –æ—Ä–≥–∞–Ω–∏–∑–º–∞–º –≤ —Ü–µ–ª–æ–º, –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–º–µ—é—Ç –ª–∏—à—å –∫–æ—Å–≤–µ–Ω–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ. –ü–æ—ç—Ç–æ–º—É —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏–Ω–∞—è —Ñ–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–Ω—è—Ç–∏—è ¬´–∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–µ–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è¬ª, –≥–¥–µ –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å—Å—è –Ω–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞—Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–¢–µ—Ä–º–∏–Ω \"–ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ\" (**reinforcement**) –ø—Ä–∏—à–µ–ª –∏–∑ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ –∏ –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç –Ω–∞–≥—Ä–∞–¥—É –∏–ª–∏ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∑–∞–≤–∏—Å—è—â–∏–π –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç –ø—Ä–∏–Ω—è—Ç—ã—Ö —Ä–µ—à–µ–Ω–∏–π, –Ω–æ –∏ –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö, –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–æ–¥–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤.\n",
    "\n",
    "–í —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –ø–æ–Ω–∏–º–∞–µ—Ç—Å—è –∫–∞–∫ –ø–æ–∏—Å–∫ —Å–ø–æ—Å–æ–±–æ–≤ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –º–µ—Ç–æ–¥–æ–º –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫ (trial and error), —Ç–æ –µ—Å—Ç—å –ø–æ–ø—ã—Ç–æ–∫ —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ –¥–ª—è —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Å–≤–æ–µ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤ –±—É–¥—É—â–µ–º.\n",
    "\n",
    "**–ü—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–º –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫**:\n",
    " –í—ã —É—á–∏—Ç–µ—Å—å –≥–æ—Ç–æ–≤–∏—Ç—å –Ω–æ–≤–æ–µ –±–ª—é–¥–æ –±–µ–∑ —Ä–µ—Ü–µ–ø—Ç–∞. –í –ø–µ—Ä–≤—ã–π —Ä–∞–∑ –≤—ã –¥–æ–±–∞–≤–ª—è–µ—Ç–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–æ–ª–∏, –∏ –±–ª—é–¥–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Å–ª–∏—à–∫–æ–º —Å–æ–ª–µ–Ω—ã–º. –í–æ –≤—Ç–æ—Ä–æ–π —Ä–∞–∑ –≤—ã —É–º–µ–Ω—å—à–∞–µ—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–ª–∏ –∏ –±–ª—é–¥–æ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –≤–∫—É—Å–Ω—ã–º. –ò—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥ –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫, –≤—ã –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–ª–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ä–µ—Ü–µ–ø—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ú–æ—Ç–∏–≤–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏ –Ω–µ–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–æ—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –∫–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —É–∂–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ –Ω–∞–º –ø–æ–¥—Ö–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –≤–≤–µ–¥–µ–º –∫–ª—é—á–µ–≤—É—é –º–æ—Ç–∏–≤–∞—Ü–∏—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.\n",
    "\n",
    "–î–ª—è —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö —É—Å–ª–æ–≤–∏–π:\n",
    "1. –ò—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ —É—á–∏—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö.\n",
    "\n",
    "2. –ù–µ–æ–±—Ö–æ–¥–∏–º—ã –æ—Ç–≤–µ—Ç—ã –∏–ª–∏ —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "3. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏, –∏ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "*–ü—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º* ‚Äî —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ü–∏—Ñ—Ä –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ MNIST –ø—Ä–∏ –ø–æ–º–æ—â–∏ CNN.\n",
    "\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ, –≤ —Ä—è–¥–µ –∑–∞–¥–∞—á –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö —Ç—Ä–µ—Ö –ø—É–Ω–∫—Ç–æ–≤ (–∏–ª–∏ —á–∞—Å—Ç–∏ –∏–∑ –Ω–∏—Ö) –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ. –ò–º–µ–Ω–Ω–æ –≤ —Ç–∞–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∑–∞—Ä–∞–Ω–µ–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∞–≥–µ–Ω—Ç –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å–æ —Å—Ä–µ–¥–æ–π, –ø–æ–ª—É—á–∞—è –Ω–∞–≥—Ä–∞–¥—ã –∑–∞ —Å–≤–æ–∏ –¥–µ–π—Å—Ç–≤–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –∑–∞–¥–∞—á–∞—Ö, –≥–¥–µ –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ, –∞ —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞—Ç—Ä—É–¥–Ω–∏—Ç–µ–ª—å–Ω–∞ –∏–ª–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/sl_rl_comp.png\" alt=\"Drawing\" width=\"700\"></center>\n",
    "\n",
    "<center><em>–û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º vs –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—è –∞–≤—Ç–æ–ø–∏–ª–æ—Ç –¥–ª—è –∞–≤—Ç–æ–º–æ–±–∏–ª—è, –º—ã –≤—Ä—è–¥ –ª–∏ —Å–º–æ–∂–µ–º —Å–æ—Å—Ç–∞–≤–∏—Ç—å –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É, –≤ –∫–æ—Ç–æ—Ä–æ–π –±—É–¥—É—Ç –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥–æ—Ä–æ–∂–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏ –≤ —É—Å–ª–æ–≤–∏—è—Ö –ø–ª–æ—Ç–Ω–æ–≥–æ –≥–æ—Ä–æ–¥—Å–∫–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –∫ —Ç–æ–º—É –∂–µ –±—É–¥–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω–∞ —Ä–∞–∑–º–µ—Ç–∫–∞ \"ground truth\" —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–º —Ä–µ—à–µ–Ω–∏–∏ –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–∞ –≤ –Ω–∏—Ö.\n",
    "\n",
    "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–±—Ä–∞—Ç—å –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –≤–æ –º–Ω–æ–≥–æ–º –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–∞ —Å —Ç–µ–º, –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ —Å –∞–≤—Ç–æ–ø–∏–ª–æ—Ç–æ–º –∞–≤—Ç–æ–º–æ–±–∏–ª—è, —á—Ç–æ **—Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –º–æ–∂–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π –µ–≥–æ —Å—Ä–µ–¥—ã**, –∫–æ—Ç–æ—Ä–∞—è –∏ —è–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " –ü—Ä–∏ —ç—Ç–æ–º, –Ω–µ –∏–º–µ—è –≤—Å–µ–π –∫–∞—Ä—Ç–∏–Ω—ã –≤–æ–∑–º–æ–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏ –∏—Ö –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–π, –º—ã –º–æ–∂–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞—à–µ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –¥–∞–Ω–Ω—ã—Ö (**—Å—Ä–µ–¥–æ–π**) –∏ –ø–æ—Å—Ç—Ñ–∞–∫—Ç—É–º –æ—Ü–µ–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞—à–µ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ (**–∞–≥–µ–Ω—Ç–∞**), –∏, –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç—É –æ—Ü–µ–Ω–∫—É, –ø–æ–¥—Å—Ç—Ä–æ–∏—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω —á–∞—â–µ —Å–æ–≤–µ—Ä—à–∞–ª –∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è, –∏ —Ä–µ–∂–µ ‚Äî –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç—Ä–æ–∏—Ç—Å—è —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –∞–ª–≥–æ—Ä–∏—Ç–º (**–∞–≥–µ–Ω—Ç**) —Å—Ç—Ä–µ–º–∏–ª—Å—è –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∞–µ–º–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ (**reward**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º—ã –º–æ–∂–µ–º —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞—Ç—å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø–∞—Ä–∞–¥–∏–≥–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.\n",
    "\n",
    "**–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º**:\n",
    "- –°—É—â–µ—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ‚Äî —Å—Ä–µ–¥–∞.\n",
    "- –°—Ä–µ–¥–∞ **–Ω–µ –æ–±—è–∑–∞–Ω–∞** –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –≤ –≤–∏–¥–µ –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–µ–≥–æ –∏ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.\n",
    "- –°—Ä–µ–¥–∞ **–º–æ–∂–µ—Ç** –∏–∑–º–µ–Ω—è—Ç—å—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ (—Ä–µ—à–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞).\n",
    "- –°—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º (–∞–≥–µ–Ω—Ç), –∫–æ—Ç–æ—Ä—ã–π —Å—Ç—Ä–æ–∏—Ç —Å–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ —Å—Ä–µ–¥—ã.\n",
    "- –ú–æ–∂–Ω–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ —Å–æ —Å—Ä–µ–¥–æ–π (reward, —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞–≥—Ä–∞–¥—ã).\n",
    "- –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã **–Ω–µ –æ–±—è–∑–∞–Ω–∞** –±—ã—Ç—å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–æ–π –ø–æ –≤–µ—Å–∞–º –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–∞–≥–µ–Ω—Ç–∞).\n",
    "- –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã **–º–æ–∂–µ—Ç** –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ—à–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–∞.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/rl_def.png\" width=\"750\"></center>\n",
    "\n",
    "<center><em>–û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º<em/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –£—Å—Ç–æ—è–≤—à–∞—è—Å—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–í–≤–µ–¥–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –æ–ø–∏—à–µ–º —Ñ–æ—Ä–º–∞–ª—å–Ω—É—é –ø–æ—Å—Ç–∞–Ω–æ–≤–∫—É –∑–∞–¥–∞—á–∏.\n",
    "\n",
    "* –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å—É—â–Ω–æ—Å—Ç—å (—Å–∏—Å—Ç–µ–º—É/—Ä–æ–±–æ—Ç–∞/–∞–ª–≥–æ—Ä–∏—Ç–º), –ø—Ä–∏–Ω–∏–º–∞—é—â—É—é —Ä–µ—à–µ–Ω–∏—è, –±—É–¥–µ–º –Ω–∞–∑—ã–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–æ–º (**agent**).\n",
    "\n",
    "* –ê–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –º–∏—Ä–æ–º –∏–ª–∏ —Å—Ä–µ–¥–æ–π (**environment**), –∫–æ—Ç–æ—Ä–∞—è –∑–∞–¥–∞–µ—Ç—Å—è –∑–∞–≤–∏—Å—è—â–∏–º –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º (**state**). –ê–≥–µ–Ω—Ç—É –≤ –∫–∞–∂–¥—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω–æ –ª–∏—à—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ (**observation**) —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–∏—Ä–∞.\n",
    "\n",
    "* –°–∞–º –∞–≥–µ–Ω—Ç –∑–∞–¥–∞–µ—Ç –ø—Ä–æ—Ü–µ–¥—É—Ä—É –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è (**action**) –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è–º, —ç—Ç—É –ø—Ä–æ—Ü–µ–¥—É—Ä—É –±—É–¥–µ–º –Ω–∞–∑—ã–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –∏–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–æ–π (**policy**).\n",
    "\n",
    "* –ü–æ–¥ –∂–µ–ª–∞–µ–º—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –º—ã –±—É–¥–µ–º –ø–æ–Ω–∏–º–∞—Ç—å –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—é –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å–∫–∞–ª—è—Ä–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω—ã, –Ω–∞–∑—ã–≤–∞–µ–º–æ–π –Ω–∞–≥—Ä–∞–¥–æ–π (**reward**).\n",
    "\n",
    "* –ü—Ä–æ—Ü–µ—Å—Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ –∏ —Å—Ä–µ–¥—ã –∑–∞–¥–∞–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏–∫–æ–π —Å—Ä–µ–¥—ã, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–µ–π –ø—Ä–∞–≤–∏–ª–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å—Ä–µ–¥—ã –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã.\n",
    "\n",
    "* **–≠–ø–∏–∑–æ–¥** ‚Äî —Å–µ—Ä–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞ —Å–æ —Å—Ä–µ–¥–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–∞—Ä—Ç–∏—è –≤ –∏–≥—Ä–µ).\n",
    "\n",
    "\n",
    "\n",
    "–ë—É–∫–≤—ã $s, a, r$ –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä—É–µ–º –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–π, –¥–µ–π—Å—Ç–≤–∏–π –∏ –Ω–∞–≥—Ä–∞–¥ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –±—É–∫–≤–æ–π $t$ –±—É–¥–µ–º –æ–±–æ–∑–Ω–∞—á–∞—Ç—å –≤—Ä–µ–º—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/rl_msuai.png\" alt=\"Drawing\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä, –≥–¥–µ –∞–≥–µ–Ω—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–±–µ–Ω–æ–∫ (**agent**), –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—Å—è –µ–∑–¥–∏—Ç—å –Ω–∞ –≤–µ–ª–æ—Å–∏–ø–µ–¥–µ:\n",
    "\n",
    "–°—Ä–µ–¥–∞ (**environment**) –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≤–µ–ª–æ—Å–∏–ø–µ–¥ –∏ –æ–∫—Ä—É–∂–∞—é—â–∏–π –º–∏—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω –µ–∑–¥–∏—Ç.\n",
    "\n",
    "–í –∫–∞–∂–¥—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (**state**) –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∞–∂–∞—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ–∫—É—â—É—é —Å–∫–æ—Ä–æ—Å—Ç—å, —É–≥–æ–ª –Ω–∞–∫–ª–æ–Ω–∞ –≤–µ–ª–æ—Å–∏–ø–µ–¥–∞ –∏ –ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ä–µ–±–µ–Ω–∫–∞.\n",
    "\n",
    "–î–µ–π—Å—Ç–≤–∏–µ (**action**) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä—É–ª–µ–º –∏ –ø–µ–¥–∞–ª—è–º–∏ –≤ –∫–∞–∂–¥—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏.\n",
    "\n",
    " –ù–∞–≥—Ä–∞–¥–∞ (**reward**) - —ç—Ç–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –≤ –≤–∏–¥–µ –ø–æ—Ö–≤–∞–ª—ã –æ—Ç —Ä–æ–¥–∏—Ç–µ–ª–µ–π –∏–ª–∏ —Ä–∞–¥–æ—Å—Ç–∏ –æ—Ç –µ–∑–¥—ã –±–µ–∑ –ø–∞–¥–µ–Ω–∏–π –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –≤ –≤–∏–¥–µ –±–æ–ª–∏ –æ—Ç –ø–∞–¥–µ–Ω–∏–π.\n",
    "\n",
    " –ü–æ–ª–∏—Ç–∏–∫–∞ (**policy**) - —ç—Ç–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–µ–±–µ–Ω–∫–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∏ –∫—Ä—É—Ç–∏—Ç—å –ø–µ–¥–∞–ª–∏, –∫–æ—Ç–æ—Ä—É—é –æ–Ω —É–ª—É—á—à–∞–µ—Ç —Å –æ–ø—ã—Ç–æ–º, —Å—Ç—Ä–µ–º—è—Å—å –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞–¥–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–∞—Ç—å –±–æ–ª—å—à–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–º–æ—Ü–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á, —Ä–µ—à–∞–µ–º—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º, –∑–∞–¥–∞—á–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ç–µ–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è**\n",
    "\n",
    "[[arxiv] üéì Deep Reinforcement Learning for Autonomous Driving: A Survey](https://arxiv.org/abs/2002.00444)<br>\n",
    "[[arxiv] üéì Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review](https://arxiv.org/abs/1805.00909)<br>\n",
    "[[git] üêæ Lecture notes and exercises for control theory course](https://github.com/DPritykin/Control-Theory-Course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/cooling_cs.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–æ–π –æ—Ö–ª–∞–∂–¥–µ–Ω–∏—è –¥–∞—Ç–∞—Ü–µ–Ω—Ç—Ä–∞ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://deepmind.google/discover/blog/safety-first-ai-for-autonomous-data-centre-cooling-and-industrial-control/\">Safety-first AI for autonomous data centre cooling and industrial control</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≥—Ä—É–ø–ø –∑–∞–Ω—è—Ç–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π —Ä–µ—à–µ–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞. –ê–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ —Ç–∞–∫–∏—Ö —Ä–µ—à–µ–Ω–∏—è—Ö —á–∞—Å—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º ([Deep reinforcement learning on-board an autonomous car ‚úèÔ∏è[blog]](https://wayve.ai/thinking/learning-to-drive-in-a-day/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Source: https://wayve.ai/wp-content/uploads/2022/06/ezgif.com-gif-maker1.mp4\n",
    "!wget -qN https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/rl_control_system.mp4\n",
    "\n",
    "mp4 = open(\"rl_control_system.mp4\", \"rb\").read()\n",
    "data_url = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "HTML(f\"<video width=1000 controls><source src={data_url} type='video/mp4'></video>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ–¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã—Ö —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–≤**\n",
    "\n",
    "[[article] üéì Reinforcement learning for combinatorial optimization: A survey](https://doi.org/10.1016/j.cor.2021.105400)<br>\n",
    "[[arxiv] üéì Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **–°–æ–∑–¥–∞–Ω–∏–µ –∏–≥—Ä–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤**\n",
    "\n",
    "[[arxiv] üéì Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) <br>\n",
    "[[blog] ‚úèÔ∏è AlphaGo](https://deepmind.google/technologies/alphago/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/alpha_zero.png\" width=\"750\"></center>\n",
    "\n",
    "<center><em>The trained network is used to guide a search algorithm ‚Äì known as Monte-Carlo Tree Search (MCTS) ‚Äî to select the most promising moves in games. For each move, AlphaZero searches only a small fraction of the positions considered by traditional chess engines. In Chess, for example, it searches only 60 thousand positions per second, compared to roughly 60 million for Stockfish.</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/\">AlphaZero: Shedding new light on chess, shogi, and Go</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞**\n",
    "\n",
    "[[book] üìö Reinforcement Learning for NLP](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf)<br>\n",
    "[[arxiv] üéì Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/rlhf.png\" width=\"900\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://huyenchip.com/2023/05/02/rlhf.html\">RLHF: Reinforcement Learning from Human Feedback</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **–ü—Ä–æ—á–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è:**\n",
    "  \n",
    "  - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã\n",
    "  - –¢—Ä–µ–π–¥–∏–Ω–≥, –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º–∏\n",
    "  - Drug discovery\n",
    "  - (Neural Architecture Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —É –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤ –∫—É—Ä—Å–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ä–∞–±–æ—Ç–µ **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏–µ–º —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–æ–≥–æ —Ä–æ–±–æ—Ç–∞ —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏** —Ä–µ—à–∞–ª–∞—Å—å –∑–∞–¥–∞—á–∞ –ø–æ–¥–±–æ—Ä–∞ —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –¥–ª—è —Ä–æ–±–æ—Ç–∞, –ø–æ–∑–≤–æ–ª—è—é—â–µ–≥–æ –µ–º—É –¥–≤–∏–≥–∞—Ç—å—Å—è –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –∑–∞—Ä–∞–Ω–µ–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –Ω–∞ –ø–ª–æ—Å–∫–æ—Å—Ç–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/sphere_robot.png\" width=\"350\"></center>\n",
    "\n",
    "<center><em>–°—Ñ–µ—Ä–∏—á–µ—Å–∫–∏–π —Ä–æ–±–æ—Ç<em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/EduNetArchive/Nor_RL_sphere_robot\">EduNet-archive: RL sphere robot</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/sphere_robot_trajectories.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–≤–∏–∂–µ–Ω–∏—è —Å –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –≤—Å–ª–µ–¥—Å—Ç–≤–∏–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–æ–º –ø—Ä–∏ –ø–æ–º–æ—â–∏ RL-–∞–ª–≥–æ—Ä–∏—Ç–º–∞</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/EduNetArchive/Nor_RL_sphere_robot\">EduNet-archive: RL sphere robot</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥—Ä–æ–±–Ω–µ–µ:\n",
    "\n",
    "* [[video] üì∫ –í–∏–¥–µ–æ–∑–∞–ø–∏—Å—å –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è](https://www.youtube.com/watch?v=X_Wywj8lVHM)\n",
    "* [[slides] üìä –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è](https://docs.google.com/presentation/d/1qBizkCjiv2UsAnfdvN7yyOXY7QD0Ni7i/edit#slide=id.p1)\n",
    "* [[git] üêæ –ö–æ–¥](https://github.com/EduNetArchive/Nor_RL_sphere_robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ä–∞–±–æ—Ç–µ **–°–∞–º–æ—é—Å—Ç–∏—Ä—É—é—â–∏–µ—Å—è –æ–ø—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º** –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —É–ø—Ä–∞–≤–ª—è—é—â–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–ª–æ–∂–Ω–æ–π –æ–ø—Ç–∏—á–µ—Å–∫–æ–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/optical_system.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>–°—Ö–µ–º–∞ –æ–ø—Ç–∏—á–µ—Å–∫–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–µ–π —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–∏–µ –ª—É—á–∏</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/EduNetArchive/Mareev_X_ray_AI\">EduNet-archive: NN for controlling z-axis</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/optical_system_results.png\" width=\"500\"></center>\n",
    "\n",
    "<em><center>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ RL-–∞–ª–≥–æ—Ä–∏—Ç–º–∞</em></center>\n",
    "\n",
    "<em><center>Source: <a href=\"https://github.com/EduNetArchive/Mareev_X_ray_AI\">EduNet-archive: NN for controlling z-axis</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥—Ä–æ–±–Ω–µ–µ:\n",
    "\n",
    "* [[video] üì∫ –í–∏–¥–µ–æ–∑–∞–ø–∏—Å—å –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è](https://www.youtube.com/watch?v=mYKIvb9Y2z8)\n",
    "* [[slides] üìä –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è](https://docs.google.com/presentation/d/1gfFgQY_gm6eQ3yrL5p2b_CjbFE2xvzAL/edit#slide=id.p1)\n",
    "* [[git] üêæ –ö–æ–¥](https://github.com/EduNetArchive/Mareev_X_ray_AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateless environment in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–æ—Å—Ç–µ–π—à–∏–π –ø—Ä–∏–º–µ—Ä —Å—Ä–µ–¥—ã –¥–ª—è –∑–∞–¥–∞—á–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:\n",
    "- —Å—Ä–µ–¥–∞ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–∞, —Ç.–µ. –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏\n",
    "- —Å—Ä–µ–¥–∞ –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞\n",
    "\n",
    "–§–∞–∫—Ç–∏—á–µ—Å–∫–∏ –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ç–∞–∫–∞—è —Å—Ä–µ–¥–∞ –≤—Å–µ–≥–¥–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –æ–¥–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∏–ª–∏ –≤–æ–æ–±—â–µ *–Ω–µ –∏–º–µ–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è*. –î–ª—è –Ω–∞—Å –±—É–¥–µ—Ç –≤–∞–∂–Ω–æ, —á—Ç–æ –≤—ã–¥–∞–≤–∞–µ–º—ã–π —Å—Ä–µ–¥–æ–π reward –Ω–µ –æ–±—è–∑–∞–Ω –±—ã—Ç—å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º—ã–º, –∏ –ø–æ—Ç–æ–º—É —Ä–∞–±–æ—Ç–∞ –∞–≥–µ–Ω—Ç–∞ –ø–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é —Å —Ç–∞–∫–æ–π –≤—ã—Ä–æ–∂–¥–µ–Ω–Ω–æ–π —Å—Ä–µ–¥–æ–π –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–µ—à–µ–Ω–∞ –≤ —Ä–∞–º–∫–∞—Ö –ø–∞—Ä–∞–¥–∏–≥–º—ã –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–¥–∞—á–∞ –æ –º–Ω–æ–≥–æ—Ä—É–∫–∏—Ö –±–∞–Ω–¥–∏—Ç–∞—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ—Ä–æ–º stateless-—Å—Ä–µ–¥—ã –º–æ–∂–µ—Ç –ø–æ—Å–ª—É–∂–∏—Ç—å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è [–∑–∞–¥–∞—á–∞ –æ –º–Ω–æ–≥–æ—Ä—É–∫–∏—Ö –±–∞–Ω–¥–∏—Ç–∞—Ö üìö[wiki]](https://en.wikipedia.org/wiki/Multi-armed_bandit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/mab.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–≥–ª–∞—Å–Ω–æ —É—Å–ª–æ–≤–∏—è–º –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ –º—ã –∏–º–µ–µ–º:\n",
    "- $K$ –∏–≥—Ä–æ–≤—ã—Ö –∞–≤—Ç–æ–º–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º –¥–µ—Ä–Ω—É—Ç—å –∑–∞ —Ä—É—á–∫—É –≤ –Ω–∞–¥–µ–∂–¥–µ –ø–æ–ª—É—á–∏—Ç—å –≤—ã–∏–≥—Ä—ã—à,\n",
    "- $k$-—Ç—ã–π –∏–≥—Ä–æ–≤–æ–π –∞–≤—Ç–æ–º–∞—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—ã–∏–≥—Ä—ã—à —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é $p_k$,\n",
    "- –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—ã–∏–≥—Ä—ã—à–∞ $\\{p_k\\}$ –Ω–µ –∏–∑–º–µ–Ω—è—é—Ç—Å—è –≤–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –≤—Å–ª–µ–¥—Å—Ç–≤–∏–µ —Ä–µ—à–µ–Ω–∏–π –∏–≥—Ä–æ–∫–∞,\n",
    "- –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—ã–∏–≥—Ä—ã—à–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã –∏–≥—Ä–æ–∫—É.\n",
    "\n",
    "**–ó–∞–¥–∞—á–∞:** –∏–≥—Ä–æ–∫—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–¥—É–º–∞—Ç—å —Ç–∞–∫—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞–∂–∞—Ç–∏—è –Ω–∞ —Ä—ã—á–∞–≥–∏ –º–Ω–æ–≥–æ—Ä—É–∫–æ–≥–æ –±–∞–Ω–¥–∏—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –±—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞–ª–∞ –º–∞–∫—Å–∏–º—É–º –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –≤—ã–∏–≥—Ä—ã—à–∞.\n",
    "\n",
    "–î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ —Ä–µ—à–µ–Ω–∏—é —ç—Ç–æ–π –∑–∞–¥–∞—á–∏, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å –æ–ø–∏—Å–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã. –í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π Gymnasium, –∫–æ—Ç–æ—Ä—É—é —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–µ–¥ —Ç–µ–º, –∫–∞–∫ –ø—Ä–∏—Å—Ç—É–ø–∏—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–µ–π –Ω–∞—Å —Å—Ä–µ–¥—ã, —Ä–∞–∑–±–µ—Ä–µ–º—Å—è, —á—Ç–æ —Ç–∞–∫–æ–µ Gymnasium –∏ –∫–∞–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º.\n",
    "\n",
    "**Gymnasium** (Gym) ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ RL, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ RL –∏ —Å—Ä–µ–¥–∞–º–∏. –¢–∞–∫–∂–µ –æ–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Å—Ä–µ–¥, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç, –≤ —Ç–æ–º —á–∏—Å–ª–µ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞.\n",
    "\n",
    "* [[git] üêæ Gymnasium](https://github.com/Farama-Foundation/Gymnasium)\n",
    "* [[doc] üõ†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://gymnasium.farama.org/)\n",
    "* [[colab] ü•® Colab demonstration](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Å—Ä–µ–¥—ã Gym –≤ —Ü–µ–ª–æ–º.\n",
    "__–°—Ä–µ–¥–∞__ ‚Äî —ç—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä–∞—è –º–æ–¥–µ–ª—å –º–∏—Ä–∞, –æ—Ç–≤–µ—á–∞—é—â–∞—è –∑–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä–æ–π —Å—É—â–µ—Å—Ç–≤—É–µ—Ç __–∞–≥–µ–Ω—Ç__. –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã –±—É–¥–µ—Ç –∏–∑–º–µ–Ω—è—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞.\n",
    "\n",
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –¥–ª—è –Ω–∞—á–∞–ª–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å—Ä–µ–¥—É MountainCar, –≤ –∫–æ—Ç–æ—Ä–æ–π —Å—Ç–æ–∏—Ç –∑–∞–¥–∞—á–∞ ‚Äî –¥–æ–≤–µ—Å—Ç–∏ –º–∞—à–∏–Ω—É –¥–æ –≤–µ—Ä—à–∏–Ω—ã –≥–æ—Ä—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/mountain-car-v0.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym as openaigym\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í Gym —Å—Ä–µ–¥—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –∫–ª–∞—Å—Å–æ–º `gym.Env`, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º —Å—Ä–µ–¥—ã —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏ –∏ –º–µ—Ç–æ–¥–∞–º–∏:\n",
    "* `action_space`: –æ–ø–∏—Å–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π, –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –≤ –¥–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ;\n",
    "* `observation_space`: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Ä–µ–¥—ã;\n",
    "* `reset()`: —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—Ä–µ–¥—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω–æ–µ –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ;\n",
    "* `step(action)`: –º–µ—Ç–æ–¥, –ø—Ä–æ–¥–≤–∏–≥–∞—é—â–∏–π —Ä–∞–∑–≤–∏—Ç–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã –Ω–∞ –æ–¥–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç—Ç–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è, –∞ –∏–º–µ–Ω–Ω–æ:\n",
    "    * observation ‚Äî —Å–ª–µ–¥—É—é—â–µ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ;\n",
    "    * reward ‚Äî –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ;\n",
    "    * done ‚Äî —Ñ–ª–∞–≥ –∫–æ–Ω—Ü–∞ —ç–ø–∏–∑–æ–¥–∞.\n",
    "\n",
    "–¢–∞–∫–∂–µ –≤ –∫–ª–∞—Å—Å–µ `gym.Env` –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä, `render()`, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –≤ –ø–æ–Ω—è—Ç–Ω–æ–π —á–µ–ª–æ–≤–µ–∫—É —Ñ–æ—Ä–º–µ, –Ω–æ –º—ã –Ω–µ –±—É–¥–µ–º –∏—Ö –∫–∞—Å–∞—Ç—å—Å—è –≤ –Ω–∞—à–µ–π –ª–µ–∫—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏, –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏ –∏ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏. –î–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –∏ –≤–∑–∞–∏–º–Ω–æ –∏—Å–∫–ª—é—á–∞—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞–∂–∞—Ç–∏–µ/–æ—Ç–ø—É—Å–∫–∞–Ω–∏–µ –∫–ª–∞–≤–∏—à–∏. –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏—è–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ –Ω–µ–∫–æ—Ç–æ—Ä–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–≤–æ—Ä–æ—Ç —Ä—É–ª—è –æ—Ç -720 –¥–æ 720 –≥—Ä–∞–¥—É—Å–æ–≤. –í —Å—Ä–µ–¥–µ –º—ã –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –∫–∞–∫–∏–º-—Ç–æ –æ–¥–Ω–∏–º –¥–µ–π—Å—Ç–≤–∏–µ–º, –∏ –≤–ø–æ–ª–Ω–µ –¥–æ–ø—É—Å—Ç–∏–º–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –Ω–∞–∂–∞—Ç–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–Ω–æ–ø–æ–∫ –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–≤–æ—Ä–æ—Ç —Ä—É–ª—è. –ê–Ω–∞–ª–æ–≥–∏—á–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ (–ª–∞–º–ø–æ—á–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞/–≤—ã–∫–ª—é—á–µ–Ω–∞) –∏–ª–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º–∏ (—Ç–µ–Ω–∑–æ—Ä—ã, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ü–≤–µ—Ç–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º).\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–æ, –∫–∞–∫ –≤—ã–≥–ª—è–¥—è—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ —Å—Ä–µ–¥–µ MountainCar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# Action and observation space\n",
    "action_space = env.action_space\n",
    "obs_space = env.observation_space\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"The action space: {action_space}\")\n",
    "print(f\"The observation space: {obs_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –≤–∏–¥–∏–º, —á—Ç–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ `Box` –∏ `Discrete` —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ß—Ç–æ –∂–µ —ç—Ç–æ –∑–∞ –∫–ª–∞—Å—Å—ã? –î–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤ –¥–µ–π—Å—Ç–≤–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∏ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã—Ö) –≤ –æ–¥–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ, –≤ Gym —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤.\n",
    "\n",
    "* –ö–ª–∞—Å—Å `Discrete` –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–±–æ—Ä $n$ –≤–∑–∞–∏–º–æ–∏—Å–∫–ª—é—á–∞—é—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, `Discrete(n=4)` –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π —Å 4 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ –¥–≤–∏–∂–µ–Ω–∏—è ($\\leftarrow \\downarrow \\rightarrow \\uparrow$).\n",
    "* –ö–ª–∞—Å—Å `Box` –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç n-–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —á–∏—Å–µ–ª –≤ –Ω–µ–∫–æ—Ç–æ—Ä–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ `[low, high]`. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞–∂–∞—Ç–∏–µ –ø–µ–¥–∞–ª–∏ –≥–∞–∑–∞ –æ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è 0 –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ ‚Äî 1, –º–æ–∂–Ω–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ `—Å`, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ —ç–∫—Ä–∞–Ω–∞ –∏–≥—Ä—ã, –º–æ–∂–Ω–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ `Box(low=0, high=255, shape=(100, 50, 3), dtype=np.float32)`.\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –¥–∏–∞–ø–∞–∑–æ–Ω –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ —Å—Ä–µ–¥–µ MountainCar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upper Bound for Env Observation\", env.observation_space.high)\n",
    "print(\"Lower Bound for Env Observation\", env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –æ—Ç –∫–ª–∞—Å—Å–∞ `gym.Space`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(env.action_space))\n",
    "print(type(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ —Å—Ç–æ–∏—Ç —É–ø–æ–º—è–Ω—É—Ç—å –µ—â–µ –æ–¥–∏–Ω –¥–æ—á–µ—Ä–Ω–∏–π –∫–ª–∞—Å—Å  `gym.Space` &mdash; `Tuple`, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –∫–ª–∞—Å—Å–∞ `gym.Space` –≤–º–µ—Å—Ç–µ. –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É –∫–ª–∞—Å—Å—É –º—ã –º–æ–∂–µ–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –ª—é–±–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏.\n",
    "\n",
    "–í –∫–ª–∞—Å—Å–µ `gym.Space` —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –º–µ—Ç–æ–¥—ã, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏ –¥–µ–π—Å—Ç–≤–∏–π –∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π:\n",
    "* `sample()`: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π,\n",
    "* `contains(x)`: –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç–∞ –∫ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É –Ω–∞–±–ª—é–¥–µ–Ω–∏–π.\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ –≤–æ–∑—å–º–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, –¥–æ—Å—Ç—É–ø–Ω–æ–µ –Ω–∞–º –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å—Ä–µ–¥—ã MountainCar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_action = env.action_space.sample()  # random number from 0 to 2\n",
    "print(random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å–æ —Å—Ä–µ–¥–æ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º —Å–æ–≤–µ—Ä—à–∏—Ç—å —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º—ã –≤—ã–±—Ä–∞–ª–∏ –≤—ã—à–µ, –¥–ª—è —ç—Ç–æ–≥–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏–º —Å—Ä–µ–¥—É, —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –≤ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –∏ —Å–¥–µ–ª–∞–µ–º —à–∞–≥ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `step()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and see the initial observation\n",
    "obs, info = env.reset()\n",
    "print(f\"The initial observation is {obs}\")\n",
    "\n",
    "# Take the action and get the new observation space\n",
    "new_obs, reward, done, truncated, info = env.step(random_action)\n",
    "print(f\"The new observation is {new_obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–µ–π —Å—Ä–µ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–Ω–∞ –∏–∑ –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–¥–∞—á –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º ‚Äî –∑–∞–¥–∞—á–∞ –æ –º–Ω–æ–≥–æ—Ä—É–∫–∏—Ö –±–∞–Ω–¥–∏—Ç–∞—Ö. –í —ç—Ç–æ–π –∑–∞–¥–∞—á–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è, –∞ —É –∞–≥–µ–Ω—Ç–∞ –µ—Å—Ç—å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å.\n",
    "\n",
    "–≠—Ç—É –∑–∞–¥–∞—á—É –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º: –≤—ã –∏–≥—Ä–æ–∫ –≤ –∫–∞–∑–∏–Ω–æ —Å –∏–≥—Ä–æ–≤—ã–º–∏ –∞–≤—Ç–æ–º–∞—Ç–∞–º–∏, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —Å —Ä–∞–∑–ª–∏—á–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–∏–≥—Ä–∞—Ç—å –ø—Ä–∏–∑ —Ä–∞–∑–ª–∏—á–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω—ã. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ –∫–∞–∫ –∞–≥–µ–Ω—Ç–∞ ‚Äî –≤—ã–∏–≥—Ä–∞—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –¥–µ–Ω–µ–≥, –±—Ä–æ—Å–∞—è –º–æ–Ω–µ—Ç–∫–∏ –≤ –∞–≤—Ç–æ–º–∞—Ç—ã –Ω–∞ –≤–∞—à –≤—ã–±–æ—Ä.\n",
    "\n",
    "–î–æ–ø—É—Å—Ç–∏–º, –≤ –∫–∞–∑–∏–Ω–æ –Ω–∞—Ö–æ–¥—è—Ç—Å—è 4 –∞–≤—Ç–æ–º–∞—Ç–∞, –ª–∏—à—å –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ä–µ–¥–Ω–∏–π –≤—ã–∏–≥—Ä—ã—à –±—É–¥–µ—Ç > 1.\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ–ø–∏—Å—ã–≤–∞—Ç—å —ç—Ç—É —Å–∏—Ç—É–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from gym import Env, spaces\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class MultiArmedBanditEnv(Env):\n",
    "    def __init__(self, n=4):\n",
    "        \"\"\"\n",
    "        n - number of arms in the bandit\n",
    "        \"\"\"\n",
    "        self.num_bandits = n\n",
    "        self.action_space = spaces.Discrete(self.num_bandits)\n",
    "        self.observation_space = spaces.Discrete(1)  # the reward of the last action\n",
    "        self.bandit_success_prob = np.array(\n",
    "            [0.5, 0.1, 0.9, 0.2]\n",
    "        )  # success probabilities\n",
    "        self.bandit_reward = np.array([2, 20, 1, 3])\n",
    "\n",
    "    def step(self, action):\n",
    "        done = True\n",
    "        result_prob = np.random.random()\n",
    "        if result_prob < self.bandit_success_prob[action]:\n",
    "            reward = self.bandit_reward[action]\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–µ—à–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–ª—É—á–∞–π–Ω—ã–π –∞–≥–µ–Ω—Ç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –Ω–∞—à–µ–º –∫–∞–∑–∏–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ –≤ –∏–≥—Ä–µ, –Ω—É–∂–Ω–æ –∑–∞–ø–ª–∞—Ç–∏—Ç—å 1 –º–æ–Ω–µ—Ç—É. –ü–æ—Å–ª–µ –æ–ø–ª–∞—Ç—ã –∏–≥—Ä–æ–∫ –º–æ–∂–µ—Ç –Ω–∞–∂–∞—Ç—å –Ω–∞ –ª—é–±–æ–π —Ä—ã—á–∞–≥ –∏–≥—Ä–æ–≤–æ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∞ –∏ –ø–æ–ª—É—á–∏—Ç—å –≤—ã–∏–≥—Ä—ã—à —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é.\n",
    "\n",
    "–ö–∞–∫ –∏–º–µ–Ω–Ω–æ –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ –º—ã –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∑–∞ –∫–∞–∫–æ–π —Ä—ã—á–∞–≥ –Ω–∞–º —Å—Ç–æ–∏—Ç –¥–µ—Ä–≥–∞—Ç—å, —á—Ç–æ–±—ã –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã–∏–≥—Ä—ã—à? –í–µ–¥—å –º—ã –∫–∞–∫ –∏–≥—Ä–æ–∫ –Ω–µ –∑–Ω–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—ã–∏–≥—Ä—ã—à–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä—ã—á–∞–≥–æ–≤.\n",
    "\n",
    "–î–ª—è –Ω–∞—á–∞–ª–∞ —Ä–µ–∞–ª–∏–∑–∞—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –Ω–∞—à –∞–≥–µ–Ω—Ç –±—É–¥–µ—Ç —Ä–∞–≤–Ω–æ–≤–µ—Ä–æ—è—Ç–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç –¥–ª—è –∏–≥—Ä—ã –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Our agent initializes reward estimates as zeros.\n",
    "        This estimates will be updated incrementally after each\n",
    "        interaction with the environment.\n",
    "        \"\"\"\n",
    "        self.reward_estimates = np.zeros(4)\n",
    "        self.action_count = np.zeros(4)\n",
    "        self.cache = 1000  # initial amount of coins agent possesses\n",
    "\n",
    "    def get_action(self):\n",
    "        # pay 1 coin for the action\n",
    "        self.cache -= 1\n",
    "\n",
    "        # select a random action\n",
    "        action = np.random.choice(len(self.reward_estimates))\n",
    "\n",
    "        # add 1 to action selected in the action count\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_estimates(self, reward, action):\n",
    "        # update amount of cache by reward from our previous action\n",
    "        self.cache += reward\n",
    "\n",
    "        # compute the difference between the received rewards vs the reward estimates\n",
    "        error = reward - self.reward_estimates[action]\n",
    "\n",
    "        # update the reward estimate incrementally\n",
    "        n = self.action_count[action]\n",
    "        self.reward_estimates[action] += (1 / n) * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = MultiArmedBanditEnv()\n",
    "agent = RandomAgent()\n",
    "\n",
    "mean_reward_random = []\n",
    "rewards = []\n",
    "\n",
    "while (agent.cache > 0) and (len(mean_reward_random) != 10000):\n",
    "    act = agent.get_action()\n",
    "    reward, done = env.step(act)\n",
    "    agent.update_estimates(reward, act)\n",
    "    rewards.append(reward)\n",
    "    mean_reward_random.append(sum(rewards) / (len(rewards)))\n",
    "\n",
    "print(f\"Action counts: {agent.action_count}\")\n",
    "print(f\"Reward estimates: {agent.reward_estimates}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(mean_reward_random, label='Random Agent')\n",
    "ax[0].set_title('Average Reward over Time')\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "actions = ['Arm 1 (p=0.5, r=2)',\n",
    "           'Arm 2 (p=0.1, r=20)',\n",
    "           'Arm 3 (p=0.9, r=1)',\n",
    "           'Arm 4 (p=0.2, r=3)']\n",
    "ax[1].bar(actions, agent.action_count, label='Random Agent')\n",
    "ax[1].set_title('Action Selection Distribution')\n",
    "ax[1].set_ylabel('Count')\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=15)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ñ–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "–ü–æ–ø—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∂–∞–¥–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º: –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –±—É–¥–µ–º —Å–æ–≤–µ—Ä—à–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ, –∏–º–µ—é—â–µ–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É, –∏, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –±—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å —ç—Ç—É –æ—Ü–µ–Ω–∫—É.\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º—Å—è, —á—Ç–æ –æ–Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ü–µ–Ω–∏—Ç—å —Ü–µ–Ω–Ω–æ—Å—Ç—å —Ç–æ–≥–æ –∏–ª–∏ –∏–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –º–æ–∂–Ω–æ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É –≤—ã–∏–≥—Ä—ã—à—É, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –æ–Ω–æ –ø—Ä–∏–≤–µ–¥–µ—Ç.\n",
    "\n",
    "–ü—É—Å—Ç—å $R_n$ ‚Äî –Ω–∞–≥—Ä–∞–¥–∞, –ø–æ–ª—É—á–µ–Ω–Ω–∞—è –∑–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ $n$-–≥–æ –¥–µ–π—Å—Ç–≤–∏—è. –¢–æ–≥–¥–∞ –æ—Ü–µ–Ω–∫–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—è –±—É–¥–µ—Ç $\\displaystyle Q_n=\\frac{R_1+R_2+...+R_n}{n}$. –í–µ–ª–∏—á–∏–Ω–∞ $Q_n$ –ø–æ —Å—É—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø—Ä–æ—Å—Ç–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ (_simple moving average_), –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç –≤—ã—á–∏—Å–ª—è—Ç—å—Å—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ:\n",
    "\n",
    "$\\displaystyle Q_n = \\frac{1}{n}(R_n+(n-1)Q_{n-1}) = \\frac{1}{n}(R_n+nQ_{n-1}-Q_{n-1}) = Q_{n-1}+\\frac{1}{n}(R_n-Q_{n-1})$\n",
    "\n",
    "–í —Ü–µ–ª–æ–º, –ø–æ–¥–æ–±–Ω–∞—è —Å—Ö–µ–º–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º–∞—è –∫–∞–∫ `–Ω–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞:=—Å—Ç–∞—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ + —à–∞–≥[—Ü–µ–ª—å ‚Äî —Å—Ç–∞—Ä–∞—è –æ—Ü–µ–Ω–∫–∞]`, –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ —á–∞—Å—Ç–æ –∏ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è _incremental update rule_.\n",
    "\n",
    "–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π –∂–∞–¥–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è `argmax`, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –æ—Ü–µ–Ω–∫–æ–π, –∞ –≤ —Å–ª—É—á–∞–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –±—É–¥–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(x):\n",
    "    return np.random.choice(np.flatnonzero(x == x.max()))\n",
    "\n",
    "\n",
    "class GreedyAgent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Our agent initializes reward estimates as zeros.\n",
    "        This estimates will be updated incrementally after each\n",
    "        interaction with the environment.\n",
    "        \"\"\"\n",
    "        self.reward_estimates = np.zeros(4)\n",
    "        self.action_count = np.zeros(4)\n",
    "        self.cache = 1000  # initial amount of coins agent posesses\n",
    "\n",
    "    def get_action(self):\n",
    "        # Pay 1 coin for the action\n",
    "        self.cache -= 1\n",
    "\n",
    "        # Select greedy action\n",
    "        action = argmax(self.reward_estimates)\n",
    "\n",
    "        # Add a 1 to action selected in the action count\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        return action\n",
    "\n",
    "    def update_estimates(self, reward, action):\n",
    "        # Update amount of cache by reward from our previuos action\n",
    "        self.cache += reward\n",
    "\n",
    "        # Compute the difference between the received rewards vs the reward estimates\n",
    "        error = reward - self.reward_estimates[action]\n",
    "\n",
    "        # Update the reward estimate incementally\n",
    "        n = self.action_count[action]\n",
    "        self.reward_estimates[action] += (1 / n) * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = MultiArmedBanditEnv()\n",
    "agent = GreedyAgent()\n",
    "\n",
    "mean_reward_greedy = []\n",
    "rewards = []\n",
    "\n",
    "while (agent.cache > 0) and (len(mean_reward_greedy) != 10000):\n",
    "    act = agent.get_action()\n",
    "    reward, done = env.step(act)\n",
    "    agent.update_estimates(reward, act)\n",
    "    rewards.append(reward)\n",
    "    mean_reward_greedy.append(sum(rewards) / (len(rewards)))\n",
    "\n",
    "print(f\"Action counts: {agent.action_count}\")\n",
    "print(f\"Reward estimates: {agent.reward_estimates}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(mean_reward_greedy, label='Greedy Agent')\n",
    "ax[0].plot(mean_reward_random, label='Random Agent')\n",
    "ax[0].set_title('Average Reward over Time')\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "actions = ['Arm 1 (p=0.5, r=2)',\n",
    "           'Arm 2 (p=0.1, r=20)',\n",
    "           'Arm 3 (p=0.9, r=1)',\n",
    "           'Arm 4 (p=0.2, r=3)']\n",
    "ax[1].bar(actions, agent.action_count, label='Greedy Agent')\n",
    "ax[1].set_title('Action Selection Distribution')\n",
    "ax[1].set_ylabel('Count')\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=15)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω. –ê–≥–µ–Ω—Ç –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ —Å –Ω–∞–∏–≤—ã—Å—à–µ–π –æ—Ü–µ–Ω–∫–æ–π, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-–∂–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–¥–Ω–æ, —á—Ç–æ –∂–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –¥–æ–±–∏—Ç—å—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ª–µ–≥–∫–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å, –µ—Å–ª–∏ –≤ –Ω–∞—á–∞–ª–µ –∞–≥–µ–Ω—Ç—É –Ω–µ –ø–æ–≤–µ–∑–µ—Ç.\n",
    "\n",
    "–í–º–µ—Å—Ç–æ –∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å $\\varepsilon$-–∂–∞–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –æ–ø–∏—Å–∞—Ç—å –∫–∞–∫ \"–æ–ø—Ç–∏–º–∏–∑–º –ø—Ä–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏\". –°—É—Ç—å $\\varepsilon$-–∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å–ª–µ–¥—É—é—â–µ–º: —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é $\\varepsilon$ —Å–æ–≤–µ—Ä—à–∞—Ç—å —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, –∞ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é $1-\\varepsilon$ –≤–µ—Å—Ç–∏ —Å–µ–±—è –∂–∞–¥–Ω–æ.\n",
    "\n",
    "$\\varepsilon$ ‚Äî –≤–∞–∂–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–∞. –ö–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –≤ –Ω–∞—á–∞–ª–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è $\\varepsilon$, —Ç–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç –¥–µ–π—Å—Ç–≤—É–µ—Ç –ø–æ—á—Ç–∏ —Å–ª—É—á–∞–π–Ω–æ –∏ \"–∏—Å—Å–ª–µ–¥—É–µ—Ç –º–∏—Ä\", –∞ –∑–∞—Ç–µ–º –∏—Ö —É–º–µ–Ω—å—à–∞—é—Ç, –∏ –¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –±–ª–∏–∑–∫–∏–º–∏ –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º.\n",
    "\n",
    "–í—ã–±–æ—Ä –≤–µ–ª–∏—á–∏–Ω—ã $\\varepsilon$ —Å–≤—è–∑–∞–Ω —Å —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º—ã–º __exploration-exploitation trade-off__. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–µ–≥–∫–æ –ø—Ä–æ–∏–ª–ª—é—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏–º–µ—Ä–æ–º, –∫–æ–≥–¥–∞ –≤—ã –Ω–µ –º–æ–∂–µ—Ç–µ —Ä–µ—à–∏—Ç—å, –ø–æ–π—Ç–∏ –ª–∏ —Å–µ–≥–æ–¥–Ω—è –≤ –ª—é–±–∏–º—ã–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω –∏–ª–∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å—Ö–æ–¥–∏—Ç—å –≤ –Ω–æ–≤—ã–π? –ù–æ–≤—ã–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω –º–æ–∂–µ—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è –ª—É—á—à–µ–≥–æ –ø—Ä–∏–≤—ã—á–Ω–æ–≥–æ, –Ω–æ —ç—Ç–æ —Ä–µ—à–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–æ —Å —Ä–∏—Å–∫–æ–º —Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/exploration_vs_exploitation.png\" alt=\"Drawing\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent(GreedyAgent):\n",
    "    def __init__(self, epsilon):\n",
    "        GreedyAgent.__init__(self)\n",
    "        # Store the epsilon value\n",
    "        assert epsilon >= 0 and epsilon <= 1\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_action(self):\n",
    "        # We need to redefine this function so that it takes an exploratory action with epsilon probability\n",
    "\n",
    "        # Pay 1 coin for the action\n",
    "        self.cache -= 1\n",
    "        # One hot encoding: 0 if exploratory, 1 otherwise\n",
    "        action_type = int(np.random.random() > self.epsilon)\n",
    "        # Generate both types of actions for every experiment\n",
    "        exploratory_action = np.random.randint(4)\n",
    "        greedy_action = argmax(self.reward_estimates)\n",
    "        # Use the one hot encoding to mask the actions for each experiment\n",
    "        action = greedy_action * action_type + exploratory_action * (1 - action_type)\n",
    "\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = EpsilonGreedyAgent(epsilon=0.25)\n",
    "\n",
    "mean_reward_e_greedy = []\n",
    "rewards = []\n",
    "\n",
    "while (agent.cache > 0) and (len(mean_reward_e_greedy) != 10000):\n",
    "    act = agent.get_action()\n",
    "    reward, done = env.step(act)\n",
    "    agent.update_estimates(reward, act)\n",
    "    rewards.append(reward)\n",
    "    mean_reward_e_greedy.append(sum(rewards) / len(rewards))\n",
    "    # Decrease epsilon with time until we reach 0.01 chanse to perform exploratory action\n",
    "    if agent.epsilon > 0.01:\n",
    "        agent.epsilon -= 0.0001\n",
    "\n",
    "print(f\"Action counts: {agent.action_count}\")\n",
    "print(f\"Reward estimates: {agent.reward_estimates}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(mean_reward_e_greedy, label='e-Greedy Agent')\n",
    "ax[0].plot(mean_reward_greedy, label='Greedy Agent')\n",
    "ax[0].plot(mean_reward_random, label='Random Agent')\n",
    "ax[0].set_title('Average Reward over Time')\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "actions = ['Arm 1 (p=0.5, r=2)',\n",
    "           'Arm 2 (p=0.1, r=20)',\n",
    "           'Arm 3 (p=0.9, r=1)',\n",
    "           'Arm 4 (p=0.2, r=3)']\n",
    "ax[1].bar(actions, agent.action_count, label='e-Greedy Agent')\n",
    "ax[1].set_title('Action Selection Distribution')\n",
    "ax[1].set_ylabel('Count')\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=15)\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " –í —ç–ø—Å–∏–ª–æ–Ω-–∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –∞–≥–µ–Ω—Ç –¥–µ–π—Å—Ç–≤—É–µ—Ç –∂–∞–¥–Ω–æ –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –≤—Ä–µ–º–µ–Ω–∏, –Ω–æ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é —ç–ø—Å–∏–ª–æ–Ω ($Œµ$) –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ —Å–ª—É—á–∞–π–Ω–æ, —á—Ç–æ –¥–æ–±–∞–≤–ª—è–µ—Ç —ç–ª–µ–º–µ–Ω—Ç —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, –Ω–æ —Ç–æ–ª—å–∫–æ –≤ —Ç–µ –º–æ–º–µ–Ω—Ç—ã, –∫–æ–≥–¥–∞ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–ø—Å–∏–ª–æ–Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ $\\epsilon$-–∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∑–≤–æ–ª–∏–ª–æ –Ω–∞–º –∏–Ω–æ–≥–¥–∞ —Å–æ–≤–µ—Ä—à–∞—Ç—å \"—Ä–∞–∑–≤–µ–¥—ã–≤–∞—Ç–µ–ª—å–Ω—ã–µ\" —Ä–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã—Ç—å –Ω–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±—É–∫–≤–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–≥–æ –Ω–∞–º–∏ —Ä–∞–Ω–µ–µ –æ–ø—ã—Ç–∞. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞ –º—ã –±—ã –º–æ–≥–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å, –µ—Å–ª–∏ –±—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –Ω–∞—à—É —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π.\n",
    "\n",
    "–ü—É—Å—Ç—å –º—ã –Ω–∞—Ö–æ–¥–∏–º—Å—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ $s$, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ –≤–æ–∑–º–æ–∂–Ω—ã $K$ –¥–µ–π—Å—Ç–≤–∏–π $\\{a_i\\}$. –û–ø—Ä–µ–¥–µ–ª–∏–º –≤–µ–ª–∏—á–∏–Ω—É $\\pi(a_i|s)$:\n",
    "- $\\pi(a_i| s)$ ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ $a_i$, –µ—Å–ª–∏ –∞–≥–µ–Ω—Ç –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ $s$.\n",
    "\n",
    "–î–∞–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω—è—Ç–æ –Ω–∞–∑—ã–≤–∞—Ç—å **–ø–æ–ª–∏—Ç–∏–∫–æ–π (policy)**.\n",
    "\n",
    "–î–ª—è \"–∏–≥—Ä—ã\" —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–∞–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞–º –Ω—É–∂–Ω–æ:\n",
    "1. –í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ $\\pi(a_i| s)$.\n",
    "2. –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ $a$, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–µ–µ –¥–∞–Ω–Ω–æ–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é: $a \\sim \\pi(a_i| s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "class SoftmaxAgent(GreedyAgent):\n",
    "    def __init__(self):\n",
    "        GreedyAgent.__init__(self)\n",
    "\n",
    "    def get_action(self):\n",
    "        # We need to redefine this function so that it takes an action with probability pi(a,s)\n",
    "\n",
    "        # Pay 1 coin for the action\n",
    "        self.cache -= 1\n",
    "\n",
    "        # get \\pi(a_i|s):\n",
    "        pi = softmax(self.reward_estimates)\n",
    "        # sample action\n",
    "        action = choice(np.arange(0, 4, 1), 1, p=pi)[0]\n",
    "\n",
    "        self.action_count[action] += 1\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SoftmaxAgent()\n",
    "\n",
    "mean_reward_softmax_agent = []\n",
    "rewards = []\n",
    "\n",
    "while (agent.cache > 0) and (len(mean_reward_softmax_agent) != 10000):\n",
    "    act = agent.get_action()\n",
    "    reward, done = env.step(act)\n",
    "    agent.update_estimates(reward, act)\n",
    "    rewards.append(reward)\n",
    "    mean_reward_softmax_agent.append(sum(rewards) / len(rewards))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"Action counts: {agent.action_count}\")\n",
    "print(f\"Reward estimates: {agent.reward_estimates}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(mean_reward_softmax_agent, label='Softmax Agent')\n",
    "ax[0].plot(mean_reward_e_greedy, label='e-Greedy Agent')\n",
    "ax[0].plot(mean_reward_greedy, label='Greedy Agent')\n",
    "ax[0].plot(mean_reward_random, label='Random Agent')\n",
    "ax[0].set_title('Average Reward over Time')\n",
    "ax[0].set_xlabel('Steps')\n",
    "ax[0].set_ylabel('Average Reward')\n",
    "ax[0].legend()\n",
    "\n",
    "actions = ['Arm 1 (p=0.5, r=2)',\n",
    "           'Arm 2 (p=0.1, r=20)',\n",
    "           'Arm 3 (p=0.9, r=1)',\n",
    "           'Arm 4 (p=0.2, r=3)']\n",
    "ax[1].bar(actions, agent.action_count, label='Softmax Agent')\n",
    "ax[1].set_title('Action Selection Distribution')\n",
    "ax[1].set_ylabel('Count')\n",
    "plt.setp(ax[1].get_xticklabels(), rotation=15)\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í softmax-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏  –∞–≥–µ–Ω—Ç –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞–º –æ—Ü–µ–Ω–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –¥–µ–π—Å—Ç–≤–∏—è –≤—Å–µ–≥–¥–∞ —Å—ç–º–ø–ª–∏—Ä—É—é—Ç—Å—è –∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ú–µ—Ç–∞-—ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º\n",
    "\n",
    "–ú–µ—Ç–∞-—ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –æ–±–æ–±—â–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ—à–µ–Ω–∏–π –≤ —Å–ª–æ–∂–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≥–¥–µ –º—ã –Ω–µ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.\n",
    "\n",
    "–ú—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –∏—Ö –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ black-box –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏. –ú—ã –º–æ–∂–µ–º –ø–æ–º–µ—Å—Ç–∏—Ç—å –∞–≥–µ–Ω—Ç–∞ –≤ —Å—Ä–µ–¥—É —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –æ—Ü–µ–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥ –æ —Ç–æ–º —Ö–æ—Ä–æ—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º—ã –ø–æ–¥–æ–±—Ä–∞–ª–∏ –∏–ª–∏ –Ω–µ—Ç.\n",
    "\n",
    "–ó–∞–¥–∞—á–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏—à—å —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å, –∫–∞–∫–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–ª–µ–¥—É–µ—Ç –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–º–∏. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ–¥–∏–Ω –∏–∑ —Ç–∞–∫–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy method (CEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–ö—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–π –º–µ—Ç–æ–¥ (CEM) ‚Äî —ç—Ç–æ –º–µ—Ç–∞-—ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏ —Ä–µ—à–µ–Ω–∏–π. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "\n",
    "1. **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:**\n",
    "   - –ù–∞—á–Ω–∏—Ç–µ —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è $( P(\\theta)$), –Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ —Å—Ä–µ–¥–Ω–∏–º–∏ –∏ –¥–∏—Å–ø–µ—Ä—Å–∏—è–º–∏.\n",
    "\n",
    "2. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–±–æ—Ä–∫–∏:**\n",
    "   - –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –≤—ã–±–æ—Ä–∫—É –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ $( \\{\\theta_1, \\theta_2, ..., \\theta_N\\} $) –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è $( P(\\theta) $).\n",
    "\n",
    "3. **–û—Ü–µ–Ω–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤:**\n",
    "   - –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ $( \\theta_i $) –æ—Ü–µ–Ω–∏—Ç–µ –µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–æ –∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã $( S(\\theta_i) $).\n",
    "\n",
    "4. **–û—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤:**\n",
    "   - –û—Ç–±–µ—Ä–∏—Ç–µ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ $( k $) –ª—É—á—à–∏—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö –æ—Ü–µ–Ω–æ–∫. –≠—Ç–∏ –ª—É—á—à–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç —ç–ª–∏—Ç–Ω—ã–π –Ω–∞–±–æ—Ä.\n",
    "\n",
    "5. **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:**\n",
    "   - –û–±–Ω–æ–≤–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è $( P(\\theta) $) –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–ª–∏—Ç–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º, –æ–±–Ω–æ–≤–∏—Ç–µ –µ–≥–æ —Å—Ä–µ–¥–Ω–∏–µ –∏ –¥–∏—Å–ø–µ—Ä—Å–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è –∑–Ω–∞—á–µ–Ω–∏—è —ç–ª–∏—Ç–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.\n",
    "\n",
    "6. **–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏:**\n",
    "   - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –ª–∏ —É—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–ª–∏ –∏—Å—Ç–µ—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —á–∏—Å–ª–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π). –ï—Å–ª–∏ –¥–∞, –∑–∞–≤–µ—Ä—à–∏—Ç–µ –∞–ª–≥–æ—Ä–∏—Ç–º. –ï—Å–ª–∏ –Ω–µ—Ç, –≤–µ—Ä–Ω–∏—Ç–µ—Å—å –∫ —à–∞–≥—É 2. –í –∫–æ–Ω—Ü–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã $( \\theta $), –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏—Ç–µ—Ä–∞—Ü–∏–π, –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/schematic_view_cross_entropy.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ—Ä CEM –≤ Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –≥–æ—Ç–æ–≤–æ–π —Å—Ä–µ–¥–æ–π [MountainCarContinuous üõ†Ô∏è[doc]](https://mgoulao.github.io/gym-docs/environments/classic_control/mountain_car_continuous/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/mountain-car-v0.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env.seed(42)\n",
    "clear_output()\n",
    "\n",
    "print(\"observation space:\", env.observation_space)\n",
    "print(\"action space:\", env.action_space)\n",
    "print(\"  - low:\", env.action_space.low)\n",
    "print(\"  - high:\", env.action_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –∞–≥–µ–Ω—Ç–∞:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__()\n",
    "        self.env = env\n",
    "        # state, hidden layer, action sizes\n",
    "        self.s_size = env.observation_space.shape[0]\n",
    "        self.h_size = h_size\n",
    "        self.a_size = env.action_space.shape[0]\n",
    "        # define layers (we used 2 layers)\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size)\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size)\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        s_size = self.s_size\n",
    "        h_size = self.h_size\n",
    "        a_size = self.a_size\n",
    "        # separate the weights for each layer\n",
    "        fc1_end = (s_size * h_size) + h_size\n",
    "        fc1_W = torch.from_numpy(weights[: s_size * h_size].reshape(s_size, h_size))\n",
    "        fc1_b = torch.from_numpy(weights[s_size * h_size : fc1_end])\n",
    "        fc2_W = torch.from_numpy(\n",
    "            weights[fc1_end : fc1_end + (h_size * a_size)].reshape(h_size, a_size)\n",
    "        )\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end + (h_size * a_size) :])\n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "\n",
    "    def get_weights_dim(self):\n",
    "        return (self.s_size + 1) * self.h_size + (self.h_size + 1) * self.a_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x.cpu().data\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            action = self.forward(state)\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n",
    "\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∞–ª–∏–∑—É–µ–º Cross-Entropy Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def cem(\n",
    "    agent,\n",
    "    n_iterations=5,\n",
    "    max_t=1000,\n",
    "    gamma=1.0,\n",
    "    print_every=10,\n",
    "    pop_size=50,\n",
    "    elite_frac=0.2,\n",
    "    sigma=0.5,\n",
    "):\n",
    "    \"\"\"PyTorch implementation of the cross-entropy method.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        Agent (object): agent instance\n",
    "        n_iterations (int): maximum number of training iterations\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        pop_size (int): size of population at each iteration\n",
    "        elite_frac (float): percentage of top performers to use in update\n",
    "        sigma (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    n_elite = int(pop_size * elite_frac)\n",
    "\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Initialize the weight with random noise\n",
    "    best_weight = sigma * np.random.randn(agent.get_weights_dim())\n",
    "\n",
    "    for i_iteration in tqdm(range(1, n_iterations + 1)):\n",
    "        # Define the cadidates and get the reward of each candidate\n",
    "        weights_pop = [\n",
    "            best_weight + (sigma * np.random.randn(agent.get_weights_dim()))\n",
    "            for i in range(pop_size)\n",
    "        ]\n",
    "        rewards = np.array(\n",
    "            [agent.evaluate(weights, gamma, max_t) for weights in weights_pop]\n",
    "        )\n",
    "\n",
    "        # Select best candidates from collected rewards\n",
    "        elite_idxs = rewards.argsort()[-n_elite:]\n",
    "        elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "\n",
    "        reward = agent.evaluate(best_weight, gamma=1.0)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "\n",
    "        torch.save(agent.state_dict(), \"checkpoint.pth\")\n",
    "\n",
    "        if i_iteration % print_every == 0:\n",
    "            print(\n",
    "                \"Episode {}\\tAverage Score: {:.2f}\".format(\n",
    "                    i_iteration, np.mean(scores_deque)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if np.mean(scores_deque) >= 90.0:\n",
    "            print(\n",
    "                \"\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}\".format(\n",
    "                    i_iteration - 100, np.mean(scores_deque)\n",
    "                )\n",
    "            )\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "agent = Agent(env).to(device)\n",
    "scores = cem(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import base64, io\n",
    "from IPython import display\n",
    "from IPython.display import HTML\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "\n",
    "\n",
    "def show_video(env_name):\n",
    "    mp4list = glob.glob(\"video/*.mp4\")\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = \"video/{}.mp4\".format(env_name)\n",
    "        video = io.open(mp4, \"r+b\").read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(\n",
    "            HTML(\n",
    "                data=\"\"\"<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>\"\"\".format(\n",
    "                    encoded.decode(\"ascii\")\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "\n",
    "def show_video_of_model(agent, env_name):\n",
    "    env = gym.make(env_name)\n",
    "    vid = video_recorder.VideoRecorder(env, path=\"video/{}.mp4\".format(env_name))\n",
    "    agent.load_state_dict(torch.load(\"checkpoint.pth\"))\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        vid.capture_frame()\n",
    "\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env).to(device)\n",
    "show_video_of_model(agent, \"MountainCarContinuous-v0\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "print(\"–ù–µ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å:\")\n",
    "show_video(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –≤–µ—Å–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –≤ —Ç–µ—á–µ–Ω–∏–µ 500 —ç–ø–∏–∑–æ–¥–æ–≤ –º–æ–¥–µ–ª–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qN https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/weights/checkpoint.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env).to(device)\n",
    "show_video_of_model(agent, \"MountainCarContinuous-v0\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_output()\n",
    "print(\"–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å:\")\n",
    "show_video(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π (Markov decision process, MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/mdp.png\" alt=\"Drawing\" width=\"600\">\n",
    "\n",
    "<center><em> –ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π<em><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–∞–∫, –º—ã —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å —Å —Å–∞–º–æ–π –ø—Ä–æ—Å—Ç–æ–π —Å–∏—Ç—É–∞—Ü–∏–µ–π, –º–Ω–æ–≥–æ—Ä—É–∫–∏–º–∏ –±–∞–Ω–¥–∏—Ç–∞–º–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π —Å—Ä–µ–¥–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ —Å–æ—Å—Ç–æ—è–Ω–∏–∏. –û–¥–Ω–∞–∫–æ –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —á–∞—â–µ –≤—Å–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –∞–≥–µ–Ω—Ç –¥–µ–ª–∞–µ—Ç –Ω–æ–≤—ã–π —Ö–æ–¥, –±—É–¥–µ—Ç –∏–∑–º–µ–Ω—è—Ç—å—Å—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –∏–≥—Ä–µ –≤ —à–∞—Ö–º–∞—Ç—ã –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω —É—á–∏—Ç—ã–≤–∞—Ç—å —Ç–æ, —á—Ç–æ –ø–æ—Å–ª–µ –µ–≥–æ —Ö–æ–¥–∞ –∏ —Ö–æ–¥–∞ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞ –ø–æ–∑–∏—Ü–∏—è –Ω–∞ –¥–æ—Å–∫–µ –∏–∑–º–µ–Ω–∏—Ç—Å—è.\n",
    "\n",
    "–ù–∞–º –Ω—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–µ–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å, –≤ –∫–æ—Ç–æ—Ä–æ–º –∞–≥–µ–Ω—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–≤–æ–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π, –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏—è—Ö –ø–æ–ª—É—á–∞—è –Ω–∞–≥—Ä–∞–¥—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–¥–µ–ª–∞–µ–º —Ç–∞–∫–∂–µ –≤–∞–∂–Ω–æ–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ –ø—Ä–∏—Ä–æ–¥–µ –Ω–∞—à–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞: **–æ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Å–≤–æ–∏–º —Ç–µ–∫—É—â–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º**. –í—Å–µ, —á—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç –≤ –±—É–¥—É—â–µ–º, –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ, –∫—Ä–æ–º–µ —Ç–æ–π, —á—Ç–æ –º—ã —É–∂–µ –Ω–∞–±–ª—é–¥–∞–µ–º –≤ –Ω–∞—Å—Ç–æ—è—â–µ–º. –¢–∞–∫–æ–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è __–º–∞—Ä–∫–æ–≤—Å–∫–∏–º__. –ó–∞–º–µ—Ç–∏–º, —á—Ç–æ –æ–ø–∏—Å–∞–Ω–Ω—ã–π –Ω–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å –≤ –ø—Ä–∏–º–µ—Ä–µ —Å –º–Ω–æ–≥–æ—Ä—É–∫–∏–º–∏ –±–∞–Ω–¥–∏—Ç–∞–º–∏ —Ç–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è –º–∞—Ä–∫–æ–≤—Å–∫–∏–º.\n",
    "\n",
    "–ü—Ä–∏–≤–µ–¥–µ–º –¥—Ä—É–≥–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –º–∞—Ä–∫–æ–≤—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞:\n",
    "\n",
    "1. –ò–≥—Ä–∞–ª—å–Ω—ã–π –∫—É–±–∏–∫. –ú—ã –∑–Ω–∞–µ–º, —á—Ç–æ –Ω–∞ –Ω–µ–º –≤—ã–ø–∞–¥–µ—Ç –ª—é–±–∞—è –∏–∑ –≥—Ä–∞–Ω–µ–π —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é. –ù–∞ —ç—Ç–æ –Ω–∏–∫–∞–∫ –Ω–µ –≤–ª–∏—è–µ—Ç —Ç–æ, —á—Ç–æ –¥–æ —ç—Ç–æ–≥–æ –Ω–∞ –∫—É–±–∏–∫–µ –≤—ã–ø–∞–ª–æ 6 —à–µ—Å—Ç–µ—Ä–æ–∫ –ø–æ–¥—Ä—è–¥. –≠—Ç–æ –º–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –Ω–∞—à—É –æ—Ü–µ–Ω–∫—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤—ã–ø–∞–¥–µ–Ω–∏—è —Ç–æ–π –∏–ª–∏ –∏–Ω–æ–π –≥—Ä–∞–Ω–∏, –Ω–æ –Ω–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.\n",
    "\n",
    "2. –®–∞—Ö–º–∞—Ç—ã. \"–¢–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è –Ω–∞ –¥–æ—Å–∫–µ + —á–µ–π —Ö–æ–¥\" –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∏–≥—Ä—É.\n",
    "\n",
    "\n",
    "–û–¥–Ω–∞–∫–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —ç—Ç–æ —Å–≤–æ–π—Å—Ç–≤–æ, –º—ã —Å–∞–º–∏ –º–æ–∂–µ–º —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤–∫–ª—é—á–∏–≤ –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –º—ã –º–æ–∂–µ–º –æ–±–µ—Å–ø–µ—á–∏—Ç—å, —á—Ç–æ–±—ã –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª –º–∞—Ä–∫–æ–≤—Å–∫–æ–º—É —Å–≤–æ–π—Å—Ç–≤—É, –¥–∞–∂–µ –µ—Å–ª–∏ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ —ç—Ç–æ –Ω–µ –±—ã–ª–æ —Ç–∞–∫. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Markov Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Å—Ç–æ—è–Ω–∏–µ $S_{t}$ —è–≤–ª—è–µ—Ç—Å—è –ú–∞—Ä–∫–æ–≤—Å–∫–∏–º —Ç–æ–≥–¥–∞ –∏ —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞:\n",
    "$$ \\large\n",
    "p\\left(r_{t}, s_{t+1} \\mid s_{0}, a_{0}, r_{0}, \\ldots, s_{t}, a_{t}\\right)=p\\left(r_{t}, s_{t+1} \\mid s_{t}, a_{t}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –æ–ø–∏—Å—ã–≤–∞–µ—Ç **–ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞–±–ª—é–¥–∞–µ–º—ã–µ (fully observable)** —Å—Ä–µ–¥—ã. –ú–æ–∂–Ω–æ –æ–ø–∏—Å–∞—Ç—å –∏—Ö —Ç–∞–∫:\n",
    "\n",
    "\n",
    "- –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –±—É–¥—É—â–µ–º, –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.\n",
    "\n",
    "- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—É—é –ø–æ–ª—É—á–∞–µ—Ç –∞–≥–µ–Ω—Ç –≤ –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ t, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á—Ç–æ–±—ã –ø—Ä–∏–Ω—è—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –º–æ–º–µ–Ω—Ç–∞ –≤—Ä–µ–º–µ–Ω–∏ t.\n",
    "\n",
    "- –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã ‚Äî –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è.\n",
    "\n",
    "- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç —Ç–æ–ª—å–∫–æ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ —Ç–µ–∫—É—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–µ–¥—Å—Ç–∞–≤–∏–º —Å–µ–±–µ, —á—Ç–æ —Å—Ç—É–¥–µ–Ω—Ç –∂–∏–≤–µ—Ç –≤–æ—Ç –ø–æ —Ç–∞–∫–æ–π —Å—Ö–µ–º–µ. –ó–∞–º–µ—Ç–∏–º, —á—Ç–æ –≤–ª–∏—è—Ç—å –≤ —Ç–∞–∫–æ–π —Å—Ö–µ–º–µ –Ω–∞ —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è –æ–Ω –Ω–µ –º–æ–∂–µ—Ç: –≤—Å–µ —Ä–µ—à–∞–µ—Ç—Å—è –ø–æ–¥–∫–∏–¥—ã–≤–∞–Ω–∏–µ–º –∫—É–±–∏–∫–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/markov_process.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å (—Ü–µ–ø—å) ‚Äî —ç—Ç–æ –∫–æ—Ä—Ç–µ–∂ $(S, P)$, –≥–¥–µ\n",
    "- $S$ ‚Äî –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ (–∫–æ–Ω–µ—á–Ω—ã–µ) –∑–Ω–∞—á–µ–Ω–∏—è,\n",
    "- $P$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ (transition matrix):\n",
    "$$\\large\n",
    "P_{s s^{\\prime}}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s\\right)\n",
    "$$\n",
    "\n",
    "–°—Ç—Ä–æ–≥–æ –≥–æ–≤–æ—Ä—è, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –µ—â–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π (–Ω–æ –º—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–Ω–æ –≤—ã—Ä–æ–∂–¥–µ–Ω–æ, —Ç.–µ. –º—ã –∑–Ω–∞–µ–º, –≥–¥–µ –Ω–∞—á–∏–Ω–∞–µ–º, —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 1).\n",
    "\n",
    "–ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å ‚Äî –æ—Å–Ω–æ–≤–∞ –¥–ª—è RL. –ú—ã –±—É–¥–µ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—Å–ª–æ–∂–Ω—è—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å, –¥–æ–±–∞–≤–ª—è—è rewards –∏ actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§—É–Ω–∫—Ü–∏—è –Ω–∞–≥—Ä–∞–¥—ã (reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—à–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –æ—á–µ–≤–∏–¥–Ω–æ, –Ω–µ—Ä–∞–≤–Ω–æ—Ü–µ–Ω–Ω—ã. –î–∞–≤–∞–π—Ç–µ –∫ –∫–∞–∂–¥–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é –ø—Ä–∏–≤—è–∂–µ–º –Ω–∞–≥—Ä–∞–¥—É $R_s$. –ù–∞–≥—Ä–∞–¥—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º–∏, –Ω—É–ª–µ–≤—ã–º–∏ –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∏ –Ω–∞–∑–Ω–∞—á–∞—Ç—å—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–∫–∞—Ö, —á—Ç–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫ –∞–≥–µ–Ω—Ç —É—á–∏—Ç—Å—è –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏—è.\n",
    "\n",
    "* **–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã.** –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∞–≥–µ–Ω—Ç—É –æ –∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö –∏–ª–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π. –û–Ω–∏ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—Ç —É—Å–∏–ª–µ–Ω–∏—é —Ç–µ—Ö –¥–µ–π—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ –ø–æ–ª—É—á–µ–Ω–∏—é —ç—Ç–æ–π –Ω–∞–≥—Ä–∞–¥—ã.\n",
    "\n",
    "* **–ù—É–ª–µ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã.** –ù—É–ª–µ–≤—ã–µ –Ω–∞–≥—Ä–∞–¥—ã —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∞–≥–µ–Ω—Ç—É, —á—Ç–æ –µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –Ω–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ –∫–∞–∫–∏–º-–ª–∏–±–æ –∑–Ω–∞—á–∏–º—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º. –¢–∞–∫–∏–µ –Ω–∞–≥—Ä–∞–¥—ã —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–ª–∏ –¥–ª—è –¥–µ–π—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n",
    "\n",
    "* **–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã.** –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã –∏–ª–∏ —à—Ç—Ä–∞—Ñ—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –∞–≥–µ–Ω—Ç –∏–∑–±–µ–≥–∞–ª –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –∏–ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π. –û–Ω–∏ –ø–æ–º–æ–≥–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞, —É–∫–∞–∑—ã–≤–∞—è –Ω–∞ –æ—à–∏–±–∫–∏ –∏–ª–∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è.\n",
    "\n",
    "–ù–∞–≥—Ä–∞–¥—ã –º–æ–≥—É—Ç –Ω–∞–∑–Ω–∞—á–∞—Ç—å—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–∫–∞—Ö:\n",
    "\n",
    "\n",
    "* **–ù–∞–≥—Ä–∞–¥—ã –≤ –∫–æ–Ω—Ü–µ —ç–ø–∏–∑–æ–¥–∞.** –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞–∑–Ω–∞—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ —ç–ø–∏–∑–æ–¥–∞, –∫–æ–≥–¥–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∫–æ–Ω–µ—á–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–ª–∏ –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π.\n",
    "–ü—Ä–∏–º–µ—Ä: –í —à–∞—Ö–º–∞—Ç–∞—Ö –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞–≥—Ä–∞–¥—É —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω—Ü–µ –∏–≥—Ä—ã, –∫–æ–≥–¥–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ–±–µ–¥–∏—Ç–µ–ª—å, –ø—Ä–æ–∏–≥—Ä–∞–≤—à–∏–π –∏–ª–∏ —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∏—á—å—è. –í—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –º–æ–≥—É—Ç –Ω–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–∞—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ, –Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n",
    "\n",
    "* **–ù–∞–≥—Ä–∞–¥—ã –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—Ä–µ–º–µ–Ω–∏ (t).** –í –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞–≥—Ä–∞–¥—ã –º–æ–≥—É—Ç –Ω–∞–∑–Ω–∞—á–∞—Ç—å—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ–±—ã –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–ª –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.\n",
    "–ü—Ä–∏–º–µ—Ä: –í –∑–∞–¥–∞—á–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥—Ä–æ–Ω–æ–º –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞ —É–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≤—ã—Å–æ—Ç—ã, –∏–∑–±–µ–≥–∞–Ω–∏–µ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π. –≠—Ç–∏ –Ω–∞–≥—Ä–∞–¥—ã –ø–æ–º–æ–≥–∞—é—Ç –∞–≥–µ–Ω—Ç—É –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.\n",
    "\n",
    "–î–∏–∑–∞–π–Ω —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–∞—è –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–Ω–æ–≥–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤.\n",
    "\n",
    "\n",
    "–°—Ö–µ–º–∞ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–∞ –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º. –ó–∞–º–µ—Ç—å—Ç–µ, –Ω–∞–≥—Ä–∞–¥—ã —Ç—É—Ç —Ä–∞—Å—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∞–∫—Ç–æ—Ä–∞ ‚Äî —Å—Ç—É–¥–µ–Ω—Ç–∞:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/markov_reward.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ú–∞—Ä–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ —ç—Ç–æ–≥–æ —É –Ω–∞—Å –ø–æ–ª—É—á–∞–ª–æ—Å—å –Ω–µ —Å–æ–≤—Å–µ–º –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ ‚Äî —Å—Ç—É–¥–µ–Ω—Ç –Ω–µ –º–æ–≥ –Ω–∏ –Ω–∞ —á—Ç–æ –ø–æ–≤–ª–∏—è—Ç—å. –ù–∞ —Å–∞–º–æ–º –∂–µ –¥–µ–ª–µ —Å—Ç—É–¥–µ–Ω—Ç –º–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å, –∫—É–¥–∞ –µ–º—É –Ω–∞–¥–æ: –≤ –∞—É–¥–∏—Ç–æ—Ä–∏—é, –¥–æ–º–æ–π —Å–ø–∞—Ç—å –∏–ª–∏ –≤ –±–∞—Ä. –û–¥–Ω–∞–∫–æ –Ω–∞—à–µ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ –≤—Å–µ–≥–¥–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–µ –º—ã –ø–µ—Ä–µ–π–¥–µ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–•–æ—á–µ—Ç –Ω–∞—à —Å—Ç—É–¥–µ–Ω—Ç –ø–æ–π—Ç–∏ –Ω–∞ –ª–µ–∫—Ü–∏—é, –Ω–æ –ø–æ –ø—É—Ç–∏ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç —Ç–æ–≤–∞—Ä–∏—â–∞, –∏ –∏–¥—É—Ç –æ–Ω–∏ –≤ –±–∞—Ä. –•–æ—á–µ—Ç –æ–Ω –ø–æ–π—Ç–∏ —Å–ø–∞—Ç—å, –∞ –ø–æ –ø—É—Ç–∏ –∫ –≤—ã—Ö–æ–¥—É –∏–∑ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç –ª–µ–∫—Ç–æ—Ä–∞ —Å —Ö–æ—Ä–æ—à–µ–π –ø–∞–º—è—Ç—å—é –∏ –∏–¥–µ—Ç –Ω–∞ –ª–µ–∫—Ü–∏—é.\n",
    "\n",
    "\n",
    "–ö–∞–∫ —ç—Ç–æ –æ—Ç—Ä–∞–∑–∏—Ç—å –Ω–∞ —Å—Ö–µ–º–µ —Ç–∞–∫, —á—Ç–æ–±—ã —ç—Ç–æ –º–æ–∂–Ω–æ –±—ã–ª–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å? –í–≤–µ–¥–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∫—É–¥–∞ –Ω–∞—Å –ø–µ—Ä–µ–≤–æ–¥—è—Ç –¥–µ–π—Å—Ç–≤–∏—è —Å—Ç—É–¥–µ–Ω—Ç–∞. –ê —É–∂–µ –∏–∑ —ç—Ç–∏—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å–ª—É—á–∞–π–Ω–æ –±—É–¥–µ–º –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Ä–µ–¥—ã.\n",
    "\n",
    "–ü—Ä–æ—Ü–µ—Å—Å, –∫–æ–≥–¥–∞ –≤ —Å—Ä–µ–¥–µ –∞–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞–∑—ã–≤–∞—é—Ç Markov decision process (MDP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/markov_decision_process_return_random.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP ‚Äî —ç—Ç–æ –∫–æ—Ä—Ç–µ–∂ $(S, A, R, P, \\gamma)$, –≥–¥–µ:\n",
    "- $S$ ‚Äî —Å–æ—Å—Ç–æ—è–Ω–∏—è (–¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ),\n",
    "- $A$ ‚Äî –¥–µ–π—Å—Ç–≤–∏—è (–¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ),\n",
    "- $R$ ‚Äî —Ñ—É–Ω–∫—Ü–∏—è reward: $R_{s,s'}^{a}$ (–º–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–µ—Ö–æ–¥ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è $s$ –≤ $s'$ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –¥–µ–π—Å—Ç–≤–∏—è $a$)\n",
    "- $P$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ (transition matrix): $P_{s s^{\\prime}}^{a}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s, A_{t}=a\\right)$,\n",
    "- $\\gamma$ ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\gamma$-–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–¶–µ–ª—å –∞–≥–µ–Ω—Ç–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—é—â—É—é —Å—Ä–µ–¥–Ω—é—é —Å—É–º–º–∞—Ä–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É:\n",
    "\n",
    "\n",
    "\n",
    "$$\\text{Return} = \\sum_i {R_i}$$\n",
    "\n",
    "\n",
    "\n",
    "–°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ ‚Äî —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—É–º–º–∞ –≤—Å–µ—Ö –Ω–∞–≥—Ä–∞–¥, –∫–æ—Ç–æ—Ä—ã–µ –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç –∑–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –¥–æ –∫–æ–Ω–µ—á–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.\n",
    "\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—É–º–º–∞—Ä–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –∏–º–µ–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤:\n",
    "\n",
    "1. **–û—Ç—Å—Ä–æ—á–µ–Ω–Ω—ã–µ –Ω–∞–≥—Ä–∞–¥—ã.** –î–µ–π—Å—Ç–≤–∏—è —Å–¥–µ–ª–∞–Ω–Ω—ã–µ —Å–µ–π—á–∞—Å –≤–ª–∏—è—é—Ç –Ω–∞ —Å–æ–±—ã—Ç–∏—è –Ω–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ, –∞ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –≤–ø–µ—Ä–µ–¥. –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–∏–≤–µ–ª–æ –∫ –Ω–∞–≥—Ä–∞–¥–µ —á–µ—Ä–µ–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Å–∏–ª—å–Ω–æ —É—Å–ª–æ–∂–Ω—è–µ—Ç –∑–∞–¥–∞—á—É. –ï—Å–ª–∏ –¥–µ–π—Å–≤—Ç–∏–π –º–Ω–æ–≥–æ, —Ç–æ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –¥–µ–π—Å–≤—Ç–∏–µ —Å –Ω–∞–≥—Ä–∞–¥–æ–π –∑–∞—Ç—Ä—É–¥–Ω–∏—Ç–µ–ª—å–Ω–æ.\n",
    "\n",
    "2. **–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ —Ü–∏–∫–ª—ã.** –í –Ω–∞—à–µ–π –∑–∞–¥–∞—á–µ –º–æ–≥—É—Ç –±—ã—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ –±–æ–ª—å—à–∏–µ –Ω–∞–≥—Ä–∞–¥—ã, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã –∏—Ö —Å—Ä–∞–≤–Ω–∏—Ç—å.\n",
    "\n",
    "3. **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.** –ë–µ–∑ —É—á–µ—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞—Å–ø–µ–∫—Ç–∞, –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –ø–µ—Ä–µ–æ—Ü–µ–Ω–∏–≤–∞—Ç—å –±—É–¥—É—â–∏–µ –Ω–∞–≥—Ä–∞–¥—ã, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É –∏ –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—ç—Ç–æ–º—É –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —à–∞–≥–µ $t$ $(G_t)$ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è $\\gamma$, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ü–µ–Ω–Ω—ã–º–∏ —è–≤–ª—è—é—Ç—Å—è –±—É–¥—É—â–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –Ω–∞ —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç (—Å–º. [Markov Decision Processes (David Silver Lectures) üìö[book]](https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf)):\n",
    "\n",
    "$$\\large G_t = R_{t+1} + \\gamma R_{t+2} + ... =\\sum^{\\infty}_{k=0} \\gamma ^ kR_{t+k+1},\n",
    "$$\n",
    "\n",
    "–≥–¥–µ:\n",
    "- $R_{t+1}, R_{t+1}, \\dots$ ‚Äî –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø–æ —Ä—ë–±—Ä–∞–º –ø–µ—Ä–µ—Ö–æ–¥–æ–≤,\n",
    "- –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ $\\gamma \\in[0,1]$ ‚Äî —ç—Ç–æ —Ç–µ–∫—É—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –±—É–¥—É—â–∏—Ö –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–π,\n",
    "- $\\gamma^{k} R$ ‚Äî —Ü–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞–≥—Ä–∞–¥—ã $R$ –ø–æ—Å–ª–µ $k+1$ —à–∞–≥–æ–≤.\n",
    "\n",
    "–ü—Ä–∏ —ç—Ç–æ–º:\n",
    "- –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ —Ü–µ–Ω–∏—Ç—Å—è –≤—ã—à–µ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è,\n",
    "- $\\gamma$ –±–ª–∏–∑–∫–æ –∫ 0 –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ ¬´–±–ª–∏–∑–æ—Ä—É–∫–æ—Å—Ç–∏¬ª,\n",
    "- $\\gamma$ –±–ª–∏–∑–∫–æ –∫ 1 –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ ¬´–¥–∞–ª—å–Ω–æ–≤–∏–¥–Ω–æ–π¬ª –æ—Ü–µ–Ω–∫–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **–°–Ω–∏–∂–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –±—É–¥—É—â–µ–≥–æ:**\n",
    "–í —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –±—É–¥—É—â–µ–µ —á–∞—Å—Ç–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ. –î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–º–µ–Ω—å—à–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ —ç—Ç–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏, —Ç–∞–∫ –∫–∞–∫ –¥–µ–ª–∞–µ—Ç –¥–∞–ª—å–Ω–µ–π—à–∏–µ –Ω–∞–≥—Ä–∞–¥—ã –º–µ–Ω–µ–µ –∑–Ω–∞—á–∏–º—ã–º–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏—è.\n",
    "\n",
    "* **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:**\n",
    "–î–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ —É–º–µ–Ω—å—à–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∏ —á–∞—Å—Ç–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥ –Ω–∞ —Ç–µ–∫—É—â–∏–µ —Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–≥–µ–Ω—Ç–∞.\n",
    "\n",
    "* **–ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–º–∏ –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏:**\n",
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç—É –Ω–∞—Ö–æ–¥–∏—Ç—å –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–º–∏ –≤—ã–≥–æ–¥–∞–º–∏ –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏, —á—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –±–æ–ª–µ–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é.\n",
    "\n",
    "* **–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –Ω–∞–≥—Ä–∞–¥:**\n",
    "–í —Å—Ä–µ–¥–∞—Ö, –≥–¥–µ –Ω–∞–≥—Ä–∞–¥—ã –º–æ–≥—É—Ç –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å—Å—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ, –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–∏—Ç—É–∞—Ü–∏—é, –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç —Å—Ç—Ä–µ–º–∏—Ç—Å—è –∫ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—é –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –±—É–¥—É—â–∏—Ö –Ω–∞–≥—Ä–∞–¥.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–ö–∞–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ $\\gamma$ –≤—ã–±—Ä–∞—Ç—å?\n",
    "\n",
    "\n",
    "**–ú–æ–∂–Ω–æ –ª–∏ –µ–≥–æ –≤—ã–±—Ä–∞—Ç—å —Ä–∞–≤–Ω—ã–º 1?**\n",
    "\n",
    "–î–∞, –ø–æ–ª—É—á–∏–º —Å—É–º–º–∞—Ä–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É\n",
    "\n",
    "**–ú–æ–∂–Ω–æ –ª–∏ –µ–≥–æ –≤—ã–±—Ä–∞—Ç—å —Ä–∞–≤–Ω—ã–º 0?**\n",
    "\n",
    " –¢–æ–∂–µ –¥–∞. –í —ç—Ç–æ–º —Å–ª—É—á–∞–µ —É –Ω–∞—Å –ø–æ–ª—É—á–∏—Ç—Å—è \"–∂–∞–¥–Ω—ã–π\" –∞–ª–≥–æ—Ä–∏—Ç–º: –º—ã –≤—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –¥–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É —Å–µ–π—á–∞—Å, –Ω–∞—Å –Ω–µ –≤–æ–ª–Ω—É—é—Ç –±—É–¥—É—â–∏–µ –Ω–∞–≥—Ä–∞–¥—ã.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–û–±—ã—á–Ω–æ $0 \\le \\gamma \\le 1$**\n",
    "\n",
    "–û–±—ã—á–Ω–æ $\\gamma$ —Å—Ç–∞–≤—è—Ç —Ä–∞–≤–Ω–æ–π —á–µ–º—É-—Ç–æ –º–µ–∂–¥—É –¥–≤—É–º—è —ç—Ç–∏–º–∏ –∫—Ä–∞–π–Ω–æ—Å—Ç—è–º–∏.\n",
    "\n",
    "–ë–ª–∏–∑–æ—Å—Ç—å $\\gamma$ –∫ 0 –æ—Ç—Ä–∞–∂–∞–µ—Ç –Ω–∞—à—É \"–Ω–µ—Ç–µ—Ä–ø–µ–ª–∏–≤–æ—Å—Ç—å\" ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–≥—Ä–∞–¥—É –∏–º–µ–Ω–Ω–æ —Å–µ–π—á–∞—Å.\n",
    "\n",
    "–ù–∞–ª–∏—á–∏–µ —Ç–∞–∫–æ–≥–æ $\\gamma$ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –Ω–µ –¥–µ–ª–∞—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º —á–∏—Å–ª–æ–º —à–∞–≥–æ–≤ –∏ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º: —Ç–µ–ø–µ—Ä—å –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ return –±—É–¥–µ—Ç –∫–æ–Ω–µ—á–Ω—ã–º —á–∏—Å–ª–æ–º, —Ç.–∫. –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è return –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ —Å–≤–µ—Ä—Ö—É —Å—É–º–º–æ–π –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ —É–±—ã–≤–∞—é—â–µ–π –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–∏.\n",
    "\n",
    "–ü—É—Å—Ç—å $R_{\\max} = \\max R_i$ ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞, –∫–æ—Ç–æ—Ä—É—é –º—ã –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –≤ –∫–∞–∫–æ–º-—Ç–æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏.\n",
    "\n",
    "$\\displaystyle G_t = R_{t+1} + \\gamma \\cdot R_{t+2} + \\gamma^2 \\cdot R_{t+3} + ... \\le R_{\\max} + \\gamma \\cdot R_{\\max} + \\gamma^2 \\cdot R_{\\max} + ... = R_{\\max} (1 + \\gamma + \\gamma^2 + ... ) \\le R_{\\max} \\cdot \\frac{1}{1-\\gamma} = const $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discounting makes sums finite**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/discounting_makes_sums_finite.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G_{0}=\\sum_{k=0}^{\\infty} \\gamma^{k}=\\frac{1}{1-\\gamma}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –≤—ã–±–æ—Ä $\\gamma$ –º–µ–Ω—è–µ—Ç –∑–∞–¥–∞—á—É, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ä–µ—à–∞–µ–º, –∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –º–µ–Ω—è–µ—Ç —Ä–µ—à–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –Ω–∞—à MDP –≤ –≤–∏–¥–µ —Å—Ä–µ–¥—ã:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# After this setup we can import mdp package\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
    "!touch .setup_complete\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import MDP\n",
    "\n",
    "transition_probs = {\n",
    "    \"s0\": {\"a0\": {\"s1\": 1}},\n",
    "    \"s1\": {\"a0\": {\"s4\": 1}, \"a1\": {\"s2\": 0.6, \"s4\": 0.2, \"s3\": 0.2}, \"a2\": {\"s2\": 0.1, \"s3\": 0.9}},\n",
    "    \"s2\": {\"a0\": {\"s4\": 0.2, \"s3\":0.8}},\n",
    "    \"s3\": {\"a0\": {\"s4\": 1}},\n",
    "    \"s4\": {\"a0\": {\"s4\": 1}}\n",
    "}\n",
    "rewards = {\"s0\": {\"a0\": {\"s1\": -2}},\n",
    "           \"s1\": {\"a0\": {\"s4\": 0}, \"a1\": {\"s2\": -4, \"s4\": 0, \"s3\": 10}, \"a2\": {\"s2\":-4, \"s3\":10}},\n",
    "           \"s2\": {\"a0\": {\"s4\": 0, \"s3\": 10}}\n",
    "           }\n",
    "\n",
    "mdp = MDP(transition_probs, rewards, initial_state=\"s0\")\n",
    "\n",
    "# We can now use MDP just as any other gym environment:\n",
    "print(\"initial state =\", mdp.reset())\n",
    "next_state, reward, done, info = mdp.step(\"a0\")\n",
    "print(\"next_state = %s, reward = %s, done = %s\" % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP methods\n",
    "\n",
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions(\"s1\"))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states(\"s1\", \"a0\"))\n",
    "\n",
    "# state, action, next_state\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward(\"s1\", \"a0\", \"s0\"))\n",
    "\n",
    "# get_transition_prob(self, state, action, next_state)\n",
    "print(\n",
    "    \"mdp.get_transition_prob('s1', 'a1', 's2') = \",\n",
    "    mdp.get_transition_prob(\"s1\", \"a1\", \"s2\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_jpeg\n",
    "import mdp as MDP\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "display_jpeg(MDP.plot_graph(mdp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–æ–±–ª–µ–º–∞ –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä–æ–π –±—É–¥–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ ($G_t$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ª–∏—Ç–∏–∫–∞ (policy) $\\pi$ ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è $s$ –¥–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ –¥–µ–π—Å—Ç–≤–∏–π $A$.\n",
    "\n",
    "$$ \\large\n",
    "\\pi(a|s)=\\mathbb{P}\\left[A_{t}=a \\mid S_{t}=s\\right]\n",
    "$$\n",
    "- –ü–æ–ª–∏—Ç–∏–∫–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞.\n",
    "- –ü–æ–ª–∏—Ç–∏–∫–∏ MDP –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Ä–µ–¥—ã (–∞ –Ω–µ –æ—Ç –ø—Ä–æ—à–ª—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π).\n",
    "- –¢. –µ. –ø–æ–ª–∏—Ç–∏–∫–∏ —è–≤–ª—è—é—Ç—Å—è —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º–∏ (–Ω–µ –∑–∞–≤–∏—Å—è—â–∏–º–∏ –æ—Ç –≤—Ä–µ–º–µ–Ω–∏):\n",
    "\n",
    "$$ \\large\n",
    "A_{t} \\sim \\pi\\left(\\cdot \\mid S_{t}\\right), \\forall t>0\n",
    "$$\n",
    "\n",
    "–î–ª—è –Ω–∞—à–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ —Å–æ —Å—Ç—É–¥–µ–Ω—Ç–æ–º:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/markov_policy_example.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-value function $v_{\\pi}(s)$ (V-—Ñ—É–Ω–∫—Ü–∏—è)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $v_\\pi(s)$ ‚Äî –∏–∑–º–µ—Ä—è–µ—Ç —Ü–µ–Ω–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∞ –∏–º–µ–Ω–Ω–æ –∫–∞–∫–æ–µ –æ–∂–∏–¥–∞–µ–º–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å, –µ—Å–ª–∏ –Ω–∞—á–∞—Ç—å –¥–≤–∏–≥–∞—Ç—å—Å—è –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è $s$ –≤ —Ç–µ—á–µ–Ω–∏–µ –≤—Å–µ–≥–æ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏, –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—è—Å—å –ø–æ–ª–∏—Ç–∏–∫–∏ $\\pi$.\n",
    "\n",
    "–¢–∞–∫, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç —Å–ª–µ–¥–æ–≤–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫–µ, –∑–∞–∫–ª—é—á–∞—é—â–µ–π—Å—è –≤ –ø–µ—Ä–µ—Ö–æ–¥–µ –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏ —Å –≤—ã—Å–æ–∫–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç—å—é. –û–¥–Ω–∞–∫–æ —Ñ—É–Ω–∫—Ü–∏—è —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —É–¥–∞–ª–µ–Ω–∞ –æ—Ç –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–º, –ø–æ—Å–∫–æ–ª—å–∫—É –∞–≥–µ–Ω—Ç –∑–∞—á–∞—Å—Ç—É—é –Ω–µ –º–æ–∂–µ—Ç —Å–∞–º –≤—ã–±—Ä–∞—Ç—å —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–µ –ø–µ—Ä–µ–π–¥–µ—Ç, –ø–æ—Å–∫–æ–ª—å–∫—É –Ω–∞ –Ω–µ–≥–æ –±—É–¥–µ—Ç –æ–∫–∞–∑—ã–≤–∞—Ç—å—Å—è –≤–ª–∏—è–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –±—Ä–æ—Å–∫–∏ –∫—É–±–∏–∫–æ–≤) –∏–ª–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å–∏–ª (—Ö–æ–¥ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞). –ü–æ—ç—Ç–æ–º—É –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Ü–µ–Ω–Ω–æ—Å—Ç—å –Ω–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π, –∞ –¥–µ–π—Å—Ç–≤–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/chess.png\" alt=\"Drawing\" width=\"400\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–§–æ—Ä–º–∞–ª—å–Ω–æ **V-—Ñ—É–Ω–∫—Ü–∏—è** $v_\\pi(s)$ **–≤–≤–æ–¥–∏—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è** (–ø–æ –ø–æ–ª–∏—Ç–∏–∫–µ $\\pi$ –∏ –ø–æ MDP —Å—Ä–µ–¥—ã $P_{s s^{\\prime}}^{a}$) –æ—Ç –±—É–¥—É—â–µ–π –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã $G_t$, –≤ —Å–ª—É—á–∞–µ –µ—Å–ª–∏ –º—ã –Ω–∞—á–∏–Ω–∞–µ–º —Å–≤–æ–∏ –¥–µ–π—Å—Ç–≤–∏—è –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å—Ä–µ–¥—ã $s$ –∏ –±—É–¥–µ–º –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–≥–æ —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—à–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ $\\pi$:\n",
    "\n",
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right]\n",
    "\\end{aligned}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action-value function $q_{\\pi}(s, a)$ (Q-—Ñ—É–Ω–∫—Ü–∏—è)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $q_\\pi(s,a)$ ‚Äî –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –Ω–æ –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ $s (s_0 = s)$ –±—ã–ª–æ –≤—ã–±—Ä–∞–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ $a$ (–º—ã —Ñ–∏–∫—Å–∏—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ). –¢–æ –µ—Å—Ç—å –≤ $q$ –º—ã, –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç $v$, —Ñ–∏–∫—Å–∏—Ä—É–µ–º –Ω–∞—à–µ –ø–µ—Ä–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ –ø–æ–ª–∏—Ç–∏–∫–µ), –∞ –≤ $v$ –º—ã –ø–µ—Ä–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤—ã–±–∏—Ä–∞–µ–º —Å–æ–≥–ª–∞—Å–Ω–æ –ø–æ–ª–∏—Ç–∏–∫–µ $\\pi$.\n",
    "\n",
    "–§–æ—Ä–º–∞–ª—å–Ω–æ Q-—Ñ—É–Ω–∫—Ü–∏—è –≤–≤–æ–¥–∏—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç –±—É–¥—É—â–µ–π –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã $G_t$, –≤ —Å–ª—É—á–∞–µ –µ—Å–ª–∏ –º—ã –Ω–∞—Ö–æ–¥—è—Å—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ $s$ **–≤—ã–±—Ä–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ $a$**, –∞ –≤—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å–æ–≤–µ—Ä—à–∞–ª–∏ —Å—Ç—Ä–æ–≥–æ —Å–æ–≥–ª–∞—Å–Ω–æ –ø–æ–ª–∏—Ç–∏–∫–µ $\\pi$:\n",
    "\n",
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s, a) &=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s \\mid A_{t}=a\\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "–°—Ä–∞–∑—É –æ—Ç–º–µ—Ç–∏–º, —á—Ç–æ Q –∏ V —Ñ—É–Ω–∫—Ü–∏–∏, –æ—á–µ–≤–∏–¥–Ω–æ, —Å–≤—è–∑–∞–Ω—ã. –ï—Å–ª–∏ –º—ã –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –Ω–∞—à–µ–π –ø–æ–ª–∏—Ç–∏–∫–æ–π $\\pi$ –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è $a$, —Ç–æ —Å—Ä–∞–∑—É –∂–µ –ø—Ä–∏–¥—ë–º –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é V-—Ñ—É–Ω–∫—Ü–∏–∏ (—Ç–æ –µ—Å—Ç—å —É—Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏—è Q-—Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—ë—Ç –ø–æ–ª–∏—Ç–∏–∫–∞):\n",
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\sum_a \\pi(a|s) q_{\\pi}(s, a) &=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s \\right]\\\\\n",
    "&=v_{\\pi}(s).\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "—Ç–æ –µ—Å—Ç—å V-—Ñ—É–Ω–∫—Ü–∏—é –ª–µ–≥–∫–æ –∑–∞–ø–∏—Å–∞—Ç—å —á–µ—Ä–µ–∑ Q —Ñ—É–Ω–∫—Ü–∏—é –∏ –ø–æ–ª–∏—Ç–∏–∫—É $\\pi$. –û–±—Ä–∞—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ, –ø–æ–∑–≤–æ–ª—è—é—â–µ–µ –≤—ã—Ä–∞–∑–∏—Ç—å Q —Ñ—É–Ω–∫—Ü–∏—é —á–µ—Ä–µ–∑ V —Ñ—É–Ω–∫—Ü–∏—é –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–∏–∂–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[–ë–µ–ª–ª–º–∞–Ω üìö[wiki]](https://en.wikipedia.org/wiki/Richard_E._Bellman) –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –∑–∞–¥–∞—á–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∞ –≤ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–π –ø–æ—à–∞–≥–æ–≤–æ–π —Ñ–æ—Ä–º–µ, –ø—É—Ç–µ–º –∑–∞–ø–∏—Å–∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É **—Ñ—É–Ω–∫—Ü–∏–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—É—â–∏–π –ø–µ—Ä–∏–æ–¥ –∏ —Ñ—É–Ω–∫—Ü–∏–µ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º –ø–µ—Ä–∏–æ–¥–µ**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma\\left(R_{t+2}+\\gamma R_{t+3}+\\ldots\\right) \\mid S_{t}=s\\right] \\\\\n",
    "&=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right]\\\\\n",
    "&=\\mathbb{E}_{\\pi}\\left[R_{t+1}\\right]+\\mathbb{E}_{\\pi}\\left[\\gamma G_{t+1} \\mid S_{t}=s\\right],\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "–≥–¥–µ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–∞–≥–∞–µ–º–æ–µ –º–æ–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é V-—Ñ—É–Ω–∫—Ü–∏–∏, —É—á–∏—Ç—ã–≤–∞—è —á—Ç–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–π—Ç–∏ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è $s$ –≤ –≤–æ–∑–º–æ–∂–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ $s'$ –≤ –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ $t$ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ–º –ø–æ–ª–∏—Ç–∏–∫–∏ $\\pi(a|s)$ –∏ MDP —Å—Ä–µ–¥–æ–π $P_{s s^{\\prime}}^{a}$:\n",
    "$$\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{\\pi}\\left[\\gamma G_{t+1} \\mid S_{t}=s\\right] = \\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}P_{s s^{\\prime}}^{a}\\left[{\\mathbb{E}_{\\pi}\\left[\\gamma G_{t+1} \\mid S_{t+1}=s'\\right]}\\right] = \\\\\\gamma\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}P_{s s^{\\prime}}^{a}\\left[v_{\\pi}(s')\\right],\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "—á—Ç–æ —Ç–µ–ø–µ—Ä—å –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–ø–∏—Å–∞—Ç—å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞:\n",
    "$$\\Large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[R_{t+1}\\right] + \\gamma\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}P_{s s^{\\prime}}^{a}\\left[v_{\\pi}(s')\\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "***–£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ë–µ–ª–ª–º–∞–Ω–∞ - —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ, –ø–æ–∑–≤–æ–ª—è—é—â–µ–µ —Å–≤—è–∑–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ V-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –¥–≤—É—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π $s$ –∏ $s'$, –µ—Å–ª–∏ –Ω–∞–º –∏–∑–≤–µ—Å—Ç–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ $\\pi$ –∏ –º–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ MDP $P$.*** –î–∞–Ω–Ω–æ–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö RL-–∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.\n",
    "\n",
    "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≤—ã—à–µ –≤—ã–≤–æ–¥ –º–æ–∂–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –¥–ª—è Q-—Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∑–∞–ø–∏—Å–∞—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ:\n",
    "$$\\Large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left[R_{t+1}\\right] + \\gamma\\sum_{a}\\pi(a|s)\\sum_{s^{\\prime}}P_{s s^{\\prime}}^{a}\\left[\\sum_{a'}\\pi(a'|s')q_{\\pi}(s', a')\\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –≤ –≤–∏–¥–µ [—Ä–µ–∑–µ—Ä–≤–Ω–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã üéì[article]](https://openreview.net/pdf/45363cfb2c1b1123fc49b52916f8f3a451e09bbd.pdf) (_backup diagram_):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/bellman_backup_diagram.png\" width=\"600\"/></center>\n",
    "\n",
    "<center></em>Backup Diagram for State Value $v_\\pi(s)$ and Action Value $q_\\pi(s,a)$ with all components</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mathbf {s} $ ‚Äî **–Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ**, –∏–∑ –Ω–µ–≥–æ –∏—Å—Ö–æ–¥—è—Ç —Å—Ç—Ä–µ–ª–∫–∏, –æ—Ç–æ–±—Ä–∞–∂–∞—é—â–∏–µ **–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å** —Å–æ–≤–µ—Ä—à–∏—Ç—å —Ç–æ –∏–ª–∏ –∏–Ω–æ–µ **–¥–µ–π—Å—Ç–≤–∏–µ** –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å **–ø–æ–ª–∏—Ç–∏–∫–æ–π** $ \\mathbf{\\pi} $, **–¥–µ–π—Å—Ç–≤–∏—è** $\\mathbf{a}$ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç—Å—è –ø–µ—Ä–µ—Ö–æ–¥–æ–º –∏–∑ –∫—Ä—É–∂–∫–∞ –≤ —á—ë—Ä–Ω—É—é —Ç–æ—á–∫—É, —á–µ—Ä–Ω—ã–µ —Ç–æ—á–∫–∏ –æ–±–æ–∑–Ω–∞—á–∞—é—Ç $\\mathbf{q} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ —Ä–∞—Å–∫—Ä—ã—Ç—å –∑–Ω–∞–∫ –º–∞—Ç–æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –Ω–∞–≥—Ä–∞–¥—ã $R_t$ –ø–æ–ª—É—á–∏–º –∑–∞–ø–∏—Å—å —Å–∏—Å—Ç–µ–º—ã —É—Ä–∞–≤–Ω–µ–Ω–∏–π –Ω–∞–∏–±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –≤ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ:\n",
    "\n",
    "$$\n",
    "\\large v_{\\pi}(s) = \\sum_a\\pi(a|s)\\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma\\mathbb{E}_{\\pi}\\left[G_{t+1}\\mid S_{t+1}=s'\\right]) = \\sum_a\\pi(a|s)\\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma v_{\\pi}(s')).\n",
    "$$\n",
    "\n",
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏—è:\n",
    "\n",
    "$$\n",
    "\\large q_{\\pi}(s, a) = \\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma\\sum_{a'}\\pi(a'|s') q_{\\pi}(s', a')) = \\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma \\large v_{\\pi}(s')).\n",
    "$$\n",
    "\n",
    "–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –æ–ø–∏—Å–∞–Ω–Ω—ã–µ –≤—ã—à–µ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ —Ç–æ–º —á–∏—Å–ª–µ –∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ $\\pi$, –∫ –ø–æ–∏—Å–∫—É –∫–æ—Ç–æ—Ä–æ–≥–æ –º—ã –∏ —Å—Ç—Ä–µ–º–∏–º—Å—è. –î–ª—è —ç—Ç–æ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –µ—â—ë –æ–¥–Ω–æ–π –∏–¥–µ–µ–π, –∫–æ—Ç–æ—Ä–∞—è –ª–µ–∂–∏—Ç –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ V –∏ Q —Ñ—É–Ω–∫—Ü–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–∏–Ω—Ü–∏–ø –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç–∏ –ë–µ–ª–ª–º–∞–Ω–∞**\n",
    "\n",
    "–ë–µ–ª–ª–º–∞–Ω–æ–º –±—ã–ª —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏–Ω—Ü–∏–ø, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –≤—ã–ø–∏—Å–∞–Ω–Ω—ã—Ö –≤—ã—à–µ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –≤ MDP.\n",
    "\n",
    "*–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –æ–±–ª–∞–¥–∞–µ—Ç —Ç–µ–º —Å–≤–æ–π—Å—Ç–≤–æ–º, —á—Ç–æ –∫–∞–∫–∏–º–∏ –±—ã –Ω–∏ –±—ã–ª–∏ –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏ –Ω–∞—á–∞–ª—å–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏–µ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —Å–æ–±–æ–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–µ—Ä–≤–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.*\n",
    " (Bellman, 1957, Chap. III.3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/backup_diagram.png\" alt=\"Drawing\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. –¶–µ–Ω–Ω–æ—Å—Ç—å (Value) $v$ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ $s$ ‚Äî —ç—Ç–æ $v_{\\pi}(s)$\n",
    "\n",
    "2. –ò–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è $s$ –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å 3 –¥–µ–π—Å—Ç–≤–∏—è: $(a_1, a_2, a_3)$\n",
    "\n",
    "3. –¶–µ–Ω–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—è (Action Value) ‚Äî —ç—Ç–æ $q_{\\pi}(s,a)$, –≥–¥–µ $a = \\{a_1, a_2, a_3\\}$\n",
    "\n",
    "4. –ê–≥–µ–Ω—Ç –ø—Ä–µ–¥–ø—Ä–∏–Ω—è–ª –¥–µ–π—Å—Ç–≤–∏–µ $a_3$. –ê —Ç–∞–∫ –æ–Ω –º–æ–∂–µ—Ç –ø–µ—Ä–µ–π—Ç–∏ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏—è $s‚Äô_1$, $s‚Äô_2$ –∏–ª–∏ $s‚Äô_3$ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Ö–æ–¥–∞ p1, p2 –∏–ª–∏ p3 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ\n",
    "\n",
    "5. –ü–æ–ª—É—á–µ–Ω–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ ‚Äî $r_1$, $r_2$ –∏–ª–∏ $r_3$ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–Ω—è—Ç—å —ç—Ç–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä–æ–º–æ–∑–¥–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–∂–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –∑–∞–¥–∞–≤ —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:\n",
    "\n",
    "1. **–ß—Ç–æ –≤–æ–æ–±—â–µ –º–æ–∂–Ω–æ –Ω–∞–∑–≤–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–æ–π?**\n",
    "\n",
    "–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è –ª—é–±–æ–≥–æ —É–∑–ª–∞ –≥—Ä–∞—Ñ–∞ MDP, –æ—á–µ–≤–∏–¥–Ω–æ, –±—É–¥–µ—Ç **—Ç–∞–∫–æ–π –≤—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π $a$, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –º–∞—Ç–æ–∂–∏–¥–∞–Ω–∏–µ –±—É–¥—É—â–µ–π –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –±—É–¥–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ**. –ë—É–¥—É—â—É—é –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –Ω–∞–≥—Ä–∞–¥—É –∏–∑–º–µ—Ä—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—è V. –¢–æ –µ—Å—Ç—å $\\pi^*$ - –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞, –µ—Å–ª–∏ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –ë–µ–ª–ª–º–∞–Ω–∞ V-—Ñ—É–Ω–∫—Ü–∏–π $v_{\\pi^*}$ —Ç–∞–∫–∞—è, —á—Ç–æ –¥–ª—è –ª—é–±–æ–≥–æ $s$ –æ–Ω–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ($v_{\\pi^{*}}(s) \\rightarrow max$)\n",
    "\n",
    "2. **–ö–∞–∫ –º—ã –º–æ–∂–µ–º –æ—Ü–µ–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ V-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏?**\n",
    "\n",
    "–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å **—Ü–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è** $ \\mathbf{s}$ **–Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π** $ \\mathbf{s'}$, –≤ –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–π—Ç–∏, –º—ã –º–æ–∂–µ–º —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ü–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –¥–µ–π—Å—Ç–≤–∏–π $\\mathbf{a}$, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º —Å–æ–≤–µ—Ä—à–∏—Ç—å, –Ω–∞—Ö–æ–¥—è—Å—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ $\\mathbf{s}$. –ü–æ–ª—É—á–∞–µ–º —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ –¥—Ä—É–≥—É—é:\n",
    "\n",
    "$$\\large v_{\\pi}(s) = \\sum_a\\pi(a|s)q_{\\pi}(s,a).$$\n",
    "\n",
    "3. **–ö–∞–∫ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ü–µ–Ω–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è $s$ –≤ MDP?**\n",
    "\n",
    "–î–ª—è —ç—Ç–æ–≥–æ –º—ã –≤–≤–µ–ª–∏ Q-—Ñ—É–Ω–∫—Ü–∏—é. –í —Å–ª—É—á–∞–µ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ $q_{\\pi^{*}}$ –ª—É—á—à–∏–º —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –º–∞—Ç–æ–∂–∏–¥–∞–Ω–∏—è –Ω–∞–≥—Ä–∞–¥—ã –±—É–¥–µ—Ç –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è $\\text{argmax}_a q_{\\pi^{*}}(s,a)$. –û—Ç—Å—é–¥–∞ —Å—Ä–∞–∑—É —Å–ª–µ–¥—É–µ—Ç, —á—Ç–æ –≤ —Å–ª—É—á–∞–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∏–≥—Ä—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ V —Ñ—É–Ω–∫—Ü–∏–∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º –∑–Ω–∞—á–µ–Ω–∏–∏ Q —Ñ—É–Ω–∫—Ü–∏–∏:\n",
    "$$\\large v_{\\pi^*}(s) = \\max_{a(s)}q_{\\pi^*}(s,a) = \\max_{a(s)}\\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma v_{\\pi^*}(s')).$$\n",
    "\n",
    "\n",
    "**–û—Ç—Å—é–¥–∞ —Å—Ä–∞–∑—É —Å–ª–µ–¥—É–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ë–µ–ª–ª–º–∞–Ω-–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏** $\\pi^*$:\n",
    "$$\\large \\pi^*(s) = \\text{argmax}_aq_*(s,a)$$\n",
    "\n",
    "*–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: $\\text{argmax}$ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ $\\text{softmax}$, –µ—Å–ª–∏ –º–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ MDP —è–≤–Ω–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç $a$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –ë–µ–ª–ª–º–∞–Ω–∞ –¥–ª—è MDP (—Ä–µ—à–µ–Ω–∏–µ \"MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ú–µ—Ç–æ–¥—ã –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ–∑—å–º—ë–º —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ –≤—ã—à–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è –ë–µ–ª–ª–º–∞–Ω–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏–º —Å –∏—Ö –ø–æ–º–æ—â—å—é  –æ–ø–µ—Ä–∞—Ç–æ—Ä $\\mathcal{T}$ –ø–æ –µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—é –Ω–∞ V –∏ Q —Ñ—É–Ω–∫—Ü–∏–∏:\n",
    "\n",
    "$$\\large \\mathcal{T}[\\color{red}{v}](s) = \\sum_a\\pi(a|s)\\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma \\color{red}{v(s')}),$$\n",
    "$$\\large \\mathcal{T}[\\color{red}{q}](s,a) = \\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma\\sum_{a'}\\pi(a'|s') \\color{red}{q(s', a')}).$$\n",
    "\n",
    "–ö–∞–∫ –ø–æ–¥—Ä–æ–±–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ [[book] üìö Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) —Ç–∞–∫–æ–π –æ–ø–µ—Ä–∞—Ç–æ—Ä $\\mathcal{T}$ —è–≤–ª—è–µ—Ç—Å—è *—Å–∂–∏–º–∞—é—â–∏–º*. –¢–æ –µ—Å—Ç—å –µ—Å–ª–∏ –≤–∑—è—Ç—å –ª—é–±—É—é –ø–∞—Ä—É —Ñ—É–Ω–∫—Ü–∏–π $V_1$ –∏ $V_2$, –Ω–∞–π–¥—ë—Ç—Å—è —Ç–∞–∫–æ–µ –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ $C$, —á—Ç–æ:\n",
    "\n",
    "$$\\large || \\mathcal{T}[V_1] - \\mathcal{T}[V_2]|| \\leq C||V_1 - V_2||,$$\n",
    "—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ $\\mathcal{T}$ —Ñ—É–Ω–∫—Ü–∏–∏ $V_1$ –∏ $V_2$ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –±–ª–∏–∂–µ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É.\n",
    "\n",
    "[[wiki] üìö –ü–æ —Ç–µ–æ—Ä–µ–º–µ –æ –Ω–µ–ø–æ–¥–≤–∏–∂–Ω–æ–π —Ç–æ—á–∫–µ](https://en.wikipedia.org/wiki/Banach_fixed-point_theorem) —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∫–∞–∫ –º–∏–Ω–∏–º—É–º –æ–¥–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç—Å—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º $\\mathcal{T}$. –¢–∞–∫ –∫–∞–∫ –º—ã –ø–æ—Å—Ç—Ä–æ–∏–ª–∏ –Ω–∞—à –æ–ø–µ—Ä–∞—Ç–æ—Ä –∫–∞–∫ –ø—Ä–∞–≤—É—é —á–∞—Å—Ç—å —É—Ä–∞–≤–Ω–µ–Ω–∏—è –ë–µ–ª–ª–º–∞–Ω–∞, —Ç–æ —ç—Ç–∞ –Ω–µ–ø–æ–¥–≤–∏–∂–Ω–∞—è —Ç–æ—á–∫–∞ - –∏—Å–∫–æ–º–æ–µ –Ω–∞–º–∏ —Ä–µ—à–µ–Ω–∏–µ. –û—Ç—Å—é–¥–∞ –ø–æ—è–≤–ª—è–µ—Ç—Å—è –∏–¥–µ—è –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π Q –∏–ª–∏ V:\n",
    "\n",
    "1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—É—é $V_0$.\n",
    "2. –î–µ–π—Å—Ç–≤—É–µ–º –Ω–∞ –Ω–µ—ë –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º $\\mathcal{T}[V_0] = V_1$.\n",
    "3. –ü–æ–≤—Ç–æ—Ä—è–µ–º $\\mathcal{T}[V_k] = V_{k+1}$ –ø–æ–∫–∞ –Ω–µ  $||V_{k} - V_{k+1}|| \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –¥–≤–∞ —ç—Ç–∞–ø–∞:\n",
    "\n",
    "1. **Policy evaluation**\n",
    "\n",
    "–§–∏–∫—Å–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É $\\pi$ –∏ V-—Ñ—É–Ω–∫—Ü–∏—é $V_0$. –ü—Ä–æ–≤–µ–¥—ë–º –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º $\\large \\mathcal{T}[\\color{red}{v}](s)$, –ø–æ–∫–∞ –Ω–µ —Å–æ–π–¥—ë–º—Å—è –∫ –Ω–µ–ø–æ–¥–≤–∏–∂–Ω–æ–π —Ç–æ—á–∫–µ.\n",
    "\n",
    "2. **Policy improvement**\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –º—ã –∑–Ω–∞–µ–º –ø—Ä–∞–≤–∏–ª–æ $v$ –¥–ª—è –Ω–∞—à–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏. –ö–∞–∫ –Ω–∞–º –µ–≥–æ —É–ª—É—á—à–∏—Ç—å?\n",
    "–ë—É–¥–µ–º –≤ –∫–∞–∂–¥–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –º–µ–Ω—è—Ç—å –Ω–∞—à—É –ø–æ–ª–∏—Ç–∏–∫—É —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –º—ã —à–ª–∏ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å –ª—É—á—à–∏–º $q$. –ú—ã –º–æ–≥–ª–∏ –±—ã —É–ª—É—á—à–∏—Ç—å —ç—Ç–æ, –¥–µ–π—Å—Ç–≤—É—è –∂–∞–¥–Ω–æ $\\mathrm{q}(\\mathrm{s}, \\mathrm{a}) !$\n",
    "$$\n",
    "\\large\n",
    "\\pi^{\\prime}(s) \\leftarrow \\underset{a}{\\arg \\max } \\overbrace{\\sum_{r, s^{\\prime}} P_{ss'}^a\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right]}^{q_{\\pi}(s, a)}\n",
    "$$\n",
    "\n",
    "–≠—Ç–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ —É–ª—É—á—à–µ–Ω–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏!\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&\\text { –µ—Å–ª–∏ } \\quad q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) \\geq v_{\\pi}(s) \\quad \\text { –¥–ª—è –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π, } \\\\\n",
    "&\\text { —Ç–æ–≥–¥–∞ } \\quad v_{\\pi^{\\prime}}(s) \\geq v_{\\pi}(s) \\text { –æ–∑–Ω–∞—á–∞–µ—Ç, } \\\\\n",
    "&\\text { —á—Ç–æ } \\qquad \\pi^{\\prime} \\geq \\pi .\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/policy_iteration.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–µ—Ç–æ–¥ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—Ö–æ–¥–∏—Ç—Å—è:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/convergence_of_method.png\" alt=\"Drawing\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/random_vs_greedy_policy.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–§–∞–∫—Ç–∏—á–µ—Å–∫–∏, —ç—Ç–∞–ø policy evaluation –æ—Ç policy iteration  –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–æ–∫—Ä–∞—â–µ–Ω —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –≥–∞—Ä–∞–Ω—Ç–∏–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ policy iteration.\n",
    "\n",
    "–û–¥–∏–Ω –≤–∞–∂–Ω—ã–π –æ—Å–æ–±—ã–π —Å–ª—É—á–∞–π &mdash; —ç—Ç–æ –∫–æ–≥–¥–∞ **–æ—Ü–µ–Ω–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è** —Å—Ä–∞–∑—É **–ø–æ—Å–ª–µ –æ–¥–Ω–æ–≥–æ —Ü–∏–∫–ª–∞** (–æ–¥–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è). –≠—Ç–æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º **–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è value iteration**. –ï–≥–æ –º–æ–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å –∫–∞–∫ –ø—Ä–æ—Å—Ç—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ —ç—Ç–∞–ø—ã —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ —É—Å–µ—á–µ–Ω–Ω–æ–π policy evaluation (–æ–¥–∏–Ω —à–∞–≥).\n",
    "\n",
    "–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –≤–æ–æ–±—â–µ –Ω–µ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –≤ —è–≤–Ω–æ–º –≤–∏–¥–µ, —Ç.–∫. –æ–Ω–∞ –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è Q-—Ñ—É–Ω–∫—Ü–∏–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration (VI) vs. Policy iteration (PI):\n",
    "\n",
    "- VI –±—ã—Å—Ç—Ä–µ–µ –∑–∞ –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é $-\\mathrm{O}\\left(|\\mathrm{A} \\| \\mathrm{S}|^{2}\\right)$\n",
    "- VI —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–Ω–æ–≥–æ –∏—Ç–µ—Ä–∞—Ü–∏–π\n",
    "-PI –º–µ–¥–ª–µ–Ω–Ω–µ–µ –∑–∞ –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é $-\\mathrm{O}\\left(|\\mathrm{A} \\| \\mathrm{S}|^{2}+|\\mathrm{S}|^{3}\\right)$\n",
    "- PI —Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∞–ª–æ –∏—Ç–µ—Ä–∞—Ü–∏–π\n",
    "\n",
    "–ü—Ä–æ–≤–µ–¥–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –∫–∞–∫–∏–º-—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —à–∞–≥–æ–≤ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ª—É—á—à–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ê–ª–≥–æ—Ä–∏—Ç–º**\n",
    "\n",
    "1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:\n",
    "\n",
    "- —Å–æ–∑–¥–∞–µ–º –º–∞—Å—Å–∏–≤ V —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —Ä–∞–≤–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ—Å—Ç–æ—è–Ω–∏–π,\n",
    "- –∑–∞–ø–æ–ª–Ω—è–µ–º –µ–≥–æ –Ω—É–ª—è–º–∏.\n",
    "\n",
    "2. –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–∏—Ç–∏–∫–∏ (Policy evaluation):\n",
    "\n",
    "- –¥–ª—è –≤—Å–µ—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å—á–∏—Ç–∞–µ–º Q(s,a),\n",
    "- –æ–±–Ω–æ–≤–ª—è–µ–º –º–∞—Å—Å–∏–≤ V[s] $\\rightarrow$ max(Q(s,a)).\n",
    "\n",
    "*–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç Policy iteration, —Å–∞–º–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ –≤ –ø–∞–º—è—Ç–∏ –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è, –æ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ –ø–æ–º–æ—â–∏ Q-—Ñ—É–Ω–∫—Ü–∏–∏.\n",
    "\n",
    "3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ (Policy improvement):\n",
    "\n",
    "- –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—á–∏—Ç–∞–µ–º Q(s,a) –¥–ª—è –≤—Å–µ—Ö a,\n",
    "- –≤—ã–±–∏—Ä–∞–µ–º a, –¥–ª—è –∫–æ—Ç–æ—Ä–æ–≥–æ Q(s,a) –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ,\n",
    "- –µ—Å–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —à–∞–≥—É 2, –∏–Ω–∞—á–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å—Ç—Ä–æ–∏–º —á—Ç–æ-–Ω–∏–±—É–¥—å, —á—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É MDP.\n",
    "\n",
    "–ó–∞–ø–∏—à–µ–º —Ä–µ—à–µ–Ω–∏—è –¥–ª—è —ç—Ç–æ–≥–æ MDP. –°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º ‚Äî —ç—Ç–æ  __V__alue __I__teration.\n",
    "\n",
    "–ü—Å–µ–≤–¥–æ –∫–æ–¥ –¥–ª—è VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    "\n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P_{ss'}^a \\cdot [ R_{ss'}^a + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "$R_{ss'}^a$ ‚Äî –Ω–∞–≥—Ä–∞–¥–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –≤—ã–±–æ—Ä—É –¥–µ–π—Å—Ç–≤–∏—è a –≤ s,\n",
    "\n",
    "$s$ ‚Äî –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ,\n",
    "\n",
    "$s'$ ‚Äî –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è-–¥–µ–π—Å—Ç–≤–∏—è $Q^{\\pi}$ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "$$\\large Q_i(s, a) = \\sum_{s'} P_{ss'}^a \\cdot [ R_{ss'}^a + \\gamma V_{i}(s')],$$\n",
    "\n",
    "$s'$ ‚Äî –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\"\n",
    "    Computes Q(s,a) as in formula above\n",
    "\n",
    "    mdp : MDP object\n",
    "    state_values : dictionayry of { state_i : V_i }\n",
    "    state: string id of current state\n",
    "    gamma: float discount coeff\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    next_states = mdp.get_next_states(state, action)\n",
    "\n",
    "    Q = 0.0\n",
    "\n",
    "    for next_state in next_states.keys():\n",
    "        # alternatively p = mdp.get_transition_prob(state, action, next_state)\n",
    "        p = next_states[next_state]\n",
    "        Q += p * (\n",
    "            mdp.get_reward(state, action, next_state) + gamma * state_values[next_state]\n",
    "        )\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É—è $Q(s,a)$, –º—ã –º–æ–∂–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å \"—Å–ª–µ–¥—É—é—â–µ–µ\" V(s) –¥–ª—è VI:\n",
    "\n",
    "$$\\large V_{(i+1)}(s) = \\max_a \\sum_{s'} P_{ss'}^a \\cdot [ R_{ss'}^a + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\"Computes next V(s) as in formula above. Please do not change state_values in process.\"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return 0  # Game over\n",
    "\n",
    "    q_max = float(\"-inf\")\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "        q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "        q_max = max(q_max, q)\n",
    "    return q_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–∫–æ–Ω–µ—Ü, –æ–±—ä–µ–¥–∏–Ω–∏–º –≤—Å–µ, —á—Ç–æ –º—ã –Ω–∞–ø–∏—Å–∞–ª–∏, –≤ –∞–ª–≥–æ—Ä–∏—Ç–º –∏—Ç–µ—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—á–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "gamma = 0.9  # discount for MDP\n",
    "num_iter = 100  # maximum iterations, excluding initialization\n",
    "# stop VI if new values are this close to old values (or closer)\n",
    "min_difference = 0.001\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s: 0 for s in mdp.get_all_states()}\n",
    "\n",
    "display_jpeg(MDP.plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–¥–µ—Å—å –Ω–µ—Ç –Ω–∏–∫–∞–∫–æ–≥–æ \"—Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è\"! –î–æ–±–∞–≤–∏–º –º–∞—Ç—Ä–∏—Ü—É —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –ø–æ—Å—Ç—Ä–æ–∏–º –Ω–æ–≤—ã–π –≥—Ä–∞—Ñ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iter):\n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "\n",
    "    new_state_values = {}\n",
    "    for s in state_values.keys():\n",
    "        new_state_values[s] = get_new_state_value(mdp, state_values, s, gamma)\n",
    "\n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \" % (i, diff), end=\"\")\n",
    "    print(\"   \".join(\"V(%s) = %.3f\" % (s, v) for s, v in state_values.items()))\n",
    "    state_values = new_state_values\n",
    "\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_jpeg(MDP.plot_graph_with_state_values(mdp, state_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —ç—Ç–∏–º $V^{*}(s)$, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–∞–∂–¥–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏:\n",
    "\n",
    " $$\\pi^*(s) = \\text{argmax}_a \\sum_{s'} P_{ss'}^a \\cdot [ R_{ss'}^a + \\gamma V_{i}(s')] = \\text{argmax}_a Q_i(s,a)$$\n",
    "\n",
    "–ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç $V(s)$ –≤ —Ç–æ–º, —á—Ç–æ –∑–¥–µ—Å—å –º—ã –±–µ—Ä–µ–º –Ω–µ $\\max$, –∞ $\\text{argmax}$: –Ω–∞–π—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º $Q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\"Finds optimal action using formula above.\"\"\"\n",
    "    if mdp.is_terminal(state):\n",
    "        return None\n",
    "\n",
    "    best_action = None\n",
    "    q_max = float(\"-inf\")\n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    for a in actions:\n",
    "        q = get_action_value(mdp, state_values, state, a, gamma)\n",
    "        if q > q_max:\n",
    "            best_action = a\n",
    "            q_max = q\n",
    "\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_jpeg(\n",
    "    MDP.plot_graph_optimal_strategy_and_state_values(\n",
    "        mdp, state_values, get_action_value\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "\n",
    "print(\"average reward: \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal difference (TD)-–æ–±—É—á–µ–Ω–∏–µ (TD-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–∞–∫, –≤ —Ç–µ—Ö —Å–ª—É—á–∞—è—Ö, –∫–æ–≥–¥–∞ –∏–∑–≤–µ—Å—Ç–Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å $P_{ss'}^a$, –∑–∞–¥–∞—á–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ—à–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–∞–∫ –±—ã–ª–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ –≤—ã—à–µ. –í —Ä–µ–∞–ª—å–Ω—ã—Ö –∂–µ –∑–∞–¥–∞—á–∞—Ö –¥–∏–Ω–∞–º–∏–∫–∞ —Å—Ä–µ–¥—ã –∑–∞—á–∞—Å—Ç—É—é –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞. –î–≤–∞ –≥–ª–∞–≤–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∏–µ –æ–±—Ö–æ–¥–∏—Ç—å –Ω–µ–∑–Ω–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ —Å—Ä–µ–¥—ã, –≤–∫–ª—é—á–∞—é—Ç __–º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ –ö–∞—Ä–ª–æ__ (MC) –∏  __temporal difference (TD)-–æ–±—É—á–µ–Ω–∏–µ__ (TD-learning). –û–±–∞ —ç—Ç–∏ –ø–æ–¥—Ö–æ–¥–∞ –æ—Å–Ω–æ–≤—ã–≤–∞—é—Ç—Å—è –Ω–∞ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ–ø—ã—Ç–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å–æ —Å—Ä–µ–¥–æ–π –≤ –≤–∏–¥–µ __—Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π__ (MDP trajectory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDP trajectory**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/q_learning_scheme.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∏–∑ —Å–µ–±—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "\n",
    "‚Ä¢ states ($s$)\n",
    "‚Ä¢ actions ($a$)\n",
    "‚Ä¢ rewards ($r$)\n",
    "\n",
    "–ü—Ä–∏ —ç—Ç–æ–º MC –æ–ø–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–º–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏, –æ–∫–∞–Ω—á–∏–≤–∞—é—â–∏–º–∏—Å—è –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏, –∞ TD-learning –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—á–∏—Ç—å—Å—è –Ω–∞ –Ω–µ–ø–æ–ª–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–∞—Ö, –Ω–µ –¥–æ–∂–∏–¥–∞—è—Å—å –∫–æ–Ω–µ—á–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/TD_MC_DP_backups_.png\" alt=\"Drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –≤—Ç–æ—Ä–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥—Ä–æ–±–Ω–µ–µ.\n",
    "\n",
    "**TD-learning** ‚Äî –æ–¥–∏–Ω –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –º–æ—â–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤–æ –º–Ω–æ–≥–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö RL. –°–º—ã—Å–ª –≤ —Ç–æ–º, —á—Ç–æ–±—ã **–æ–±–Ω–æ–≤–ª—è—Ç—å –æ—Ü–µ–Ω–∫–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏** (—Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏—è) –Ω–∞ **–æ—Å–Ω–æ–≤–µ —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫** –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π. –≠—Ç—É –ø—Ä–æ—Ü–µ–¥—É—Ä—É –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ \"–ø–æ–¥—Ç—è–≥–∏–≤–∞–Ω–∏–µ\" –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–º –º—ã –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —à–∞–≥–µ, –∫ –∑–Ω–∞—á–µ–Ω–∏—é —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–º –º—ã –Ω–∞—Ö–æ–¥–∏–º—Å—è —Å–µ–π—á–∞—Å. –ö–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —ç—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë–∞–∑–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º TD-–æ–±—É—á–µ–Ω–∏—è ‚Äî $TD(0)$ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å–ª–µ–¥—É—é—â–µ–º:\n",
    "\n",
    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ü–µ–Ω–Ω–æ—Å—Ç–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, $V(s)$, –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é $\\pi$. –ü—É—Å—Ç—å, —É –Ω–∞—Å –µ—Å—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ $s$ –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –º—ã:\n",
    "- –≤—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ $a$ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ $\\pi$,\n",
    "- —Å–æ–≤–µ—Ä—à–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ $a$, –Ω–∞–±–ª—é–¥–∞–µ–º –Ω–∞–≥—Ä–∞–¥—É $r$ –∏ —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ $s'$,\n",
    "- –æ–±–Ω–æ–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É $V(s)$:\n",
    "\n",
    "$$V(s) := V(s) + \\alpha(\\underbrace{\\overbrace{r+\\gamma V(s')}^{\\text{TD target}} - \\overbrace{V(s)}^{\\text{old estimate}}}_{\\text{TD error}}),$$\n",
    "\n",
    "- –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É $s:=s'$\n",
    "\n",
    "–ü—Ä–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ $\\pi$:\n",
    "$$\\overbrace{r+\\gamma V(s')}^{\\text{TD target}} = \\overbrace{V(s)}^{\\text{old estimate}}$$\n",
    "\n",
    "C–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –∏—Ö —Ä–∞–∑–Ω–∏—Ü—É, –Ω–∞–∑—ã–≤–∞–µ–º—É—é _TD-–æ—à–∏–±–∫–æ–π_ (TD-error), –º—ã –∏ –±—É–¥–µ–º –ø—ã—Ç–∞—Ç—å—Å—è –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç? –í–µ–¥—å –º—ã –æ–±—É—á–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É $V(s)$ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥—Ä—É–≥–∏—Ö –æ—Ü–µ–Ω–æ–∫ $V(s')$, –∫–æ—Ç–æ—Ä—ã–µ —Ç–∞–∫–∂–µ –±—ã–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —Å–ª—É—á–∞–π–Ω–æ. –î–µ–ª–æ –≤ —Ç–æ–º, —á—Ç–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—Ç –ø–æ–ª—É—á–µ–Ω–∏—é —Ä–µ–∞–ª—å–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–∞ –ø–æ–±–µ–¥—É) –±—É–¥—É—Ç –æ–±—É—á–∞—Ç—å—Å—è –ø–µ—Ä–≤—ã–º–∏ –∏ –∑–∞—Ç–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—Ç—å —Å–≤–æ—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏–µ –∏–º.\n",
    "\n",
    "–°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å —Ç–∞–∫–∂–µ, —á—Ç–æ TD-–æ–±—É—á–µ–Ω–∏–µ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –æ–¥–∏–Ω —à–∞–≥ –≤–ø–µ—Ä–µ–¥. –ê–ª–≥–æ—Ä–∏—Ç–º, –Ω–∞–∑—ã–≤–∞–µ–º—ã–π $TD(\\lambda)$, –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å –æ—Ü–µ–Ω–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å—Ä–∞–∑—É –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –Ω–∞–∑–∞–¥.\n",
    "\n",
    "–†–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç—è–º–∏ TD-–æ–±—É—á–µ–Ω–∏—è —è–≤–ª—è—é—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º—ã __Q-learning__ –∏ __SARSA__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∞—Ç—Ä–∏—Ü–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ $P_{ss'}^{a}$ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞ —è–≤–Ω–æ, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω–∏—Ç –ø—Ä—è–º–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è Q –∏–ª–∏ V —Ñ—É–Ω–∫—Ü–∏–π –≤ –æ–±—â–µ–º —Å–ª—É—á–∞–µ:\n",
    "$$\n",
    "\\large q_{\\pi}(s, a) = \\sum_{s'}P_{ss'}^{a}(R_{ss'}^{a}+\\gamma\\sum_{a'}\\pi(a'|s') q_{\\pi}(s', a')) = \\mathbb{E}(R_{ss'}^a +\\gamma \\mathbb{E}_{\\pi} q_{\\pi}(s', a')).\n",
    "$$\n",
    "\n",
    "–ú—ã –º–æ–∂–µ–º –æ–±–æ–π—Ç–∏ —ç—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ, –∑–∞–º–µ–Ω–∏–≤ \"—á–µ—Å—Ç–Ω–æ–µ\" –º–∞—Ç–æ–∂–∏–¥–∞–Ω–∏–µ –ø–æ MDP, –Ω–∞ –≤—ã–±–æ—Ä–æ—á–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –æ—Ç —Å–µ—Ä–∏–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤–¥–æ–ª—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞:\n",
    "\n",
    "$$\n",
    "\\large q_{\\pi}(s, a) =  \\frac{1}{N}\\sum_n(R_{ss'}^a +\\gamma \\mathbb{E}_{\\pi} q_{\\pi}(s', a')).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–æ–ø—É—Å—Ç–∏–º, –º—ã —Ö–æ–¥–∏–º –ø–æ –ø–æ–ª—é —Ä–∞–∑–º–µ—Ä–æ–º 12√ó4. –ù–∞—à–∞ —Ü–µ–ª—å ‚Äî –ø–æ–ø–∞—Å—Ç—å –≤ —è—á–µ–π–∫—É (3,11).\n",
    "**–ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É—è —Å—É–º–º—É –±—É–¥—É—â–∏—Ö –Ω–∞–≥—Ä–∞–¥**, –º—ã —Ç–∞–∫–∂–µ –Ω–∞—Ö–æ–¥–∏–º **—Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å** –∫ —Ü–µ–ª–∏, –ø–æ—ç—Ç–æ–º—É –Ω–∞—à–∞ —Ü–µ–ª—å —Å–µ–π—á–∞—Å ‚Äî –Ω–∞–π—Ç–∏ —Å–ø–æ—Å–æ–± —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/q_learning_field.png\" alt=\"Drawing\" width=\"700\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/443240/\">–ü–æ–Ω–∏–º–∞–Ω–∏–µ Q-learning, –ø—Ä–æ–±–ª–µ–º–∞ ¬´–ü—Ä–æ–≥—É–ª–∫–∞ –ø–æ —Å–∫–∞–ª–µ¬ª</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—á–Ω–µ–º —Å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –≤ –ª—é–±–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ (—á–µ–º –±–æ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º –ª—É—á—à–µ –¥–µ–π—Å—Ç–≤–∏–µ).\n",
    "\n",
    "* –¢–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –∏–º–µ—Ç—å –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ –æ–¥–∏–Ω —Å—Ç–æ–ª–±–µ—Ü –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è. –°–µ—Ç–∫–∞ –∏–º–µ–µ—Ç 48 (4 –ø–æ Y –Ω–∞ 12 –ø–æ X) —Å–æ—Å—Ç–æ—è–Ω–∏–π, –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã 4 –¥–µ–π—Å—Ç–≤–∏—è ($\\leftarrow \\downarrow \\rightarrow \\uparrow$), –ø–æ—ç—Ç–æ–º—É —Ç–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç 48√ó4.\n",
    "* **–ó–Ω–∞—á–µ–Ω–∏—è**, —Ö—Ä–∞–Ω—è—â–∏–µ—Å—è **–≤ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ**, –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è **¬´Q-values¬ª**.\n",
    "* –≠—Ç–æ –æ—Ü–µ–Ω–∫–∏ —Å—É–º–º—ã –±—É–¥—É—â–∏—Ö –Ω–∞–≥—Ä–∞–¥. –î—Ä—É–≥–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –æ–Ω–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç, —Å–∫–æ–ª—å–∫–æ –µ—â–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –º—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å –¥–æ –∫–æ–Ω—Ü–∞ –∏–≥—Ä—ã, –Ω–∞—Ö–æ–¥—è—Å—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ S –∏ –≤—ã–ø–æ–ª–Ω—è—è –¥–µ–π—Å—Ç–≤–∏–µ A.\n",
    "* –ú—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–∏–ª–∏ –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Å–µ–º–∏ –Ω—É–ª—è–º–∏)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è ¬´Q-table¬ª –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –Ω–∞–º –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç—å –ª—É—á—à–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ –∫–∞–∂–¥–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏, –¥–∞–≤–∞—è –Ω–∞–º –≤ –∏—Ç–æ–≥–µ –ª—É—á—à–∏–π –ø—É—Ç—å –∫ –ø–æ–±–µ–¥–µ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/q_table.png\" alt=\"Drawing\" width=\"300\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/443240/\">–ü–æ–Ω–∏–º–∞–Ω–∏–µ Q-learning, –ø—Ä–æ–±–ª–µ–º–∞ ¬´–ü—Ä–æ–≥—É–ª–∫–∞ –ø–æ —Å–∫–∞–ª–µ¬ª</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-learning** ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º, –∫–æ—Ç–æ—Ä—ã–π ¬´–∏–∑—É—á–∞–µ—Ç¬ª —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥—É –º—ã –ø–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–∏—Ä–µ. –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Ç–∞–±–ª–∏—Ü–µ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L15/q_learning.gif\" alt=\"Drawing\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://habr.com/ru/articles/443240/\">–ü–æ–Ω–∏–º–∞–Ω–∏–µ Q-learning, –ø—Ä–æ–±–ª–µ–º–∞ ¬´–ü—Ä–æ–≥—É–ª–∫–∞ –ø–æ —Å–∫–∞–ª–µ¬ª</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–∞–∫, –º—ã –Ω–µ –∑–Ω–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤. –ú—ã –º–æ–∂–µ–º —Ç–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ —É—á–∏—Ç—å—Å—è –Ω–∞ –Ω–∏—Ö.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/q_learning_scheme.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ–∑–Ω–∏–∫–∞–µ—Ç –≤–æ–ø—Ä–æ—Å, –∫–∞–∫—É—é –∏–∑ —Ñ—É–Ω–∫—Ü–∏–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞–º –ª—É—á—à–µ –ø—ã—Ç–∞—Ç—å—Å—è —É—á–∏—Ç—å: $V(s)$ –∏–ª–∏ $Q(s,a)$? –ü–æ—Å–∫–æ–ª—å–∫—É $V(s)$ –Ω–µ –ø–æ–∑–≤–æ–ª–∏—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –±–µ–∑ –∑–Ω–∞–Ω–∏—è $P_{ss'}^a$, –±—É–¥–µ—Ç –≤—ã–≥–æ–¥–Ω–µ–µ —É—á–∏—Ç—å $Q(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/q_learning_possible_actions.png\" alt=\"Drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É $Q(s, a)$ –Ω—É–ª—è–º–∏.\n",
    "\n",
    "* –¶–∏–∫–ª:\n",
    "   * –°–µ–º–ø–ª–∏—Ä–æ–≤–∞—Ç—å $<s, a, r, s^{'}>$ –∏–∑ —Å—Ä–µ–¥—ã\n",
    "   * –í—ã—á–∏—Å–ª–∏—Ç—å $\\hat{Q}(s, a) = r(s,a) + \\gamma Q(s^{'}, a_{i})$\n",
    "   * –û–±–Ω–æ–≤–∏—Ç—å $Q(s,a) \\longleftarrow \\alpha \\hat{Q}(s, a) + (1-\\alpha)Q(s,a)$\n",
    "\n",
    "$\\color{red}\\triangle$ –ú—ã —Ö–æ—Ç–∏–º **–Ω–µ—Å–º–µ—â—ë–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É —Å –º–µ–Ω—å—à–µ–π –¥–∏—Å–ø–µ—Ä–∏—Å–µ–π**, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º **—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ**.\n",
    "\n",
    "$\\color{red}\\triangle$ **–ü–µ—Ä–µ—Å—á—ë—Ç** —Ñ—É–Ω–∫—Ü–∏–∏ (–ø–æ–ª–µ –≤ —Ç–∞–±–ª–∏—Ü–µ) –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è **–ø–æ—Å–ª–µ –ø–µ—Ä–µ—Ö–æ–¥–∞**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{Q}$ ‚Äî —Ä–∞—Å—á–µ—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, $Q$ ‚Äî —Ç–æ, —á—Ç–æ –±—ã–ª–æ –≤ —Ç–∞–±–ª–∏—Ü–µ, $\\alpha$ ‚Äî —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "–î–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è __Q-learning__. –û–Ω –Ω–∞–ø—Ä—è–º—É—é –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é $q_*(s,a)$, –∫–æ—Ç–æ—Ä–∞—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø–æ–ª–∏—Ç–∏–∫—É $\\pi_*$. –í –¥–∞–Ω–Ω–æ–º –∞–ª–≥–æ—Ä–∏—Ç–º–µ –æ—Ü–µ–Ω–∫–∞ $Q(s, a)$ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ TD-–æ–±—É—á–µ–Ω–∏—è:\n",
    "\n",
    "$$\\large Q(s,a) := Q(s,a) + \\alpha(R_{ss'}^a+\\gamma\\max_{a'}Q(s',a') -Q(s,a))$$\n",
    "\n",
    "–í–∏–¥–Ω–æ, —á—Ç–æ –∑–∞ —Å—á–µ—Ç –º–∞–∫—Å–∏–º—É–º–∞ Q-learning –±—É–¥–µ—Ç –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –∏–º–µ–Ω–Ω–æ $q_*(s,a)$, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–æ–≥–æ, –∫–∞–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –º—ã –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–µ–º—Å—è (–∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –±—ã—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä $\\varepsilon$-–∂–∞–¥–Ω–æ–π). –ù–∞—à –≤–∞—Ä–∏–∞–Ω—Ç Q-learning –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–≥–æ, —á—Ç–æ –º—ã —Ö–æ—Ç–∏–º –∏–Ω–æ–≥–¥–∞ \"—Ä–∞–Ω–¥–æ–º–∏—Ç—å\". –í —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç–∏ –≥—Ä—É–ø–ø–∞ –º–µ—Ç–æ–¥–æ–≤, –ø—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –º–æ–∂–µ–º –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è –∞–±—Å–æ–ª—é—Ç–Ω–æ –ª—é–±–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –∞ –æ–±—É—á–∞—Ç—å—Å—è –≤—Å–µ —Ä–∞–≤–Ω–æ –±—É–¥—É—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏, –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è __off-policy__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å—Ç—å –∑–∞–¥–∞—á–∏, –≥–¥–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –∏ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –æ–≥—Ä–æ–º–Ω–æ, –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω—ã–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/large_num_of_states.png\" width=\"350\"/><center>\n",
    "\n",
    "<center><em>–ò–ª–ª—é—Å—Ç—Ä–∞—Ü–∏—è –∏–≥—Ä—ã Space Invaders</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–ª—è $8$-–º–∏–±–∏—Ç–Ω–æ–π —Ü–≤–µ—Ç–Ω–æ–π ($3$ RGB-–∫–∞–Ω–∞–ª–∞) –∏–≥—Ä—ã Space Invaders —Å —Ä–∞–∑–º–µ—Ä–æ–º –∏–≥—Ä–æ–≤–æ–≥–æ –ø–æ–ª—è $210 \\times 160$ –ø–∏–∫—Å–µ–ª–µ–π:\n",
    "\n",
    "$$\\large |S| = 210\\times160\\times2^{8\\times3}$$\n",
    "\n",
    "–í —Ç–∞–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö Q-—Ñ—É–Ω–∫—Ü–∏—é –Ω–µ —Å—á–∏—Ç–∞—é—Ç –≤ —è–≤–Ω–æ–º –≤–∏–¥–µ, –∞ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É—é—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/approximately_q_function_by_network.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã —Å—á–∏—Ç–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç, –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ Q-—Ñ—É–Ω–∫—Ü–∏–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ –Ω–µ –∑–∞–≤–∏—Å—è—â–∏–º –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞—à–µ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –ß—Ç–æ, –∫–æ–Ω–µ—á–Ω–æ, –Ω–µ —Ç–∞–∫.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/deep_q_learning_loss.png\" alt=\"Drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-values:\n",
    "$$\n",
    "\\large \\hat{Q}\\left(s_{t}, a_{t}\\right)=r+\\gamma \\cdot \\max _{a^{\\prime}} Q\\left(s_{t+1}, a^{\\prime}\\right)\n",
    "$$\n",
    "Objective:\n",
    "$$\n",
    "\\large L=\\left(Q\\left(s_{t}, a_{t}\\right)-\\left[r+\\gamma \\cdot \\max _{a^{\\prime}} Q\\left(s_{t+1}, a^{\\prime}\\right)\\right]\\right)^{2}\n",
    "$$\n",
    "Gradient step:\n",
    "$$\n",
    "\\large w_{t+1}=w_{t}-\\alpha \\cdot \\frac{\\delta L}{\\delta w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic deep Q-learning**\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/basic_deep_q_learning_scheme.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\varepsilon -greedy$ –Ω—É–∂–Ω–∞ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—Ä–µ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞**:\n",
    "\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/information_about_states_is_unevenly_distributed.png\" alt=\"Drawing\" width=\"550\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –æ–∫—Ä—É–∂–µ–Ω–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –ø–æ–ª—É—á–∞–µ–º–∞—è –∞–≥–µ–Ω—Ç–æ–º, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –Ω–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ. –¢.–µ. –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π (—á—Ç–æ –ø–æ–Ω—è—Ç–Ω–æ –∏–∑ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç.–∫. –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –æ–∫—Ä—É–∂–µ–Ω–∏–π, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è RL, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ –≤—Å–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –Ω–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã).\n",
    "\n",
    "–ú—ã –º–æ–∂–µ–º —Å—Ç–æ–ª–∫–Ω—É—Ç—å—Å—è —Å —Ç–∞–∫–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, –µ—Å–ª–∏ –≤ –∏–≥—Ä–µ –µ—Å—Ç—å –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ —Ä–µ–¥–∫–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è \"–±–æ—Å—Å—ã\". –ü–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–∏–ø–æ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–µ–π—Ä–æ—Å–µ—Ç—å –±—É–¥–µ—Ç –∑–∞–±—ã–≤–∞—Ç—å, –∫–∞–∫ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –±–æ—Å—Å–æ–≤.\n",
    "\n",
    "–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ —É—Ö—É–¥—à–∞–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –Ω–∞–º –Ω—É–∂–µ–Ω —Å–ø–æ—Å–æ–±, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–ª—É—á—à–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—É—Å—Ç—Ä–∞–Ω–∏—Ç—å –∏–ª–∏ —Å–Ω–∏–∑–∏—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –Ω–∏–º–∏)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/experience_replay_scheme.png\" alt=\"Drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ò–¥–µ—è**: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π $<s,a,r,s^{'}>$.\n",
    "\n",
    "–û–±—É—á–∞—Ç—å –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞—Ö.\n",
    "\n",
    "**–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è**:\n",
    "* –°—ã–≥—Ä–∞—Ç—å 1 —à–∞–≥ –∏ –∑–∞–ø–∏—Å–∞—Ç—å –µ–≥–æ\n",
    "* –í—ã–±—Ä–∞—Ç—å $N$ —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "**Profit**: –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–Ω–æ–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å—Å—è –≤ —Ç–µ –∂–µ –ø–∞—Ä—ã *—Å–æ—Å—Ç–æ—è–Ω–∏–µ-–¥–µ–π—Å—Ç–≤–∏–µ* $(s,a)$, —á—Ç–æ–±—ã –≤—ã—É—á–∏—Ç—å –∏—Ö.\n",
    "* –ú–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ö–æ—Ä–æ—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é, —Å–¥–µ–ª–∞–≤ –º–µ–Ω—å—à–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å–æ —Å—Ä–µ–¥–æ–π\n",
    "* –ë–æ—Ä–µ–º—Å—è —Å –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏–º –∑–∞–±—ã–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "$\\color{red}\\triangle$ **–†–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –±–µ–∑ –ø–æ–ª–∏—Ç–∏–∫.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ **–ø—Ä–æ–∏–≥—Ä—ã–≤–∞–Ω–∏—è –æ–ø—ã—Ç–∞ (experience replay)**. –°—É—Ç—å —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≤ —Ç–æ–º, —á—Ç–æ –º—ã —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å–æ—Å—Ç–æ—è–Ω–∏–µ, –¥–µ–π—Å—Ç–≤–∏—è, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ) –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–º –±—É—Ñ–µ—Ä–µ –∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –º–∏–Ω–∏-–±–∞—Ç—á–∏ –∏–∑ —ç—Ç–æ–≥–æ –±—É—Ñ–µ—Ä–∞.\n",
    "\n",
    "–¢–∞–∫ –∂–µ **experience replay** –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–π –ø—Ä–æ—à–ª—ã–π –æ–ø—ã—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/DQN-Loss_.png\" alt=\"Drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í DQN –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–≤–µ —Å–µ—Ç–∏: –æ–¥–Ω–∞ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è, –¥—Ä—É–≥–∞—è –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ü–µ–ª—å (TD target). –ü–æ—á–µ–º—É –º—ã –Ω–µ –º–æ–∂–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ —Å–µ—Ç—å—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ —Ç–µ–∫—É—â–µ–≥–æ $Q(s', a')$ –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ $Q(s, a)$ —Å–æ—Å—Ç–æ—è–Ω–∏–π?\n",
    "\n",
    "–ù–∞–ø–æ–º–Ω–∏–º, —á—Ç–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ TD-–æ–±—É—á–µ–Ω–∏—è –º—ã \"–ø–æ–¥—Ç—è–≥–∏–≤–∞–µ–º\" –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–º –º—ã –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —à–∞–≥–µ, –∫ –∑–Ω–∞—á–µ–Ω–∏—é —Ñ—É–Ω–∫—Ü–∏–∏ —Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–º –º—ã –Ω–∞—Ö–æ–¥–∏–º—Å—è —Å–µ–π—á–∞—Å, –∞ –≤ DQN –º—ã –¥–µ–ª–∞–µ–º —ç—Ç–æ —Å –ø–æ–º–æ—â—å—é –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –∏ –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω—É –∏ —Ç—É –∂–µ —Å–µ—Ç—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ $Q(s', a')$ –∏ $Q(s, a)$, —Ç–æ —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –º–æ–∂–µ—Ç –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –Ω–µ—Å–∫–æ–Ω—á–∞–µ–º—É—é –ø–æ–≥–æ–Ω—é –∑–∞ —Ü–µ–ª—å—é, –∫–æ—Ç–æ—Ä—É—é –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–æ—Å—Ç–∏—á—å, –ø–æ—Å–∫–æ–ª—å–∫—É —Å –∫–∞–∂–¥—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤, –ø—Ä–∏–±–ª–∏–∂–∞—é—â–∏–º –Ω–∞—Å –∫ —Ü–µ–ª–∏, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å–∞–º–∞ —Ü–µ–ª—å –±—É–¥–µ—Ç —É–¥–∞–ª—è—Ç—å—Å—è –æ—Ç –Ω–∞—Å (–∫–∞–∫ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ –Ω–∏–∂–µ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/At_first_everything_look.png\" alt=\"Drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö —Å—á–∞—Å—Ç—å—é, —ç—Ç–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Ä–µ—à–∞–µ–º–∞: –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã —Å–µ—Ç—å –æ–±—É—á–∞–ª–∞—Å—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ü–µ–Ω–∫—É $\\max_{a'} Q(s', a')$, –ø–æ–ª—É—á–µ–Ω–Ω—É—é –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ Q-—Å–µ—Ç–∏. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç __—Ü–µ–ª–µ–≤—É—é —Å–µ—Ç—å__ (target network) —Å –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏, –æ–±–Ω–æ–≤–ª—è–µ–º—ã–º–∏ —Ä–∞–∑ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —ç–ø–æ—Ö –∏–ª–∏ –∏–≥—Ä–æ–≤—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∏–≥—Ä–æ–≤–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L15/out/Suppose_we_freeze.png\" alt=\"Drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –±—É–¥—É—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –¥–≤–µ —Å–µ—Ç–∏, –æ–¥–Ω–∞ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –∑–∞ –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è, –¥—Ä—É–≥–∞—è ‚Äî –∑–∞ —Ü–µ–ª–µ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é, –æ–¥–Ω–∞ –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –±—É–¥–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è —Å –æ—Ç—Å—Ç–∞–≤–∞–Ω–∏–µ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ—Ä c CartPole DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–µ—Ç—å Deep Q = DQN (Deep Q-Network).\n",
    "\n",
    "CartPole ‚Äî –ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç—ã–π –º–∞—è—Ç–Ω–∏–∫ —Å —Ü–µ–Ω—Ç—Ä–æ–º —Ç—è–∂–µ—Å—Ç–∏ –Ω–∞–¥ —Å–≤–æ–µ–π —Ç–æ—á–∫–æ–π –ø–æ–≤–æ—Ä–æ—Ç–∞. –û–Ω –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω, –Ω–æ –µ–≥–æ –º–æ–∂–Ω–æ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å, –ø–µ—Ä–µ–º–µ—â–∞—è —Ç–æ—á–∫—É –ø–æ–≤–æ—Ä–æ—Ç–∞ –ø–æ–¥ —Ü–µ–Ω—Ç—Ä–æ–º –º–∞—Å—Å. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–µ, –ø—Ä–∏–∫–ª–∞–¥—ã–≤–∞—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —É—Å–∏–ª–∏—è –∫ —Ç–æ—á–∫–µ –ø–æ–≤–æ—Ä–æ—Ç–∞.\n",
    "\n",
    "–î—Ä—É–≥–æ–π env –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫–æ–¥–∞. –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–æ—Å—Ç–æ—è–Ω–∏–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –µ–¥–∏–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º, –¥–µ–π—Å—Ç–≤–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏.\n",
    "\n",
    "CartPole ‚Äî —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π. –ù–∞ –µ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —É–π—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.\n",
    "\n",
    "–î–ª—è LunarLander –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è 1‚Äì2 —á–∞—Å–∞, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å 200 –±–∞–ª–ª–æ–≤ (—Ö–æ—Ä–æ—à–∏–π –±–∞–ª–ª) –≤ Colab, –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ–±—É—á–µ–Ω–∏–∏ –Ω–µ –≤—ã–≥–ª—è–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "################################################\n",
    "# For CartPole\n",
    "################################################\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/atari_wrappers.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/utils.py\n",
    "!wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/replay_buffer.py\n",
    "\n",
    "!pip install -q swig\n",
    "!pip install -q gym[box2d]\n",
    "\n",
    "!touch .setup_complete\n",
    "\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ[\"DISPLAY\"] = \":1\"\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "\n",
    "def make_env(seed=None):\n",
    "    # some envs are wrapped with a time limit wrapper by default\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\", new_step_api=True).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "env_img = np.squeeze(env.render())\n",
    "clear_output()\n",
    "plt.imshow(env_img)\n",
    "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –Ω–∞–º –Ω—É–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —Å —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º q-–∑–Ω–∞—á–µ–Ω–∏–π.\n",
    "\n",
    "–ú–æ–¥–µ–ª—å –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–æ–π: 1‚Äì2 —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è —Å < 200 –Ω–µ–π—Ä–æ–Ω–∞–º–∏ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π ReLU, –≤–µ—Ä–æ—è—Ç–Ω–æ, –±—É–¥–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ.\n",
    "\n",
    "Batch Normalization –∏ Dropout –º–æ–≥—É—Ç –≤—Å–µ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å, –ø–æ—ç—Ç–æ–º—É –∏—Ö –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "print(state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        assert len(state_shape) == 1\n",
    "        state_dim = state_shape[0]\n",
    "\n",
    "        # Define NN\n",
    "        ##############################################\n",
    "        hidden_size = 150\n",
    "        self._nn = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        ##############################################\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "\n",
    "        ##############################################\n",
    "        qvalues = self._nn(state_t)\n",
    "        ##############################################\n",
    "\n",
    "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "        assert (\n",
    "            len(qvalues.shape) == 2\n",
    "            and qvalues.shape[0] == state_t.shape[0]\n",
    "            and qvalues.shape[1] == n_actions\n",
    "        )\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy.\"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice([0, 1], batch_size, p=[1 - epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward.\"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = (\n",
    "                qvalues.argmax(axis=-1)[0]\n",
    "                if greedy\n",
    "                else agent.sample_actions(qvalues)[0]\n",
    "            )\n",
    "            s, r, done, _, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer and Target Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–æ–≤–æ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç:**\n",
    "\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç (s,a,r,s',done) –∫–æ—Ä—Ç–µ–∂ –≤ –±—É—Ñ–µ—Ä?\n",
    "* `exp_replay.sample(batch_size)` ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç observations, actions, rewards, next_observations –∏ is_done –¥–ª—è `batch_size` random samples,\n",
    "* `len(exp_replay)` ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —Ö—Ä–∞–Ω—è—â–∏—Ö—Å—è –≤ replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "\n",
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—ã—á–∏—Å–ª–∏–º –æ—à–∏–±–∫—É TD Q-learning:\n",
    "\n",
    "$$ \\large L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_\\text{reference}(s,a) ] ^2 $$\n",
    "\n",
    "–° Q-reference, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∫–∞–∫:\n",
    "\n",
    "$$ \\large Q_\\text{reference}(s,a) = r(s,a) + \\gamma \\cdot \\max_{a'} Q_\\text{target}(s', a'), $$\n",
    "\n",
    "–≥–¥–µ:\n",
    "* $Q_\\text{target}(s',a')$ –æ–±–æ–∑–Ω–∞—á–∞–µ—Ç $Q$-–∑–Ω–∞—á–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è __target_network__,\n",
    "* $s, a, r, s'$ ‚Äî —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –¥–µ–π—Å—Ç–≤–∏–µ, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∏ —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ,\n",
    "* $\\gamma$ —è–≤–ª—è–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –¥–≤—É–º—è —è—á–µ–π–∫–∞–º–∏ –≤—ã—à–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(\n",
    "    states,\n",
    "    actions,\n",
    "    rewards,\n",
    "    next_states,\n",
    "    is_done,\n",
    "    agent,\n",
    "    target_network,\n",
    "    gamma=0.99,\n",
    "    check_shapes=False,\n",
    "    device=device,\n",
    "):\n",
    "    \"\"\"Compute td loss using torch operations only. Use the formulae above.\"\"\"\n",
    "    states = torch.tensor(\n",
    "        states, device=device, dtype=torch.float32\n",
    "    )  # shape: [batch_size, *state_shape]\n",
    "    actions = torch.tensor(\n",
    "        actions, device=device, dtype=torch.int64\n",
    "    )  # shape: [batch_size]\n",
    "    rewards = torch.tensor(\n",
    "        rewards, device=device, dtype=torch.float32\n",
    "    )  # shape: [batch_size]\n",
    "    # shape: [batch_size, *state_shape]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype(\"float32\"),\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # with torch.no_grad():\n",
    "    predicted_next_qvalues = target_network(\n",
    "        next_states\n",
    "    )  # shape: [batch_size, n_actions]\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[\n",
    "        range(len(actions)), actions\n",
    "    ]  # shape: [batch_size]\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    ##############################################\n",
    "    next_state_values = predicted_next_qvalues.max(axis=-1)[0]\n",
    "    ##############################################\n",
    "\n",
    "    assert (\n",
    "        next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0]\n",
    "    ), \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    ###############################################\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values * is_not_done\n",
    "    ##############################################\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean(\n",
    "        (predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2\n",
    "    )\n",
    "\n",
    "    if check_shapes:\n",
    "        assert (\n",
    "            predicted_next_qvalues.data.dim() == 2\n",
    "        ), \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert (\n",
    "            next_state_values.data.dim() == 1\n",
    "        ), \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert (\n",
    "            target_qvalues_for_actions.data.dim() == 1\n",
    "        ), \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = make_env()\n",
    "env.reset(seed=seed)\n",
    "state_dim = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ TD loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import fftconvolve, gaussian\n",
    "\n",
    "\n",
    "# Exponention Moving Average for plot smoothing\n",
    "def smoothen(values):\n",
    "    kernel = gaussian(100, std=100)\n",
    "    # kernel = np.concatenate([np.arange(100), np.arange(99, -1, -1)])\n",
    "    kernel = kernel / np.sum(kernel)\n",
    "    return fftconvolve(values, kernel, \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∞–≥–µ–Ω—Ç –¥–ª—è –∏–≥—Ä—ã, –≤—Ç–æ—Ä–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –≤–µ—Å–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∫–æ–ø–∏—Ä—É—é—Ç—Å—è –≤ \"–∏–≥—Ä–æ–∫–∞\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "\n",
    "    hint: use agent.sample.actions\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for _ in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "\n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        # action = action.argmax(axis=-1)[0]\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        sum_rewards += reward\n",
    "\n",
    "        exp_replay.add(s, action, reward, state, done)\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "\n",
    "        s = state\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 10**4\n",
    "\n",
    "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\n",
    "            \"\"\"\n",
    "            Less than 100 Mb RAM available.\n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "        )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 4 * 10**4\n",
    "decay_steps = 1 * 10**4\n",
    "\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 20\n",
    "refresh_target_network_freq = 100\n",
    "eval_freq = 1000\n",
    "\n",
    "max_grad_norm = 5000\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []\n",
    "step = 0\n",
    "\n",
    "\n",
    "def wait_for_keyboard_interrupt():\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "state = env.reset()\n",
    "with trange(step, total_steps + 1) as progress_bar:\n",
    "    for step in progress_bar:\n",
    "        if not utils.is_enough_ram():\n",
    "            print(\"less that 100 Mb RAM available, freezing\")\n",
    "            print(\"make sure everything is ok and use KeyboardInterrupt to continue\")\n",
    "            wait_for_keyboard_interrupt()\n",
    "\n",
    "        agent.epsilon = utils.linear_decay(\n",
    "            init_epsilon, final_epsilon, step, decay_steps\n",
    "        )\n",
    "\n",
    "        # play\n",
    "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "        # train\n",
    "        # sample batch_size of data from experience replay\n",
    "        s, a, r, next_s, is_done = exp_replay.sample(batch_size)\n",
    "        # loss = compute TD loss\n",
    "        loss = compute_td_loss(s, a, r, next_s, is_done, agent, target_network)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % loss_freq == 0:\n",
    "            td_loss_history.append(loss.data.cpu().item())\n",
    "            grad_norm_history.append(grad_norm.data.cpu().item())\n",
    "\n",
    "        if step % refresh_target_network_freq == 0:\n",
    "            # Load agent weights into target_network\n",
    "            target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "        if step % eval_freq == 0:\n",
    "            mean_rw_history.append(\n",
    "                evaluate(make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "            )\n",
    "            initial_state_q_values = agent.get_qvalues([make_env(seed=step).reset()])\n",
    "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "            clear_output(True)\n",
    "            print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
    "\n",
    "            plt.figure(figsize=[16, 9])\n",
    "\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.title(\"Mean reward per episode\")\n",
    "            plt.plot(mean_rw_history)\n",
    "            plt.grid()\n",
    "\n",
    "            assert not np.isnan(td_loss_history[-1])\n",
    "            plt.subplot(2, 2, 2)\n",
    "            plt.title(\"TD loss history (smoothened)\")\n",
    "            plt.plot(smoothen(td_loss_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.title(\"Initial state V\")\n",
    "            plt.plot(initial_state_v_history)\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.title(\"Grad norm history (smoothened)\")\n",
    "            plt.plot(smoothen(grad_norm_history))\n",
    "            plt.grid()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(make_env(), agent, n_games=30, greedy=True, t_max=1000)\n",
    "print(\"final score:\", final_score)\n",
    "if final_score > 300:\n",
    "    print(\"Well done\")\n",
    "else:\n",
    "    print(\"not good enough for DQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞</font>\n",
    "\n",
    "* [[arxiv] üéì Deep Reinforcement Learning for Autonomous Driving: A Survey](https://arxiv.org/abs/2002.00444)<br>\n",
    "* [[arxiv] üéì Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review](https://arxiv.org/abs/1805.00909)<br>\n",
    "* [[git] üêæ Lecture notes and exercises for control theory course](https://github.com/DPritykin/Control-Theory-Course)\n",
    "* [[article] üéì Reinforcement learning for combinatorial optimization: A survey](https://doi.org/10.1016/j.cor.2021.105400)<br>\n",
    "* [[arxiv] üéì Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940)\n",
    "* [[arxiv] üéì Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815) <br>\n",
    "* [[blog] ‚úèÔ∏è AlphaGo](https://deepmind.google/technologies/alphago/)\n",
    "* [[book] üìö Reinforcement Learning for NLP](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf)<br>\n",
    "* [[arxiv] üéì Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n",
    "* [[doc] üõ†Ô∏è Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "<font size=\"5\">–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:</font>\n",
    "* [[git] üêæ A course in reinforcement learning in the wild](https://github.com/yandexdataschool/Practical_RL/tree/master)\n",
    "* [[book] üìö 8.1. –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –£—á–µ–±–Ω–∏–∫ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –®–ê–î](https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem)\n",
    "* [[course] üìö UC Berkeley CS188 Intro to AI](http://ai.berkeley.edu/lecture_slides.html)\n",
    "* [[course] üìö Deep Reinforcement Learning](https://rail.eecs.berkeley.edu/deeprlcourse/)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
