{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Базовые методы Классического Машинного Обучения</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разделение train-validation-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры ошибок в данных и при разбиении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинное обучение - это не только об алгоритмах, но и о данных. При работе с данными необходимо соблюдать некоторую культуру. В этой части лекции мы разберем ряд ошибок, допускаемых при работе с данными и их разбиением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утечка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Утечкой данных называется ситуация, когда в процессе обучения модели используется информация, которая не будет доступна при ее последующем использовании. Это приводит к **завышению** оценки **качества** модели.\n",
    "\n",
    "Самый простой пример утечки данных - это **дублирование** одних и тех же объектов в **train** и **test** выборках. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Дублирование данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дублирование данных часто случается при сборе данных из различных источников. Посмотрим, чем оно опасно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для **примера** возьмем 10 картинок из CIFAR10. Будем считать это train данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "dataset = datasets.CIFAR10(\"content\", train=True, download=True)\n",
    "\n",
    "data, _, labels, _ = train_test_split(dataset.data / 255,   # normalize\n",
    "                                      np.array(dataset.targets),\n",
    "                                      train_size=10,        # get only 10 imgs\n",
    "                                      random_state=42,\n",
    "                                      stratify=dataset.targets)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    axs[i//5][i%5].imshow(data[i])\n",
    "    axs[i//5][i%5].set_title(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим картика из train оказалась в test. Выберем картинку из этих 10 и применим алгоритм k-nearest neighbors с k=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data[3]\n",
    "\n",
    "# L1 distance\n",
    "def compute_L1(a, b):\n",
    "    return np.sum(np.abs(a - b))\n",
    "\n",
    "# distance calculation\n",
    "distances = []\n",
    "for i in range(10):\n",
    "    l1 = compute_L1(x_test, data[i])\n",
    "    distances.append(l1)\n",
    "\n",
    "distances = np.array(distances)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = np.argmin(distances)\n",
    "print(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test, _, labels_test, _ = train_test_split(dataset.data / 255,   # normalize\n",
    "                                      np.array(dataset.targets),\n",
    "                                      train_size=10,        # get only 10 imgs\n",
    "                                      random_state=24,\n",
    "                                      stratify=dataset.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ближайшим соседом для картинки, просочившейся в test стала эта же картинка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все данные из test будут присутствовать в train, то мы просто будем искать эту же картинку в train с чем алгоритм k-nearest neighbors с $k=1$ справляется идеально. Итогом станет $accuracy = 1$ на выходе. Но с применением на незнакомой картинке результат будет хуже. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    axs[i//5][i%5].imshow(data_test[i])\n",
    "    axs[i//5][i%5].set_title(labels_test[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data_test[1]\n",
    "\n",
    "# distance calculation\n",
    "distances = []\n",
    "for i in range(10):\n",
    "    l1 = compute_L1(x_test, data[i])\n",
    "    distances.append(l1)\n",
    "\n",
    "distances = np.array(distances)\n",
    "print(distances)\n",
    "\n",
    "indx = np.argmin(distances)\n",
    "print(labels[indx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим картинки с train. Ближайшим соседом для кота стала лягушка. \n",
    "\n",
    "**Если вы получили $accuracy = 1$, то, скорее всего, вы что-то делаете не так!**\n",
    "\n",
    "Дубли в данных следует удалить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример** поиска очевидных дубликатов в данных в датасете [\"DOHMH Dog Bite Data\"](https://data.cityofnewyork.us/Health/DOHMH-Dog-Bite-Data/rsgh-akpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "!wget https://data.cityofnewyork.us/api/views/rsgh-akpg/rows.csv?accessType=DOWNLOAD -O dogs.csv\n",
    "\n",
    "# Load into pandas and display a sample\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('dogs.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dataset) == len(dataset.drop_duplicates()):\n",
    "    print('No duplicates')\n",
    "else:\n",
    "    print('%.2f percent of data is duplicate' \n",
    "          % len(dataset.drop_duplicates())/len(dataset) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Утечка, спрятанная в признаках "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто данные, используемые для обучения могут содержать “подсказки” для модели, которых не будет в реальных данных.\n",
    "\n",
    "Самый простой **пример**: таблица со столбцом - порядковым номером строки `row_number`, в которую сначала записали все данные, принадлежащие отрицательному классу, а потом все данные, принадлежащие положительному. Если не удалить этот столбец из данных, то вместо выделения сложных закономерностей модель будет искать решение в виде `if row_number > N`. В реальных данных записи не будут упорядочены. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примером** датасета в котором нумерация строк может “все испортить” является, например, [Iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris). Посмотрим на значения target этого набора данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "iris = sklearn.datasets.load_iris()\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Утечка часто может прятаться в метаданных записи: столбец id, название файла, время записи/загрузки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда “подсказки” спрятаны внутри признаков, поэтому важно понимать с какими данными вы работаете. \n",
    "\n",
    "**Пример:** у вас есть истории болезней пациентов с подозрением на онкологию. Необходимо решить задачу постановки диагнозов для новых пациентов. Вы обучаете модель и получаете подозрительно хороший результат. Начинаете разбираться и понимаете, что в данных, на которых обучалась модель присутствует отметка о назначении пациенту химиотерапии, которая назначается только после определения диагноза. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut learning - это термин, которым обозначается ситуация, когда модель принимает правильное решение по неправильной причине: “right for the wrong reasons\".\n",
    "Очень важно, чтобы данные, на которых происходит обучение были **репрезентативными** и модель не пыталась сделать предсказание по косвеным признакам. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример**: рассмотрим датасет фотографий объектов, где изображения в обучающем и тестовом наборе были получены на открытом воздухе в солнечный день. Наличие одинаковых погодных условий означает, что тестовое множество не будет независимым, а поскольку оно не охватывает более широкий спектр погодных условий, оно также не будет репрезентативным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример:** необходимо было обучить нейросеть отличать хаски от волка. Для обучения были выбраны фотографии хаски на фоне зелени и волков на фоне снега, вместо того, чтобы учиться отличать мордочки нейросеть научилась делать предсказание по фону. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/EXTRA_publish/img_license/husky.png\" width=\"850\"></center>\n",
    "<center><i>Первая строка - обучающие данные (обратите внимания, что все хаски были сфотографированы летом, а волки зимой). Вторая строка - тестовые данные (нейросеть выучила, что лето - это признак хасок, по этому она не правильно предсказывает класс волков).</center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичные ситуации могут возникать, когда для сбора обучающих и тестовых данных используется разное оборудование. **Например**, для обучения вы используете томограф из одного госпиталя, а оценку качества модели проводите на томографе из другого госпиталя. Если модель не учитывает характеристики оборудования, она, скорее всего, не будет обобщаться на томограф из второй больницы (при этом на своем собственном датасете из первого госпиталя, эту ошибку вы отловить не сможете)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение на train и test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перемешивание данных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы уже упоминали метки классов в датасете могут быть распределены не равномерно. Для того, чтобы сохранить соотношение классов при разделении на train и test необходимо указать параметр `stratify` при разбиении.\n",
    "\n",
    "Еще одним параметром, используемым при разбиении, является `shuffle` (значение по умолчанию `True`). При `shuffle = True` датасет перед разбиением перемешивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наглядности будем делить датасет пополам. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lables(lables):\n",
    "    lable_count = {}\n",
    "    for item in lables:\n",
    "        if item not in lable_count:\n",
    "            lable_count[item] = 0\n",
    "        lable_count[item] += 1\n",
    "    return lable_count\n",
    "\n",
    "def print_split_stat(X_train, X_test, y_train, y_test):\n",
    "    print(\"Train labels: \", y_train)\n",
    "    print(\"Test labels:  \", y_test)\n",
    "    print(\"Train statistics: \", count_lables(y_train))\n",
    "    print(\"Test statistics:  \", count_lables(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data, labels = load_iris(return_X_y=True)\n",
    "print(\"DataSet labels: \", labels)\n",
    "print(\"DataSet statistics: \", count_lables(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.5,\n",
    "                                                    shuffle = False, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print_split_stat(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.5, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print_split_stat(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, train_size=0.5, \n",
    "                                                    random_state=42, stratify=labels)\n",
    "\n",
    "print_split_stat(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В некоторых случаях данные нельзя перемешивать. Это касается задач в которых мы пытаемся предсказать будущее. В таких задачах train должен предшествовать test по времени. Более подробно об этом будет рассказано в лекции про рекуррентные нейронные сети. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Данные из различных источников"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании данных из различных источников нужно учитывать это при разбиении. \n",
    "\n",
    "**Пример:** вы анализируете данные ЭКГ на предмет патологий. У вас есть три источника данных:\n",
    "* аппарат ЭКГ в кардиологическом отделении (много патологий),\n",
    "* аппарат ЭКГ, который используют на медосмотрах (мало патологий),\n",
    "* аппарат ЭКГ из приемного покоя больницы (среднее число патологий). \n",
    "\n",
    "Каждый прибор имеет свои особенности: характерные шумы, точность измерения и т.п. Если модель научится определять с какого прибора пришли данные она получит “подсказку”. Хорошим решением будет оставить данные с аппарата ЭКГ из приемного покоя больницы для test, а обучаться только на данных с аппарата ЭКГ в кардиологическом отделении и аппарата ЭКГ, который используют на медосмотрах. Это позволит оценить, как обученная модель работает с “незнакомым\" прибором.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дисбаланс классов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема подбора гиперпараметров на тестовой выборке\n",
    "(Разбираем ДЗ, выясняем что:\n",
    "\n",
    "а) у KNN есть гиперпараметры (расстояние, K)\n",
    "\n",
    "б) Точность(+/- 3%) сильно зависит от разбиения\n",
    "Первый пункт позволяет ввести понятие гиперпараметра,второй — кросс валидации.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4. Nearest Neighbors для картинок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с помощью функций torchvision, пока можно не очень вдумываться как это работает, дальше в курсе мы познакомимся с PyTorch значительно ближе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = datasets.CIFAR10(\"content\", train=True, download=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "data, _, labels, _ = train_test_split(dataset.data / 255,  # Normalize\n",
    "                                      np.array(dataset.targets),\n",
    "                                      train_size=0.1, # get only fraction of the dataset\n",
    "                                      random_state=42,\n",
    "                                      stratify=dataset.targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    axs[i//5][i%5].imshow(data[i])\n",
    "    axs[i//5][i%5].set_title(labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что это за датасет такой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR - 4х мерный массив (N, W, H, C). $N$ - количество картинок, $W$ - ширина картинки, $H$ - высота картинки, $C$ - количество каналов (RGB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте subplots с 2мя строками и 2мя колонками и отобразите 4 любых картинки из `data`. \n",
    "Используйте `plt.imshow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))\n",
    "\n",
    "ax[0, 0].imshow(data[0])\n",
    "ax[0, 1].imshow(data[1])\n",
    "ax[1, 0].imshow(data[2])\n",
    "ax[1, 1].imshow(data[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте датасет на тренировочный и тестовый наборы. Укажите аргументы `random_state=42`, `stratify=labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=labels)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмите первую картинку из тестового набора и найдите ее ближайшего соседа из тренировочного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_L1(a, b):\n",
    "    return np.sum(np.abs(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_test[0]\n",
    "\n",
    "l1s = []\n",
    "for i in range(len(X_train)):\n",
    "    l1 = compute_L1(a, X_train[i])\n",
    "    l1s.append(l1)\n",
    "\n",
    "distances = np.array(l1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = np.argmin(distances)\n",
    "print(indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Отобразите эти картинки на subplots с ncols=2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow(X_test[0])\n",
    "ax[1].imshow(X_train[indx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрите какой класс предсказывается**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = y_train[indx]\n",
    "class_to_idx = dataset.class_to_idx\n",
    "\n",
    "print(list(class_to_idx.keys())[list(class_to_idx.values()).index(class_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмите первую картинку из тестового набора и найдите K ее ближайших соседей (KNN) из тренировочного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "indx = np.argsort(distances)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразите ближайших соседей в виде subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=k, figsize=(20, 5))\n",
    "ax[0].imshow(X_test[0])\n",
    "for i in range(1, k):\n",
    "    ax[i].imshow(X_train[indx[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте KNN для всего датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем больше данных - тем дольше процесс. Реализуйте функцию для расчета расстояний. Если вы используете `for loops` - сделайте к ним *progress bars* с помощью [tqdm](https://github.com/tqdm/tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(train, test, distance_func):\n",
    "    train_size = len(train)\n",
    "    test_size = len(test)\n",
    "    distances = np.full((test_size, train_size), np.inf)\n",
    "    for i in tqdm(range(test_size)):\n",
    "        for j in range(train_size):\n",
    "            distances[i, j] = distance_func(test[i], train[j])\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = compute_distances(X_train, X_test, compute_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдите k ближайших соседей и предскажите класс. [scipy.stats.mode](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def get_accuracy(distances, train_labels, test_labels, k):\n",
    "    indexes = np.argsort(distances, axis=1)[:, :k]\n",
    "    labels_of_top_classes = train_labels[indexes]\n",
    "    predicted_class, _ = mode(labels_of_top_classes, axis=1)\n",
    "    accuracy = np.mean(test_labels == predicted_class.flatten())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = get_accuracy(distances, y_train, y_test, k)\n",
    "print(f'Accuracy = {accuracy * 100:.0f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посчитайте точность для k=1..100 и постройте график точности от k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for k in range(1, 100):\n",
    "    acc.append(get_accuracy(distances, y_train, y_test, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 100), acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поменяйте расстоянние L1 на L2 и сравните точность на всем датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_L2(a, b):\n",
    "    return np.sqrt(np.sum((a - b)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_l2 = compute_distances(X_train, X_test, compute_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_l2 = []\n",
    "for k in range(1, 100):\n",
    "    acc_l2.append(get_accuracy(distances_l2, y_train, y_test, k=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 100), acc, label='L1')\n",
    "plt.plot(np.arange(1, 100), acc_l2, label='L2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизируйте гиперпараметры вашей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие модели имеют **гиперпараметры** - то есть числа или параметры, которые влияют на конфигурацию модели. Примерами могут служить: функция ядра, используемая в SVM; количество деревьев в случайном лесу и архитектура нейронной сети. Многие из этих гиперпараметров существенно влияют на производительность модели, и, как правило, универсальных гиперпараметров (таких, которые были бы оптимальны для всех возможных задач) не существует.\n",
    "\n",
    "То есть, чтобы получить максимальную отдачу от модели, гииперпараметры нужно подбирать под конкретный набор данных. Хотя может возникнуть соблазн возиться с гиперпараметрами до тех пор, пока вы не найдете что-то подходящее, такой подход, скорее всего, не будет оптимальным. Гораздо лучше использовать какую-то стратегию **оптимизации гиперпараметров** (в качестве бонуса, обоснованная стратегия, в публикации смотрится значительно лучше, чем что-то в стиле *hyperparameters were chosen by chance*). Базовые стратегии включают случайный поиск и поиск по сетке, но они не очень хорошо масштабируются для большого количества гиперпараметров или для моделей, которые дорого обучать, поэтому стоит использовать инструменты, которые ищут оптимальные конфигурации более интеллектуальным способом (исследовано в [Yang et al., 2020](https://arxiv.org/abs/2007.15745))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Библиотеки для оптимизации гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует довольно много библиотек для оптимизации гиперпараметров. Ключевыми являются [Ray-tune](https://docs.ray.io/en/latest/tune/index.html), [Optuna](https://optuna.readthedocs.io/en/stable/) и [Hyperopt](https://github.com/hyperopt/hyperopt). В целом они друг от друга принципиально не отличаются, так что скорее это вопрос вкуса. В качестве примера (и для разнообразия) рассмотрим библиотеку `Ray-Tune`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ray-Tune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray[tune] tune-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 14)\n",
    "\n",
    "# Example parameters to tune from SGDClassifier\n",
    "parameter_grid = {\"alpha\": [1e-4, 1e-1, 1], \"epsilon\": [0.01, 0.1]}\n",
    "\n",
    "tune_search = TuneGridSearchCV(\n",
    "    SGDClassifier(),\n",
    "    parameter_grid,\n",
    "    early_stopping=True,\n",
    "    max_iters=10)\n",
    "\n",
    "tune_search.fit(x_train, y_train)\n",
    "\n",
    "#best set of perameter\n",
    "print(tune_search.best_params_)\n",
    "\n",
    "#best score with best set of perameters\n",
    "print(tune_search.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запомните, когда вы оптимизируете гиперпараметры или признаки, используемые моделью, для их отбора нужно использовать отдельный, валидационный датасет, или же кросс-валидацию. Вы **НЕ должны подбирать модель или ее гиперпараметры на тесте**.\n",
    "\n",
    "Также можно использовать методы **AutoML** для оптимизации выбора модели и ее гиперпараметров (см. обзор в [He et al., 2021](https://arxiv.org/abs/1908.00709))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков, соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30 признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n",
    "\n",
    "Можно иметь сколь угодно хороший алгоритм для классификации - но до тех пор, пока данные на входе - мусор, на выходе из нашего чудесного классификатора мы тоже будем получать мусор (*garbage in - garbage out*). Давайте разберемся, что конкретно надо сделать, чтобы kNN реально заработал.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer() # load data\n",
    "X = cancer.data # features\n",
    "Y = cancer.target # labels(classes)\n",
    "print(f'X shape: {X.shape}, Y shape: {Y.shape}') \n",
    "print(f'X[0]: \\n {X[0]}') \n",
    "print(f'Y[0]: \\n {Y[0]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько данных в классе `0` и сколько данных в классе `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5)) # set fig size \n",
    "plt.bar(1,Y[Y==1].shape, label=cancer.target_names[0]) # 1 label \n",
    "plt.bar(0,Y[Y==0].shape, label=cancer.target_names[1]) # 0 label\n",
    "plt.title('Class balance') \n",
    "plt.ylabel('Num examples') \n",
    "plt.xticks(ticks=[1,0], labels=['1','0']) \n",
    "plt.legend(loc='upper left') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк в каждой, из которой, по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.boxenplot(data=pd.DataFrame(X), orient=\"h\", palette=\"Set2\")\n",
    "ax.set(xscale='log', xlim=(1e-4, 1e4), xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы адекватно сравнить данные между собой нам следует использовать нормализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Нормализация, выбор Scaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация — это преобразование данных к неким безразмерным единицам.\n",
    "Ключевая цель нормализации — приведение различных данных в самых разных единицах измерения и диапазонах значений к единому виду, который позволит сравнивать их между собой.\n",
    "\n",
    "Главное условие правильной нормализации — все признаки должны быть равны в возможностях своего влияния.\n",
    "\n",
    "Например, у нас есть данные по группе людей: *возраст* (в годах) и *размер дохода* (в рублях). Возраст может измениться в диапазоне от 18 до 70 ( интервал 70-18 = 52). А доход от 30 000 р до 500 000 р (интервал 500 000 - 30 000 = 470 000). В таком варианте разница в возрасте имеет меньшее влияние, чем разница в доходе. Получается, что доход становится более важным признаком, изменения в котором влияют больше при сравнении схожести двух людей.\n",
    "\n",
    "Должно быть так, чтобы максимальные изменения любого признака в «основной массе объектов» были одинаковы. Тогда потенциально все признаки будут равноценны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось определиться с выбором инструмента, часто используют следующие варианты: `MinMaxScaler`, `StandardScaler`, `RobustScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для признака `data[:,0]`. **Обратите внимание на ось X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # setting the initialization parameter for random values\n",
    "\n",
    "# generate random values from 1 to 255, shape (30,1)\n",
    "test = X[:,0].reshape(-1,1)\n",
    "\n",
    "plt.figure(1, figsize=(30, 5))  \n",
    "plt.subplot(141)  # set location\n",
    "plt.scatter(test, range(len(test)), c=Y)  \n",
    "plt.ylabel(\"Num examples\", fontsize=15)  \n",
    "plt.xticks(fontsize=15)  \n",
    "plt.yticks(fontsize=15)  \n",
    "plt.title(\"Non scaled data\", fontsize=18)  \n",
    "\n",
    "# scale data with MinMaxScaler\n",
    "test_scaled = MinMaxScaler().fit_transform(test)  \n",
    "plt.subplot(142)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"MinMaxScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with StandardScaler\n",
    "test_scaled = StandardScaler().fit_transform(test)  \n",
    "plt.subplot(143)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"StandardScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with RobustScaler\n",
    "test_scaled = RobustScaler().fit_transform(test)  \n",
    "plt.subplot(144)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"RobustScaler\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные в диапазоне от 0 до 1. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (e.g., масштабирование RGB пикселей)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$z=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "$X_{min}$ и $X_{max}$ задаются как минимальное и максимальное допустимое значение, по умолчанию:  $X_{min}=0$  и $X_{max}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение 0 и стандартное отклонение 1. Большинство значений будет в  диапазоне от -1 до 1. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-u}{s}$$\n",
    "\n",
    "$u$ — среднее значение (или 0 при `with_mean=False`) и $s$ — стандартное отклонение (или 0 при `with_std=False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И StandardScaler и MinMaxScaler очень чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. Процентиль — мера, в которой процентное значение общих значений равно этой мере или меньше ее. Например, 90 % значений данных находятся ниже 90-го процентиля, а 10 % значений данных находятся ниже 10-го процентиля. Соответственно, RobustScaler не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-X_{median}}{IQR}$$\n",
    "\n",
    "$X_{median}$ — значение медианы, $IQR$ — межквартильный диапазон равный разнице между 75-ым и 25-ым процентилями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нашей задачи по определению раковых опухолей обработаем наши 30 признаков с помощью StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = StandardScaler().fit_transform(X)  # scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что они стали намного более сравнимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_norm).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxenplot(data=pd.DataFrame(X_norm), \n",
    "                   orient=\"h\", \n",
    "                   palette=\"Set2\")\n",
    "ax.set(xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим kNN для общей выборки данных, при разном значении количества соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  # array of the number of neighbors\n",
    "\n",
    "quality = np.zeros(\n",
    "    n_nei_rng.shape[0]\n",
    ")  \n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  # for all elements\n",
    "    # create knn for all num neighbors \n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_nei_rng[ind]\n",
    "    )  \n",
    "    knn.fit(X_norm, Y)  \n",
    "    q = accuracy_score(y_pred=knn.predict(X_norm), y_true=Y)  # accuracy\n",
    "    quality[ind] = q  # fill quality\n",
    "\n",
    "plt.figure(figsize=(8, 5))  \n",
    "plt.title(\"KNN on train\", size=20)  \n",
    "plt.xlabel(\"Neighbors\", size=15)  \n",
    "plt.ylabel(\"Accuracy\", size=15)  \n",
    "plt.plot(n_nei_rng, quality)  \n",
    "plt.xticks(n_nei_rng) \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество на 1 соседе - самое лучшее. Но это и понятно - ближайшим соседом элемента из обучающей выборки будет сам объект. Мы просто **запомнили** все объекты.\n",
    "\n",
    "Если теперь мы попробуем взять какой-то новый образец опухоли и классифицировать его - у нас скорее всего ничего не получится. В таких случаях мы говорим, что наша модель не умеет обобщать (*generalization*).\n",
    "\n",
    "Для того, чтобы знать заранее обобщает ли наша модель или нет, мы можем разбить все имеющиеся у нас данныe на 2 части. Но одной части мы будем обучать классификатор (*train set*), а на другой тестировать насколько хорошо он работает (*test set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data to train/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train_norm = scaler.transform(X_train)  # scaling data\n",
    "X_test_norm = scaler.transform(X_test)  # scaling data\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  \n",
    "train_quality = np.zeros(n_nei_rng.shape[0])  # quality on train data\n",
    "test_quality = np.zeros(n_nei_rng.shape[0])  # quality on test data\n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=n_nei_rng[ind])  \n",
    "    knn.fit(X_train_norm, Y_train)  \n",
    "    \n",
    "    # accuracy on train data\n",
    "    trq = accuracy_score(y_pred=knn.predict(X_train_norm), y_true=Y_train)  \n",
    "    train_quality[ind] = trq  \n",
    "\n",
    "    # accuracy on test data\n",
    "    teq = accuracy_score(y_pred=knn.predict(X_test_norm), y_true=Y_test)  \n",
    "    test_quality[ind] = teq  \n",
    "\n",
    "# accuracy plot  on train and test data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот, теперь мы видим, что 1 сосед был \"ложной тревогой\". Такие случаи мы называем *переобучением*. Чтобы действительно предсказывать что-то полезное, нам надо выбирать число соседей, начиная минимум с 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кросс-валидация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм кросс валидации\n",
    "\n",
    "Каждая модель имеет как ряд **параметров**, которые она меняет в процессе обучения (например, веса модели), так и ряд **гиперпараметров**, которые влияют на то, каким способом модель меняет параметры в процессе обучения. \n",
    "\n",
    "В случае kNN параметры, строго говоря, отсутствует - модель просто запоминает объекты обучающей выборки. Особо упорные могут считать их параметрами. \n",
    "\n",
    "А вот гиперпараметры есть, даже несколько групп. Какие? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Число соседей \n",
    "2. Функция, которой считаем расстояние между объектами (L2=eucledian, L1=manhattan)\n",
    "3. Веса, с которыми складываем метки ближайших соседей\n",
    "4. Признаки! (но об этом с вами поговорим позже)\n",
    "5. Сама модель - мы могли выбрать не kNN, а нагуглить что-нибудь другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим еще раз на график, который нарисовали на прошлом шаге. Какое число соседей считать оптимальным? Метрика явно скачет? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Не понятно, насколько результат зависит от того, как нам повезло или не повезло с разбиением данных на обучение и тест. Может оказаться так, что для конкретного разбиения хорошо выбрать k=5, а для другого - k=7. \n",
    "\n",
    "Кроме того, опять же - фактически, мы сами выступаем в роли модели, которая учит гиперпараметры (а не параметры) под видимую ей выборку. \n",
    "\n",
    "Представим себе, что у нас есть 10000 моделей, полученных подкручиванием разных гиперпараметров (в том числе, выбором просто разного типа модели). Представим, что все эти модели не работают. Вообще. Представим так же, что каждая модель угадывает класс в задаче разделения на два класса с вероятностью 0.5 (и будем считать, что классы у нас сбалансированны - то есть 50% одного класса и 50% другого). \n",
    "Опять же, понятно, что классификация такими моделями ничем не лучше подбрасывания монетки. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_model(Y_real):\n",
    "    # array with values True/False, for random choice by mask \n",
    "    guessed = np.random.choice(\n",
    "        [True,False],  \n",
    "        size=Y_real.shape[0],\n",
    "        replace=True,\n",
    "    )\n",
    "    Y_pred = np.zeros_like(Y_real)  # zeros array, shape Y_real\n",
    "    \n",
    "    # with mask 'guessed' assign values \n",
    "    Y_pred[guessed] = Y_real[guessed]  \n",
    "    Y_pred[~guessed] = 1 - Y_real[~guessed]  \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_num = 10000  # num of experiments\n",
    "best_quality = 0.5  # quality threshold for accuracy\n",
    "\n",
    "# array with random values in range 0, 1\n",
    "Y_real = np.random.choice(\n",
    "    [0, 1], size=250, replace=True\n",
    ")  \n",
    "\n",
    "for i in range(models_num):  # for all expirements\n",
    "    Y_pred = guess_model(Y_real)  # predicted values\n",
    "    q = accuracy_score(y_pred=Y_pred, y_true=Y_real)  # accuracy\n",
    "    if q > best_quality:  \n",
    "        best_quality = q   \n",
    "print(f\"Best result {best_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы перебором всех возможных моделей вполне можем получить для абсолютно бесполезной модели приемлемое качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Получается, что если подбирать гиперпараметры модели на *train set*, то:\n",
    "1. Можно переобучитьcя, просто на более \"высоком\" уровне. Особенно если гиперпараметров у модели много и все они разнообразны\n",
    "2. Нельзя быть уверенным, что выбор параметров не зависит от разбиения на обучение и тест "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы:\n",
    "\n",
    "1. Подбираем гиперпараметры моделей на отдельном датасете, называемым валидационным. Получаем мы его разбиением обучающего датасета на собственно обучающий и валидационный \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/split_dataset_for_train_val_test.png\" width=\"700\">\n",
    "\n",
    "2. Чаще всего делаем несколько таких разбиений по какой-то схеме, чтобы получить уверенность оценок качества для моделей с разными гиперпараметрами - **кросс-валидация**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/cross_validation_on_train_data.png\" width=\"500\">\n",
    "\n",
    "Часто применяется следующий подход, называемый [K-Fold кросс-валидацией](https://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "\n",
    "Берется тренировочная часть датасета, разбивается на части - блоки. Дальше мы будем использовать для проверки первую часть (Fold 1), а на остальных учиться. И так последовательно для всех частей. В результате у нас будут информация о точности для разных фрагментов данных и уже на основании этого можно понять, насколько значение этого параметра, который мы проверяем, зависит или не зависит от данных. То есть если у нас от разбиения точность при одном и том же К меняться не будет, значит мы подобрали правильное К. Если она будет сильно меняться в зависимости от того, на каком куске данных мы проводим тестирование, значит, надо попробовать другое К и если ни при каком не получилось - то это такие данные.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Оценка результата кросс-валидации\n",
    "(вообще не понятно, что это)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Типичные ошибки при кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Можно ли делать только кросс-валидацию (без теста)?**\n",
    "\n",
    "\n",
    "Нет, нельзя. Кросс-валидация не до конца спасает от подгона параметров модели под выборку, на которой она проводится. Оценка конечного качества модели должно производиться на отложенной тестовой выборке. Если у вас очень мало данных, можно рассмотреть [вложенную кросс-валидацию](https://weina.me/nested-cross-validation/), Речь об этом пойдет позже, в последующих лекциях. Но даже в этом случае придется анализировать поведение модели, чтобы показать, что она учит что-то разумное. Кстати, вложенную кросс-валидацию можно использовать, чтобы просто получить более устойчивую оценку поведения модели на тесте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-валидация для научных исследований - на что обратить внимание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перекрестная валидация (*cross-validation*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы не можете получить больше данных - то вы можете более эффективно использовать уже имеющиеся, используя перекрестную валидацию (*cross validation*). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще говоря, кросс-валидация используется для более точной оценки модели. Одна оценка модели может быть ненадежной и может либо недооценить, либо переоценить истинный потенциал модели. По этой причине обычно проводится несколько оценок. Существует несколько способов провести множественную оценку модели, и большинство из них предполагает многократное обучение модели с использованием различных подмножеств обучающих данных. **Перекрсная валидация** (CV) особенно популярна и имеет множество разновидностей [Arlot et al., 2010](https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перекрсная валидация** в машинном обучении подразумеват, что вместо того, чтобы разделить наш набор данных на две части, одну для обучения (`train`), а другую для тестирования (`test`), мы разбиваем наш набор данных на несколько частей, обучаем на некоторых из них, а остальные используем для тестирования. Затем мы используем другие части для обучения и тестирования нашей модели. Это гарантирует, что наша модель обучается и тестируется на новых данных на каждом новом шаге.\n",
    "\n",
    "Чаще всего используют десятикратную прекресную валидацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разберем на примере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И разделим его на 10 (*K*) складок (*folds*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "X, y = make_moons(noise=0.3, random_state=42, n_samples=1000)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Lets test 100 times\n",
    "for i in range(0,100):\n",
    "    # For each experiment lets choose a different random state for splitting\n",
    "    random_state = np.random.randint(0,2**32)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # normalize data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # predict using each classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    scores.append([random_state, classifier.score(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Split data intto 10 folds\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "#Define scaler and classifier\n",
    "scaler = MinMaxScaler()\n",
    "clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "\n",
    "#Define figure space\n",
    "fig,ax = plt.subplots(nrows=2, ncols=5, figsize=(10,4))\n",
    "\n",
    "row = 0\n",
    "scores = []\n",
    "\n",
    "#Itterate over folds\n",
    "for col, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    #Split\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    #Normalize\n",
    "    scaler.fit(X_train)\n",
    "    scaler.fit(X_test)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    #Classify\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    #Gauge performance\n",
    "    score = clf.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "    #Plot figure\n",
    "    if col > 4:\n",
    "        col-=5\n",
    "        row=1\n",
    "\n",
    "    ax[row, col].scatter(X_train[:,0], X_train[:,1], c=y_train, alpha=0.05)\n",
    "    ax[row, col].scatter(X_test[:,0], X_test[:,1], c=y_test, marker='x')\n",
    "    ax[row, col].set_title(score)\n",
    "    ax[row, col].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результирующую точность модели мы определим как среднее от всех складок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Финальная точность = %.2f ± %.2f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для б**о**льшей надежности можно использовать метод **вложенной перекрестной валидации** (также известный как двойная кросс-валидация, (см. [Cawley, 2010](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf) и [Wainer et al., 2021](https://research-portal.uea.ac.uk/en/publications/nested-cross-validation-when-selecting-classifiers-is-overzealous))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем параметры для модели с помощью **GridSearchCV**.\n",
    "\n",
    "GridSearchCV – это инструмент для автоматического подбирания параметров для моделей машинного обучения. GridSearchCV находит наилучшие параметры, путем обычного перебора: он создает модель для каждой возможной комбинации параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from IPython.display import clear_output\n",
    "\"\"\"\n",
    "Parameters for GridSearchCV:\n",
    "estimator — model\n",
    "cv — num of fold to cross-validation splitting \n",
    "param_grid — parameters names\n",
    "scoring — metrics \n",
    "n_jobs - number of jobs to run in parallel, -1 means using all processors.\n",
    "\"\"\"\n",
    "model = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    cv=KFold(3, shuffle=True, random_state=42),\n",
    "    param_grid={\n",
    "        \"n_neighbors\": np.arange(1, 31),\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train_norm, Y_train)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем лучшие гиперпараметры для модели, которые подобрали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metric:\", model.best_params_[\"metric\"])\n",
    "print(\"Num neighbors:\", model.best_params_[\"n_neighbors\"])\n",
    "print(\"Weigths:\", model.best_params_[\"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Объект GridSearchCV можно использовать как обычную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "Y_pred = model.predict(X_test_norm)\n",
    "print(f\"Percent correct predictions {np.round(accuracy_score(y_pred=Y_pred, y_true=Y_test)*100,2)} %\")\n",
    "print(f\"Percent correct predictions(balanced classes) {np.round(balanced_accuracy_score(y_pred=Y_pred, y_true=Y_test)*100,2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем извлечь дополнительные данные о кроссвалидации и по ключу обратиться к результатам всех моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем для примера mean_test_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(model.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"mean_test_score\", size=20)\n",
    "plt.xlabel(\"Num of experiment\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим, например, при фиксированных остальных параметрах (равных лучшим параметрам), качество модели на валидации в зависимости от числа соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_means = []\n",
    "selected_std = []\n",
    "n_nei = []\n",
    "for ind, params in enumerate(model.cv_results_[\"params\"]):\n",
    "    if (\n",
    "        params[\"metric\"] == model.best_params_[\"metric\"]\n",
    "        and params[\"weights\"] == model.best_params_[\"weights\"]\n",
    "    ):\n",
    "        n_nei.append(params[\"n_neighbors\"])\n",
    "        selected_means.append(model.cv_results_[\"mean_test_score\"][ind])\n",
    "        selected_std.append(model.cv_results_[\"std_test_score\"][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим error bar, для сравнения разброса ошибки при разном количестве соседей Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на самом деле большой разницы в числе соседей и нет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"KNN CV, {params['metric']}, {params['weights']}\", size=20)\n",
    "plt.errorbar(n_nei, selected_means, yerr=selected_std, linestyle=\"None\", fmt=\"-o\")\n",
    "plt.xticks(n_nei)\n",
    "plt.ylabel(\"Mean_test_score\", size=15)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RandomGridSearch\n",
    "(нету)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Признаки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные, требующие предобработки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Потратьте время на понимание своих данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конечном итоге вы захотите опубликовать свою работу. Публиковать работы основанные на данных из надежного источника (собранных с использованием надежной методологии) значительно проще.\n",
    "\n",
    "Если вы используете данные, скачанные с интернет-ресурса, **убедитесь, что вы знаете, откуда они взяты** (описаны ли они в статье? Если да, посмотрите на документ; убедитесь, что он был опубликован в авторитетном месте, и проверьте, упоминают ли авторы какие-либо ограничения данных).\n",
    "\n",
    "Не предполагайте, что если набор данных использовался в ряде работ, то он хорошего качества - иногда данные используются только потому, что их легко достать, а некоторые широко используемые наборы данных, как известно, имеют существенные ограничения (см. [Paullada et al., 2020]((https://arxiv.org/abs/2012.05345))). **Например**, при исследовании категории `faces` в ImageNet ([Deng et al., 2009](https://ieeexplore.ieee.org/document/5206848), [Crawford & Paglen, 2019](https://excavating.ai/) ) обнаружили миллионы изображений людей, которые были помечены оскорбительными категориями, включая расистские и унизительные фразы. В ответ на эту работу, большая часть набора данных ImageNet была удалена ([Yang et al., 2020](https://dl.acm.org/doi/abs/10.1145/3351095.3375709))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы обучаете свою модель на плохих данных, то, скорее всего, у вас получится плохая модель: процесс, известный как **garbage in garbage out** (чушь на входе - чушь на выходе). Поэтому всегда начинайте с проверки, что ваши **данные имеют смысл**. \n",
    "\n",
    "Проведите **эксплораторный анализ данных** (см. [Cox, 2017](https://www.oreilly.com/library/view/translating-statistics-to/9781484222560/A426308_1_En_3_Chapter.html)). Ищите недостающие или непоследовательные записи. Гораздо проще сделать это сейчас, до обучения модели, чем потом, когда вы будете пытаться объяснить рецензентам, почему вы использовали плохие данные. \n",
    "\n",
    "Анализ важно провести независимо от того, используете ли вы существующие наборы данных или генерируете новые данные в рамках своего исследования (в этом случае учитывайте, что исследовательский анализ может быть ценен сам по себе, а результаты этого анализа могут стать важной частью вашей статьи).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отдельным пунктом, хочется отметить, что помимо \"содержания\", важна и \"форма\" данных. **Формат хранения** ваших данных повлияет на **скорость, с которой вы сможете завершить свое исследование**. **Например**, у вас есть массив, который называется `ID` и в нем хранятся следующие данные `[1,30,111,221,234]` в формате `float64`. Проверьте, а точно ли тут нужен `float64`, возможно, ваши данные представлены целыми положительными числами, и для их хранения будет достаточно формата `uint32` или даже `uint16` (см. подробный обзор форматов данных в [Understanding Data Types](https://jakevdp.github.io/PythonDataScienceHandbook/02.01-understanding-data-types.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разберем на конкретном примере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем датасет: **\"Когда и где кого-то покусала собака в NYC\"** и загрузим его в `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "!wget https://data.cityofnewyork.us/api/views/rsgh-akpg/rows.csv?accessType=DOWNLOAD -O dogs.csv\n",
    "\n",
    "# Load into pandas and display a sample\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('dogs.csv')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим есть ли **дупликаты**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(dataset) == len(dataset.drop_duplicates()):\n",
    "    print('Очевидных дупликатов нет')\n",
    "else:\n",
    "    print('%.2f процентов данных являются дупликатами' % len(dataset.drop_duplicates())/len(dataset) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть колонки: `UniqueID`, `DateOfBite`, `Species`,\t`Breed`, `Age`, `Gender`, `SpayNeuter`, `Borough`, `ZipCode`, давайте проверим все ли с ними в порядке. Начнем с того, что определим в каком виде хранятся наши данные в памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Object` - это далеко не самая эффективная форма хранения информации в `Pandas-DataFrame` и, как правило, отличный индикатор того, что с данными, что-то не так. Давайте разберемся с каждой колонкой по отдельности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `UniqueID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ожидаем, что в этой колонке каждому объявлению был присвоен уникальный ID. Судя по сэмплу, это просто порядковый номер начинающийся с 1. Можем визуализировать эту колонку, что бы убедиться что там никаких сюрпризов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(len(dataset))\n",
    "plt.scatter(x, dataset['UniqueID'], s=0.1)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('UniqueID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что уникальных идентификаторов меньше чем строк в датафрейме. Давайте убедимся:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['UniqueID'].max(), len(dataset['UniqueID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть ID повторяются?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values('UniqueID').head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Судя по всему, в какой-то момент времени нумерация была запущена заново. А значит ID совсем даже не unique => использовать эту колонку как уникальный идентификатор мы не можем. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каком формате хранятся данные в этой колонке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['UniqueID'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `int64` можно записывать целые числа в диапазоне от `-9223372036854775808` до `9223372036854775807`. Мы уже по графику видим, что знак нам не нужен, и что наше максимальное значение явно меньше. Определим какой у нас максимум."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['UniqueID'].min(), dataset['UniqueID'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значит нам подойдет `uint16` целое число без знака в диапазоне от  `0` до `65535`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered = dataset.copy()\n",
    "dataset_filtered['UniqueID'] = dataset['UniqueID'].astype('uint16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем сколько памяти мы выиграли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resources_gain(column = 'UniqueID', orig_dataset=dataset, filtered_dataset=dataset_filtered):\n",
    "    original_memory = orig_dataset[column].memory_usage(deep=True)\n",
    "    memory_after_conversion = filtered_dataset[column].memory_usage(deep=True)\n",
    "\n",
    "    return original_memory/memory_after_conversion\n",
    "\n",
    "resources_gain(column='UniqueID', orig_dataset=dataset, filtered_dataset=dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь колонка `UniqueID` занимает в 4 раза меньше места (а значит и обрабатывается быстрее)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `DateOfBite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `DateOfBite` судя по всему записано время укуса, но в формате `str`. Нам было бы удобнее работать с `timestamps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['DateOfBite'] = pd.to_datetime(dataset['DateOfBite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим выигрыш в ресурсах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_gain(column='DateOfBite', orig_dataset=dataset, filtered_dataset=dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим нет ли каких-то странных дат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['DateOfBite'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С датами все в порядке, кстати можно заметить, что во время Ковида собакам было явно меньше кого кусать =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Species`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы ожидаем, что в этом отчете сообщается только об укусах собак, убедимся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Species'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у нас целая колонка в которой исключительно значение `DOG`, то зачем нам эта колонка? Правильно. Удаляем ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_filtered['Species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Breed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, какие значения есть в этой колонке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Breed'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что тип нашего массива - `object`. Обычно так бывает, когда в массиве есть несколько разных типов данных (например `float` и `str`). Давайте найдем все значения, которые str не являются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Breed'][dataset['Breed'].apply(lambda x: type(x) != str)].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ага - `NaN`. А выше мы уже видели что есть категория `UNKOWN`. Давайте поправим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['Breed'][dataset['Breed'].apply(lambda x: type(x) != str)] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на поправленный список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(dataset_filtered['Breed'].unique()).tolist()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу видно, что категории часто повторяются с опечатками. Править это придется в ручную (чем мы заниматься на этой лекции конечно не будем). Но для примера поправим опечатку в `ALASKAN MALMUTE` (заменим на `ALASKAN MALAMUTE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['Breed'][dataset['Breed'] == 'ALASKAN MALMUTE'] = 'ALASKAN MALAMUTE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть ограниченное (хоть и большое) количество пород, с точки зрения памяти, их выгоднее хранить как категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['Breed'] = dataset_filtered['Breed'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим выигрыш в производительности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_gain(column='Breed', orig_dataset=dataset, filtered_dataset=dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках примера, будем считать что колонку почистили"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Age`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ожидаем, что колонка будет в числовом формате, на деле видим `object`, давайте разбираться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Age'].unique()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну тут только руками... Нам надо выбрать единую единицу измерения (например месяцы) и все привести к ней. Там где непонятно будем писать `NaN` (хотя лучше бы выяснить конечно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['Age'][ (dataset['Age'] == '4') | \n",
    "                        (dataset['Age'] == '7') |\n",
    "                        (dataset['Age'] == '6') |\n",
    "                        (dataset['Age'] == '5') |\n",
    "                        (dataset['Age'] == '8') |\n",
    "                        (dataset['Age'] == '11') |\n",
    "                        (dataset['Age'] == '3')] = np.nan #так как не понятно 4 чего\n",
    "dataset_filtered['Age'][dataset['Age'] == '4Y'] = 4*12\n",
    "dataset_filtered['Age'][dataset['Age'] == '5Y'] = 5*12\n",
    "dataset_filtered['Age'][dataset['Age'] == '3Y'] = 3*12\n",
    "#ну и так далее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Gender`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие есть варианты пола собаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Gender'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну хоть тут без сюрпризов, но для увеличения производительности тоже сконвертируем данные в категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['Gender'] = dataset['Gender'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_gain(column='Gender', orig_dataset=dataset, filtered_dataset=dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кто чаще кусается?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['Gender'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SpayNeuter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spay/Neutter` - это была ли собака стерилизована. Мы ожидаем только True и False (хотя удивительно, что не колонки unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['SpayNeuter'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А формат у нас и так уже `bool`, который занимает мало места. Значит с этой колонкой закончили"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Borough`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boroughs - это что-то типа наших округов (например ЮЗАО)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Borough'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже переведем в категориальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['Borough'] = dataset['Borough'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['Borough'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_gain(column='Borough', orig_dataset=dataset, filtered_dataset=dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ZipCode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['ZipCode'].unique()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что есть категории `?` и `NaN`. Их можно объединить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['ZipCode'][dataset['ZipCode'] == np.nan] = '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остальные значения тоже пусть будут категориальными (у многих значений есть 0 в начале, пусть  остается, что бы не вводить дополнительную путаницу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered['ZipCode'] = dataset['ZipCode'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим насколько меньше места теперь занимает **весь** датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.memory_usage(deep=True).sum()/dataset_filtered.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом можем считать нашу чистку завершенной. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных и инструменты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Типы признаков \n",
    "\n",
    "Традиционно признаки делятся на:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вещественные \n",
    "Вещественные признаки бывают:\n",
    "\n",
    " * дискретные. Например - число лайков от пользователей\n",
    " \n",
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/discrete_features_social_media_likes.jpg\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * непрерывные. Например - температура\n",
    "\n",
    "\n",
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/continuous_features_thermometer.png\" width=\"600\">\n",
    " \n",
    "Понятно, что разделение часто условное. Тот же возраст можно посчитать и дискретной переменной (пользователь всегда нам сообщает свои полные года), и непрерывной (возраст можно считать с любой точностью, но никто не будет) )\n",
    "\n",
    "\n",
    "Также иногда вещественные признаки делят на относительные (считаются относительно чего-то, уже нормированные и тд)  и интервальные. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Категориальные \n",
    "\n",
    "\n",
    "\n",
    "Значение -  принадлежность к какой-то из категорий. Традиционно делятся на сильно отличающиеся по свойствам:\n",
    " * упорядоченные (ординальные) - для каждой пары возможных категорий можем сказать, какая больше, а какая меньше. Например - класс места. Или размер одежды\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/categorical_ordered_features.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * неупорядоченные (номинальные) - категории между собой несравнимы. Обычно нельзя сказать, что красный телефон больше синего. Или что солнечная погода больше снежной\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/categorical_unordered_features.png\" width=\"900\">\n",
    "\n",
    "\n",
    "\n",
    "Часто мы сталкиваемся с бинарными категориальными признаками, для которых известно только две возможных категории (например, биологический пол человека). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гнерация и преобразования признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразования признаков\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вещественных признаков \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинаризация \n",
    "\n",
    "Например, нам может быть неинтересно, сколько конкретно раз встретилось явление в наблюдении - главное, что оно вообще встретилось. Тогда мы просто превращаем наш вещественный признак в бинарный \"было ли явление\", и работаем уже с ним. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Округление\n",
    "\n",
    "Часто данные до нас доходят с очень высокой точностью после запятой. Нужно ли это нашей модели - часто нет. Иногда по факту два наблюдения не различаются  по этому признаку (разница в пределах статошибки), но по признаку их отличить можно. Это может приводить к переобучению. В таких случаях разумно признаки округлить. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bining (Бинирование)\n",
    "\n",
    "Опять же, нам не интересны точные значения - например, что видео набрало 1000 лайков, а не 1001. \n",
    "\n",
    "К тому же, число просмотров/лайков некоторых видео может быть очень большим в сравнении с остальными, что будет приводить к неадекватному поведению. \n",
    "В итоге часть значений у нас встречается часто, а часть - очень редко. Это может приводить к неадекватному поведению модели. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed-width bining\n",
    "\n",
    "Просто бьем наши значения по диапазонам фиксированной длины. Так часто поступают с возрастом. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/fixed_width_binning.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaptive Binning\n",
    "\n",
    "Это не всегда работает хорошо. Например, распределение зарплат у нас очень сильно скошено вправо. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/adaptive_binning.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И бинирование с фиксированной длиной бина нам не поможет справиться с редкими значениями.\n",
    "\n",
    "В этой ситуации помогает бинирование, например, по квантилям - когда границы бина представляют собой квантили. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/quantiles_binning.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логарифмирование\n",
    "\n",
    "С ситуацией, когда распределение скошено вправо работает и другой подход - прологаримфировать величину. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/log_binning.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Обобщением этого подхода является [Box-Cox Transform](https://www.statisticshowto.com/box-cox-transformation/#:~:text=A%20Box%20Cox%20transformation%20is,a%20broader%20number%20of%20tests.), общей целью которой является придать данным вид более похожий на нормальное распределение, с которым работает бОльшее число моделей и сходимость лучше \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Категориальных признаков "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding \n",
    "\n",
    "Просто берем и каждой категории однозначно сопоставляем число. \n",
    "Очень простой способ, если признак ординальный - будет работать почти всегда. \n",
    "\n",
    "Если же наш признак - номинальный, то могут возникнуть проблемы. Мы не можем сказать, что салатовый больше красного (в большинстве случаев). Но модель ничего про это не знает и после нашего кодирования спокойно такие сравнения может производить. Это может приводить к более низкому качеству модели и выучиванию ею неправильной информации. Кроме того, например, деревьям решений, чтобы выделить в таком случае конкретную категорию придется делать сразу несколько действий, которые, в силу жадности алгоритма их построения, могут и не быть найдены\n",
    "\n",
    "\n",
    "\n",
    " Некоторые модели (например, lightgbm) автоматически могут перекодировать все правильно, если им сообщить, что переданный признак - категориальный. Для некоторых это придется делать вручную. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding \n",
    "\n",
    "Схемой, которая часто используется на практике, является one-hot encoding. Он состоит том, что вместо одного категориального признака X создается набор бинарных категориальных признаков, которые отвечают на вопрос \"X == C? \", где C пробегает все возможные значения категориального признака. \n",
    "\n",
    "Теперь чтобы обусловиться на конкретное значение категориального признака, дереву решений достаточно задать один вопрос. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/one_hot_encoding.png\" width=\"450\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у такой схемы есть один минус - мы получаем линейно зависимые признаки. Это может плохо влиять на некоторые модели (для случая нейронных сетей - обычно нет, но полезно держать в голове). \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/problem_of_ohe.png\" width=\"850\">\n",
    "\n",
    "Потому иногда одну из категорий исключают при кодировании, например, в примере выше можно исключить Fish, ведь если все три других признака-категории равны 0, то точно верно, что категория - Fish. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target encoding \n",
    "\n",
    "Кодируем каждую категорию каким-то численным параметром, характеризующим то, что мы предсказываем. Например, можно каждую категорию категориального признака заменять на среднее \n",
    "\n",
    "На самом деле, так просто делать нельзя, можно получить переобученную модель. Как делать - можете подробно посмотреть, к примеру, [здесь](https://github.com/Dyakonov/PZAD/blob/master/2020/PZAD2020_042featureengineering_07.pdf) или [здесь](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv?redirectTo=%2Flecture%2Fcompetitive-data-science%2Fconcept-of-mean-encoding-b5Gxv) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "Можно научить вашу модель саму сопоставлять каждой категории некий вектор определенной размерности. Для этого вначале сопоставляем каждой категории случайный вектор заданной длины. А далее изменяем этот вектор как обычные веса. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/embedding.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование циклических категориальных признаков\n",
    "\n",
    "В случае с такими признаками, как день недели или время суток, мы сталкиваемся с проблемой того, что нам нужно предложить кодирование, которое будет учитывать, что понедельник близок к воскресенью так же, как понедельник же ко вторнику, и тд. \n",
    "\n",
    "В случае деревьев решений и методов на них основанных можно \"забить\" - такие методы сами разберутся. Для некоторых других методов, тех же нейросетей, правильно кодирование может улучшить качество и сходимость. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Давайте нанесем наши категории, например, дни недели - на окружность. Как это сделать? \n",
    "Пусть понедельнику соответствует 1, а воскресенью - 7. Далее посчитаем два таких вспомогательных признака по следующим формулам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weekdays = np.arange(1, 8) #create an array of weekdays\n",
    "print(weekdays)\n",
    "sina = np.sin(weekdays * np.pi * 2 / np.max(weekdays)) #feature 1\n",
    "cosa = np.cos(weekdays * np.pi * 2 / np.max(weekdays)) #feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7,7)) #Decide figure size\n",
    "plt.scatter(sina, cosa) #Plot scatter of feature 1 vs feature 2\n",
    "for  i, z in enumerate( (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\") ): #for each day in a week\n",
    "  plt.text(sina[i], cosa[i], s=z) #add text labels to plot\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать дальше? А по сути, мы уже все сделали. Теперь расстояния между понедельником и вторником и воскресеньем и понедельником одинаковые:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mon_tue = (sina[1] - sina[0]) ** 2 + (cosa[1] - cosa[0]) ** 2 #distance between Monday and Tuesday\n",
    "dist_sun_mon = (sina[6] - sina[0]) ** 2 + (cosa[6] - cosa[0]) ** 2 #distance between Sunday and Monday\n",
    "print('Distance between Mon-Tue = %.2f' % dist_mon_tue)\n",
    "print('Distance between Sun-Mon = %.2f' % dist_sun_mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "то же самое верно и для любых отстоящих друг от друга на одинаковое число дней\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mon_wed = (sina[2] - sina[0]) ** 2 + (cosa[2] - cosa[0]) ** 2 #distance between Monday and Wednesday\n",
    "dist_fri_sun = (sina[4] - sina[6]) ** 2 + (cosa[4] - cosa[6]) ** 2 #distance between Friday and Sunday\n",
    "print('Distance between Mon-Wed = %.2f' % dist_mon_wed)\n",
    "print('Distance between Fri-Sun = %.2f' % dist_fri_sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, циклические признаки можно кодировать парой признаков - sin и cos, полученных по схеме, описанной выше. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемы подхода\n",
    "\n",
    "1. Деревья решений могут решить задачу и так. А такое кодирование им, наоборот, будет мешать, т.к. они работают с одним признаком за раз\n",
    "\n",
    "2. Надо понимать, что важность исходной категориальной фичи неочевидным образом делится между двумя полученными из нее таким образом. \n",
    "\n",
    "3. В некоторых задачах one-hot работает лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование взаимодействия признаков\n",
    "\n",
    "Признаки могут по-разному взаимодейстовать и некоторые модели в принципе не могут моделировать это взаимодействие. \n",
    "\n",
    "\n",
    "Взаимодействовать могут вещественные переменные и категориальные\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/strength_vs_speed.png\" width=\"550\">\n",
    "\n",
    "**Сила vs. скорость**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "категориальные и категориальные \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/categorical_and_categorical.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "вещественные и вещественные \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/real_and_real.png\" width=\"300\">\n",
    "\n",
    "\n",
    "Могут быть и более высокоуровневые взаимодействия - взаимодействуют много разных признаков.\n",
    "\n",
    "Взаимодействия могут быть самые разные - много способов кодировать. Например, добавлять в число признаков их произведение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация признаков при помощи модели\n",
    "\n",
    "Если у вас есть модель, обученная на другом датасете, можно генерировать признаки при помощи нее. Например, при помощи случайного леса\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/generating_features_using_model.png\" width=\"700\">\n",
    "\n",
    "**Генерация бинарного признакового пространства с помощью RandomForest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошие источники: \n",
    "\n",
    "1. [Feature Selection for High-Dimensional Data](https://www.springer.com/gp/book/9783319218571)\n",
    "2. [How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science)\n",
    "3. **Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists Paperback** – April 14, 2018 by Alice Zheng , Amanda Casar\n",
    "4. [Сайт](https://dyakonov.org/) и [курс](https://github.com/Dyakonov/PZAD) Дьяконова\n",
    "5. Серия статей на towardsdatascience, [первая из серии](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "6. [A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "7. [Про кодирование циклических признаков](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Визуализация признаков\n",
    " (то есть до этого делаем все без картинок!!!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация данных\n",
    "(ибо поврторение - мать заикания)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Понятие линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример простой линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ненадолго отвлечемся от SVM и рассмотрим другую задачу. В этой задаче мы будем прогнозировать успеваемость студента, в зависимости от количества часов, которые он учил материал. Это простая задача линейной регрессии, поскольку она включает всего две переменные.\n",
    "\n",
    "**Регрессия** - это статистический метод, используемый в финансах, инвестировании и других дисциплинах, который пытается определить силу и характер связи между одной зависимой переменной (обычно обозначаемой **Y**) и рядом других переменных (известных как независимые переменные). В задачах регрессии, мы будем пытаться минимизировать ошибку между предсказанием и истинными данными. \n",
    "\n",
    "\"Регрессия\" происходит от слова \"регресс\", которое, в свою очередь, происходит от латинского \"regressus\" - возвращаться (к чему-либо). В этом смысле регрессия - это техника, которая позволяет \"вернуться назад\" от беспорядочных, трудно интерпретируемых данных к более четкой и осмысленной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L02/student_scores.csv\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что там в нем. Видим, что у нас есть два признака - часы и результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"/content/student_scores.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости одного от другого, а так же отобразим распределения каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=dataset, x=\"Scores\", y=\"Hours\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.iloc[:, :-1].values # column Hours\n",
    "Y = dataset.iloc[:, 1].values # column Score\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель для линейной регрессии. Чтобы не писать с нуля, воспользуемся готовой моделью из библиотеки `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_points = np.linspace(\n",
    "    min(X_train), max(X_train), 100\n",
    ")  # 100 dots at min to max\n",
    "Y_pred = regressor.predict(X_points)  \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X_train, Y_train, \"o\", label=\"Scores\")\n",
    "plt.plot(X_points, Y_pred, label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_ ))\n",
    "plt.title(\"Hours vs Percentage\", size=15)\n",
    "plt.xlabel(\"Hours Studied\", size=15)\n",
    "plt.ylabel(\"Percentage Score\", size=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем предсказание для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regressor.predict(X_test)  \n",
    "\n",
    "X_points = np.linspace(\n",
    "    min(X_test), max(X_test), 100\n",
    ")  \n",
    "Y_pred = regressor.predict(X_points) \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X_test, Y_test, \"o\", label=\"Scores\")\n",
    "plt.plot(X_points, Y_pred, label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_ ))\n",
    "plt.title(\"Hours vs Percentage\", size=15)\n",
    "plt.xlabel(\"Hours Studied\", size=15)\n",
    "plt.ylabel(\"Percentage Score\", size=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит не плохо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики для наших значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "print(\"Mean Absolute Error: %9.2f\" % metrics.mean_absolute_error(Y_test, Y_pred))\n",
    "print(\"Mean Squared Error: %10.2f\" % metrics.mean_squared_error(Y_test, Y_pred))\n",
    "print(\"Root Mean Squared Error: %5.2f\" % np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Геометрическая интерпретация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрались с тем, что такое регрессия и с чем ее едят, вернемся к нашим картинкам. Как можно применить регрессию для классификации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим у нас есть только 2 класса. Как можно использовать регрессию для того, чтобы определить относится ли изображение к классу 0 или к классу 1? В упрощенном варианте, задача будет состоять в том, чтобы провести разделяющую плоскость (прямую) между 2-мя классами. Например, мы можем провести прямую через 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/regression_for_classification_imgs.png\" width=\"270\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другую ситуацию, в этом случае, мы не можем просто провести прямую через 0. Но можем отступить от 0 на какое-то расстояние и провести ее там. Вспомним, что уравнение прямой это $y=wx+b$, где $b$ - это смещение (*bias*). Соответственно если b != 0, то прямая через 0 проходить не будет, а будет проходить через значение b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/regression_for_classification_add_bias.png\" width=\"270\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Classification Loss Visualization\n",
    "](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)\n",
    "\n",
    "\n",
    "Если у нас есть несколько классов (несколько шаблонов), мы можем для каждого из них посчитать уравнение $y_{i} = w_{i}x_{i}+b_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/regression_for_classification_add_bias_add_multiclasses.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На картинке нас интересуют 3 класса. Соответственно, мы можем записать систему линейных уравнений:\n",
    "\n",
    "\\begin{aligned}\n",
    "y_{0} = w_{0}x_{0} + b_{0} \\\\\n",
    "y_{1} = w_{1}x_{1} + b_{1} \\\\\n",
    "y_{2} = w_{2}x_{2} + b_{2} \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики регрессии (MSE, MAE, R2)\n",
    "MSE - повторение мать заикания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналитический расчет производной от функции потерь SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые производные \n",
    "\n",
    "$$x' = \\frac {\\delta x} {\\delta x} = 1$$ \n",
    "\n",
    "$$(x^2)' = \\frac {\\delta x^2} {\\delta x} = 2x$$\n",
    "\n",
    "$$(\\log x)'  = \\frac {\\delta \\log x} {\\delta x} = \\frac 1 x $$\n",
    "\n",
    "$$(e^x)'  = \\frac {\\delta e^x} {\\delta x} = e^x $$\n",
    "\n",
    "$$\\frac {\\delta cf(x)} {\\delta x}= c \\cdot \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$$\\frac {\\delta f(x) + c} {\\delta x}= \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$c$ - константа, не зависящая от $x$\n",
    "\n",
    "$$ \\frac {\\delta [f(x) + g(x)]} {\\delta x} = \\frac {\\delta f(x)} {\\delta x}  + \\frac {\\delta g(x)} {\\delta x} $$ \n",
    "\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta x} = 2x  $$\n",
    "так как $y$ по отношению к $x$ - константа и мы меняем только $x$\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta y} = 3y^2  $$\n",
    "так как $x$ по отношению к $y$ - константа и мы меняем только $y$\n",
    "\n",
    "$$(e^y)'  = \\frac {\\delta e^y} {\\delta x} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-rule\n",
    "\n",
    "Производная функции $f(g)$:\n",
    "\n",
    "$$\\frac {\\delta f} {\\delta g}$$\n",
    "\n",
    "Пусть $g$  на самом деле не просто переменная, а зависит от $h$. Тогда производная от $f$ по $g$ **не меняется**, а производная $f$ по $h$ запишется следующим образом:\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h))} {\\delta h} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h}$$\n",
    "\n",
    "Пусть теперь $h$ зависит от $x$. Все аналогично\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h(x)))} {\\delta x} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h} \\frac {\\delta h} {\\delta x}$$\n",
    "\n",
    "Так можно делать до бесконечности, находя производную сколь угодно сложной функции. И, что важно - мы можем считать градиенты частями - посчитать сначала $f$ по $g$, потом $g$ по $h$....\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x}$$\n",
    "\n",
    "$$h = x^2 + 5$$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac {\\delta log(h)} {\\delta h} \\frac {\\delta h} {\\delta x}$$ \n",
    "\n",
    "$$\\frac {\\delta log(h)} {\\delta h} = \\frac 1 h$$\n",
    "\n",
    "$$\\frac  {\\delta h} {\\delta x} = 2x $$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac 1 {x^2 + 5} \\cdot 2x = \\frac {2x} {x^2 + 5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть MSE-loss\n",
    "\n",
    "$$loss = (y - \\hat{y})^2 $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta (y - \\hat{y})^2 } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = 2(y-\\hat{y}) \\cdot -1 = 2 (\\hat{y} - y)$$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = 2 x \\cdot (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE-loss\n",
    "$$MSE = \\frac 1 N \\sum_i(y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "$y_i$ - константы\n",
    "$\\hat{y_i}$ - не являются функциями друг от друга \n",
    "$$\\hat{y} = wx_i + b $$\n",
    "\n",
    "$$\\frac {\\delta MSE} {\\delta w} = \\frac 1 N \\sum \\frac {\\delta (y_i - \\hat{y_i}) ^2} {\\delta \\hat{y_i}} \\frac {\\delta \\hat{y_i}} {\\delta w}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть MAE-Loss\n",
    "\n",
    "$$loss = |y - \\hat{y}| $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}}$$\n",
    "\n",
    "Строго говоря, у модуля не существует производной в 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [i for i in range(-5, 6)]\n",
    "Y = [abs(i) for i in range(-5, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X, Y, label=\"y = |x|\")\n",
    "plt.title(\"y = |x|\", size=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы можем сказать, что в этой точке производная равна 0.\n",
    "Если аргумент модуля меньше 0, то производная будет -1.\n",
    "\n",
    "Если больше +1\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} = - sign(y - \\hat{y}) =  sign(\\hat{y} - y)$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i for i in range(-5, 1, 1)]\n",
    "Y = [i * 0 - 1 for i in range(6)]\n",
    "X_1 = [i for i in range(0, 6)]\n",
    "Y_1 = [i * 0 + 1 for i in range(0, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X, Y, \"b\")\n",
    "plt.plot(X_1, Y_1, \"b\")\n",
    "plt.plot(0, 0, \"ro\")\n",
    "plt.plot(0, 1, \"bo\")\n",
    "plt.plot(0, -1, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Loss\n",
    "\n",
    "$$b = max(x, y)$$\n",
    "\n",
    "$$b = x~~if~~x > y~~else~~y$$\n",
    "\n",
    "\n",
    "$$\\frac {\\delta b} {\\delta x} =  \\frac {\\delta x} {\\delta x}~~if~~x > y~~else~~\\frac {\\delta y} {\\delta x} = 1~~if~~x > y~~else~~0$$\n",
    "\n",
    "Если $x > y$, то он оказал влияние на $b$. Иначе, его вклада в $b$ НЕ БЫЛО - градиент равен 0\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из: \n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем:\n",
    "\n",
    "$ \\nabla_W\tL(W) = {1 \\over N}\\sum_{i=1}^N \\nabla_W L_i(x_i, y_i, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{blue}{\\text{*Не обязательное задание:}}$\n",
    "\n",
    "Поставлю дополнительно 20 баллов тому, кто графически оформит результаты этого эксперимента и напишет вывод о зависимости качества обучения от learning_rate и batch_size (возможно, нужно будет добавить варианты их сочетаний и/или увеличить количество эпох до 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "file_exists = os.path.exists(\"/content/cifar-10-batches-py\")\n",
    "if file_exists == False:\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !tar -xzf cifar-10-python.tar.gz   \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "X_train = np.zeros((0, 3072))\n",
    "Y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    X_train = np.append(X_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    Y_train = np.append(Y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "X_test = np.array(test[b\"data\"])\n",
    "Y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_eng = [\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_train shape: {X_test.shape}, Y_train shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LinearClassifier():\n",
    "    def __init__(self, labels, batch_size, random_state=42):\n",
    "        self.labels = labels  # classes names\n",
    "        self.classes_num = len(labels)  # num of classes\n",
    "\n",
    "        np.random.seed(\n",
    "            random_state\n",
    "        )  \n",
    "        self.W = (\n",
    "            np.random.randn(3073, self.classes_num) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias \n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, X_train, Y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # обнуляем loss\n",
    "        train_len = X_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)  \n",
    "\n",
    "        for i in range(\n",
    "            0, train_len, self.batch_size\n",
    "        ):  \n",
    "            idx = indexes[\n",
    "                i : i + self.batch_size\n",
    "            ]  # \n",
    "            X_batch = X_train[idx]  \n",
    "            Y_batch = Y_train[idx]  \n",
    "\n",
    "            X_batch = np.hstack(\n",
    "                [X_batch, np.ones((X_batch.shape[0], 1))]\n",
    "            )  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(X_batch, Y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        current_batch_size = X.shape[0]  # batch_size\n",
    "        loss = 0.0  \n",
    "        dW = np.zeros(self.W.shape) \n",
    "        for i in range(current_batch_size):  \n",
    "            scores = X[i].dot(\n",
    "                self.W\n",
    "            )  # vector of shape 10\n",
    "            correct_class_score = scores[\n",
    "                int(Y[i])\n",
    "            ]  \n",
    "            above_zero_loss_count = 0  \n",
    "            for j in range(self.classes_num): \n",
    "                if j == Y[i]:  # predict class\n",
    "                    continue\n",
    "                margin = scores[j] - correct_class_score + 1  # loss\n",
    "                if margin > 0:  \n",
    "                    above_zero_loss_count += (\n",
    "                        1  \n",
    "                    )\n",
    "                    loss += margin  # \n",
    "                    dW[:, j] += X[i]  # \n",
    "            dW[:, int(Y[i])] -= above_zero_loss_count * X[i]  \n",
    "        loss /= current_batch_size  \n",
    "        dW /= current_batch_size  \n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = np.append(X, 1)  # add 1 (bias)\n",
    "        scores = X.dot(self.W)  \n",
    "        return np.argmax(scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, X_test, Y_test, noprint=False):\n",
    "    correct = 0  \n",
    "    for i, img in enumerate(X_test):  \n",
    "        index = model.forward(img)  \n",
    "        correct += (\n",
    "            1 if index == Y_test[i] else 0\n",
    "        )  \n",
    "        if noprint is False:  \n",
    "            if i > 0 and i % 1000 == 0:  \n",
    "                print(\n",
    "                    \"Accuracy {:.3f}\".format(correct / i)\n",
    "                )  \n",
    "    return correct / len(Y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How learning quality depends of speed:\")\n",
    "\n",
    "for lr in [1e-2, 1e-8]:\n",
    "    for bs in [256, 2048]:\n",
    "\n",
    "        print(\"-\" * 50, \"\\n\", \"learning_rate =\", lr, \"\\tbatch_size =\", bs)\n",
    "        print()\n",
    "        lc_model = LinearClassifier(labels_eng, batch_size=bs)\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(10):\n",
    "            loss = lc_model.fit(X_train, Y_train, learning_rate=lr)\n",
    "            accuracy = validate(lc_model, X_test, Y_test, noprint=True)\n",
    "            if best_accuracy < accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_epoch = epoch\n",
    "            print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance tradeoff на примере линейной регрессии и KNN\n",
    "(просто переписать все)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias, Variance, Irreducible error \n",
    "\n",
    "Можно показать, что ошибка любой модели раскладывается в сумму трех компонент:\n",
    "\n",
    "$$ Model\\_error = Bias^2 + Variance + Irreducible\\_error $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias<a class=\"anchor\" style=\"autocontent\" id=\"Bias\"/><br>\n",
    "Обычно, высокий bias имеют недообученные модели. Например, реальная зависимость, которую мы наблюдаем - нелинейная, а мы пытаемся аппроксимировать ее линией. В этом случае наше решение заведомо смещено (biassed) в сторону линейной модели и мы всегда будем ошибаться в сравнении с реальной моделью данных\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L03/img_license/problem_of_hight_bias.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance\n",
    "Можно получить и обратную ситуацию. Реальная закономерность имеет линейный вид, а мы пытаемся ее аппроксимировать нелинейной моделью. \n",
    "В этом случае мы будем выучивать любой шум в данных и пытаться объяснить его нашей моделью. \n",
    "Малое изменение в данных будет приводить к большим изменениям в прогнозе модели. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L03/img_license/problem_of_hight_variance.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда bias и variance представляет еще таким образом:\n",
    "1. можно быть очень точным и попадать всегда в центр мишени - это соответствует низкому bias и низкому variance;\n",
    "2. можно попадать примерно в центр мишени, но при этом с большим разбросом - низкий bias, но высокий variance;\n",
    "3. можно стрелять кучно, но не туда) - это высокий bias и низкий variance;\n",
    "4. ну а можно просто стрелять наугад, куда душа зовет - это высокий bias и высокий variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L03/img_license/low_hight_bias_variance.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irreducible error<a class=\"anchor\" style=\"autocontent\" id=\"Irreducible-error\"/><br>\n",
    "В идеальном для нас случае - когда мы угадали с моделью наших данных, гипотетически можно получить. $$Bias=0, Variance=0$$\n",
    "Однако, у нас есть ошибки в измерении самой предсказываемой величины. Из-за этого наша модель всегда будет иметь некий уровень ошибки, ниже которого опуститься нельзя. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias vs variance<a class=\"anchor\" style=\"autocontent\" id=\"Bias-vs-variance\"/><br>\n",
    "\n",
    "В реальности же, когда реальную модель данных угадать в точности почти невозможно, есть bias-variance tradeoff - нельзя бесконечно уменьшать и Bias, и Variance. Есть какая-то точка оптимума. С какого-то момента при уменьшении Bias начнет увеличиваться Variance, и наоборот. \n",
    "При этом, можно построить  связь этих величин с увеличением сложности модели (capacity)\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L03/img_license/bias_variance_tradeoff.png\" width=\"450\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применительно к деревьям<a class=\"anchor\" style=\"autocontent\" id=\"Применительно-к-деревьям\"/><br>\n",
    "Дерева малой глубины имеет малую сложность - и высокий bias. \n",
    "Дерево большой глубины имеет высокую сложность - и высокий variance. \n",
    "\n",
    "Можно подобрать для дерева идеальную capacity, когда Bias и Variance будут суммарно давать наименьший вклад в ошибку. Этим мы занимаемся при подборе параметров. \n",
    "Но, оказывается, есть и другие способы борьбы с variance и/или bias, которые мы разберем позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что если бы могли говорить не просто решение дерева, а привязывать к этому какую-то статистику, например, сколько деревьев, построенных по подобной процедуре, приняли такое же решение - было бы легче.\n",
    "\n",
    "Если наложить решающие границы 100 решающих деревьев, построенных на разных выборках из X, y, то мы увидим, что \"хорошие области\", соответствующие реальному разделению данных будут общими между деревьями, а плохие - индивидуальны. \n",
    "К сожалению, в реальности, мы не можем брать бесконечное число наборов данных из генеральной совокупности (представленной в данном случае X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    " \n",
    "for i in range(1,101):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split( X, Y, random_state = i)\n",
    "    clf = DecisionTreeClassifier(max_depth = 20, random_state = 0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    plot_decision_boundary(clf, X, Y, alpha = 0.02, contour=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возврат к классификации\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## на примере SVM лосс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аналитический расчет производной от функции потерь SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые производные \n",
    "\n",
    "$$x' = \\frac {\\delta x} {\\delta x} = 1$$ \n",
    "\n",
    "$$(x^2)' = \\frac {\\delta x^2} {\\delta x} = 2x$$\n",
    "\n",
    "$$(\\log x)'  = \\frac {\\delta \\log x} {\\delta x} = \\frac 1 x $$\n",
    "\n",
    "$$(e^x)'  = \\frac {\\delta e^x} {\\delta x} = e^x $$\n",
    "\n",
    "$$\\frac {\\delta cf(x)} {\\delta x}= c \\cdot \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$$\\frac {\\delta f(x) + c} {\\delta x}= \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$c$ - константа, не зависящая от $x$\n",
    "\n",
    "$$ \\frac {\\delta [f(x) + g(x)]} {\\delta x} = \\frac {\\delta f(x)} {\\delta x}  + \\frac {\\delta g(x)} {\\delta x} $$ \n",
    "\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta x} = 2x  $$\n",
    "так как $y$ по отношению к $x$ - константа и мы меняем только $x$\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta y} = 3y^2  $$\n",
    "так как $x$ по отношению к $y$ - константа и мы меняем только $y$\n",
    "\n",
    "$$(e^y)'  = \\frac {\\delta e^y} {\\delta x} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-rule\n",
    "\n",
    "Производная функции $f(g)$:\n",
    "\n",
    "$$\\frac {\\delta f} {\\delta g}$$\n",
    "\n",
    "Пусть $g$  на самом деле не просто переменная, а зависит от $h$. Тогда производная от $f$ по $g$ **не меняется**, а производная $f$ по $h$ запишется следующим образом:\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h))} {\\delta h} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h}$$\n",
    "\n",
    "Пусть теперь $h$ зависит от $x$. Все аналогично\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h(x)))} {\\delta x} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h} \\frac {\\delta h} {\\delta x}$$\n",
    "\n",
    "Так можно делать до бесконечности, находя производную сколь угодно сложной функции. И, что важно - мы можем считать градиенты частями - посчитать сначала $f$ по $g$, потом $g$ по $h$....\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x}$$\n",
    "\n",
    "$$h = x^2 + 5$$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac {\\delta log(h)} {\\delta h} \\frac {\\delta h} {\\delta x}$$ \n",
    "\n",
    "$$\\frac {\\delta log(h)} {\\delta h} = \\frac 1 h$$\n",
    "\n",
    "$$\\frac  {\\delta h} {\\delta x} = 2x $$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac 1 {x^2 + 5} \\cdot 2x = \\frac {2x} {x^2 + 5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть MSE-loss\n",
    "\n",
    "$$loss = (y - \\hat{y})^2 $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta (y - \\hat{y})^2 } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = 2(y-\\hat{y}) \\cdot -1 = 2 (\\hat{y} - y)$$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = 2 x \\cdot (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE-loss\n",
    "$$MSE = \\frac 1 N \\sum_i(y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "$y_i$ - константы\n",
    "$\\hat{y_i}$ - не являются функциями друг от друга \n",
    "$$\\hat{y} = wx_i + b $$\n",
    "\n",
    "$$\\frac {\\delta MSE} {\\delta w} = \\frac 1 N \\sum \\frac {\\delta (y_i - \\hat{y_i}) ^2} {\\delta \\hat{y_i}} \\frac {\\delta \\hat{y_i}} {\\delta w}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть MAE-Loss\n",
    "\n",
    "$$loss = |y - \\hat{y}| $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}}$$\n",
    "\n",
    "Строго говоря, у модуля не существует производной в 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [i for i in range(-5, 6)]\n",
    "Y = [abs(i) for i in range(-5, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X, Y, label=\"y = |x|\")\n",
    "plt.title(\"y = |x|\", size=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы можем сказать, что в этой точке производная равна 0.\n",
    "Если аргумент модуля меньше 0, то производная будет -1.\n",
    "\n",
    "Если больше +1\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} = - sign(y - \\hat{y}) =  sign(\\hat{y} - y)$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i for i in range(-5, 1, 1)]\n",
    "Y = [i * 0 - 1 for i in range(6)]\n",
    "X_1 = [i for i in range(0, 6)]\n",
    "Y_1 = [i * 0 + 1 for i in range(0, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X, Y, \"b\")\n",
    "plt.plot(X_1, Y_1, \"b\")\n",
    "plt.plot(0, 0, \"ro\")\n",
    "plt.plot(0, 1, \"bo\")\n",
    "plt.plot(0, -1, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Loss\n",
    "\n",
    "$$b = max(x, y)$$\n",
    "\n",
    "$$b = x~~if~~x > y~~else~~y$$\n",
    "\n",
    "\n",
    "$$\\frac {\\delta b} {\\delta x} =  \\frac {\\delta x} {\\delta x}~~if~~x > y~~else~~\\frac {\\delta y} {\\delta x} = 1~~if~~x > y~~else~~0$$\n",
    "\n",
    "Если $x > y$, то он оказал влияние на $b$. Иначе, его вклада в $b$ НЕ БЫЛО - градиент равен 0\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из: \n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем:\n",
    "\n",
    "$ \\nabla_W\tL(W) = {1 \\over N}\\sum_{i=1}^N \\nabla_W L_i(x_i, y_i, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{blue}{\\text{*Не обязательное задание:}}$\n",
    "\n",
    "Поставлю дополнительно 20 баллов тому, кто графически оформит результаты этого эксперимента и напишет вывод о зависимости качества обучения от learning_rate и batch_size (возможно, нужно будет добавить варианты их сочетаний и/или увеличить количество эпох до 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "file_exists = os.path.exists(\"/content/cifar-10-batches-py\")\n",
    "if file_exists == False:\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !tar -xzf cifar-10-python.tar.gz   \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "X_train = np.zeros((0, 3072))\n",
    "Y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    X_train = np.append(X_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    Y_train = np.append(Y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "X_test = np.array(test[b\"data\"])\n",
    "Y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_eng = [\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_train shape: {X_test.shape}, Y_train shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LinearClassifier():\n",
    "    def __init__(self, labels, batch_size, random_state=42):\n",
    "        self.labels = labels  # classes names\n",
    "        self.classes_num = len(labels)  # num of classes\n",
    "\n",
    "        np.random.seed(\n",
    "            random_state\n",
    "        )  \n",
    "        self.W = (\n",
    "            np.random.randn(3073, self.classes_num) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias \n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, X_train, Y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # обнуляем loss\n",
    "        train_len = X_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)  \n",
    "\n",
    "        for i in range(\n",
    "            0, train_len, self.batch_size\n",
    "        ):  \n",
    "            idx = indexes[\n",
    "                i : i + self.batch_size\n",
    "            ]  # \n",
    "            X_batch = X_train[idx]  \n",
    "            Y_batch = Y_train[idx]  \n",
    "\n",
    "            X_batch = np.hstack(\n",
    "                [X_batch, np.ones((X_batch.shape[0], 1))]\n",
    "            )  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(X_batch, Y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        current_batch_size = X.shape[0]  # batch_size\n",
    "        loss = 0.0  \n",
    "        dW = np.zeros(self.W.shape) \n",
    "        for i in range(current_batch_size):  \n",
    "            scores = X[i].dot(\n",
    "                self.W\n",
    "            )  # vector of shape 10\n",
    "            correct_class_score = scores[\n",
    "                int(Y[i])\n",
    "            ]  \n",
    "            above_zero_loss_count = 0  \n",
    "            for j in range(self.classes_num): \n",
    "                if j == Y[i]:  # predict class\n",
    "                    continue\n",
    "                margin = scores[j] - correct_class_score + 1  # loss\n",
    "                if margin > 0:  \n",
    "                    above_zero_loss_count += (\n",
    "                        1  \n",
    "                    )\n",
    "                    loss += margin  # \n",
    "                    dW[:, j] += X[i]  # \n",
    "            dW[:, int(Y[i])] -= above_zero_loss_count * X[i]  \n",
    "        loss /= current_batch_size  \n",
    "        dW /= current_batch_size  \n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = np.append(X, 1)  # add 1 (bias)\n",
    "        scores = X.dot(self.W)  \n",
    "        return np.argmax(scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, X_test, Y_test, noprint=False):\n",
    "    correct = 0  \n",
    "    for i, img in enumerate(X_test):  \n",
    "        index = model.forward(img)  \n",
    "        correct += (\n",
    "            1 if index == Y_test[i] else 0\n",
    "        )  \n",
    "        if noprint is False:  \n",
    "            if i > 0 and i % 1000 == 0:  \n",
    "                print(\n",
    "                    \"Accuracy {:.3f}\".format(correct / i)\n",
    "                )  \n",
    "    return correct / len(Y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How learning quality depends of speed:\")\n",
    "\n",
    "for lr in [1e-2, 1e-8]:\n",
    "    for bs in [256, 2048]:\n",
    "\n",
    "        print(\"-\" * 50, \"\\n\", \"learning_rate =\", lr, \"\\tbatch_size =\", bs)\n",
    "        print()\n",
    "        lc_model = LinearClassifier(labels_eng, batch_size=bs)\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(10):\n",
    "            loss = lc_model.fit(X_train, Y_train, learning_rate=lr)\n",
    "            accuracy = validate(lc_model, X_test, Y_test, noprint=True)\n",
    "            if best_accuracy < accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_epoch = epoch\n",
    "            print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обновления весов методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/backpropagation_weight_optimization.png\" width=\"750\">\n",
    "\n",
    "Идея в следующем: у нас есть некоторая поверхность, на которой будет точка минимума функции. Мы должны обновлять веса таким образом, чтобы смещаться в сторону этой точки. Loss показывает, как возрастает функция потерь. Так как потери - это плохо, мы должны их минимизировать, то есть мы должны уменьшать веса в сторону, обратную росту этой функции.\n",
    "\n",
    "Если переходить на случай n-мерного (в данном случае трехмерного) пространства, здесь достаточно очевидна аналогия с поверхностью: у нас есть поверхность земли, которая описывается координатами $x, y, z$. Если мы движемся по этой поверхности, высота $(z)$ будет зависеть от $x, y$. \n",
    "\n",
    "$x, y$ - это координаты. Мы можем записать их как вектор. Наша функция будет работать с вектором координат и выдавать скаляр, третью координату. Если в качестве координат мы будем использовать веса нашей модели, а в качестве $z$ - loss, то аналогия станет полной. Наша задача сведется к тому, чтобы найти такой набор весов, при котором значение функции будет минимально. То есть мы окажемся в каком-то минимуме, где ошибка будет минимально возможна для этих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы предположить, где он может находиться, надо понимать, в какую сторону функция растет, а в какую - убывает. Для этого существует производная.\n",
    "\n",
    "Поскольку у нас функция от нескольких переменных, если мы будем брать от нее производную, у нас получится вектор частных производных, то есть градиент этой функции, который будет показывать, как она меняется в каждом направлении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент функции потерь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент — вектор, своим направлением указывающий направление наибольшего возрастания некоторой величины **φ**, значение которой меняется от одной точки пространства к другой (скалярного поля), а по величине (модулю) равный скорости роста этой величины в этом направлении.\n",
    "\n",
    "Например, если взять в качестве **φ**  высоту поверхности земли над уровнем моря, то её градиент в каждой точке поверхности будет показывать «направление самого крутого подъёма», и своей величиной характеризовать крутизну склона.\n",
    "\n",
    "Другими словами, градиент — это производная по пространству, но в отличие от производной по одномерному времени, градиент является не скаляром, а векторной величиной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случая трёхмерного пространства градиентом скалярной функции **φ=φ(x,y,z)** координат **(x,y,z)** называется векторная функция с компонентами\n",
    "$$\\frac{\\partial\\varphi}{\\partial x}, \\frac{\\partial\\varphi}{\\partial y}, \\frac{\\partial\\varphi}{\\partial z}$$.\n",
    "\n",
    "Если **φ**  — функция **n** переменных **X1...xn**, то её градиентом называется **n**-мерный вектор,\n",
    "$$\\frac{\\partial\\varphi}{\\partial x_1},...,\\frac{\\partial\\varphi}{\\partial x_n}$$\n",
    "\n",
    "\n",
    "\n",
    "компоненты которого, равны частным производным **φ** по всем её аргументам.\n",
    "\n",
    "Размерность вектора градиента определяется размерностью пространства (или многообразия), на котором задано скалярное поле, о градиенте которого идёт речь.\n",
    "Оператором градиента называется оператор, действие которого на скалярную функцию (поле) даёт её градиент. Этого оператора иногда коротко называют просто «градиентом»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$W$ - матрица(вектор) весов\n",
    "\n",
    "$L$ - функция потерь\n",
    "\n",
    "$\\partial W = W_2 - W_1$\n",
    "\n",
    "$\\partial L = L_2 - L_1$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W}=\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial W_1} \\\\\n",
    "\\frac{\\partial L}{\\partial W_2} \\\\\n",
    "... \\\\\n",
    "\\frac{\\partial L}{\\partial W_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Наша задача будет сводиться к тому, что мы будем искать градиент loss функции по весам, которые будут состоять из производной по каждому направлению. Поскольку у нас здесь числа, можно считать этот градиент численно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Численный расчет производной\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/gradient_descent_analytical_calculation.png\" width=\"650\">\n",
    "\n",
    "\n",
    "\n",
    "Посчитаем градиент приближенно, воспользовавшись определением (в формуле аргумент обозначен как $x$, у нас же аргументом будет $W$):\n",
    "На нулевом шаге у нас есть $W_0$ найдем $L_0 = Loss(f(W_0,x))$\n",
    "Прибавим к первому элементу $W_0$ небольшую величину  $h$ = 0.0001 и получим новую матрицу весов $W_1$ отличающуюся от $W_0$ на один единственный элемент.\n",
    "\n",
    "Найдем Loss от $\\frac {W_1}  {L_1} = Loss(f(W_1,x))$\n",
    "По определению производной $\\frac {dL}{W_0} = \\frac{( L_1 - L_0 )}   {h}$\n",
    "\n",
    "Повторяя этот процесс для каждого элемента из $W$, найдем вектор частных производных, то есть градиент $\\frac{dL}{dW}$.\n",
    "\n",
    "Плюсы:\n",
    "Это просто. \n",
    "\n",
    "Проблемы:\n",
    "\n",
    "1. Это очень долго, нам придется заново искать значение loss функции для каждого $W_i$.\n",
    "\n",
    "2. Это неточно, так как по определению приращение $h$ бесконечно мало, а мы используем конкретное пусть и небольшое число. И если мы сделаем его слишком маленьким, то столкнемся с ошибками связанными с округлением в памяти компьютера.\n",
    "\n",
    "\n",
    "Поэтому данный метод может использоваться как проверочный. \n",
    "А на практике вместо него используется **аналитический расчет градиента**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аналитический расчет производной от функции потерь SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Простые производные \n",
    "\n",
    "$$x' = \\frac {\\delta x} {\\delta x} = 1$$ \n",
    "\n",
    "$$(x^2)' = \\frac {\\delta x^2} {\\delta x} = 2x$$\n",
    "\n",
    "$$(\\log x)'  = \\frac {\\delta \\log x} {\\delta x} = \\frac 1 x $$\n",
    "\n",
    "$$(e^x)'  = \\frac {\\delta e^x} {\\delta x} = e^x $$\n",
    "\n",
    "$$\\frac {\\delta cf(x)} {\\delta x}= c \\cdot \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$$\\frac {\\delta f(x) + c} {\\delta x}= \\frac {\\delta f(x)} {\\delta x}$$\n",
    "\n",
    "$c$ - константа, не зависящая от $x$\n",
    "\n",
    "$$ \\frac {\\delta [f(x) + g(x)]} {\\delta x} = \\frac {\\delta f(x)} {\\delta x}  + \\frac {\\delta g(x)} {\\delta x} $$ \n",
    "\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta x} = 2x  $$\n",
    "так как $y$ по отношению к $x$ - константа и мы меняем только $x$\n",
    "\n",
    "$$\\frac {\\delta (x^2 + y^3)} {\\delta y} = 3y^2  $$\n",
    "так как $x$ по отношению к $y$ - константа и мы меняем только $y$\n",
    "\n",
    "$$(e^y)'  = \\frac {\\delta e^y} {\\delta x} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-rule\n",
    "\n",
    "Производная функции $f(g)$:\n",
    "\n",
    "$$\\frac {\\delta f} {\\delta g}$$\n",
    "\n",
    "Пусть $g$  на самом деле не просто переменная, а зависит от $h$. Тогда производная от $f$ по $g$ **не меняется**, а производная $f$ по $h$ запишется следующим образом:\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h))} {\\delta h} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h}$$\n",
    "\n",
    "Пусть теперь $h$ зависит от $x$. Все аналогично\n",
    "\n",
    "\n",
    "$$\\frac {\\delta f(g(h(x)))} {\\delta x} = \\frac {\\delta f} {\\delta g} \\frac {\\delta g} {\\delta h} \\frac {\\delta h} {\\delta x}$$\n",
    "\n",
    "Так можно делать до бесконечности, находя производную сколь угодно сложной функции. И, что важно - мы можем считать градиенты частями - посчитать сначала $f$ по $g$, потом $g$ по $h$....\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x}$$\n",
    "\n",
    "$$h = x^2 + 5$$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac {\\delta log(h)} {\\delta h} \\frac {\\delta h} {\\delta x}$$ \n",
    "\n",
    "$$\\frac {\\delta log(h)} {\\delta h} = \\frac 1 h$$\n",
    "\n",
    "$$\\frac  {\\delta h} {\\delta x} = 2x $$\n",
    "\n",
    "$$\\frac {\\delta log(x^2 + 5)} {\\delta x} = \\frac 1 {x^2 + 5} \\cdot 2x = \\frac {2x} {x^2 + 5}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Часть MSE-loss\n",
    "\n",
    "$$loss = (y - \\hat{y})^2 $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta (y - \\hat{y})^2 } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = 2(y-\\hat{y}) \\cdot -1 = 2 (\\hat{y} - y)$$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = 2 x \\cdot (\\hat{y} - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE-loss\n",
    "$$MSE = \\frac 1 N \\sum_i(y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "$y_i$ - константы\n",
    "$\\hat{y_i}$ - не являются функциями друг от друга \n",
    "$$\\hat{y} = wx_i + b $$\n",
    "\n",
    "$$\\frac {\\delta MSE} {\\delta w} = \\frac 1 N \\sum \\frac {\\delta (y_i - \\hat{y_i}) ^2} {\\delta \\hat{y_i}} \\frac {\\delta \\hat{y_i}} {\\delta w}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть MAE-Loss\n",
    "\n",
    "$$loss = |y - \\hat{y}| $$\n",
    "\n",
    "$$\\hat{y} = wx + b $$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta w} = \\frac {\\delta loss} {\\delta \\hat{y}} \\cdot \\frac {\\delta \\hat{y}} {\\delta w} $$\n",
    "\n",
    "$$ \\frac {\\delta \\hat{y}} {\\delta w} = \\frac {\\delta wx + b} {\\delta w} = x$$\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}|) } {\\delta {(y - \\hat{y})}}$$\n",
    "\n",
    "Строго говоря, у модуля не существует производной в 0. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [i for i in range(-5, 6)]\n",
    "Y = [abs(i) for i in range(-5, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X, Y, label=\"y = |x|\")\n",
    "plt.title(\"y = |x|\", size=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы можем сказать, что в этой точке производная равна 0.\n",
    "Если аргумент модуля меньше 0, то производная будет -1.\n",
    "\n",
    "Если больше +1\n",
    "\n",
    "$$ \\frac {\\delta loss} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\frac {\\delta y - \\hat{y}} {\\delta \\hat{y}} = \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} \\cdot -1 = - \\frac {\\delta |y - \\hat{y}| } {\\delta {(y - \\hat{y})}} = - sign(y - \\hat{y}) =  sign(\\hat{y} - y)$$\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i for i in range(-5, 1, 1)]\n",
    "Y = [i * 0 - 1 for i in range(6)]\n",
    "X_1 = [i for i in range(0, 6)]\n",
    "Y_1 = [i * 0 + 1 for i in range(0, 6)]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X, Y, \"b\")\n",
    "plt.plot(X_1, Y_1, \"b\")\n",
    "plt.plot(0, 0, \"ro\")\n",
    "plt.plot(0, 1, \"bo\")\n",
    "plt.plot(0, -1, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Loss\n",
    "\n",
    "$$b = max(x, y)$$\n",
    "\n",
    "$$b = x~~if~~x > y~~else~~y$$\n",
    "\n",
    "\n",
    "$$\\frac {\\delta b} {\\delta x} =  \\frac {\\delta x} {\\delta x}~~if~~x > y~~else~~\\frac {\\delta y} {\\delta x} = 1~~if~~x > y~~else~~0$$\n",
    "\n",
    "Если $x > y$, то он оказал влияние на $b$. Иначе, его вклада в $b$ НЕ БЫЛО - градиент равен 0\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из: \n",
    "\n",
    "$L_i=\\sum_{j\\neq y_i}max(0,s_j-s_{y_i}+1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем:\n",
    "\n",
    "$ \\nabla_W\tL(W) = {1 \\over N}\\sum_{i=1}^N \\nabla_W L_i(x_i, y_i, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{blue}{\\text{*Не обязательное задание:}}$\n",
    "\n",
    "Поставлю дополнительно 20 баллов тому, кто графически оформит результаты этого эксперимента и напишет вывод о зависимости качества обучения от learning_rate и batch_size (возможно, нужно будет добавить варианты их сочетаний и/или увеличить количество эпох до 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "file_exists = os.path.exists(\"/content/cifar-10-batches-py\")\n",
    "if file_exists == False:\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "    !tar -xzf cifar-10-python.tar.gz   \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "\n",
    "X_train = np.zeros((0, 3072))\n",
    "Y_train = np.array([])\n",
    "for i in range(1, 6):\n",
    "    raw = unpickle(f\"/content/cifar-10-batches-py/data_batch_{i}\")\n",
    "    X_train = np.append(X_train, np.array(raw[b\"data\"]), axis=0)\n",
    "    Y_train = np.append(Y_train, np.array(raw[b\"labels\"]), axis=0)\n",
    "\n",
    "test = unpickle(\"/content/cifar-10-batches-py/test_batch\")\n",
    "X_test = np.array(test[b\"data\"])\n",
    "Y_test = np.array(test[b\"labels\"])\n",
    "\n",
    "labels_eng = [\n",
    "    \"Airplane\",\n",
    "    \"Car\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\",\n",
    "]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_train shape: {X_test.shape}, Y_train shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LinearClassifier():\n",
    "    def __init__(self, labels, batch_size, random_state=42):\n",
    "        self.labels = labels  # classes names\n",
    "        self.classes_num = len(labels)  # num of classes\n",
    "\n",
    "        np.random.seed(\n",
    "            random_state\n",
    "        )  \n",
    "        self.W = (\n",
    "            np.random.randn(3073, self.classes_num) * 0.0001\n",
    "        )  # generate random weights, reshape to add bias \n",
    "        self.batch_size = batch_size  # batch_size\n",
    "\n",
    "    def fit(self, X_train, Y_train, learning_rate=1e-8):\n",
    "        loss = 0.0  # обнуляем loss\n",
    "        train_len = X_train.shape[0]  # num of examples\n",
    "        indexes = list(range(train_len))  # indexes train_len\n",
    "        random.shuffle(indexes)  \n",
    "\n",
    "        for i in range(\n",
    "            0, train_len, self.batch_size\n",
    "        ):  \n",
    "            idx = indexes[\n",
    "                i : i + self.batch_size\n",
    "            ]  # \n",
    "            X_batch = X_train[idx]  \n",
    "            Y_batch = Y_train[idx]  \n",
    "\n",
    "            X_batch = np.hstack(\n",
    "                [X_batch, np.ones((X_batch.shape[0], 1))]\n",
    "            )  # add bias\n",
    "\n",
    "            loss_val, grad = self.loss(X_batch, Y_batch)  # loss and gradient\n",
    "            self.W -= learning_rate * grad  # update weigths\n",
    "\n",
    "            loss += loss_val  # loss sum\n",
    "        return loss / (train_len)  # mean loss\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        current_batch_size = X.shape[0]  # batch_size\n",
    "        loss = 0.0  \n",
    "        dW = np.zeros(self.W.shape) \n",
    "        for i in range(current_batch_size):  \n",
    "            scores = X[i].dot(\n",
    "                self.W\n",
    "            )  # vector of shape 10\n",
    "            correct_class_score = scores[\n",
    "                int(Y[i])\n",
    "            ]  \n",
    "            above_zero_loss_count = 0  \n",
    "            for j in range(self.classes_num): \n",
    "                if j == Y[i]:  # predict class\n",
    "                    continue\n",
    "                margin = scores[j] - correct_class_score + 1  # loss\n",
    "                if margin > 0:  \n",
    "                    above_zero_loss_count += (\n",
    "                        1  \n",
    "                    )\n",
    "                    loss += margin  # \n",
    "                    dW[:, j] += X[i]  # \n",
    "            dW[:, int(Y[i])] -= above_zero_loss_count * X[i]  \n",
    "        loss /= current_batch_size  \n",
    "        dW /= current_batch_size  \n",
    "        return loss, dW\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = np.append(X, 1)  # add 1 (bias)\n",
    "        scores = X.dot(self.W)  \n",
    "        return np.argmax(scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, X_test, Y_test, noprint=False):\n",
    "    correct = 0  \n",
    "    for i, img in enumerate(X_test):  \n",
    "        index = model.forward(img)  \n",
    "        correct += (\n",
    "            1 if index == Y_test[i] else 0\n",
    "        )  \n",
    "        if noprint is False:  \n",
    "            if i > 0 and i % 1000 == 0:  \n",
    "                print(\n",
    "                    \"Accuracy {:.3f}\".format(correct / i)\n",
    "                )  \n",
    "    return correct / len(Y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How learning quality depends of speed:\")\n",
    "\n",
    "for lr in [1e-2, 1e-8]:\n",
    "    for bs in [256, 2048]:\n",
    "\n",
    "        print(\"-\" * 50, \"\\n\", \"learning_rate =\", lr, \"\\tbatch_size =\", bs)\n",
    "        print()\n",
    "        lc_model = LinearClassifier(labels_eng, batch_size=bs)\n",
    "\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(10):\n",
    "            loss = lc_model.fit(X_train, Y_train, learning_rate=lr)\n",
    "            accuracy = validate(lc_model, X_test, Y_test, noprint=True)\n",
    "            if best_accuracy < accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_epoch = epoch\n",
    "            print(f\"Epoch {epoch} \\tLoss: {loss}, \\tAccuracy:{accuracy}\")\n",
    "\n",
    "        print()\n",
    "        print(f\"Best accuracy is {best_accuracy} in {best_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор шага обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/update_weghts_values.png\" width=\"450\"> \n",
    "\n",
    "<!-- [Визуализация](https://docs.google.com/file/d/0Byvt-AfX75o1ZWxMRkxrUFJ2ZUE/preview)\n",
    " -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Шаг обучения - некоторый коэффициент, как правило, небольшой, который не позволяет нам двигаться слишком быстро. У нас есть точка, в которую мы хотим попасть. Если мы сделаем слишком большой шаг, то мы ее перескочим (график справа), поэтому надо подобрать шаг, который не позволит ее перескочить, но в то же время, чтобы тот же процесс не шел слишком медленно (как на графике слева)\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/learning_rate_optimal_value.png\" > \n",
    "\n",
    " \n",
    "\n",
    "Пока изменения loss функции достаточно большие, сам по себе градиент тоже большой. За счет этого можно двигаться быстро. Когда он будет уменьшаться, мы должны оказаться рядом с этим минимумом, и шаг, который мы выбрали, не должен мешать этому процессу. \n",
    "\n",
    "Бывают ситуации, когда шаг можно менять в самом процессе обучения, когда в начале обучения модели шаг большой, потом, по мере того, как она сходится, чтобы более четко найти минимум, шаг можно изменить вручную. Но изначально его нужно каким-то образом подобрать, и это зависит от данных и от самой модели. Это тоже гиперпараметр, связанный с обучением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор размера батча"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/define_size_of_batch.png\" width=\"500\"> \n",
    "\n",
    "Мы с самого начала говорили о выборках, некоторого количества примеров. В дальнейшем, мы будем называть их батчами. Батч (англ. *batch*) - это некоторое подмножество обучающей выборки фиксированного размера.\n",
    "\n",
    "При этом было не очень понятно, чем они мотивированны. Точнее, мы мотивировали это тем, что у нас много данных, и мы не сможем их обработать все, и это правда. Даже если мы сможем загрузить все данные в память, нам нужно будет загрузить их и использовать при расчете, в том числе градиента. Это ещё более затратно. \n",
    "\n",
    "Зачем батчи нужны при рассчете loss? Если мы посчитаем loss по одному изображению, скорее всего он будет очень специфичен, и это движение, которое произойдет, будет направлено в сторону минимума, потому что по этому конкретному изображению мы улучшим показатели, что не отражает обобщения всех данных, поэтому мы используем батчи.\n",
    "\n",
    "Нас в первую очередь интересует более общее направление, которое будет работать на большинстве наших данных. Поэтому если мы будем оптимизировать модели, исходя из одного элемента данных, путь будет витиеватый, и процесс будет происходить достаточно долго. Если же данные не помещаются в память, то **можно использовать батч того размера, который у нас есть**. Можно загрузить в память и считать по батчам градиент. В этом случае спуск будет более плавным, чем по одному изображению. \n",
    "\n",
    "Также при использовании всего датасета тоже есть свои минусы. **Не всегда загрузка всего датасета приводит к увеличению точности**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее: \n",
    "* [Методы оптимизации нейронных сетей\n",
    "](https://habr.com/ru/post/318970/)\n",
    "* [Обучение нейронной сети](https://alxmamaev.medium.com/deep-learning-на-пальцах-часть-1-341cfe021ef9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понятие градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Большие объемы данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Сложность модели__ (*model complexity*) &mdash; важный гиперпараметр. В частности, для линейной модели, сложность может быть представлена количеством параметров, для полиномиальных моделей &mdash; степенью полинома, для деревьев решений &mdash; глубиной дерева и т.д.\n",
    "\n",
    "Сложность модели тесно связана с __ошибкой обобщения__ (_generalization error_). Ошибка обобщения отличается от ошибки обучения, измеряемой на тренировочных данных, тем, что позволяет оценить обобщающую способность модели, приобретенную в процессе обучения, давать точные ответы на неизвестных ей объектах. Cлишком простой модели не будет хватать мощности для обобщения сложной закономерности в данных, что приводит к большой ошибке обобщения, с другой стороны слишком сложная модель также приводит к большой ошибке обобщения за счет того, что в силу своей сложности модель начинает пытаться искать закономерности в шуме, добиваясь большей точности на тренировочных данных, теряя при этом часть обобщающей способности.\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/model_complexity.png\" width=\"500\"> \n",
    "\n",
    "Проиллюстрируем описанное явление на примере полиномиальной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2*np.pi, 10)\n",
    "y = np.sin(x) + np.random.normal(scale=0.25, size=len(x))\n",
    "plt.scatter(x, y, s=50, facecolors='none', edgecolors='b', label='noisy data')\n",
    "\n",
    "x_true = np.linspace(0, 2*np.pi, 200)\n",
    "y_true = np.sin(x_true)\n",
    "plt.plot(x_true, y_true, c='lime', label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем аппроксимировать имеющуюся зависимость с помощью полиномиальной модели, используя шумные данные в качестве тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x_train = x.reshape(-1,1)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "for i, degree in enumerate([0,1,3,9]):\n",
    "\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "    model.fit(x_train, y)\n",
    "    y_plot = model.predict(x_true.reshape(-1,1))\n",
    "\n",
    "    fig.add_subplot(2,2,i+1)\n",
    "    plt.plot(x_true, y_plot, c='red', label=f'M={degree}')\n",
    "    plt.scatter(x, y, s=50, facecolors='none', edgecolors='b')\n",
    "    plt.plot(x_true, y_true, c='lime')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель может переобучаться, подстраиваясь под тренировочную выборку. В полиноме степень, и как следствие количество весов &mdash; это гиперпараметр, который можно подбирать на кросс-валидации, однако, когда мы, таким образом, подбираем сложность модели мы налагаем довольно грубое ограничение на обобщающую способность модели в целом. Вместо этого более разумным было бы оставить модель сложной, но использовать некий ограничитель (__регуляризатор__), который будет заставлять модель отдавать предпочтение выбору более простого обобщения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(9), LinearRegression())\n",
    "model_ridge = make_pipeline(PolynomialFeatures(9), Ridge(alpha=0.1))\n",
    "\n",
    "model.fit(x_train, y)\n",
    "y_plot = model.predict(x_true.reshape(-1,1))\n",
    "\n",
    "model_ridge.fit(x_train, y)\n",
    "y_plot_ridge = model_ridge.predict(x_true.reshape(-1,1))\n",
    "\n",
    "plt.plot(x_true, y_plot, c='red', label=f'M={degree}')\n",
    "plt.plot(x_true, y_plot_ridge, c='black', label=f'M={degree}, alpha=0.1')\n",
    "plt.scatter(x, y, s=50, facecolors='none', edgecolors='b')\n",
    "plt.plot(x_true, y_true, c='lime', label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "poly_coef = model[1].coef_\n",
    "\n",
    "eq = f'y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x'\n",
    "for i in range(2, 10):\n",
    "    eq += f'+{round(poly_coef[i], 2)}*x^{i}'\n",
    "    \n",
    "print('Without regularization: ', eq)\n",
    "\n",
    "poly_coef = model_ridge[1].coef_\n",
    "\n",
    "eq = f'y = {round(poly_coef[0], 2)}+{round(poly_coef[1], 2)}*x'\n",
    "for i in range(2, 10):\n",
    "    eq += f'+{round(poly_coef[i], 2)}*x^{i}'\n",
    "\n",
    "print('With regularization: ', eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что одним из \"симптомов\" переобучения являются аномально большие веса. Модель Ridge Regression, показанная в примере выше, использует L2 регуляризуцию для борьбы с этим явлением.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/l2_regularization.jpg\" width=\"700\"> \n",
    "\n",
    "\n",
    "L2 Regularization = weights decay\n",
    "\n",
    "Идея состоит в том, что мы можем наложить некоторое требование на сами веса. Дело в том, что можно получить один и тот же выход модели при разных весах (выход модели соответствует умножению весов на $x$), при разных $w$ выход может быть идентичен.\n",
    "\n",
    "Эти параметры задают некоторую аппроксимацию нашей целевой функции, аппроксимировать функцию можно двумя способами:\n",
    "1. Использовать все имеющиеся данные и провести ее строго через все точки, которые нам известны; \n",
    "2. Использовать более простую функцию: (в данном случае линейную), которая не попадет точно во все данные, но зато будет соответствовать некоторым общим закономерностям, которые у них есть. \n",
    "\n",
    "Характерной чертой переобучения является второй сценарий, и сопровождается он, как правило, большими весами. Введение L2-регуляризации приводит к тому, что большие веса штрафуются и предпочтение отдается решениям, использующим малые значения весов. \n",
    "\n",
    "Модель может попробовать схитрить и по-другому - использовать все веса, все признаки, даже незначимые, но с маленькими коэффициентами. С этим L2-loss поможет хуже, так как он не сильнее штрафует мелкие веса. Результатов его применения - малые значения весов, которые использует модель\n",
    "\n",
    "В этом случае на помощь приходит L1-loss, который штрафует вес за сам факт отличия его от нуля. Но и штрафует он все веса одинаково. Результат его применения - малое число весов, которые использует модель в принципе. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/l1_and_l2_regularization.gif\" alt=\"alttext\" width=\"550\"/>\n",
    "\n",
    "\n",
    "Это лоссы можно комбинировать - получится Elastic Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$\\lambda=$ regularization strength (hyperparameter)\n",
    "\n",
    "$L(W)=\\underbrace{\\frac1N\\sum_{i=1}^NL_i(f(x_i,W),y_i)}_{\\textbf{Data loss} }+\\underbrace {\\lambda R(W)}_{\\textbf{Regularization}}$\n",
    "\n",
    "Берем сумму всех весов по всей матрице $w$, и добавляем ее к loss. Соответственно, чем больше будет эта сумма, тем больше будет суммарный loss. \n",
    "\n",
    "\n",
    "В дальнейшем проблема с переобучением будет вставать довольно часто. Методов регуляризации модели существует достаточно много. Этот — один из базовых, который будет использоваться практически во всех оптимизаторах, с которыми познакомимся позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия для бинарной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение с помощью градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мультиклассовая классификация (объект может принадлежать к нескольким классам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-энтропия, softmax и логистическая регрессия для мультиклассовой постановки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к вероятностям\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "[Видео от StatQuest, которое объясняет Softmax](https://www.youtube.com/watch?v=KpKog-L9veg)\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/scores_to_probability.png\" width=\"750\">\n",
    "\n",
    "Перейти к вероятностям мы сможем, проведя с весами некоторые не очень сложные математические преобразования. \n",
    "\n",
    "На слайде выше показано, почему выходы модели часто называют [logit’ами](https://en.wikipedia.org/wiki/Logit). Если предположить, что у нас есть некая вероятность, от которой мы берем такую функцию (logit), то она может принимать значения от нуля до бесконечности. Мы можем считать, что выходы модели - это logit’ы. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы могли бы просто взять индекс массива, в котором значение (logit) максимально. Предположим, что наша сеть выдала следующие значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "logits = [\n",
    "          5.1, # cat\n",
    "          3.2, # car\n",
    "          -1.7, # frog\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, чтобы узнать какой класс наша сеть предсказала, мы могли бы просто взять `argmax` от наших `logits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted class = %i (Cat)' % (np.argmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но от argmax нельзя посчитать градиент, так как производная от константы равна 0. Соответственно, если бы мы вставили производную от argmax в градиентный спуск, мы бы получили везде нули, и соответственно, наша сеть бы вообще ничему не научилась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(np.arange(3), [1,0,0], color='red', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А мы бы хотели получить не logit’ы, а настоящую вероятность на выходе модели. Да еще и таким образом, что бы от наших вероятностей можно было бы посчитать градиент. Для этого мы можем применить к нашим логитам функцию **Softmax**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/linear_classifier_softmax.jpg\" width=\"1300\">\n",
    "\n",
    "Мы можем провести над логитами операцию экспоненцирования, то есть число Эйлера (2.71828) **возвести в степень**, соответствующую этому выходу. В результате, мы получим вектор, гарантировано неотрицательных чисел (потому что мы ввели неположительное число в степень, пускай даже отрицательную, то есть выходы могут быть маленькие, но всегда положительные). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше, чтобы можно было интерпретировать эти числа как вероятности, их сумма должна быть равна единице. Мы должны их нормализовать, то есть **поделить на сумму**. Это преобразование называется **Softmax функцией**.\n",
    "\n",
    "**Получаются вероятности**, то есть числа, которые можно интерпретировать таким образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{Softmax}_\\text{кошка} = \\frac{e^{5.1}}{e^{5.1} + e^{3.2} + e^{-1.7}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return np.exp(logits)/np.sum(np.exp(logits))\n",
    "\n",
    "print(softmax(logits))\n",
    "print('Sum = %.2f' % np.sum(softmax(logits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что Softmax, никоим образом не поменял порядок значений. Самому большому logit'у соответствует самая большая вероятность, а самому маленькому, соответственно самая маленькая"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графиках. Возьмем массив случайных логитов и применим к ним softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_logits = np.linspace(-1,1,50)\n",
    "fig,ax = plt.subplots(ncols=2)\n",
    "\n",
    "ax[0].plot(np.arange(50), rand_logits)\n",
    "ax[0].set_title('Logits')\n",
    "ax[1].plot(np.arange(50), softmax(rand_logits))\n",
    "ax[1].set_title('Softmax')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-entropy / log loss**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/cross_entropy_plot_loss_with_probability.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Теперь, с использованием такого вектора можно определить другую loss функцию. Если не вникать в детали, можно **взять от нее логарифм**. Тогда loss от такого выхода будет выглядеть вот таким образом:\n",
    "\n",
    "$ L_i =  -log(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}})$\n",
    "\n",
    "как нормализованный выход от верного класса на сумму остальных, к которому добавили минус. \n",
    "\n",
    "Получится график loss (слева). Его плюс заключается в том, что у него нет участка с плато, практически по всей длине функция получается гладкой, с хорошими мощными производными. Когда loss большой, производная тоже большая, и за счет этого можно быстро обучать модель. Для кусочно-линейной функции потерь она же равна константе.\n",
    "\n",
    "До тех пор, пока модель не будет работать с точностью 100%, то есть пока все остальные выходы не станут нулевыми, мы можем продолжить обучение, даже если у нас нет явной регуляризации.\n",
    "\n",
    "Осталось выяснить, почему такая простая на вид функция **называется так сложно** - Кросс-энтропия. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение Кросс-энтропии\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Начнем с простого примера, пусть у нас есть 10 точек со следующими значениями признака x: \n",
    "\n",
    "`x = [-2.2, -1.4, -0.8, 0.2, 0.4, 0.8, 1.2, 2.2, 2.9, 4.6]`\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/cross_entropy_ten_dots.png\">\n",
    "\n",
    "Пусть наши точки принадлежат двум классам: зеленый и красный:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/cross_entropy_two_classes.png\">\n",
    "\n",
    "Перед нами простая задача классификации: по признаку x предсказать класс наших точек. Мы можем переформулировать задачу как нахождение вероятности того, что точка зеленая или красная. В идеальной ситуации для зеленой точки вероятность того, что она зеленая равна 1, в то же время вероятность того, что красная точка &mdash; зеленая должна быть равна 0. \n",
    "\n",
    "Вероятности, которые выдает, обучаемая нами модель, зачастую далеки от идеальной ситуации. В нашем примере сравнить насколько сильно предсказанная вероятность класса отличается от вероятности, выдаваемой \"идеальной моделью\" можно за счет __бинарной кросс-энтропии__ частного случая кросс-энропии.\n",
    "\n",
    "$$H_p(q)=-\\frac{1}{N}\\sum^N_{i=1}y_i\\cdot log(p(y_i))+(1-y_i)\\cdot log(1-p(y_i))$$\n",
    "\n",
    "где $y$ &mdash; метка класса (1 для зеленого, 0 для красного), которую можно также интерпретировать как вероятность, предсказанную \"идеальной моделью\", $p(y)$ &mdash; вероятность того, что точка зеленая, предсказываемая оцениваемой моделью.\n",
    "\n",
    "Какое же отношение энтропия имеет к этой формуле? Давайте углубимся в детали.\n",
    "\n",
    "Поскольку $y$ представляет метку классов точек, то его распределение $q(y)$ выглядит следующим образом:\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/cross_entropy_distribution.png\">\n",
    "\n",
    "__Энтропия__ &mdash; мера неуверенности, связанная с распределением $q(y)$. Какова была бы мера неуверенности распределения $q(y)$ если бы все точки были зелеными? Так как у нас бы не было сомнений насчет цвета точки (он всегда зеленый), значение энтропии было бы 0. Теперь представим другую ситуацию, пусть у нас поровну точек зеленого и красного цвета. Для нас это наихудшая ситуация, поскольку попытка определить цвет точки, по сути, представляет случайное угадывание. В этом случае энтропия вычисляется по формуле Хартли:\n",
    "\n",
    "$$H(q)=log(2)$$\n",
    "\n",
    "Мы рассмотрели два крайних случая, но как быть с промежуточными ситуациями? Для этих случаев мы можем использовать формулу Шеннона:\n",
    "\n",
    "$$H(q)=-\\sum^C_{c=1}q(y_c)\\cdot log(q(y_c))$$\n",
    "\n",
    "где C &mdash; количество классов. Нетрудно заметить, что формула Хартли является частным случаем формулы Шеннона.\n",
    "\n",
    "Таким образом, зная истинное распределение случайной величины, мы можем рассчитать его энтропию. А что будет если мы попытаемся аппроксимировать истинное распределение $q(y)$ некоторым другим распределением $p(y)$? Допустим, что наши цветные точки подчиняются этому распределению $p(y)$, также мы знаем, что исходят они из неизвестного нам истинного распределения $q(y)$, если мы посчитаем следующую энтропию, это и будет __кросс-энтропия__:\n",
    "\n",
    "$$H_p(q)=-\\sum^C_{c=1}q(y_c)\\cdot log(p(y_c))$$\n",
    "\n",
    "Если окажется, что распределения $p(y)$ и $q(y)$ совпадают, в этом случае энтропия $H(q)$ и кросс-энтропия $H_p(q)$ также будут совпадать. Однако в реальности такое случается редко и кросс-энтропия бывает больше энтропии истинного распределения\n",
    "\n",
    "$$H_p(q)-H(q)\\geq0$$\n",
    "\n",
    "Разница между кросс-энтропией и энтропией называется __дивергенцией Кульбака-Лейблера__, которая является мерой различия между двумя распределениями:\n",
    "\n",
    "$$D_{KL}(q||p)=H_p(q)-H(q)=\\sum^C_{c=1}q(y_c)\\cdot [log(q(y_c))-log(p(y_c))]$$\n",
    "\n",
    "Это значит, что чем ближе $p(y)$ к $q(y)$, тем меньше будет значение дивергенции Кульбака-Лейблера и, следовательно, меньше значение кросс-энтропии.\n",
    "\n",
    "Таким образом, мы хотим добиться, чтобы модель, которую мы оцениваем, порождала $p(y)$ близкое к $q(y)$. Для этих целей мы стремимся __минимизировать кросс-энтропию__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/define_cross_entropy_kulbak_shennon.png\" >\n",
    "\n",
    "Если мы рассматриваем выходы нашей модели как вероятности, то мы можем их сравнивать. У нас есть номер правильного класса. Соответственно, можно сказать, что вероятность этого класса - единица, а всех остальных 0, и получить, таким образом, вероятностное распределение. Выход модели тоже можно интерпретировать как вероятности (тоже можно получить вероятностное распределение). И для работы с этими распределением есть некоторый математический аппарат, который основан на понятии энтропии, который ввел Клод Шеннон.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/efficient_way_to_send_signals.png\" >\n",
    "\n",
    "\n",
    "Идея в следующем: у нас есть некоторые данные, которые мы передаем по каналу связи. Например, у нас есть метеостанция, которая сообщает прогноз погоды. Допустим, она может передавать 8 вариантов прогноза. Мы в каждый момент времени получаем от нее сообщение. Предположим, что потребуется 3 бит, чтобы передавать это сообщение. \n",
    "\n",
    "Допустим, мы знаем некую вероятность, с которой в нашем регионе может наступить хорошая либо плохая погода. То есть мы знаем вероятностное распределение, по которому у нас, допустим, солнечное место, где почти не бывает бури. Если мы знаем об этом, мы можем сэкономить на передаче информации. Мы можем закодировать наиболее вероятные сообщения двумя битами, причем придумать такие коды, чтобы они почти не пересекались. Они будут начинаться с нуля и передавать два бита вместо трех. Таким образом, можно сэкономить на передачи информации. То есть идея такая: **если мы знаем, что события маловероятны, мы можем кодировать их более длинной цепочкой, а более вероятные события - более короткими цепочками**. В этом случае в среднем количество информации, которое можно передать, сократится.\n",
    "\n",
    "**Формула Шеннона позволяет посчитать, насколько сильно мы можем сэкономить для конкретного вероятностного распределения**. То есть если подставить эти данные в формулу, то получим 2,23.\n",
    "\n",
    "На практике реализовать это, используя биты, возможно, не выйдет, но это свойство данного вероятностного распределения можно посчитать. Оно несёт в себе некоторое количество полезной информации, соответствующее этой энтропии.\n",
    "\n",
    "**Что же такое кросс-энтропия?**\n",
    "\n",
    "Давайте представим, что мы каким-то образом начали кодировать эту информацию, прошили погодную станцию так, что она может эту кодированную информацию передавать, а потом перенесли в другой регион, где ситуация противоположная. И мы кодируем события, которые стали частыми, длинными цепочками, а маловероятные - короткой, и у нас в этом случае явно передается больше информации, чем мы могли бы, если бы сделали все банально. \n",
    "\n",
    "Кросс-энтропия позволяет посчитать, насколько большая будет потеря в данном случае. То есть это некий **способ сравнить между собой вероятностные распределения**.\n",
    "\n",
    "Что, собственно говоря, отображено в формуле, потому что мы считаем кросс-энтропию по нашему вектору $P$ и $Q$. Вектор $P$ состоит из нулей и единиц, соответственно, во всех случаях, кроме одного, это выражение будет нулевым, кроме одного случая с единицей. Случай с единицей соответствует нашему правильному классу. Поэтому сумма исчезает, остаются логарифм и вектор, а вероятность для правильного класса мы считаем как Soft max. Отсюда название кросс-энтропия.  \n",
    "\n",
    "\n",
    "(В теории информации кросс-энтропия между двумя распределениями вероятностей $p$ и $q$  по одному и тому же базовому набору событий измеряет среднее число битов, необходимых для идентификации события, взятого из набора, если схема кодирования, используемая для набора, оптимизирована для оценочного распределения вероятностей $q$, а не для истинного распределения $p$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/logits_to_scores_to_probabilitys.png\" width=\"900\">\n",
    "\n",
    "Аналогичное обоснование: есть понятие дивергенция, которая позволяет оценить расхождение между вероятностными распределениями, которая нам здесь и нужна. Есть дивергенция **Кульбака-Лейблера**, которая выражается через кросс-энтропию, а энтропия от нашего вектора нулевая. Поэтому фактически здесь кросс-энтропия равна дивергенции, которая показывает, насколько не похожи два распределения. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/softmax_formula.jpg\" width=\"800\">\n",
    "\n",
    "\n",
    "Эта функция очень популярна, она используется при обучении реальных моделей на практике. Поскольку внутри находится Softmax, часто ее называют Softmax-классификатором, что, строго говоря, не совсем верно. Здесь функция потерь - кросс-энтропия, Softmax же просто используется внутри нее.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
