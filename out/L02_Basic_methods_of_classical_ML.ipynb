{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Базовые методы Классического Машинного Обучения</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Признаки и работа с ними"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine\" learning is basically feature engineering. @Andrew Ng\n",
    "\n",
    "Общая схема классического машинного обучения выглядит так. Даже в случае нейросетей некая предобработка исходных данных все равно не бывает лишней.\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/data_preparation.png\" width=\"600\">\n",
    "\n",
    "> Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. @Dr. Jason Brownlee\n",
    "\n",
    "**Генерация признаков** - процесс придумывания способов описания данных с помощью простых значений, которые должны отражать характеристики объектов исследований, через которые выражаются целевые значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные, требующие предобработки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально объекты в нашем датасете могут быть представлены в виде описаний, которые не являются признаковыми, либо, очевидно, требует некоторой предобработки:\n",
    "\n",
    "1. веб-страницы \n",
    "2. файлы \n",
    "3. ссылки на участников группы\n",
    "4. измерения в разных единицах (см, м, дц)\n",
    "и т.д \n",
    "________\n",
    "todo вклейка информации\n",
    "\n",
    "IP адрес: \n",
    "\n",
    "xxx.xxx.xxx.xxx -> регион, провайдер\n",
    "\n",
    "Координаты атомов:\n",
    "\n",
    "[AlphaFold](https://yakovlev.me/para-slov-za-alphafold2/\n",
    ")\n",
    "\n",
    "\n",
    "\"как вообще математически можно выразить структуру белка. До того мы постоянно неявно подразумевали, что это координаты атомов (всех, каркасных, Cα или каких-то ещё), но на практике это очень неудачное представление, поскольку оно не единственное. Мы обычно считаем, что предсказание работает как некоторая детерминированная функция: принимает на вход последовательность и всегда возвращает один единственный ответ. Но какой из бесконечного набора координат \"канонический\"?\n",
    "\n",
    "\n",
    "\n",
    "\"... инвариантом структуры является матрица (таблица) всех попарных расстояний между атомами\"\n",
    "\n",
    "_______\n",
    "\n",
    "Большая часть моделей неспособна работать с такими представлениями в сыром виде и или просто не запустится, либо будет выдавать неадекватные результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс создания признаков зависит от модели, которую мы собираемся использовать. Для одних моделей полезно добавить признаки, полученные делением/перемножением исходных. Другие модели могут провести эти операции сами и экономнее/менее переобучаясь. Как вариант, добавление признаков, явно зависящих друг от друга, может даже мешать некоторым моделям. \n",
    "\n",
    "\n",
    "Например, плохая идея добавлять в обычную линейную модель как признаки X1 и X2, так и их сумму. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/process_of_generation_features.png\" width=\"700\">\n",
    "\n",
    "Вообще говоря, надо понимать, что процесс feature engneering является критическим местом, bottleneck, в машинном обучении. Все, что ваша модель будет знать о данных, решается на этом этапе. Больше, чем вы ей дадите - она не узнает. \n",
    "\n",
    "Если вы в данных дадите явную подсказку об ответе - то она будет использовать эту подсказку, а реальные закономерности может и не выучить. К примеру, можно дать ей в качестве признака id покупателя, который каждую неделю покупает одно и то же. Если таких ситуаций будет много, то она и выучит, что надо предсказывать все по id. Когда же к вам придет новый покупатель или у старого, что-то поменяется в поведении, модель начнет вести себя неадекватно. \n",
    "\n",
    "Точно такую же роль может сыграть информация о номере эксперимента, лаборатории, в которой его проводили, аспиранте, который его проводил и тд. \n",
    "\n",
    "Такая ситуация будет называться **data leakage**.\n",
    "\n",
    "\n",
    "Ну и понятно, что если вы дадите модели только нерелевантную информацию, она ничего из нее не вытащит. \n",
    "\n",
    "> At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. @ Prof. Pedro Domingos\n",
    "\n",
    "> The algorithms we used are very standard for Kagglers. …We spent most of our efforts in feature engineering. … We were also very careful to discard features likely to expose us to the risk of over-fitting our model.  @Xavier Conort, топ-участник Kaggle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Типы признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Традиционно признаки делятся на:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вещественные \n",
    "Вещественные признаки бывают:\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* дискретные. Например - число лайков от пользователей\n",
    " \n",
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/discrete_features_social_media_likes.jpg\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * непрерывные. Например - температура\n",
    "\n",
    "\n",
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/continuous_features_thermometer.png\" width=\"600\">\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятно, что разделение часто условное. Тот же возраст можно посчитать и дискретной переменной (пользователь всегда нам сообщает свои полные года), и непрерывной (возраст можно считать с любой точностью, но никто не будет) )\n",
    "\n",
    "\n",
    "Также иногда вещественные признаки делят на относительные (считаются относительно чего-то, уже нормированные и тд)  и интервальные. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Категориальные \n",
    "\n",
    "\n",
    "\n",
    "Значение -  принадлежность к какой-то из категорий. Традиционно делятся на сильно отличающиеся по свойствам:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * упорядоченные (ординальные) - для каждой пары возможных категорий можем сказать, какая больше, а какая меньше. Например - класс места. Или размер одежды\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/categorical_ordered_features.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * неупорядоченные (номинальные) - категории между собой несравнимы. Обычно нельзя сказать, что красный телефон больше синего. Или что солнечная погода больше снежной\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/categorical_unordered_features.png\" width=\"900\">\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто мы сталкиваемся с бинарными категориальными признаками, для которых известно только две возможных категории (например, биологический пол человека)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразования признаков\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вещественных признаков \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Бинаризация \n",
    "\n",
    "Например, нам может быть неинтересно, сколько конкретно раз встретилось явление в наблюдении - главное, что оно вообще встретилось. Тогда мы просто превращаем наш вещественный признак в бинарный \"было ли явление\", и работаем уже с ним. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Округление\n",
    "\n",
    "Часто данные до нас доходят с очень высокой точностью после запятой. Нужно ли это нашей модели - часто нет. Иногда по факту два наблюдения не различаются  по этому признаку (разница в пределах статошибки), но по признаку их отличить можно. Это может приводить к переобучению. В таких случаях разумно признаки округлить. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bining (Бинирование)\n",
    "\n",
    "Опять же, нам не интересны точные значения - например, что видео набрало 1000 лайков, а не 1001. \n",
    "\n",
    "К тому же, число просмотров/лайков некоторых видео может быть очень большим в сравнении с остальными, что будет приводить к неадекватному поведению. \n",
    "В итоге часть значений у нас встречается часто, а часть - очень редко. Это может приводить к неадекватному поведению модели. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fixed-width bining\n",
    "\n",
    "Просто бьем наши значения по диапазонам фиксированной длины. Так часто поступают с возрастом. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/fixed_width_binning.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaptive Binning\n",
    "\n",
    "Это не всегда работает хорошо. Например, распределение зарплат у нас очень сильно скошено вправо. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/adaptive_binning.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И бинирование с фиксированной длиной бина нам не поможет справиться с редкими значениями.\n",
    "\n",
    "В этой ситуации помогает бинирование, например, по квантилям - когда границы бина представляют собой квантили. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/quantiles_binning.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логарифмирование\n",
    "\n",
    "С ситуацией, когда распределение скошено вправо работает и другой подход - прологаримфировать величину. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/log_binning.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Обобщением этого подхода является [Box-Cox Transform](https://www.statisticshowto.com/box-cox-transformation/#:~:text=A%20Box%20Cox%20transformation%20is,a%20broader%20number%20of%20tests.), общей целью которой является придать данным вид более похожий на нормальное распределение, с которым работает бОльшее число моделей и сходимость лучше \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категориальных признаков "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding \n",
    "\n",
    "Просто берем и каждой категории однозначно сопоставляем число. \n",
    "Очень простой способ, если признак ординальный - будет работать почти всегда. \n",
    "\n",
    "Если же наш признак - номинальный, то могут возникнуть проблемы. Мы не можем сказать, что салатовый больше красного (в большинстве случаев). Но модель ничего про это не знает и после нашего кодирования спокойно такие сравнения может производить. Это может приводить к более низкому качеству модели и выучиванию ею неправильной информации. Кроме того, например, деревьям решений, чтобы выделить в таком случае конкретную категорию придется делать сразу несколько действий, которые, в силу жадности алгоритма их построения, могут и не быть найдены\n",
    "\n",
    "\n",
    "\n",
    " Некоторые модели (например, lightgbm) автоматически могут перекодировать все правильно, если им сообщить, что переданный признак - категориальный. Для некоторых это придется делать вручную. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding \n",
    "\n",
    "Схемой, которая часто используется на практике, является one-hot encoding. Он состоит том, что вместо одного категориального признака X создается набор бинарных категориальных признаков, которые отвечают на вопрос \"X == C? \", где C пробегает все возможные значения категориального признака. \n",
    "\n",
    "Теперь чтобы обусловиться на конкретное значение категориального признака, дереву решений достаточно задать один вопрос. \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/one_hot_encoding.png\" width=\"450\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у такой схемы есть один минус - мы получаем линейно зависимые признаки. Это может плохо влиять на некоторые модели (для случая нейронных сетей - обычно нет, но полезно держать в голове). \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/problem_of_ohe.png\" width=\"850\">\n",
    "\n",
    "Потому иногда одну из категорий исключают при кодировании, например, в примере выше можно исключить Fish, ведь если все три других признака-категории равны 0, то точно верно, что категория - Fish. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target encoding \n",
    "\n",
    "Кодируем каждую категорию каким-то численным параметром, характеризующим то, что мы предсказываем. Например, можно каждую категорию категориального признака заменять на среднее \n",
    "\n",
    "На самом деле, так просто делать нельзя, можно получить переобученную модель. Как делать - можете подробно посмотреть, к примеру, [здесь](https://github.com/Dyakonov/PZAD/blob/master/2020/PZAD2020_042featureengineering_07.pdf) или [здесь](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv?redirectTo=%2Flecture%2Fcompetitive-data-science%2Fconcept-of-mean-encoding-b5Gxv) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "Можно научить вашу модель саму сопоставлять каждой категории некий вектор определенной размерности. Для этого вначале сопоставляем каждой категории случайный вектор заданной длины. А далее изменяем этот вектор как обычные веса. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/embedding.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование циклических категориальных признаков\n",
    "\n",
    "В случае с такими признаками, как день недели или время суток, мы сталкиваемся с проблемой того, что нам нужно предложить кодирование, которое будет учитывать, что понедельник близок к воскресенью так же, как понедельник же ко вторнику, и тд. \n",
    "\n",
    "В случае деревьев решений и методов на них основанных можно \"забить\" - такие методы сами разберутся. Для некоторых других методов, тех же нейросетей, правильно кодирование может улучшить качество и сходимость. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Давайте нанесем наши категории, например, дни недели - на окружность. Как это сделать? \n",
    "Пусть понедельнику соответствует 1, а воскресенью - 7. Далее посчитаем два таких вспомогательных признака по следующим формулам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weekdays = np.arange(1, 8) #create an array of weekdays\n",
    "print(weekdays)\n",
    "sina = np.sin(weekdays * np.pi * 2 / np.max(weekdays)) #feature 1\n",
    "cosa = np.cos(weekdays * np.pi * 2 / np.max(weekdays)) #feature 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(7,7)) #Decide figure size\n",
    "plt.scatter(sina, cosa) #Plot scatter of feature 1 vs feature 2\n",
    "for  i, z in enumerate( (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\") ): #for each day in a week\n",
    "  plt.text(sina[i], cosa[i], s=z) #add text labels to plot\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать дальше? А по сути, мы уже все сделали. Теперь расстояния между понедельником и вторником и воскресеньем и понедельником одинаковые:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mon_tue = (sina[1] - sina[0]) ** 2 + (cosa[1] - cosa[0]) ** 2 #distance between Monday and Tuesday\n",
    "dist_sun_mon = (sina[6] - sina[0]) ** 2 + (cosa[6] - cosa[0]) ** 2 #distance between Sunday and Monday\n",
    "print('Distance between Mon-Tue = %.2f' % dist_mon_tue)\n",
    "print('Distance between Sun-Mon = %.2f' % dist_sun_mon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "то же самое верно и для любых отстоящих друг от друга на одинаковое число дней\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mon_wed = (sina[2] - sina[0]) ** 2 + (cosa[2] - cosa[0]) ** 2 #distance between Monday and Wednesday\n",
    "dist_fri_sun = (sina[4] - sina[6]) ** 2 + (cosa[4] - cosa[6]) ** 2 #distance between Friday and Sunday\n",
    "print('Distance between Mon-Wed = %.2f' % dist_mon_wed)\n",
    "print('Distance between Fri-Sun = %.2f' % dist_fri_sun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, циклические признаки можно кодировать парой признаков - sin и cos, полученных по схеме, описанной выше. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемы подхода\n",
    "\n",
    "1. Деревья решений могут решить задачу и так. А такое кодирование им, наоборот, будет мешать, т.к. они работают с одним признаком за раз\n",
    "\n",
    "2. Надо понимать, что важность исходной категориальной фичи неочевидным образом делится между двумя полученными из нее таким образом. \n",
    "\n",
    "3. В некоторых задачах one-hot работает лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование взаимодействия признаков\n",
    "\n",
    "Признаки могут по-разному взаимодейстовать и некоторые модели в принципе не могут моделировать это взаимодействие. \n",
    "\n",
    "\n",
    "Взаимодействовать могут вещественные переменные и категориальные\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/strength_vs_speed.png\" width=\"550\">\n",
    "\n",
    "**Сила vs. скорость**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "категориальные и категориальные \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/categorical_and_categorical.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "вещественные и вещественные \n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/real_and_real.png\" width=\"300\">\n",
    "\n",
    "\n",
    "Могут быть и более высокоуровневые взаимодействия - взаимодействуют много разных признаков.\n",
    "\n",
    "Взаимодействия могут быть самые разные - много способов кодировать. Например, добавлять в число признаков их произведение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация признаков\n",
    "\n",
    "Если у вас есть модель, обученная на другом датасете, можно генерировать признаки при помощи нее. Например, при помощи случайного леса\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L04/img_license/generating_features_using_model.png\" width=\"700\">\n",
    "\n",
    "**Генерация бинарного признакового пространства с помощью RandomForest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет с образцами здоровой и раковой ткани. Датасет состоит из 569 примеров, где каждой строчке из 30 признаков, соответствует класс `1` злокачественной (*malignant*) или `0` доброкачественной (*benign*) ткани. Задача состоит в том, чтобы по 30 признакам обучить модель определять тип ткани (злокачественная или доброкачественная).\n",
    "\n",
    "Можно иметь сколь угодно хороший алгоритм для классификации - но до тех пор, пока данные на входе - мусор, на выходе из нашего чудесного классификатора мы тоже будем получать мусор (*garbage in - garbage out*). Давайте разберемся, что конкретно надо сделать, чтобы kNN реально заработал.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "cancer = sklearn.datasets.load_breast_cancer() # load data\n",
    "X = cancer.data # features\n",
    "Y = cancer.target # labels(classes)\n",
    "print(f'X shape: {X.shape}, Y shape: {Y.shape}') \n",
    "print(f'X[0]: \\n {X[0]}') \n",
    "print(f'Y[0]: \\n {Y[0]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько данных в классе `0` и сколько данных в классе `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5)) # set fig size \n",
    "plt.bar(1,Y[Y==1].shape, label=cancer.target_names[0]) # 1 label \n",
    "plt.bar(0,Y[Y==0].shape, label=cancer.target_names[1]) # 0 label\n",
    "plt.title('Class balance') \n",
    "plt.ylabel('Num examples') \n",
    "plt.xticks(ticks=[1,0], labels=['1','0']) \n",
    "plt.legend(loc='upper left') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на сами данные. У нас есть 569 строк в каждой из которой по 30 колонок. Такие колонки называют признаками или *features*. Попробуем математически описать все эти признаки (mean, std, min и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое, но в виде графика. Видно, что у фич совершенно разные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.boxenplot(data=pd.DataFrame(X), orient=\"h\", palette=\"Set2\")\n",
    "ax.set(xscale='log', xlim=(1e-4, 1e4), xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы адекватно сравнить данные между собой нам следует использовать нормализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Нормализация, выбор Scaler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализация — это преобразование данных к неким безразмерным единицам.\n",
    "Ключевая цель нормализации — приведение различных данных в самых разных единицах измерения и диапазонах значений к единому виду, который позволит сравнивать их между собой.\n",
    "\n",
    "Главное условие правильной нормализации — все признаки должны быть равны в возможностях своего влияния.\n",
    "\n",
    "Например, у нас есть данные по группе людей: *возраст* (в годах) и *размер дохода* (в рублях). Возраст может измениться в диапазоне от 18 до 70 ( интервал 70-18 = 52). А доход от 30 000 р до 500 000 р (интервал 500 000 - 30 000 = 470 000). В таком варианте разница в возрасте имеет меньшее влияние, чем разница в доходе. Получается, что доход становится более важным признаком, изменения в котором влияют больше при сравнении схожести двух людей.\n",
    "\n",
    "Должно быть так, чтобы максимальные изменения любого признака в «основной массе объектов» были одинаковы. Тогда потенциально все признаки будут равноценны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось определиться с выбором инструмента, часто используют следующие варианты: `MinMaxScaler`, `StandardScaler`, `RobustScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним `MinMaxScaler`, `StandardScaler`, `RobustScaler` для признака `data[:,0]`. **Обратите внимание на ось X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # setting the initialization parameter for random values\n",
    "\n",
    "# generate random values from 1 to 255, shape (30,1)\n",
    "test = X[:,0].reshape(-1,1)\n",
    "\n",
    "plt.figure(1, figsize=(30, 5))  \n",
    "plt.subplot(141)  # set location\n",
    "plt.scatter(test, range(len(test)), c=Y)  \n",
    "plt.ylabel(\"Num examples\", fontsize=15)  \n",
    "plt.xticks(fontsize=15)  \n",
    "plt.yticks(fontsize=15)  \n",
    "plt.title(\"Non scaled data\", fontsize=18)  \n",
    "\n",
    "# scale data with MinMaxScaler\n",
    "test_scaled = MinMaxScaler().fit_transform(test)  \n",
    "plt.subplot(142)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"MinMaxScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with StandardScaler\n",
    "test_scaled = StandardScaler().fit_transform(test)  \n",
    "plt.subplot(143)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"StandardScaler\", fontsize=18)\n",
    "\n",
    "# scale data  with RobustScaler\n",
    "test_scaled = RobustScaler().fit_transform(test)  \n",
    "plt.subplot(144)\n",
    "plt.scatter(test_scaled, range(len(test)), c=Y)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"RobustScaler\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`MinMaxScaler`** заключается в том, что он преобразует данные в диапазоне от 0 до 1. Может быть полезно, если нужно выполнить преобразование, в котором отрицательные значения не допускаются (e.g., масштабирование RGB пикселей)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$z=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
    "\n",
    "$X_{min}$ и $X_{max}$ задаются как минимальное и максимальное допустимое значение, по умолчанию:  $X_{min}=0$  и $X_{max}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **`StandardScaler`** заключается в том, что он преобразует данные таким образом, что распределение будет иметь среднее значение 0 и стандартное отклонение 1. Большинство значений будет в  диапозоне от -1 до 1. Это стандартная трансформация, и она применима во многих ситуациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-u}{s}$$\n",
    "\n",
    "$u$ — среднее значение (или 0 при `with_mean=False`) и $s$ — стандартное отклонение (или 0 при `with_std=False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И StandardScaler и MinMaxScaler очень чувствительны к наличию выбросов. **`RobustScaler`** использует медиану и основан на *процентилях*. Процентиль — мера, в которой процентное значение общих значений равно этой мере или меньше ее. Например, 90 % значений данных находятся ниже 90-го процентиля, а 10 % значений данных находятся ниже 10-го процентиля. Соответственно, RobustScaler не зависит от небольшого числа очень больших предельных выбросов (outliers). Следовательно, результирующий диапазон преобразованных значений признаков больше, чем для предыдущих скэйлеров и, что более важно, примерно одинаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z=\\frac{X-X_{median}}{IQR}$$\n",
    "\n",
    "$X_{median}$ — значение медианы, $IQR$ — межквартильный диапазон равный разнице между 75-ым и 25-ым процентилями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нашей задачи по определению раковых опухолей обработаем наши 30 признаков с помощью StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = StandardScaler().fit_transform(X)  # scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим что они стали намного более сравнимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_norm).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxenplot(data=pd.DataFrame(X_norm), \n",
    "                   orient=\"h\", \n",
    "                   palette=\"Set2\")\n",
    "ax.set(xlabel='Values', ylabel='Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные источники по работе с признаками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Feature Selection for High-Dimensional Data](https://www.springer.com/gp/book/9783319218571)\n",
    "2. [How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science)\n",
    "3. **Feature Engineering for Machine Learning: Principles and Techniques for Data Scientists Paperback** – April 14, 2018 by Alice Zheng , Amanda Casar\n",
    "4. [Сайт](https://dyakonov.org/) и [курс](https://github.com/Dyakonov/PZAD) Дьяконова\n",
    "5. Серия статей на towardsdatascience, [первая из серии](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "6. [A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "7. [Про кодирование циклических признаков](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Разделение train-validation-test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ??? Проблема переобучения \n",
    "idea: дать студентам визуальное представление о том, что такое переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: Опраксимация линейной зашумленной функции полиномом n-ой степени. Показать ошибку на трейне и на тесте. Гипперпараметр - степень полинома https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим kNN для общей выборки данных, при разном значении количества соседей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "np.random.seed(42)  # setting the initialization parameter for random values\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  # array of the number of neighbors\n",
    "\n",
    "quality = np.zeros(\n",
    "    n_nei_rng.shape[0]\n",
    ")\n",
    "\n",
    "X_norm = StandardScaler().fit_transform(X)  # scaled data\n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  # for all elements\n",
    "    # create knn for all num neighbors \n",
    "    knn = KNeighborsClassifier(\n",
    "        n_neighbors=n_nei_rng[ind]\n",
    "    )  \n",
    "    knn.fit(X_norm, Y)  \n",
    "    q = accuracy_score(y_pred=knn.predict(X_norm), y_true=Y)  # accuracy\n",
    "    quality[ind] = q  # fill quality\n",
    "\n",
    "plt.figure(figsize=(8, 5))  \n",
    "plt.title(\"KNN on train\", size=20)  \n",
    "plt.xlabel(\"Neighbors\", size=15)  \n",
    "plt.ylabel(\"Accuracy\", size=15)  \n",
    "plt.plot(n_nei_rng, quality)  \n",
    "plt.xticks(n_nei_rng) \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что качество на 1 соседе - самое лучшее. Но это и понятно - ближайшим соседом элемента из обучающей выборки будет сам объект. Мы просто **запомнили** все объекты.\n",
    "\n",
    "Если теперь мы попробуем взять какой-то новый образец опухоли и классифицировать его - у нас скорее всего ничего не получится. В таких случаях мы говорим, что наша модель не умеет обобщать (*generalization*).\n",
    "\n",
    "Для того, чтобы знать заранее обобщает ли наша модель или нет, мы можем разбить все имеющиеся у нас данныe на 2 части. Но одной части мы будем обучать классификатор (*train set*), а на другой тестировать насколько хорошо он работает (*test set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data to train/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "X_train_norm = scaler.transform(X_train)  # scaling data\n",
    "X_test_norm = scaler.transform(X_test)  # scaling data\n",
    "\n",
    "n_nei_rng = np.arange(1, 31)  \n",
    "train_quality = np.zeros(n_nei_rng.shape[0])  # quality on train data\n",
    "test_quality = np.zeros(n_nei_rng.shape[0])  # quality on test data\n",
    "\n",
    "for ind in range(n_nei_rng.shape[0]):  \n",
    "    knn = KNeighborsClassifier(n_neighbors=n_nei_rng[ind])  \n",
    "    knn.fit(X_train_norm, Y_train)  \n",
    "    \n",
    "    # accuracy on train data\n",
    "    trq = accuracy_score(y_pred=knn.predict(X_train_norm), y_true=Y_train)  \n",
    "    train_quality[ind] = trq  \n",
    "\n",
    "    # accuracy on test data\n",
    "    teq = accuracy_score(y_pred=knn.predict(X_test_norm), y_true=Y_test)  \n",
    "    test_quality[ind] = teq  \n",
    "\n",
    "# accuracy plot  on train and test data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот, теперь мы видим, что 1 сосед был \"ложной тревогой\". Такие случаи мы называем *переобучением*. Чтобы действительно предсказывать что-то полезное нам надо выбирать число соседей начиная минимум с 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Проблема подбора гиперпараметров на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая модель имеет как ряд **параметров**, которые она меняет в процессе обучения (например, веса модели), так и ряд **гиперпараметров**, которые влияют на то, каким способом модель меняет параметры в процессе обучения. \n",
    "\n",
    "В случае kNN параметры, строго говоря, отсутствует - модель просто запоминает объекты обучающей выборки. Особо упорные могут считать их параметрами. \n",
    "\n",
    "А вот гиперпараметры есть, даже несколько групп. Какие? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. число соседей \n",
    "2. функция, которой считаем расстояние между объектами (L2=eucledian, L1=manhattan)\n",
    "3. веса, с которыми складываем метки ближайших соседей\n",
    "4. признаки! (но об этом с вами поговорим позже)\n",
    "5. сама модель - мы могли выбрать не kNN, а нагуглить что-нибудь другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим еще раз на график, который нарисовали на прошлом шаге. Какое число соседей считать оптимальным? Метрика явно скачет? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Не понятно, насколько результат зависит от того, как нам повезло или не повезло с разбиением данных на обучение и тест. Может оказаться так, что для конкретного разбиения хорошо выбрать k=5, а для другого - k=7. \n",
    "\n",
    "Кроме того, опять же - фактически, мы сами выступаем в роли модели, которая учит гиперпараметры (а не параметры) под видимую ей выборку. \n",
    "\n",
    "Представим себе, что у нас есть 10000 моделей, полученных подкручиванием разных гиперпараметров (в том числе, выбором просто разного типа модели). Представим, что все эти модели не работают. Вообще. Представим так же, что каждая модель угадывает класс в задаче разделения на два класса с вероятностью 0.5 (и будем считать, что классы у нас сбалансированны - то есть 50% одного класса и 50% другого). \n",
    "Опять же, понятно, что классификация такими моделями ничем не лучше подбрасывания монетки. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_model(Y_real):\n",
    "    # array with values True/False, for random choice by mask \n",
    "    guessed = np.random.choice(\n",
    "        [True,False],  \n",
    "        size=Y_real.shape[0],\n",
    "        replace=True,\n",
    "    )\n",
    "    Y_pred = np.zeros_like(Y_real)  # zeros array, shape Y_real\n",
    "    \n",
    "    # with mask 'guessed' assign values \n",
    "    Y_pred[guessed] = Y_real[guessed]  \n",
    "    Y_pred[~guessed] = 1 - Y_real[~guessed]  \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_num = 10000  # num of experiments\n",
    "best_quality = 0.5  # порог качества по точности\n",
    "\n",
    "# array with random values in range 0, 1\n",
    "Y_real = np.random.choice(\n",
    "    [0, 1], size=250, replace=True\n",
    ")  \n",
    "\n",
    "for i in range(models_num):  # for all expirements\n",
    "    Y_pred = guess_model(Y_real)  # predicted values\n",
    "    q = accuracy_score(y_pred=Y_pred, y_true=Y_real)  # accuracy\n",
    "    if q > best_quality:  \n",
    "        best_quality = q   \n",
    "print(f\"Best result {best_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы перебором всех возможных моделей вполне можем получить для абсолютно бесполезной модели приемлемое качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Получается, что если подбирать гиперпараметры модели на *train set*, то:\n",
    "1. можно переобучитьcя, просто на более \"высоком\" уровне. Особенно если гиперпараметров у модели много и все они разнообразны\n",
    "2. нельзя быть уверенным, что выбор параметров не зависит от разбиения на обучение и тест "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы:\n",
    "\n",
    "1. подбираем гиперпараметры моделей на отдельном датасете, называмым валидационным. Получаем мы его разбиением обучающего датасета на собственно обучающий и валидационный \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/split_dataset_for_train_val_test.png\" width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры ошибок в данных и при разбиении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo Сюда можно добавить истоию с Хаски "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одним преимуществом наличия валидационного набора является то, что вы можете сделать **раннюю остановку** (*early stopping*), когда во время обучения одной модели, модель оценивается по валидационному набору на каждой итерации процесса обучения. Обучение прекращается, когда результат валидации начинает снижаться, так как это указывает на то, что модель начинает переобучаться на *train set*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе обучения модели, мы вообще-то хотели бы знать насколько хорошо она справляется с поставленной задачей. Если для оценки модели на каждой итерации (работы над моделью) мы будем использовать тестовый сет, мы просто оптимизируем модель к этому конкретному (тестовому) разбиению и потерям возможность объективно оценивать обобщающую способность модели (тестовое множество станет неявной частью процесса обучения [Cawley, 2010](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)). Вместо этого для измерения качества модели (и \"руководством над процессом обучения\") мы можем использовать отдельный (третий) набор для проверки - *validation/val set*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующий раз вам стоит воспользоваться тестом только уже при проверке обученной модели. А вот валидационный (*validation set*) нам понадобиться при обучении модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,0], X_train[:,1], color='red', alpha=0.5, label='Train')\n",
    "plt.scatter(X_val[:,0], X_val[:,1], color='green', alpha=0.5, label='Val')\n",
    "plt.scatter(X_test[:,0], X_test[:,1], color='blue', alpha=0.5, label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним \"Таблицу умножения\". Если мы хотим проверить умение умножать, то проверки примерами из таблицы умножения будет недостаточно, ведь она может быть полностью запомнена. Нужно давать новые примеры, которых не было в таблице умножения (обучающей выборке)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если модель \"запомнит всё\", то она будет идеально работать на данных, которые мы ей показали, но может вообще не работать на любых других данных.\n",
    "\n",
    "С практической точки зрения важно, как модель будет вести себя именно на незнакомых для неё данных. То есть насколько хорошо она научилась обобщать закономерности, которые в данных присутствовали (если они вообще существуют)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки этой способности набор данных разделяют на две, а иногда даже на три части:\n",
    "\n",
    "* train — Данные, на которых модель учится;\n",
    "* validation/test — Данные, на которых идет проверка.\n",
    "\n",
    "В `sklearn.model_selection` есть модель для разделения массива данных на тренировочную и тестовую часть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, class_labels, test_size=0.2) # 80% training and 20% test\n",
    "\n",
    "print(\"X_train shape\",X_train.shape)\n",
    "print(\"X_test shape\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А мы в дальнейшем будем пользоваться аналогичными инструментами библиотеки PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info: L1-Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение данных на обучающую и тестовую выборки \n",
    "\n",
    "Самым простым способом научиться чему-либо является \"запомнить всё\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Используйте соответствующий тестовый датасет(из publish or perish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде всего, всегда используйте тестовый набор для измерения обобщающей способности модели ML. То, насколько хорошо модель работает на обучающем множестве (*train set*), на практике не имеет никакого значения, и достаточно сложная модель может на нем полностью переобучится, но так и не научится обобщать тестовые данные. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####❗**КАРТИНКА С ПЕРЕОБУЧЕНИЕМ (Расхождение train и test)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичные ситуации могут возникать, когда для сбора обучающих и тестовых данных используется разное оборудование. **Например**, для обучения вы используете томограф из одного госпиталя, а оценку качества модели проводите на томографе из другого госпиталя. Если модель не учитывает характеристики оборудования, она, скорее всего, не будет обобщаться на томограф из второй больницы (при этом на своем собственном датасете из первого госпиталя, эту ошибку вы отловить не сможете)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####❗ **МЕСТО ДЛЯ КАРТИНКИ С ФОТОГРАФИЯМИ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также важно убедиться, что данные в тестовом наборе на самом деле подходят. То есть, они не должны пересекаться с обучающим набором и в то же время в достаточной степени отражать реальные данные. **Например**, рассмотрим датасет фотографий объектов, где изображения в обучающем и тестовом наборе были получены на открытом воздухе в солнечный день. Наличие одинаковых погодных условий означает, что тестовое множество не будет независимым, а поскольку оно не охватывает более широкий спектр погодных условий, оно также не будет репрезентативным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Правильно делайте разбиения (из Publish or perish) \n",
    "\n",
    "Что бы обучить модель, нам понадобиться разбить свои данные на несколько частей (`train-val-test`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассмотрим пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте создадим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset_size = 1000\n",
    "n_features = 2\n",
    "\n",
    "# Создадим рандомный датасет\n",
    "X = np.random.normal(size=(dataset_size,n_features))\n",
    "y = np.random.normal(size=(dataset_size,)) \n",
    "print('Размерность X', X.shape)\n",
    "print('Размерность y', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И разделим его на `train`, `val` и `test`. Если вы применяете случайное разбиение (как в следующем блоке кода), не забудьте зафиксировать random state, что бы в следующий раз вы получили ровно такую же тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Размерность X_train', X_train.shape)\n",
    "print('Размерность X_val', X_val.shape)\n",
    "print('Размерность X_test', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизируйте гиперпараметры вашей модели (Publish or perish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многие модели имеют **гиперпараметры** - то есть числа или параметры, которые влияют на конфигурацию модели. Примерами могут служить: функция ядра, используемая в SVM; количество деревьев в случайном лесу и архитектура нейронной сети. Многие из этих гиперпараметров существенно влияют на производительность модели, и, как правило, универсальных гиперпараметров (таких, которые были бы оптимальны для всех возможных задач) не существует.\n",
    "\n",
    "То есть, чтобы получить максимальную отдачу от модели, гииперпараметры нужно подбирать под конкретный набор данных. Хотя может возникнуть соблазн возиться с гиперпараметрами до тех пор, пока вы не найдете что-то подходящее, такой подход, скорее всего, не будет оптимальным. Гораздо лучше использовать какую-то стратегию **оптимизации гиперпараметров** (в качестве бонуса, обоснованная стратегия, в публикации смотрится значительно лучше, чем что-то в стиле *hyperparameters were chosen by chance*). Базовые стратегии включают случайный поиск и поиск по сетке, но они не очень хорошо масштабируются для большого количества гиперпараметров или для моделей, которые дорого обучать, поэтому стоит использовать инструменты, которые ищут оптимальные конфигурации более интеллектуальным способом (исследовано в [Yang et al., 2020](https://arxiv.org/abs/2007.15745))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Библиотеки для оптимизации гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует довольно много библиотек для оптимизации гиперпараметров. Ключевыми являются [Ray-tune](https://docs.ray.io/en/latest/tune/index.html), [Optuna](https://optuna.readthedocs.io/en/stable/) и [Hyperopt](https://github.com/hyperopt/hyperopt). В целом они друг от друга принципиально не отличаются, так что скорее это вопрос вкуса. В качестве примера (и для разнообразия) рассмотрим библиотеку `Ray-Tune`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ray-Tune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray[tune] tune-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.sklearn import TuneGridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 14)\n",
    "\n",
    "# Example parameters to tune from SGDClassifier\n",
    "parameter_grid = {\"alpha\": [1e-4, 1e-1, 1], \"epsilon\": [0.01, 0.1]}\n",
    "\n",
    "tune_search = TuneGridSearchCV(\n",
    "    SGDClassifier(),\n",
    "    parameter_grid,\n",
    "    early_stopping=True,\n",
    "    max_iters=10)\n",
    "\n",
    "tune_search.fit(x_train, y_train)\n",
    "\n",
    "#best set of perameter\n",
    "print(tune_search.best_params_)\n",
    "\n",
    "#best score with best set of perameters\n",
    "print(tune_search.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запомните, когда вы оптимизируете гиперпараметры или признаки, используемые моделью, для их отбора нужно использовать отдельный, валидационный датасет, или же кросс-валидацию. Вы **НЕ должны подбирать модель или ее гиперпараметры на тесте**.\n",
    "\n",
    "Также можно использовать методы **AutoML** для оптимизации выбора модели и ее гиперпараметров (см. обзор в [He et al., 2021](https://arxiv.org/abs/1908.00709))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Кросс-валидация\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перекрестная валидация (*cross-validation*) Publish or perish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы не можете получить больше данных - то вы можете лучше использовать уже имеющиеся, используя перекрестную валидацию (*cross validation*). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще говоря, кросс-валидация используется для более точной оценки модели. Одна оценка модели может быть ненадежной и может либо недооценить, либо переоценить истинный потенциал модели. По этой причине обычно проводится несколько оценок. Существует несколько способов провести множественную оценку модели, и большинство из них предполагает многократное обучение модели с использованием различных подмножеств обучающих данных. **Перекрсная валидация** (CV) особенно популярна и имеет множество разновидностей [Arlot et al., 2010](https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перекрсная валидация** в машинном обучении подразумеват, что вместо того, чтобы разделить наш набор данных на две части, одну для обучения (`train`), а другую для тестирования (`test`), мы разбиваем наш набор данных на несколько частей, обучаем на некоторых из них, а остальные используем для тестирования. Затем мы используем другие части для обучения и тестирования нашей модели. Это гарантирует, что наша модель обучается и тестируется на новых данных на каждом новом шаге.\n",
    "\n",
    "Чаще всего используют десятикратную прекресную валидацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разберем на примере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И разделим его на 10 (*K*) складок (*folds*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "X, y = make_moons(noise=0.3, random_state=42, n_samples=1000)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Lets test 100 times\n",
    "for i in range(0,100):\n",
    "    # For each experiment lets choose a different random state for splitting\n",
    "    random_state = np.random.randint(0,2**32)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # normalize data\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # predict using each classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    scores.append([random_state, classifier.score(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Split data intto 10 folds\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "#Define scaler and classifier\n",
    "scaler = MinMaxScaler()\n",
    "clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "\n",
    "#Define figure space\n",
    "fig,ax = plt.subplots(nrows=2, ncols=5, figsize=(10,4))\n",
    "\n",
    "row = 0\n",
    "scores = []\n",
    "\n",
    "#Itterate over folds\n",
    "for col, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    #Split\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    #Normalize\n",
    "    scaler.fit(X_train)\n",
    "    scaler.fit(X_test)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    #Classify\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    #Gauge performance\n",
    "    score = clf.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "    #Plot figure\n",
    "    if col > 4:\n",
    "        col-=5\n",
    "        row=1\n",
    "\n",
    "    ax[row, col].scatter(X_train[:,0], X_train[:,1], c=y_train, alpha=0.05)\n",
    "    ax[row, col].scatter(X_test[:,0], X_test[:,1], c=y_test, marker='x')\n",
    "    ax[row, col].set_title(score)\n",
    "    ax[row, col].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результирующую точность модели мы определим как среднее от всеех складок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Финальная точность = %.2f ± %.2f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для б**о**льшей надежности можно использовать метод **вложенной перекрестной валидации** (также известный как двойная кросс-валидация, (см. [Cawley, 2010](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf) и [Wainer et al., 2021](https://research-portal.uea.ac.uk/en/publications/nested-cross-validation-when-selecting-classifiers-is-overzealous))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кросс-валидация (L02)\n",
    "\n",
    "Каждая модель имеет как ряд **параметров**, которые она меняет в процессе обучения (например, веса модели), так и ряд **гиперпараметров**, которые влияют на то, каким способом модель меняет параметры в процессе обучения. \n",
    "\n",
    "В случае kNN параметры, строго говоря, отсутствует - модель просто запоминает объекты обучающей выборки. Особо упорные могут считать их параметрами. \n",
    "\n",
    "А вот гиперпараметры есть, даже несколько групп. Какие? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. число соседей \n",
    "2. функция, которой считаем расстояние между объектами (L2=eucledian, L1=manhattan)\n",
    "3. веса, с которыми складываем метки ближайших соседей\n",
    "4. признаки! (но об этом с вами поговорим позже)\n",
    "5. сама модель - мы могли выбрать не kNN, а нагуглить что-нибудь другое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим еще раз на график, который нарисовали на прошлом шаге. Какое число соседей считать оптимальным? Метрика явно скачет? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"KNN on train vs test\", size=20)\n",
    "plt.plot(n_nei_rng, train_quality, label=\"train\")\n",
    "plt.plot(n_nei_rng, test_quality, label=\"test\")\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.legend()\n",
    "plt.xticks(n_nei_rng)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Не понятно, насколько результат зависит от того, как нам повезло или не повезло с разбиением данных на обучение и тест. Может оказаться так, что для конкретного разбиения хорошо выбрать k=5, а для другого - k=7. \n",
    "\n",
    "Кроме того, опять же - фактически, мы сами выступаем в роли модели, которая учит гиперпараметры (а не параметры) под видимую ей выборку. \n",
    "\n",
    "Представим себе, что у нас есть 10000 моделей, полученных подкручиванием разных гиперпараметров (в том числе, выбором просто разного типа модели). Представим, что все эти модели не работают. Вообще. Представим так же, что каждая модель угадывает класс в задаче разделения на два класса с вероятностью 0.5 (и будем считать, что классы у нас сбалансированны - то есть 50% одного класса и 50% другого). \n",
    "Опять же, понятно, что классификация такими моделями ничем не лучше подбрасывания монетки. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_model(Y_real):\n",
    "    # array with values True/False, for random choice by mask \n",
    "    guessed = np.random.choice(\n",
    "        [True,False],  \n",
    "        size=Y_real.shape[0],\n",
    "        replace=True,\n",
    "    )\n",
    "    Y_pred = np.zeros_like(Y_real)  # zeros array, shape Y_real\n",
    "    \n",
    "    # with mask 'guessed' assign values \n",
    "    Y_pred[guessed] = Y_real[guessed]  \n",
    "    Y_pred[~guessed] = 1 - Y_real[~guessed]  \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_num = 10000  # num of experiments\n",
    "best_quality = 0.5  # порог качества по точности\n",
    "\n",
    "# array with random values in range 0, 1\n",
    "Y_real = np.random.choice(\n",
    "    [0, 1], size=250, replace=True\n",
    ")  \n",
    "\n",
    "for i in range(models_num):  # for all expirements\n",
    "    Y_pred = guess_model(Y_real)  # predicted values\n",
    "    q = accuracy_score(y_pred=Y_pred, y_true=Y_real)  # accuracy\n",
    "    if q > best_quality:  \n",
    "        best_quality = q   \n",
    "print(f\"Best result {best_quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть мы перебором всех возможных моделей вполне можем получить для абсолютно бесполезной модели приемлемое качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Получается, что если подбирать гиперпараметры модели на *train set*, то:\n",
    "1. можно переобучитьcя, просто на более \"высоком\" уровне. Особенно если гиперпараметров у модели много и все они разнообразны\n",
    "2. нельзя быть уверенным, что выбор параметров не зависит от разбиения на обучение и тест "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому мы:\n",
    "\n",
    "1. подбираем гиперпараметры моделей на отдельном датасете, называмым валидационным. Получаем мы его разбиением обучающего датасета на собственно обучающий и валидационный \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/split_dataset_for_train_val_test.png\" width=\"700\">\n",
    "\n",
    "2. чаще всего делаем несколько таких разбиений по какой-то схеме, чтобы получить уверенность оценок качества для моделей с разными гиперпараметрами - **кросс-валидация**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/cross_validation_on_train_data.png\" width=\"500\">\n",
    "\n",
    "Часто применяется следующий подход, называемый [K-Fold кросс-валидацией](https://scikit-learn.org/stable/modules/cross_validation.html):\n",
    "\n",
    "Берется тренировочная часть датасета, разбивается на части - блоки. Дальше мы будем использовать для проверки первую часть (Fold 1), а на остальных учиться. И так последовательно для всех частей. В результате у нас будут информация о точности для разных фрагментов данных и уже на основании этого можно понять, насколько значение этого параметра, который мы проверяем, зависит или не зависит от данных. То есть если у нас от разбиения точность при одном и том же К меняться не будет, значит мы подобрали правильное К. Если она будет сильно меняться в зависимости от того, на каком куске данных мы проводим тестирование, значит, надо попробовать другое К и если ни при каком не получилось - то это такие данные.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем параметры для модели с помощью **GridSearchCV**.\n",
    "\n",
    "GridSearchCV – это инструмент для автоматического подбирания параметров для моделей машинного обучения. GridSearchCV находит наилучшие параметры, путем обычного перебора: он создает модель для каждой возможной комбинации параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from IPython.display import clear_output\n",
    "\"\"\"\n",
    "Parameters for GridSearchCV:\n",
    "estimator — model\n",
    "cv — num of fold to cross-validation splitting \n",
    "param_grid — parameters names\n",
    "scoring — metrics \n",
    "n_jobs - number of jobs to run in parallel, -1 means using all processors.\n",
    "\"\"\"\n",
    "model = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    cv=KFold(3, shuffle=True, random_state=42),\n",
    "    param_grid={\n",
    "        \"n_neighbors\": np.arange(1, 31),\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train_norm, Y_train)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем лучшие гиперпараметры для модели, которые подобрали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metric:\", model.best_params_[\"metric\"])\n",
    "print(\"Num neighbors:\", model.best_params_[\"n_neighbors\"])\n",
    "print(\"Weigths:\", model.best_params_[\"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Объект GridSearchCV можно использовать как обычную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "Y_pred = model.predict(X_test_norm)\n",
    "print(f\"Percent correct predictions {np.round(accuracy_score(y_pred=Y_pred, y_true=Y_test)*100,2)} %\")\n",
    "print(f\"Percent correct predictions(balanced classes) {np.round(balanced_accuracy_score(y_pred=Y_pred, y_true=Y_test)*100,2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем извлечь дополнительные данные о кроссвалидации и по ключу обратиться к результатам всех моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем для примера mean_test_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(model.cv_results_[\"mean_test_score\"])\n",
    "plt.title(\"mean_test_score\", size=20)\n",
    "plt.xlabel(\"Num of experiment\", size=15)\n",
    "plt.ylabel(\"Accuracy\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим, например, при фиксированных остальных параметрах (равных лучшим параметрам), качество модели на валидации в зависимости от числа соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_means = []\n",
    "selected_std = []\n",
    "n_nei = []\n",
    "for ind, params in enumerate(model.cv_results_[\"params\"]):\n",
    "    if (\n",
    "        params[\"metric\"] == model.best_params_[\"metric\"]\n",
    "        and params[\"weights\"] == model.best_params_[\"weights\"]\n",
    "    ):\n",
    "        n_nei.append(params[\"n_neighbors\"])\n",
    "        selected_means.append(model.cv_results_[\"mean_test_score\"][ind])\n",
    "        selected_std.append(model.cv_results_[\"std_test_score\"][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим error bar, для сравнения разброса ошибки при разном количестве соседей Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(f\"KNN CV, {params['metric']}, {params['weights']}\", size=20)\n",
    "plt.errorbar(n_nei, selected_means, yerr=selected_std, linestyle=\"None\", fmt=\"-o\")\n",
    "plt.xticks(n_nei)\n",
    "plt.ylabel(\"Mean_test_score\", size=15)\n",
    "plt.xlabel(\"Neighbors\", size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на самом деле большой разницы в числе соседей и нет. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Можно ли делать только кросс-валидацию (без теста)?**\n",
    "\n",
    "\n",
    "Нет, нельзя. Кросс-валидация не до конца спасает от подгона параметров модели под выборку, на которой она проводится. Оценка конечного качества модели должно производиться на отложенной тестовой выборке. Если у вас очень мало данных, можно рассмотреть [вложенную кросс-валидацию](https://weina.me/nested-cross-validation/), Речь об этом пойдет позже, в последующих лекциях. Но даже в этом случае придется анализировать поведение модели, чтобы показать, что она учит что-то разумное. Кстати, вложенную кросс-валидацию можно использовать, чтобы просто получить более устойчивую оценку поведения модели на тесте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм кросс валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Оценка результата кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Типичные ошибки при кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-валидация для научных исследований - на что обратить внимание. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RandomGridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/use_knn_to_comare_imgs_to_find_similar.jpg\" width=\"800\">\n",
    "\n",
    "Давайте подумаем, как избавиться от проблемы со скоростью K-Nearest Neighbors. Реализуя метод ближайшего соседа, мы сравнивали одно изображение со всеми, при этом мы предполагали, что изображения из одного класса будут чем-то похожи друг на друга, и поэтому разница будет меньше. Метод как-то работал. \n",
    "\n",
    "Чтобы ускорить этот процесс, мы можем взять весь блок изображений (справа), который относится, например, к машинам и усредним. Таким образом, получим некоторый шаблон для класса “автомобиль”. Скорее всего, работать такой подход будет не слишком здорово, но зато вместо тысяч изображений (в данном случае 50000) появится одно. Возможно, это поможет сильно сэкономить время."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Переход к сравнению с шаблоном\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/one_template_for_each_class_for_classification.jpg\" width=\"450\">\n",
    "\n",
    "Идея в следующем: вместо того, чтобы сравнивать каждое изображение со всеми остальными по очереди, будем сравнивать изображения с шаблоном для каждого класса. Их будет всего 10 для данного датасета. Этот подход позволит радикально увеличить скорость. \n",
    "\n",
    "Проверим, что получится из этой идеи. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cкачиваем CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import numpy as np\n",
    "\n",
    "dataset_train = CIFAR10(\"content\", train=True,  download=True)\n",
    "dataset_test = CIFAR10(\"content\", train=False,  download=True)\n",
    "\n",
    "X_train = dataset_train.data\n",
    "Y_train = np.array(dataset_train.targets)\n",
    "\n",
    "X_test = dataset_test.data\n",
    "Y_test = np.array(dataset_test.targets)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_train shape : {X_test.shape}, Y_test shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cоздадим шаблоны посчитав среднее значение пикселя по всем изображениям одного класса.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = []  \n",
    "\n",
    "for i in range(len(dataset_train.classes)):\n",
    "    imgs = X_train[Y_train == i] # select images by class mask\n",
    "    mn = np.mean(imgs, 0 )  \n",
    "    templates.append(mn.astype(int)) # convert to int for display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем полученные шаблоны:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (20.0, 2.0))\n",
    "\n",
    "def show_templates(templates, labels):\n",
    "    for i, template in enumerate(templates):\n",
    "        plt.subplot(1, len(labels), i + 1)\n",
    "        plt.title(labels[i])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(template)\n",
    "\n",
    "show_templates(templates, dataset_train.classes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть 10 шаблонов, с которыми мы будем сравнивать изображение и на основании этого делать предсказание. На этих шаблонах можно увидеть очертания объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем классификатор, который будет сравнивать изображение с шаблоном, который генерируется во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateBasedClassifier():\n",
    "    def __init__(self, labels):\n",
    "        self.templates = []          \n",
    "        self.labels = labels\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.templates = []\n",
    "        for label_num in range(len(self.labels)):  \n",
    "          imgs = X_train[Y_train == label_num] # select images by class mask\n",
    "          mn = np.mean(imgs, 0 )  \n",
    "          self.templates.append(mn)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute distance score\n",
    "        distances = np.sum(np.abs(self.templates - x), axis=(1,2,3))  \n",
    "        return np.argmin(distances)  # return minimum score index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим сравнение на конкретном примере с кошкой. Построим изображения картинки, шаблона, а затем посчитаем расстояние L1 между ними. Чем желтее цвет - тем больше изображение похоже на шаблон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig,ax = plt.subplots(ncols=3, figsize=(10,5))\n",
    "\n",
    "car_class_num = 1\n",
    "imgs = X_train[Y_train == car_class_num] # images class 3(cat)\n",
    "\n",
    "img = imgs[1] # some image with car\n",
    "template = templates[car_class_num] \n",
    "residual = np.mean(np.abs(img-template), axis = -1) \n",
    "\n",
    "# Code for display\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(template)\n",
    "r_plot = ax[2].imshow(residual, cmap='inferno_r')\n",
    "\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(r_plot, cax=cax, orientation='vertical', label='L1')\n",
    "\n",
    "ax[0].set_title('Image')\n",
    "ax[1].set_title('Template')\n",
    "ax[2].set_title('D = Image - Template')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemplateBasedClassifier(dataset_train.classes)  \n",
    "model.fit(X_train, Y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказание и замерим время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def validate(model, X_test, Y_test):\n",
    "    y_pred = []  # num of correct predictions\n",
    "    for i, img in enumerate(X_test):  # for all images\n",
    "        index = model.forward(img)  # predict class\n",
    "        y_pred.append(index)\n",
    "    return accuracy_score(Y_test, y_pred)\n",
    "\n",
    "start = time.perf_counter()  \n",
    "accuracy= validate(model, X_test, Y_test)  \n",
    "tm = time.perf_counter() - start \n",
    "print(\n",
    "    \"\\nAccuracy {:.2f} Train {:d} /test {:d} in {:.1f} sec. speed {:.2f} samples per second.\".format(\n",
    "        accuracy, len(X_train), len(X_test), tm, len(X_test) / tm\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если бы мы обучали KNN на таком количестве данных, это заняло бы у нас четверть часа.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Переход к весам\n",
    "----\n",
    "Умножение вместо вычитания - для чего?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/compute_scalar_product_with_compare_with_template.jpg\" width=\"450\">\n",
    "\n",
    "В сравнении с шаблонами, мы вычитали нашу картинку из шаблона и таким образом смотрели, насколько они похожи (L1). Теперь давайте сделаем следующий, пока ничем не обоснованный, ход: оставим всё, как было, но заменим вычитание умножением. Логика в том, что не все пиксели одинаково важны. Вероятно, если на изображении совпадут какие-то пиксели, которые отвечают, например, за глаза кошки, это будет намного важнее, чем фон, который может быть точно таким же у собаки. Здесь будут какие-то важные особенности, которым можно придать больший вес.\n",
    "\n",
    "Если мы распишем в виде формул наложение шаблона на картинку таким образом с умножением, то получается, что мы скалярно перемножаем два вектора. Подробнее про [скалярное произведение](https://ru.wikipedia.org/wiki/Скалярное_произведение) векторов. Для того, чтобы получить вектор из изображения размерностью 32х32х3, достаточно \"выпрямить\" его, получим вектор размерностью 1х3072.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опять рассмотрим сравнение на конкретном примере с кошкой. Построим изображения картинки, шаблона, а затем посчитаем расстояние между ними, используя наш новый метод перемножения. Чем желтее цвет - тем больше изображение похоже на шаблон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3, figsize=(10,5))\n",
    "\n",
    "indexes = np.where(Y_train == 3)  \n",
    "mask = np.zeros(len(Y_train), dtype=bool) \n",
    "mask[indexes, ] = True  \n",
    "imgs = X_train[mask]\n",
    "\n",
    "img = imgs[0].reshape(3, 32, 32).transpose(1, 2, 0).astype(int)\n",
    "template = templates[3].reshape(3, 32, 32).transpose(1, 2, 0).astype(int)\n",
    "residual = np.mean(np.abs(img*template),-1)\n",
    "\n",
    "ax[0].imshow(img)\n",
    "ax[1].imshow(template)\n",
    "r_plot = ax[2].imshow(residual, cmap='inferno_r')\n",
    "\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(r_plot, cax=cax, orientation='vertical', label='Distance')\n",
    "\n",
    "ax[0].set_title('Image')\n",
    "ax[1].set_title('Template')\n",
    "ax[2].set_title('D = Image * Template')\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Математическая запись"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/img_to_vector_to_compute_scalar_product.jpg\" width=\"700\">\n",
    "\n",
    "\n",
    "\n",
    "Обозначим входное изображение как $x$, а шаблон для первого из классов как $w_0$.\n",
    "\n",
    "Элементы пронумеруем подряд 1,2,3 … $n$. То есть развернем матрицу пикселей изображения в вектор. \n",
    "\n",
    "Тогда результат сравнения изображения с этим шаблоном будет вычисляться по формуле: $x[0]*w0[0] + x[1]*w0[1] + … x[n-1]*w0[n-1]$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/scalar_product_ways_to_use.png\" width=\"800\">\n",
    "\n",
    "\n",
    "Эта простая модель лежит в основе практически всех сложных, которые мы будем рассматривать дальше. Внутри мы будем также пользоваться скалярным произведением.\n",
    "\n",
    "В дальнейшем мы будем проходить сверточные сети, они работают очень похоже:\n",
    "мы тоже накладываем шаблон на некоторую матрицу и перемножаем элементы, затем складываем. Единственное отличие – обычно ядро свертки меньше, чем размер самого изображения. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (метод опорных векторов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Отличное видео про SVM от Stat Quest которое все объясняет](https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим одномерный пример. У нас есть данные по массе мышей. Часть из них определена как нормальные, а часть как мыши с ожирением. Чтобы их отделить друг от друга, нам достаточно одного критерия. Мы можем посмотреть на график, и визуально определить предельную массу, после которой мышки будут жирненькими"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def generate_data(total_len=40):\n",
    "    X = np.hstack([np.random.uniform(14,21, total_len//2), \n",
    "                   np.random.uniform(24,33, total_len//2)])\n",
    "    Y = np.hstack([np.zeros(total_len//2), \n",
    "                   np.ones(total_len//2)])\n",
    "    return X,Y\n",
    "\n",
    "def plot_data(X, Y, total_len=40, s=50, threshold=None, margin=None):\n",
    "    ax = sns.scatterplot(x=X, y=np.zeros(len(X)), hue=Y, s=s)\n",
    "    if threshold:\n",
    "        ax.axvline(threshold, color='red', ls='dashed')\n",
    "    if margin:\n",
    "        for line in margin:\n",
    "            ax.axvline(line, color='pink', ls='dashed')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, threshold=21.5, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пользуясь нашим простым критерием, попробуем классифицировать каких-то новых мышей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.random.uniform(14,30, 5)\n",
    "\n",
    "def classify(X, threshold=21.5):\n",
    "    y = np.zeros_like(X)\n",
    "    y[X > threshold] = 1\n",
    "    return y\n",
    "\n",
    "total_len = 40\n",
    "threshold = 21.5\n",
    "X, Y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, threshold=threshold, total_len=total_len)\n",
    "ax = plot_data(X_test, classify(X_test, threshold), total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если наши мыши находятся тут?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([21.45, 22.5])\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)\n",
    "ax = plot_data(X_test, classify(X_test), threshold = 21.5, total_len=total_len, s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения нашего классификатора - все четко. Больше порогового значения - значит перевес, меньше - значит нормальные. Но с точки зрения здравого смысла, логичнее было бы классифицировать обоих мышей как нормальных, так как они значительно ближе к нормальным, чем к ожиревшим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вооружившись этим новым знанием, попробуем классифицировать наших отъевшихся мышек по-умному. Возьмем крайние точки в каждом класстере. И в качестве порогового значения будем использовать среднее между ними"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data(total_len=total_len)\n",
    "normal_limit = X[Y==0].max() # extreme point for 'normal'\n",
    "obese_limit = X[Y==1].min() # extreme point for 'obese'\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit]) # separated with mean value \n",
    "\n",
    "X_test = np.array([21.5, 23])\n",
    "ax = plot_data(X, Y, total_len=total_len, threshold=threshold, margin=[normal_limit, obese_limit])\n",
    "ax = plot_data(X_test, classify(X_test, threshold=threshold), total_len=total_len, s=300, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем посчитать, насколько наша мышь близка к тому, чтобы оказаться в другом классе. Такое расстояние называется **margin**. И оно считается как $\\mathrm{margin} = |\\mathrm{threshold} - \\mathrm{observation}|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = np.abs(X_test - threshold)\n",
    "print(margins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если мы посчитаем margins для наших крайних точек `normal_limit` и `obese_limit`, мы найдем самое большое возможное значение margin для нашего классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_0 = np.abs(normal_limit - threshold)\n",
    "margin_1 = np.abs(obese_limit - threshold)\n",
    "print(margin_0, margin_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой классификатор, мы называем **Maximum Margin Classifier**. Он хорошо работает в случае, когда все данные размечены аккуратно. Теперь рассмотрим более реалистичный пример, где что-то пошло не так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_data(total_len=40):\n",
    "    X = np.hstack([np.random.uniform(14,21, total_len//2), np.random.uniform(24,33, total_len//2)])\n",
    "    Y = np.hstack([np.zeros(total_len//2), np.ones(total_len//2)])\n",
    "    indx = np.where(X == X[Y==1].min())[0]\n",
    "    Y[indx] = 0\n",
    "    s = np.ones_like(X)*50\n",
    "    s[indx] = 300\n",
    "    return X,Y,s\n",
    "\n",
    "total_len = 40\n",
    "X,Y,s = generate_realistic_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len, s=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае, наш **Maximum Margin Classifier** работать не будет. Исходя из этого, мы можем прийти к выводу, что наш классификатор очень чувствителен к выбросам. Давайте подумаем можно ли это как-то исправить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы можем разрешить нашему классификатору ошибаться. Если мы будем использовать в качестве порогового значения не самое крайнее, а следующее за ним - мы промажем в классификации конкретно этой странной точки. Но в целом будем правы. Margin определенный таким образом, называется **Soft margin**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "X, Y, s = generate_realistic_data(total_len=total_len)\n",
    "\n",
    "normal_limit = np.sort(X[Y==0])[-2]\n",
    "obese_limit = np.sort(X[Y==1])[1]\n",
    "\n",
    "threshold = np.mean([normal_limit, obese_limit])\n",
    "\n",
    "X_test = np.array([20.5, 25])\n",
    "ax = plot_data(X, Y, total_len=total_len, threshold=threshold)\n",
    "ax = plot_data(X_test, classify(X_test, threshold=threshold), total_len=total_len, s=300, threshold=threshold, margin=[normal_limit, obese_limit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но почему мы решили взять именно следующее значение? Почему не через 2? Откуда мы знаем что это лучший из возможных вариантов? А ни откуда не знаем. Чтобы узнать какой из margins лучше, нам стоит численно это проверить и посчитать, сколько раз мы ошибемся, если возьмем в качестве порогового значения между каждой парой точек. Для этого мы вновь воспользуемся *кросс-валидацией*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 40\n",
    "X, Y, s = generate_realistic_data(total_len=total_len)\n",
    "\n",
    "indxs = []\n",
    "accuracies = []\n",
    "thresholds = []\n",
    "\n",
    "X_test, Y_test = generate_data(total_len=total_len)\n",
    "\n",
    "for i in range(total_len//2):\n",
    "    for j in range(total_len//2-1):\n",
    "        normal_limit = np.sort(X[Y==0])[-i]\n",
    "        obese_limit = np.sort(X[Y==1])[j]\n",
    "\n",
    "        threshold = np.mean([normal_limit, obese_limit])\n",
    "        \n",
    "        Y_pred = classify(X_test, threshold=threshold)\n",
    "        accuracy = np.mean(Y_test == Y_pred)\n",
    "        indxs.append((-i,j))\n",
    "        accuracies.append(accuracy)\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "print(f'Accuracy = {np.max(accuracies)*100}%')\n",
    "best_index = indxs[np.argmax(accuracies)][1]\n",
    "print(f'Best indexes', best_index)\n",
    "best_treshold = thresholds[np.argmax(accuracies)]\n",
    "print('Best treshhold value %.2f'% best_treshold)\n",
    "\n",
    "ax = plot_data(X, Y, total_len=total_len, threshold=best_treshold)\n",
    "ax = plot_data(X_test, Y_test, \n",
    "               total_len=total_len, \n",
    "               s=200, \n",
    "               threshold=best_treshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда для классификатора используется **Soft Margin** - такой классификатор называют **Soft Margin Classifier** или по другому - **Support Vector Classifier**. По сути это уже SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D класификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример, где мы измерили не только вес мышей, но и их длину от хвоста до носа. Мы можем вновь применить наш метод Support Vector Classifier, и теперь классы разделяет не одно пороговое значение (по сути, точка), а линия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import svm\n",
    "\n",
    "def generate_2d_data(total_len=40):\n",
    "    X, Y = make_blobs(n_samples=total_len, centers=2, random_state=42)\n",
    "    X[:,0] += 10 \n",
    "    X[:,1] += 20 \n",
    "    return X, Y\n",
    "\n",
    "def plot_data(X, Y, total_len=40, s=50, threshold=21.5):\n",
    "    ax = sns.scatterplot(x=X[:,0], y=X[:,1], hue=Y, s=s)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g', ylabel='Length, cm');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_2d_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)\n",
    "\n",
    "# Code for illustration, later we will understand how it works\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel='linear', C=1000)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы добавим еще одно измерение - возраст, мы обнаружим, что наши данные стали трехмерными, а разделяет их теперь не линия, а плоскость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_data(total_len=40):\n",
    "    X, Y = make_blobs(n_samples=total_len, centers=2, random_state=42, n_features=3)\n",
    "    X[:,0] += 10 \n",
    "    X[:,1] += 20 \n",
    "    X[:,2] += 10\n",
    "    return X, Y\n",
    "\n",
    "def plot_data(X, Y, total_len=40, s=50, threshold=21.5):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(xs=X[:,0], ys=X[:,1], zs=X[:,2], c=Y, s=s, cmap='Set1')\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    ax.plot_surface(XX, YY, XX*YY*0.2, alpha=0.2)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Normal', 'Obese'])\n",
    "    ax.set(xlabel='Mass, g', ylabel='Length, cm', zlabel='Age, days');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_3d_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, если бы у нас было 4 измерения и больше (например: вес, длинна, возраст, кровяное давление), то многомерная плоскость которая бы разделяла наши классы - называлась бы **гиперплоскость** (рисовать мы ее, конечно же, не будем). Чисто технически, и точка, и линия - тоже гиперплоскости. Но все же гиперплоскостью принято называть то, что нельзя нарисовать на бумаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные не всегда разделяются так хорошо как в случае нашего мышиного датасета. Например, рассмотрим следующее: у нас есть данные по дозировке лекарства и 2 класса - пациенты, которые поправились, и те, которым лучше не стало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patients_data(total_len=40):\n",
    "    X = np.random.uniform(0,50, total_len)\n",
    "    Y = np.zeros_like(X)\n",
    "    Y[(X > 15) & (X < 35)] = 1\n",
    "    return X, Y\n",
    "\n",
    "def plot_data(X, Y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=X, y=np.zeros(len(X)), hue=Y, s=s)\n",
    "\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Recover', 'Sick'])\n",
    "    ax.set(xlabel='dose, mg');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X, Y = generate_patients_data(total_len=total_len)\n",
    "ax = plot_data(X, Y, total_len=total_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно мы не можем найти такое пороговое значение, которое будет разделять наши классы на больных и здоровых, а следовательно, и Support Vector Classifier работать тоже не будет.  Для начала, давайте преобразуем наши данные таким образом, что бы они стали 2-х мерными. В качестве значений по оси Y будем использовать дозу возведенную в квадрат (**доза**$^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, Y, total_len=40, s=50):\n",
    "    ax = sns.scatterplot(x=X[0,:], y=X[1,:], hue=Y, s=s)\n",
    "    handles, labels  =  ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, ['Recover', 'Sick'])\n",
    "    ax.set(xlabel='Dose, mg');\n",
    "    ax.set(ylabel='Dose$^2$');\n",
    "    return ax\n",
    "\n",
    "total_len = 40\n",
    "X_1, Y = generate_patients_data(total_len=total_len)\n",
    "X_2 = X_1**2\n",
    "X = np.vstack([X_1, X_2])\n",
    "\n",
    "plot_data(X, Y, total_len=40, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем вновь использовать Support Vector Classifier для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, Y, total_len=40, s=50)\n",
    "\n",
    "x = np.linspace(0,50,50)\n",
    "xs = [X[0,:][Y==1].min(), X[0,:][Y==1].max()]\n",
    "ys = [X[1,:][Y==1].min(), X[1,:][Y==1].max()]\n",
    "\n",
    "# Calculate the coefficients.\n",
    "coefficients = np.polyfit(xs, ys, 1)\n",
    "\n",
    "# Let's compute the values of the line...\n",
    "polynomial = np.poly1d(coefficients)\n",
    "y_axis = polynomial(x)\n",
    "\n",
    "# ...and plot the points and the line\n",
    "plt.plot(x, y_axis, 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тут возникает резонный вопрос - почему мы решили возвести в квадрат? Почему не в куб? Или наоборот не извлечь корень? Как нам решить какое преобразование использовать?\n",
    "\n",
    "И у нас есть **вторая проблема** - а если перейти надо в пространство очень большой размерности? В этом случае наши данные очень сильно увеличатся в размере.\n",
    "\n",
    "Комбинация двух проблем дает нам **много радости** - надо перебирать большое число возможных пространств большей размерности\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Однако основная фишка Support Vector Machine состоит в том, что внутри он работает на скалярных произведениях. И можно эти скалярные произведения считать, **не переходя в пространство большей размерности**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого SVM использует **Kernel Function**. \n",
    "\n",
    "Kernel Function может, например, быть полиномом (**Polynomial Kernel Function**), который имеет параметр $d$ - сколько размерностей выбрать. \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/svm_kernel_function.png\" width=\"700\">\n",
    "\n",
    "Примеры ядер :\n",
    "\n",
    "* $k(x_i, x_j) = (<x_i, x_j> + c)^d, с, d \\in \\mathbb{R}$ - полиномиальное ядро, считает расстояние между объектами в пространстве размерности d\n",
    "\n",
    "* $k(x_i, x_j) = \\frac{1}{z} e^{-\\frac{h(x_i, x_j)^2}{h}}$ - радиальная базисная функция RBF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, в случае SVM можно легко перебрать много таких пространств на кроссвалидации и выбрать более удобное. \n",
    "\n",
    "Более того, SVM может проверять пространства признаков бесконечного размера. Если для такого пространства существует kernel function. Иногда такие пространства оказываются очень удобными для решения задач. Часто используют тот же RBF-пространство, приведенное выше. А оно как раз бесконечно-мерное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример простой линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ненадолго отвлечемся от SVM и рассмотрим другую задачу. В этой задаче мы будем прогнозировать успеваемость студента, в зависимости от количества часов, которые он учил материал. Это простая задача линейной регрессии, поскольку она включает всего две переменные.\n",
    "\n",
    "**Регрессия** - это статистический метод, используемый в финансах, инвестировании и других дисциплинах, который пытается определить силу и характер связи между одной зависимой переменной (обычно обозначаемой **Y**) и рядом других переменных (известных как независимые переменные). В задачах регрессии, мы будем пытаться минимизировать ошибку между предсказанием и истинными данными. \n",
    "\n",
    "\"Регрессия\" происходит от слова \"регресс\", которое, в свою очередь, происходит от латинского \"regressus\" - возвращаться (к чему-либо). В этом смысле регрессия - это техника, которая позволяет \"вернуться назад\" от беспорядочных, трудно интерпретируемых данных к более четкой и осмысленной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L02/student_scores.csv\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что там в нем. Видим, что у нас есть два признака - часы и результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"/content/student_scores.csv\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график зависимости одного от другого, а так же отобразим распределения каждой из переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=dataset, x=\"Scores\", y=\"Hours\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данные на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.iloc[:, :-1].values # column Hours\n",
    "Y = dataset.iloc[:, 1].values # column Score\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим модель для линейной регрессии. Чтобы не писать с нуля, воспользуемся готовой моделью из библиотеки `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И обучим ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_points = np.linspace(\n",
    "    min(X_train), max(X_train), 100\n",
    ")  # 100 dots at min to max\n",
    "Y_pred = regressor.predict(X_points)  \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X_train, Y_train, \"o\", label=\"Scores\")\n",
    "plt.plot(X_points, Y_pred, label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_ ))\n",
    "plt.title(\"Hours vs Percentage\", size=15)\n",
    "plt.xlabel(\"Hours Studied\", size=15)\n",
    "plt.ylabel(\"Percentage Score\", size=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем предсказание для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regressor.predict(X_test)  \n",
    "\n",
    "X_points = np.linspace(\n",
    "    min(X_test), max(X_test), 100\n",
    ")  \n",
    "Y_pred = regressor.predict(X_points) \n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(X_test, Y_test, \"o\", label=\"Scores\")\n",
    "plt.plot(X_points, Y_pred, label=\"y = %.2fx+%.2f\" % (regressor.coef_[0], regressor.intercept_ ))\n",
    "plt.title(\"Hours vs Percentage\", size=15)\n",
    "plt.xlabel(\"Hours Studied\", size=15)\n",
    "plt.ylabel(\"Percentage Score\", size=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит не плохо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики для наших значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "print(\"Mean Absolute Error: %9.2f\" % metrics.mean_absolute_error(Y_test, Y_pred))\n",
    "print(\"Mean Squared Error: %10.2f\" % metrics.mean_squared_error(Y_test, Y_pred))\n",
    "print(\"Root Mean Squared Error: %5.2f\" % np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Геометрическая интерпретация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы разобрались с тем, что такое регрессия и с чем ее едят, вернемся к нашим картинкам. Как можно применить регрессию для классификации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим у нас есть только 2 класса. Как можно использовать регрессию для того, чтобы определить относится ли изображение к классу 0 или к классу 1? В упрощенном варианте, задача будет состоять в том, чтобы провести разделяющую плоскость (прямую) между 2-мя классами. Например, мы можем провести прямую через 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/regression_for_classification_imgs.png\" width=\"270\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим другую ситуацию, в этом случае, мы не можем просто провести прямую через 0. Но можем отступить от 0 на какое-то расстояние и провести ее там. Вспомним, что уравнение прямой это $y=wx+b$, где $b$ - это смещение (*bias*). Соответственно если b != 0, то прямая через 0 проходить не будет, а будет проходить через значение b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/regression_for_classification_add_bias.png\" width=\"270\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear Classification Loss Visualization\n",
    "](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)\n",
    "\n",
    "\n",
    "Если у нас есть несколько классов (несколько шаблонов), мы можем для каждого из них посчитать уравнение $y_{i} = w_{i}x_{i}+b_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/regression_for_classification_add_bias_add_multiclasses.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На картинке нас интересуют 3 класса. Соответственно, мы можем записать систему линейных уравнений:\n",
    "\n",
    "\\begin{aligned}\n",
    "y_{0} = w_{0}x_{0} + b_{0} \\\\\n",
    "y_{1} = w_{1}x_{1} + b_{1} \\\\\n",
    "y_{2} = w_{2}x_{2} + b_{2} \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление смещения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы их можем собрать в матрицу, тогда получится следующее:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/scalar_product_add_bias.jpg\" width=\"750\">\n",
    "\n",
    "У нас есть матрица коэффициентов, которые мы каким-то образом подобрали, пока ещё не понятно как. Есть вектор $x$, соответствующий изображению. \n",
    "\n",
    "Мы умножаем вектор на матрицу, получаем нашу гиперплоскость для четырехмерного пространства в данном случае. Чтобы оно не лежало в 0, мы должны добавить смещение. И мы можем сделать это после, но можно взять и этот вектор смещения (вектор **b**) просто приписать к матрице **W**.\n",
    "\n",
    "Что будет выходом такой конструкции? Мы умножили матрицу весов на наш вектор, соответствующий изображению, получили некоторый отклик. По этому отклику мы так же, как и при реализации метода ближайшего соседа можем судить: если он больше остальных, то мы предполагаем, что это кошка.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([56, 231, 24, 2])\n",
    "w_cat = np.array([0.2, -0.5, 0.1, 2.0])\n",
    "print(\"Image \", img)\n",
    "print(\"Weights \", w_cat)\n",
    "print(\"img * w_cat \", img * w_cat)\n",
    "print(\"sum \", (img * w_cat).sum())\n",
    "print(\"Add bias \", (img * w_cat).sum() + 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/img_to_function_get_scores.jpg\" width=\"600\">\n",
    "\n",
    "\n",
    "Собирая все вместе, получаем какое-то компактное представление, что у нас есть некоторая функция, на вход которой мы подаем изображение, и у нее есть параметры (веса). Пока происходит просто умножение вектора на матрицу, в дальнейшем это может быть что-то более сложное, функция будет представлять какую-то более сложную модель. А на выходе (для классификатора) мы получаем числа, которые интерпретируют уверенность модели в том, что изображение принадлежит к определенному классу.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L02/img_license/input_img_scalar_product_add_bias_get_scores.jpg\" width=\"450\">\n",
    "\n",
    "\n",
    "\n",
    "Соответственно, эти коэффициенты, которые являются весами модели, надо каким-то образом подбирать. Но прежде чем подбирать коэффициенты, давайте определимся со следующим: как мы будем понимать, что модель работает хорошо или плохо? Ведь она возвращает достаточно абстрактные числа, которые нужно уметь интерпретировать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понятие линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспоминаем KNN: низкая скорость"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переход к сравнению с шаблоном"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переход к скалярному произведению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример линейной регресии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики регрессии (MSE, MAE, R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance tradeoff на примере линейной регрессии и KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возврат к классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## на примере SVM лосс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Понятие градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Большие объемы данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия для бинарной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение с помощью градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мультиклассовая классификация (объект может принадлежать к нескольким классам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-энтропия, softmax и логистическая регрессия для мультиклассовой постановки"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
