{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Обучение на реальных данных</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проблемы при работе с реальной задачей машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальных задачах, особенно если вы работаете над новой проблемой, вы с столкнетесь с широким спектром проблем. Приведем часть из них и сопроводим примером на основе задач нахождения клеток крови на фотографии мазка крови\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/bloods_cell.jpg\" width=\"500px\">\n",
    "\n",
    "- **нехватка данных** - фотографий мазков крови может быть недостаточно для построения сложной модели с нуля\n",
    "\n",
    "- **недостаток размеченных данных** - возможно, существует достаточно большое количество фотографий мазков крови (например, в историях болезни), но очень малая часть из них размечена\n",
    "\n",
    "- **некачественная разметка** - мазок крови могли доверить анализировать студенту-практиканту. Размечать его мог вообще человек не из профессии - например, хотевший таким образом увеличить обучающую выборку для модели на конкурс kaggle. Даже в широко известных MNIST, CIFAR и ImageNet есть ошибки в разметке ([визуализация](https://labelerrors.com/))\n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/wrong_imagenet_prediction.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "- **низкое качество данных** - не все фотографии будут в хорошем разрешении. Да и не все образцы были правильно подготовлены. А еще могут попадаться фотографии от пациентов, больных чем-нибудь экзотическим, в результате чего их клетки выглядят сильно отлично от обычных. Или же в крови пациента просто плавают паразиты\n",
    "\n",
    "Серповидная клеточная анемия приводит к аномальным эритроцитам\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/normal_and_sickle_shaped_erythrocytes.JPG\" width=\"500px\">\n",
    "\n",
    "А так выглядит мазок крови при сонной болезни \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/trypanosoma_among_red_blood_cells.jpg\" width=\"500px\">\n",
    "\n",
    "  \n",
    "\n",
    "- **несбалансированность датасета** - клетки крови встречаются в разных пропорциях. Какие классы могут быть плохо представлены (**минорные класссы**) Потому в вашем датасете может быть всего 10 фотографий, на которых присутствуют базофилы. Нейросети будет очень заманчиво вообще не пытаться найти базофилы (всего 10 ошибок). \n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/leukocyte_blood_formula_table.jpg\" width=\"500px\">\n",
    "\n",
    "- **полные дупликаты** - в данных могут быть полные дубликаты. Кто-то до вас аггрегировал фотографии из разных источников и вы либо не обратили на это внимание, либо он забыл об этом сказать. Такие данные надо сразу помечать и использовать только после предварительного размышления, т.к они могут мешать вам и на этапе обучения модели, и на итоговой валидации ее качества (если один и тот же объект попадет и в обучение, и в валидацию). \n",
    "\n",
    "- **неполные дубликаты** - в данных могут быть данные от одного и того же пациента. Кажется, что если это разные мазки крови - то ок. На самом деле и это уменьшает количество информации, которую может извлечь нейросеть из данных. С такими данными также нужно аккуратно работать и не допускать попадания одного пациента и в обучение, и в тест\n",
    "\n",
    "- **малое число источников данных** - проблема, родственная предыдущей. В вашем датасете могут быть данные только от одного микроскопа или одной модели микроскопа. Могут быть данные, снятые только одним специалистом или в одной больнице или только у взрослых (фотографий мазков детей нет). Это так же может влияьб на способность вашего алгоритма генерализовывать полученное решение и требует пристального внимания. \n",
    "\n",
    "\n",
    "Все это приводит к целому спектру проблем, из которых самой типичной будет переобучение модели - какую простую б модель вы не взяли, она все равно будет выучивать искажения вашего датасета. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общие подходы при работе с реальными данными\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Большее количество данных \n",
    "\n",
    "Если у вас мало данных - попробуйте найти еще данных для вашей задачи. \n",
    "Совет приводится во многих инструкциях по борьбе с малым количеством данных и может быть воспринят с юмором. \n",
    "Однако часто для вашей задаче действительно существуют данные, собранные другими людьми. Также часто можно найти данные, которые очень похожи на ваши и их можно использовать в обучении, но, например, учитывать с меньшим весом\n",
    "Даже 20 дополнительных примеров могут сильно облегчить ситуацию. \n",
    "\n",
    "Вы также можете использовать данные, которые не совсем похожи на ваши, в качестве внешней валидации. Тем самым вы можете разбивать свой изначальный датасет только для кросс-валидации, и не выделять отдельную часть для теста. Тестом послужат как раз \"не совсем\" похожие данные. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изменение баланса класса сэмплированием \n",
    "\n",
    "Если в данных недостаток именно конкретного класса, то можно бороться с этим при помощи разных способов сэмплирования. \n",
    "\n",
    "Важно понимать, что в большинстве случаев данные, полученные таким способом должны использоваться в качестве **обучающего набора**, но ни в коем случае не в качестве **валидации** или **теста**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дублирование примеров меньшего класса (oversampling)\n",
    "\n",
    "Мы можем увеличить число объектов меньшего класса за счет дублирования. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/oversampling_and_undersampling.png\" width=\"600\">\n",
    "\n",
    "В этом случае наша модель будет \"вынуждена\" обращать внимание на минорный класс. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уменьшение числа примеров бОльшего класса (undersampling)\n",
    "\n",
    "Аналогично, можно взять для обучения не всех представителей бОльшего класса. \n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/oversampling_and_undersampling.png\" width=\"600\"> \n",
    "\n",
    "Это также вынуждает модель обращать внимание на оба класса. \n",
    "Минус подхода очевиден - мы можем выбросить важных представителей бОльшего класса, ответственных за существенное улучшение генерализации,  и за счет этого качество модели существенно ухудшится. \n",
    "Можно пытаться выбрасывать объекты бОльшего класса как-то по-умному - например, кластеризовать объекты бОльшего класса и брать по заданному количеству объектов из каждого класса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ансамбли + undersampling\n",
    "\n",
    "Можно использовать ансамбли вместе с undersampling. В этом случае мы можем, к примеру, делать сэмплирование только бОльшего класса, а объекты минорного класса оставлять как есть. \n",
    "\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/ensembles_and_undersampling.png\" width=\"700px\">\n",
    "\n",
    "Или просто сэмплировать объектов и того, и другого класса равное количество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Балансирование представленности объектов в батчах\n",
    "\n",
    "В случае нейросетей можно балансировать встречаемость каждого класса не на уровне датасета, а на уровне батча. Например, собираем каждый батч таким образом, чтобы в нем было поровну всех классов.\n",
    "\n",
    " Это может улучшать сходимость даже в случае небольшого дисбаланса или его отсутствия, т.к мы будем избегать шагов обучения нейросети, в которых она просто не увидела какого-то класса в силу случайных причин.\n",
    "\n",
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/batch_balancing.png\" width=\"500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация синтетических данных\n",
    "\n",
    "Другой подход к решению этой проблемы - создание синтетических данных. Делать это можно по-разному. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация данных на основе имеющихся \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SMOTE\n",
    "\n",
    "**Synthetic Minority Over-sampling Technique (SMOTE)** позволяет генерировать синтетические данные за счет реальных объектов из минорного класса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм работает следующим образом:\n",
    "\n",
    "1. для случайной точки из минорного класса выбираем $k$ ближайших соседей из того же класса. \n",
    "2. Для первого соседа проводим отрезок, соединяющий его и выбранную точку. На этом отрезке случайно выбираем точку. \n",
    "3. Эта точка - новый **синтетический** объект минорного класса.\n",
    "4. Повторяем процедуру для оставшихся соседей.\n",
    "\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/generate_synthetic_data.png\" width=\"700\">\n",
    "\n",
    "Число соседей, как и число раз, которое мы запускаем описанную выше процедуру можно регулировать.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что можно сделать, что бы как-то улучшить ситуацию используя методы ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Можно **изменить функцию потерь**: для задач классификации мы часто используем кросс-энтропийный лосс и редко используем среднюю абсолютную ошибку или среднеквадратичную ошибку для обучения и оптимизации нашей модели.\n",
    "\n",
    "- В случае несбалансированных данных, модель учится распознавать доминирующий класс чаще чем все остальные, соответственно, она выучивает статистическое распределение классов и считает что чем чаще она предсказывает этот класс - тем меньше она ошибается. Что бы как-то решить эту проблему - мы можем **добавить веса к потерям**, соответствующим различным классам, чтобы выровнять это смещение данных. Например, если у нас есть два класса в соотношении 4:1, мы можем применить веса в соотношении 1:4 к вычислению функции потерь, чтобы данные были сбалансированы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнаружение аномалий / изменений\n",
    "\n",
    "В случае сильно несбалансированных наборов данных, таких как мошенничество или машинный сбой, стоит задуматься, могут ли такие примеры рассматриваться как аномалия (выброс) или нет. Если такое событие и впрямь может считаться аномальным, мы можем использовать такие модели, как `OneClassSVM`, методы кластеризации или методы обнаружения гауссовских аномалий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти методы требуют изменения способа мышления: мы будем рассматривать аномалии, как отдельный класс выбросов. Это может помочь нам найти новые способы разделения и классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обнаружение изменений похоже на обнаружение аномалий, за исключением того, что мы ищем изменение или отличие, а не аномалию. Это могут быть изменения в поведении пользователя, наблюдаемые по шаблонам использования или банковским транзакциям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем пример обнаружения аномалий с помощью `OneClassSVM` из библиотеки Sk-Learn (там же, можно найти еще множеество различных алгоритмов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create dataset\n",
    "X_data, _ = make_blobs(n_samples=1000, centers=1, cluster_std=1.1, center_box=(0, 0))\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1], alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроим наш детектор аномалий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "svm = OneClassSVM(kernel=\"rbf\", gamma=0.01, nu=0.03)\n",
    "print(f'SVM parameters:\\n{svm}')\n",
    "svm.fit(X_data)\n",
    "pred = svm.predict(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем аномальные точки предсказанные детектором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "anom_index = np.where(pred == -1)\n",
    "values = X_data[anom_index]\n",
    "\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1], alpha=0.25)\n",
    "plt.scatter(values[:, 0], values[:, 1], color=\"r\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не используйте точность для несбалансированных наборов данных\n",
    "\n",
    "Будьте осторожны с тем, какие метрики вы используете для оценки ваших ML-моделей. Например, в случае моделей классификации, наиболее часто используемой метрикой является точность (*accuracy*), которая представляет собой долю сэмплов в наборе данных, которые были правильно классифицированы моделью.\n",
    "\n",
    "Этот метод хорошо работает, если классы сбалансированы, т.е. каждый класс представлен одинаковым количество образцов в наборе данных. Но многие наборы данных не являются сбалансированными, и в этом случае точность может быть очень обманчивой метрикой. Рассмотрим, например, датасет, в котором 90% сэмплов представляют один класс, а 10% сэмплов представляют\n",
    "другой класс. Бинарный классификатор, который всегда выдает первый класс, независимо от его входных данных, будет иметь точность 90%, несмотря на то, что он совершенно бесполезен. В такой ситуации предпочтительнее использовать такие метрики, как коэффициент каппы Коэна или коэффициент корреляции Мэтьюса (MCC), которые относительно нечувствительны к дисбалансу размеров классов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аугментация\n",
    "Другой способ побороть маленькое количество данных для обучения - аугментация. Сам термин пришел из музыки:\n",
    "\n",
    "**Аугмента́ция** (позднелат. augmentatio — увеличение, расширение) — [техника ритмической композиции в старинной музыке](https://ru.wikipedia.org/wiki/Аугментация).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели глубокого обучения обычно требуют большого количества данных для обучения. В целом, чем больше данных, тем лучше для обучения модели. В то же время получение огромных объемов данных сопряжено со своими проблемами (например с нехваткой размеченных данных или с трудозатратами сопряженными с разметкой).\n",
    "\n",
    "Вместо того, чтобы тратить дни на сбор данных вручную, мы можем использовать методы аугментации для автоматической генерации новых примеров из уже имеющихся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо увеличения размеченных датасетов, многие методы *self-supervised learning* построены на использовании разных аугментаций одного и того же сэмпла.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/augmentations_examples.png\" width=\"700\"></center>\n",
    "<center><em>Примеры аугментаций картинки. </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важный момент**: при обучении модели мы используем разбиение данных на `train-val-test`. Аугментации стоит применять только на `train`. Почему так? Конечная цель обучения нейросети - это применение на реальных данных, которые сеть не видела. Вот и портить их не надо.\n",
    "\n",
    "В любом случае, `test` должен быть отделен от данных еще до того как они попали в `DataLoader` или нейросеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое дело, что аугментации на тесте можно использовать как метод ансамблинга в случае классификации. Можно взять sample -> создать несколько его копий -> по разному их аугментировать -> предсказать класс на каждой из этих аугментированных копий -> а потом выбрать наиболее вероятный класс голосованием (такой функционал реализован например в [YOLOv5](https://github.com/ultralytics/yolov5/blob/d204a61834d0f6b2e73c1f43facf32fbadb6b284/models/yolo.py#L121), о которой речь пойдет в следующих лекциях)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и отобразим пример картинки. Картинку отмасштабируем, что бы она не занимала весь экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "# fix random_seed\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "URL = 'https://edunet.kea.su/repo/EduNet-web_dependencies/L12/capybara_image.jpg'\n",
    "!wget -q $URL -O test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "input_img = Image.open('/content/test.jpg')\n",
    "input_img = transforms.Resize(size=300)(input_img) \n",
    "display(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций картинок. С полным списком можно ознакомиться на сайте [[doc] документации torchvision](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие параметры принимает на вход `Random Rotation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.RandomRotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим перменную `transform` в которую добавим нашу аугментацию и применим ее к исходному изображению. Затем запустим следующую ячейку несколько раз подряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomRotation(degrees=(0, 180))\n",
    "\n",
    "def plot_augmented_img(transform, input_img):\n",
    "    \n",
    "    f,ax = plt.subplots(1,2,figsize=(15,15))\n",
    "    augmented_img = transform(input_img)\n",
    "    ax[0].imshow(input_img)\n",
    "    ax[0].set_title('Original img')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(augmented_img)\n",
    "    ax[1].set_title('Augmented img')\n",
    "    ax[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.GaussianBlur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.RandomErasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [ \n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=1),\n",
    "        transforms.ToPILImage()\n",
    "    ]\n",
    ")\n",
    "    \n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.ColorJitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ColorJitter(brightness=.5, hue=.3)\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого будем использовать метод `Compose`. Нам нужно будет создать `list` со всеми аугментациями, которые будут применены последовательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0),\n",
    "        transforms.ColorJitter(brightness=.5, hue=.3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А что если мы хотим применять аугментации случайным образом?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого воспользуемся методом `RandomApply`, который на вход принимает метод аугментации, и вероятность `p` что эта аугментация будет применена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? transforms.RandomApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomApply(\n",
    "    transforms=[\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5),\n",
    "        transforms.ColorJitter(brightness=.5, hue=.3)\n",
    "    ],\n",
    "    p=0.7\n",
    ")\n",
    "\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация внутри `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем папку с картинками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.chdir('/content')\n",
    "# download files\n",
    "!wget --no-check-certificate 'https://edunet.kea.su/repo/EduNet-web_dependencies/L11/for_transforms.Compose.zip' -O data.zip\n",
    "with ZipFile('data.zip', 'r') as folder: # Create a ZipFile Object and load sample.zip in it\n",
    "    folder.extractall() # Extract all the contents of zip file in current directory\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/content/for_transforms.Compose\")\n",
    "img_list = os.listdir()\n",
    "print(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, img_list, transforms=None):\n",
    "        self.img_list = img_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = plt.imread(self.img_list[i])\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "        img = np.array(img).astype(np.uint8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вспомогательную функцию для отображения картинок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.figure(figsize=(40, 38))\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `list` с аугментациями, которые мы хотим применить. Что бы загрузить аугментации в `PyTorch`, нам необходимо эти картинки преобразовать в тензоры. Для этого воспользуемся родным торчевым преобразованием `transforms.ToTensor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose(\n",
    "    [   \n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обернем все в `DataLoader` и отобразим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, tensor_transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует и более сложные способы аугментации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mixup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/mixup_augmentation_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Аугументация при помощи синтеза данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/augmentation_using_data_synthesis.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме методов, реализованных в Pytorch, существуют и специализированные библиотеки для аугментации изображений. \n",
    "\n",
    "Например:\n",
    "- [lbumentations](https://albumentations.ai)\n",
    "- [imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "- [augly](https://github.com/facebookresearch/AugLy)\n",
    "\n",
    "**При выборе методов аугментации имеет смысл использовать те, которые будут в реальной жизни. Например, нет смысла делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным, или отражать человека вверх-ногами, если мы не предполагаем его таким распознавать.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций аудио. С полным списком можно ознакомиться здесь [[git] audiomentations](https://github.com/iver56/audiomentations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеку и посмотрим на пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "os.chdir('/content')\n",
    "\n",
    "!pip install audiomentations\n",
    "\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L11/audio_example.wav\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "\n",
    "# Get input audio\n",
    "input_audio ='/content/audio_example.wav'\n",
    "\n",
    "display(Audio(input_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data, sr = librosa.load('/content/audio_example.wav') # sr - sampling rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, AddGaussianSNR\n",
    "import numpy as np\n",
    "\n",
    "augment = AddGaussianSNR(min_snr_in_db=3, max_snr_in_db=7, p=1)\n",
    "\n",
    "# Augment/transform the audio data\n",
    "augmented_data = augment(samples=data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним волновые картины и спектрограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "import numpy as np \n",
    "\n",
    "def produce_plots(input_audio_arr, aug_audio, sr):\n",
    "    f,t, Sxx_in = spectrogram(input_audio_arr, fs=sr) # Compute spectrogram for the original signal (f - frequency, t - time)\n",
    "    f,t, Sxx_aug = spectrogram(aug_audio, fs=sr)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20,5)) \n",
    "\n",
    "    ax[0,0].plot(input_audio_arr)\n",
    "    ax[0,0].set_xlim(0,len(input_audio_arr))\n",
    "    ax[0,0].set_xticks([])\n",
    "    ax[0,0].set_title('Original audio')\n",
    "\n",
    "    ax[0,1].plot(aug_audio)\n",
    "    ax[0,1].set_xlim(0,len(input_audio_arr))\n",
    "    ax[0,1].set_xticks([])\n",
    "    ax[0,1].set_title('Augmented  audio')\n",
    "\n",
    "    ax[1,0].imshow(np.log(Sxx_in), \n",
    "                   extent = [t.min(), t.max(), f.min(), f.max()],\n",
    "                   aspect='auto',\n",
    "                   cmap='inferno')\n",
    "    ax[1,0].set_ylabel('Frequecny, Hz')\n",
    "    ax[1,0].set_xlabel('Time,s')\n",
    "    \n",
    "    ax[1,1].imshow(np.log(Sxx_aug), \n",
    "                   extent = [t.min(), t.max(), f.min(), f.max()],\n",
    "                   aspect='auto',\n",
    "                   cmap='inferno')\n",
    "    ax[1,1].set_ylabel('Frequecny, Hz')\n",
    "    ax[1,1].set_xlabel('Time,s')\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = TimeStretch(min_rate=0.8, max_rate=1.5, p=1) \n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае с картинками мы можем совмещать несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=1),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=1),\n",
    "    Shift(min_fraction=-0.5, max_fraction=0.5, p=1),\n",
    "])\n",
    "\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные библиотеки для аугментации звука (и волновых функций в целом):\n",
    "- [torchaudio](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)\n",
    "- [torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations)\n",
    "- [Augly](https://github.com/facebookresearch/AugLy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим несколько примеров аугментаций текста. С полным списком можно ознакомиться здесь [[git] сайте библиотеки](https://github.com/makcedward/nlpaug)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input text\n",
    "text = \"Hello, future of AI for Science! How are you today?\"\n",
    "print(f'input text: {text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация символов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменой на выглядящие похоже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "augment = nac.OcrAug()\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С опечатками, которые учитывают расположение символов на клавиатуре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = nac.KeyboardAug()\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С орфографическими ошибками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "augment = naw.SpellingAug()\n",
    "augmented_text = augment.augment(text, n=3)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С использованием модели для предсказания новых слов в зависимости от контекста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "# model_type: word2vec, glove or fasttext\n",
    "augment = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=\"insert\")\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем переводить текстовые данные на какой-либо язык, а затем перевести их обратно на язык оригинала. Это может помочь сгенерировать текстовые данные с разными словами, сохраняя при этом контекст текстовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='facebook/wmt19-en-de', \n",
    "    to_model_name='facebook/wmt19-de-en'\n",
    ")\n",
    "augmented_text = back_translation_aug.augment(text)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные  библиотеки для аугментации текста:\n",
    "\n",
    "\n",
    "- [TextAugment](https://github.com/dsfsi/textaugment)\n",
    "- [Augly](https://github.com/facebookresearch/AugLy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "----\n",
    "Как обучить нейросеть на своих данных, когда их мало?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для таких типовых задач, как классификация изображений, можно воспользоваться одной из существующих архитектур (AlexNet, VGG, Inception, ResNet и т.д.) и просто обучить нейросеть на своих данных. Реализации таких сетей с помощью различных фреймворков уже существуют, так что на данном этапе можно использовать одну из них как черный ящик, не вникая в принцип её работы. Например в PyTorch есть много уже реализованных известных архитектур: [`torchvision.models`](https://pytorch.org/vision/stable/models.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, глубокие нейронные сети требуют большие объемы данных для успешного обучения. И зачастую, в нашей частной задаче недостаточно данных для того, чтобы хорошо обучить нейросеть с нуля. `Transfer learning` решает эту проблему. По сути, мы пытаемся использовать опыт, полученный нейронной сетью при обучении на некоторой задаче $T_1$, чтобы решать схожую задачу $T_2$.\n",
    "\n",
    "К примеру, `transfer learning` можно использовать при решении задачи классификации изображений на небольшом наборе данных. Как уже ранее обсуждалось, при обработке изображений свёрточные нейронные сети в первых слоях \"реагируют\" на некие простые пространственные шаблоны (к примеру, углы), после чего комбинируют их в сложные осмысленные формы (к примеру, глаза или носы). Вся эта информация извлекается из изображения, создавая новые сложные представления данных, в результате классифицируемые линейной моделью. \n",
    "\n",
    "Идея заключается в том, что если изначально обучить модель на некоторой сложной и довольно общей задаче, можно надеяться что она (как минимум часть ее слоеев) в общем случае будет извлекать важную информацию из изображений и полученные представления можно будет успешно использовать для классификации линейной моделью.\n",
    "\n",
    "Таким образом, берем часть модели, которая, по нашему представлению, отвечает за выделение хороших признаков (часто - все слои, кроме последнего) - feature extractor. Присоединяем к это части дополнительные слой/cлои для решения уже новой задачи. И учим только эти слои. Cлои feature extractor не учим - они \"заморожены"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/transfer_learning_change_classes_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятно, что не все фильтры модели будут использованы максимально эффективно - к примеру, если мы работаем с изображениями, связанными с едой, возможно не все фильтры на скрытых слоях предобученной на ImageNet модели окажутся полезны для нашей задачи. Почему бы не попробовать **не только обучить новый классификатор, но и дообучить некоторые промежуточные слои**? При использовании этого подхода мы при обучении дополнительно \"настраиваем\" и промежуточные слои, называется он `fine-tuning`.\n",
    "\n",
    "В дальнейшем примере мы вообще не будем фиксировать веса - этот вариант по сути также относится к `fine-tuning`. В таком подходе learning rate ставится ниже, чем при обучении нейросети с нуля - мы знаем, что по крайней мере часть весов нейросети выполняют свою задачу хорошо и не хотим испортить это быстрыми первыми изменениями. \n",
    "\n",
    "Кроме этого, можно делать комбинации этих методов - сначала учить только последние, добавленные нами слои сети. Затем учить еще и самые близкие к ним. Затем учить уже все веса нейросети вместе. То есть мы можем определить свою **стратегию fine-tuning**. \n",
    "\n",
    "Иногда **fine-tuning** считается синонимом \"transfer learning\", в этом случае часть от предтренированной сети называют **backbone**, а добавленную часть - **head**.  Подробнее про это можно прочитать [здесь](https://lightning-flash.readthedocs.io/en/latest/general/finetuning.html)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структурные компоненты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения данной задачи нам понадобятся несколько компонент.\n",
    "\n",
    "1. В первую очередь, нам необходима сама обученная модель, которую мы будем адаптировать под нашу локальную задачу. Модель можно получить из фреймворка, либо скачать по ссылке, приложенной авторами интересующей Вас статьи. \n",
    "\n",
    "2. Также, как и при работе с обычной моделью, понадобится кодовая база для обучения, включающая в себя известные шаги: подсчёт градиента, шаг оптимизатора, подсчёт качества на проверочной выборке и т.п.  \n",
    "\n",
    "3. После того как обучение будет выполнено, понадобится использовать модель в реальных условиях, либо как минимум проверить её качество на тестовой выборке. В данном случае, нам понадобится кодовая база для выполнения предсказаний. \n",
    "\n",
    "4. В некоторых случаях, веса модели находятся отдельно от описания (архитектуры) модели, в таком случае необходимо дополнительно озаботиться их получением и загрузкой в модель.  \n",
    "\n",
    "5. Последняя важная компонента - данные. Необходимо тщательно подготовить данные, на которых будет дообучаться модель, проверить разбиение на обучающую, проверяющую и тестировочную выборки, чтобы избежать утечек данных и т.п.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/structural_components_scheme.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практический пример Transfer Learning\n",
    "\n",
    "Давайте рассмотрим пример практической реализации такого подхода ([код переработан из этой статьи](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Загрузим датасет и удалим из него 90% файлов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.chdir('/content')\n",
    "path = '/content/2750/'\n",
    "\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L11/EuroSAT.zip # http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
    "!unzip EuroSAT.zip\n",
    "\n",
    "from random import sample\n",
    "for folder in os.listdir(path):\n",
    "  files = os.listdir(path+folder)\n",
    "  for file in sample(files,int(len(files)*0.9)):\n",
    "      os.remove(path+folder+'/'+file)\n",
    "      \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим аугментации. Для разнообразия будем использовать родные аугментации из библиотеки PyTorch Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Applying Transforms to the Data\n",
    "img_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # No augmentations on valid data!\n",
    "    \"valid\": transforms.Compose( \n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # No augmentations on test data!\n",
    "    \"test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.utils\n",
    "\n",
    "dataset = datasets.ImageFolder(root=path)\n",
    "# split to train/valid/test\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.1), int(len(dataset)*0.1)])\n",
    "# define augmentations\n",
    "train_set.dataset.transform = img_transforms['train']\n",
    "valid_set.dataset.transform = img_transforms['valid']\n",
    "test_set.dataset.transform = img_transforms['test']\n",
    "\n",
    "print(f'Train size: {len(train_set)}')\n",
    "print(f'Valid size: {len(valid_set)}')\n",
    "print(f'Test size: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size, valid_data_size = len(train_set), len(valid_set)\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "print('indexes to class: ')\n",
    "idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наборе данных не так уж и много изображений. При обучении с нуля нейросеть скорее всего не достигнет высокой точности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим AlexNet без весов и попробуем обучить с нуля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "alexnet = models.alexnet(pretrained=False)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы заменяем последний слой модели AlexNet слоем с `num_classes` нейронами, в соответствии с числом классов в нашем подмножестве.\n",
    "\n",
    "То есть мы \"сказали\" нашей модели распознавать не `1000`, а только `num_classes` классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "import torch.nn as nn\n",
    "\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes) # change out classes, from 1000 to 10\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim=1)) # add activation\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы определяем функцию потерь и оптимизатор, который будет использоваться для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-4)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренировки и валидации нашей модели напишем отдельную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_and_validate(model, criterion, optimizer, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "\n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clean existing gradients\n",
    "            outputs = model(inputs)  # Forward pass - compute outputs on input data using the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the parameters\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Compute correct predictions\n",
    "            train_correct += (torch.argmax(outputs, dim=-1)== labels).float().sum()\n",
    "\n",
    "        # Compute the mean train accuracy\n",
    "        train_accuracy = 100 * train_correct / (len(train_loader)*batch_size)\n",
    "\n",
    "        val_correct = 0\n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.eval()  # Set to evaluation mode\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass - compute outputs on input data using the model\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                valid_loss += loss.item() * inputs.size(0)  # Compute the total loss for the batch and add it to valid_loss\n",
    "\n",
    "                val_correct += (torch.argmax(outputs, dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Compute mean val accuracy       \n",
    "        val_accuracy = 100 * val_correct / (len(valid_loader)*batch_size)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss / (len(train_loader)*batch_size)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss / (len(valid_loader)*batch_size)\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, train_accuracy, val_accuracy])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(\n",
    "            \"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
    "                epoch + 1,\n",
    "                avg_train_loss,\n",
    "                train_accuracy.detach().cpu(),\n",
    "                avg_valid_loss,\n",
    "                val_accuracy.detach().cpu(),\n",
    "                epoch_end - epoch_start,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), criterion, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_fresh.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты оставляют желать лучшего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем использовать *transfer learning*. \n",
    "\n",
    "Загрузим предобученную модель Alexnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del alexnet\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае, мы не дообучаем скрытые слои нашей модели, потому отключаем подсчёт градиентов (\"**замораживаем**\" параметры). Данный шаг гарантирует, что обучение (изменение параметров) будет происходить только на последнем слое модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы поменяем последний слой таким образом, что бы он вместо 1000 классов ImageNet нам выдавал количество классов которое у нас есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), criterion, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_finetuning.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сравним между собой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history_fresh = np.array(torch.load('history_fresh.pt'))\n",
    "history_finetuning = np.array(torch.load('history_finetuning.pt'))\n",
    "\n",
    "ax[0].plot(history_fresh[:,:2])\n",
    "ax[0].plot(history_finetuning[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "\n",
    "ax[1].plot(history_fresh[:,2:])\n",
    "ax[1].plot(history_finetuning[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определенно стало лучше =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_img_name, device):\n",
    "    \"\"\"\n",
    "    Function to predict the class of a single test image\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param test_img_name: Test image\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    transform = img_transforms[\"test\"]\n",
    "    test_img = torch.tensor(np.asarray(test_img_name)) \n",
    "    test_img = transforms.ToPILImage()(test_img)\n",
    "    plt.imshow(test_img)\n",
    "\n",
    "    test_img_tensor = test_img_name.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_img_tensor).to(device)\n",
    "        ps = torch.exp(out).to(device)\n",
    "        topk, topclass = ps.topk(3, dim=1)\n",
    "        for i in range(3):\n",
    "            print(\n",
    "                \"Predcition\",\n",
    "                i + 1,\n",
    "                \":\",\n",
    "                idx_to_class[topclass.cpu().numpy()[0][i]],\n",
    "                \", Score: \",\n",
    "                round(topk.cpu().numpy()[0][i], 2),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[0])\n",
    "predict(\n",
    "    trained_model.to(device),\n",
    "    valid_set[[np.where([x[1] == 0 for x in valid_set])[0]][0][1]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[6])\n",
    "predict(\n",
    "    trained_model,\n",
    "    valid_set[[np.where([x[1] == 6 for x in valid_set])[0]][0][10]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[8])\n",
    "predict(\n",
    "    trained_model,\n",
    "    valid_set[[np.where([x[1] == 8 for x in valid_set])[0]][0][5]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Мы только что увидели, как использовать предварительно обученную модель, обученную для 1000 классов ImageNet.\n",
    "\n",
    "* Она очень эффективно классифицирует изображения, принадлежащие к интересующим нас $N$ классам.\n",
    "\n",
    "* Также мы убедились в эффективности упомянутых подходов для обучения нейросети на небольшом наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ищем знакомые лица](https://habr.com/ru/post/317798/)\n",
    "\n",
    "Глубокие нейронные сети на стали самыми современными методами для задач классификации изображений. Однако одно из самых больших ограничений - они требуют большого количества размеченных данных.\n",
    "\n",
    "Во многих приложениях иногда невозможно собрать такой объем данных и **Few-Shots Learning** направлен на решение этой проблемы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частным случаем Few-shots является **One-Shot learning**. Предполагается, что при классификации по методу One-Shot Learning нам требуется только один обучающий пример для каждого класса (соответственно для few-shots примеров нужно несколько). Отсюда и название One-Shot. Давайте попробуем разобраться на реальном практическом примере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, мы хотим создать систему распознавания лиц для небольшой организации, в которой всего 10 сотрудников (небольшое количество упрощает задачу).\n",
    "\n",
    "Используя традиционный подход к классификации, мы можем придумать систему, которая выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/classifier_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемы:\n",
    "\n",
    "а) Чтобы обучить такую ​​систему, нам сначала потребуется много разных изображений каждого из 10 человек в организации, что может оказаться невозможным (представьте, что вы делаете это для организации с тысячами сотрудников).\n",
    "\n",
    "б) Что делать, если новый человек присоединяется к организации или покидает ее? Вам нужно снова взять на себя боль сбора данных и заново обучить всю модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это практически невозможно, особенно для крупных организаций, где набор и увольнение происходит почти каждую неделю.\n",
    "\n",
    "Теперь давайте разберемся, как подойти к этой проблеме, используя классификацию по **One-Shot Learning**, которая помогает решить обе указанные выше проблемы:\n",
    "\n",
    "Вместо того, чтобы напрямую классифицировать входное (тестовое) изображение каждого из 10 человек в организации, эта сеть вместо этого принимает дополнительное эталонное изображение человека в качестве входных данных и будет производить оценку сходства, обозначающую шансы того, что два входных изображения принадлежат одному и тому же человеку.\n",
    "\n",
    "Обратите внимание, что эта сеть не учится классифицировать изображение напрямую по какому-либо из выходных классов. Скорее, он изучает функцию подобия, которая принимает два изображения в качестве входных данных и выражает, насколько они похожи.\n",
    "\n",
    "Как это решает две проблемы, о которых мы говорили выше?\n",
    "- Для обучения этой сети нам не требуется слишком много экземпляров класса, а для построения хорошей модели достаточно лишь нескольких экземпляров.\n",
    "- Но самое большое преимущество в простоте ее обучения в случае появления нового сотрудника."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, чтобы сеть могла обнаружить его лицо, нам требуется только одно изображение его лица, которое будет сохранено в базе данных. Используя его в качестве эталонного изображения, сеть будет вычислять сходство для любого нового экземпляра,  представленного ей.\n",
    "\n",
    "Таким образом, мы говорим, что сеть предсказывает счет за `One-Shot` (один выстрел)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код, который мы будем использовать, является реализацией методологии, описанной в статье [Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf). Но прежде чем углубляться в детали, давайте разберемся с методологией на высоком уровне.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/siamese_neural_network_scheme.png\" width=\"700\"></center>\n",
    "<center><em>Две сверточные нейронные сети не являются разными сетями, а представляют собой две копии одной и той же сети, отсюда и название Siamese Networks. В основном они имеют одинаковые параметры.</em></center>\n",
    "\n",
    "Два входных изображения ($x_1$ и $x_2$) проходят через ConvNet, чтобы сгенерировать вектор признаков фиксированной длины для каждого $h_{x_{1}}$ и $h_{x_{2}}$.\n",
    "\n",
    "Предполагая, что модель нейронной сети обучена правильно, мы можем сделать следующую гипотезу: если два входных изображения принадлежат одному и тому же персонажу, то их векторы признаков также должны быть похожими, а если два входных изображения принадлежат разным символам, то их векторы признаков также будут разными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, поэлементная абсолютная разница между двумя векторами признаков должна сильно отличаться в обоих вышеупомянутых случаях. И, следовательно, оценка сходства, генерируемая выходным сигмовидным слоем, также должна быть разной в этих двух случаях.\n",
    "\n",
    "Это центральная идея сиамских сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/siamese_neural_network_idea_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss\n",
    "\n",
    "Для распознавания лиц, мы будем использовать `Triplet loss`, то есть подавать три изображения вместо двух.\n",
    "\n",
    "Триплет состоит из анкора `anchor`, положительного и отрицательного образцов и в основном применяется для распознавания лиц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/triplet_loss_scheme.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `Triplet loss` расстояние между анкором и положительной выборки минимизировано, а расстояние между анкором и отрицательной выборки максимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/img_license/triplet_loss_idea_scheme.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но можно также использовать и `Contrastive Loss` о ней подробнее в статье [Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация сиамской сети\n",
    "---\n",
    "на примере датасета [Georgia Tech face database](http://www.anefian.com/research/face_reco.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "Загрузим небольшой фрагмент датасета с лицами. Внутри архива фото лиц сгруппированны по папкам\n",
    "\n",
    "\n",
    "\n",
    "*   s1/\n",
    "**  photo1.pgm\n",
    "**  photo2.pgm\n",
    "**  ...\n",
    "*   s2/\n",
    "*   s2/\n",
    "*    ...\n",
    "*   sn/\n",
    "\n",
    "В каждой папке фото лица одного и того же человека."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!wget http://edunet.kea.su/repo/src/L11_Transfer_learning/small_face_dataset.zip\n",
    "!unzip small_face_dataset.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что бы результаты воспроизводились зафиксируем SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for TripletLoss\n",
    "\n",
    "Для TripletLoss потребуются три изображения: anchor, positive, negative и метод __get_item__ должен возвращать их нам. Первые два должны принадлежать одному человеку, а третье другому. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,dir=None,transform=None, splitter = '/'):\n",
    "        self.dir = dir\n",
    "        self.splitter = splitter\n",
    "        self.transform = transform        \n",
    "        self.files = glob(f\"{self.dir}/**/*.pgm\",recursive=True)\n",
    "        self.data =self.build_index()        \n",
    "    \n",
    "    def build_index(self):\n",
    "      index = {}      \n",
    "      for f in self.files:\n",
    "        id = self.path2id(f) \n",
    "        if not id in index:\n",
    "          index[id] = []\n",
    "        index[id].append(f)\n",
    "      return index\n",
    "    \n",
    "    def path2id(self,path):\n",
    "        return path.replace(self.dir,\"\").split(self.splitter)[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        anchor_path = self.files[index]\n",
    "        positive_path = self.find_positive(anchor_path)\n",
    "        negative_path = self.find_negative(anchor_path)\n",
    "        \n",
    "        # Loading the images\n",
    "        anchor = Image.open(anchor_path)\n",
    "        positive = Image.open(positive_path)\n",
    "        negative = Image.open(negative_path)\n",
    "                \n",
    "        if self.transform is not None:  # Apply image transformations           \n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive , negative\n",
    "\n",
    "    def find_positive(self,path):\n",
    "        id = self.path2id(path) \n",
    "        all_exept_my = self.data[id].copy()\n",
    "        all_exept_my.remove(path)\n",
    "        return random.choice(all_exept_my)\n",
    "\n",
    "    def find_negative(self,path):\n",
    "        all_exept_my_ids = list(self.data.keys())\n",
    "        id = self.path2id(path) \n",
    "        all_exept_my_ids.remove(id)\n",
    "        selected_id = random.choice(all_exept_my_ids)\n",
    "        return random.choice(self.data[selected_id])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем несколько изображений что бы убедится что класс датасета функционирует должным образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Create dataset instance\n",
    "siamese_dataset = SiameseNetworkDataset(\"faces/training/\",\n",
    "                                          transform=transforms.Compose([\n",
    "                                            transforms.Resize((105,105)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                          ])\n",
    "                                        )\n",
    "\n",
    "# Create dataloader & extract batch of data from it\n",
    "vis_dataloader = DataLoader(siamese_dataset, batch_size =8, shuffle=True)\n",
    "dataiter = iter(vis_dataloader)\n",
    "example_batch = next(dataiter) # anc, pos, neg\n",
    "\n",
    "# Show batch contents \n",
    "concatenated = torch.cat((example_batch[0],example_batch[1],example_batch[2]),0)\n",
    "grid = torchvision.utils.make_grid(concatenated)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1,2,0).numpy())\n",
    "plt.gcf().set_size_inches(20,60)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждом столбце тройка изображений.Первое и второе принадлежат одному человеку, третье - другому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели\n",
    "\n",
    "Нас устроит любая модель для работы с изображениями. Например Resnet18. \n",
    "\n",
    "Все что от нас требуется это:\n",
    "- заменить последний слой\n",
    "- отправлять на анализ три изображения вместо одного. Соответственно на выходе тоже будут три вектора признаков (embedding)\n",
    "\n",
    "\n",
    "Пожалуй единственный вопрос это размерность последнего слоя . В промышленных системмах распознавания лиц, которые тренируются на датасетах из миллионов изображений, используются embedding размерностью от 128 до 512.\n",
    "\n",
    "Значит для демонстрационной задачи нам с запасом хватить 64 значений. Значит  выход последнего линейного слоя должен быть равен 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(pretrained = False)\n",
    "        # Because we use grayscale images reduce input channel count to one\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        # Replace ImageNet 1000 class classifier with 64- out linear layer \n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 64)\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        output1 = self.model(anchor)\n",
    "        output2 = self.model(positive)\n",
    "        output3 = self.model(negative)\n",
    "        \n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "\n",
    "Загрузчики данных не отличаются от загрузчиков для обычной сети. \n",
    "Единственное отличие это две аугментации которые добавили к данным:\n",
    "\n",
    "*   Случайный поворот по вертикали (RandomHorizontalFlip)\n",
    "*   Размытие (GaussianBlur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as img_transf\n",
    "from torchvision import datasets as ds\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Apply augmentations on train data\n",
    "img_trans_train = img_transf.Compose(\n",
    "    [\n",
    "        img_transf.Resize((105, 105)),\n",
    "        img_transf.RandomHorizontalFlip(),\n",
    "        img_transf.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        img_transf.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_trans_test = img_transf.Compose(\n",
    "    [img_transf.Resize((105, 105)), img_transf.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = SiameseNetworkDataset(\"faces/training/\",transform = img_trans_train)\n",
    "val_dataset = SiameseNetworkDataset(\"faces/testing/\",transform = img_trans_test)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, num_workers=2, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, num_workers=2, batch_size=1, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличие от  сетей для классификации в том что у модели 3 выхода и все их надо передать в loss. При этом нет меток в явном виде.\n",
    "Какой embedding отностся к позитивному образцу а какой к негативному определяется только порядком их следования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, criterion, optimizer, train_loader):\n",
    "    loss_history = []\n",
    "    l = []\n",
    "    model.train()\n",
    "    for epoch in range(0, num_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            anc, pos, neg = batch\n",
    "            output_anc, output_pos, output_neg = model(anc.to(device), pos.to(device), neg.to(device))\n",
    "            loss = criterion(output_anc, output_pos, output_neg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            l.append(loss.item())\n",
    "        last_epoch_loss =  torch.tensor(l[-len(train_loader):-1]).mean()\n",
    "        print(\"Epoch {} with {:.4f} loss\".format(epoch,last_epoch_loss))\n",
    "        \n",
    "    return l, last_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании GPU, обучение на 5-ти эпохах займет около 15 сек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SiameseNet().to(device)\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001 ) \n",
    "\n",
    "l, _ = train(5, model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем график loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([i for i in range(len(l))], l)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('num of epochs')\n",
    "x = [0,6,12,18,24,30]\n",
    "labels = [0,1,2,3,4,5]\n",
    "plt.xticks(x, labels)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно что он график лосс уже вышел на плато, значит можно проверять результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выведем тройки изображений из проверочного датасета и посмотрим на расстояния для позитивных и негативных пар. Если модель обучилась, расстояния для позитивных пар будут меньше. Другими словами, расстояние между похожими лицами будет маленьким, между разными большим.\n",
    "\n",
    "P.S. По умолчанию TripletLoss минимизирует Евклидово расстояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Helper method for visualization\n",
    "def show(img, text=None):\n",
    "    img_np = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(75, 120, text, fontweight=\"bold\")\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def plot_imgs(model,test_loader):\n",
    "    distances_pos = []\n",
    "    distances_neg = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      for i, batch in enumerate(test_loader, 0):\n",
    "        anc, pos, neg = batch\n",
    "        output_anc, output_pos, output_neg = model(anc.to(device), pos.to(device), neg.to(device))\n",
    "        # compute euc. distance\n",
    "        distance_pos = F.pairwise_distance(output_anc, output_pos).item() \n",
    "        distance_neg = F.pairwise_distance(output_anc, output_neg).item() \n",
    "\n",
    "        distances_pos.append(distance_pos)\n",
    "        distances_neg.append(distance_neg)\n",
    "\n",
    "        if not i % 5 :\n",
    "                concatenated = torch.cat((anc, pos, neg))\n",
    "                show(    \n",
    "                    torchvision.utils.make_grid(concatenated),\n",
    "                    f\"Positive / negative euclidean distances: {distance_pos:.3f} / {distance_neg:.3f}\",\n",
    "                )\n",
    "\n",
    "    return distances_pos, distances_neg\n",
    "\n",
    "\n",
    "distances_pos, distances_neg = plot_imgs(model,val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Но такая оценка субъективна, давайте посмотрим на распределение расстояний по категориям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas==1.1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas \n",
    "\n",
    "distances = {\"The same person\": distances_pos, \"Another person\": distances_neg}\n",
    "\n",
    "ax = sns.displot(distances, kde=True, stat=\"density\")\n",
    "ax.set(xlabel=\"Pairwise distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно что для фото одного и того же человека, в большинстве случаев расстояние лежит в интервале от 0 до 5.5. \n",
    "\n",
    "А вот для фото разных людей от 5.5 до 19. \n",
    "\n",
    "Если бы мы проектировали систему распознавания лиц, нужно было бы выбрать порог что бы сравнивать с ним расстояние и принимать решение о том верифицировать фото как подлинное или нет.\n",
    "\n",
    "Соответственно, для нашего игрушечного датасета такой порог следует выбирать ~= 6 при условии что ошибки первого и второго рода для нас равнозначны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shots learning in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и другой интересный вариант, как можно использовать few-shots learning. Огромные модели (такие как GPT-3 и варианты) в целом способны генерировать любой контент. Вопрос лишь в том, как бы им объяснить, что мы от них хотим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "GPT-3 не доступна для свободного пользования (OpenAI оказался не очень-то и open). Но, умельцы из сообщества ElutherAI сделали open-source версию **GPT-J**, которая работает сравнимо с оригинальной моделью. Более того, они ее захостили у себя на сайте, что бы каждый мог пользоваться без долгой и муторной подгрузки модели (установка зависимостей и загрузка весов для GPT-J занимает ~20 минут в колабе). Ей-то мы и воспользуемся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зайдем на [сайт модели](https://6b.eleuther.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед нами текстовый редактор, в который можно написать что угодно. Для примера давайте попробуем перефразирововать предложение: \n",
    "\n",
    "    Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.\n",
    "\n",
    "Например сформулировав задачу так: *paraphrase* `...sentence...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то результаты не впечатляют. Тут-то на помощь и приходит идея Few-shots. \n",
    "\n",
    "Что если мы в качестве затравки (*prompt*) скормим моделе 2-3 примера в которых покажим чего от нее хотим?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем вот такой prompt:\n",
    "\n",
    "    Original: Her life spanned years of incredible change for women as they gained more rights than ever before.\n",
    "    Paraphrase: She lived through the exciting era of women's liberation.\n",
    "\n",
    "    Original: Giraffes like Acacia leaves and hay, and they can consume 75 pounds of food a day.\n",
    "    Paraphrase: A giraffe can eat up to 75 pounds of Acacia leaves and hay daily.\n",
    "\n",
    "    Original: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.\n",
    "    Paraphrase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значительно лучше! Ответ который получил автор блокнота с первого раза: \n",
    "\n",
    "**Paraphrase:** *Recent work has shown that you can get a lot of money for a lot of text if you pre-train it for a specific task, then fine-tune it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем с числовыми рядами.\n",
    "\n",
    "    Sequence: 2, 4,  6, 8\n",
    "    Continuation: 16, 32, 64, 128\n",
    "\n",
    "    Sequence: 3, 9, 27, 81\n",
    "    Continuation: 243, 729, 2187, 6561\n",
    "\n",
    "    Sequence: 4, 16, 64, 256\n",
    "    Continuation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И с геологической викториной:\n",
    "\n",
    "    Q: The molten rock that sits below the earth's surface is called what?\n",
    "    A: Magma\n",
    "\n",
    "    Q: Diamonds rank at number 10 on the Mohs scale, meaning they are the hardest mineral listed. Where would cubic zirconia (CZ) rank on the scale?\n",
    "    A: 8-8.5\n",
    "\n",
    "    Q: The emerald is a type of beryl. The distinctive green color of the emerald derives from trace amounts of what metallic elements?\n",
    "    A: \n",
    "\n",
    "Ответы лучше перепроверить, а вот вопросы для викторины готовы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не стесняйтесь придумывать свои собственные примеры и задачи! Have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто, когда мы пишем и обучаем сети (будь то с нуля или с помощью transfer learning) мы вынуждены угадывать гиперпараметры (lr, betas и тд). В случае с learning rate нам есть от чего оттолкнуться (маленький lr для transfer learning), [константа Karpathy (3e-4)](https://twitter.com/karpathy/status/801621764144971776?lang=en) для Adam, но все же, такой подход не кажется оптимальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации гиперпараметров существуют готовые решения, которые используют различные методы black-box оптимизации. Разберем одну из наиболее популярных библиотек - [**Optuna**](https://optuna.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте оптимизируем наш learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "\n",
    "# define function which will optimized\n",
    "def objective(trial):\n",
    "    # boundaries for the optimizer's \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-2)\n",
    "    \n",
    "    ##### If you need more parameters for optimization, it is done like this:\n",
    "    #new_parameter =  trial.suggest_loguniform(\"new_parameter\", lower_bound, upper_bound)\n",
    "\n",
    "    # create new model(and all parameters) every iteration\n",
    "    model = SiameseNet().to(device)\n",
    "    criterion = nn.TripletMarginLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=lr\n",
    "    )  # learning step regulates by optuna\n",
    "    \n",
    "\n",
    "    # To save time, we will take only 3 epochs\n",
    "    _, last_epoch_loss = train(3, model, criterion, optimizer, train_loader)\n",
    "    return last_epoch_loss\n",
    "\n",
    "\n",
    "# Create \"exploration\"\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"Optimal lr\")\n",
    "\n",
    "#\n",
    "study.optimize(\n",
    "    objective, n_trials=10\n",
    ")  # The more iterations, the higher the chances of catching the most optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best params\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно же, можно оптимизировать сразу несколько параметров за раз, а еще в качестве параметров можеть выступать сама архитектура сети (например количесвто слоев и каналов). Это можно легко сделать по анологии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на историю оптимизации нашего lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, проверим а станет ли реально лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNet().to(device)\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=study.best_params[\"lr\"]\n",
    ")  # take lr, which choosen Optuna\n",
    "#scheduler = ReduceLROnPlateau(optimizer, \"min\", verbose=True, factor=0.5, patience=3)\n",
    "\n",
    "l_optim, _ = train(5,  model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(l))], l, label=\"no optimization\")\n",
    "plt.plot([i for i in range(len(l))], l_optim[:len(l)], label=\"optimal params\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('num of epochs')\n",
    "x = [0,6,12,18,24,30]\n",
    "labels = [0,1,2,3,4,5]\n",
    "plt.xticks(x, labels)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_pos, distances_neg = plot_imgs(model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "distances_optim = {\"The same person\": distances_pos, \"Another person\": distances_neg}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "sns.histplot(distances, kde=True, stat=\"density\", alpha=0.5, ax=axes[0])\n",
    "sns.histplot(distances_optim, kde=True, stat=\"density\", alpha=0.5, ax=axes[1])\n",
    "\n",
    "axes[0].set(title='No optimization')\n",
    "axes[1].set(title='Otimization with Optuna')\n",
    "axes[0].set(xlabel=\"Pairwise distance\")\n",
    "axes[1].set(xlabel=\"Pairwise distance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\"> Заключение\n",
    "\n",
    "\n",
    "Мы затронули проблемы, которые возникают при обучени на реальных данных.\n",
    "\n",
    "Одна из основных проблем - малые датасеты. Для того, чтобы обучить нейронку на небольшом датасете можно использовать:\n",
    "- `аугментацию`\n",
    "- `Tranfer learning`\n",
    "Однако необходимо помнить, что ни один из этих методов не защитит от ситуации, когда реальные данные будут сильно отличаться от тренировочных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае, когда у нас не только мало данных, но и еще и очень большое (возможно, неизвестное) число классов, можно воспользоваться `One-shot Learning`. В этом случае нейронка обучается не классифицировать изображения, а, наоборот, находить различия между классом и новыми данными. Для этого используются нейронные сети, относящиеся к классу сиамских нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же мы разобрали автоматическую оптимизацию гиперпараметров с помощью Optuna и показали, что это эффективный способ сделать нейросеть лучше малой ценой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\">Литература\n",
    "\n",
    "\n",
    "<font size = \"5\"> Обучение на реальных данных\n",
    "\n",
    "[How to avoid machine learning pitfalls: a guide for academic researchers (Lones, 2021)](https://arxiv.org/abs/2108.02497)\n",
    "\n",
    "[Understanding data augmentation for classification: when to warp? (Wong et al., 2016)](https://arxiv.org/abs/1609.08764) \n",
    "\n",
    "[Learning from class-imbalanced data: Review of methods and applications (Haixiang et al., 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175?via%3Dihub)\n",
    "\n",
    "<font size = \"5\"> Как решить проблему маленького количества данных?\n",
    "\n",
    "[Блог-пост о том как решать проблему малого количества данных.](https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d)\n",
    "\n",
    "<font size = \"5\"> Несбалансированные данные\n",
    "\n",
    "[Imbalanced Data: How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/).\n",
    "\n",
    "[SMOTE explained for noobs - Synthetic Minority Over-sampling TEchnique line by line](https://rikunert.com/SMOTE_explained)\n",
    "\n",
    "[Блог пост про 8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "\n",
    "[Метрики разработаные для работы с несбалансированными классами.](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/#:~:text=Classification%20Accuracy%20is%20Not%20Enough%3A%20More%20Performance%20Measures%20You%20Can%20Use,-By%20Jason%20Brownlee&text=When%20you%20build%20a%20model,This%20is%20the%20classification%20accuracy)\n",
    "\n",
    "[Творческий подход](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)\n",
    "\n",
    "<font size = \"5\"> Transfer Learning\n",
    "\n",
    "[Image Classification using Transfer Learning in Pytorch](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/)\n",
    "\n",
    "[How To Do Transfer Learning For Computer Vision | PyTorch Tutorial](https://www.youtube.com/watch?v=6nQlxJvcTr0)\n",
    "\n",
    "[Transfer learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "[Python Pytorch Tutorials # 2 Transfer Learning : Inference with ImageNet Models](https://www.youtube.com/watch?v=Upw4RaERZic)\n",
    "\n",
    "[PyTorch - The Basics of Transfer Learning with TorchVision and AlexNet](https://www.youtube.com/watch?v=8etkVC93yU4)\n",
    "\n",
    "\n",
    "<font size = \"5\">Augmentation\n",
    "\n",
    "[A survey on Image Data Augmentation for Deep Learning (Shorten and Khoshgoftaar, 2019)](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)\n",
    "\n",
    "[Data augmentation for improving deep learning in image classification problem](https://www.researchgate.net/publication/325920702_Data_augmentation_for_improving_deep_learning_in_image_classification_problem)\n",
    "\n",
    "\n",
    "<font size = \"5\"> Few-shot learning\n",
    "\n",
    "[One-Shot Learning with Siamese Networks using Keras](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d)\n",
    "\n",
    "[One-Shot image classification by meta learning](https://medium.com/nerd-for-tech/one-shot-learning-fe1087533585)\n",
    "\n",
    "[One-Shot Learning (Part 1/2): Definitions and fundamental techniques](https://heartbeat.fritz.ai/one-shot-learning-part-1-2-definitions-and-fundamental-techniques-1df944e5836a)\n",
    "\n",
    "[One-Shot Learning (Part 2/2): Facial Recognition Using a Siamese Network](https://heartbeat.fritz.ai/one-shot-learning-part-2-2-facial-recognition-using-a-siamese-network-5aee53196255)\n",
    "\n",
    "[FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff et al., 2015)](https://arxiv.org/abs/1503.03832)\n",
    "\n",
    "[One-Shot Learning explained using FaceNet](https://medium.com/intro-to-artificial-intelligence/one-shot-learning-explained-using-facenet-dff5ad52bd38)\n",
    "\n",
    "[Ищем знакомые лица](https://habr.com/ru/post/317798/)\n",
    "\n",
    "[Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "[Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)\n",
    "\n",
    "[Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "<font size = \"5\"> Hyperparameter optimization\n",
    "\n",
    "[Tuning Hyperparameters with Optuna](https://towardsdatascience.com/tuning-hyperparameters-with-optuna-af342facc549)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
