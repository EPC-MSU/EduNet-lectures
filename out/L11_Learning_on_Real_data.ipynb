{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Обучение на реальных данных</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проблемы при работе с реальной задачей машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С какими проблемами чаще всего сталкиваются люди, работающие с машинным обучением, в реальной жизни?\n",
    "- нехватка данных\n",
    "- недостаток размеченных данных\n",
    "- не качественная разметка\n",
    "- низкое качество изображений\n",
    "- не сбалансированность датасета\n",
    "- переобучение\n",
    "- **какие еще?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to avoid machine learning pitfalls: a guide for academic researchers (Lones, 2021)](https://arxiv.org/abs/2108.02497)\n",
    "\n",
    "Если у вас недостаточно данных, то обучение хорошо обобщающей модели может оказаться невозможным. Выяснить, генерализуется модель или нет, может быть непросто, пока вы не начнете строить модели: все зависит от соотношения сигнал/шум в наборе данных.\n",
    "\n",
    "Если сигнал сильный, то можно обойтись меньшим количеством данных; если слабый, то необходимо больше данных. Если вы не можете получить больше данных - а это распространенная проблема во многих исследовательских тогда вы можете лучше использовать имеющиеся данные, используя перекрестную проверку (*cross validation*). Вы также можете использовать методы увеличения данных (например, см. [Understanding data augmentation for classification: when to warp? (Wong et al., 2016)](https://arxiv.org/abs/1609.08764) и [A survey on Image Data Augmentation for Deep Learning (Shorten and Khoshgoftaar, 2019)](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)). Такие методы могут быть весьма эффективны для повышения эффективности небольших наборов данных. Увеличение данных также полезно в ситуациях, когда у вас ограниченное количество данных в определенных частях вашего набора данных, например, в задачах классификации, когда у вас меньше образцов в одних классах, чем в других - ситуация, известная как дисбаланс классов (см. обзор методов решения этой проблемы в [Learning from class-imbalanced data: Review of methods and applications (Haixiang et al., 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175?via%3Dihub)).\n",
    "\n",
    "Однако если у вас ограниченное количество данных, то, скорее всего, вам также придется ограничить сложность используемых вами ML-моделей, поскольку модели с большим количеством параметров, такие как глубокие нейронные сети, могут легко переоценить маленькие наборы данных. В любом случае, важно выявить эту проблему на ранней стадии и выработать подходящую (и обоснованную) стратегию для ее решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как решить проблему малого количества данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ResNet (популярная архитектура для классификации изображений), заняла 1-е место в классификационном конкурсе ILSVRC 2015 с улучшением примерно на 50% по сравнению с предыдущим уровнем развития техники. ResNet не только имела очень сложную и глубокую архитектуру, но и была обучена на 1,2 млн изображений.\n",
    "\n",
    "Как в отрасли, так и в академических кругах хорошо известно, что при достаточно большом количестве данных, очень разные алгоритмы DL работают практически одинаково. Следует отметить, что большие данные должны содержать значимую информацию, а не просто шум, чтобы модель могла извлекать уроки из них. -->\n",
    "[Блог-пост о том как решать проблему малого количества данных.](https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_1-1.png\" width=\"800\"></center>\n",
    "<center><em>На рисунке показаны основные проблемы, с которыми приходится сталкиваться при работе с небольшими наборами данных, а также возможные подходы и методы их решения.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Работа с несбалансированным датасетом\n",
    "\n",
    "Сначала, давайте рассмотрим методы работы с датасетами в которых классы представлены не равномерно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем датасет"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_data, Y_data = make_classification(n_samples=10000, n_features=10, n_classes=2)\n",
    "sns.displot(Y_data, height=4)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь его испортим (изменим соотношение класса `0` и класса `1`)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "Y_spoiled = np.random.choice(Y_data[Y_data == 1], size=np.int(0.1 * len(Y_data[Y_data == 1])))\n",
    "Y_spoiled = np.hstack([Y_data[Y_data == 0], Y_spoiled])\n",
    "X_spoiled = X_data[Y_spoiled]\n",
    "\n",
    "sns.displot(Y_spoiled, height=4)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что можно сделать, что бы как-то улучшить ситуацию используя методы ML\n",
    "\n",
    "- Можно **изменить функцию потерь**: для задач классификации мы часто используем кросс-энтропийный лосс и редко используем среднюю абсолютную ошибку или среднеквадратичную ошибку для обучения и оптимизации нашей модели.\n",
    "\n",
    "- В случае несбалансированных данных, модель учится распознавать доминирующий класс чаще чем все остальные, соответственно, она выучивает статистическое распределение классов и считает что чем чаще она предсказывает этот класс - тем меньше она ошибается. Что бы как-то решить эту проблему - мы можем **добавить веса к потерям**, соответствующим различным классам, чтобы выровнять это смещение данных. Например, если у нас есть два класса в соотношении 4:1, мы можем применить веса в соотношении 1:4 к вычислению функции потерь, чтобы данные были сбалансированы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод помогает нам легко смягчить проблему несбалансированных данных и улучшить обобщение модели для разных классов. Например:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y_spoiled), y=Y_spoiled)\n",
    "print(f'class_weights: {class_weights}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем указать явные веса классов в соответствии с нашими требованиями, дополнительные сведения смотри в документации Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "  # Create decision tree classifer object\n",
    " clf = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "\n",
    "  # Train model\n",
    " model = clf.fit(X_data, Y_spoiled)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один способ решение проблемы несбалансированных данных - собственно их балансирование. Это может быть сделано либо за счет увеличения частоты класса меньшинства (_oversampling_), либо за счет уменьшения частоты класса большинства (_undersampling_) с помощью методов случайной или кластерной выборки.\n",
    "\n",
    "Обычно _oversampling_ предпочтителен, когда общий размер данных небольшой, а _undersampling_ полезен, когда у нас есть большой объем данных. Точно так же случайная или кластерная выборка определяется тем, насколько хорошо распределены данные.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_0.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот тут можно прочитать подробнее: [Imbalanced Data: How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/).\n",
    "\n",
    "`Resampling` может быть легко выполнен с помощью пакета [imbalanced-learn](https://pypi.org/project/imbalanced-learn/), как показано ниже:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, Y_rus = rus.fit_resample(X_spoiled, Y_spoiled)\n",
    "sns.displot(Y_rus, height=3)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация синтетических данных\n",
    "\n",
    "Хотя `oversampling` или `undersampling` помогает сбалансировать данные, дублирование данных увеличивает вероятность переобучения.\n",
    "\n",
    "Другой подход к решению этой проблемы - создание синтетических данных с помощью данных о классе меньшинств.\n",
    "\n",
    "**Synthetic Minority Over-sampling Technique (SMOTE)** или **Modified- SMOTE** - два таких метода, которые генерируют синтетические данные. Проще говоря, SMOTE берет точки данных класса меньшинства и создает новые точки данных, которые лежат между любыми двумя ближайшими точками данных, соединенными прямой линией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого алгоритм вычисляет расстояние между двумя точками данных в пространстве признаков, умножает расстояние на случайное число от 0 до 1 и помещает новую точку данных на этом новом расстоянии от одной из точек данных, используемых для определения расстояния. **Обратите внимание,** что количество ближайших соседей, учитываемых для генерации данных, также является гиперпараметром и может быть изменено в зависимости от требований.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_vera/l11_0-1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M-SMOTE** - это модифицированная версия SMOTE, которая также принимает во внимание базовое распределение класса меньшинства.\n",
    "\n",
    "Алгоритм классифицирует образцы классов меньшинств на 3 отдельные группы - образцы безопасности / безопасности, образцы границ и образцы скрытого шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это делается путем вычисления расстояний между выборками класса меньшинства и выборками обучающих данных.\n",
    "\n",
    "В отличие от SMOTE, алгоритм случайным образом выбирает точку данных из k ближайших соседей для выборки безопасности, выбирает ближайшего соседа из выборок границ и ничего не делает для устранения скрытого шума.\n",
    "\n",
    "Вот тут можно прочитать подробнее: [SMOTE explained for noobs - Synthetic Minority Over-sampling TEchnique line by line](https://rikunert.com/SMOTE_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датасет и посмотрим на то какие в нем данные"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "# define dataset\n",
    "X_data, Y_data = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=42)\n",
    "# summarize class distribution\n",
    "counter = Counter(Y_data)\n",
    "print(f'0 class: {counter[0]}')\n",
    "print(f'1 class: {counter[1]}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from numpy import where\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(Y_data == label)[0]\n",
    "\tplt.scatter(X_data[row_ix, 0], X_data[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте сделаем oversampling класас меньшинства с помощью SMOTE и построим график преобразованного набора данных.\n",
    "\n",
    "Мы будем использовать реализацию `SMOTE` из библиотеки `imbalanced-learn`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X_data, Y_data = oversample.fit_resample(X_data, Y_data)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(Y_data)\n",
    "print(f'0 class: {counter[0]}')\n",
    "print(f'1 class: {counter[1]}')\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(Y_data == label)[0]\n",
    "\tplt.scatter(X_data[row_ix, 0], X_data[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнаружение аномалий / изменений\n",
    "\n",
    "В случае сильно несбалансированных наборов данных, таких как мошенничество или машинный сбой, стоит задуматься, могут ли такие примеры рассматриваться как аномалия (выброс) или нет. Если такое событие и впрямь может считаться аномальным, мы можем использовать такие модели, как `OneClassSVM`, методы кластеризации или методы обнаружения гауссовских аномалий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти методы требуют изменения способа мышления: мы будем рассматривать аномалии, как отдельный класс выбросов. Это может помочь нам найти новые способы разделения и классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обнаружение изменений похоже на обнаружение аномалий, за исключением того, что мы ищем изменение или отличие, а не аномалию. Это могут быть изменения в поведении пользователя, наблюдаемые по шаблонам использования или банковским транзакциям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем пример обнаружения аномалий с помощью `OneClassSVM` из библиотеки Sk-Learn (там же, можно найти еще множеество различных алгоритмов)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create dataset\n",
    "X_data, _ = make_blobs(n_samples=1000, centers=1, cluster_std=1.1, center_box=(0, 0))\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1], alpha=0.25)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроим наш детектор аномалий"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "svm = OneClassSVM(kernel=\"rbf\", gamma=0.01, nu=0.03)\n",
    "print(f'SVM parameters:\\n{svm}')\n",
    "svm.fit(X_data)\n",
    "pred = svm.predict(X_data)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем аномальные точки предсказанные детектором"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "anom_index = where(pred == -1)\n",
    "values = X_data[anom_index]\n",
    "\n",
    "plt.scatter(X_data[:, 0], X_data[:, 1], alpha=0.25)\n",
    "plt.scatter(values[:, 0], values[:, 1], color=\"r\", marker=\"x\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы объединения (Ensembling Techniques)\n",
    "\n",
    "Помимо того, что бы делать что-то с данными, мы можем попробовать сделать что-то с самой моделью. \n",
    "Идея объединения нескольких слабых учеников / разных моделей показала отличные результаты при работе с несбалансированными наборами данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bagging` и `Boosting` методы также показали отличные результаты при решении различных задач, и их следует изучить вместе с методами, описанными выше, для получения лучших результатов.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "\n",
    "Мы рассмотрели несколько наиболее часто используемых методов работы с несбалансированными данными для традиционных алгоритмов машинного обучения.\n",
    "\n",
    "Один или многие из вышеперечисленных методов могут быть хорошей отправной точкой в ​​зависимости от конкретной задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог пост про 8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "\n",
    "1. **Определите, можете ли вы собрать больше данных?**\n",
    "2. **Попробуйте изменить метрику оценки модели.**\n",
    "    Accuracy не является показателем, который следует использовать при работе с несбалансированным набором данных. [Есть метрики, которые были разработаны для работы с несбалансированными классами.](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/#:~:text=Classification%20Accuracy%20is%20Not%20Enough%3A%20More%20Performance%20Measures%20You%20Can%20Use,-By%20Jason%20Brownlee&text=When%20you%20build%20a%20model,This%20is%20the%20classification%20accuracy)\n",
    "\n",
    "3. **Применить `Resampling`** (см. выше)\n",
    "4. **Попробовать создать синтетические образцы данных**\n",
    "5. **Не следует использовать свой любимый алгоритм для каждой задачи.**\n",
    "    Необходимо проверять различные типы алгоритмов для решения конкретной проблемы. Например, `Random Forest` часто хорошо работает с несбалансированными наборами данных.\n",
    "6. **Попробуйте модели с санкциями**\n",
    "    Штрафная классификация накладывает дополнительные штрафы на модель за ошибки классификации в классе меньшинства во время обучения. Эти штрафы могут склонить модель к уделению большего внимания классу меньшинства.\n",
    "7. **Попробуйте другую точку зрения**\n",
    "    Есть области исследований, посвященные несбалансированным наборам данных. Двe, которые вы могли бы рассмотреть, - это [обнаружение аномалий](https://en.wikipedia.org/wiki/Anomaly_detection) и [обнаружение изменений](https://en.wikipedia.org/wiki/Change_detection).\n",
    "8. **[Попробуйте проявить творческий подход](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)**.\n",
    "    По-настоящему залезьте внутрь своей проблемы и подумайте, как разбить ее на более мелкие проблемы, которые легче решить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не используйте точность для несбалансированных наборов данных\n",
    "\n",
    "Будьте осторожны с тем, какие метрики вы используете для оценки ваших ML-моделей. Например, в случае моделей классификации, наиболее часто используемой метрикой является точность (*accuracy*), которая представляет собой долю сэмплов в наборе данных, которые были правильно классифицированы моделью.\n",
    "\n",
    "Этот метод хорошо работает, если классы сбалансированы, т.е. каждый класс представлен одинаковым количество образцов в наборе данных. Но многие наборы данных не являются сбалансированными, и в этом случае точность может быть очень обманчивой метрикой. Рассмотрим, например, датасет, в котором 90% сэмплов представляют один класс, а 10% сэмплов представляют\n",
    "другой класс. Бинарный классификатор, который всегда выдает первый класс, независимо от его входных данных, будет иметь точность 90%, несмотря на то, что он совершенно бесполезен. В такой ситуации предпочтительнее использовать такие метрики, как коэффициент каппы Коэна или коэффициент корреляции Мэтьюса (MCC), которые относительно нечувствительны к дисбалансу размеров классов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аугментация\n",
    "Другой способ побороть маленькое количество данных для обучения - аугментация. Сам термин пришел из музыки:\n",
    "\n",
    "**Аугмента́ция** (позднелат. augmentatio — увеличение, расширение) — [техника ритмической композиции в старинной музыке](https://ru.wikipedia.org/wiki/Аугментация).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели глубокого обучения обычно требуют большого количества данных для обучения. В целом, чем больше данных, тем лучше для обучения модели. В то же время получение огромных объемов данных сопряжено со своими проблемами (например с нехваткой размеченных данных или с трудозатратами сопряженными с разметкой).\n",
    "\n",
    "Вместо того, чтобы тратить дни на сбор данных вручную, мы можем использовать методы аугментации для автоматической генерации новых примеров из уже имеющихся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо увеличения размеченных датасетов, многие методы *self-supervised learning* построены на использовании разных аугмеентаций одного и того же сэмпла.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_vera/l11_2.png\" width=\"700\"></center>\n",
    "<center><em>Примеры аугментаций картинки. </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для аугментации будем использовать мультимодальную (работающую с различными модальностями, как например текст, картинки, звук и тд) библиотеку с забавным названием `AugLy` (*созвучно с ugly - стремный*)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ATTENTION, AFTER INSTALLATION, AN ERROR MESSAGE WILL APPEAR \n",
    "# COLAB WILL RESTART. IGNORE AND CONTINUE EXECUTION \n",
    "\n",
    "!pip install -U augly\n",
    "!sudo apt-get install python3-magic\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "# fix random_seed\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Compute on cpu or gpu \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важный момент**: при обучении модели мы используем разбиение данных на `train-val-test`. Аугментации стоит применять только на `train`. Почему так? Конечная цель обучения нейросети - это применение на реальных данных, которые сеть не видела. Вот и портить их не надо.\n",
    "\n",
    "В любом случае, `test` должен быть отделен от данных еще до того как они попали в `DataLoader` или нейросеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое дело, что аугментации на тесте можно использовать как метод ансамблинга в случае классификации. Можно взять sample -> создать несколько его копий -> по разному их аугментировать -> предсказать класс на каждой из этих аугментированных копий -> а потом выбрать наиболее вероятный класс голосованием (такой функционал реализован например в [YOLOv5](https://github.com/ultralytics/yolov5/blob/d204a61834d0f6b2e73c1f43facf32fbadb6b284/models/yolo.py#L121), о которой речь пойдет в следующих лекциях)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеку и отобразим пример картинки. Картинку отмасштабируем, что бы она не занимала весь экран"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import augly.image as imaugs\n",
    "import augly.utils as utils\n",
    "from IPython.display import display\n",
    "\n",
    "# Get input image, scale it down to avoid taking up the whole screen\n",
    "input_img_path = os.path.join(\n",
    "    utils.TEST_URI, \"image\", \"inputs\", \"dfdc_1.jpg\"\n",
    ")\n",
    "\n",
    "# We can use the AugLy scale augmentation\n",
    "input_img = imaugs.scale(input_img_path, factor=0.2)\n",
    "display(input_img)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций картинок. С полным списком можно ознакомиться на [сайте библиотеки](https://github.com/facebookresearch/AugLy/blob/main/augly/image/__init__.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие параметры принимает на вход `Random Rotation`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? imaugs.RandomRotation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим перменную `transform` в которую добавим нашу аугментацию и применим ее к исходному изображению. Затем запустим следующую ячейку несколько раз подряд"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = imaugs.RandomRotation(min_degrees=-45.0, # Minimum threshold value of rotation\n",
    "                                  max_degrees=45     # Maximum threshold value of rotation\n",
    "                                  )\n",
    "\n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание как `AugLy` делает `crop` - на выходе мы получаем только ту часть повернутого изображения, которая содержит собственно изображение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? imaugs.RandomBlur"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = imaugs.RandomBlur(min_radius=0.1, # Minimum threshold value of blur  (more large radius, make more blur of image)\n",
    "                              max_radius=10   # Maximum threshold value of blur\n",
    "                              )\n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect Ratio"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? imaugs.RandomAspectRatio"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = imaugs.RandomAspectRatio(min_ratio=0.1, \n",
    "                                     max_ratio=3\n",
    "                                     ) \n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого будем использовать метод `Compose`. Нам нужно будет создать `list` со всеми аугментациями, которые будут применены последовательно."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = imaugs.Compose(\n",
    "    [\n",
    "     imaugs.RandomPixelization(min_ratio=0.1,\n",
    "                               max_ratio=0.9),\n",
    "     imaugs.RandomEmojiOverlay(emoji_size=0.25, \n",
    "                               x_pos=0.43, \n",
    "                               y_pos=0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А что если мы хотим применять аугментации случайным образом?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого воспользуемся методом `ApplyLambda`, который на вход принимает метод аугментации, и вероятность `p` что эта аугментация будет применена."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? imaugs.ApplyLambda"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этот раз мы добавим еще один элемент случайности - будем генерировать цвет фона и цвет текста случайным образом."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "meme_bg_color=(np.random.randint(0,255), # Red channel - random values 0 to 255\n",
    "               np.random.randint(0,255), # Green channel \n",
    "               np.random.randint(0,255)) # Blue channel\n",
    "\n",
    "text_color = (255-meme_bg_color[0], # Find a contrasting color\n",
    "              255-meme_bg_color[1], \n",
    "              255-meme_bg_color[2])\n",
    "\n",
    "transform_to_apply = imaugs.MemeFormat(caption_height=50, \n",
    "                                       meme_bg_color=meme_bg_color,\n",
    "                                       text_color=text_color)\n",
    "\n",
    "transform = imaugs.ApplyLambda(aug_function=transform_to_apply, p=0.9)\n",
    "\n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аугментация внутри `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем папку с картинками"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "image_folder = os.path.join(utils.TEST_URI, \"image\", \"inputs\")\n",
    "input_img_path = os.listdir(image_folder)\n",
    "input_img_path = [image_folder +'/' + x for x in input_img_path]\n",
    "print(f'Num of images: {len(input_img_path)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, imgs_list, transforms=None):\n",
    "        self.imgs_list = imgs_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = imaugs.scale(self.imgs_list[i], factor=0.5)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return torch.tensor(img, dtype=torch.float)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вспомогательную функцию для отображения картинок"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def show_img(img):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `list` с аугментациями, которые мы хотим применить"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "augmentations = [\n",
    "    imaugs.RandomBrightness(),\n",
    "    imaugs.RandomEmojiOverlay(),\n",
    "    imaugs.RandomRotation(),\n",
    "    imaugs.Resize(128,128),\n",
    "]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AugLy` кросс-фреймворковая библиотека, которая работает с `PIL` картинками. Что бы загрузить аугментации в `PyTorch`, нам необходимо эти картинки преобразовать в тензоры. Для этого воспользуемся родным торчевым преобразованием `transforms.ToTensor()`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torchvision.transforms as transforms\n",
    "tensor_transforms = transforms.Compose(augmentations + [transforms.ToTensor()])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обернем все в `DataLoader` и отобразим"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(input_img_path, tensor_transforms), batch_size=3\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций аудио. С полным списком можно ознакомиться на [сайте библиотеки](https://github.com/facebookresearch/AugLy/blob/main/augly/image/__init__.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеку и посмотрим на пример"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import display, Audio\n",
    "\n",
    "# Get input audio\n",
    "input_audio ='https://edunet.kea.su/repo/EduNet-web_dependencies/L11/audio_example.mp3'\n",
    "\n",
    "display(Audio(input_audio))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим наш аудиоофайл в Colab"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from augly.audio.utils import validate_and_load_audio\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "input_audio_arr, sr = validate_and_load_audio(input_audio) # sr - sampling rate"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Noise"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? audaugs.AddBackgroundNoise"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import augly.audio as audaugs\n",
    "\n",
    "snr = np.random.randint(1,10) # Random Signal-to-Noise ratio\n",
    "\n",
    "transform = audaugs.AddBackgroundNoise(snr_level_db=snr) \n",
    "aug_audio, aug_sr = transform(input_audio_arr, sample_rate=sr)\n",
    "\n",
    "display(Audio(aug_audio, rate=aug_sr))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним волновые картины и спектрограммы"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "def produce_plots(input_audio_arr, aug_audio, sr):\n",
    "    f,t, Sxx_in = spectrogram(input_audio_arr, fs=sr) # Compute spectrogram for the original signal (f - frequency, t - time)\n",
    "    f,t, Sxx_aug = spectrogram(aug_audio, fs=sr)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20,5)) \n",
    "\n",
    "    ax[0,0].plot(input_audio_arr)\n",
    "    ax[0,0].set_xlim(0,len(input_audio_arr))\n",
    "    ax[0,0].set_xticks([])\n",
    "    ax[0,0].set_title('Original audio')\n",
    "\n",
    "    ax[0,1].plot(aug_audio)\n",
    "    ax[0,1].set_xlim(0,len(input_audio_arr))\n",
    "    ax[0,1].set_xticks([])\n",
    "    ax[0,1].set_title('Augmented  audio')\n",
    "\n",
    "    ax[1,0].imshow(np.log(Sxx_in), \n",
    "                   extent = [t.min(), t.max(), f.min(), f.max()],\n",
    "                   aspect='auto',\n",
    "                   cmap='inferno')\n",
    "    ax[1,0].set_ylabel('Frequecny, Hz')\n",
    "    ax[1,0].set_xlabel('Time,s')\n",
    "    \n",
    "    ax[1,1].imshow(np.log(Sxx_aug), \n",
    "                   extent = [t.min(), t.max(), f.min(), f.max()],\n",
    "                   aspect='auto',\n",
    "                   cmap='inferno')\n",
    "    ax[1,1].set_ylabel('Frequecny, Hz')\n",
    "    ax[1,1].set_xlabel('Time,s')\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "produce_plots(input_audio_arr, aug_audio, sr)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Stretch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? audaugs.TimeStretch"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "stretch_factor = np.random.uniform(0.25, 0.75) # Random stretching coefficient\n",
    "\n",
    "transform = audaugs.TimeStretch(rate=stretch_factor) \n",
    "aug_audio, aug_sr = transform(input_audio_arr, sample_rate=sr)\n",
    "\n",
    "display(Audio(aug_audio, rate=aug_sr))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае с картинками мы можем совмещать несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? audaugs.ChangeVolume()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "time_between_clicks = np.random.uniform(0.5, 1)\n",
    "\n",
    "transform = audaugs.Compose(\n",
    "    [\n",
    "     audaugs.Clicks(seconds_between_clicks=time_between_clicks), # Add clicks\n",
    "     audaugs.Reverb(), # и add reverb\n",
    "    ]\n",
    ")\n",
    "\n",
    "aug_audio, aug_sr = transform(input_audio_arr, sample_rate=sr)\n",
    "\n",
    "display(Audio(aug_audio, rate=aug_sr))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "produce_plots(input_audio_arr, np.mean(aug_audio, axis=0), sr)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что порядок имеет значение"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "time_between_clicks = np.random.uniform(0.5, 1)\n",
    "\n",
    "transform = audaugs.Compose(\n",
    "    [\n",
    "     audaugs.Reverb(), # add reverb first \n",
    "     audaugs.Clicks(seconds_between_clicks=time_between_clicks), # then add clicks \n",
    "    ]\n",
    ")\n",
    "\n",
    "aug_audio, aug_sr = transform(input_audio_arr, sample_rate=sr)\n",
    "\n",
    "display(Audio(aug_audio, rate=aug_sr))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "produce_plots(input_audio_arr, np.mean(aug_audio, axis=0), sr)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим несколько примеров аугментаций текста. С полным списком можно ознакомиться на [сайте библиотеки](https://github.com/facebookresearch/AugLy/blob/main/augly/text/__init__.py)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import augly.text as textaugs\n",
    "\n",
    "# Define input text\n",
    "input_text = \"Hello, future of AI for Science! How are you today?\"\n",
    "print(f'input text: {input_text}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Иммитация опечаток"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? textaugs.SimulateTypos()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = textaugs.SimulateTypos()\n",
    "\n",
    "aug_text = transform(input_text)\n",
    "print(f'original  text: {input_text}')\n",
    "print(f'augmented text: {aug_text}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замена букв на похожие символы"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "? textaugs.ReplaceSimilarChars"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = textaugs.ReplaceSimilarChars()\n",
    "\n",
    "aug_text = transform(input_text)\n",
    "print(f'original  text: {input_text}')\n",
    "print(f'augmented text: {aug_text}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И многое многое другое. А еще AugLy умеет работать с видео, но это выходит за рамки сегодняшнего обсуждения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме методов, реализованных в AugLy, существуют и специализированные библиотеки. \n",
    "\n",
    "Для аугментации изображений:\n",
    "- [torchvision](https://pytorch.org/vision/stable/transforms.html)\n",
    "- [Albumentations](https://albumentations.ai)\n",
    "- [imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "Для аугментации звука (и волновых функций в целом):\n",
    "- [torchaudio](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)\n",
    "- [torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations)\n",
    "\n",
    "Для аугментации текста:\n",
    "- [TextAugment](https://github.com/dsfsi/textaugment)\n",
    "\n",
    "**При выборе методов аугментации имеет смысл использовать те, которые будут в реальной жизни. Например, нет смысла делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным, или отражать человека вверх-ногами, если мы не предполагаем его таким распознавать.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\color{brown}{\\text{Дополнительная информация}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Примеры аугументации в [TORCHVISION.TRANSFORMS](https://pytorch.org/vision/stable/transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.chdir('/content')\n",
    "# download files\n",
    "!wget --no-check-certificate 'https://edunet.kea.su/repo/EduNet-web_dependencies/L11/for_transforms.Compose.zip' -O data.zip\n",
    "with ZipFile('data.zip', 'r') as folder: # Create a ZipFile Object and load sample.zip in it\n",
    "    folder.extractall() # Extract all the contents of zip file in current directory\n",
    "clear_output()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "os.chdir(\"/content/for_transforms.Compose\")\n",
    "img_list = os.listdir()\n",
    "print(img_list)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, img_list, transforms=None):\n",
    "        self.img_list = img_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = plt.imread(self.img_list[i])\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "        img = np.array(img).astype(np.uint8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return torch.tensor(img, dtype=torch.float)\n",
    "\n",
    "\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(40, 38))\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько методов аугментации изображений, и мы обсудим некоторые из наиболее распространенных и широко используемых.\n",
    "\n",
    "Вы можете попробовать другие методы в соответствии с вашими требованиями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Image Rotation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.RandomRotation(50, expand=True),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Cropping"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.RandomCrop((120, 120)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Perspective"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.GaussianBlur(7, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Erasing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует и более сложные способы аугментации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mixup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_vera/l11_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Аугументация при помощи синтеза данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_vera/l11_4.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В домашнем задании Вам предстоит провести эксперимент: обучить нейронную сеть на датасете без аугментации, и после нее.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме методов, реализованных в Pytorch, существуют и специализированные библиотеки для аугментации изображений. \n",
    "\n",
    "Например:\n",
    "- [lbumentations](https://albumentations.ai)\n",
    "- [imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "- [augly](https://github.com/facebookresearch/AugLy)\n",
    "\n",
    "**При выборе методов аугментации имеет смысл использовать те, которые будут в реальной жизни. Например, нет смысла делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным, или отражать человека вверх-ногами, если мы не предполагаем его таким распознавать.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "----\n",
    "Как быстро обучить нейросеть на своих данных, когда их мало?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для таких типовых задач, как классификация изображений, можно воспользоваться готовой архитектурой (AlexNet, VGG, Inception, ResNet и т.д.) и обучить нейросеть на своих данных. Реализации таких сетей с помощью различных фреймворков уже существуют, так что на данном этапе можно использовать одну из них как черный ящик, не вникая глубоко в принцип её работы. Например в PyTorch есть много предобученных сетей: [`torchvision.models`](https://pytorch.org/vision/stable/models.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, глубокие нейронные сети требовательны к большим объемам данных для сходимости обучения. И зачастую, в нашей частной задаче недостаточно данных для того, чтобы хорошо натренировать все слои нейросети. `Transfer Learning` решает эту проблему. Зачем обучать сеть заново, если можно использовать уже обученную на миллионе изображений и дообучить на свой датасет?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого,  нужно отключить какие-то промежуточные слои. Тогда можно использовать то, что называется `Fine turning` - не нужно обучать всю модель, а достаточно только ее новую часть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети, которые используются для классификации, как правило, содержат N выходных нейронов в последнем слое, где N — это количество классов. Такой выходной вектор трактуется как набор вероятностей принадлежности к классу. \n",
    "\n",
    "В реальных задачах распознавания изображений, количество классов может отличаться от того, которое было в исходном датасете. В таком случае нам придётся полностью выкинуть этот последний слой и поставить новый, с нужным количеством выходных нейронов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_vera/l11_5.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачастую в конце классификационных сетей используется полносвязный слой. Так как мы заменили этот слой, использовать предобученные веса для него уже не получится. Придется тренировать его с нуля, инициализировав его веса случайными значениями. Веса для всех остальных слоев мы загружаем из предобученной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные стратегии дообучения модели. Мы воспользуемся следующей: будем тренировать всю сеть из конца в конец (end-to-end), а предобученные веса не будем фиксировать, чтобы дать им немного скорректироваться и подстроиться под наши данные. Такой процесс называется тонкой настройкой (fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Структурные компоненты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи нам понадобятся следующие компоненты:\n",
    "\n",
    "1. Описание модели нейросети\n",
    "2. Пайплайн обучения\n",
    "3. Инференс пайплайн\n",
    "4. Предобученные веса для этой модели\n",
    "5. Данные для обучения и валидации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_6.png\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как обучать модель используя Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код обучения модели состоит из следующих шагов:\n",
    "\n",
    "1. Построение train/validation пайплайнов данных\n",
    "2. Построение train/validation графов (сетей)\n",
    "3. Надстраивание классификационной функция потерь (cross entropy loss) поверх train графа\n",
    "4. Код, необходимый для вычисления точности предсказания на валидационной выборке во время обучения\n",
    "5. Логика загрузки предобученных весов из снэпшота\n",
    "6. Создание различных структур для обучения\n",
    "7. Непосредственно сам цикл обучения (итерационная оптимизация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний слой графа конструируется с нужным нам количеством нейронов и исключается из списка параметров, загружаемых из предобученного снэпшота.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практический пример Transfer Learning\n",
    "\n",
    "Давайте рассмотрим пример практической реализации такого подхода ([код переработан из этой статьи](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Загрузим датасет и удалим из него 90% файлов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.chdir('/content')\n",
    "path = '/content/2750/'\n",
    "\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L11/EuroSAT.zip # http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
    "!unzip EuroSAT.zip\n",
    "\n",
    "from random import sample\n",
    "for folder in os.listdir(path):\n",
    "  files = os.listdir(path+folder)\n",
    "  for file in sample(files,int(len(files)*0.9)):\n",
    "      os.remove(path+folder+'/'+file)\n",
    "      \n",
    "clear_output()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим аугментации. Для разнообразия будем использовать родные аугментации из библиотеки PyTorch Vision"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Applying Transforms to the Data\n",
    "img_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # No augmentations on valid data!\n",
    "    \"valid\": transforms.Compose( \n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # No augmentations on test data!\n",
    "    \"test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.utils\n",
    "\n",
    "dataset = datasets.ImageFolder(root=path)\n",
    "# split to train/valid/test\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.1), int(len(dataset)*0.1)])\n",
    "# define augmentations\n",
    "train_set.dataset.transform = img_transforms['train']\n",
    "valid_set.dataset.transform = img_transforms['valid']\n",
    "test_set.dataset.transform = img_transforms['test']\n",
    "\n",
    "print(f'Train size: {len(train_set)}')\n",
    "print(f'Valid size: {len(valid_set)}')\n",
    "print(f'Test size: {len(test_set)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size, valid_data_size = len(train_set), len(valid_set)\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "print('indexes to class: ')\n",
    "idx_to_class"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас не так уж и много изображений!! Обычная нейронка справиться не должна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим Alexnet без весов и попробуем обучить с нуля"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision import models\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "alexnet = models.alexnet(pretrained=False)\n",
    "print(alexnet)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы заменяем последний слой модели Alexnet одним последовательным слоем с 4096 нейронами и который имеет `num_classes` выходов, соответствующих числу классов в нашем подмножестве.\n",
    "\n",
    "То есть мы \"сказали\" нашей модели распознавать не 1000, а только N классов."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "import torch.nn as nn\n",
    "\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes) # change out classes, from 1000 to 10\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim=1)) # add activation\n",
    "print(alexnet)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы определяем функцию потерь и оптимизатор, который будет использоваться для обучения.\n",
    "\n",
    "Мы используем функцию Negative Loss Likelihood, поскольку она полезна для классификации нескольких классов и оптимизатор Adam - один из самых популярных оптимизаторов (например потому что он может адаптировать скорость обучения для каждого параметра индивидуально."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-4)\n",
    "print(optimizer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренировки и валидации нашей можели напишем отдельную функцию, которую итмеет смысл изучить подробнее."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def train_and_validate(model, criterion, optimizer, num_epochs=25):\n",
    "    \"\"\"\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "\n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clean existing gradients\n",
    "            outputs = model(inputs)  # Forward pass - compute outputs on input data using the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the parameters\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Compute correct predictions\n",
    "            train_correct += (torch.argmax(outputs, dim=-1)== labels).float().sum()\n",
    "\n",
    "        # Compute the mean train accuracy\n",
    "        train_accuracy = 100 * train_correct / (len(train_loader)*batch_size)\n",
    "\n",
    "        val_correct = 0\n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.eval()  # Set to evaluation mode\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)  # Forward pass - compute outputs on input data using the model\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                valid_loss += loss.item() * inputs.size(0)  # Compute the total loss for the batch and add it to valid_loss\n",
    "\n",
    "                val_correct += (torch.argmax(outputs, dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Compute mean val accuracy       \n",
    "        val_accuracy = 100 * val_correct / (len(valid_loader)*batch_size)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss / (len(train_loader)*batch_size)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss / (len(valid_loader)*batch_size)\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, train_accuracy, val_accuracy])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(\n",
    "            \"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
    "                epoch + 1,\n",
    "                avg_train_loss,\n",
    "                train_accuracy.detach().cpu(),\n",
    "                avg_valid_loss,\n",
    "                val_accuracy.detach().cpu(),\n",
    "                epoch_end - epoch_start,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    return model, history"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), criterion, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_fresh.pt\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графики"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну так себе результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробовать использовать *Fine-tunning*. Загрузим предобученную Alexnet:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "del alexnet\n",
    "alexnet = models.alexnet(pretrained=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку большинство параметров в нашей предварительно обученной модели уже обучены, мы сбрасываем значение поля requires_grad в значение false."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Freeze model parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заморозка параметров**: отключение переменных предобученной модели для последующих изменений. \"Замораживая\" переменные предобученной модели мы гарантируем, что обучаться будет только последний классификационный слой, а значения предобученной модели останутся прежними."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы поменяем последний слой таким образом, что бы он вместо 1000 классов ImageNet нам выдавал количество классов которое у нас есть"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim=1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-4)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), criterion, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_finetuning.pt\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сравним между собой"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history_fresh = np.array(torch.load('history_fresh.pt'))\n",
    "history_finetuning = np.array(torch.load('history_finetuning.pt'))\n",
    "\n",
    "ax[0].plot(history_fresh[:,:2])\n",
    "ax[0].plot(history_finetuning[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "\n",
    "ax[1].plot(history_fresh[:,2:])\n",
    "ax[1].plot(history_finetuning[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определенно стало лучше =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на картинки"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def predict(model, test_img_name, device):\n",
    "    \"\"\"\n",
    "    Function to predict the class of a single test image\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param test_img_name: Test image\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    transform = img_transforms[\"test\"]\n",
    "    test_img = torch.tensor(np.asarray(test_img_name)) \n",
    "    test_img = transforms.ToPILImage()(test_img)\n",
    "    plt.imshow(test_img)\n",
    "\n",
    "    test_img_tensor = test_img_name.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_img_tensor).to(device)\n",
    "        ps = torch.exp(out).to(device)\n",
    "        topk, topclass = ps.topk(3, dim=1)\n",
    "        for i in range(3):\n",
    "            print(\n",
    "                \"Predcition\",\n",
    "                i + 1,\n",
    "                \":\",\n",
    "                idx_to_class[topclass.cpu().numpy()[0][i]],\n",
    "                \", Score: \",\n",
    "                round(topk.cpu().numpy()[0][i], 2),\n",
    "            )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[0])\n",
    "predict(\n",
    "    trained_model.to(device),\n",
    "    valid_set[[np.where([x[1] == 0 for x in valid_set])[0]][0][1]][0],\n",
    "    device,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[6])\n",
    "predict(\n",
    "    trained_model,\n",
    "    valid_set[[np.where([x[1] == 6 for x in valid_set])[0]][0][10]][0],\n",
    "    device,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[8])\n",
    "predict(\n",
    "    trained_model,\n",
    "    valid_set[[np.where([x[1] == 8 for x in valid_set])[0]][0][5]][0],\n",
    "    device,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Мы только что увидели, как использовать предварительно обученную модель, обученную для 1000 классов ImageNet.\n",
    "\n",
    "* Она очень эффективно классифицирует изображения, принадлежащие к N различным классам, которые нас интересуют.\n",
    "\n",
    "* Также мы убедились в эффективности такого подхода для обучении классификации на небольшом наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few/One - Shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Существуют задачи где не представляется возможным разбить данные на классы так что бы в каждом классе было достаточно много объектов.\n",
    "\n",
    "Рассмотрим например задачу распознавания лиц."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L01_Intro/img/mp/video.png\"  width=\"700\">\n",
    "\n",
    "На вход системе подается фото лица человека. Требуется сопоставить его с другим изображениям(и) например хранящимися в БД и таким образом идентифицировать человека на фотографии."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "На первый взгляд кажется что это задача классификации.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_vera/l11_7.png\" width=\"700\">\n",
    "\n",
    "Все изображения одного человека будем считать относящимися к одному классу и модель будет этот класс предсказывать. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для небольшой организации, в которой всего несколько десятков сотрудников такой подход может сработать.\n",
    "При этом возникнут проблемы:\n",
    "\n",
    "1. Чтобы обучить такую ​​систему, нам сначала потребуется много (сотни) разных \n",
    "изображений каждого сотрудника.\n",
    "\n",
    "2. Когда человек присоединяется к организации или покидает ее придется поменять структуру модели и обучать ее заново.\n",
    "\n",
    "\n",
    "Это практически невозможно, для крупных организаций, где набор и увольнение происходит почти каждую неделю. И в принципе невозможно для города масштаба Москвы или Лондона в котором миллионы жителей и сотни тысяч приезжих."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Генерация embedding\n",
    "\n",
    "Поэтому используется другой подход.\n",
    "Вместо того что бы классифицировать изображения, модель учится выделять ключевые признаки и на их основе строить компактный вектор точно описывающий лицо.\n",
    "\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/emb.png\" width=\"500\">\n",
    "\n",
    "\n",
    "В англоязычной литературе такие вектора признаков называются **embedding** и мы тоже будем использовать это обозначение.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Может возникнуть вопрос \"Не потеряем ли мы важную информацию сжав так сильно\". \n",
    "Что бы ответить на него вспомним про фотороботы. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/photo_robot.png\" width=\"700\" >\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Для получения фотореалистичного изображения лица достаточно нескольких ключевых признаков: Глаза, волосы рот, нос ...\n",
    "\n",
    "Каждый из которых кодируется максимум несколькими сотнаями значений.\n",
    "\n",
    "То есть вектора - признака из 128 значений будет более чем достаточно.\n",
    "Правда интерпретировать значения которые закодирует в него нейросеть будет не столь просто.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Если нам удасться обучить модель кодировать в embedding признаки важные для сравнения, то мы сможем сравнивать веткора между собой.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/face_dist.png\" width=\"500\" >\n",
    "\n",
    "\n",
    "При этом расстояние между векторами для лиц которые похожи друг на друга будут маленькими а у непохожих наборот большими.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/clustrobo.png\" width=\"700\" >\n",
    "\n",
    "Это позволит найти человека сравнивая embedding его фотографии с другим из БД используя [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) или иной метод кластеризации.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, чтобы сеть могла обнаружить его лицо, нам требуется только одно изображение его лица, которое будет сохранено в базе данных. Используя его в качестве эталонного изображения, можно вычислять сходство для любого нового экземпляра,  представленного ей."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Сиамская сеть (Siamese Network)\n",
    "\n",
    "Как будет устроенна модель для генерации таких векторов-признаков?\n",
    "\n",
    "Можно было бы использовать обычную сеть обученную для задачи классифиикации, и затем удалить из нее последний(е) слой.\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/gan/vanilla_gen.png\" width=\"700\">\n",
    "\n",
    "Активации последнего слоя представляют собой отклики на некие высокоуровневые признаки важные для классификации и их можно интерпретировать как embeddind.\n",
    "\n",
    "Такой подход будет работать, но можно заметно улучшить точность используя лосс-функцию котороя оценивает именно качество сравнения а не классификации.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход, который мы будем рассматривать, является реализацией методологии, описанной в статье [Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf).\n",
    "\n",
    "Состоит он в следующем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_vera/l11_8.png\" width=\"700\"></center>\n",
    "<center><em>Используются две копии одной и той же сети, отсюда и название Siamese Networks.</em></center>\n",
    "\n",
    "Два входных изображения ($x_1$ и $x_2$) проходят через одну и ту же сверточную сеть, на выходе для каждого изображения генерируется вектор признаков фиксированной длины $h_{x_{1}}$ и $h_{x_{2}}$.\n",
    "\n",
    "Если модель обучена правильно, мы можем предположить: если два входных изображения принадлежат одному и тому же персонажу, то их векторы признаков также должны быть похожими, а если два входных изображения принадлежат разным символам, то их векторы признаков также будут разными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_9.png\" width=\"700\">\n",
    "\n",
    "Таким образом, оцениевая расстояние между двумя векторами признаков \n",
    "которое будет мамым для похожих объектов и большим для различных мы сможем оценить их сходство.\n",
    "\n",
    "Это центральная идея сиамских сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss\n",
    "\n",
    "Какой loss использовать для обучения такой сети?\n",
    "\n",
    "Видимо loss функция должна будет анализировать не один выход а два или больше.\n",
    "\n",
    "Популярной на сегодняшний день является  `Triplet loss`, которой требуется  три embedding вместо двух. \n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_10.png\" width=\"600\">\n",
    "\n",
    "Что бы сгенерировать три эмбеддинга модель должна получать на вход три изображения. \n",
    "\n",
    "Первые два должны относится к одному и тому же объекту (человеку) а третье к другому.\n",
    "\n",
    "\n",
    "Таким образом Триплет состоит из базового (\"якорного\" `anchor`), положительного и отрицательного образцов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама лосс функция будет выглядеть следующим образом:\n",
    "\n",
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_10.png\" width=\"600\">\n",
    "\n",
    "\n",
    "$\\color{red}{\\text{Перерисовать!  Обе верхних фотографии должны принадлежать одному объекту а не классу!\n",
    "}}$\n",
    "\n",
    "\n",
    "$$TripletLoss = \\sum_{1}^{N} L_i(x_i^{a},x_i^{p},x_i^{n})$$\n",
    "\n",
    "$$L_i(x_i^{a},x_i^{p},x_i^{n})=max(0,\\left\\| f(x_i^{a}) -f(x_i^{p}) \\right\\|_2^{2} - \\left\\| f(x_i^{a}) -f(x_i^{n}) \\right\\|_2^{2} + \\alpha)$$\n",
    "\n",
    "Где:\n",
    "\n",
    "\n",
    "$x_i^{a}$ - базовое изображение (anchor)\n",
    "\n",
    "$x_i^{p}$ - изображение того же объекта (positive)\n",
    "\n",
    "$x_i^{n}$ - изображение другого объекта (negative)\n",
    "\n",
    "$f(x)$ - выход модели (embedding) для входа $x$\n",
    "\n",
    "\n",
    "$\\left\\| a \\right\\|_2$ -это L2 (Euclidean norm), соответственно $\\left\\| a \\right\\|_2^{2}$ это L2 в квадрате.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "В `Triplet loss` расстояние между анкором и положительной выборки минимизировано, а расстояние между анкором и отрицательной выборки максимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_11.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но можно также использовать и `Contrastive Loss` о ней подробнее в статье [Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая модель не учится классифицировать изображение напрямую по какому-либо из выходных классов. Скорее, он изучает функцию подобия, которая принимает два изображения в качестве входных данных и выражает, насколько они похожи.\n",
    "\n",
    "Как это решает две проблемы, о которых мы говорили выше?\n",
    "- Для обучения этой сети нам не требуется слишком много экземпляров класса, а для построения хорошей модели достаточно лишь нескольких экземпляров.\n",
    "- Но самое большое преимущество в простоте ее обучения в случае появления нового сотрудника."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация сиамской сети\n",
    "---\n",
    "на примере датасета [Georgia Tech face database](http://www.anefian.com/research/face_reco.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "Загрузим небольшой фрагмент датасета с лицами. Внутри архива фото лиц сгруппированны по папкам\n",
    "\n",
    "\n",
    "\n",
    "*   s1/\n",
    "**  photo1.pgm\n",
    "**  photo2.pgm\n",
    "**  ...\n",
    "*   s2/\n",
    "*   s2/\n",
    "*    ...\n",
    "*   sn/\n",
    "\n",
    "В каждой папке фото лица одного и того же человека."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import clear_output\n",
    "!wget http://edunet.kea.su/repo/src/L11_Transfer_learning/small_face_dataset.zip\n",
    "!unzip small_face_dataset.zip\n",
    "clear_output()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Что бы результаты воспроизводились зафиксируем SEED"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "def set_random_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "set_random_seed(42)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset for TripletLoss\n",
    "\n",
    "Для TripletLoss потребуются три изображения: anchor, positive, negative и метод __get_item__ должен возвращать их нам. Первые два должны принадлежать одному человеку, а третье другому. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import random \n",
    "from PIL import Image\n",
    "\n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,dir=None,transform=None, splitter = '/'):\n",
    "        self.dir = dir\n",
    "        self.splitter = splitter\n",
    "        self.transform = transform        \n",
    "        self.files = glob(f\"{self.dir}/**/*.pgm\",recursive=True)\n",
    "        self.data =self.build_index()        \n",
    "    \n",
    "    def build_index(self):\n",
    "      index = {}      \n",
    "      for f in self.files:\n",
    "        id = self.path2id(f) \n",
    "        if not id in index:\n",
    "          index[id] = []\n",
    "        index[id].append(f)\n",
    "      return index\n",
    "    \n",
    "    def path2id(self,path):\n",
    "        return path.replace(self.dir,\"\").split(self.splitter)[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        anchor_path = self.files[index]\n",
    "        positive_path = self.find_positive(anchor_path)\n",
    "        negative_path = self.find_negative(anchor_path)\n",
    "        \n",
    "        # Loading the images\n",
    "        anchor = Image.open(anchor_path)\n",
    "        positive = Image.open(positive_path)\n",
    "        negative = Image.open(negative_path)\n",
    "                \n",
    "        if self.transform is not None:  # Apply image transformations           \n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive , negative\n",
    "\n",
    "    def find_positive(self,path):\n",
    "        id = self.path2id(path) \n",
    "        all_exept_my = self.data[id].copy()\n",
    "        all_exept_my.remove(path)\n",
    "        return random.choice(all_exept_my)\n",
    "\n",
    "    def find_negative(self,path):\n",
    "        all_exept_my_ids = list(self.data.keys())\n",
    "        id = self.path2id(path) \n",
    "        all_exept_my_ids.remove(id)\n",
    "        selected_id = random.choice(all_exept_my_ids)\n",
    "        return random.choice(self.data[selected_id])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выведем несколько изображений что бы убедится что класс датасета функционирует должным образом."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Create dataset instance\n",
    "siamese_dataset = SiameseNetworkDataset(\"faces/training/\",\n",
    "                                          transform=transforms.Compose([\n",
    "                                            transforms.Resize((105,105)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                          ])\n",
    "                                        )\n",
    "\n",
    "# Create dataloader & extract batch of data from it\n",
    "vis_dataloader = DataLoader(siamese_dataset, batch_size =8, shuffle=True)\n",
    "dataiter = iter(vis_dataloader)\n",
    "example_batch = next(dataiter) # anc, pos, neg\n",
    "\n",
    "# Show batch contents \n",
    "concatenated = torch.cat((example_batch[0],example_batch[1],example_batch[2]),0)\n",
    "grid = torchvision.utils.make_grid(concatenated)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1,2,0).numpy())\n",
    "plt.gcf().set_size_inches(20,60)\n",
    "plt.show()    "
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "В каждом столбце тройка изображений.Первое и второе принадлежат одному человеку, третье - другому."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели\n",
    "\n",
    "Нас устроит любая модель для работы с изображениями. Например Resnet18. \n",
    "\n",
    "Все что от нас требуется это:\n",
    "- заменить последний слой\n",
    "- отправлять на анализ три изображения вместо одного. Соответственно на выходе тоже будут три вектора признаков (embedding)\n",
    "\n",
    "\n",
    "Пожалуй единственный вопрос это размерность последнего слоя . В промышленных системмах распознавания лиц, которые тренируются на датасетах из миллионов изображений, используются embedding размерностью от 128 до 512.\n",
    "\n",
    "Значит для демонстрационной задачи нам с запасом хватить 64 значений. Значит  выход последнего линейного слоя должен быть равен 64."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(pretrained = False)\n",
    "        # Because we use grayscale images reduce input channel count to one\n",
    "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        # Replace ImageNet 1000 class classifier with 64- out linear layer \n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 64)\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        output1 = self.model(anchor)\n",
    "        output2 = self.model(positive)\n",
    "        output3 = self.model(negative)\n",
    "        \n",
    "        return output1, output2, output3"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "\n",
    "Загрузчики данных не отличаются от загрузчиков для обычной сети. \n",
    "Единственное отличие это две аугментации которые добавили к данным:\n",
    "\n",
    "*   Случайный поворот по вертикали (RandomHorizontalFlip)\n",
    "*   Размытие (GaussianBlur)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torchvision import transforms as img_transf\n",
    "from torchvision import datasets as ds\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Apply augmentations on train data\n",
    "img_trans_train = img_transf.Compose(\n",
    "    [\n",
    "        img_transf.Resize((105, 105)),\n",
    "        img_transf.RandomHorizontalFlip(),\n",
    "        img_transf.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        img_transf.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_trans_test = img_transf.Compose(\n",
    "    [img_transf.Resize((105, 105)), img_transf.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = SiameseNetworkDataset(\"faces/training/\",transform = img_trans_train)\n",
    "val_dataset = SiameseNetworkDataset(\"faces/testing/\",transform = img_trans_test)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, num_workers=2, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, num_workers=2, batch_size=1, shuffle=False) \n",
    "\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Отличие от  сетей для классификации в том что у модели 3 выхода и все их надо передать в loss. При этом нет меток в явном виде.\n",
    "Какой embedding отностся к позитивному образцу а какой к негативному определяется только порядком их следования."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def train(num_epochs, model, criterion, optimizer, train_loader):\n",
    "    loss_history = []\n",
    "    l = []\n",
    "    model.train()\n",
    "    for epoch in range(0, num_epochs):\n",
    "        \n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            anc, pos, neg = batch\n",
    "            output_anc, output_pos, output_neg = model(anc.to(device), pos.to(device), neg.to(device))\n",
    "            loss = criterion(output_anc, output_pos, output_neg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            l.append(loss.item())\n",
    "        last_epoch_loss =  torch.tensor(l[-len(train_loader):-1]).mean()\n",
    "        print(\"Epoch {} with {:.4f} loss\".format(epoch,last_epoch_loss))\n",
    "        \n",
    "    return l, last_epoch_loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "При использовании GPU, обучение на 5-ти эпохах займет около 15 сек:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SiameseNet().to(device)\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001 ) \n",
    "\n",
    "l, _ = train(5, model, criterion, optimizer, train_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выведем график loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([i for i in range(len(l))], l)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('num of epochs')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видно что он график лосс уже вышел на плато, значит можно проверять результат."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для начала выведем тройки изображений из проверочного датасета и посмотрим на расстояния для позитивных и негативных пар. Если модель обучилась, расстояния для позитывных пар будут меньше.\n",
    "\n",
    "P.S. По умолчанию TripletLoss минимизирует Евклидово расстояние."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Helper method for visualization\n",
    "def show(img, text=None):\n",
    "    img_np = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(75, 120, text, fontweight=\"bold\")\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def plot_imgs(model,test_loader):\n",
    "    distances_pos = []\n",
    "    distances_neg = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      for i, batch in enumerate(test_loader, 0):\n",
    "        anc, pos, neg = batch\n",
    "        output_anc, output_pos, output_neg = model(anc.to(device), pos.to(device), neg.to(device))\n",
    "        # compute euc. distance\n",
    "        distance_pos = F.pairwise_distance(output_anc, output_pos).item() \n",
    "        distance_neg = F.pairwise_distance(output_anc, output_neg).item() \n",
    "\n",
    "        distances_pos.append(distance_pos)\n",
    "        distances_neg.append(distance_neg)\n",
    "\n",
    "        if not i % 5 :\n",
    "                concatenated = torch.cat((anc, pos, neg))\n",
    "                show(    \n",
    "                    torchvision.utils.make_grid(concatenated),\n",
    "                    f\"Positive / negative euclidean distances: {distance_pos:.3f} / {distance_neg:.3f}\",\n",
    "                )\n",
    "\n",
    "    return distances_pos, distances_neg\n",
    "\n",
    "\n",
    "distances_pos, distances_neg = plot_imgs(model,val_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Но такая оценка субъективна, давайте посмотрим на распределение расстояний по категориям:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "distnaces = {\"The same person\": distances_pos, \"Another person\": distances_neg}\n",
    "\n",
    "ax = sns.displot(distnaces, kde=True, stat=\"density\")\n",
    "ax.set(xlabel=\"Pairwise distance\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Видно что для фото одного и того же человека, в большинстве случаев расстояние лежит в интервале от 0 до 5.5. \n",
    "\n",
    "А вот для фото разных людей от 5.5 до 19. \n",
    "\n",
    "Если бы мы проектировали систему распознавания лиц, нужно было бы выбрать порог что бы сравнивать с ним расстояние и принимать решение о том верифицировать фото как подлинное или нет.\n",
    "\n",
    "Соответственно, для нашего игрушечного датасета такой порог следует выбирать ~= 6 при условии что ошибки первого и второго рада для нас равнозначны."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ссылки по теме:\n",
    "\n",
    "[Ищем знакомые лица](https://habr.com/ru/post/317798/)\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S2351978920300263?via%3Dihub"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shots learning in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и другой интересный вариант, как можно использовать few-shots learning. Огромные модели (такие как GPT-3 и варианты) в целом способны генерировать любой контент. Вопрос лишь в том, как бы им объяснить что мы от них хотим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "GPT-3 не доступна для свободного пользования (OpenAI оказался не очень-то и open). Но, умельцы из сообщества ElutherAI сделали open-source версию **GPT-J**, которая работает сравнимо с оригинальной моделью. Более того, они ее захостили у себя на сайте, что бы каждый мог пользоваться без долгой и муторной подгрузки модели (установка зависимостей и загрузка весов для GPT-J занимает ~20 минут в колабе). Ей-то мы и воспользуемся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зайдем на [сайт модели](https://6b.eleuther.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед нами текстовый редактор, в который можно написать что угодно. Для примера давайте попробуем перефразирововать предложение: \n",
    "\n",
    "    Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.\n",
    "\n",
    "Например сформулировав задачу так: *paraphrase* `...sentence...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то результаты не впечатляют. Тут-то на помощь и приходит идея Few-shots. \n",
    "\n",
    "Что если мы в качестве затравки (*prompt*) скормим моделе 2-3 примера в которых покажим чего от нее хотим?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем вот такой prompt:\n",
    "\n",
    "    Original: Her life spanned years of incredible change for women as they gained more rights than ever before.\n",
    "    Paraphrase: She lived through the exciting era of women's liberation.\n",
    "\n",
    "    Original: Giraffes like Acacia leaves and hay, and they can consume 75 pounds of food a day.\n",
    "    Paraphrase: A giraffe can eat up to 75 pounds of Acacia leaves and hay daily.\n",
    "\n",
    "    Original: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task.\n",
    "    Paraphrase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значительно лучше! Ответ который получил автор блокнота с первого раза: \n",
    "\n",
    "**Paraphrase:** *Recent work has shown that you can get a lot of money for a lot of text if you pre-train it for a specific task, then fine-tune it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем с числовыми рядами.\n",
    "\n",
    "    Sequence: 2, 4,  6, 8\n",
    "    Continuation: 16, 32, 64, 128\n",
    "\n",
    "    Sequence: 3, 9, 27, 81\n",
    "    Continuation: 243, 729, 2187, 6561\n",
    "\n",
    "    Sequence: 4, 16, 64, 256\n",
    "    Continuation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И с геологической викториной:\n",
    "\n",
    "    Q: The molten rock that sits below the earth's surface is called what?\n",
    "    A: Magma\n",
    "\n",
    "    Q: Diamonds rank at number 10 on the Mohs scale, meaning they are the hardest mineral listed. Where would cubic zirconia (CZ) rank on the scale?\n",
    "    A: 8-8.5\n",
    "\n",
    "    Q: The emerald is a type of beryl. The distinctive green color of the emerald derives from trace amounts of what metallic elements?\n",
    "    A: \n",
    "\n",
    "Ответы лучше перепроверить, а вот вопросы для викторины готовы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не стесняйтесь придумывать свои собственные примеры и задачи! Have fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто, когда мы пишем и обучаем сети (будь то с нуля или с помощью transfer learning) мы вынуждены угадывать гиперпараметры (lr, betas и тд). В случае с learning rate нам есть от чего оттолкнуться (маленький lr для transfer learning), [константа Karpathy (3e-4)](https://twitter.com/karpathy/status/801621764144971776?lang=en) для Adam, но все же, такой подход не кажется оптимальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации гиперпараметров существуют готовые решения, которые используют различные методы black-box оптимизации. Разберем одну из наиболее популярных библиотек - [**Optuna**](https://optuna.org/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install --quiet optuna"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте оптимизируем наш learning rate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import optuna\n",
    "\n",
    "# define function which will optimized\n",
    "def objective(trial):\n",
    "    # boundaries for the optimizer's \n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-2)\n",
    "    \n",
    "    ##### If you need more parameters for optimization, it is done like this:\n",
    "    #new_parameter =  trial.suggest_loguniform(\"new_parameter\", lower_bound, upper_bound)\n",
    "\n",
    "    # create new model(and all parameters) every iteration\n",
    "    model = SiameseNet().to(device)\n",
    "    criterion = nn.TripletMarginLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=lr\n",
    "    )  # learning step regulates by optuna\n",
    "    \n",
    "\n",
    "    # To save time, we will take only 3 epochs\n",
    "    _, last_epoch_loss = train(3, model, criterion, optimizer, train_loader)\n",
    "    return last_epoch_loss\n",
    "\n",
    "\n",
    "# Create \"exploration\"\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"Optimal lr\")\n",
    "\n",
    "#\n",
    "study.optimize(\n",
    "    objective, n_trials=10\n",
    ")  # The more iterations, the higher the chances of catching the most optimal hyperparameters"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# show best params\n",
    "study.best_params"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно же, можно оптимизировать сразу несколько параметров за раз, а еще в качестве параметров можеть выступать сама архитектура сети (например количесвто слоев и каналов). Это можно легко сделать по анологии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на историю оптимизации нашего lr"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, проверим а станет ли реально лучше"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = SiameseNet().to(device)\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=study.best_params[\"lr\"]\n",
    ")  # take lr, which choosen Optuna\n",
    "#scheduler = ReduceLROnPlateau(optimizer, \"min\", verbose=True, factor=0.5, patience=3)\n",
    "\n",
    "l_optim, _ = train(5,  model, criterion, optimizer, train_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.plot([i for i in range(len(l))], l, label=\"no optimization\")\n",
    "plt.plot([i for i in range(len(l))], l_optim[:len(l)], label=\"optimal params\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('num of epochs')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "distances_pos, distances_neg = plot_imgs(model,val_loader)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "distnaces_optim = {\"The same person\": distances_pos, \"Another person\": distances_neg}\n",
    "\n",
    "ax_0 = sns.displot(distnaces, kde=True, stat=\"density\", alpha=0.5)\n",
    "ax_1 = sns.displot(distnaces_optim, kde=True, stat=\"density\", alpha=0.5)\n",
    "ax_0.set(xlabel=\"Pairwise distance\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\"> Заключение\n",
    "\n",
    "\n",
    "Мы затронули проблемы, которые возникают при обучени на реальных данных.\n",
    "\n",
    "Одна из основных проблем - малые датасеты. Для того, чтобы обучить нейронку на небольшом датасете можно использовать:\n",
    "- `аугментацию`\n",
    "- `Tranfer learning`\n",
    "Однако необходимо помнить, что ни один из этих методов не защитит от ситуации, когда реальные данные будут сильно отличаться от тренировочных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае, когда у нас не только мало данных, но и еще и очень большое (возможно, неизвестное) число классов, можно воспользоваться `One-shot Learning`. В этом случае нейронка обучается не классифицировать изображения, а, наоборот, находить различия между классом и новыми данными. Для этого используются нейронные сети, относящиеся к классу сиамских нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же мы разобрали автоматическую оптимизацию гиперпараметров с помощью Optuna и показали, что это эффективный способ сделать нейросеть лучше малой ценой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\">Литература\n",
    "\n",
    "\n",
    "<font size = \"5\"> Обучение на реальных данных\n",
    "\n",
    "[How to avoid machine learning pitfalls: a guide for academic researchers (Lones, 2021)](https://arxiv.org/abs/2108.02497)\n",
    "\n",
    "[Understanding data augmentation for classification: when to warp? (Wong et al., 2016)](https://arxiv.org/abs/1609.08764) \n",
    "\n",
    "[Learning from class-imbalanced data: Review of methods and applications (Haixiang et al., 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175?via%3Dihub)\n",
    "\n",
    "<font size = \"5\"> Как решить проблему маленького количества данных?\n",
    "\n",
    "[Блог-пост о том как решать проблему малого количества данных.](https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d)\n",
    "\n",
    "<font size = \"5\"> Несбалансированные данные\n",
    "\n",
    "[Imbalanced Data: How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/).\n",
    "\n",
    "[SMOTE explained for noobs - Synthetic Minority Over-sampling TEchnique line by line](https://rikunert.com/SMOTE_explained)\n",
    "\n",
    "[Блог пост про 8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "\n",
    "[Метрики разработаные для работы с несбалансированными классами.](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/#:~:text=Classification%20Accuracy%20is%20Not%20Enough%3A%20More%20Performance%20Measures%20You%20Can%20Use,-By%20Jason%20Brownlee&text=When%20you%20build%20a%20model,This%20is%20the%20classification%20accuracy)\n",
    "\n",
    "[Творческий подход](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)\n",
    "\n",
    "<font size = \"5\"> Transfer Learning\n",
    "\n",
    "[Image Classification using Transfer Learning in Pytorch](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/)\n",
    "\n",
    "[How To Do Transfer Learning For Computer Vision | PyTorch Tutorial](https://www.youtube.com/watch?v=6nQlxJvcTr0)\n",
    "\n",
    "[Transfer learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "[Python Pytorch Tutorials # 2 Transfer Learning : Inference with ImageNet Models](https://www.youtube.com/watch?v=Upw4RaERZic)\n",
    "\n",
    "[PyTorch - The Basics of Transfer Learning with TorchVision and AlexNet](https://www.youtube.com/watch?v=8etkVC93yU4)\n",
    "\n",
    "\n",
    "<font size = \"5\">Augmentation\n",
    "\n",
    "[A survey on Image Data Augmentation for Deep Learning (Shorten and Khoshgoftaar, 2019)](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)\n",
    "\n",
    "[Data augmentation for improving deep learning in image classification problem](https://www.researchgate.net/publication/325920702_Data_augmentation_for_improving_deep_learning_in_image_classification_problem)\n",
    "\n",
    "\n",
    "<font size = \"5\"> Few-shot learning\n",
    "\n",
    "[One-Shot Learning with Siamese Networks using Keras](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d)\n",
    "\n",
    "[One-Shot image classification by meta learning](https://medium.com/nerd-for-tech/one-shot-learning-fe1087533585)\n",
    "\n",
    "[One-Shot Learning (Part 1/2): Definitions and fundamental techniques](https://heartbeat.fritz.ai/one-shot-learning-part-1-2-definitions-and-fundamental-techniques-1df944e5836a)\n",
    "\n",
    "[One-Shot Learning (Part 2/2): Facial Recognition Using a Siamese Network](https://heartbeat.fritz.ai/one-shot-learning-part-2-2-facial-recognition-using-a-siamese-network-5aee53196255)\n",
    "\n",
    "[FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff et al., 2015)](https://arxiv.org/abs/1503.03832)\n",
    "\n",
    "[One-Shot Learning explained using FaceNet](https://medium.com/intro-to-artificial-intelligence/one-shot-learning-explained-using-facenet-dff5ad52bd38)\n",
    "\n",
    "[Ищем знакомые лица](https://habr.com/ru/post/317798/)\n",
    "\n",
    "[Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "[Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)\n",
    "\n",
    "[Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "<font size = \"5\"> Hyperparameter optimization\n",
    "\n",
    "[Tuning Hyperparameters with Optuna](https://towardsdatascience.com/tuning-hyperparameters-with-optuna-af342facc549)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {}
}
