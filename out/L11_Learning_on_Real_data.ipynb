{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Обучение на реальных данных</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проблемы при работе с реальной задачей машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В реальных задачах, особенно если вы работаете над новой проблемой, вы столкнетесь с широким спектром трудностей. Приведем часть из них и сопроводим примером на основе задач нахождения клеток крови на фотографии мазка крови."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/bloods_cell.jpg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **нехватка данных** — фотографий мазков крови может быть недостаточно для построения сложной модели с нуля\n",
    "\n",
    "- **недостаток размеченных данных** — возможно, существует достаточно большое количество фотографий мазков крови (например, в историях болезни), но очень малая часть из них размечена\n",
    "\n",
    "- **некачественная разметка** — мазок крови могли доверить анализировать студенту-практиканту. Размечать его мог вообще человек не из профессии — например, хотевший таким образом увеличить обучающую выборку для модели на конкурс Kaggle. Даже в широко известных MNIST, CIFAR-10 и ImageNet есть ошибки в разметке ([примеры](https://labelerrors.com/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/wrong_imagenet_prediction.png\" width=\"300\">\n",
    "\n",
    "<em>Source: <a href=\"https://labelerrors.com/\">labelerrors.com</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **низкое качество данных** — не все фотографии будут в хорошем разрешении. Да и не все образцы были правильно подготовлены. А еще могут попадаться фотографии от пациентов, больных чем-нибудь экзотическим, в результате чего их клетки выглядят сильно отличающимися от обычных. Или же в крови пациента просто плавают паразиты\n",
    "\n",
    "Серповидная клеточная анемия приводит к аномальным эритроцитам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/normal_and_sickle_shaped_erythrocytes.JPG\" width=\"500px\">\n",
    "\n",
    "<em>Source: <a href=\"https://commons.wikimedia.org/wiki/File:Sicklecell3.jpg\">Wikipedia</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А так выглядит мазок крови при сонной болезни"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/trypanosoma_among_red_blood_cells.jpg\" width=\"500px\">\n",
    "\n",
    "<em>Source: <a href=\"https://commons.wikimedia.org/wiki/File:Trypanosoma_sp._PHIL_613_lores.jpg\">Wikipedia</a></em> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **несбалансированность датасета** — клетки крови встречаются в разных пропорциях. Какие-то классы могут быть плохо представлены (**минорные классы**) Потому в вашем датасете может быть всего 10 фотографий, на которых присутствуют базофилы. Нейросети будет очень заманчиво вообще не пытаться найти базофилы (всего 10 ошибок)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L11/out/leukocyte_blood_formula_table.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **ковариантный сдвиг** — явление, когда признаки тренировочной и тестовой выборок **распределены по-разному**. Ковариантный сдвиг может стать серьезной проблемой для практического применения моделей.\n",
    "\n",
    "Модель учится сопоставлять целевые значения признакам. В такой ситуации модель не в состоянии делать адекватные предсказания на тесте, так как во время обучения она не видела области пространства, в которой расположены тестовые объекты.\n",
    "Источники ошибок, приводящих к ковариантному сдвигу, обсуждались ранее [в лекции №7](https://edunet.kea.su/repo/msu_ai/html/L07_Batch_normalization.html#Covariate-shift-(Ковариантный-сдвиг)).\n",
    "\n",
    "**Практический совет:** для быстрого обнаружения ковариантного сдвига можно **обучить модель**, которая будет предсказывать, относится ли объект к **train** или **test** выборке. Если модель легко делит данные, то имеет смысл визуализировать значения признаков, по которым она это делает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/covariate_shift.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **полные дубликаты** — в данных могут быть полные дубликаты. Кто-то до вас агрегировал фотографии из разных источников, и вы либо не обратили на это внимание, либо он забыл об этом сказать. Такие данные надо сразу помечать и использовать только после предварительного размышления, т.к. они могут мешать вам и на этапе обучения модели, и на итоговой валидации ее качества (если один и тот же объект попадет и в обучение, и в валидацию). \n",
    "\n",
    "- **неполные дубликаты** — в данных могут быть данные от одного и того же пациента. Кажется, что если это разные мазки крови, то всё нормально. На самом деле и это уменьшает количество информации, которую может извлечь нейросеть из данных. С такими данными также нужно аккуратно работать и не допускать попадания одного пациента и в обучение, и в тест.\n",
    "\n",
    "- **малое число источников данных** — проблема, родственная предыдущей. В вашем датасете могут быть данные только от одного микроскопа или одной модели микроскопа. Могут быть данные, снятые только одним специалистом, или в одной больнице, или только у взрослых (фотографий мазков детей нет). Это так же может влиять на способность вашего алгоритма генерализовывать полученное решение и требует пристального внимания.\n",
    "\n",
    "Все это приводит к целому спектру проблем, из которых самой типичной будет переобучение модели — какую бы простую модель вы не взяли, она все равно будет выучивать искажения вашего датасета. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общие подходы при работе с реальными данными\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нехватка данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас мало данных, попробуйте найти еще данные для вашей задачи. \n",
    "Совет приводится во многих инструкциях по борьбе с малым количеством данных и может быть воспринят с юмором. \n",
    "Однако часто для вашей задачи действительно существуют данные, собранные другими людьми. Также часто можно найти данные, которые очень похожи на ваши, и их можно использовать в обучении, но, например, учитывать с меньшим весом.\n",
    "Даже 20 дополнительных примеров могут сильно облегчить ситуацию. \n",
    "\n",
    "Вы также можете использовать данные, которые не совсем похожи на ваши, в качестве внешней валидации. Тем самым вы можете разбивать свой изначальный датасет только для кросс-валидации и не выделять отдельную часть для теста. Тестом послужат как раз \"не совсем\" похожие данные. \n",
    "\n",
    "Также можно использовать две техники, которые мы сегодня рассмотрим подробнее: Аугментация и Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде всего надо убедиться, что датасет сбалансирован:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "\n",
    "def show_class_balance(y, classes):\n",
    "    _, counts = torch.unique(torch.tensor(y), return_counts=True)\n",
    "    plt.bar(classes, counts)\n",
    "    plt.ylabel(\"n_samples\")\n",
    "    plt.ylim([0, 75])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "wine = load_wine()\n",
    "classes = wine.target_names\n",
    "\n",
    "show_class_balance(wine.target, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница в 10-20% будет незначительна, поэтому для наглядности мы искусственно разбалансируем наш датасет при помощи метода [make_imbalance](https://imbalanced-learn.org/stable/references/generated/imblearn.datasets.make_imbalance.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "\n",
    "x, y = make_imbalance(\n",
    "    wine.data, wine.target, sampling_strategy={0: 10, 1: 70, 2: 40}, random_state=42\n",
    ")\n",
    "show_class_balance(y, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение баланса класса сэмплированием "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в данных недостаток именно конкретного класса, то можно бороться с этим при помощи разных способов сэмплирования. \n",
    "\n",
    "Важно понимать, что в большинстве случаев данные, полученные таким способом, должны использоваться в качестве **обучающего набора**, но ни в коем случае не в качестве **валидации** или **теста**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дублирование примеров меньшего класса (oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем увеличить число объектов меньшего класса за счет дублирования. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/oversampling_scheme.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Дублирование примеров меньшего класса</em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае наша модель будет \"вынуждена\" обращать внимание на минорный класс. \n",
    "\n",
    "Такой `Resampling` может быть выполнен с помощью пакета [imbalanced-learn](https://pypi.org/project/imbalanced-learn/), как показано ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x_ros, y_ros = ros.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_ros, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Уменьшение числа примеров большего класса (undersampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично, можно взять для обучения не всех представителей большего класса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/undersampling_scheme.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Удаление примеров преобладающего класса</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это также вынуждает модель обращать внимание на оба класса. \n",
    "Минус подхода очевиден: мы можем выбросить важных представителей большего класса, ответственных за существенное улучшение генерализации,  и за счет этого качество модели существенно ухудшится. \n",
    "Можно пытаться выбрасывать объекты большего класса как-то по-умному. Например, кластеризовать объекты большего класса и брать по заданному количеству объектов из каждого класса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_res, y_res = rus.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_res, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ансамбли + undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать ансамбли вместе с undersampling. В этом случае мы можем, к примеру, делать сэмплирование только большего класса, а объекты минорного класса оставлять как есть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L11/out/ensembles_and_undersampling.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или просто сэмплировать объектов и того, и другого класса равное количество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Балансирование представленности объектов в батчах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае нейросетей можно балансировать встречаемость каждого класса не на уровне датасета, а на уровне батча. Например, собираем каждый батч таким образом, чтобы в нем было поровну всех классов.\n",
    "\n",
    " Это может улучшать сходимость даже в случае небольшого дисбаланса или его отсутствия, т.к. мы будем избегать шаги обучения нейросети, в которых она просто не увидела какого-то класса в силу случайных причин.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/L11/out/batch_balancing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch эту функциональность можно получить, используя класс [WeightedRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler) . Для его инициализации требуется рассчитать вес каждого класса. Сумма весов не обязана быть равной единице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.unique.html\n",
    "_, counts = torch.unique(torch.tensor(y), return_counts=True)\n",
    "weights = counts.max() / counts\n",
    "print(\"Classes: \", classes)\n",
    "print(\"Weights: \", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создаем объект [WeightedRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler), в конструктор подаем два аргумента:\n",
    "\n",
    "- список весов для **каждого** элемента в датасете;\n",
    "- количество элементов (можно использовать не весь датасет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "tensor_x = torch.Tensor(x)  # transform to torch tensor\n",
    "tensor_y = torch.Tensor(y)\n",
    "dataset = TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "weight_for_sample = []  # Every sample must have a weight\n",
    "for l in y:\n",
    "    weight_for_sample.append(weights[l].item())\n",
    "\n",
    "sampler = WeightedRandomSampler(torch.tensor(weight_for_sample), len(dataset))\n",
    "loader = DataLoader(dataset, batch_size=32, drop_last=True, sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение элементов разных классов по батчам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels = []\n",
    "for data, labels in loader:\n",
    "    print(\n",
    "        \"Labels:\",\n",
    "        labels.int().tolist(),\n",
    "        \"Classes in batch:\",\n",
    "        torch.unique(labels, return_counts=True)[1].tolist(),\n",
    "    )\n",
    "    batch_labels.append(labels.tolist())\n",
    "\n",
    "show_class_balance(batch_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты будут отличаться от запуска к запуску. Но видно, что в батчах объекты каждого класса встречаются почти равномерно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что нужно быть осторожным со взвешиванием объектов в батчах и **контролировать состав батчей**. Дело в том, что при существенном дисбалансе веса при объектах минорного класса могут оказываться на несколько порядков больше, чем при объектах мажорного класса. Данные веса преобразуются в вероятности для сэмплирования, и может случиться так, что вероятности при объектах мажорного класса станут численно неотличимы от нуля. Тем самым можно получить обратный эффект: батчи будут состоять исключительно из объектов минорного класса. В таком случае нужно намеренно ограничивать веса, чтобы не допускать такого эффекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация синтетических данных\n",
    "\n",
    "Другой подход к решению этой проблемы — создание синтетических данных. Делать это можно по-разному. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE\n",
    "\n",
    "**Synthetic Minority Over-sampling Technique (SMOTE)** позволяет генерировать синтетические данные за счет реальных объектов из минорного класса. \n",
    "\n",
    "Алгоритм работает следующим образом:\n",
    "\n",
    "1. Для случайной точки из минорного класса выбираем $k$ ближайших соседей из того же класса. \n",
    "2. Для первого соседа проводим отрезок, соединяющий его и выбранную точку. На этом отрезке случайно выбираем точку. \n",
    "3. Эта точка — новый **синтетический** объект минорного класса.\n",
    "4. Повторяем процедуру для оставшихся соседей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/generate_synthetic_data.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число соседей, как и число раз, которое мы запускаем описанную выше процедуру, можно регулировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "x_smote, y_smote = oversample.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_smote, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество объектов каждого класса, которое должно получиться после генерации, можно задать явно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy={0: 20, 1: 70, 2: 70})\n",
    "x_smote, y_smote = over.fit_resample(x, y)\n",
    "\n",
    "show_class_balance(y_smote, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее про использование пакета можно прочесть в статье: [SMOTE for Imbalanced Classification with Python](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменение функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели в машинном обучении “ленивы”. При работе с несбалансированными классами модель будет чаще сталкиваться с доминирующим классом и вместо того, чтобы разбираться в признаках объектов, может начать ориентироваться на статистическое распределение классов. \n",
    "\n",
    "Пример: датасет, в котором 95% объектов относятся к классу 1 и 5% к классу 0. Модель может выучиться всегда относить объекты к классу 1, и в 95% случаях она будет права."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Веса классов\n",
    "\n",
    "Чтобы это поправить, можно изменить функцию потерь так, чтобы больше штрафовать модель за ошибки в минорных классах. Для этого можно:\n",
    "- Добавлять веса в функцию потерь для компенсации дисбаланса классов.\n",
    "Во многих функциях потерь в PyTorch (например, [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) есть параметр `weight`, который имеет по умолчанию значение `None`. В него можно передать тензор весов, соответствующий размеру вектора целевых значений, и получить взвешенную функцию ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как это работает. Допустим, мы получили от нейросети неверные предсказания: \n",
    "\n",
    "второй объект должен относиться к классу 1, а не 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.tensor([[30.0, 2.0], [30.0, 2.0]])  # Scores for batch of two samples\n",
    "target = torch.tensor([0, 1])  # Second sample belongs to class 1\n",
    "# but logit for class 0 greater: 30 > 2. So it was misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем Cross-Entropy Loss без весов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Loss =  \\frac{1}{2} \\biggr[- \\ln\\frac{e^{30}}{e^{30}+e^{2}} - \\ln\\frac{e^{2}}{e^{30}+e^{2}}\\biggr]\\approx 14.0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(scores, target)\n",
    "print(f\"Loss = {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у нас есть два класса с соотношением 4:1, можно задать веса `weight = [0.2, 0.8]`. И так как сеть ошиблась на классе с большим весом, то ошибка вырастет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Loss =  \\biggr[-0.2 \\ln\\frac{e^{30}}{e^{30}+e^{2}} -0.8 \\ln\\frac{e^{2}}{e^{30}+e^{2}}\\biggr]\\approx 22.4 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.2, 0.8], dtype=torch.float32)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "loss = criterion(scores, target)\n",
    "print(f\"Loss = {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сумма весов может быть не равна единице:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor([2.0, 8.0]))\n",
    "loss = criterion(scores, target)\n",
    "print(f\"Loss = {loss.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Иногда качество модели можно улучшить, взяв квадратные корни от полученных таким образом весов (немного снижает штрафы за ошибки на редких классах).\n",
    "\n",
    "- Не смотря на интуитивно понятную логику работы способа, он не всегда дает значительный эффект. Тем не менее, на практике стоит пробовать экспериментировать с этим способом наряду с прочими техниками борьбы с дисбалансом.\n",
    "\n",
    "- Можно менять форму функции потерь. В 2017 году для работы с несбалансированными классами был предложен [Focal Loss](https://arxiv.org/abs/1708.02002?context=cs). Это — модификация Cross-Entropy Loss, которая модифицирует ее [форму](https://medium.com/visionwizard/understanding-focal-loss-a-quick-read-b914422913e7) для различных классов. С тех пор появились различные [модификации](https://paperswithcode.com/method/focal-loss) функции ошибок, с которыми можно и нужно экспериментировать. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss — это функция потерь, используемая в нейронных сетях для решения проблемы классификации *сложных* объектов (hard examples).\n",
    "\n",
    "**Приведем пример:** пусть модель должна классифицировать фрукты на два класса: яблоки и груши. В наборе данных есть много явных представителей того и иного класса: зеленые яблоки и желтые груши. Для модели они будут *простыми*: она на них не ошибается, и предсказываемая вероятность истинного класса для них велика и равна $0.9$. \n",
    "\n",
    "В то же время в наборе данных есть малое количество зеленых груш: они по форме похожи на груши, а по цвету — на яблоки. Говоря более общими словами, зеленые груши *ближе к границе классов в пространстве признаков*. Для модели такие примеры могут оказаться *сложными*, и она будет на них ошибаться. Пусть для зеленой груши вероятность истинного класса равна $0.2$, то есть модель ошиблась и предсказала класс \"яблоко\" с вероятностью $0.8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://edunet.kea.su/repo/EduNet-content/L11/out/hard_examples_fruits.png' width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема состоит в том, что сумма большого количества малых ошибок на *простых* объектах может перевешивать сумму малого количества ошибок потерь на *сложных* объектах. Поэтому модель будет плохо учиться верно классифицировать сложные объекты: ей будет \"лень\" исправлять незначительные ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения этой проблемы была предложена специальная функция потерь — Focal Loss. Она немного модифицирует кросс-энтропию для придания большей значимости ошибкам на сложных объектах.\n",
    "\n",
    "Focal Loss была предложена в статье [Focal Loss for Dense Object Detection (Lin et al., 2017)](https://arxiv.org/abs/1708.02002) изначально для задачи детектирования объектов на изображениях. Она определяется так:\n",
    "\n",
    "$$\\large\\text{FL}(p_t) = -(1 - p_t)^\\gamma\\text{log}(p_t)$$\n",
    "\n",
    "Здесь $p_t$ — предсказанная вероятность истинного класса, а $\\gamma$ — настраиваемый гиперпараметр. \n",
    "\n",
    "Focal Loss уменьшает потери на уверенно классифицируемых примерах (где $p_t>0.5$) и больше фокусируется на сложных примерах, которые классифицированы неправильно. Параметр $\\gamma$ управляет относительной важностью неправильно классифицируемых примеров. Более высокое значение $\\gamma$ увеличивает важность неправильно классифицированных примеров. В экспериментах авторы показали, что параметр $\\gamma=2$ показывал себя наилучшим образом в их задаче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При $\\gamma=0$ Focal Loss становится равной Cross-Entropy Loss, которая выражается как обратный логарифм вероятности истинного класса:\n",
    "\n",
    "$$\\large\\text{CE}(p_t)=-\\text{log}(p_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разберем на нашем примере** с яблоками и грушами. \n",
    "\n",
    "Мы имеем **20 простых** объектов с вероятностью истинного класса $0.9$: 10 яблок и 10 груш, **и один сложный** объект с вероятностью истинного класса $0.2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{\\text{CE} = \\overbrace{\\sum^{20}-\\text{log}(0.9)}^{\\large\\color{#3C8031}{\\text{loss(easy apples and pears)=2.11}}} + \\overbrace{(-\\text{log}(0.2))}^{\\large\\color{#F26035}{\\text{loss(hard pear)=1.61}}} \\approx 3.72}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{\\text{FL}(\\gamma=2) = \\overbrace{\\sum^{20}-\\color{#AF3235}{\\underbrace{(1-0.9)^2}_{0.01}}\\text{log}(0.9)}^{\\large\\color{#3C8031}{\\text{loss(easy apples and pears)=0.02}}} + \\overbrace{(-\\color{#AF3235}{\\underbrace{(1-0.2)^2}_{0.64}}\\text{log}(0.2))}^{\\large\\color{#F26035}{\\text{loss(hard pear)=1.03}}} \\approx 1.05}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, потери для уверенно классифицированных объектов дополнительно занижаются.\n",
    "\n",
    "Этот эффект достигается путем домножения на коэффициент: $ \\large(1-p_{t})^\\gamma$\n",
    "\n",
    "Пока модель ошибается, $p_{t}$ — мала, и значение выражения в скобках соответственно близко к 1.\n",
    "\n",
    "Когда модель обучилась, значение $p_{t}$ становится близким к 1, а разность в скобках становится маленьким числом, которое возводится в степень $ \\gamma \\ge 0 $. Таким образом, домножение на это небольшое число нивелирует вклад верно классифицированных объектов.\n",
    "\n",
    "Это позволяет модели сосредоточиться (сфокусироваться, отсюда и название) на изучении сложных объектов (hard examples).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примере коэффициент $(1-p_t)^\\gamma$ в Focal Loss в 100 раз занизил потери при уверенной классификации простых яблок и груш, и потери при неверной классификации сложной груши стали преобладать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/focal_loss_vs_ce.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/1708.02002\">Focal Loss for Dense Object Detection (Lin et al., 2018)</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посчитаем для различных значений $γ$, сколько понадобится примеров с небольшой ошибкой (высокой вероятностью истинного класса, равной $0.9$), чтобы получить суммарный **Focal Loss** примерно такой же, как у одного примера с большой ошибкой (низкой вероятностью истинного класса, равной $0.2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(prob_true):\n",
    "    return -np.log(prob_true)\n",
    "\n",
    "\n",
    "def focal_loss(prob_true, gamma=2):\n",
    "    return (1 - prob_true) ** gamma * cross_entropy(prob_true)\n",
    "\n",
    "\n",
    "p1 = 0.9  # probability of easy examples predictions\n",
    "p2 = 0.2  # probability of hard examples predictions\n",
    "gammas = [0, 0.5, 1, 2, 5, 10, 15]\n",
    "\n",
    "print(\n",
    "    f\"For probability of easy examples predictions {p1} and probability of hard examples predictions {p2}\\n\"\n",
    ")\n",
    "\n",
    "for gamma in gammas:\n",
    "    fl1 = focal_loss(p1, gamma)\n",
    "    fl2 = focal_loss(p2, gamma)\n",
    "\n",
    "    print(\n",
    "        f\"gamma = {gamma},\".ljust(15),\n",
    "        f\"for an equal loss with a problematic prediction, almost correct ones are required {int(fl2 / fl1)}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, при увеличении значения $\\gamma$ можно достичь значительного роста \"важности\" примеров с высокой ошибкой, что по сути позволяет модели обращать внимание на \"hard examples\".\n",
    "\n",
    "Этот пример также показывает **опасность Focal Loss**: если мы имеем **ошибки в разметке**, то при большом $\\gamma$ можно начать очень сильно наказывать модель за ошибки на неверно размеченных примерах, что может привести к переобучению под ошибки в разметке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss может применяться также и в задачах с дисбалансом классов. В этом смысле объекты преобладающего класса могут считаться простыми, а объекты минорного класса — сложными.\n",
    "\n",
    "Однако для работы с дисбалансом в Focal Loss могут быть добавлены веса для классов. Тогда формула будет выглядеть так:\n",
    "\n",
    "$$\\large\\text{FL}(p_t) = -\\alpha_t(1 - p_t)^\\gamma\\text{log}(p_t)$$\n",
    "\n",
    "Здесь $\\alpha_t$ — вес для истинного класса, имеющий такой же смысл, как параметр `weight` в Cross-Entropy Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss не реализована в PyTorch нативно, но существуют сторонние совместимые реализации. Посмотрим, как воспользоваться [одной из них](https://github.com/AdeelH/pytorch-multi-class-focal-loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!wget https://raw.githubusercontent.com/AdeelH/pytorch-multi-class-focal-loss/master/focal_loss.py\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from focal_loss import FocalLoss\n",
    "\n",
    "\n",
    "criterion = FocalLoss(alpha=None, gamma=2.)\n",
    "\n",
    "model_output = torch.rand(3, 3)  # model output is logits, as in CELoss\n",
    "print(f\"model_output:\\n {model_output}\")\n",
    "\n",
    "target = torch.empty(3, dtype=torch.long).random_(3)\n",
    "print(f\"target: {target}\")\n",
    "\n",
    "loss_fl = criterion(model_output, target)\n",
    "print(f\"loss_fl: {loss_fl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что сторонняя реализация вычисляет то, что нужно, и вычислим значение вручную. В первую очередь нужно перевести `model_output` из логитов в вероятности с помощью softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(model_output, dim=1)\n",
    "\n",
    "print(f\"probabilities after softmax:\\n {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вручную рассчитаем значение функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(prob_true, gamma=2):\n",
    "    return - (1 - prob_true) ** gamma * np.log(prob_true)\n",
    "\n",
    "hand_calculated_loss = 0\n",
    "\n",
    "for i in range(3):\n",
    "    hand_calculated_loss += focal_loss(probs[i, target[i]])\n",
    "\n",
    "hand_calculated_loss /= 3  # average by number of samples\n",
    "\n",
    "print(f\"hand-calculated focal loss:    {hand_calculated_loss.item()}\")\n",
    "print(f\"library-calculated focal loss: {loss_fl}\")\n",
    "print(f\"Are results almost equal? {torch.isclose(loss_fl, hand_calculated_loss).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, при расчете вручную получили то же значение, что и при расчете с помощью сторонней реализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обнаружение аномалий / изменений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае сильно несбалансированных наборов данных, таких как fraud (мошенничество) или машинный сбой, стоит задуматься, могут ли такие примеры рассматриваться как аномалия (выброс) или нет. Если такое событие и впрямь может считаться аномальным, мы можем использовать такие модели, как `OneClassSVM`, методы кластеризации или методы обнаружения гауссовских аномалий.\n",
    "\n",
    "Эти методы требуют изменения взгляда на задачу: мы будем рассматривать аномалии как отдельный класс выбросов. Это может помочь нам найти новые способы разделения и классификации.\n",
    "\n",
    "Если продолжать пример с фруктами, то задача обнаружения аномалий возникла бы, если бы мы предполагали, что среди яблок и груш может вдруг возникнуть мандарин, или любой другой фрукт, и нам бы нужно было не отнести его к одному из известных классов, а пометить как отдельный, отличающийся класс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://edunet.kea.su/repo/EduNet-content/L11/out/anomaly_fruits.png' width='800'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемой при работе с аномалиями является то, что аномальных значений может быть очень мало или вообще не быть. В таком случае алгоритм учит паттерны нормального поведения и реагирует на отличия от паттернов.\n",
    "\n",
    "Разберем примеры обнаружения аномалий с помощью трех алгоритмов из библиотеки Scikit-Learn (там можно найти еще много различных алгоритмов).\n",
    "\n",
    "Создадим датасет из двух кластеров и случайных значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Train\n",
    "x = 0.3 * rng.randn(100, 2)  # 100 2D points\n",
    "x_train = np.r_[x + 2, x - 2]  # split into two clusters\n",
    "\n",
    "# Test norlmal\n",
    "x = 0.3 * rng.randn(20, 2)  # 20 2D points\n",
    "x_test_norlmal = np.r_[x + 2, x - 2]  # split into two clusters\n",
    "\n",
    "# Test outliers\n",
    "x_test_outliers = rng.uniform(low=-4, high=4, size=(20, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию визуализации, которая будет изображать созданный датасет на рисунке слева, а результат поиска аномалий — на рисунке справа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(x_train, x_test_norlmal, x_test_outliers, model=None):\n",
    "    fig, (plt_data, plt_model) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    plt_data.set_title(\"Created Dataset (real labels)\")\n",
    "    plot_train = plt_data.scatter(\n",
    "        x_train[:, 0], x_train[:, 1], c=\"white\", s=40, edgecolor=\"k\"\n",
    "    )\n",
    "    plot_test_normal = plt_data.scatter(\n",
    "        x_test_norlmal[:, 0], x_test_norlmal[:, 1], c=\"green\", s=40, edgecolor=\"k\"\n",
    "    )\n",
    "    plot_test_outliers = plt_data.scatter(\n",
    "        x_test_outliers[:, 0], x_test_outliers[:, 1], c=\"red\", s=40, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    plt_data.set_xlim((-5, 5))\n",
    "    plt_data.set_ylim((-5, 5))\n",
    "\n",
    "    plt_data.legend(\n",
    "        [plot_train, plot_test_normal, plot_test_outliers],\n",
    "        [\"train\", \"test normal\", \"test outliers\"],\n",
    "        loc=\"lower right\",\n",
    "    )\n",
    "\n",
    "    if model:\n",
    "        plt_model.set_title(\"Model Results\")\n",
    "        # plot decision function\n",
    "        xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "        Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        plt_model.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "        # plot prediction\n",
    "        full_data = np.concatenate((x_train, x_test_norlmal, x_test_outliers), axis=0)\n",
    "        predicted = model.predict(full_data)\n",
    "\n",
    "        anom_index = np.where(predicted == -1)\n",
    "        anom_values = full_data[anom_index]\n",
    "\n",
    "        plot_all_data = plt_model.scatter(\n",
    "            full_data[:, 0], full_data[:, 1], c=\"white\", s=40, edgecolor=\"k\"\n",
    "        )\n",
    "\n",
    "        plot_anom_data = plt_model.scatter(\n",
    "            anom_values[:, 0], anom_values[:, 1], c=\"red\", s=40, marker=\"x\"\n",
    "        )\n",
    "        plt_model.legend(\n",
    "            [plot_all_data, plot_anom_data],\n",
    "            [\"normal\", \"outliers\"],\n",
    "            loc=\"lower right\",\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как работает на этих данных алгоритм [OneClassSVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html).\n",
    "\n",
    "Идея алгоритма состоит в поиске функции, которая положительна для областей с высокой плотностью и отрицательна для областей с малой плотностью. Подробнее об алгоритме можно прочитать в оригинальной [статье](https://proceedings.neurips.cc/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Paper.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "gamma = 2.0\n",
    "contamination = 0.05\n",
    "\n",
    "model = OneClassSVM(gamma=gamma, kernel=\"rbf\", nu=contamination)\n",
    "model.fit(x_train)\n",
    "\n",
    "plot_outliers(x_train, x_test_norlmal, x_test_outliers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как на этих же данных работает алгоритм [IsolationForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html). \n",
    "\n",
    "IsolationForest состоит из деревьев, которые «изолируют» (пытаются отделить от остальной выборки) наблюдения, случайным образом выбирая признак и случайное значение порога для этого признака (между max и min значениями признака). Такой алгоритм чаще и проще отделяет значения аномалии. Если построить по такому принципу множество деревьев, то значения, которые чаще других отделяются раньше, будут аномалиями.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "n_estimators = 200\n",
    "contamination = 0.05\n",
    "\n",
    "model = IsolationForest(\n",
    "    n_estimators=n_estimators, contamination=contamination, random_state=rng\n",
    ")\n",
    "model.fit(x_train)\n",
    "\n",
    "plot_outliers(x_train, x_test_norlmal, x_test_outliers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последним алгоритмом, на который мы посмотрим, будет [LocalOutlierFactor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html).\n",
    "\n",
    "В нем используется метод k-NN. Расстояние до ближайших соседей используется для оценки расположения точек. Если соседи далеко, то точка с большой вероятностью является аномалией. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "n_neighbors = 10\n",
    "contamination = 0.05\n",
    "\n",
    "model = LocalOutlierFactor(\n",
    "    n_neighbors=n_neighbors, novelty=True, contamination=contamination\n",
    ")\n",
    "model.fit(x_train)\n",
    "\n",
    "plot_outliers(x_train, x_test_norlmal, x_test_outliers, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики качества на несбалансированных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обращайте внимание на то, какие метрики вы используете. При решении задачи классификации часто используется accuracy (точность), равная доле правильно классифицированных объектов. Эта метрика позволяет адекватно оценить результат классификации в случае сбалансированных классов. В случае дисбаланса классов данная метрика может выдать обманчиво хороший результат.\n",
    "\n",
    "Пример: датасет, в котором 95% объектов относятся к классу 0 и 5% к классу 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "x, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.95],\n",
    "    flip_y=0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "counter = Counter(y)\n",
    "print(\"Class distribution \", Counter(y))\n",
    "\n",
    "for label, _ in counter.items():\n",
    "    row_ix = np.where(y == label)[0]\n",
    "    plt.scatter(x[row_ix, 0], x[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И модель, которая для всех данных выдает класс 0, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    def predict(self, x):\n",
    "        return np.zeros(x.shape[0])  # always predict class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая модель будет иметь $accuracy = 0.95$, хотя не выдает никакой полезной информации:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dummy_model = DummyModel()\n",
    "y_pred = dummy_model.predict(x)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для несбалансированных данных лучше выбирать [F1 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn.metrics.f1_score),  [MCC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html?highlight=matthews%20correlation%20coefficient#sklearn.metrics.matthews_corrcoef) (Matthews correlation coefficient, коэффициент корреляции Мэтьюса) или [balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) (среднее между recall разных классов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, matthews_corrcoef, balanced_accuracy_score\n",
    "\n",
    "print(\"F1\", f1_score(y, y_pred))\n",
    "print(\"MCC\", matthews_corrcoef(y, y_pred))\n",
    "print(\"Balanced accuracy\", balanced_accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эти метрики оказываются равны нулю, что отражает отсутствие связи предсказаний с данными на входе модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительно: [Your Dataset Is Imbalanced? Do Nothing!](https://towardsdatascience.com/your-dataset-is-imbalanced-do-nothing-abf6a0049813)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой способ побороть маленькое количество данных для обучения — аугментация.\n",
    "\n",
    "**Аугмента́ция** (от лат. augmentatio — увеличение, расширение) — увеличение выборки обучающих данных через модификацию существующих данных.\n",
    "\n",
    "Модели глубокого обучения обычно требуют большого количества данных для обучения. В целом, чем больше данных, тем лучше для обучения модели. В то же время получение огромных объемов данных сопряжено со своими проблемами (например, с нехваткой размеченных данных или с трудозатратами, сопряженными с разметкой).\n",
    "\n",
    "Вместо того, чтобы тратить дни на сбор данных вручную, мы можем использовать методы аугментации для автоматической генерации новых примеров из уже имеющихся.\n",
    "\n",
    "Помимо увеличения размеченных датасетов, многие методы *self-supervised learning* построены на использовании разных аугментаций одного и того же сэмпла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/augmentations_examples.png\" width=\"700\"></center>\n",
    "<center><em>Примеры аугментаций картинки. </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важный момент**: при обучении модели мы используем разбиение данных на `train-val-test`. Аугментации стоит применять только на `train`. Почему так? Конечная цель обучения нейросети — это применение на реальных данных, которые сеть не видела. Поэтому для адекватной оценки качества модели, валидационные и тестовые данные изменять не нужно.\n",
    "\n",
    "В любом случае, `test` должен быть отделен от данных еще до того, как они попали в `DataLoader` или нейросеть.\n",
    "\n",
    "Другое дело, что аугментации на тесте можно использовать как метод ансамблирования в случае классификации. Можно взять sample → создать несколько его копий → по-разному их аугментировать → предсказать класс на каждой из этих аугментированных копий → а потом выбрать наиболее вероятный класс голосованием (такой функционал реализован, например, в [YOLOv5](https://github.com/ultralytics/yolov5/blob/d204a61834d0f6b2e73c1f43facf32fbadb6b284/models/yolo.py#L121), о которой речь пойдет в следующих лекциях)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изображения\n",
    "\n",
    "Загрузим и отобразим пример картинки. Картинку отмасштабируем, чтобы она не занимала весь экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "# Compute on cpu or gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "URL = \"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/capybara_image.jpg\"\n",
    "!wget -q $URL -O test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "input_img = Image.open(\"/content/test.jpg\")\n",
    "input_img = transforms.Resize(size=300)(input_img)\n",
    "display(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций картинок. С полным списком можно ознакомиться на сайте [[doc] документации torchvision](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформация [`transforms.Random Rotation`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html?highlight=randomrotation#torchvision.transforms.RandomRotation) принимает параметр `degrees` — диапазон углов, из которого выбирается случайный угол для поворота изображения.\n",
    "\n",
    "Создадим переменную `transform`, в которую добавим нашу аугментацию, и применим ее к исходному изображению. Затем запустим следующую ячейку несколько раз подряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_augmented_img(transform, input_img):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    augmented_img = transform(input_img)\n",
    "    ax[0].imshow(input_img)\n",
    "    ax[0].set_title(\"Original img\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(augmented_img)\n",
    "    ax[1].set_title(\"Augmented img\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "transform = transforms.RandomRotation(degrees=(0, 180))\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Blur\n",
    "[`transforms.GaussianBlur`](https://pytorch.org/vision/main/generated/torchvision.transforms.GaussianBlur.html?highlight=gaussianblur#torchvision.transforms.GaussianBlur) размывает изображение с помощью фильтра Гаусса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Erasing\n",
    "[`transforms.RandomErasing`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomErasing.html?highlight=transforms+randomerasing#torchvision.transforms.RandomErasing) стирает на изображении произвольный прямоугольник. Она имеет параметр `p` — вероятность, с которой данная трансформация вообще применится к изображению.\n",
    "\n",
    "Данная трансформация работает только с `torch.Tensor`, поэтому предварительно нужно применить трансформацию `ToTensor`, а затем `ToPILImage`, чтобы воспользоваться нашей функцией для отображения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.RandomErasing(p=1), transforms.ToPILImage()]\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не лишним будет заметить, что некоторые трансформации могут существенно исказить изображение. Например, здесь, `RandomErasing` практически полностью стерла основной объект на снимке — капибару. Такая грубая аугментация может только навредить процессу обучения, и на практике нужно быть осторожным.\n",
    "\n",
    "`RandomErasing` также имеет параметр `scale` — диапазон соотношения стираемой области к входному изображению. Попробуем уменьшить этот диапазон относительно значения по умолчанию, чтобы избежать нежелательного эффекта стирания капибары."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=1, scale=(0.02, 0.1)),\n",
    "        transforms.ToPILImage(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColorJitter\n",
    "[`transforms.ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html?highlight=colorjitter#torchvision.transforms.ColorJitter) случайным образом меняет яркость, контрастность, насыщенность и оттенок изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ColorJitter(brightness=0.5, hue=0.3)\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещаем несколько аугментаций вместе\n",
    "\n",
    "Для этого будем использовать метод [`transforms.Compose`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html?highlight=compose#torchvision.transforms.Compose). Нам нужно будет создать `list` со всеми аугментациями, которые будут применены последовательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0),\n",
    "        transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещение нескольких аугментаций случайным образом\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font size=\"4\">Random Apply</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что если мы хотим применять аугментации случайным образом?\n",
    "\n",
    "Для этого мы можем воспользоваться методом [`transforms.RandomApply`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomApply.html?highlight=randomapply#torchvision.transforms.RandomApply), который на вход принимает список аугментаций и вероятность `p`, с которой каждая аугментация будет применена или не применена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomApply(\n",
    "    transforms=[\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5),\n",
    "        transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "    ],\n",
    "    p=0.9,\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font size=\"4\">Random Choice</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В других случаях может быть полезен метод [`transforms.RandomChoice`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomChoice.html?highlight=randomchoice#torchvision.transforms.RandomChoice), который на вход принимает список аугментаций `transforms`, выбирает из него **одну** случайную аугментацию и применяет ее к изображению. Необязательным параметром является список вероятностей `p`, который укзаывает, с какой вероятностью каждая из аугментаций может быть выбрана из списка (по умолчанию каждая может быть выбрана равновероятно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomChoice(\n",
    "    transforms=[\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0),\n",
    "        transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "    ],\n",
    "    p=[0.2, 0.4, 0.6],\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)\n",
    "plot_augmented_img(transform, input_img)\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример создания собственной аугментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда может оказаться, что среди широкого спектра реализованных аугментаций нет такой, какую вы хотели бы применить к своим данным. В таком случае ее можно описать в виде класса и использовать наравне с реализованными в библиотеке.\n",
    "\n",
    "Главное, что необходимо описать при создании класса — метод `__call__`. Он должен принимать изображение (оно может быть представлено в формате `PIL.Image`, `np.array` или `torch.Tensor`), делать с ним интересующие нас видоизменения и возвращать измененное изображение.\n",
    "\n",
    "Рассмотрим пример добавления на изображение [шума \"соль и перец\"](https://ru.wikipedia.org/wiki/Salt_and_pepper). Наш метод аугментации будет и принимать на вход, и возвращать `PIL.Image`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SaltAndPepperNoise:\n",
    "    \"\"\"\n",
    "    Add a \"salt and pepper\" noise to the PIL image\n",
    "    __call__ method returns PIL Image with noise\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.01):\n",
    "        self.p = p  # noise level\n",
    "\n",
    "    def __call__(self, pil_image):\n",
    "        np_image = np.array(pil_image)\n",
    "\n",
    "        # create random mask for \"salt\" and \"pepper\" pixels\n",
    "        salt_ind = np.random.choice(\n",
    "            a=[True, False], size=np_image.shape[:2], p=[self.p, 1 - self.p]\n",
    "        )\n",
    "        pepper_ind = np.random.choice(\n",
    "            a=[True, False], size=np_image.shape[:2], p=[self.p, 1 - self.p]\n",
    "        )\n",
    "\n",
    "        # add \"salt\" and \"pepper\"\n",
    "        np_image[salt_ind] = 255\n",
    "        np_image[pepper_ind] = 0\n",
    "\n",
    "        return Image.fromarray(np_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SaltAndPepperNoise(p=0.03)\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация внутри `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем папку с картинками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "# download files\n",
    "!wget --no-check-certificate 'https://edunet.kea.su/repo/EduNet-web_dependencies/L11/for_transforms.Compose.zip' -O data.zip\n",
    "with ZipFile(\n",
    "    \"data.zip\", \"r\"\n",
    ") as folder:  # Create a ZipFile Object and load sample.zip in it\n",
    "    folder.extractall()  # Extract all the contents of zip file in current directory\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/content/for_transforms.Compose\")\n",
    "img_list = os.listdir()\n",
    "print(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, img_list, transforms=None):\n",
    "        self.img_list = img_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = plt.imread(self.img_list[i])\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "        img = np.array(img).astype(np.uint8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вспомогательную функцию для отображения картинок. Напомним, что в PyTorch размерность каналов идет в первом, а не в последнем измерении тензора, описывающего картинку: `Channels x Height x Width`. Для отображения при помощи Matplotlib необходимо перевести массив в формат `Height x Width x Channels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.figure(figsize=(40, 38))\n",
    "    img_np = img.numpy()\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))  # [CxHxW] -> [HxWxC] for imshow\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `list` с аугментациями, которые мы хотим применить. Чтобы загрузить аугментации в PyTorch, нам необходимо эти картинки преобразовать в тензоры. Для этого воспользуемся стандартным преобразованием `transforms.ToTensor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обернем все в `DataLoader` и отобразим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, tensor_transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(next(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нестандартные способы аугментации\n",
    "\n",
    "Существуют и более сложные способы аугментации. Ниже приведена пара примеров таких способов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font size=\"4\">Mixup</font>\n",
    "\n",
    "Mixup — это \"смешение\" признаков двух объектов в определенных пропорциях. Mixup можно представить с помощью простого уравнения:\n",
    "\n",
    "$\\text{New image} = \\alpha * \\text{image}_1 + (1-\\alpha) * \\text{image}_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/mixup_augmentation_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее в статьях:\n",
    "\n",
    "[mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)\n",
    "\n",
    "[On Mixup Training](https://proceedings.neurips.cc/paper/2019/file/36ad8b5f42db492827016448975cc22d-Paper.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font size=\"4\">Аугментация при помощи генерации данных</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ряде случаев возможно расширение набора данных путем синтеза новых данных.\n",
    "\n",
    "Например, при создании системы распознавания текста, можно генерировать новые образцы путем набора распознаваемых фраз или символов различными шрифтами на различных фонах и с добавлением каких-либо шумов и искажений.\n",
    "\n",
    "В ряде областей для синтеза новых образов могут создаваться 3D-модели распознаваемых объектов. Например, в работе от Microsoft [Fake It Till You Make It: Face analysis in the wild using synthetic data alone](https://microsoft.github.io/FaceSynthetics/) анализ лиц людей производился на синтетических 3D-моделях лиц. Датасет доступен на [GitHub](https://github.com/microsoft/FaceSynthetics).\n",
    "\n",
    "Также созданием новых образов, похожих на имеющиеся в датасете, можно заниматься при помощи генеративных моделей. Примером генеративных моделей является [GAN](https://ru.wikipedia.org/wiki/Генеративно-состязательная_сеть) (Generative Adversarial Network). Мы познакомимся с такими моделями в одной из следующих лекций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L11/augmentation_using_data_synthesis.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация в реальных задачах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме методов, реализованных в PyTorch, существуют и специализированные библиотеки для аугментации изображений, в которых реализованы дополнительные возможности (например, наложение теней, бликов или пятен воды на изображение).\n",
    "\n",
    "Например:\n",
    "- [Albumentations](https://albumentations.ai)\n",
    "- [imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "- [AugLy](https://github.com/facebookresearch/AugLy)\n",
    "\n",
    "**Важно: при выборе методов аугментации имеет смысл использовать только те, которые будут в реальной жизни.**\n",
    "\n",
    "Например, нет смысла:\n",
    "- делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным\n",
    "- отражать человека вверх ногами, если мы не предполагаем его таким распознавать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций аудио. С полным списком можно ознакомиться здесь: [[git] audiomentations](https://github.com/iver56/audiomentations).\n",
    "\n",
    "Импортируем библиотеку и посмотрим на пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/content\")\n",
    "\n",
    "!pip install audiomentations\n",
    "\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L11/audio_example.wav\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Get input audio\n",
    "input_audio = \"/content/audio_example.wav\"\n",
    "\n",
    "display(Audio(input_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "data, sr = librosa.load(\"/content/audio_example.wav\")  # sr - sampling rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background Noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import AddGaussianSNR\n",
    "\n",
    "augment = AddGaussianSNR(min_snr_in_db=3, max_snr_in_db=7, p=1)\n",
    "\n",
    "# Augment/transform the audio data\n",
    "augmented_data = augment(samples=data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним волновые картины и спектрограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "\n",
    "\n",
    "def produce_plots(input_audio_arr, aug_audio, sr):\n",
    "    f, t, Sxx_in = spectrogram(\n",
    "        input_audio_arr, fs=sr\n",
    "    )  # Compute spectrogram for the original signal (f - frequency, t - time)\n",
    "    f, t, Sxx_aug = spectrogram(aug_audio, fs=sr)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 5))\n",
    "\n",
    "    ax[0, 0].plot(input_audio_arr)\n",
    "    ax[0, 0].set_xlim(0, len(input_audio_arr))\n",
    "    ax[0, 0].set_xticks([])\n",
    "    ax[0, 0].set_title(\"Original audio\")\n",
    "\n",
    "    ax[0, 1].plot(aug_audio)\n",
    "    ax[0, 1].set_xlim(0, len(input_audio_arr))\n",
    "    ax[0, 1].set_xticks([])\n",
    "    ax[0, 1].set_title(\"Augmented  audio\")\n",
    "\n",
    "    ax[1, 0].imshow(\n",
    "        np.log(Sxx_in),\n",
    "        extent=[t.min(), t.max(), f.min(), f.max()],\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"inferno\",\n",
    "    )\n",
    "    ax[1, 0].set_ylabel(\"Frequecny, Hz\")\n",
    "    ax[1, 0].set_xlabel(\"Time,s\")\n",
    "\n",
    "    ax[1, 1].imshow(\n",
    "        np.log(Sxx_aug, where=Sxx_aug > 0),\n",
    "        extent=[t.min(), t.max(), f.min(), f.max()],\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"inferno\",\n",
    "    )\n",
    "    ax[1, 1].set_ylabel(\"Frequecny, Hz\")\n",
    "    ax[1, 1].set_xlabel(\"Time,s\")\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import TimeStretch\n",
    "\n",
    "augment = TimeStretch(min_rate=0.8, max_rate=1.5, p=1)\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pitch Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение тональности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import PitchShift\n",
    "\n",
    "augment = PitchShift(min_semitones=1, max_semitones=12, p=1)\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае с картинками, мы можем совмещать несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, Shift\n",
    "\n",
    "augment = Compose(\n",
    "    [\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1),\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.25, p=1),\n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=1),\n",
    "        Shift(min_fraction=-0.5, max_fraction=0.5, p=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "augmented_data = augment(data, sample_rate=sr)\n",
    "\n",
    "display(Audio(augmented_data, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(data, augmented_data, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные библиотеки для аугментации звука (и волновых функций в целом):\n",
    "- [torchaudio](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)\n",
    "- [torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations)\n",
    "- [AugLy](https://github.com/facebookresearch/AugLy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим несколько примеров аугментаций текста. С полным списком можно ознакомиться здесь: [[git] библиотеки](https://github.com/makcedward/nlpaug)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input text\n",
    "text = \"Hello, future of AI for Science! How are you today?\"\n",
    "print(f\"input text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация символов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменой на похоже выглядящие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "\n",
    "augment = nac.OcrAug()\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С опечатками, которые учитывают расположение символов на клавиатуре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = nac.KeyboardAug()\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С орфографическими ошибками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "augment = naw.SpellingAug()\n",
    "augmented_text = augment.augment(text, n=3)\n",
    "\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С использованием модели для предсказания новых слов в зависимости от контекста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# model_type: word2vec, glove or fasttext\n",
    "augment = naw.ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"insert\")\n",
    "augmented_text = augment.augment(text)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация предложений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем перевести текстовые данные на какой-либо язык, а затем перевести их обратно на язык оригинала. Это может помочь сгенерировать текстовые данные с разными словами, сохраняя при этом контекст текстовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name=\"facebook/wmt19-en-de\", to_model_name=\"facebook/wmt19-de-en\"\n",
    ")\n",
    "augmented_text = back_translation_aug.augment(text)\n",
    "\n",
    "clear_output()\n",
    "print(f\"Original:\\n{text}\")\n",
    "print(f\"Augmented Texts:\\n{augmented_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instance Crossover Augmentation* — составление новых объектов класса из отдельных предложений того же класса. Например, есть два объекта одного класса \"положительный отзыв\":\n",
    "- <font color='red'>очень удобное приложение. Мне понравилось им пользоваться</font>\n",
    "- <font color='green'>класс! Отличный интерфейс</font>\n",
    "\n",
    "Тогда можно составить новый объект того же класса из их частей:\n",
    "- <font color='red'>очень удобное приложение!</font> <font color='green'>Отличный интерфейс</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно: при любых аугментациях текста на уровне предложений есть шанс создать странные и нелогичные объекты, поэтому использовать их следует с особой осторожностью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные  библиотеки для аугментации текста:\n",
    "\n",
    "- [TextAugment](https://github.com/dsfsi/textaugment)\n",
    "- [AugLy](https://github.com/facebookresearch/AugLy)\n",
    "\n",
    "[Обзор методов аугментации текста с примерами](https://amitness.com/2020/05/data-augmentation-for-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обучить нейросеть на своих данных, когда их мало?\n",
    "\n",
    "Для таких типовых задач, как классификация изображений, можно воспользоваться одной из существующих архитектур (AlexNet, VGG, Inception, ResNet и т.д.) и просто обучить нейросеть на своих данных. Реализации таких сетей с помощью различных фреймворков уже существуют, так что на данном этапе можно использовать одну из них как черный ящик, не вникая в принцип её работы. Например, в PyTorch есть много уже реализованных известных архитектур: [torchvision.models](https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Однако, глубокие нейронные сети требуют больших объемов данных для успешного обучения. И, зачастую, в нашей частной задаче недостаточно данных для того, чтобы хорошо обучить нейросеть с нуля. **Transfer learning** решает эту проблему. По сути мы пытаемся использовать опыт, полученный нейронной сетью при обучении на некоторой задаче $T_1$, чтобы решать схожую задачу $T_2$.\n",
    "\n",
    "К примеру, transfer learning можно использовать при решении задачи классификации изображений на небольшом наборе данных. Как уже ранее обсуждалось, при обработке изображений свёрточные нейронные сети в первых слоях \"реагируют\" на некие простые пространственные шаблоны (к примеру, углы), после чего комбинируют их в сложные осмысленные формы (к примеру, глаза или носы). Вся эта информация извлекается из изображения, на её основе создаются сложные представления данных, которые в результате классифицируются линейной моделью. \n",
    "\n",
    "Идея заключается в том, что если изначально обучить модель на некоторой сложной и довольно общей задаче, то можно надеяться, что она (как минимум часть ее слоев), в общем случае, будет извлекать важную информацию из изображений, и полученные представления можно будет успешно использовать для классификации линейной моделью.\n",
    "\n",
    "Таким образом, берем часть модели, которая, по нашему представлению, отвечает за выделение хороших признаков (часто — все слои, кроме последнего) — feature extractor. Присоединяем к этой части один или несколько дополнительных слоёв для решения уже новой задачи. И учим только эти слои. Cлои feature extractor не учим — они \"заморожены\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/transfer_learning_change_classes_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятно, что не все фильтры модели будут использованы максимально эффективно — к примеру, если мы работаем с изображениями, связанными с едой, возможно, не все фильтры на скрытых слоях предобученной на ImageNet модели окажутся полезны для нашей задачи. Почему бы не попробовать **не только обучить новый классификатор, но и дообучить некоторые промежуточные слои**? При использовании этого подхода мы при обучении дополнительно \"настраиваем\" и промежуточные слои, называется он **fine-tuning**.\n",
    "\n",
    "При fine-tuning используют меньший learning rate, чем при обучении нейросети с нуля: мы знаем, что по крайней мере часть весов нейросети выполняет свою задачу хорошо, и не хотим испортить это быстрыми изменениями. \n",
    "\n",
    "Кроме этого, можно делать комбинации этих методов: сначала учить только последние добавленные нами слои сети, затем учить еще и самые близкие к ним, и после этого учить уже все веса нейросети вместе. То есть мы можем определить свою **стратегию fine-tuning**. \n",
    "\n",
    "Иногда fine-tuning считается синонимом Transfer learning, в этом случае часть от предтренированной сети называют **backbone** (\"позвоночник\"), а добавленную часть — **head** (\"голова\").\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Порядок действий при transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательно рассмотрим шаги, необходимые для реализации подхода transfer learning.\n",
    "\n",
    "**Шаг 1. Получение предварительно обученной модели**\n",
    "\n",
    "Первым шагом является выбор предварительно обученной модели, которую мы хотели бы использовать в качестве основы для обучения. Основным предположением является то, что признаки, которые умеет выделять из данных предобученная модель, хорошо подойдут для решения нашей частной задачи. Поэтому эффект от Transfer learning будет тем лучше, чем более схожими будут **домены** в нашей задаче и в задаче, на которой предварительно обучалась модель.\n",
    "\n",
    "Для задач обработки изображений очень часто используются модели, предобученные на ImageNet. Такой подход распространен, однако, если ваша задача связана, например, с обработкой снимков клеток под микроскопом, то модель, предобученная на более близком домене (тоже на снимках клеток, пусть и совсем других), может быть лучшим начальным решением. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/transfer_learning_step_1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 2. Заморозка предобученных слоев**\n",
    "\n",
    "Мы предполагаем, что первые слои модели уже хорошо натренированы выделять какие-то абстрактные признаки из данных. Поэтому мы не хотим \"сломать\" их, особенно если начнем на этих признаках обучать новые слои, которые инициализируются случайно: на первых шагах обучения ошибка будет большой и мы можем сильно изменить \"хорошие\" предобученные веса.\n",
    "\n",
    "Поэтому требуется \"заморозить\" предобученные веса. На практике заморозка означает **отключение подсчета градиентов**. Таким образом при последующем обучении параметры с отключенным подсчетом градиентов не будут обновляться.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/transfer_learning_step_2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 3. Добавление новых обучаемых слоев**\n",
    "\n",
    "В отличие от начальных слоев, которые выделяют достаточно общие признаки из данных, более близкие к выходу слои предобученной модели сильно специфичны конкретно под ту задачу, на которую она обучалась. Для моделей, предобученных на ImageNet, последний слой заточен конкретно под предсказание 1000 классов из этого набора данных. Кроме этого, последние слои могут не подходить под новую задачу архитектурно: в новой задаче может быть меньше классов, 10 вместо 1000. Поэтому, требуется **заменить последние один или несколько слоев** предобученной модели на новые, подходящие под нашу задачу. При этом, естественно, веса в этих слоях будут инициализированы случайно. Именно эти слои мы и будем обучать на следующем шаге.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/transfer_learning_step_3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 4. Обучение новых слоев**\n",
    "\n",
    "Все, что нам теперь нужно — обучить новые слои на наших данных. При этом замороженные слои используются лишь как **экстрактор высокоуровневых признаков**.\n",
    "Обучение такой модели существенно ничем не отличается от обучения любой другой модели: используется обучающая и валидационная выборка, контролируется изменение функции потерь и функционала качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/transfer_learning_step_4.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 5. Тонкая настройка модели (fine-tuning)**\n",
    "\n",
    "После того, как мы обучили новые слои модели, и они уже как-то решают задачу, мы можем разморозить ранее замороженные веса, чтобы **тонко настроить** их под нашу задачу, в надежде, что это позволит еще немного повысить качество.\n",
    "\n",
    "Нужно быть осторожным на этом этапе, использовать learning rate на порядок или два меньший, чем при основном обучении, и одновременно с этим следить за возникновением переобучения. Переобучение при fine-tuning может возникать из-за того, что мы резко увеличиваем количество настраиваемых параметров модели, но при этом наш датасет остается небольшим, и мощная модель может начать заучивать обучающие данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/transfer_learning_step_5.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практический пример transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим пример практической реализации такого подхода ([код переработан из этой статьи](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/)).\n",
    "\n",
    "Загрузим датасет EuroSAT и удалим из него 90% файлов. EuroSAT — датасет для классификации спутниковых снимков по типам местности: лес, река, жилая застройка и т. п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "!wget -N https://edunet.kea.su/repo/EduNet-web_dependencies/L11/EuroSAT.zip # http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
    "!unzip -n EuroSAT.zip\n",
    "\n",
    "os.chdir(\"/content\")\n",
    "path = \"/content/2750/\"\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "    files = os.listdir(path + folder)\n",
    "    for file in sample(files, int(len(files) * 0.9)):\n",
    "        os.remove(path + folder + \"/\" + file)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим аугментации. Для примера будем использовать родные аугментации из библиотеки Torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Applying Transforms to the Data\n",
    "img_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=224),  # as in ImageNet\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # No augmentations on valid data!\n",
    "    \"valid\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=224),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # No augmentations on test data!\n",
    "    \"test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=224),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from copy import deepcopy\n",
    "\n",
    "dataset = datasets.ImageFolder(root=path)\n",
    "# split to train/valid/test\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(\n",
    "    dataset, [int(len(dataset) * 0.8), int(len(dataset) * 0.1), int(len(dataset) * 0.1)]\n",
    ")\n",
    "\n",
    "train_set.dataset = deepcopy(dataset)\n",
    "valid_set.dataset = deepcopy(dataset)\n",
    "test_set.dataset = deepcopy(dataset)\n",
    "\n",
    "# define augmentations\n",
    "train_set.dataset.transform = img_transforms[\"train\"]\n",
    "valid_set.dataset.transform = img_transforms[\"valid\"]\n",
    "test_set.dataset.transform = img_transforms[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_set)}\")\n",
    "print(f\"Valid size: {len(valid_set)}\")\n",
    "print(f\"Test size: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size, valid_data_size = len(train_set), len(valid_set)\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "print(\"indexes to class: \")\n",
    "idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наборе данных не так уж и много изображений. При обучении с нуля нейросеть скорее всего не достигнет высокой точности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение готовой архитектуры \"с нуля\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим MobileNet v2 без весов и попробуем обучить \"с нуля\", то есть с весов, инициализированных случайно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.mobilenet_v2(weights=None)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний слой MobileNet дает на выходе предсказания для 1000 классов, а в нашем датасете классов всего 10. Поэтому мы должны изменить выход сети так, чтобы он выдавал 10 предсказаний. Поэтому мы заменяем последний слой модели MobileNet слоем с `num_classes` нейронами, равным числу классов в нашем датасете.\n",
    "\n",
    "То есть мы \"сказали\" нашей модели распознавать не 1000, а только `num_classes` классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of MobileNet Model for Transfer Learning\n",
    "import torch.nn as nn\n",
    "\n",
    "# change out classes, from 1000 to 10\n",
    "model.classifier[1] = nn.Linear(1280, num_classes)  \n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы определяем функцию потерь и оптимизатор, которые будут использоваться для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренировки и валидации нашей модели напишем отдельную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_and_validate(model, criterion, optimizer, num_epochs=25, save_state=False):\n",
    "    \"\"\"\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "\n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clean existing gradients\n",
    "            outputs = model(\n",
    "                inputs\n",
    "            )  # Forward pass - compute outputs on input data using the model\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the parameters\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Compute correct predictions\n",
    "            train_correct += (torch.argmax(outputs, dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Compute the mean train accuracy\n",
    "        train_accuracy = 100 * train_correct / (len(train_loader) * batch_size)\n",
    "\n",
    "        val_correct = 0\n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "            model.eval()  # Set to evaluation mode\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    inputs\n",
    "                )  # Forward pass - compute outputs on input data using the model\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                valid_loss += loss.item() * inputs.size(\n",
    "                    0\n",
    "                )  # Compute the total loss for the batch and add it to valid_loss\n",
    "\n",
    "                val_correct += (torch.argmax(outputs, dim=-1) == labels).float().sum()\n",
    "\n",
    "        # Compute mean val accuracy\n",
    "        val_accuracy = 100 * val_correct / (len(valid_loader) * batch_size)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss / (len(train_loader) * batch_size)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss / (len(valid_loader) * batch_size)\n",
    "\n",
    "        history.append(\n",
    "            [\n",
    "                avg_train_loss,\n",
    "                avg_valid_loss,\n",
    "                train_accuracy.detach().cpu(),\n",
    "                val_accuracy.detach().cpu(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(\n",
    "            \"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
    "                epoch + 1,\n",
    "                avg_train_loss,\n",
    "                train_accuracy.detach().cpu(),\n",
    "                avg_valid_loss,\n",
    "                val_accuracy.detach().cpu(),\n",
    "                epoch_end - epoch_start,\n",
    "            )\n",
    "        )\n",
    "        # Saving state for fine_tuning (because we may overfit)\n",
    "        if save_state:\n",
    "            os.makedirs(\"check_points\", exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"check_points/fine_tuning_{epoch + 1}.pth\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим нашу модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    model.to(device), criterion, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_fresh.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "fig.suptitle(\"Fresh learning\", fontsize=14)\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:, :2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:, 2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность на валидационной выборке не превысила 66%. Посмотрим, сможем ли мы добиться большей точности при использовании предобученной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение готовой архитектуры с предобученными весами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение классификационной \"головы\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем использовать transfer learning. \n",
    "\n",
    "Загрузим **предобученную на ImageNet** модель MobileNet v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = models.mobilenet_v2(weights=\"MobileNet_V2_Weights.DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае мы не дообучаем скрытые слои нашей модели, поэтому отключаем подсчёт градиентов (\"**замораживаем**\" параметры)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам снова нужно изменить выход сети так, чтобы он выдавал 10 классов вместо 1000.\n",
    "\n",
    "Мы могли бы изменить количество выходов сети, просто подменив последний линейный слой, как в примере обучения с нуля:\n",
    "\n",
    "`model.classifier[1] = nn.Linear(1280, num_classes)`\n",
    "\n",
    "Но нужно понимать, что **мы не ограничены архитектурой готовой сети**, и можем как подменять слои, так и добавлять новые. Поэтому в целях демонстрации мы заменим выходной слой исходной сети на два слоя: первый мы добавим \"подменой\" модуля, а затем добавим активацию и новый выходной слой с `num_classes` выходами с помощью метода `add_module()` класса `Sequential`.\n",
    "\n",
    "Когда мы подменяем или добавляем слои, по умолчанию подсчет градиентов на них будет включен, и таким образом мы добьемся, что **учиться будут только новые слои**, веса которых инициализируются случайно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layers of MobileNet Model for Transfer Learning\n",
    "\n",
    "model.classifier[1] = nn.Linear(1280, 500)  # replace last module to our custom, e.g. with 500 neurons\n",
    "model.classifier.add_module(\"2\", nn.ReLU())  # add activation\n",
    "model.classifier.add_module(\"3\", nn.Linear(500, num_classes))  # add new output layer with 10  out classes\n",
    "\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    model.to(device), criterion, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_transfer_learning.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "fig.suptitle(\"Transfer learning\", fontsize=14)\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:, :2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:, 2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним между собой обучение с нуля и обучение с предобученными весами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "fig.suptitle(\"Fresh Learning (FL) vs Transfer Learning (TL)\", fontsize=14)\n",
    "\n",
    "history_fresh = np.array(torch.load(\"history_fresh.pt\"))\n",
    "history_transfer_learning = np.array(torch.load(\"history_transfer_learning.pt\"))\n",
    "\n",
    "ax[0].plot(history_fresh[:, :2], linestyle='--')\n",
    "ax[0].set_prop_cycle('color', ['tab:blue', 'tab:orange'])\n",
    "ax[0].plot(history_transfer_learning[:, :2])\n",
    "ax[0].legend([\"Train Loss (FL)\", \"Val Loss (FL)\", \"Train Loss (TL)\", \"Val Loss (TL)\"])\n",
    "\n",
    "ax[1].plot(history_fresh[:, 2:], linestyle='--')\n",
    "ax[1].set_prop_cycle('color', ['tab:blue', 'tab:orange'])\n",
    "ax[1].plot(history_transfer_learning[:, 2:])\n",
    "ax[1].legend(\n",
    "    [\n",
    "        \"Train Accuracy (FL)\",\n",
    "        \"Val Accuracy (FL)\",\n",
    "        \"Train Accuracy (TL)\",\n",
    "        \"Val Accuracy (TL)\",\n",
    "    ]\n",
    ")\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начная с предобученных весов, процесс обучения идет более плавно и модель выдает бо́льшую точность. На валидационной выборке мы получили точность около 74%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дообучение всех слоев (Fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сможем ли мы еще немного повысить точность путем тонкой донастройки всех всесов сети.\n",
    "\n",
    "Проведём процедуру **fine-tuning**. \n",
    "В предыдущем варианте с transfer learning обучался только последний слой, добавленный вручную. Давайте проверим это, выведя те слои, в которых включён градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы оставим дообученную голову нейронной сети и продолжим **обучение всей сети с уменьшением темпа обучения**. \n",
    "\n",
    "**Разморозим** параметры. `criterion` остаётся тот же, в `optimizer` уменьшим параметр `lr` на порядок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пройдём дополнительные 20 эпох и построим графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    model.to(device), criterion, optimizer, num_epochs, save_state=True\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_finetuning.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(16, 5))\n",
    "fig.suptitle(\"Transfer Learning (TL) AND Finetuning (FT)\", fontsize=14)\n",
    "\n",
    "history_transfer_learning = np.array(torch.load(\"history_transfer_learning.pt\"))\n",
    "history_finetuning = np.array(torch.load(\"history_finetuning.pt\"))\n",
    "\n",
    "train_val_loss = np.concatenate(\n",
    "    (history_transfer_learning[:, :2], history_finetuning[:, :2]), axis=0\n",
    ")\n",
    "ax[0].plot(train_val_loss)\n",
    "ax[0].vlines(19, -0.1, 2.1, color=\"tab:green\", linewidth=2, linestyle=\"--\")\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\", \"TL/FT boundary\"])\n",
    "\n",
    "train_val_acc = np.concatenate(\n",
    "    (history_transfer_learning[:, 2:], history_finetuning[:, 2:]), axis=0\n",
    ")\n",
    "\n",
    "ax[1].plot(train_val_acc)\n",
    "ax[1].vlines(19, -5, 105, color=\"tab:green\", linewidth=2, linestyle=\"--\")\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\", \"TL/FT boundary\"])\n",
    "\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли эффект от **fine-tuning**? После дообучения ещё на 20 эпохах мы наблюдаем следующие эффекты:\n",
    "\n",
    "\n",
    "*   Loss дополнительно снизился, хотя до fine-tuning он стремился к выходу на плато\n",
    "*   точность на валидации превысила 80%, то есть мы получили дополнительно около 6% точности.\n",
    "\n",
    "При fine-tuning модель может быть склонна к переобучению, так как мы обучаем сложную модель с большим числом параметров на небольшом количестве данных. Поэтому мы используем learning rate на порядок меньший, чем при обычном обучении. Для контроля переобучения следует следить за метриками и ошибкой на валидационной выборке.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшее качество на валидационных данных мы получили на 38 эпохе. При fine-tuning мы сохраняли состояния нейросети на каждой эпохе. Возьмём состояние с 38 эпохи как наиболее оптимальное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.load_state_dict(torch.load(\"check_points/fine_tuning_18.pth\"))  # 38 = 20 (TL) + 18 (FT)\n",
    "trained_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_img_name, device):\n",
    "    \"\"\"\n",
    "    Function to predict the class of a single test image\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param test_img_name: Test image\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    transform = img_transforms[\"test\"]\n",
    "    test_img = torch.tensor(np.asarray(test_img_name))\n",
    "    test_img = transforms.ToPILImage()(test_img)\n",
    "    plt.imshow(test_img)\n",
    "\n",
    "    test_img_tensor = test_img_name.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs is logits\n",
    "        out = model(test_img_tensor).to(device)\n",
    "        probs = torch.softmax(out, dim=1).to(device)\n",
    "        topk, topclass = probs.topk(3, dim=1)\n",
    "        for i in range(3):\n",
    "            print(\n",
    "                \"Predcition\",\n",
    "                i + 1,\n",
    "                \":\",\n",
    "                idx_to_class[topclass.cpu().numpy()[0][i]],\n",
    "                \", Score: \",\n",
    "                round(topk.cpu().numpy()[0][i], 2),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[0])\n",
    "predict(\n",
    "    trained_model.to(device),\n",
    "    test_set[np.where([x[1] == 0 for x in test_set])[0][0]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[6])\n",
    "predict(\n",
    "    trained_model,\n",
    "    test_set[np.where([x[1] == 6 for x in test_set])[0][0]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[8])\n",
    "predict(\n",
    "    trained_model,\n",
    "    test_set[np.where([x[1] == 8 for x in test_set])[0][0]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Мы увидели, как использовать предварительно обученную модель на 1000 классов ImageNet для нашей задачи на 10 классов.\n",
    "\n",
    "* Мы сравнили качество **обучения с нуля, transfer learning и fine-tuning** и научились добиваться максимального качества с помощью этих принципов.\n",
    "\n",
    "\n",
    "На практике не забывайте о характерной **опасности fine-tuning — переобучении**. Используйте **низкий learning rate** и отслеживайте Loss и показатели качества — возможно, вам будет достаточно **небольшого количества эпох**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют задачи, где не представляется возможным разбить данные на классы так, чтобы в каждом классе было достаточно много объектов.\n",
    "\n",
    "Рассмотрим, например, задачу распознавания лиц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/videoanalytics.png\"  width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход системе подается фото лица человека. Требуется сопоставить его с другим изображением или изображениями, например, хранящимися в БД, и таким образом идентифицировать человека на фотографии.\n",
    "\n",
    "На первый взгляд кажется, что это задача классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/classifier_scheme.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все изображения одного человека будем считать относящимися к одному классу, и модель будет этот класс предсказывать. \n",
    "\n",
    "Для небольшой организации, в которой всего несколько десятков сотрудников такой подход может сработать.\n",
    "При этом возникнут проблемы:\n",
    "\n",
    "1. Чтобы обучить такую ​​систему, нам сначала потребуется много (сотни) разных \n",
    "изображений каждого сотрудника.\n",
    "\n",
    "2. Когда человек присоединяется к организации или покидает ее, приходится менять структуру модели и обучать ее заново.\n",
    "\n",
    "\n",
    "Это практически невозможно для крупных организаций, где набор и увольнение происходит почти каждую неделю. И в принципе невозможно для города масштаба Москвы или Лондона, в котором миллионы жителей и сотни тысяч приезжих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование векторов-признаков (embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому используется другой подход.\n",
    "Вместо того, чтобы классифицировать изображения, модель учится выделять ключевые признаки и на их основе строить компактный вектор, достаточно точно описывающий лицо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/face_as_embedding.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В англоязычной литературе такие вектора признаков называются **embedding**, и мы тоже будем использовать это обозначение.\n",
    "\n",
    "Может возникнуть вопрос: не потеряем ли мы важную информацию, сжав изображение в несколько сотен чисел? \n",
    "\n",
    "Чтобы ответить на него, вспомним, как работает [фоторобот](https://en.wikipedia.org/wiki/Facial_composite). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/photorobot.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения фотореалистичного изображения лица достаточно нескольких ключевых признаков: глаза, волосы, рот, нос...\n",
    "Каждый из них кодируется максимум несколькими сотнями целых значений. \n",
    "\n",
    "Значит, вектора-признака из 128 вещественных чисел будет более чем достаточно.\n",
    "Правда интерпретировать значения, которые закодирует в него нейросеть, будет не столь просто.\n",
    "\n",
    "Если нам удастся обучить модель кодировать в embedding признаки, важные для сравнения, то мы сможем сравнивать векторы между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/face_dist.png\"  width=\"700\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если расстояние между векторами для лиц, которые похожи друг на друга, будут маленькими, а у непохожих, наоборот, большими, то мы сможем экспериментально подобрать порог $d$ и, сравнивая с ним расстояние между двумя векторами, принимать решение: принадлежат ли они одному человеку или нет.\n",
    "\n",
    "*Можно оценивать не расстояние, а степень схожести similarity. В этом случае неравенства поменяют знак, но логика останется прежней*\n",
    "\n",
    "Теперь, чтобы идентифицировать человека, требуется только одно изображение его лица. Эмбеддинг  этого изображения можно сравнить с эмбеддингами других лиц из БД, используя  [k-NN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) или иной метод кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/EduNet-content/L11/out/search_in_embedding_space.jpg\"  width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая модель не учится классифицировать изображение напрямую по какому-либо из выходных классов. Она учится выделять признаки, важные при сравнении.\n",
    "\n",
    "Такой подход решает обе проблемы, о которых мы говорили выше:\n",
    "- Для обучения такой сети нам не требуется много экземпляров объектов одного класса, а   достаточно лишь нескольких.\n",
    "- Но самое большое преимущество в простоте ее обучения в случае появления новых объектов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сиамская сеть (Siamese Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какая архитектура должна быть у модели, генерирующей векторы признаков?\n",
    "\n",
    "Можно было бы использовать обычную сеть, обученную для задачи классификации, и затем удалить из нее последний(е) слой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/embedding_from_classifier.png\"  width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Активации последнего слоя представляют собой отклики на некие высокоуровневые признаки, потенциально важные для классификации, и их можно интерпретировать как embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet\n",
    "import torch\n",
    "\n",
    "face1 = torch.randn((3, 224, 224))\n",
    "face2 = torch.randn((3, 224, 224))\n",
    "\n",
    "model = alexnet(weights=\"AlexNet_Weights.DEFAULT\")\n",
    "# remove classification layer\n",
    "model.fc = model.classifier[6] = torch.nn.Identity()\n",
    "\n",
    "# get embeddings\n",
    "embedding1 = model(face1.unsqueeze(0))\n",
    "embedding2 = model(face2.unsqueeze(0))\n",
    "\n",
    "diff = torch.nn.functional.pairwise_distance(embedding1, embedding2)\n",
    "print(\"L2 distance: \", diff.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход будет работать.\n",
    "Однако можно заметно улучшить точность, используя функцию потерь, которая оценивает именно качество сравнения, а не классификации.\n",
    "\n",
    "\n",
    "Рассмотрим подход, основанный на методологии, описанной в статье [Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/siamese_neural_network_scheme.png\"  width=\"800\">\n",
    "\n",
    "<center><em>Используются две копии одной и той же сети, отсюда и название Siamese Networks.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два входных изображения ($x_1$ и $x_2$) проходят через одну и ту же сверточную сеть, на выходе для каждого изображения генерируется вектор признаков фиксированной длины $h_1$ и $h_2$.\n",
    "\n",
    "Модель обучается генерировать близкие вектора для изображений одного объекта и далекие для разных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/siamese_neural_network_idea_scheme.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценивая расстояние между двумя векторами признаков, \n",
    "которое будет малым для одних и тех же объектов и большим для различных, мы сможем оценить их сходство.\n",
    "\n",
    "Это центральная идея сиамских сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какую функцию потерь использовать для обучения такой сети?\n",
    "\n",
    "Очевидно, loss function должна будет учитывать не один выход, а как минимум два.\n",
    "\n",
    "Популярной на сегодняшний день является  `Triplet loss`, которой требуется  три embedding вместо двух."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/triplet_loss_scheme.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сгенерировать три эмбеддинга, модель должна получать на вход три изображения. \n",
    "\n",
    "Первые два должны относиться к одному и тому же объекту (человеку), а третье — к другому.\n",
    "\n",
    "\n",
    "Таким образом, триплет состоит из опорного (\"якорного\" `anchor`), положительного (`positive`) и отрицательного (`negative`) образцов.\n",
    "\n",
    "Описание в статье [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Сама функция потерь будет выглядеть следующим образом:\n",
    "\n",
    "$$TripletLoss = \\sum_{1}^{N} L_i(x_i^{a},x_i^{p},x_i^{n})$$\n",
    "\n",
    "$$L_i(x_i^{a},x_i^{p},x_i^{n})=max(0,\\left\\| f(x_i^{a}) -f(x_i^{p}) \\right\\|_2^{2} - \\left\\| f(x_i^{a}) -f(x_i^{n}) \\right\\|_2^{2} + margin)$$\n",
    "\n",
    "Где:\n",
    "\n",
    "\n",
    "$x_i^{a}$ — базовое изображение (anchor),\n",
    "\n",
    "$x_i^{p}$ — изображение того же объекта (positive),\n",
    "\n",
    "$x_i^{n}$ — изображение другого объекта (negative),\n",
    "\n",
    "$f(x)$ — **нормированный** выход модели (embedding) для входа $x$,\n",
    "\n",
    "\n",
    "$\\left\\| x \\right\\|_2$ — это L2 (Euclidean norm), соответственно $\\left\\| a \\right\\|_2^{2}$ — это L2 в квадрате,\n",
    "\n",
    "$margin$ — это константа или минимальный \"зазор\", на который расстояние до эмбеддинга негативного объекта обязано превосходить расстояние до позитивного (идея такая же, что в SVM Loss) .\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/triplet_loss_idea_scheme.png\"  width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе обучения с  Triplet Loss расстояние между эмбеддингами опорного объекта и позитивного уменьшается, а для отрицательного увеличивается.\n",
    "\n",
    "Важным дополнением является то, что embedding-и нормируются. В результате нормировки каждый вектор-признак будет иметь единичную длину."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L11/out/sphere_distance.png\"  width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем рассматривать embedding-и как точки на n-мерной сфере с радиусом 1. \n",
    "\n",
    "Это удобно, так как все расстояния между embedding будут лежать в интервале [0..2], и нам будет проще подобрать порог для сравнения.\n",
    "\n",
    "Кроме того, можно использовать другие меры расстояния, например, [косинусное расстояние](https://buildwiki.ru/wiki/Cosine_similarity), которое определяется углом между векторами, лежит в интервале [-1 ..1] и соответствует расстоянию между точками на поверхности сферы.\n",
    "\n",
    "В [статье](https://arxiv.org/abs/1503.03832) авторы минимизируют Евклидово расстояние, но подход будет работать и для других метрик сходства, например, косинусного расстояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch есть две реализации TripletLoss\n",
    "\n",
    "[TripletMarginLoss](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html) — минимизирует $L_p$ норму "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "loss = triplet_loss(anchor, positive, negative)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TripletMarginWithDistanceLoss](https://pytorch.org/docs/stable/generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss) — позволяет задать произвольную функцию расстояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "triplet_loss = nn.TripletMarginWithDistanceLoss(\n",
    "    margin=1, distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    ")\n",
    "loss = triplet_loss(anchor, positive, negative)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие функции потерь для сиамских сетей:\n",
    "\n",
    "Исторически первой появилась `Contrastive Loss`, о ней подробнее в статье [Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](https://www.researchgate.net/publication/4246277_Dimensionality_Reduction_by_Learning_an_Invariant_Mapping)\n",
    "\n",
    "\n",
    "В PyTorch есть реализация \n",
    "[CosineEmbeddingLoss](https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html), она позволяет обучать модель на парах изображений, минимизировав [косинусное расстояние](https://buildwiki.ru/wiki/Cosine_similarity) между embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация сиамской сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим небольшой фрагмент датасета с лицами. Внутри архива фото лиц сгруппированы по папкам\n",
    "\n",
    "```\n",
    "faces/\n",
    "├── training/\n",
    "|   ├── s1/\n",
    "|   |   ├── 1.pgm\n",
    "|   |   ├ ...\n",
    "|   |   └── 9.pgm\n",
    "|   ├ ... (excluding 5...7)\n",
    "|   └── s40/\n",
    "|       ├── 1.pgm\n",
    "|       ├ ...\n",
    "|       └── 9.pgm\n",
    "└── testing/\n",
    "    ├── s5/\n",
    "    |   ├── 1.pgm\n",
    "    |   ├ ...\n",
    "    |   └── 9.pgm\n",
    "    ├ ...\n",
    "    └── s7/\n",
    "        ├── 1.pgm\n",
    "        ├ ...\n",
    "        └── 9.pgm\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "В каждой папке фото лица одного и того же человека."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L11/small_face_dataset.zip\n",
    "!unzip small_face_dataset.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы результаты воспроизводились, зафиксируем SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for TripletLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для TripletLoss потребуются три изображения: anchor, positive, negative и метод __get_item__ должен возвращать их нам. Первые два должны принадлежать одному человеку, а третье — другому. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self, dir=None, transform=None, splitter=\"/\"):\n",
    "        self.dir = dir\n",
    "        self.splitter = splitter\n",
    "        self.transform = transform\n",
    "        self.files = glob(f\"{self.dir}/**/*.pgm\", recursive=True)\n",
    "        self.data = self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        index = {}\n",
    "        for f in self.files:\n",
    "            id = self.path2id(f)\n",
    "            if not id in index:\n",
    "                index[id] = []\n",
    "            index[id].append(f)\n",
    "        return index\n",
    "\n",
    "    def path2id(self, path):\n",
    "        return path.replace(self.dir, \"\").split(self.splitter)[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_path = self.files[index]\n",
    "        positive_path = self.find_positive(anchor_path)\n",
    "        negative_path = self.find_negative(anchor_path)\n",
    "\n",
    "        # Loading the images\n",
    "        anchor = Image.open(anchor_path)\n",
    "        positive = Image.open(positive_path)\n",
    "        negative = Image.open(negative_path)\n",
    "\n",
    "        if self.transform is not None:  # Apply image transformations\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def find_positive(self, path):\n",
    "        id = self.path2id(path)\n",
    "        all_exept_my = self.data[id].copy()\n",
    "        all_exept_my.remove(path)\n",
    "        return random.choice(all_exept_my)\n",
    "\n",
    "    def find_negative(self, path):\n",
    "        all_exept_my_ids = list(self.data.keys())\n",
    "        id = self.path2id(path)\n",
    "        all_exept_my_ids.remove(id)\n",
    "        selected_id = random.choice(all_exept_my_ids)\n",
    "        return random.choice(self.data[selected_id])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем несколько изображений, чтобы убедиться, что класс датасета функционирует должным образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Create dataset instance\n",
    "siamese_dataset = SiameseNetworkDataset(\n",
    "    \"faces/training/\",\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((105, 105)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create dataloader & extract batch of data from it\n",
    "vis_dataloader = DataLoader(siamese_dataset, batch_size=8, shuffle=True)\n",
    "dataiter = iter(vis_dataloader)\n",
    "example_batch = next(dataiter)  # anc, pos, neg\n",
    "\n",
    "# Show batch contents\n",
    "concatenated = torch.cat((example_batch[0], example_batch[1], example_batch[2]), 0)\n",
    "grid = torchvision.utils.make_grid(concatenated)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "plt.gcf().set_size_inches(20, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждом столбце тройка изображений. Первое и второе принадлежат одному человеку, третье — другому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нас устроит любая модель для работы с изображениями. Например, ResNet18. \n",
    "\n",
    "Все, что от нас требуется, это:\n",
    "- заменить последний слой\n",
    "- отправлять на анализ три изображения вместо одного. Соответственно на выходе тоже будут три вектора признаков (embedding)\n",
    "\n",
    "\n",
    "Пожалуй, единственный вопрос — это размерность последнего слоя. В промышленных системах распознавания лиц, которые тренируются на датасетах из миллионов изображений, используются embedding размерностью от 128 до 512.\n",
    "\n",
    "Для демонстрационной задачи нам должно хватить 32 значений. Количество выходов последнего линейного слоя установим равным 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = resnet18(weights=None)\n",
    "        # Because we use grayscale images reduce input channel count to one\n",
    "        self.model.conv1 = nn.Conv2d(\n",
    "            1, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
    "        )\n",
    "        # Replace ImageNet 1000 class classifier with 64- out linear layer\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, latent_dim)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        out = self.model(x)\n",
    "        # normalize embedding to unit vector\n",
    "        out = torch.nn.functional.normalize(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, anchor, positive, negative, latent_dim):\n",
    "        output1 = self._forward(anchor)\n",
    "        output2 = self._forward(positive)\n",
    "        output3 = self._forward(negative)\n",
    "\n",
    "        return output1, output2, output3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузчики данных не отличаются от загрузчиков для обычной сети. \n",
    "Единственное отличие — это две аугментации, которые добавили к данным:\n",
    "\n",
    "*   Случайное отражение по вертикали (RandomHorizontalFlip)\n",
    "*   Размытие (GaussianBlur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as img_transf\n",
    "\n",
    "# Apply augmentations on train data\n",
    "img_trans_train = img_transf.Compose(\n",
    "    [\n",
    "        img_transf.Resize((105, 105)),\n",
    "        img_transf.RandomHorizontalFlip(),\n",
    "        img_transf.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        img_transf.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img_trans_test = img_transf.Compose(\n",
    "    [img_transf.Resize((105, 105)), img_transf.ToTensor()]\n",
    ")\n",
    "\n",
    "train_dataset = SiameseNetworkDataset(\"faces/training/\", transform=img_trans_train)\n",
    "val_dataset = SiameseNetworkDataset(\"faces/testing/\", transform=img_trans_test)\n",
    "\n",
    "batch_size = 300\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, num_workers=2, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, num_workers=2, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличие от  сетей для классификации в том, что у модели 3 выхода, и все их надо передать в loss. При этом нет меток в явном виде.\n",
    "Определить, какой embedding относится к позитивному образцу, а какой — к негативному, можно только порядком их следования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, criterion, optimizer, train_loader, latent_dim):\n",
    "    loss_history = []\n",
    "    l = []\n",
    "    model.train()\n",
    "    for epoch in range(0, num_epochs):\n",
    "        for i, batch in enumerate(train_loader, 0):\n",
    "            anc, pos, neg = batch\n",
    "            output_anc, output_pos, output_neg = model(\n",
    "                anc.to(device), pos.to(device), neg.to(device), latent_dim\n",
    "            )\n",
    "            loss = criterion(output_anc, output_pos, output_neg)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            l.append(loss.item())\n",
    "        last_epoch_loss = torch.tensor(l[-len(train_loader) : -1]).mean()\n",
    "        print(\"Epoch {} with {:.4f} loss\".format(epoch, last_epoch_loss))\n",
    "\n",
    "    return l, last_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовании GPU обучение на 5-ти эпохах займет около 15 секунд:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "latent_dim = 32\n",
    "model = SiameseNet(latent_dim).to(device)\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 6\n",
    "l, _ = train(num_epochs, model, criterion, optimizer, train_loader, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем график loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(l))], l)\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"num of epochs\")\n",
    "x = range(0, len(l), len(l) // num_epochs)\n",
    "labels = range(0, num_epochs)\n",
    "plt.xticks(x, labels)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что он график loss уже вышел на плато, значит, можно проверять результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала выведем тройки изображений из проверочного датасета и посмотрим на расстояния для позитивных и негативных пар. Если модель обучилась, расстояния для позитивных пар будут меньше. Другими словами: расстояние между похожими лицами будет маленьким, а между разными будет большим.\n",
    "\n",
    "P.S. По умолчанию TripletLoss минимизирует Евклидово расстояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method for visualization\n",
    "def show(img, text=None):\n",
    "    img_np = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(75, 120, text, fontweight=\"bold\")\n",
    "    plt.imshow(np.transpose(img_np, (1, 2, 0)))  # [CxHxW] -> [HxWxC] for imshow\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_imgs(model, test_loader, latent_dim):\n",
    "    distances_pos = []\n",
    "    distances_neg = []\n",
    "    # print(f'latent_dim: {latent_dim}')  # if you want to check latent_dim\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for i, batch in enumerate(test_loader, 0):\n",
    "            anc, pos, neg = batch\n",
    "            output_anc, output_pos, output_neg = model(\n",
    "                anc.to(device), pos.to(device), neg.to(device), latent_dim\n",
    "            )\n",
    "            # compute euc. distance\n",
    "            distance_pos = F.pairwise_distance(output_anc, output_pos).item()\n",
    "            distance_neg = F.pairwise_distance(output_anc, output_neg).item()\n",
    "\n",
    "            distances_pos.append(distance_pos)\n",
    "            distances_neg.append(distance_neg)\n",
    "\n",
    "            if not i % 5:\n",
    "                concatenated = torch.cat((anc, pos, neg))\n",
    "                result = \"OK\" if distance_neg - distance_pos > 0 else \"BAD\"\n",
    "                show(\n",
    "                    torchvision.utils.make_grid(concatenated),\n",
    "                    f\"Positive / negative euclidean distances: {distance_pos:.3f} / {distance_neg:.3f} - {result}\",\n",
    "                )\n",
    "\n",
    "    return distances_pos, distances_neg\n",
    "\n",
    "\n",
    "distances_pos, distances_neg = plot_imgs(model, val_loader, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но такая оценка субъективна, давайте посмотрим на распределение расстояний по категориям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "distances = {\"The same person\": distances_pos, \"Another person\": distances_neg}\n",
    "\n",
    "ax = sns.displot(distances, kde=True, stat=\"density\")\n",
    "ax.set(xlabel=\"Pairwise distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что расстояние между двумя фото одного и того же человека в среднем меньше, чем расстояние между фото разных людей.\n",
    "\n",
    "Если бы мы проектировали систему распознавания лиц, нужно было бы выбрать порог, чтобы сравнивать с ним расстояние и принимать решение о том, верифицировать фото как подлинное или нет.\n",
    "\n",
    "Соответственно, для нашего игрушечного датасета такой порог следует выбирать ~= 0.75 при условии, что ошибки первого и второго рода для нас равнозначны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто, когда мы пишем и обучаем сети (будь то с нуля или с помощью transfer learning), мы вынуждены угадывать гиперпараметры (lr, betas и т.д). В случае с learning rate нам есть от чего оттолкнуться (маленький lr для transfer learning), но все же такой подход не кажется оптимальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации гиперпараметров существуют готовые решения, которые используют различные методы black-box оптимизации. Разберем одну из наиболее популярных библиотек — [**Optuna**](https://optuna.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте оптимизируем learning rate и latent_dim. Для того, чтобы код не выполнялся очень долго, укажем небольшой диапазон параметров и небольшое число эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "\n",
    "\n",
    "# define function which will optimized\n",
    "def objective(trial):\n",
    "    # boundaries for the optimizer's\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 8, 64, step=8)\n",
    "\n",
    "    # create new model(and all parameters) every iteration\n",
    "    model = SiameseNet(latent_dim).to(device)\n",
    "    criterion = nn.TripletMarginLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=lr\n",
    "    )  # learning step regulates by optuna\n",
    "\n",
    "    # To save time, we will take only 3 epochs\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, num_workers=2, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    _, last_epoch_loss = train(3, model, criterion, optimizer, train_loader, latent_dim)\n",
    "    return last_epoch_loss\n",
    "\n",
    "\n",
    "# Create \"exploration\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", study_name=\"Optimizer\", sampler=RandomSampler(42)\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective, n_trials=10\n",
    ")  # The more iterations, the higher the chances of catching the most optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видите, такой упрощенный подбор даже трех параметров модели, занимает много времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best params\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на историю оптимизации наших параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, проверим, а станет ли реально лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNet(study.best_params[\"latent_dim\"]).to(device)\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=study.best_params[\"lr\"]\n",
    ")  # take lr, which choosen Optuna\n",
    "\n",
    "num_epochs = 6\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, num_workers=2, batch_size=batch_size, shuffle=True\n",
    ")  # take batch_size\", which choosen Optuna\n",
    "l_optim, _ = train(\n",
    "    num_epochs,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    study.best_params[\"latent_dim\"],\n",
    ")  # take latent_dim, which choosen Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(l))], l, label=\"no optimization\")\n",
    "plt.plot([i for i in range(len(l_optim))], l_optim, label=\"optimal params\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"num of epochs\")\n",
    "plt.xticks(x, labels)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_pos, distances_neg = plot_imgs(\n",
    "    model, val_loader, study.best_params[\"latent_dim\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_optim = {\"The same person\": distances_pos, \"Another person\": distances_neg}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.histplot(distances, kde=True, stat=\"density\", alpha=0.5, ax=axes[0])\n",
    "sns.histplot(distances_optim, kde=True, stat=\"density\", alpha=0.5, ax=axes[1])\n",
    "\n",
    "axes[0].set(title=\"No optimization\")\n",
    "axes[1].set(title=\"Otimization with Optuna\")\n",
    "axes[0].set(xlabel=\"Pairwise distance\")\n",
    "axes[1].set(xlabel=\"Pairwise distance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение расстояний стало лучше, хотя по-прежнему не идеально.\n",
    "\n",
    "Это объясняется тем, что 10 итераций для подбора гиперпараметров все-таки маловато."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\">Заключение</font>\n",
    "\n",
    "\n",
    "Мы затронули проблемы, которые возникают при обучении на реальных данных.\n",
    "\n",
    "Одна из основных проблем — малые датасеты. Для того, чтобы обучить нейронную сеть на небольшом датасете, можно использовать:\n",
    "- аугментации;\n",
    "- Transfer learning.\n",
    "\n",
    "Однако необходимо помнить, что ни один из этих методов не защитит от ситуации, когда реальные данные будут сильно отличаться от тренировочных.\n",
    "\n",
    "В случае когда у нас не только мало данных, но еще и очень большое (возможно, неизвестное) число классов, можно воспользоваться One-shot Learning. В этом случае нейронная сеть обучается не классифицировать изображения, а, наоборот, находить различия между классом и новыми данными. Для этого используются нейронные сети, относящиеся к классу сиамских нейронных сетей.\n",
    "\n",
    "Также мы разобрали автоматическую оптимизацию гиперпараметров с помощью Optuna и показали, что это эффективный способ сделать нейросеть лучше малой ценой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\">Литература</font>\n",
    "\n",
    "<font size = \"5\">Обучение на реальных данных</font>\n",
    "\n",
    "[How to avoid machine learning pitfalls: a guide for academic researchers (Lones, 2021)](https://arxiv.org/abs/2108.02497)\n",
    "\n",
    "[Understanding data augmentation for classification: when to warp? (Wong et al., 2016)](https://arxiv.org/abs/1609.08764) \n",
    "\n",
    "[Learning from class-imbalanced data: Review of methods and applications (Haixiang et al., 2017)](https://www.sciencedirect.com/science/article/abs/pii/S0957417416307175?via%3Dihub)\n",
    "\n",
    "<font size = \"5\">Как решить проблему маленького количества данных?</font>\n",
    "\n",
    "[Блог-пост о том, как решать проблему малого количества данных.](https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d)\n",
    "\n",
    "<font size = \"5\">Несбалансированные данные</font>\n",
    "\n",
    "[Imbalanced Data: How to handle Imbalanced Classification Problems](https://medium.com/@cmukesh8688/how-to-handle-imbalanced-classification-problems-4a96f42ae4c4).\n",
    "\n",
    "[SMOTE explained for noobs - Synthetic Minority Over-sampling TEchnique line by line](https://rikunert.com/SMOTE_explained)\n",
    "\n",
    "[Блог пост про 8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "\n",
    "[Метрики, разработаные для работы с несбалансированными классами.](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/#:~:text=Classification%20Accuracy%20is%20Not%20Enough%3A%20More%20Performance%20Measures%20You%20Can%20Use,-By%20Jason%20Brownlee&text=When%20you%20build%20a%20model,This%20is%20the%20classification%20accuracy)\n",
    "\n",
    "\n",
    "<font size = \"5\">Transfer Learning</font>\n",
    "\n",
    "[Image Classification using Transfer Learning in PyTorch](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/)\n",
    "\n",
    "[How To Do Transfer Learning For Computer Vision | PyTorch Tutorial](https://www.youtube.com/watch?v=6nQlxJvcTr0)\n",
    "\n",
    "[Transfer learning for Computer Vision Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "[Python Pytorch Tutorials # 2 Transfer Learning : Inference with ImageNet Models](https://www.youtube.com/watch?v=Upw4RaERZic)\n",
    "\n",
    "[PyTorch - The Basics of Transfer Learning with TorchVision and AlexNet](https://www.youtube.com/watch?v=8etkVC93yU4)\n",
    "\n",
    "\n",
    "<font size = \"5\">Augmentation</font>\n",
    "\n",
    "[A survey on Image Data Augmentation for Deep Learning (Shorten and Khoshgoftaar, 2019)](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)\n",
    "\n",
    "[Data augmentation for improving deep learning in image classification problem](https://www.researchgate.net/publication/325920702_Data_augmentation_for_improving_deep_learning_in_image_classification_problem)\n",
    "\n",
    "\n",
    "<font size = \"5\">Few-shot learning</font>\n",
    "\n",
    "[One-Shot Learning with Siamese Networks using Keras](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d)\n",
    "\n",
    "[One-Shot image classification by meta learning](https://medium.com/nerd-for-tech/one-shot-learning-fe1087533585)\n",
    "\n",
    "[One-Shot Learning (Part 1/2): Definitions and fundamental techniques](https://heartbeat.fritz.ai/one-shot-learning-part-1-2-definitions-and-fundamental-techniques-1df944e5836a)\n",
    "\n",
    "[One-Shot Learning (Part 2/2): Facial Recognition Using a Siamese Network](https://heartbeat.fritz.ai/one-shot-learning-part-2-2-facial-recognition-using-a-siamese-network-5aee53196255)\n",
    "\n",
    "[FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff et al., 2015)](https://arxiv.org/abs/1503.03832)\n",
    "\n",
    "[One-Shot Learning explained using FaceNet](https://medium.com/intro-to-artificial-intelligence/one-shot-learning-explained-using-facenet-dff5ad52bd38)\n",
    "\n",
    "[Ищем знакомые лица](https://habr.com/ru/post/317798/)\n",
    "\n",
    "[Siamese Neural Networks for One-shot Image Recognition (Koch et al., 2015)](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "[Dimensionality Reduction by Learning an Invariant Mapping (Hadsell et al., 2005)](https://www.researchgate.net/publication/4246277_Dimensionality_Reduction_by_Learning_an_Invariant_Mapping)\n",
    "\n",
    "[Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
    "\n",
    "<font size = \"5\">Hyperparameter optimization</font>\n",
    "\n",
    "[Tuning Hyperparameters with Optuna](https://towardsdatascience.com/tuning-hyperparameters-with-optuna-af342facc549)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
