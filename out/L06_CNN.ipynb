{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Сверточные нейронные сети</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в сверточные нейронные сети\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полносвязная нейронная сеть\n",
    "\n",
    "Fully-connected Neural Network (FCN). В современных статьях чаще используется термин Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии мы научились строить сети из нескольких слоев.\n",
    "\n",
    "Вспомним как мы это делали:\n",
    "\n",
    "1. Превращаем исходные данные в вектор. \n",
    "\n",
    ">***Примечание***: Для цветного изображения bз CIFAR10 размером $32\\times32$ пикселя ($32\\times32\\times3$), размерность входного вектора будет равна $3072$. \n",
    "\n",
    "2. Перемножаем матрицу данных с матрицей весов. Размер последней может быть, например, $100\\times3072$. Где $3072$ &mdash; размер входного вектора, а $100$ &mdash; количество признаков, которое мы хотим получить. Результат обработки одного входа будет иметь размер $100\\times1$.\n",
    "\n",
    "3. Поэлементно применяем к вектору признаков нелинейную функцию (функцию активации), например, Sigmoid или ReLU. Размерность данных при этом не меняется ($100\\times1$). В результате получаем вектор активаций.\n",
    "\n",
    "4. Используем полученные активации как входные данные для нового слоя. Количество весов слоя будет зависеть от размерности входной матрицы и того, что мы хотим получить на выходе. Если мы делаем классификатор на $10$ классов, то матрица весов должна иметь размерность $10\\times100$, и на этом можно остановиться. Но в общем случае количество слоев может быть произвольным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/fully_connected_neural_network.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На изображении представлена описанная выше нейронная сеть, функцией активации в которой является ReLU. Добавление второго слоя позволило модели использовать более одного шаблона на класс. Можно убедиться в этом, обучив модель на датасете CIFAR-10 и визуализировав веса первого слоя модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP: Набор шаблонов классов, выученных нейросетью**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/mlp_templates.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За счёт создания нескольких шаблонов для каждого из классов, многослойные архитектуры в общем случае показывают более высокую, чем перцептроны, эффективность на задачах классификации изображений. Однако подход с использованием многослойного перцептрона также имеет свои недостатки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нарушение связей между соседними пикселями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вытягивании изображения в вектор мы теряем информацию о взаимном расположении пикселей на исходной картинке.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/img_to_vector_problem.png\" width=\"900\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://edunet.kea.su/repo/EduNet-content/L06/out/digit.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "image = Image.open(\"digit.png\")\n",
    "\n",
    "plt.imshow(np.array(image), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector = np.array(image).flatten()\n",
    "print(list(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пиксели которые были соседними и составляли цельный объект могут оказаться на большом расстоянии внутри результирующего вектора. Получается что мы просто удалили информацию об их близости, важность которой нам как людям очевидна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея полносвязной нейронной сети пришла к нам из математической модели восприятия информации мозгом ([перцептрон Розенблатта](http://www.machinelearning.ru/wiki/index.php?title=%D0%9F%D0%B5%D1%80%D1%81%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD)). Возможно, чтобы получить хорошие результаты при обработке изображений, нужно посмотреть, как работает человеческий глаз? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рецептивное поле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей полносвязанной сети каждый нейрон \"видит\" сразу все изображение (все данные). Наша зрительная система работает иначе. \n",
    "\n",
    "*  Каждый фоторецептор на сетчатке нашего глаза (палочка или колбочка) реагирует только на свет попавший на него.\n",
    "\n",
    "* Сигнал от фоторецептора попадает на нейрон следующего уровня ([биполярная клетка](https://ru.wikipedia.org/wiki/%D0%91%D0%B8%D0%BF%D0%BE%D0%BB%D1%8F%D1%80%D0%BD%D1%8B%D0%B5_%D0%BA%D0%BB%D0%B5%D1%82%D0%BA%D0%B8_%D1%81%D0%B5%D1%82%D1%87%D0%B0%D1%82%D0%BA%D0%B8)). Этот нейрон уже соединен с несколькими фоторецепторами. Область в которой они локализованы называется **рецептивным полем**.\n",
    "Нейрон возбуждается при определенной комбинации сигналов от связанных с ним рецепторных клеток. По сути, он реагирует на простой, локально расположенный паттерн.\n",
    "\n",
    "* Клетки уровнем выше (ганглиозные) собирают информацию с нескольких близко расположенных биполярных клеток  и активируются при уникальной комбинации сигналов с них. Их рецептивное поле больше и паттерны на которые они реагируют сложнее.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Рецептивное поле** нейрона &mdash; это участок с рецепторами с которых он прямо или опосредованно, через другие нейроны, получает информацию. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/brain.png\" width=\"1000\">\n",
    "\n",
    "* Далее сигнал предается в мозг, но  там связи между нейронами продолжают оставиться иерархическими. Рецептивное поле нейронов растет и паттерны на которые они активируются становяться все более и более сложными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скользящее окно (фильтр) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается что можно не подавать на вход нейрона первого слоя информацию о всем изображении сразу, а \"показать\" ему только часть картинки. Что бы он научился распознавать простые но универсальные паттерны. А их агрегация произойдет в последующих слоях.\n",
    "\n",
    "Для этого используется т.н. \"скользящее окно\" которое двигаться по изображению, захватывая на каждом шаге только небольшу область."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/convolution_with_filter.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход используют например при наложении графических **фильтров** . Вы наверняка пользовались им, если работали в графических редакторах, например, в Photoshop. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтр Гаусса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как можно реализовать такой фильтр на примере фильтра Гаусса размером $3 \\times 3$ для размытия изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# fmt: off\n",
    "# Gaussian 3x3 kernel, sum of weights == 1\n",
    "kernel = np.array([\n",
    "                [1/16, 1/8, 1/16],   \n",
    "                [1/8,  1/4, 1/8 ],\n",
    "                [1/16, 1/8, 1/16]\n",
    "                ])\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def apply_filter(img, kernel):\n",
    "    h, w = np.array(img.shape)  # image height and width (3x3)\n",
    "    kh, kw = np.array(kernel.shape)  # kernel height and width (3x3)\n",
    "    # calculate the output size, hard work ...\n",
    "    out = np.zeros((h - kh + 1, w - kw + 1))\n",
    "    for i in range(h - kh + 1):\n",
    "        for j in range(w - kw + 1):\n",
    "            # get 3x3 patch from image\n",
    "            patch = img[i : i + kh, j : j + kw]\n",
    "            # elementwise multiply patch pixels to kernel weights and sum\n",
    "            new_pixel = np.multiply(patch, kernel).sum()\n",
    "            # store modified pixel in new blurred image\n",
    "            out[i, j] = new_pixel\n",
    "    return out\n",
    "\n",
    "\n",
    "img_cat = data.cat().mean(axis=2).astype(\"int32\")\n",
    "out = apply_filter(img_cat, kernel)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "axes[0].imshow(img_cat, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axes[1].imshow(out, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axes[0].set(title=f\"Noised image, shape: {img_cat.shape}\")\n",
    "axes[1].set(title=f\"Blurred image: {out.shape}\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Для каждого пикселя исходного изображения рем окрестность размером 3x3.\n",
    "*  Значение каждого пикселя из этой окресности умножаем на соответствующее значение из массива kernel (тоже 3x3)\n",
    "*  Затем суммируем все 9 результатов,получившееся число записывали в новый массив \n",
    "* В результате получили сглаженное изображение.\n",
    "\n",
    "Строго говоря, для получения нового изображения мы применили формулу:\n",
    "\n",
    "$output(x,y) =  \\sum_{i}^{H} \\sum_{j}^{W}k_c[i,j]I_c[x+j,y+i] +$\n",
    "\n",
    " $H, W$ - высота и ширина ядра фильтра, $I$ исходное изображение\n",
    " \n",
    "\n",
    "**Если бы мы вытягивали фрагменты картинки и ядро фильтра в вектора затем их скалярно пермножили результат был бы тем же!**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие 'hand-crafted' фильтры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой же алгоритм можно применять не для сглаживания изображения, а для поиска (выделения) на нем чего-либо например контуров объектов. Для этого достаточно заменить ядро фильтра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# fmt: off\n",
    "sobel_y_kernel = torch.tensor([\n",
    "    [ 1.0,  2.0, 1.0 ], \n",
    "    [ 0.0,  0.0, 0.0 ],\n",
    "    [-1.0, -2.0, -1.0]\n",
    "    ])\n",
    "# fmt: on\n",
    "x_edges = apply_filter(img_cat, sobel_y_kernel)\n",
    "\n",
    "plt.imshow(x_edges, cmap=\"gray\", vmin=0, vmax=255)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы применили к изображению [фильтр Cобеля](https://en.wikipedia.org/wiki/Sobel_operator), (а точнее одно из его ядер дающее отклик на перепад яркости по вертикали).\n",
    "\n",
    "\"Отклик\" - это величина яркости которую мы получили на результирующем изображении. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запрограммировать свой фильтр, который будет искать произвольный объект.\n",
    "Например найдем крест на на изображении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "cross = np.array([  [0, 0 ,0, 0, 0],\n",
    "                    [0, 0, 0, 0, 0],\n",
    "                    [0, 0, 0, 1, 0],\n",
    "                    [0, 0, 1, 1, 1],\n",
    "                    [0, 0, 0, 1, 0]])\n",
    "# fmt: on\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cross, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого создадим фильтр размером с объект (3x3). В точках где должны быть пиксели принадлежащие объекту, поместим положительные значения, а там где должен быть фон - отрицательные.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "\n",
    "kernel_cs = np.array([[-1, 1, -1],\n",
    "                      [ 1, 1,  1],\n",
    "                      [-1, 1, -1]])\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При попадании такго фильтра на объект, отрицательные значения зануляться при перемножении с пикселями фона, а положительные суммируются и дадут высокий отклик."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/cross_filter.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = apply_filter(cross, kernel_cs)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cross, cmap=\"gray\")\n",
    "plt.title(\"Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlim([0, 5])\n",
    "plt.ylim([0, 5])\n",
    "plt.imshow(features, extent=(1, 4, 1, 4))\n",
    "plt.title(\"Features\")\n",
    "\n",
    "plt.show()\n",
    "print(\"Features:\\n\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такого рода фильтров люди придумали довольно много. Есть [детектор углов Харриса](https://en.wikipedia.org/wiki/Harris_Corner_Detector) или [признаки Хаара](https://en.wikipedia.org/wiki/Haar-like_feature) которые успешно использовались для [обнаружения лиц](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework) на фотографиях. Это примеры случаев когда людям удалось подобрать удачные ядра фильтров для решения конкретных задач.\n",
    "\n",
    "Мы хотим что бы модель могла обучаться решать различные задачи. И вместо того что бы вручную создавать фильтры, мы будем подбирать их значения *в процессе обучения*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свертка с фильтром"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операцию применения фильтра к изображению будем называть *сверткой*. Это определения не вполне соответствует [математическому](https://en.wikipedia.org/wiki/Convolution) но повсеместно используется в DL.\n",
    "\n",
    "По сути своей, операция свёртки &mdash; это та же самая взвешенная сумма с добавлением свободного члена, используемая в полносвязных линейных слоях. \n",
    "\n",
    "\n",
    "В фильтрах Собеля и Гаусса свободный член осутствовал. Но в дальнейшем мы будем его использовать.\n",
    "\n",
    "Давайте выполним свертку при помощи помощи линейного слоя: Теперь заменим код внутри цикла линейным слоем и убедимся что результат вычислений не поменялся. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "local_linear = nn.Linear(9, 1, bias=False)  # 9 = 3 * 3 (weights shape: (3,3))\n",
    "\n",
    "local_linear.weight.data[0] = torch.tensor(kernel_cs).flatten()  # Bad practice\n",
    "cross_in_tensor = torch.tensor(cross).float()\n",
    "result = torch.zeros((3, 3))\n",
    "for i in range(0, result.shape[0]):\n",
    "    for j in range(0, result.shape[1]):\n",
    "        segment = cross_in_tensor[i : i + 3, j : j + 3].flatten()\n",
    "        result[i, j] = local_linear(segment)\n",
    "\n",
    "print(f\"result:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевое отличие между линейным слоем и свёрткой заключается в том, что каждый нейрон линейного слойя получает на вход всё изображение сразу, а свёртка &mdash; небольшие фрагменты.\n",
    "\n",
    "Так как при свертке для каждого фрагмента получаем свой отклик (признак), то для всего изображения получим уже массив признаков (feature map). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/neuron_output.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточный слой нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Pytorch есть класс [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) который реализует операцию свертку для целого изображения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "\n",
    "conv = Conv2d(\n",
    "    in_channels=1,  # what's this ?\n",
    "    out_channels=1,  # what's this ?\n",
    "    kernel_size=(3, 3),  # kernel.shape == 3x3\n",
    "    bias=False,\n",
    ")\n",
    "# conv2d accept input of shape BxCHxW\n",
    "feature_map = conv(\n",
    "    cross_in_tensor.unsqueeze(0).unsqueeze(0)\n",
    ")  # add batch and channel dim\n",
    "print(feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как изначально ядро нашего единственного фильтра(нейрона) инициализированно случайными небольшими значениями,то на выходе получили набор ничего незначащих чисел. Убедимся что слой работает так как мы ожидали, подменив ядро:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0] because filter can have multiple kernels. see next chapter\n",
    "conv.weight.data[0] = torch.tensor(kernel_cs)  # replace original kernel\n",
    "\n",
    "feature_map = conv(\n",
    "    cross_in_tensor.unsqueeze(0).unsqueeze(0)\n",
    ")  # add batch and channel dim\n",
    "print(\"Feature map for cross\", feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При создании экземпляра объекта класса [nn.Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) помимо размера ядра (`kernel_size`), мы передали в конструктор еще два параметра:\n",
    "\n",
    "`in_channels = 1 ` и `out_channels = 1`\n",
    "\n",
    "Разберемся что они означают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка цветных/многоканальных изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`in_channel` - это количество каналов входного тензора (изображения).\n",
    "\n",
    "В примерах выше мы рассматривали [черно-белые](https://ru.wikipedia.org/wiki/%D0%A7%D1%91%D1%80%D0%BD%D0%BE-%D0%B1%D0%B5%D0%BB%D0%B0%D1%8F_%D1%84%D0%BE%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D1%8F) изображения. Их также называют одноканальными изображениями, т.к. в них цвет пикселя определяется одним числом, характеризующим яркость. \n",
    "\n",
    "Храняться они в двумерном массиве размером [H,W]. Цветные изображения храняться в трехмерных массивах [H,W,C] или [C,W,H] где C - количество цветовых каналов. Для RGB изображений C=3. Так как `Conv2d` рассчинан на работу с многоканальным входом, то в коде выше нам пришлось написать дополнительный `unsqueeze(0)` что бы добавить к тензору с изображением это 3-е измерение.\n",
    "\n",
    "Важно что для каждого канала будет создано дополнительное ядро фильтра.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ch1 = Conv2d(in_channels=1, out_channels=1, kernel_size=5)\n",
    "print(\"One channel kernel \\t\", conv_ch1.weight.shape)\n",
    "conv_ch3 = Conv2d(in_channels=3, out_channels=1, kernel_size=5)\n",
    "print(\"Three channel kernel \\t\", conv_ch3.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опробуем трехканальную свертку на цветном изображении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://edunet.kea.su/repo/EduNet-content/L06/out/cat.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "cat_in_pil = Image.open(\"cat.jpg\")\n",
    "display(cat_in_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображение из формата Pillow надо превратить в torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "cat_in_np = np.array(cat_in_pil)  # pillow -> numpy\n",
    "cat_in_float = cat_in_np.astype(np.float32) / 255  # int->float\n",
    "cat_in_tensor = torch.tensor(cat_in_float)  # np -> torch\n",
    "\n",
    "try:\n",
    "    conv_ch3(cat_in_tensor.unsqueeze(0))  # add batch dimension\n",
    "except Exception as e:\n",
    "    print(\"Error: \\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим ошибку связанную с количеством каналов. Дело в том что в PyTorch в отличие от OpenCV, TensorFlow и ряда других библиотек, каналы(RGB) идут в  первом а не в последнем измерении тензора описывающего картинку.\n",
    "\n",
    "\n",
    "\n",
    "OpenCV, TensorFlow, Pillow, e.t.c. : `Batch x Height x Width x Channels` \n",
    "\n",
    "Torch : `Batch x Channels x Height x Width x `\n",
    "\n",
    "\n",
    "\n",
    "Придется сделать дополнительное преобразоватние что бы каналы оказались на том месте где их ожидает Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original \\t\", cat_in_tensor.shape, \"HWC\")\n",
    "cat_in_tensor_channel_first = cat_in_tensor.permute(2, 0, 1)  # HWC -> CHW\n",
    "print(\"Torch style \\t\", cat_in_tensor_channel_first.shape, \"CHW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно подать изображение на вход модели, предваритьельно добавив batch - размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_image_batch = cat_in_tensor_channel_first.unsqueeze(0)\n",
    "out = conv_ch3(one_image_batch)\n",
    "print(\"No error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет необходимости проделывать все эти манипуляции вручную, так как в torchvision реализован класс [ToTensor](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html?highlight=totensor#torchvision.transforms.ToTensor) и функция [to_tensor](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.to_tensor.html) которые выполняет эти преобразования.\n",
    "\n",
    "Убедимся что тензор преобразованный нами вручную, и получившийся после применения функции [to_tensor](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.to_tensor.html) совпали:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "cat_in_tensor2 = to_tensor(cat_in_pil)\n",
    "print(cat_in_tensor2.shape)\n",
    "\n",
    "print(\n",
    "    \"Tensor almost equal: \",\n",
    "    torch.allclose(cat_in_tensor_channel_first, cat_in_tensor2),  # float comparsion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на форму выхода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output feature map size:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такую размерность имеет выход  **единственного нейрона** в нашем сверточном слое.\n",
    "\n",
    "На входе несколько каналов (3) на выходе остался один канал.\n",
    "Как же комбинируются результаты сверток в разных каналах?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/convolution_rgb.png\" width=\"700\">\n",
    "\n",
    "Результаты сверток всех ядер фильтра с соответствующими входными каналами просто суммируются:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$feature\\_map(x,y) = \\sum_{c}^{C} \\sum_{i}^{H} \\sum_{j}^{W}k_c[i,j]I_c[x+j,y+i] +bias$\n",
    "\n",
    "$С$ - количество каналов, $H, W$ - высота и ширина ядра фильтра $K_c$ - ядро для канала с, $Ic$ канал изображения.\n",
    "\n",
    "В силу коммутативности суммы не важно в каком порядке будут скаладываться элементы. Можно считать что каждый элемент вход сначала умножается на свой коеффициент из ядра, а уже затем все суммируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table colspan=\"5\"><tr><td>\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/rgb_image_and_convolution_filter.png\" width=\"400\">\n",
    "</td><td width=\"40\">\n",
    "</td><td>\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/convolution_filter_forward_pass.png\" width=\"200\">\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так выглдит выход одного нейрона который задается несколькими ядрами и  смещением(bias).\n",
    "\n",
    "При этом bias  **один** на весь фильтр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kernels\", conv_ch3.weight.shape)\n",
    "print(\"Biases\", conv_ch3.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Входные данные не всегда будут трехканальными цветными [RGB](https://ru.wikipedia.org/wiki/RGB) изображениями, в которых цвет пикселя определяется тремя числами, характеризующими три основных цвета (красный, зеленый и синий). \n",
    "\n",
    "Входной тензор может иметь произвольное количество каналов. Например: марсоход Opportunity для получения изображений использовал [13 каналов](https://habr.com/ru/post/160621/).  \n",
    "\n",
    "Более, того в качестве входного тензора можно использовать не изображение, а **карту активаций** предыдущего сверточного слоя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование нескольких фильтров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращаемся ко второму парметру конструктора nn.Conv2D \n",
    "`out_channels = 1`\n",
    "\n",
    "Этот параметр задает количество фильтров слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv37 = Conv2d(in_channels=3, out_channels=5, kernel_size=3)\n",
    "out = conv37(cat_in_tensor_channel_first)\n",
    "\n",
    "print(f\"weights shape: {conv37.weight.shape}\")  # 5 filters 3x3x3\n",
    "print(f\"weights shape: {conv37.bias.shape}\")  # one bias per filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейном слое каждый нейрон учился активироваться на некий шаблон, например красную машину, или смотрящую направо лошадь. \n",
    "\n",
    "Мы хотим что бы нейроны сверточного слоя так же активировались различные паттерны (например ухо, нос, глаз ...). Для каждого паттерна нам нужен свой нейрон."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/neuron_is_filter.png\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Каждый нейрон сформирует свою карту признаков, размером $1\\times H_{out}\\times W_{out}$. А на выходе слоя будет их конкатенация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"result shape: {out.shape}\")  # 5 feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " При объединении этих карт будет получен тензор размерности $C_{out}\\times H_{out} \\times W_{out}$, где $C_{out}$ - количество фильтров.\n",
    " \n",
    " На изображении ниже продемонстрирован результат применения сверточного слоя, \n",
    "содержащего $5$ фильтров, к изображению из CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/convolution_layer_with_5_filters.png\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Стоит отметить, что, в отличие от полносвязанного слоя, свёрточный слой не требует информации о количестве значений во входном представлении и может быть использован как для представлений $C_{in} \\times 32 \\times 32$, так и $C_{in} \\times 100 \\times 100$. Словом, представления могут иметь практически любой размер, главное, чтобы пространственные размеры не были меньше размеров ядра свёртки.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уменьшение размера карты признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Карта признаков после применения функции активации может быть передана на вход следующей операции свёртки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "conv_1 = torch.nn.Conv2d(\n",
    "    in_channels=3,  # Number of input channels (3 for RGB images)\n",
    "    out_channels=6,  # Number of filters/output channels\n",
    "    kernel_size=5,\n",
    ")\n",
    "\n",
    "conv_2 = torch.nn.Conv2d(\n",
    "    in_channels=6,  # Number of input channels (3 for RGB images)\n",
    "    out_channels=10,  # Number of filters/output channels\n",
    "    kernel_size=5,\n",
    ")\n",
    "\n",
    "img = torch.randn((1, 3, 32, 32))  # 1-batch size, 3-num of channels, (32,32)-img size\n",
    "print(f\"img shape: {img.shape}\")\n",
    "\n",
    "out_1 = conv_1(img)\n",
    "print(f\"out_1 shape: {out_1.shape}\")  # [1, 6, 28, 28]\n",
    "\n",
    "out_2 = conv_2(relu(out_1))\n",
    "print(f\"out_2 shape: {out_2.shape}\")  # [1, 10, 24, 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что после свёртки ширина $W_{out}$ и высота $H_{out}$ **карты признаков** будут отличаться от **пространственных размерностей** $W_{in}$ и $H_{in}$ исходного тензора. К примеру, при обработке трёхканального тензора размера $32 \\times 32$ ядром размера $5 \\times 5$, можно будет произвести лишь $27$ сдвигов $(32 - 5)$ по вертикали и столько же по горизонтали. Но при этом размер итоговой карты признаков будет равен $28 \\times 28$, поскольку первый ряд (либо столбец) можно получить без сдвигов по вертикали либо горизонтали, соответственно. При повторном применении фильтра размер каждой из сторон уменьшится ещё на $4$. \n",
    "\n",
    "Итоговое значение $N'$ пространственной размерности $N$ при размере стороны фильтра $F$ вычисляется по следующей формуле: $$\\large N' = N - F + 1$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/decrease_size_of_image_after_convolution.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что при уменьшении размера представлений пиксели, находящиеся около краёв, участвуют в значительно меньшем количестве свёрток, чем пиксели в середине, хотя информация в них не обязательно менее ценна, чем информация из центральных пикселей. К примеру, пиксель в верхнем левом углу представления вне зависимости от размера фильтра будет принимать участие лишь в одной свёртке и информация о нём будет сохранена лишь в верхнем левом углу нового представления. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расширение (padding)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для борьбы с описанной выше проблемой применяется *набивка/дополнение* входного тензора (англ. *padding*). В ходе него ширина и высота тензора увеличиваются за счёт приписывания столбцов и строк с некими значениями. К примеру, на изображении ниже перед свёрткой ядром размера $3\\times3$ был применён padding нулями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/padding.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На примере убедимся, что это позволит нам сохранить пространственные размерности тензоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "img = torch.randn((1, 1, 5, 5))  # create random image BCHW\n",
    "print(f\"Original tensor:\\nshape:{img.shape}\")\n",
    "conv_3 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "conved_3 = conv_3(img)\n",
    "print(\"Shape after convolution layer(kernel 3x3):\", conved_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Карта признаков меньше чем вход. Теперь добавим padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zeros to image manually\n",
    "padded_img = torch.zeros((1, 1, 7, 7))  # create zeros array to insert image in center\n",
    "padded_img[:, :, 1:-1, 1:-1] += img  # insert image, we get image arounded by zeros\n",
    "print(f\"\\nPadded tensor:\\nshape:{padded_img.shape}:\\n {padded_img}\")\n",
    "\n",
    "conved_pad_3 = conv_3(padded_img)\n",
    "print(\"\\n\\nPadded shape:\", padded_img.shape)\n",
    "print(\"Shape after convolution with padding(kernel 3x3):\", conved_pad_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер выхода равен размеру входа. \n",
    "\n",
    "\n",
    "Однако если мы увеличим размер ядра до 5x5 то увидим что не смотря на padding выход снова стал меньше входа.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_5 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=5)\n",
    "\n",
    "conved_pad_5 = conv_5(padded_img)\n",
    "\n",
    "print(\"\\n\\nOriginal shape:\", img.shape)\n",
    "print(\"Shape after convolution with padding(kernel 5x5):\", conved_pad_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнение одним рядом и одним столбцом не является универсальным решением: для фильтра размером 5 размер выходного тензора всё равно отличается от входного. Если мы немного видоизменим полученную выше формулу (используя размер дополнения $P$), то получим : $N' = N + 2\\cdot P - F + 1$.  \n",
    "Для того чтобы пространственные размеры не изменялись ($N' = N$), для разных размеров фильтра требуются разные размеры паддинга. В общем случае, для размера фильтра $F$ требуемый размер дополнения  $$\\displaystyle P = \\frac{F-1}{2}$$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем padding, используя инструменты библиотеки PyTorch, и сравним его с ручным добавлением padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv layer without padding (padding=0 by default)\n",
    "conv_3 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=0)\n",
    "\n",
    "# conv layer with padding = 1 (add zeros)\n",
    "conv_3_padded = torch.nn.Conv2d(\n",
    "    in_channels=1, out_channels=1, kernel_size=3, padding=1\n",
    ")  # Padding added 1 zeros line to all four sides of the input\n",
    "original = conv_3(padded_img)\n",
    "padded = conv_3_padded(img)\n",
    "\n",
    "print(f\"Explicitly padded:\\n {original.shape}\\n \")\n",
    "print(f\"\\nImplicitly padded:\\n{padded.shape}\\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме чисел параметр padding принимать значение `same` — тогда padding будет рассчитан автоматически так что размер выходного тензора не отличается от размера входного тензора, а и `valid` — отсутствие паддинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация работы свертки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/convolution_with_same_padding_rgb_image.gif\" width=\"780\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование свёрточных слоёв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем заменить часть линейных слоев нашей модели насверточные. Но если решаетя задача классификации, то последним по прежнему долен идти линейный слой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку операция свертки является линейной(мы убедились в этом когда выполняли ее при помощи линейного слоя), по функция активации (например, ReLU) по прежнему требуется.\n",
    "\n",
    ">*Так как функция активации применяется к тензору поэлементно, не важно, какую именно форму имеет тензор, а значит и какой слой находился передней ней: полносвязный или сверточный. \n",
    "\n",
    "Простейшая модель для MNIST может выглядеть примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((1, 1, 28, 28))\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=1, out_channels=3, kernel_size=5\n",
    "    ),  # after conv shape: [1,3,24,24]\n",
    "    nn.ReLU(),  # Activation doesn't depend on input shape\n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=6, kernel_size=3\n",
    "    ),  # after conv shape: [1,6,22,22]\n",
    "    nn.Flatten(),  # 6*22*22=2904\n",
    "    nn.Linear(2904, 100),\n",
    "    nn.ReLU(),  # Activation doesn't depend on input shape\n",
    "    nn.Linear(100, 10),  # 10 classes, like a cifar10\n",
    ")\n",
    "\n",
    "out = model(input)\n",
    "print(f\"out shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку полносвязный слой принимает на вход набор векторов, а сверточный возвращает набор трёхмерных тензоров, нам нужно превратить эти тензоры в вектора. Для этого используется объект класса [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten). \n",
    "Он преврашает данные на входе в вектор, сохраняя при этом первое (batch) измерение.\n",
    "\n",
    "Ниже примеры других функций которыми можно выполнить аналогичное преобразование:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((16, 3, 32, 32))\n",
    "\n",
    "batch_size = input.shape[0]\n",
    "\n",
    "print(\"class Flatten\\t\", nn.Flatten()(input).shape)\n",
    "print(\n",
    "    \"view \\t\\t\", input.view(batch_size, -1).shape\n",
    ")  # data stay in same place in memory\n",
    "print(\"reshape \\t\", input.reshape(batch_size, -1).shape)  # data may be moved\n",
    "print(\"method flatten \\t\", input.flatten(1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общая структура свёрточной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросетевая модель из предыдущего примера позволяет в общем случае понять структуру свёрточных нейронных сетей: после некоторого количества свёрточных слоёв, извлекающих локальную пространственную информацию, идут полносвязные слои (как минимум в количестве одного), сопоставляющие извлечённую информацию. \n",
    "\n",
    "Внутри свёрточных слоёв происходит следующий процесс: первые слои нейронных сетей имеют малые рецептивные поля, т.е. им соответствует малая площадь на исходном изображении. Такие нейроны могут активироваться лишь на некие простые шаблоны, по типу углов или освещённости.\n",
    " Нейроны на следующего слоя уже имеют большие рецептивные поля, в результате чего в картах активации появляется информация о более сложных паттернах. С каждым слоем свёрточной нейронной сети рецептивное поле нейронов увеличиваются, увеличивается и сложность шаблонов, на которые может реагировать нейрон. В последних слоях, рецептивное поле нейрона должно быть размером со всё исходное изображение. Пример можно увидеть на схеме ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/receptive_field_size.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если на первом слое рецептивное поле имело размер $K \\times K$, то после свёртки фильтром $K\\times K$, оно стало иметь размер $(2K-1) \\times (2K-1)$. То есть, увеличилось на $K-1$ по каждому из направлений. Не сложно самостоятельно убедиться, что данная закономерность сохранится при дальнейшем применении фильтров того или иного размера.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако при обработке больших изображений, что бы нейрон \"увидел\" всю картинку нам потребуется очень много слоев.\n",
    "\n",
    "К примеру, для изображения $1024\\times1024$ понадобиться сеть глубиной ~510 сверточных слоев.\n",
    "\n",
    "Такая модель потребует огромного количества памяти и вычислительных ресурсов.\n",
    "Что бы избежать этого будем сами уменьшать размеры карт признаков, при этом рецептивное поля нейронов будут расти.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг свёртки (Stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы двигали фильтер на один пиксел, по есть перемещались с шагом(stride) один.\n",
    "\n",
    "Если двигать фильтр с большим шагом, то размер выходной карты признаков (feature map) будет уменьшаться кратно шагу, и рецептивные поля нейроновбудут расти быстрее.\n",
    "\n",
    "Для изменения шага свертки, в конструкторе [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) есть параметр `stride`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 5, 5)\n",
    "conv_s1 = Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=(1, 1))\n",
    "conv_s2 = Conv2d(1, 3, 3, stride=2)  # equal to stride = (2,2)\n",
    "\n",
    "out_stride1 = conv_s1(dummy_input)\n",
    "out_stride2 = conv_s2(dummy_input)\n",
    "\n",
    "print(\"Out with stride 1\", out_stride1.shape)\n",
    "print(\"Out with stride 2\", out_stride2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/convolution_parameter_stride.gif\" width=\"350\"></center>\n",
    "<center>Свёртка массива $5\\times5$ фильтром размером $3\\times3$ с шагом $2$ по вертикали и горизонтали.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом важно заметить, что в некоторых случаях часть данных может не попасть в свёртку. К примеру, при $N = 7,\\, K = 3,\\, S = 3$. В данном случае, $\\displaystyle N' = 1 + \\frac{7 - 3}{3} = 2\\frac13.$ В подобных ситуациях часть изображения не захватывается, в чём мы можем убедиться на наглядном примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch tensor 7x7\n",
    "# fmt: off\n",
    "input = torch.tensor([[[[1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99]]]], dtype=torch.float)\n",
    "# fmt: on\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=1,  # Number of channels\n",
    "    out_channels=1,  # Number of filters\n",
    "    kernel_size=3,\n",
    "    stride=3,\n",
    "    bias=False,  # Don't use bias\n",
    ")\n",
    "conv.weight = torch.nn.Parameter(\n",
    "    torch.ones((1, 1, 3, 3))\n",
    ")  # Replace random weights to ones\n",
    "out = conv(input)\n",
    "\n",
    "print(f\"out shape: {out.shape}\")\n",
    "print(f\"out:\\n{out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно столбец с числами 99 просто не попал в свертку.\n",
    "Поэтому на практике подбирают padding таким образом что бы при stride = 1,  размер карты признаков на выходе был равен входу, а затем делают сверку со  stride =2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"orange\">Доп инфо.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Казалось бы, с увеличением шага $S$ рецептивное поле не выросло &mdash; как увеличивалось с $1$ до $K$, так и увеличивается. Однако обратим внимание на иное: если раньше размерность $N$ становилась $N - F + 1$, то теперь она станет $\\displaystyle 1 + \\frac{N-F}{S}$. В результате, если раньше следующий фильтр с размером $K'$ имел рецептивное поле в $\\displaystyle N \\cdot \\frac{K'}{N'} = N \\cdot \\frac{K'}{N - F + 1}$, то теперь $\\displaystyle N \\cdot \\frac{K'}{N'} = N \\cdot \\frac{K'}{1 + \\frac{N-F}{S}}$. Понятно, что $\\displaystyle \\frac{K'}{N - F + 1} \\leq \\frac{K'}{1 + \\frac{N-F}{S}}$, потому рецептивное поле каждого нейрона увеличивается.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уплотнение (Субдискретизация, Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим вариантом стремительного увеличения размера рецептивного поля является использование дополнительных слоёв, требующих меньшее количество вычислительных ресурсов. Слои субдискретизации прекрасно выполняют эту функцию: подобно свёртке, производится разбиение изображения на небольшие сегменты, внутри которых выполняются операции, не требующие использования обучаемых весов. Два популярных примера подобных операций: получение максимального значения (max pooling) и получение среднего значения (average pooling). \n",
    "\n",
    "\n",
    "Аналогично разбиению на сегменты при свёртке, слои пуллинга имеют два параметра: размер фильтра $F$ (то есть, каждого из сегментов) и шаг $S$ (stride). Аналогично свёрткам, при применении пуллинга, формула размера стороны будет $\\displaystyle N' = 1+ \\frac{N-F}{S}.$\n",
    "\n",
    "Ниже приведён пример использования обоих пуллингов при обработке массива."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/subdiscretization_pooling.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем это в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor 4x4\n",
    "# fmt: off\n",
    "input = torch.tensor([[[\n",
    "                     [1, 1, 2, 4],\n",
    "                     [5, 6, 7, 8],\n",
    "                     [3, 2, 1, 0],\n",
    "                     [1, 2, 3, 4]]]], dtype=torch.float)\n",
    "# fmt: on\n",
    "\n",
    "max_pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "print(\"Input:\\n\", input)\n",
    "print(\"Max pooling:\\n\", max_pool(input))\n",
    "print(\"Average pooling:\\n\", avg_pool(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно отметить**, что субдискретизация выполняется по каждому из каналов отдельно, в результате чего количество каналов не меняется, в отличие от применения фильтра при свёртке. К примеру, ниже можно увидеть визуализацию применения max pooling к одному из каналов тензора, имеющего $64$ канала. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/changing_size_of_image_after_pooling.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свёртка фильтром $1\\times1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оба упомянутых выше метода позволяют сделать архитектуру сети не слишком глубокой, путём быстрого увеличения рецептивного поля нейронов, что позволяет уменьшить число обучаемых параметров модели. Познакомимся с ещё одним способом уменьшения числа обучаемых параметров модели. \n",
    "\n",
    "\n",
    "Рассмотрим фрагмент архитектуры CNN, состоящий из одного свёрточного слоя с размерами ядра свёртки $F_h\\times F_w$ и некоторой активации (например, [`torch.nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)):\n",
    "\n",
    "\n",
    "$$... \\rightarrow (N, C_{in}, H, W) \\rightarrow \\text{conv2d}_{F_h\\times F_w} \\rightarrow \\text{ReLu} \\rightarrow  (N, C_{out}, H', W')  \\rightarrow ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как обучаемыми параметрами являются элементы ядра свёртки и сдвиг (bias), число таких параметров очень легко посчитать:\n",
    "* для формирования одной выходной карты признаков, как мы уже подробно обсуждали выше, нам нужно свернуть все входные карты признаков с соответствующими им матрицами элементов ядра свёртки, сложить результаты вместе и добавить bias — то есть в формировании одной выходной карты признаков участвуют $C_{in} \\cdot F_{h} \\cdot F_w + 1$ обучаемых параметров.\n",
    "* чтобы получить $C_{out}$ выходных карт признаков, мы столько  же раз должны повторить описанную выше процедуру с разными $C_{in} \\cdot F_{h} \\cdot F_w + 1$  параметрами.\n",
    "Таким образом, **общее число обучаемых параметров в одном свёрточном слое:** $\\text{n_params}[\\text{conv2d}_{F_h \\times F_w}] = (C_{in} \\cdot F_{h} \\cdot F_w + 1) \\cdot C_{out}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем значительно уменьшить число обучаемых параметров, внеся небольшое изменение в рассмотренную архитектуру. Перед применением свёрточного слоя с размером ядра $F_h \\times F_w$ мы можем расположить ещё один свёрточный слой с ядром свёртки из одного единственного пространственного элемента ($1 \\times 1$), который будет предназначен для уменьшения числа карт признаков перед подачей последующему свёрточному слою без изменений из пространственных размеров $H$ и $W$:\n",
    "\n",
    "$$... \\rightarrow (N, C_{in}, H, W)  \\rightarrow \\text{conv2d}_{1 \\times 1} \\rightarrow \\text{ReLu} \\rightarrow (N, C_{mid}, H, W) \\rightarrow \\\\ \\rightarrow (N, C_{mid}, H, W) \\rightarrow \\text{conv2d}_{F_h\\times F_w} \\rightarrow \\text{ReLu} \\rightarrow  (N, C_{out}, H', W')  \\rightarrow ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея заключается в следующем: рассматривая набор входных карт признаков  $C_{in} \\times (H \\times W)$ можно выделить вектор размерностью $C_{in}$, содержащий элементы карт признаков с некоторыми фиксированным пространственными координатами. Элементы этого вектора сообщают, насколько сильно рецептивное поле соответствует каждому из $C_{in}$ шаблонов. Применение к входным картам признаков свёрточного слоя с ядром $1 \\times 1 $ и последующей активации приведёт к нелинейному преобразованию таких векторов из пространства размерности $C_{in}$ в новое пространство размерности $C_{mid}$. Так как параметры такого сжимающего преобразования будут подбираться в процессе обучения, мы ожидаем, что свёртка $1 \\times 1$ позволит подобрать полезные комбинации входных карт признаков для всех пространственных элементов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/1_times_1_convolutions_featere_maps.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если выбрать $C_{min} < C_{in}$, то общие число параметров модели действительно уменьшится:\n",
    "\n",
    "$$ \\text{n_params}[\\text{conv2d}_{1 \\times 1} \\rightarrow \\text{conv2d}_{F_h \\times F_w}] = \\\\\n",
    "= (C_{in} + 1) \\cdot C_{mid} + (C_{mid} \\cdot F_{h} \\cdot F_w + 1) \\cdot C_{out} \\approx \\frac{C_{mid}}{C_{in}}  \\text{n_params}[\\text{conv2d}_{F_h \\times F_w}] $$\n",
    "\n",
    "\n",
    "Ниже приведён пример применения такого фильтра с целью снижения количества карт признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/convolution_with_kernel_size_one.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=128,  # Number of input channels\n",
    "    out_channels=32,  # Number of filters\n",
    "    kernel_size=1,\n",
    ")\n",
    "\n",
    "input = torch.randn((1, 128, 64, 64))\n",
    "out = conv(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Shape after 1x1 conv:\", out.shape)  # [1, 32, 64, 64] batch, C_out, H_out, W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение свёрточного и полносвязного слоев "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте оценим количество ресурсов которые требуются для, обработки одого изображения из CIFAR10 при помощи сверточного и полносвязанного слоя.\n",
    "\n",
    "Пусть сверточный слой будет содержить 6 фильтров размером 3x3, padding =1 , stride = 1\n",
    "\n",
    "А полносвязанный 6 выходов (как если бы мы делали классификацию 6-ти классов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/convolution_layer.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сколько обучаемых праметров (весов) у такого сверточного слоя?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество параметров в одном фильтре: $C_{in}\\times K_{h}\\times K_{w} +1(bias) = 3 \\times 3 \\times 3 + 1 = 28$\n",
    "\n",
    "Количество фильтров $C_{out} = 6$\n",
    "\n",
    "*Итого: $(C_{in}\\times K_{h}\\times K_{w} +1) \\times C_{out} = 28 * 6 = 168$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "\n",
    "\n",
    "def get_params_count(module):\n",
    "    weights_count = 0\n",
    "    # Get all model weights: kernels + biases\n",
    "    for p in module.parameters():\n",
    "        print(p.shape)\n",
    "        # torch.prod - multiply all values in tensor\n",
    "        weights_count += torch.tensor(p.shape).prod()\n",
    "    print(\"Total weights\", weights_count.item())\n",
    "\n",
    "\n",
    "conv = Conv2d(3, 6, 3, bias=True)\n",
    "get_params_count(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сколько обучаемых праметров у полносвязанного?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Данные вытягиваем в вектор: \n",
    "$inputs\\_count = C_{in} \\times H_{in} \\times W_{in}  = 3*32*32 = 3072$\n",
    "\n",
    "2. Каждый нейрон(их 6 шт) выходного слоя хранит вес для каждого элемента входа(3072): $inputs\\_count \\times outputs\\_count = 3072 * 6 = 18432$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "linear = Linear(3072, 6, bias=True)\n",
    "get_params_count(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть для хранения весов такого линейного слоя нужно ~ 100 раз больше памяти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### А как много вычислительных ресурсов требуется полносвязанному слою ?\n",
    "\n",
    "*Считаем только умножения т.к. (умножение + сложение = 1 [FLOP](https://en.wikipedia.org/wiki/FLOPS)).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В полносвязанном слое каждый вход умножается на свой вес один раз и количество умножений совпадает с количеством весов:\n",
    "\n",
    " $C_{in} \\times H_{in} \\times W_{in} \\times 𝑜𝑢𝑡𝑝𝑢𝑡𝑠\\_𝑐𝑜𝑢𝑛𝑡  = 3∗32∗32*6 = 18438$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### А сколько ресурсов уйдет на свертку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Разовое применения фильтра эквивалентно применению линейный слой с таким же количеством весов:\n",
    "\n",
    " $C_{in} \\times K_{h} \\times K_{w} \\times C_{out} = 3*32*32*6 = 168$\n",
    "\n",
    "Т.е. умножаем каждый вес фильтра на вход.\n",
    "\n",
    "2. Сдвигаем фильтр и повторяем пункт один для каждой точки на карте признаков:\n",
    "\n",
    "$C_{in} \\times K_{h} \\times K_{w} \\times C_{out} \\times H_{out} \\times W_{out}   = 168 \\times 32 \\times 32  =172032 $\n",
    "\n",
    "То есть количество операций в ~10 раз больше чем у полносвязанного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выводы:\n",
    "\n",
    "\n",
    "Выигрыш по количеству параметров при использовании свёрточного слоя омрачается большим количеством операций перемножения. Это было проблемой в течение долгого времени, пока вычисление операции свёртки не перевели на видеокарты (Graphical Processing Unit). При выполнении свёртки одного сегмента не требуется информация о результатах свёртки на другом сегменте, поэтому данные операции можно выполнять параллельно, с чем как раз прекрасно справляются видеокарты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Conv2d\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Conv2d(1, 6, 3)\n",
    "model.to(device)  # send model to device\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 5, 5)\n",
    "out = model(dummy_input.to(device))  # send data to GPU too!\n",
    "# ... do backprop if need\n",
    "out = out.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Другие виды сверток\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как выглядит операция свертки в функциональном анализе.\n",
    "\n",
    "Есть две различные функции, определяющие локальную \"схожесть\" функций $f(t)$ и $g(t)$:\n",
    "- взаимнокорреляционная функция обозначается пятиконечной звездой <font size=\"8\">$\\large ⋆$</font> и определяет схожесть двух функций:\n",
    "$$(f ⋆ g)(t)\\stackrel{def}{=}  \\int\\limits_{-\\infty}^{\\infty}  f(\\tau)g(t+\\tau)d\\tau$$\n",
    "- свертка обозначается звездочкой (астериском) <font size=\"8\">$*$</font> и определяет схожесть одной функции и \"отраженной\" другой функции:\n",
    "$$\\large(f * g)(t)\\stackrel{def}{=}  \\int\\limits_{-\\infty}^{\\infty}  f(\\tau)g(t-\\tau)d\\tau$$\n",
    "\n",
    "Взаимная корреляция более интуитивно понятна: она представляет собой \"наложение\" шаблона на функцию, а свертка &mdash; \"наложение\" отраженного шаблона. Эти функции взаимосвязаны:\n",
    "$$f(t) ⋆ g(t) = f(-t) * g(t)$$\n",
    "\n",
    "Можно представить свертку как площадь произведения двух функций внутри скользящего окна, как на анимации ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/1d_convolution.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В машинном обучении под словом **convolution** как правило [подразумевают](https://glassboxmedicine.com/2019/07/26/convolution-vs-cross-correlation/) **взаимнокорреляционную функцию**, а не свертку. В реальности при обучении нейронной сети совершенно неважно, используется ли свертка или взаимнокорреляционная функция &mdash; они отличаются лишь порядком расположения весов внутри тензора ядра.\n",
    "\n",
    "В случае дискретных величин для вычисления взаимной корреляции сигнал $f(t)$ поэлементно умножается со смещенным ядром $g(t)$, и результат суммируется:\n",
    "\n",
    "$$\\large(f \\star g)(t) = f(1)g(t+1) + f(2)g(t+2) + f(3)g(t+3) + ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одномерная операция свертки используется для данных, имеющих последовательную структуру: текстов, аудиозаписей, цифровых сигналов. Как правило, такую структуру можно представить в виде изменения величины с течением времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch одномерная свертка задается аналогично двумерной: [torch.nn.Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.nn.Conv1d(in_channels,\n",
    "                out_channels, \n",
    "                kernel_size, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                dilation=1,\n",
    "                groups=1, \n",
    "                bias=True, \n",
    "                padding_mode='zeros')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = conv(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Двумерная операция свертки, о которой мы много говорили, применяется для обработки данных, имеющих пространственную структуру &mdash; то есть, играют роль взаимные расположения по двум осям. Совсем не обязательно, чтобы эти оси соответствовали высоте и ширине картинки. Например, одна ось может соответствовать координате сенсора в одномерной матрице, а вторая &mdash; времени получения информации с него.\n",
    "\n",
    "Трёхмерная операция свертки используется, когда данные имеются три независимых \"пространственных\" компоненты. Простейшим примером являются видео: к двумерной структуре самих изображений добавляется координата времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/3d_convolution.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    " torch.nn.Conv3d(in_channels, \n",
    "                  out_channels, \n",
    "                  kernel_size, \n",
    "                  stride=1, \n",
    "                  padding=0, \n",
    "                  dilation=1, \n",
    "                  groups=1, \n",
    "                  bias=True, \n",
    "                  padding_mode='zeros')\n",
    " \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With cubic kernels and same stride\n",
    "conv = nn.Conv3d(in_channels=16, out_channels=33, kernel_size=3, stride=2)\n",
    "\n",
    "# non-square kernels with unequal stride and padding\n",
    "conv = nn.Conv3d(\n",
    "    in_channels=16,\n",
    "    out_channels=33,\n",
    "    kernel_size=(3, 5, 2),\n",
    "    stride=(2, 1, 1),\n",
    "    padding=(4, 2, 0),\n",
    ")\n",
    "\n",
    "input = torch.randn(20, 16, 10, 50, 100)\n",
    "out = conv(input)\n",
    "\n",
    "print(\"out shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Справочник по сверткам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A guide to convolution arithmetic for deep\n",
    "learning](https://arxiv.org/pdf/1603.07285v1.pdf) \n",
    "\n",
    "[An Introduction to different Types of Convolutions in Deep Learning](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)\n",
    "\n",
    "\n",
    "the best: \n",
    "\n",
    "[A Comprehensive Introduction to Different Types of Convolutions](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример сверточной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем более детально взглянуть на типичную архитектуру свёрточной нейронной сети. Как ранее уже обсуждалось, в первую очередь необходимо последовательностью свёрточных слоёв и уплотнений достичь того, чтобы каждый элемент карты активации имел большое рецептивное поле, а значит мог отвечать за большие и сложные шаблоны. Затем данные карты активаций выпрямляются в вектора и передаются в полносвязные слои, последовательность которых, используя глобальную информацию, возвращает значение целевой переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/neural_network_architecture.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/lenet_architecture.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примером сети, построенной по такой архитектуре, является LeNet.\n",
    "Была разработана в 1989г [Яном Ле Куном](https://en.wikipedia.org/wiki/Yann_LeCun). Сеть имела 5 слоев с обучаемыми весами, из них 2 сверточных.\n",
    "\n",
    "Применялась в США для распознавания рукописных чисел на почтовых конвертах до начала 2000г.\n",
    "\n",
    "[LeNet PyTorch documentation](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "\n",
    "Ниже представлена реализация подобной сети на PyTorch для датасета MNIST:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# transforms for data\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.13), (0.31)),\n",
    "    ]\n",
    ")  # mean and std for MNIST train data\n",
    "\n",
    "train_set = MNIST(root=\"./MNIST\", train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root=\"./MNIST\", train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем сверточную сеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_model, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),  # in channel=1, out=32\n",
    "            nn.MaxPool2d(2),  # size [32,14,14]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),  # in channel=32, out=32\n",
    "            nn.MaxPool2d(2),  # size [32,7,7]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 7 * 7, 100),  # in = channel*heght*width\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # change run time to gpu to fast training\n",
    "\n",
    "model = CNN_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "loss_hist = []  # for plotting\n",
    "for epoch in range(num_epochs):\n",
    "    hist_loss = 0\n",
    "    for _, batch in enumerate(train_loader, 0):  # get batch\n",
    "        # parse batch\n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        # sets the gradients of all optimized tensors to zero.\n",
    "        optimizer.zero_grad()\n",
    "        # get outputs\n",
    "        y_pred = model(imgs)\n",
    "        # calculate loss\n",
    "        loss = criterion(y_pred, labels)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # performs a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        hist_loss += loss.item()\n",
    "    loss_hist.append(hist_loss / len(train_loader))\n",
    "    print(f\"Epoch={epoch} loss={loss_hist[epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим график обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_epochs), loss_hist)\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посчитаем accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader):\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:  # get batch\n",
    "            imgs, labels = batch  # parse batch\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            y_pred = model.forward(imgs)  # get output\n",
    "            _, predicted = torch.max(y_pred.data, 1)  # get predicted class\n",
    "            total += labels.size(0)  # all examples\n",
    "            correct += (predicted == labels).sum().item()  # correct predictions\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = round(calculate_accuracy(model, train_loader), 3)\n",
    "print(f\"Accuracy train = {acc_train}\")\n",
    "acc_test = round(calculate_accuracy(model, test_loader), 3)\n",
    "print(f\"Accuracy test = {acc_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы сравним результат с моделью, которую мы делали на прошлом занятии, то можем увидеть, как выросла точность и уменьшилась ошибка на обучении (точность выросла на ~10%, ошибка уменьшилась с 0.4 до ~ 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам может быть интересно, на какую информацию обращает внимание модель в процессе работы, на какие визуальные шаблоны реагирует, насколько они интерпретируемы? \n",
    "\n",
    "Чтобы ответить на все эти вопросы, можно визуализировать карты активаций и веса фильтров свёртки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/what_hidden_layers.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация весов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса фильтров на первом слое легко визуализировать. И результат легко интерпретируется, так как у фильтров такое же количество каналов, как и у цветных изображений (3).\n",
    "\n",
    "Ниже приведен пример того, как это можно сделать для обученной модели в PyTorch (AlexNet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять, через какие свойства можно получить доступ к весам, выведем структуры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "alexnet = models.alexnet(weights=\"AlexNet_Weights.DEFAULT\")\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что первый слой это 0-й элемент контейнера features.\n",
    "Веса хранятся в weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = alexnet.features[0].weight.data  # extract weights\n",
    "print(\"Weights shape\", weight_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы отобразить все веса на одном изображении, воспользуемся вспомогательной функцией [make_grid](https://pytorch.org/vision/stable/generated/torchvision.utils.make_grid.html#make-grid) из [torchvision.utils](https://pytorch.org/vision/stable/utils.html)\n",
    "\n",
    "На вход метод получает batch изображений (B x C x H x W) в формате torch.Tensor и визуализирует их в виде таблице. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "\n",
    "img_grid = utils.make_grid(\n",
    "    (weight_tensor + 1) / 2, pad_value=1\n",
    ")  # combine weights from all channel into table, note remapping to (0,1) range\n",
    "print(\"Output is CxHxW image\", img_grid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ее часто используют, чтобы отображать изображения в TensorBoard. \n",
    "\n",
    "А чтобы отобразить получившуюся таблицу в блокноте средствами matplotlib, нам потребуется поменять порядок хранения данных, поместив каналы на первое место."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "plt.imshow(\n",
    "    np.transpose(img_grid, (1, 2, 0))\n",
    ")  # change channel order for compability with numpy & matplotlib\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель научились улавливать простые геометрические формы: края под разными углами, точки того или иного цвета. Тем не менее, фильтры AlexNet'а оказались настолько большими, что частично захватили не только простую локальную информацию, но и сложные градиенты или решётки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация фильтров промежуточных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, выполнить ту же операцию для фильтров на скрытых слоях едва ли представляется разумным: в отличие от принятого трёхканального вида фильтров, который легко можно трактовать и визуализировать, фильтры поздних слоёв имеют гораздо больше каналов, что делает их удобное отображение практически невозможным. Пожалуй, единственным вариантом отображения является поканальное отображение весов, которое довольно сложно трактовать, в чём можно убедиться, взглянув на пример ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher Layer: Visualize Filter**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L06/weight_visualization.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\">ConvNetJS CIFAR-10 example</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем веса 2-го сверточного слоя AlexNet. \n",
    "Слой доступен через `features[3] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_of_conv2_layer = alexnet.features[3].weight.data  # extract weights\n",
    "print(weights_of_conv2_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нем 192 фильтра в каждом 64 ядра. Поэтому ограничимся первым фильтром и выведем все его ядра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_filter_kernels = weights_of_conv2_layer[0]\n",
    "print(first_filter_kernels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать image_grid, входной тензор должен иметь формат BxCxHxW.  Поэтому добавим размерность соответствующую каналам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid = utils.make_grid(\n",
    "    weights_of_conv2_layer[0].unsqueeze(1), pad_value=1  # add fake channel dim\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 15)\n",
    "plt.imshow(\n",
    "    np.transpose((img_grid + 1) / 2, (1, 2, 0))\n",
    ")  # change channel order for compability with numpy\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерпретация такой визуализации довольно затруднительна, зато мы разобрались, как получать доступ к весам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация карт активаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее очевидный метод визуализации заключается в том, чтобы показать активации сети во время прямого прохода. Для сетей ReLU активации обычно начинают выглядеть относительно сгущенными и плотными, но по мере развития обучения активации обычно становятся более редкими и локализованными. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На последних слоях свёрточной нейронной сети размеры рецептивных полей нейронов становятся сравнимы с размером исходного изображения, потому при визуализации их карт активации становится понятно, какие нейроны реагируют на какие части изображений.\n",
    "\n",
    "К примеру, на изображении ниже активация выделенного нейрона достигнута благодаря пикселям, примерно соответствующим расположению лица человека, потому можно предположить, что он научился находить лица на изображении. Более подробно об этом можно почитать в статье [Understanding Neural Networks Through Deep Visualization](https://arxiv.org/abs/1506.06579)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing Activations**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L06/visualization_activations.png\" width=\"700\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf\">Visualizing and Understanding</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от весов, карты активаций не сохраняются в памяти. Для того, чтобы получить к ним доступ, в PyTorch предусмотрен механизм под названием [Hooks](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks).\n",
    "\n",
    "Благодаря ему можно получить доступ к выходам или входам слоя как при прямом, так и при обратном распространении сигнала через сеть.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зарегистрируем свой hook. Он просто выведет в консоль размеры карты активации (выхода слоя).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def module_hook(module: nn.Module, input, output):  # For nn.Module objects only.\n",
    "    print(\"Hi, i am hook_1!\", output.shape)  # activation_map\n",
    "\n",
    "\n",
    "handle = alexnet.features[10].register_forward_hook(\n",
    "    module_hook\n",
    ")  # attach hook to last conv layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что он работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "out = alexnet(torch.randn(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы удалить hook, используйте метод `remove` дескриптора, который возвращает метод `register_forward_hook`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()\n",
    "out = alexnet(torch.randn(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывода нет, hook отключился!\n",
    "\n",
    "Теперь напишем hook, который выведет нам карту активации. \n",
    "Так как на выходе данного слоя 256 каналов, выведем каждый отдельно, подав на вход make_grid тензор с 256 элементами. \n",
    "\n",
    "Для этого потребуется:\n",
    "* удалить batch измерение\n",
    "* добавить измерение, имитирующее канал для картинок\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module_hook(module: nn.Module, input, output):\n",
    "    # activation_map = output.squeeze(0).unsqueeze(1) # alternative solution\n",
    "    activation_map = output.permute(1, 0, 2, 3)\n",
    "    print(activation_map.shape)\n",
    "    img_grid = utils.make_grid(activation_map, pad_value=10, nrow=16)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "    plt.imshow(\n",
    "        np.transpose((img_grid.clamp(-1, 1) + 1) / 2, (1, 2, 0))\n",
    "    )  # normalize to 0..1 range and change channel order for compability with numpy\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "handle = alexnet.features[10].register_forward_hook(module_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " \n",
    "\n",
    "Чтобы карта активаций  была интерпретируема, надо использовать реальное изображение. Загрузим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/L06/fox.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_fox = Image.open(\"fox.jpg\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.imshow(img_fox)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим изображение, преобразуем в тензор и подадим на вход модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "tensor = transform(img_fox)\n",
    "out = alexnet(tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут уже можно увидеть некоторые паттерны. Видно, что многие фильтры реагируют на лисицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Единственная опасная ловушка, которую можно легко заметить с помощью этой визуализации, заключается в том, что некоторые карты активации могут быть равны нулю для многих различных входов. Это может указывать на мертвые фильтры и может быть симптомом высокой скорости обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отключим наш hook, чтобы он не мешал дальнейшим экспериментам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативным способом понимания эффективности обучения сети является анализ карт активаций с последних слоёв нейронной сети. Поскольку в процессе работы нейросети создаются промежуточные представления, имеющие меньшую размерность, весь процесс представляет из себя проектирование данных в пространство меньшей размерности. Новое представление может быть использовано для решения различных задач не только с помощью линейных слоёв нейросети, но и с помощью других методов машинного обучения.\n",
    "\n",
    "В следующих примерах мы будем использовать представление, полученное на последнем скрытом слое свёрточной нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Последний слой**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L06/out/feature_extractor.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы убедиться в полезности полученных представлений, выполним поиск ближайшего соседа для изображения в исходном виде, пиксельном пространстве. В результате будут получены изображения, не всегда относящиеся к тому же классу, что и исходное изображение.\n",
    "\n",
    "Если же мы используем предобученную свёрточную нейронную сеть для получения представлений изображений в более низкой размерности и затем применим алгоритм поиска соседей, то \"близко\" друг к другу окажутся изображения, принадлежащие одному классу, даже если есть визуально более похожие изображения из иных классов, так как фон и прочие неважные части изображения имеют меньшее влияние на представление. Успешное извлечение признаков отображает успешное обучение модели. Один из вариантов выполнения кластеризации изображений — кластеризация промежуточных представлений свёрточных нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Последний слой: ближайшие соседи**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L06/last_layer_knn.jpg\" width=\"900\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\">ImageNet Classification with Deep Convolutional Neural Networks</a></p> </em></center>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
