{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Сверточные нейронные сети</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в сверточные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полносвязная нейронная сеть\n",
    "\n",
    "Fully-connected Neural Network (FCN). В современных статьях чаще используется термин Multi-Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии мы научились строить сети из нескольких слоев.\n",
    "\n",
    "Вспомним, что нам необходимо для этого сделать:\n",
    "\n",
    "1. Превращаем исходные данные в вектор.\n",
    "\n",
    ">***Примечание***: Для цветного изображения из CIFAR-10 размером $32\\times32$ пикселя ($32\\times32\\times3$) размерность входного вектора будет равна $3072$.\n",
    "\n",
    "2. Перемножаем матрицу данных с матрицей весов. Размер последней может быть, например, $100\\times3072$, где $3072$ — размер входного вектора, а $100$ — количество признаков, которое мы хотим получить. Результат обработки одного входа будет иметь размер $100\\times1$.\n",
    "\n",
    "3. Поэлементно применяем к вектору признаков нелинейную функцию (функцию активации), например, Sigmoid или ReLU. Размерность данных при этом не меняется ($100\\times1$). В результате получаем вектор активаций.\n",
    "\n",
    "4. Используем полученные активации как входные данные для нового слоя. Количество весов слоя будет зависеть от размерности входной матрицы и того, что мы хотим получить на выходе. Если мы делаем классификатор на $10$ классов, то матрица весов должна иметь размерность $10\\times100$, и на этом можно остановиться. Но в общем случае количество слоев может быть произвольным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/modified_model.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large h=W_1 \\cdot x$\n",
    "    \n",
    "$\\large S=W_2 \\cdot f(h)=W_2 \\cdot f(W_1 \\cdot x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На изображении представлена описанная выше нейронная сеть. Добавление второго слоя позволило модели использовать более одного шаблона на класс. Можно убедиться в этом, обучив модель на датасете CIFAR-10 и визуализировав веса первого слоя модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class FCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(3 * 32 * 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем файл с предобученными весами (точность $\\approx0.5$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/weights/2layer.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим веса в модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "fc_model = FCNet()\n",
    "weights_in_dict = torch.load(\"2layer.pt\")\n",
    "fc_model.load_state_dict(weights_in_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP: Набор шаблонов классов, выученных нейросетью.**\n",
    "\n",
    "Визуализируем веса первого слоя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "\n",
    "\n",
    "W1 = fc_model.layers_stack[0].weight.reshape(64, 3, 32, 32)  # layer has 64 neurons\n",
    "img_grid = utils.make_grid(W1, pad_value=1, normalize=True, nrow=16)\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.title(\"Weights visualization in 2D\")\n",
    "plt.imshow(img_grid.permute(1, 2, 0).cpu().numpy())  # CHW -> HWC\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За счёт создания нескольких шаблонов для каждого из классов многослойные архитектуры в общем случае показывают более высокую эффективность в задачах классификации изображений. Однако и здесь есть что улучшить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нарушение связей между соседними пикселями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При вытягивании изображения в вектор мы теряем информацию о взаимном расположении пикселей на исходной картинке.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/img_to_vector_problem.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/digit.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image = Image.open(\"digit.png\")\n",
    "img_np = np.array(image)\n",
    "plt.imshow(img_np, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "ax[0].imshow(img_np, cmap=\"gray\")\n",
    "ax[1].imshow(img_np.reshape(1, -1), aspect=20, cmap=\"gray\")\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[1].set_title(\"Flattened image\")\n",
    "\n",
    "vector = np.array(image).flatten()\n",
    "print(list(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пиксели, которые были соседними и составляли цельный объект, могут оказаться на большом расстоянии внутри результирующего вектора. Получается, что мы просто удалили информацию об их близости, важность которой нам как людям очевидна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея полносвязной нейронной сети пришла к нам из математической модели восприятия информации мозгом ([перцептрон Розенблатта ✏️[blog]](https://habr.com/ru/companies/sberdevices/articles/529932/)). Возможно, чтобы получить хорошие результаты при обработке изображений, нужно посмотреть, как работает человеческий глаз?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рецептивное поле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В полносвязной сети каждый нейрон \"видит\" сразу все изображение (все данные). Наша зрительная система работает иначе.\n",
    "\n",
    "*  Каждый фоторецептор на сетчатке нашего глаза (палочка или колбочка) реагирует только на свет, попавший на него.\n",
    "\n",
    "* Сигнал от фоторецептора попадает на нейрон следующего уровня ([биполярная клетка 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%91%D0%B8%D0%BF%D0%BE%D0%BB%D1%8F%D1%80%D0%BD%D1%8B%D0%B5_%D0%BA%D0%BB%D0%B5%D1%82%D0%BA%D0%B8_%D1%81%D0%B5%D1%82%D1%87%D0%B0%D1%82%D0%BA%D0%B8)). Этот нейрон уже соединен с несколькими фоторецепторами. Область, в которой они локализованы, называется **рецептивным полем**.\n",
    "Нейрон возбуждается при определенной комбинации сигналов от связанных с ним рецепторных клеток. По сути, он реагирует на простой, локально расположенный паттерн.\n",
    "\n",
    "* Клетки уровнем выше (ганглиозные) собирают информацию с нескольких близко расположенных биполярных клеток и активируются при уникальной комбинации сигналов с них. Их рецептивное поле больше, и паттерны, на которые они реагируют, сложнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Рецептивное поле** нейрона — это участок с рецепторами, с которых он прямо или опосредованно, через другие нейроны, получает информацию.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/brain.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Далее сигнал передается в мозг, но там связи между нейронами продолжают оставаться иерархическими. Рецептивное поле нейронов растет, и паттерны, на которые они активируются, становятся все более и более сложными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скользящее окно (фильтр)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что можно не подавать на вход нейрона первого слоя информацию о всем изображении сразу, а \"показать\" ему только часть картинки, чтобы он научился распознавать простые, но универсальные паттерны. А их агрегация произойдет в последующих слоях.\n",
    "\n",
    "Для этого используется так называемое \"скользящее окно\", которое двигается по изображению, захватывая на каждом шаге только небольшую область."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/convolution_with_filter_step1.png\" width=\"600\">\n",
    "\n",
    "\n",
    "<font color=2BA8E0 size=30>0</font>\n",
    "<font color=000000 size=30>×</font>\n",
    "<font color=6aa84f size=30>6 </font>\n",
    "<font color=000000 size=30>+ </font>\n",
    "<font color=2BA8E0 size=30>1</font>\n",
    "<font color=000000 size=30>×</font>\n",
    "<font color=6aa84f size=30>7 </font>\n",
    "<font color=000000 size=30>+ </font>\n",
    "<font color=2BA8E0 size=30>3</font>\n",
    "<font color=000000 size=30>×</font>\n",
    "<font color=6aa84f size=30>8 </font>\n",
    "<font color=000000 size=30>+ </font>\n",
    "<font color=2BA8E0 size=30>4</font>\n",
    "<font color=000000 size=30>×</font>\n",
    "<font color=6aa84f size=30>9 </font>\n",
    "<font color=000000 size=30>= </font>\n",
    "<font color=38761d size=30>67</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/convolution_with_filter.gif\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтры размытия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой подход используют, например, при наложении графических **фильтров**. Вы наверняка пользовались ими, если работали в графических редакторах, например, в Photoshop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейший фильтр — это [Box blur 📚[wiki]](https://en.wikipedia.org/wiki/Box_blur), который просто усредняет значения соседних пикселей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, color\n",
    "from skimage.transform import rescale\n",
    "\n",
    "# fmt: off\n",
    "\n",
    "box_blur_kernel = 1/9 * np.array([[1, 1, 1],\n",
    "                                  [1, 1, 1],\n",
    "                                  [1, 1, 1]])\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def apply_filter(img, kernel):\n",
    "    h, w = np.array(img.shape)  # image height and width\n",
    "    kh, kw = np.array(kernel.shape)  # kernel height and width (3x3)\n",
    "    # calculate the output size, hard work ...\n",
    "    out = np.zeros((h - kh + 1, w - kw + 1))\n",
    "    for i in range(h - kh + 1):\n",
    "        for j in range(w - kw + 1):\n",
    "            # get 3x3 patch from image\n",
    "            patch = img[i : i + kh, j : j + kw]\n",
    "            # elementwise multiply patch pixels to kernel weights and sum\n",
    "            new_pixel = np.multiply(patch, kernel).sum()\n",
    "            # store modified pixel in new blurred image\n",
    "            out[i, j] = new_pixel\n",
    "    return out\n",
    "\n",
    "\n",
    "def show(img, result):\n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    axes[0].imshow(img, cmap=\"gray\")\n",
    "    axes[1].imshow(out, cmap=\"gray\")\n",
    "    axes[0].set(title=f\"Original image, shape: {img_cat_resc.shape}\")\n",
    "    axes[1].set(title=f\"Blurred image: {out.shape}\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "img_cat = color.rgb2gray(data.cat())\n",
    "img_cat_resc = rescale(img_cat, 0.25, anti_aliasing=False) * 255\n",
    "\n",
    "out = apply_filter(img_cat_resc, box_blur_kernel)\n",
    "show(img_cat_resc, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно умножать каждый пиксель на свой коэффициент. Так устроен, например, [фильтр Гаусса 📚[wiki]](https://en.wikipedia.org/wiki/Gaussian_blur).\n",
    "\n",
    "Реализуем фильтр Гаусса размером $3\\times3$ для размытия изображения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "# Gaussian 3x3 kernel, sum of weights == 1\n",
    "gauss_kernel = np.array([[1/16, 1/8, 1/16],\n",
    "                         [1/8,  1/4, 1/8 ],\n",
    "                         [1/16, 1/8, 1/16]])\n",
    "# fmt: on\n",
    "\n",
    "out = apply_filter(img_cat_resc, gauss_kernel)\n",
    "show(img_cat_resc, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательность действий:\n",
    "\n",
    "*  Для каждого пикселя исходного изображения берем окрестность размером $3 \\times 3$.\n",
    "*  Значение каждого пикселя из этой окрестности умножаем на соответствующее значение из массива kernel (тоже $3 \\times 3$).\n",
    "*  Затем суммируем все 9 результатов, получившееся число записываем в новый массив.\n",
    "* В результате получаем сглаженное изображение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, для получения нового изображения мы применили формулу:\n",
    "\n",
    "$$\\large \\text{output}(x,y) =  \\sum_{i}^{H} \\sum_{j}^{W}k[i,j] I[x+j,y+i],$$\n",
    "\n",
    "где $H, W$ — высота и ширина ядра фильтра, $I$ — исходное изображение.\n",
    "\n",
    "**Если бы мы вытянули фрагменты картинки и ядро фильтра в вектора, а затем их скалярно перемножили, результат был бы тем же!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтры для обнаружения паттернов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой же алгоритм можно применять не для сглаживания изображения, а для поиска (выделения) на нем чего-либо (например, контуров объектов). Для этого достаточно заменить ядро фильтра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "sobel_y_kernel = torch.tensor([[ 1.0,  2.0, 1.0 ],\n",
    "                               [ 0.0,  0.0, 0.0 ],\n",
    "                               [-1.0, -2.0, -1.0]])\n",
    "# fmt: on\n",
    "\n",
    "x_edges = apply_filter(img_cat * 255, sobel_y_kernel)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].imshow(img_cat, cmap=\"gray\")\n",
    "axes[1].imshow(x_edges, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axes[0].set(title=f\"Original image, shape: {img_cat.shape}\")\n",
    "axes[1].set(title=f\"Horizontal edges detector: {x_edges.shape}\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы применили к изображению [фильтр Cобеля 📚[wiki]](https://en.wikipedia.org/wiki/Sobel_operator), а точнее, одно из его ядер, дающее отклик на перепад яркости по вертикали.\n",
    "\n",
    "\"Отклик\" — это величина яркости, которую мы получили на результирующем изображении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запрограммировать свой фильтр, который будет искать произвольный объект.\n",
    "Например, найдем крест на на изображении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "cross = np.array([[0, 0 ,0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 1, 0],\n",
    "                  [0, 0, 1, 1, 1],\n",
    "                  [0, 0, 0, 1, 0]])\n",
    "# fmt: on\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cross, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого создадим фильтр размером с объект ($3 \\times 3$). В точках, где должны быть пиксели, принадлежащие объекту, поместим положительные значения, а там, где должен быть фон, — отрицательные.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "\n",
    "kernel_cs = np.array([[-1, 1, -1],\n",
    "                      [ 1, 1,  1],\n",
    "                      [-1, 1, -1]])\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При попадании такого фильтра на объект отрицательные значения обнулятся при перемножении с пикселями фона, а положительные — просуммируются и дадут высокий отклик."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/cross_filter.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = apply_filter(cross, kernel_cs)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cross, cmap=\"gray\")\n",
    "plt.title(\"Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlim([0, 5])\n",
    "plt.ylim([0, 5])\n",
    "plt.imshow(features, extent=(1, 4, 1, 4))\n",
    "plt.title(\"Features\")\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.show()\n",
    "print(\"Features:\\n\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такого рода фильтров люди придумали довольно много. Есть [детектор углов Харриса 📚[wiki]](https://en.wikipedia.org/wiki/Harris_Corner_Detector) или [признаки Хаара 📚[wiki]](https://en.wikipedia.org/wiki/Haar-like_feature), которые успешно использовались для [обнаружения лиц 📚[wiki]](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework) на фотографиях. Это примеры случаев, когда людям удалось подобрать удачные ядра фильтров для решения конкретных задач.\n",
    "\n",
    "Мы хотим, чтобы модель могла обучаться решать различные задачи. И вместо того, чтобы вручную создавать фильтры, мы будем подбирать их значения **в процессе обучения**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/mlp_vs_cnn.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда модель сможет выучивать шаблоны для небольших фрагментов изображения и станет инвариантной к сдвигу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свертка с фильтром"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операцию применения фильтра к изображению будем называть *сверткой*. Это определение не вполне соответствует [математическому 📚[wiki]](https://en.wikipedia.org/wiki/Convolution), но повсеместно используется в DL.\n",
    "\n",
    "По своей сути операция свёртки — это та же самая взвешенная сумма с добавлением свободного члена, используемая в полносвязных линейных слоях.\n",
    "\n",
    "В фильтрах Собеля и Гаусса свободный член осутствовал, но в дальнейшем мы будем его использовать.\n",
    "\n",
    "Выполним свертку при помощи помощи линейного слоя: заменим код внутри цикла линейным слоем и убедимся, что результат вычислений не поменялся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_linear = nn.Linear(9, 1, bias=False)  # 9 = 3 * 3 (weights shape: (3,3))\n",
    "\n",
    "local_linear.weight.data[0] = torch.tensor(kernel_cs).flatten()  # Bad practice\n",
    "cross_in_tensor = torch.tensor(cross).float()\n",
    "result = torch.zeros((3, 3))\n",
    "for i in range(0, result.shape[0]):\n",
    "    for j in range(0, result.shape[1]):\n",
    "        segment = cross_in_tensor[i : i + 3, j : j + 3].flatten()\n",
    "        result[i, j] = local_linear(segment)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cross, cmap=\"gray\")\n",
    "plt.title(\"Image\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlim([0, 5])\n",
    "plt.ylim([0, 5])\n",
    "plt.imshow(result.detach(), extent=(1, 4, 1, 4))\n",
    "\n",
    "plt.title(\"Result\")\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.show()\n",
    "print(\"Result:\\n\", result.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевое отличие между линейным слоем и свёрткой заключается в том, что каждый нейрон линейного слоя получает на вход всё изображение сразу, а свёртка — небольшие фрагменты.\n",
    "\n",
    "Так как при свертке для каждого фрагмента получаем свой отклик (признак), то для всего изображения получим уже массив признаков (feature map).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/neuron_output.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточный слой нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch есть класс `nn.Conv2d` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), который реализует операцию свертки для целого изображения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# fmt: off\n",
    "cross = np.array([[0, 0 ,0, 0, 0],\n",
    "                  [0, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 1, 0],\n",
    "                  [0, 0, 1, 1, 1],\n",
    "                  [0, 0, 0, 1, 0]])\n",
    "\n",
    "kernel_cs = np.array([[-1, 1, -1],\n",
    "                      [ 1, 1,  1],\n",
    "                      [-1, 1, -1]])\n",
    "# fmt: on\n",
    "\n",
    "cross_in_tensor = torch.tensor(cross).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "\n",
    "conv = Conv2d(\n",
    "    in_channels=1,  # what's this ?\n",
    "    out_channels=1,  # what's this ?\n",
    "    kernel_size=(3, 3),  # kernel.shape == 3x3\n",
    "    bias=False,\n",
    ")\n",
    "# conv2d accepts input of shape BxCxHxW\n",
    "feature_map = conv(\n",
    "    cross_in_tensor.unsqueeze(0).unsqueeze(0)\n",
    ")  # add batch and channel dim\n",
    "print(feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как изначально ядро нашего единственного фильтра (нейрона) инициализированно случайными небольшими значениями, то на выходе получается набор ничего не значащих чисел. Убедимся, что слой работает так, как мы ожидали, подменив ядро:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0] because filter can have multiple kernels. see next chapter\n",
    "conv.weight.data[0] = torch.tensor(kernel_cs)  # replace original kernel\n",
    "\n",
    "feature_map = conv(\n",
    "    cross_in_tensor.unsqueeze(0).unsqueeze(0)\n",
    ")  # add batch and channel dim\n",
    "print(\"Feature map for cross\\n\", feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При создании экземпляра объекта класса `nn.Conv2D` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) помимо размера ядра (`kernel_size`) мы передали в конструктор еще два параметра:\n",
    "\n",
    "`in_channels = 1 ` и `out_channels = 1`\n",
    "\n",
    "Разберемся, что они означают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка цветных/многоканальных изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`in_channel` — это количество каналов входного тензора (изображения).\n",
    "\n",
    "В примерах выше мы рассматривали [черно-белые 📚[wiki]](https://ru.wikipedia.org/wiki/%D0%A7%D1%91%D1%80%D0%BD%D0%BE-%D0%B1%D0%B5%D0%BB%D0%B0%D1%8F_%D1%84%D0%BE%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D1%8F) изображения. Их также называют одноканальными изображениями, т. к. в них цвет пикселя определяется одним числом, характеризующим яркость.\n",
    "\n",
    "Хранятся они в двумерном массиве размером $[H,W]$. Цветные изображения хранятся в трехмерных массивах $[H,W,C]$ или $[C,W,H]$, где $C$ — количество цветовых каналов. Для $\\text{RGB}$-изображений $C=3$. Так как `Conv2d` рассчитан на работу с многоканальным входом, то в коде выше нам пришлось написать дополнительный `unsqueeze(0)`, чтобы добавить к тензору с изображением это $3$-е измерение.\n",
    "\n",
    "Важно, что для каждого канала будет создано дополнительное ядро фильтра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ch1 = Conv2d(in_channels=1, out_channels=1, kernel_size=5)\n",
    "print(\"One channel kernel \\t\", conv_ch1.weight.shape)\n",
    "conv_ch3 = Conv2d(in_channels=3, out_channels=1, kernel_size=5)\n",
    "print(\"Three channel kernel \\t\", conv_ch3.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опробуем трехканальную свертку на цветном изображении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/cat.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "cat_in_pil = Image.open(\"cat.jpg\")\n",
    "display(cat_in_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изображение из формата Pillow надо превратить в `torch.Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_in_np = np.array(cat_in_pil)  # pillow -> numpy\n",
    "cat_in_float = cat_in_np.astype(np.float32) / 255  # int->float\n",
    "cat_in_tensor = torch.tensor(cat_in_float)  # np -> torch\n",
    "\n",
    "try:\n",
    "    conv_ch3(cat_in_tensor.unsqueeze(0))  # add batch dimension\n",
    "except Exception as e:\n",
    "    print(\"Error: \\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили ошибку, связанную с количеством каналов. Дело в том, что в PyTorch, в отличие от OpenCV, TensorFlow и ряда других библиотек, каналы (RGB) идут в первом, а не в последнем измерении тензора, описывающего картинку.\n",
    "\n",
    "OpenCV, TensorFlow, Pillow, etc. : $[\\text{Batch}, \\text{Height}, \\text{Width}, \\text{Channels}]$\n",
    "\n",
    "PyTorch : $[\\text{Batch}, \\text{Channels}, \\text{Height}, \\text{Width}]$\n",
    "\n",
    "Придется сделать дополнительное преобразование, чтобы каналы оказались на том месте, где их ожидает PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original \\t\", cat_in_tensor.shape, \"HWC\")\n",
    "cat_in_tensor_channel_first = cat_in_tensor.permute(2, 0, 1)  # HWC -> CHW\n",
    "print(\"Torch style \\t\", cat_in_tensor_channel_first.shape, \"CHW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно подать изображение на вход модели, предварительно добавив batch-размерность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_image_batch = cat_in_tensor_channel_first.unsqueeze(0)\n",
    "out = conv_ch3(one_image_batch)\n",
    "print(\"No error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет необходимости проделывать все эти манипуляции вручную, так как в torchvision реализованы класс `ToTensor` [🛠️[doc]](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html?highlight=totensor#torchvision.transforms.ToTensor) и функция `to_tensor` [🛠️[doc]](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.to_tensor.html), которые выполняют эти преобразования.\n",
    "\n",
    "Убедимся, что тензор, преобразованный нами вручную, и тензор, получившийся после применения функции `to_tensor`, совпали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "cat_in_tensor2 = to_tensor(cat_in_pil)\n",
    "print(cat_in_tensor2.shape)\n",
    "\n",
    "print(\n",
    "    \"Tensor almost equal: \",\n",
    "    torch.allclose(cat_in_tensor_channel_first, cat_in_tensor2),  # float comparsion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на форму выхода, полученного нами ячейкой выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output feature map size:\", out.shape)  # first dim is batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такую размерность имеет выход  **единственного нейрона** в нашем сверточном слое.\n",
    "\n",
    "На входе несколько каналов (3), на выходе остался один канал.\n",
    "Как же комбинируются результаты сверток в разных каналах?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/convolution_rgb.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты сверток всех ядер фильтра с соответствующими входными каналами просто суммируются:\n",
    "\n",
    "$\\large \\displaystyle \\text{feature_map}(x,y) = \\sum_{c}^{C} \\sum_{i}^{H} \\sum_{j}^{W}k_c[i,j]I_c[x+j,y+i] + \\text{bias},$\n",
    "\n",
    "$C$ — количество каналов,\n",
    "\n",
    "$H, W$ — высота и ширина ядра фильтра,\n",
    "\n",
    "$K_c$ — ядро для канала $c$,\n",
    "\n",
    "$I$ — изображение (массив $С\\times H\\times W$),\n",
    "\n",
    "$I_c$ — канал изображения номер $c$ (срез массива $I$, соответствующий каналу изображения номер $c$).\n",
    "\n",
    "В силу коммутативности суммы не важно, в каком порядке будут складываться элементы. Можно считать, что каждый элемент входа сначала умножается на свой коэффициент из ядра, а уже затем все суммируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/rgb_image_convolution_filter.png\" width = \"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так выглядит выход одного нейрона, который задается несколькими ядрами и  смещением (bias).\n",
    "\n",
    "При этом bias  **один** на весь фильтр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kernels\", conv_ch3.weight.shape)\n",
    "print(\"Biases\", conv_ch3.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные данные не всегда будут трехканальными цветными [RGB 📚[wiki]](https://ru.wikipedia.org/wiki/RGB)-изображениями, в которых цвет пикселя определяется тремя числами, характеризующими три основных цвета (красный, зеленый и синий).\n",
    "\n",
    "Входной тензор может иметь произвольное количество каналов. Например: марсоход Opportunity для получения изображений использовал [13 каналов ✏️[blog]](https://habr.com/ru/post/160621/).\n",
    "\n",
    "Более того, в качестве входного тензора можно использовать не изображение, а **карту активаций** предыдущего сверточного слоя.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование нескольких фильтров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращаемся ко второму параметру конструктора `nn.Conv2D` —\n",
    "`out_channels = 1`. Этот параметр задает количество фильтров слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv35 = Conv2d(in_channels=3, out_channels=5, kernel_size=3)\n",
    "out = conv35(cat_in_tensor_channel_first)\n",
    "\n",
    "print(f\"weights shape: {conv35.weight.shape}\")  # 5 filters 3x3x3\n",
    "print(f\"weights shape: {conv35.bias.shape}\")  # one bias per filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В линейном слое каждый нейрон учился активироваться на некий шаблон, например, красную машину или смотрящую направо лошадь.\n",
    "\n",
    "Мы хотим, чтобы нейроны сверточного слоя также активировались на различные паттерны (например, ухо, нос, глаз и т.д.). Для каждого паттерна нам нужен свой нейрон $⇒$ свой фильтр."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/neuron_is_filter.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Каждый нейрон сформирует свою карту признаков размера $1\\times H_{out}\\times W_{out}$. А на выходе слоя будет их конкатенация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"result shape: {out.shape}\")  # 5 feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " При объединении этих карт получится тензор размерности $C_{out}\\times H_{out} \\times W_{out}$, где $C_{out}$ — количество фильтров.\n",
    "\n",
    "На изображении ниже продемонстрирован результат применения сверточного слоя,\n",
    "содержащего $5$ фильтров, к изображению из CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/convolution_layer_with_5_filters.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что, в отличие от полносвязного слоя, свёрточный слой не требует информации о количестве значений во входном представлении и может быть использован как для представлений $C_{in} \\times 32 \\times 32$, так и для $C_{in} \\times 100 \\times 100$. Словом, представления могут иметь практически любой размер, главное, чтобы пространственные размеры не были меньше размеров ядра свёртки.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уменьшение размера карты признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Карта признаков после применения функции активации может быть передана на вход следующей операции свёртки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import relu\n",
    "\n",
    "conv_1 = torch.nn.Conv2d(\n",
    "    in_channels=3,  # Number of input channels (3 for RGB images)\n",
    "    out_channels=6,  # Number of filters/output channels\n",
    "    kernel_size=5,\n",
    ")\n",
    "\n",
    "conv_2 = torch.nn.Conv2d(\n",
    "    in_channels=6,  # Number of input channels (3 for RGB images)\n",
    "    out_channels=10,  # Number of filters/output channels\n",
    "    kernel_size=5,\n",
    ")\n",
    "\n",
    "img = torch.randn((1, 3, 32, 32))  # 1-batch size, 3-num of channels, (32,32)-img size\n",
    "print(f\"img shape: {img.shape}\")\n",
    "\n",
    "out_1 = conv_1(img)\n",
    "print(f\"out_1 shape: {out_1.shape}\")  # [1, 6, 28, 28]\n",
    "\n",
    "out_2 = conv_2(relu(out_1))\n",
    "print(f\"out_2 shape: {out_2.shape}\")  # [1, 10, 24, 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что после свёртки ширина $W_{out}$ и высота $H_{out}$ **карты признаков** будут отличаться от **пространственных размерностей** $W_{in}$ и $H_{in}$ исходного тензора. К примеру, при обработке трёхканального тензора размера $32\\times32$ ядром размера $5\\times5$ можно будет произвести лишь $27$ сдвигов $(32 - 5)$ по вертикали и столько же по горизонтали. Но при этом размер итоговой карты признаков будет равен $28 \\times 28$, поскольку первый ряд (либо столбец) можно получить без сдвигов по вертикали либо горизонтали соответственно. При повторном применении фильтра размер каждой из сторон уменьшится ещё на $4$.\n",
    "\n",
    "Итоговое значение $N'$ пространственной размерности $N$ для квадратного фильтра $K \\times K$ фильтра $F$ вычисляется по следующей формуле:\n",
    "$$\\large N' = N - K + 1$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/decrease_size_of_image_after_convolution.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что при уменьшении размера представлений пиксели, находящиеся около краёв, участвуют в значительно меньшем количестве свёрток, чем пиксели в середине, хотя информация в них не обязательно менее ценна, чем информация из центральных пикселей. К примеру, пиксель в верхнем левом углу представления вне зависимости от размера фильтра будет принимать участие лишь в одной свёртке, и информация о нём будет сохранена лишь в верхнем левом углу нового представления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расширение (padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для борьбы с описанной выше проблемой применяется *набивка/дополнение* входного тензора (англ. *padding*). В ходе него ширина и высота тензора увеличиваются за счёт приписывания столбцов и строк с некоторыми значениями. К примеру, на изображении ниже перед свёрткой ядром размера $3\\times3$ был применён padding нулями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/padding.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На примере убедимся, что это позволит нам сохранить пространственные размерности тензоров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn((1, 1, 5, 5))  # create random image BCHW\n",
    "print(f\"Original tensor:\\nshape:{img.shape}\")\n",
    "conv_3 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "conved_3 = conv_3(img)\n",
    "print(\"Shape after convolution layer(kernel 3x3):\", conved_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Карта признаков меньше, чем вход. Теперь добавим padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zeros to image manually\n",
    "padded_img = torch.zeros((1, 1, 7, 7))  # create zeros array to insert image in center\n",
    "padded_img[:, :, 1:-1, 1:-1] += img  # insert image, we get image arounded by zeros\n",
    "print(f\"\\nPadded tensor:\\nshape:{padded_img.shape}:\\n {padded_img}\")\n",
    "\n",
    "conved_pad_3 = conv_3(padded_img)\n",
    "print(\"\\n\\nPadded shape:\", padded_img.shape)\n",
    "print(\"Shape after convolution with padding(kernel 3x3):\", conved_pad_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер выхода равен размеру входа.\n",
    "\n",
    "Однако если мы увеличим размер ядра до $5\\times5$, то увидим, что, несмотря на padding, выход снова стал меньше входа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_5 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=5)\n",
    "\n",
    "conved_pad_5 = conv_5(padded_img)\n",
    "\n",
    "print(\"Original shape:\", img.shape)\n",
    "print(\"Shape after convolution with padding(kernel 5x5):\", conved_pad_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнение одним рядом и одним столбцом не является универсальным решением: для фильтра размером $5$ размер выходного тензора всё равно отличается от входного. Если мы немного видоизменим полученную выше формулу (используя размер дополнения $P$), то получим:\n",
    "$$\\large N' = N + 2\\cdot P - K + 1$$\n",
    "\n",
    "Для того, чтобы пространственные размеры не изменялись ($N' = N$), для разных размеров фильтра требуются разные размеры дополнения. В общем случае для размера фильтра $F$ требуемый размер дополнения:\n",
    "$$\\large P = \\frac{K-1}{2}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем padding, используя инструменты библиотеки PyTorch, и сравним его с ручным добавлением padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv layer without padding (padding=0 by default)\n",
    "conv_3 = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=0)\n",
    "\n",
    "# conv layer with padding = 1 (add zeros)\n",
    "conv_3_padded = torch.nn.Conv2d(\n",
    "    in_channels=1, out_channels=1, kernel_size=3, padding=1\n",
    ")  # Padding added 1 zeros line to all four sides of the input\n",
    "original = conv_3(padded_img)\n",
    "padded = conv_3_padded(img)\n",
    "\n",
    "print(f\"Explicitly padded:\\n{original.shape}\")\n",
    "print(f\"\\nImplicitly padded:\\n{padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме чисел параметр `padding` может принимать значение `\"same\"` — тогда padding будет рассчитан автоматически так, чтобы размер выходного тензора не отличался от размера входного тензора, или `\"valid\"` — отсутствие дополнения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация работы свертки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/convolution_with_same_padding_rgb_image.gif\" width=\"780\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение свёрточных слоёв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем заменить часть линейных слоев нашей модели на сверточные. Но если решается задача классификации, то последний слой по-прежнему должен быть линейным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку операция свертки является линейной (мы убедились в этом, когда выполняли ее при помощи линейного слоя), то функция активации (например, ReLU) по-прежнему требуется.\n",
    "\n",
    ">*Так как функция активации применяется к тензору поэлементно, не важно, какую именно форму имеет тензор, а значит и какой слой находился перед ней: полносвязный или сверточный.*\n",
    "\n",
    "Простейшая модель для MNIST может выглядеть примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 1\n",
    "input = torch.randn((batch_size, 1, 28, 28))\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=1, out_channels=3, kernel_size=5\n",
    "    ),  # after conv shape: [batch_size,3,24,24]\n",
    "    nn.ReLU(),  # Activation doesn't depend on input shape\n",
    "    nn.Conv2d(\n",
    "        in_channels=3, out_channels=6, kernel_size=3\n",
    "    ),  # after conv shape: [batch_size,6,22,22]\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),  # 6*22*22=2904\n",
    "    nn.Linear(2904, 100),\n",
    "    nn.ReLU(),  # Activation doesn't depend on input shape\n",
    "    nn.Linear(100, 10),  # 10 classes, like a cifar10\n",
    ")\n",
    "\n",
    "out = model(input)\n",
    "print(f\"out shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку полносвязный слой принимает на вход набор векторов, а сверточный — возвращает набор трёхмерных тензоров, нам нужно превратить эти тензоры в вектора. Для этого используется объект класса `nn.Flatten` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten).\n",
    "Он преобразовывает данные на входе в вектор, сохраняя при этом первое (batch) измерение.\n",
    "\n",
    "Ниже примеры других функций, которыми можно выполнить аналогичное преобразование:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn((16, 3, 32, 32))\n",
    "\n",
    "batch_size = input.shape[0]\n",
    "\n",
    "print(\"class Flatten\\t\", nn.Flatten()(input).shape)\n",
    "print(\n",
    "    \"view \\t\\t\", input.view(batch_size, -1).shape\n",
    ")  # data stay in same place in memory\n",
    "print(\"reshape \\t\", input.reshape(batch_size, -1).shape)  # data may be moved\n",
    "print(\"method flatten \\t\", input.flatten(1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рецептивные поля нейронов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросетевая модель из предыдущего примера позволяет в общем случае понять структуру свёрточных нейронных сетей: после некоторого количества свёрточных слоёв, извлекающих локальную пространственную информацию, идут полносвязные слои (как минимум в количестве одного), сопоставляющие извлечённую информацию.\n",
    "\n",
    "Внутри свёрточных слоёв происходит следующий процесс: первые слои нейронных сетей имеют малые рецептивные поля, т. е. им соответствует малая площадь на исходном изображении. Такие нейроны могут активироваться лишь на некоторые простые шаблоны (по типу углов или освещённости).\n",
    "\n",
    "Нейроны следующего слоя уже имеют большие рецептивные поля, в результате чего в картах активации появляется информация о более сложных паттернах. С каждым слоем свёрточной нейронной сети рецептивное поле нейронов увеличивается. Увеличивается и сложность шаблонов, на которые может реагировать нейрон. В последних слоях рецептивное поле нейрона должно быть размером со всё исходное изображение. Пример можно увидеть на схеме ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/receptive_field_size.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если на первом слое рецептивное поле имело размер $K \\times K$, то после свёртки фильтром $K\\times K$ оно стало иметь размер $(2K-1) \\times (2K-1)$, то есть увеличилось на $K-1$ по каждому из направлений. Несложно самостоятельно убедиться, что данная закономерность сохранится при дальнейшем применении фильтров того или иного размера.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако при обработке больших изображений нам потребуется очень много слоев, чтобы нейрон \"увидел\" всю картинку.\n",
    "\n",
    "К примеру, для изображения $1024\\times1024$ понадобится сеть глубиной $\\approx510$ сверточных слоев.\n",
    "\n",
    "Такая модель потребует огромного количества памяти и вычислительных ресурсов.\n",
    "Чтобы избежать этого, будем сами уменьшать размеры карт признаков, при этом рецептивные поля нейронов будут расти.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг свёртки (Stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До этого мы двигали фильтр на один пиксель, то есть перемещались с шагом (stride) один.\n",
    "\n",
    "Если двигать фильтр с большим шагом, то размер выходной карты признаков (feature map) будет уменьшаться кратно шагу, и рецептивные поля нейронов будут расти быстрее.\n",
    "\n",
    "Для изменения шага свертки в конструкторе `nn.Conv2d` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) есть параметр `stride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 5, 5)\n",
    "conv_s1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=(1, 1))\n",
    "conv_s2 = nn.Conv2d(1, 3, 3, stride=2)  # bypass par. names, stride = (2, 2)\n",
    "\n",
    "out_stride1 = conv_s1(dummy_input)\n",
    "out_stride2 = conv_s2(dummy_input)\n",
    "\n",
    "print(\"Out with stride 1\", out_stride1.shape)\n",
    "print(\"Out with stride 2\", out_stride2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/convolution_parameter_stride.gif\" width=\"350\"></center>\n",
    "<center><em>Свёртка массива $5\\times5$ фильтром размером $3\\times3$ с шагом $2$ по вертикали и горизонтали.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом важно заметить, что в некоторых случаях часть данных может не попасть в свёртку. К примеру, при $N = 7,\\, K = 3,\\, S = 3$. В данном случае: $$\\large N' = 1 + \\frac{7 - 3}{3} = 2\\frac13.$$\n",
    " В подобных ситуациях часть изображения не захватывается, в чём мы можем убедиться на наглядном примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch tensor 7x7\n",
    "# fmt: off\n",
    "input = torch.tensor([[[[1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99],\n",
    "                        [1, 1, 1, 1, 1, 1, 99]]]], dtype=torch.float)\n",
    "# fmt: on\n",
    "\n",
    "print(f\"input shape: {input.shape}\")\n",
    "\n",
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=1,  # Number of channels\n",
    "    out_channels=1,  # Number of filters\n",
    "    kernel_size=3,\n",
    "    stride=3,\n",
    "    bias=False,  # Don't use bias\n",
    ")\n",
    "conv.weight = torch.nn.Parameter(\n",
    "    torch.ones((1, 1, 3, 3))\n",
    ")  # Replace random weights to ones\n",
    "out = conv(input)\n",
    "\n",
    "print(f\"out shape: {out.shape}\")\n",
    "print(f\"out:\\n{out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что столбец с числами $99$ просто не попал в свертку.\n",
    "Поэтому на практике подбирают padding таким образом, чтобы при `stride = 1`  размер карты признаков на выходе был равен входу, а затем делают свертку со `stride = 2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Казалось бы, с увеличением шага $S$ рецептивное поле не выросло — как увеличивалось с $1$ до $K$, так и увеличивается. Однако обратим внимание на другое: если раньше размерность $N$ становилась $N - F + 1$, то теперь она станет $\\displaystyle 1 + \\frac{N-F}{S}$.\n",
    "\n",
    "В результате если раньше следующий фильтр с размером $K'$ имел рецептивное поле:$$\\displaystyle N \\cdot \\frac{K'}{N'} = N \\cdot \\frac{K'}{N - F + 1},$$\n",
    "\n",
    "то теперь: $$\\displaystyle N \\cdot \\frac{K'}{N'} = N \\cdot \\frac{K'}{1 + \\frac{N-F}{S}}.$$\n",
    "\n",
    "Понятно, что $$\\displaystyle \\frac{K'}{N - F + 1} \\leq \\frac{K'}{1 + \\frac{N-F}{S}},$$ поэтому рецептивное поле каждого нейрона увеличивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уплотнение (Субдискретизация, Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим вариантом стремительного увеличения размера рецептивного поля является использование дополнительных слоёв, требующих меньшего количества вычислительных ресурсов. Слои субдискретизации прекрасно выполняют эту функцию: подобно свёртке производится разбиение изображения на небольшие сегменты, внутри которых выполняются операции, не требующие использования обучаемых весов. Два популярных примера подобных операций: получение максимального значения (max pooling) и получение среднего значения (average pooling).\n",
    "\n",
    "\n",
    "Аналогично разбиению на сегменты при свёртке, слои субдискретизации имеют два параметра: размер фильтра $K$ (то есть, каждого из сегментов) и шаг $S$ (stride). Аналогично свёрткам, при применении субдискретизации формула размера стороны:\n",
    "$$N' = 1+ \\frac{N-K}{S}.$$\n",
    "\n",
    "Ниже приведён пример использования операций max pooling и average pooling при обработке массива."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/subdiscretization_pooling.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем это в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor 4x4\n",
    "# fmt: off\n",
    "input = torch.tensor([[[[1, 1, 2, 4],\n",
    "                        [5, 6, 7, 8],\n",
    "                        [3, 2, 1, 0],\n",
    "                        [1, 2, 3, 4]]]], dtype=torch.float)\n",
    "# fmt: on\n",
    "\n",
    "max_pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "print(\"Input:\\n\", input)\n",
    "print(\"Max pooling:\\n\", max_pool(input))\n",
    "print(\"Average pooling:\\n\", avg_pool(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно отметить**, что субдискретизация выполняется по каждому из каналов отдельно, в результате чего количество каналов не меняется, в отличие от применения фильтра при свёртке. К примеру, ниже можно увидеть визуализацию применения max pooling к одному из каналов тензора, имеющего $64$ канала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/changing_size_of_image_after_pooling.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свёртка фильтром $1\\times1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью субдискретизации и свертки с шагом больше единицы мы можем регулировать пространственные размеры (ширину и высоту) карты признаков.\n",
    "\n",
    "\n",
    "Обычно если хотят уменьшить количество каналов в карте признаков, то используют свертку с размерами ядра $1\\times1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически такая свертка — это линейный слой, на вход которому подали все признаки из одной точки входа.\n",
    "\n",
    "Когда переводим цветное изображение в градации серого, мы делаем похожую операцию: складываем все каналы с коэффициентом $\\displaystyle\\frac{1}{3}$:\n",
    "\n",
    "$\\text{Brightness} = \\dfrac{1}{3} R + \\dfrac{1}{3} G + \\dfrac{1}{3}B $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/1_times_1_convolutions_featere_maps.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество каналов можно регулировать при помощи параметра `out_channels`. Если при этом количество каналов уменьшается, то таким образом мы обобщаем признаки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/1_times_1_convolution.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом количество таких фильтров $1\\times1$ может быть произвольным. Обычно свертку $1\\times1$ применяют для уменьшения числа каналов, но и обратная ситуация тоже возможна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведён пример применения такого фильтра с целью снижения количества карт признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv2d(\n",
    "    in_channels=64,  # Number of input channels\n",
    "    out_channels=32,  # Number of filters\n",
    "    kernel_size=1,\n",
    ")\n",
    "\n",
    "input = torch.randn((1, 64, 56, 56))\n",
    "out = conv(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Shape after 1x1 conv:\", out.shape)  # [1, 32, 56, 56] batch, C_out, H_out, W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительная информация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оба упомянутых выше метода позволяют сделать архитектуру сети не слишком глубокой путём быстрого увеличения рецептивного поля нейронов, что позволяет уменьшить число обучаемых параметров модели. Познакомимся с ещё одним способом уменьшения числа обучаемых параметров модели.\n",
    "\n",
    "\n",
    "Рассмотрим фрагмент архитектуры CNN, состоящий из одного свёрточного слоя с размерами ядра свёртки $F_h\\times F_w$ и некоторой активации (например, `torch.nn.ReLU` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)):\n",
    "\n",
    "$$\\large ... \\rightarrow (N, C_{in}, H, W) \\rightarrow \\text{conv2d}_{F_h\\times F_w} \\rightarrow \\text{ReLU} \\rightarrow  (N, C_{out}, H', W') \\rightarrow ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как обучаемыми параметрами являются элементы ядра свёртки и сдвиг (bias), число таких параметров очень легко посчитать:\n",
    "* для формирования одной выходной карты признаков, как мы уже подробно обсуждали выше, нам нужно свернуть все входные карты признаков с соответствующими им матрицами элементов ядра свёртки, сложить результаты вместе и добавить bias — то есть в формировании одной выходной карты признаков участвуют $C_{in} \\cdot F_{h} \\cdot F_w + 1$ обучаемых параметров.\n",
    "* чтобы получить $C_{out}$ выходных карт признаков, мы столько  же раз должны повторить описанную выше процедуру с разными $C_{in} \\cdot F_{h} \\cdot F_w + 1$  параметрами.\n",
    "Таким образом, **общее число обучаемых параметров в одном свёрточном слое:**\n",
    "\n",
    "$$\\large \\text{n_params}[\\text{conv2d}_{F_h \\times F_w}] = (C_{in} \\cdot F_{h} \\cdot F_w + 1) \\cdot C_{out}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем значительно уменьшить число обучаемых параметров, внеся небольшое изменение в рассмотренную архитектуру. Перед применением свёрточного слоя с размером ядра $F_h \\times F_w$ мы можем расположить ещё один свёрточный слой с ядром свёртки из одного единственного пространственного элемента ($1 \\times 1$), который будет предназначен для уменьшения числа карт признаков перед подачей последующему свёрточному слою без изменений пространственных размеров $H$ и $W$:\n",
    "\n",
    "$$\\large ... \\rightarrow (N, C_{in}, H, W)  \\rightarrow \\text{conv2d}_{1 \\times 1} \\rightarrow \\text{ReLU} \\rightarrow (N, C_{mid}, H, W) \\rightarrow \\\\\n",
    "\\large \\rightarrow (N, C_{mid}, H, W) \\rightarrow \\text{conv2d}_{F_h\\times F_w} \\rightarrow \\text{ReLU} \\rightarrow  (N, C_{out}, H', W')  \\rightarrow ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея заключается в следующем: рассматривая набор входных карт признаков  $C_{in} \\times (H \\times W)$, можно выделить вектор размерностью $C_{in}$, содержащий элементы карт признаков с некоторыми фиксированными пространственными координатами. Элементы этого вектора сообщают, насколько сильно рецептивное поле соответствует каждому из $C_{in}$ шаблонов. Применение к входным картам признаков свёрточного слоя с ядром $1 \\times 1 $ и последующей активации приведёт к нелинейному преобразованию таких векторов из пространства размерности $C_{in}$ в новое пространство размерности $C_{mid}$. Так как параметры такого сжимающего преобразования будут подбираться в процессе обучения, мы ожидаем, что свёртка $1 \\times 1$ позволит подобрать полезные комбинации входных карт признаков для всех пространственных элементов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если выбрать $C_{mid} < C_{in}$, то общее число параметров модели действительно уменьшится:\n",
    "\n",
    "$$\\large \\text{n_params}[\\text{conv2d}_{1 \\times 1} \\rightarrow \\text{conv2d}_{F_h \\times F_w}] = \\\\\n",
    "\\large = (C_{in}\\cdot 1\\cdot 1 + 1) \\cdot C_{mid} + (C_{mid} \\cdot F_{h} \\cdot F_w + 1) \\cdot C_{out} \\approx \\frac{C_{mid}}{C_{in}}  \\text{n_params}[\\text{conv2d}_{F_h \\times F_w}] $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение свёрточного и полносвязного слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте оценим количество ресурсов, которое требуется для обработки одного изображения из CIFAR-10 при помощи сверточного и полносвязного слоя.\n",
    "\n",
    "Пусть сверточный слой будет содержать $6$ фильтров размером $3 \\times 3 $, `padding = 1`, `stride = 1`, а полносвязный — $6$ выходов (как если бы мы делали классификацию $6$-ти классов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/conv_vs_linear.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сколько обучаемых праметров (весов) у свёрточного слоя?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество параметров в одном фильтре: $C_{in}\\times K_{h}\\times K_{w} +1 = 3 \\times 3 \\times 3 + 1 = 28$\n",
    "\n",
    "Количество фильтров $C_{out} = 6$\n",
    "\n",
    "Итого: $(C_{in}\\times K_{h}\\times K_{w} +1) \\times C_{out} = 28 \\times 6 = 168$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d\n",
    "\n",
    "\n",
    "def get_params_count(module):\n",
    "    weights_count = 0\n",
    "    # Get all model weights: kernels + biases\n",
    "    for p in module.parameters():\n",
    "        print(p.shape)\n",
    "        # torch.prod - multiply all values in tensor\n",
    "        weights_count += torch.tensor(p.shape).prod()\n",
    "    print(\"Total weights\", weights_count.item())\n",
    "\n",
    "\n",
    "conv = Conv2d(3, 6, 3, bias=True)\n",
    "get_params_count(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сколько обучаемых праметров у полносвязного слоя?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Данные вытягиваем в вектор:\n",
    "\n",
    "$\\text{inputs_count} = C_{in} \\times H_{in} \\times W_{in}  = 3*32*32 = 3072$\n",
    "\n",
    "2. Каждый нейрон (их $6$ шт.) выходного слоя хранит вес для каждого элемента входа ($3072$) и еще одно смещение:\n",
    "\n",
    "$(\\text{inputs_count} + 1) \\times \\text{outputs_count} = (3072 + 1) \\times 6 = 18\\ 438$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "linear = Linear(3072, 6, bias=True)\n",
    "get_params_count(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть для хранения весов такого линейного слоя нужно в $\\approx 100$ раз больше памяти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сколько вычислительных ресурсов требуется полносвязному слою?\n",
    "\n",
    "*Считаем только умножения, т. к. (умножение + сложение = 1 [FLOP 📚[wiki]](https://en.wikipedia.org/wiki/FLOPS)).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В полносвязном слое каждый вход умножается на свой вес один раз и количество умножений совпадает с количеством весов за вычетом сложения со смещением:\n",
    "\n",
    " $C_{in} × H_{in} × W_{in} × \\text{outputs_count}  = 3 × 32 \\times 32 × 6 = 18 \\ 432 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сколько вычислительных ресурсов требуется свёрточному слою?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Разовое применение фильтра эквивалентно применению линейного слоя с таким же количеством весов:\n",
    "\n",
    "$C_{in} \\times K_{h} \\times K_{w} \\times C_{out} = 3 \\times 3 \\times 3 \\times 6 = 162$\n",
    "\n",
    "Т. е. умножаем каждый вес фильтра на вход.\n",
    "\n",
    "2. Сдвигаем фильтр и повторяем п. 1 для каждой точки на карте признаков:\n",
    "\n",
    "$C_{in} \\times K_{h} \\times K_{w} \\times C_{out} \\times H_{out} \\times W_{out}   = 162 \\times 30 \\times 30  = 145\\ 800 $\n",
    "\n",
    "То есть количество операций в $\\approx 10$ раз больше, чем у полносвязного слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**:\n",
    "выигрыш по количеству параметров при использовании свёрточного слоя омрачается большим количеством операций перемножения. Это было проблемой в течение долгого времени, пока вычисление операции свёртки не перевели на видеокарты (Graphical Processing Unit). При выполнении свёртки одного сегмента не требуется информация о результатах свёртки на другом сегменте, поэтому данные операции можно выполнять параллельно, с чем как раз прекрасно справляются видеокарты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Conv2d(in_channels=1, out_channels=6, kernel_size=3)\n",
    "model.to(device)  # send model to device\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 5, 5)\n",
    "out = model(dummy_input.to(device))  # send data to GPU too!\n",
    "# ... do backprop if need\n",
    "out = out.cpu()  # move data back to main memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая структура свёрточной нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем более детально взглянуть на типичную архитектуру свёрточной нейронной сети. Как ранее уже обсуждалось, в первую очередь необходимо последовательностью свёрточных слоёв и уплотнений достичь того, чтобы каждый элемент карты активации имел большое рецептивное поле, а значит мог отвечать за большие и сложные шаблоны. Затем данные карты активаций выпрямляются в вектора и передаются в полносвязные слои, последовательность которых, используя глобальную информацию, возвращает значение целевой переменной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/neural_network_architecture.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате количество признаков уменьшается от слоя к слою."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet: пример архитектуры сверточной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/lenet_architecture.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примером сети, построенной по такой архитектуре, является LeNet.\n",
    "Она была разработана в 1989 г. [Яном Ле Куном 📚[wiki]](https://en.wikipedia.org/wiki/Yann_LeCun). Сеть имела 5 слоев с обучаемыми весами, из них 2 — сверточные.\n",
    "\n",
    "Применялась в США для распознавания рукописных чисел на почтовых конвертах до начала 2000 г.\n",
    "\n",
    "[[doc] 🛠️ LeNet PyTorch documentation](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс обучения почти не отличается от обучения полносвязной сети, используются такие же оптимизатор и функция потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Другие виды сверток\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сигнал, который мы обрабатываем при помощи сверточного слоя, не обязательно должен быть картинкой и не обязательно он должен быть двумерным.\n",
    "\n",
    "В качестве примера такого сигнала может выступать звук:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "dataset = torchaudio.datasets.YESNO(\"./\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Signal value\")\n",
    "\n",
    "waveform, sample_rate, label = dataset[0]\n",
    "\n",
    "plt.plot(waveform.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch одномерная свертка задается аналогично двумерной — `torch.nn.Conv1d` [🛠️[doc]](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.nn.Conv1d(in_channels, out_channels,             kernel_size, stride=1,  padding=0, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "conv = nn.Conv1d(1, 16, 3, stride=2)\n",
    "output = conv(waveform)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но такой сигнал чаще преобразуют в спектрограмму, а к ней уже можно применить 2D-свёртку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from torchaudio.transforms import Spectrogram\n",
    "\n",
    "\n",
    "spec_obj = Spectrogram(power=2, center=True, pad_mode=\"reflect\")\n",
    "spec = spec_obj(waveform[0])\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.imshow(librosa.power_to_db(spec))\n",
    "plt.title(\"Spectrogram\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим примером могут являться, например, **спектрограммы растворов**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример работы со спектрограммами растворов в работе выпускника курса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В работе **Fluorescent multimodal nanosensorof heavy metal ions based on carbon dots** исследовалась проблема контроля изменения содержания тяжелых металлов в жидких средах методом оптической спектроскопии. В качестве входных данных использовались спектрограммы растворов:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/s_spectr.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/EduNetArchive/Sarmanova_CD_HM_sensor\">EduNet-archive: CD HM sensor</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы воздействуем на образец светом с разной длиной волны, а фиксируем суммарную интенсивность излучения, то для обработки таких данных можно использовать 1D-свёртку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/s_spectr2.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://github.com/EduNetArchive/Sarmanova_CD_HM_sensor\">EduNet-archive: CD HM sensor</a></em></center>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы можем зафиксировать отклик на разных длинах волн, то используем 2D-свёртку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее:\n",
    "\n",
    "[[video] 📺 Видеозапись выступления](https://www.youtube.com/watch?v=b1HEBjWelys)\n",
    "\n",
    "[[slides] 📊 Презентация](https://docs.google.com/presentation/d/1qBizkCjiv2UsAnfdvN7yyOXY7QD0Ni7i/edit#slide=id.p1)\n",
    "\n",
    "[[git] 🐾 Код](https://github.com/EduNetArchive/Sarmanova_CD_HM_sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дополнительная информация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как выглядит операция свертки в функциональном анализе.\n",
    "\n",
    "Есть две различные функции, определяющие локальную \"схожесть\" функций $f(t)$ и $g(t)$:\n",
    "- взаимнокорреляционная функция обозначается пятиконечной звездой <font size=\"8\">$\\large ⋆$</font> и определяет схожесть двух функций:\n",
    "$$(f ⋆ g)(t)\\stackrel{def}{=}  \\int\\limits_{-\\infty}^{\\infty}  f(\\tau)g(t+\\tau)d\\tau$$\n",
    "- свертка обозначается звездочкой (астериском) <font size=\"8\">$*$</font> и определяет схожесть одной функции и \"отраженной\" другой функции:\n",
    "$$\\large(f * g)(t)\\stackrel{def}{=}  \\int\\limits_{-\\infty}^{\\infty}  f(\\tau)g(t-\\tau)d\\tau$$\n",
    "\n",
    "Взаимная корреляция более интуитивно понятна: она представляет собой \"наложение\" шаблона на функцию, а свертка &mdash; \"наложение\" отраженного шаблона. Эти функции взаимосвязаны:\n",
    "$$f(t) ⋆ g(t) = f(-t) * g(t)$$\n",
    "\n",
    "Можно представить свертку как площадь произведения двух функций внутри скользящего окна, как на анимации ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/1d_convolution.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В машинном обучении под словом **convolution** как правило [подразумевают **взаимнокорреляционную функцию** ✏️[blog]](https://glassboxmedicine.com/2019/07/26/convolution-vs-cross-correlation/), а не свертку. В реальности при обучении нейронной сети совершенно неважно, используется ли свертка или взаимнокорреляционная функция — они отличаются лишь порядком расположения весов внутри тензора ядра.\n",
    "\n",
    "В случае дискретных величин для вычисления взаимной корреляции сигнал $f(t)$ поэлементно умножается со смещенным ядром $g(t)$, и результат суммируется:\n",
    "\n",
    "$$\\large(f \\star g)(t) = f(1)g(t+1) + f(2)g(t+2) + f(3)g(t+3) + ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одномерная операция свертки используется для данных, имеющих последовательную структуру: текстов, аудиозаписей, цифровых сигналов. Как правило, такую структуру можно представить в виде изменения величины с течением времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные связаны в одном измерении — временном. Их тоже можно обработать при помощи свертки, но уже в одном измерении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свертка через перемножение матриц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проводить вычисления при помощи вложенных циклов малоэффективно. Но операцию свертки можно реализовать через матричное умножение, которое очень эффективно выполняется на GPU\n",
    "\n",
    "[[git] 🐾 Пример для 1D данных](https://github.com/Gan4x4/ml_snippets/blob/main/CV/Convolution_and_matrix_multiplication.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для двумерной свертки действует похожая логика, подробнее можно прочесть здесь:\n",
    "\n",
    "[[blog] ✏️ 2D Convolution as a Matrix-Matrix Multiplication](https://www.baeldung.com/cs/convolution-matrix-multiplication)\n",
    "\n",
    "И при получении градиентов это тоже работает:\n",
    "\n",
    "[[blog] ✏️ Forward and Backward Convolution Passes as Matrix Multiplication](https://danieltakeshi.github.io/2019/03/09/conv-matmul/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Двумерная операция свертки, о которой мы много говорили, применяется для обработки данных, имеющих пространственную структуру, то есть тех данных, для которых играют роль взаимные расположения по двум осям. Совсем не обязательно, чтобы эти оси соответствовали высоте и ширине картинки. Например, одна ось может соответствовать координате сенсора в одномерной матрице, а вторая — времени получения информации с него.\n",
    "\n",
    "Трёхмерная операция свертки используется, когда данные имеются три независимых \"пространственных\" компоненты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Видеопоток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейшим примером является видео: к двумерной структуре самих изображений добавляется координата времени."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/3d_convolution.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    " torch.nn.Conv3d(in_channels,\n",
    "                  out_channels,\n",
    "                  kernel_size,\n",
    "                  stride=1,\n",
    "                  padding=0,\n",
    "                  dilation=1,\n",
    "                  groups=1,\n",
    "                  bias=True,\n",
    "                  padding_mode='zeros')\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# With cubic kernels and same stride\n",
    "conv = nn.Conv3d(in_channels=16, out_channels=33, kernel_size=3, stride=2)\n",
    "\n",
    "# non-square kernels with unequal stride and padding\n",
    "conv = nn.Conv3d(\n",
    "    in_channels=16,\n",
    "    out_channels=33,\n",
    "    kernel_size=(3, 5, 2),\n",
    "    stride=(2, 1, 1),\n",
    "    padding=(4, 2, 0),\n",
    ")\n",
    "\n",
    "input = torch.randn(20, 16, 10, 50, 100)\n",
    "out = conv(input)\n",
    "\n",
    "print(\"out shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Медицинские 3D снимки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другой популярный тип 3D-данных — это медицинские снимки (KT, МРТ). Для их хранения используются специальные форматы ([NIFTI ✏️[blog]](https://brainder.org/2012/09/23/the-nifti-file-format/), [DICOM 📚[wiki]](https://en.wikipedia.org/wiki/DICOM)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с ними существуют специальные библиотеки. Воспользуемся одной из них — библиотекой [NiBabel 🛠️[doc]](https://nipy.org/nibabel/coordinate_systems.html), чтобы преобразовать файл из формата NIFTI в 3D-массив. Для этого скачаем образец МРТ мозга:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -q https://nipy.org/nibabel/_downloads/f76cc5a46e5368e2c779868abc49e497/someones_epi.nii.gz\n",
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/someones_epi.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И откроем его при помощи NiBabel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "epi_img = nib.load(\"someones_epi.nii.gz\")\n",
    "epi_img_data = epi_img.get_fdata()\n",
    "print(epi_img_data.shape)\n",
    "print(\"Max\", epi_img_data.max(), \"Min\", epi_img_data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно воспринимать этот массив как набор черно-белых изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_slices(ax, data):\n",
    "    slices = np.linspace(0, len(data) - 1, num=10).astype(int)\n",
    "    for i, sl in enumerate(slices):\n",
    "        ax[i].axis(\"off\")\n",
    "        ax[i].imshow(data[sl], cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 10, figsize=(16, 4))\n",
    "\n",
    "show_slices(axes[0], epi_img_data)\n",
    "show_slices(axes[1], np.moveaxis(epi_img_data, 0, 1))\n",
    "show_slices(axes[2], np.moveaxis(epi_img_data, 0, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем массив в `torch.Tensor`. При этом добавим размерность для каналов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "brain_mrt = torch.Tensor(epi_img_data)\n",
    "brain_mrt = brain_mrt.unsqueeze(0)  # add channel dim\n",
    "print(\"Add channel dim\", brain_mrt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно подать его на вход `Conv3d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "conv3d = nn.Conv3d(\n",
    "    in_channels=1,\n",
    "    out_channels=16,\n",
    "    kernel_size=(3, 3, 3),\n",
    "    stride=(1, 1, 1),\n",
    "    padding=(1, 1, 1),\n",
    ")\n",
    "\n",
    "out = conv3d(brain_mrt.unsqueeze(0))  # add batch dim and run inference\n",
    "\n",
    "print(\"out shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Графовые свертки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые данные естественно хранить в виде графа:\n",
    "\n",
    "- Cтруктура молекул (атомы — вершины,  связи между ними — ребра)\n",
    "- 3D объекты (mesh)\n",
    "- Cвязи: соц. сети, цитаты\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/molecule.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от картинок и текстов, количество соседей у вершины в таких графах может быть произвольным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/graph_vs_image_and_sequence.png\" width=\"900\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с графами можно использовать пакет [PyTorch Geometric 🛠️[doc]](https://pytorch-geometric.readthedocs.io/en/latest/). Установим его:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим простой граф для демонстрации.\n",
    "Сначала создаем список вершин. Для простоты каждая вершина будет содержать одно число. Но в общем случае вершина может содержать любую информацию, чаще в виде вектора чисел. Размер эмбеддингов у всех вершин графа должен совпадать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[0], [1], [2], [3], [4], [5]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ребра между вершинами можно задать:\n",
    "\n",
    "* в виде матрицы смежности ([adjacency matrix 📚[wiki]](https://en.wikipedia.org/wiki/Adjacency_matrix));\n",
    "\n",
    "* в виде списка пар вершин, между которыми есть ребра ([CCO 🛠️[doc]](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html) of coordinate list format).\n",
    "\n",
    "Второй способ компактнее, воспользуемся им. Число в первой строке — это **номер** исходящей вершины, число во второй (с тем же индексом) — номер входящей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "edge_index = torch.tensor([[0, 1,  2, 2, 3, 4, 4],\n",
    "                           [1, 2,  3, 4, 5, 2, 5]], dtype=torch.long)\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch Geometric для работы с графами используется класс `Data` [🛠️[doc]](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data). Инициализируем экземпляр этого класса списком вершин и ребер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "data.validate(raise_on_error=True)  # optional check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация графа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации созданного графа используем  пакет [NetworkX 🛠️[doc]](https://networkx.org/).\n",
    "Он совместим с [PyTorch Geometric 🛠️[doc]](https://pytorch-geometric.readthedocs.io/en/latest/).\n",
    "\n",
    "[[doc] 🛠️ Drawing — basic functionality for visualizing graphs](https://networkx.org/documentation/networkx-1.10/reference/drawing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "nx.draw(to_networkx(data, to_undirected=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам понадобится код для визуализации, поэтому поместим его в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.layout import kamada_kawai_layout\n",
    "\n",
    "\n",
    "def show_graph(graph, colors=None, embeddings=False):\n",
    "    fs = 14\n",
    "    int2label = {}\n",
    "    g = to_networkx(graph, to_undirected=False)  #\n",
    "    if embeddings:\n",
    "        for i, e in enumerate(graph.x):\n",
    "            str_emb = [\"{0:0.1f}\".format(p.item()) for p in e]\n",
    "            int2label[i] = f\"{i}: [\" + \",\".join(str_emb) + \"]\"\n",
    "        g = nx.relabel_nodes(g, int2label)\n",
    "        fs = 10\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    nx.draw_networkx(\n",
    "        g,\n",
    "        pos=kamada_kawai_layout(\n",
    "            g, dim=2, scale=1, center=None\n",
    "        ),  # nx.spring_layout(G, seed=0),\n",
    "        with_labels=True,\n",
    "        node_size=800,\n",
    "        node_color=colors,  # data.y, for clustering\n",
    "        # cmap=\"hsv\",\n",
    "        # vmin=-2,\n",
    "        # vmax=3,\n",
    "        # width=0.8,\n",
    "        edge_color=\"grey\",\n",
    "        font_size=fs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На графах тоже можно выполнять операцию свертки. Разберем классический алгоритм из статьи [Semi-Supervised Classification with Graph Convolutional Networks 🎓[arxiv]](https://arxiv.org/abs/1609.02907).\n",
    "\n",
    "[[doc] 🛠️ GNN Cheatsheet](https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/GCN_scheme.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе у графового сверточного слоя новые эмбеддинги для всех вершин.\n",
    "Новый эмбеддинг считается как взвешенная сумма скалярных произведений эмбеддингов соседних вершин на матрицу весов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это напоминает свертку $1\\times1$ для изображений с последующей субдискретизацией (pooling). Основное отличие в том, что **количество соседей** у разных вершин может отличаться, и в сумме будет получаться разное количество слагаемых. Поэтому необходимо их взвесить (множители $d_i$ на картинке.). Если все $d_i = 1$, то графовая свертка вычисляется по той же формуле:\n",
    "\n",
    "$$\\large y_i = \\sum_{i \\in N}  x_i*W,$$\n",
    "\n",
    "где $N$ — множество номеров соседей $i$-той вершины,\n",
    "\n",
    "$x_i$ — эмбеддинг $i$-той вершины,\n",
    "\n",
    "$W$ — матрица весов, **одна для всех вершин**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот как выглядят соседи каждой вершины, которые будут участвовать в свертках. **Соседняя вершина — эта та, из которой есть путь в рассматриваемую**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 4))\n",
    "plt.subplot(1, 6, 1).set_title(\"GC Node0\")\n",
    "show_graph(data, [\"green\", \"gray\", \"gray\", \"gray\", \"gray\", \"gray\"])\n",
    "plt.subplot(1, 6, 2).set_title(\"GC Node1\")\n",
    "show_graph(data, [\"lightgreen\", \"green\", \"gray\", \"gray\", \"gray\", \"gray\"])\n",
    "plt.subplot(1, 6, 3).set_title(\"GC Node2\")\n",
    "show_graph(data, [\"gray\", \"lightgreen\", \"green\", \"gray\", \"lightgreen\", \"gray\"])\n",
    "plt.subplot(1, 6, 4).set_title(\"GC Node3\")\n",
    "show_graph(data, [\"gray\", \"gray\", \"lightgreen\", \"green\", \"gray\", \"gray\"])\n",
    "plt.subplot(1, 6, 5).set_title(\"GC Node4\")\n",
    "show_graph(data, [\"gray\", \"gray\", \"lightgreen\", \"gray\", \"green\", \"gray\"])\n",
    "plt.subplot(1, 6, 6).set_title(\"GC Node5\")\n",
    "show_graph(data, [\"gray\", \"gray\", \"gray\", \"lightgreen\", \"lightgreen\", \"green\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch Geometric графовый сверточный слой реализуется классом `GCNConv`[🛠️[doc]](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html#torch_geometric.nn.conv.GCNConv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "gcn = GCNConv(in_channels=1, out_channels=3)\n",
    "print(gcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`in_channels` — это размерность эмбеддинга вершины на входе, а `out_channels` — на выходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in gcn.named_parameters():\n",
    "    print(name, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически `out_channels` — это количество столбцов в матрице W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как работает слой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку нам нужны эмбеддинги, заменим значения $x$, которые были просто равны номеру вершины, на `one_hot` векторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "embeddings = one_hot(x.flatten().long()).float()\n",
    "data = Data(x=embeddings, edge_index=edge_index)\n",
    "data.validate(raise_on_error=True)\n",
    "\n",
    "show_graph(data, embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим графовую свертку с одним нейроном средствами PyTorch Geometric.\n",
    "Для начала отключим все дополнительные опции и инициализируем веса единицами, чтобы было проще понять, как она работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCNConv(len(x), 1, add_self_loops=False, bias=False, normalize=False)\n",
    "gcn.lin.weight.data = torch.ones((1, len(x)))\n",
    "print(gcn, \" weights \", gcn.lin.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = gcn(embeddings, edge_index)\n",
    "print(out)  # Embedding (dim=1) for every node from 0 ... 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта операция соответствует перемножению эмбеддингов вершин на веса  и сумму по всем соседям:\n",
    "\n",
    "$$\\large y_i = \\sum_{j \\in N_i} \\bar x_j \\cdot \\bar w,$$\n",
    "\n",
    "где $N_i$ — это множество соседей $i$-й вершины (соседи — это  вершины, **из которых есть путь в текущую**).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проделаем ее самостоятельно.\n",
    "Эмбеддинг каждой вершины умножается на одну и ту же матрицу весов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = embeddings @ gcn.lin.weight.data.T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(Data(x=y, edge_index=edge_index), embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные после перемножения результы суммируются для соседей каждой вершины.\n",
    "\n",
    "В PyTorch Geometric эта операция реализуется пакетом `torch_scatter` [🛠️[doc]](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/scatter.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc = torch.zeros_like(y)  # out custom\n",
    "\n",
    "oc[0] = 0  # no nighbors\n",
    "oc[1] = y[0]  # one neighbor (#0)\n",
    "oc[2] = y[1] + y[4]\n",
    "oc[3] = y[2]\n",
    "oc[4] = y[2]\n",
    "oc[5] = y[3] + y[4]\n",
    "\n",
    "print(oc)\n",
    "\n",
    "assert torch.allclose(out, oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(Data(x=oc, edge_index=edge_index), embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Улучшения**\n",
    "\n",
    "1. Логично добавлять к сумме эмбеддинг самой вершины.\n",
    "Для этого нужно либо в `edge_index` добавить к каждой вершине путь в саму себя, либо установить параметр конструктора GCNConv `add_self_loops = True`, тогда\n",
    "$x_i \\in N_i$\n",
    "\n",
    "2. Число соседей у разных вершин может сильно отличаться, и результат графовой свертки для вершины с одним соседом по абсолютному значению будет сильно меньше, чем для вершины, у которой $200$ соседей. Поэтому логично нормировать выходы на количество слагаемых (число соседей вершины). Количество соседей вершины обозначим как : $deg(i) = d_i$, тогда:\n",
    "\n",
    "$$\\large y_i = \\frac{1}{d_i} \\sum_{j \\in N_i} \\bar x_j \\cdot \\bar w  \\quad (2)$$\n",
    "\n",
    "3. Авторы статьи [Semi-Supervised Classification with Graph Convolutional Networks (Kipf, Welling, 2017) 🎓[arxiv]](https://arxiv.org/abs/1609.02907) предположили, что признаки от узлов с многочисленными соседями будут распространяться легче, чем от изолированных узлов. Чтобы компенсировать этот эффект, они  предложили присваивать больший вес признакам из узлов с меньшим числом соседей:\n",
    "\n",
    "$$\\large y_i =   \\frac{1}{ \\sqrt{d_i}} \\sum_{j \\in N_i} \\frac {\\bar x_j \\cdot \\bar w} {\\sqrt{d_j}} = \\sum_{j \\in N_i} \\frac  {\\bar x_j \\cdot \\bar w} {\\sqrt {d_i d_j}  } \\quad(3)$$\n",
    "\n",
    "  Чтобы порядок значений сохранялся, добавили корень. Теперь при $d_i == d_j$ значения $(2)$ и $(3)$ будут равны.\n",
    "\n",
    "  4. У ребер могут быть собственные веса, задающие силу связи между вершинами. Их можно добавить при прямом проходе, используя параметр `edge_weight` метода `GCNConv.forward`. По умолчанию эти веса равны $1$, обозначим их как $e_{j,i}$ .\n",
    "  \n",
    "  Ослабление или усиление связи между вершинами означает изменение степени соседства. Поэтому $\\displaystyle d_i = 1+\\sum_{j \\in N_i} e_{j,i} $, и общая формула примет вид:\n",
    "\n",
    "  $$\\large y_i = \\sum_{j \\in N_i} \\frac  { e_{j,i} \\bar x_j \\cdot \\bar w} {\\sqrt {d_i d_j}  } \\quad(4)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы включить в сумму эмбеддинг самой вершины, надо передать в конструктор  GCNConv параметр `add_self_loops=True` , а чтобы  добавить нормировки на количество соседей, надо установить `normalize=True`\n",
    "\n",
    "\\* *Без установки `normalize=True` установка `add_self_loops=True` работает некорректно*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCNConv(len(x), 1, add_self_loops=True, bias=False, normalize=True)\n",
    "gcn.lin.weight.data = torch.ones((1, len(x)))\n",
    "out = gcn(embeddings, edge_index)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(Data(x=out, edge_index=edge_index), embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде это могло бы выглядеть примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "\n",
    "def get_neighbors(n):\n",
    "    # find all neighbors of node n\n",
    "    neighbors = [n]  # first put to neighbors list index of node itself\n",
    "    for i, node_num in enumerate(edge_index[1]):\n",
    "        if node_num == n:\n",
    "            neighbors.append(edge_index[0][i].item())\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "out_norm = torch.zeros_like(out)  # final summ\n",
    "for i, e in enumerate(y):\n",
    "    neighbors = get_neighbors(i)\n",
    "    deg_i = len(neighbors)  # neighbors count of node i\n",
    "    for node_num in neighbors:\n",
    "        deg_j = len(\n",
    "            get_neighbors(node_num)\n",
    "        )  # neighbors count of j-th neighbor of node i\n",
    "        out_norm[i] += y[node_num] / (\n",
    "            sqrt(deg_i) * sqrt(deg_j)\n",
    "        )  # Implementation of  equation (3)\n",
    "print(out_norm)\n",
    "assert torch.allclose(out, out_norm)  # check that results of CGNConv the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление весов к ребрам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weight = torch.Tensor([1, 1, 1, 1, 1, 1, 20])  # increase 4->5 edge weight\n",
    "\n",
    "out = gcn(embeddings, edge_index, edge_weight)\n",
    "print([\"{0:0.2f}\".format(i.item()) for i in out])\n",
    "show_graph(Data(x=out, edge_index=edge_index), embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы добавим в слой ещё нейроны, то размерность эмбеддингов на выходе увеличится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCNConv(len(x), 8, add_self_loops=True, bias=False, normalize=True)\n",
    "out = gcn(embeddings, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(Data(x=out, edge_index=edge_index), embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это имеет смысл, так как эмбеддинг должен вобрать в себя информацию о соседних вершинах, и, чтобы закодировать ее, требуется место. А если мы объединим несколько слоев вместе, то можем добиться того, что каждый эмбеддинг сможет содержать информацию о всех доступных вершинах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(6, 8)\n",
    "        self.gcn2 = GCNConv(8, 16)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index=None):\n",
    "        x = self.gcn1(x, edge_index).relu()\n",
    "        return self.gcn2(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(embeddings, edge_index)\n",
    "print(out.shape)\n",
    "# show_graph(Data(x = out, edge_index=edge_index), embeddings =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С каждым слоем количество соседей, которое участвует в получении эмбеддинга вершины, будет расти. Можно сравнить это с увеличением рецептивного поля нейрона в обычной сверточной сети. Вот пример того, как будет расти рецептивное поле 3-й вершины с увеличением числа слоев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.subplot(1, 3, 1).set_title(\"GC Node3 Layer 0\")\n",
    "show_graph(data, [\"gray\", \"gray\", \"lightgreen\", \"green\", \"gray\", \"gray\"])\n",
    "plt.subplot(1, 3, 2).set_title(\"GC Node3 Layer 1\")\n",
    "show_graph(data, [\"gray\", \"lightgreen\", \"lightgreen\", \"green\", \"lightgreen\", \"gray\"])\n",
    "plt.subplot(1, 3, 3).set_title(\"GC Node3 Layer 3\")\n",
    "show_graph(\n",
    "    data, [\"lightgreen\", \"lightgreen\", \"lightgreen\", \"green\", \"lightgreen\", \"gray\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе CGNConv слоя мы получаем граф с новыми эмбеддингами. Его можно использовать для решения ряда задач:\n",
    "\n",
    "* предсказывать класс для каждой из вершин;\n",
    "* кластеризовать вершины;\n",
    "* классифицировать граф целиком, для этого агрегируют эмбеддинги всех вершин и получившийся эмбеддинг подают на вход классификатору;\n",
    "* предсказывать свойства всего графа, решая задачу регрессии.\n",
    "\n",
    "[[colab] 🥨 Пример классификации молекул](https://colab.research.google.com/drive/1lzKAGDrmKmaCrjoNxEzoRxGxEbaiS-jS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы лучше понять, как работают сверточные сети, можно визуализировать карты активаций и веса фильтров свёртки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/what_hidden_layers.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Веса фильтров на первом слое легко визуализировать. И результат легко интерпретируется, так как у фильтров такое же количество каналов, как и у цветных изображений ($3$ канала).\n",
    "\n",
    "Ниже приведен пример того, как это можно сделать для обученной модели AlexNet из зоопарка моделей torchvision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять, через какие свойства можно получить доступ к весам, выведем структуру модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "alexnet = models.alexnet(weights=\"AlexNet_Weights.DEFAULT\")\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что первый слой — это $0$-й элемент контейнера features.\n",
    "Веса слоя хранятся в `weight.data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = alexnet.features[0].weight.data  # extract weights\n",
    "print(\"Weights shape\", weight_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы отобразить все веса на одном изображении, воспользуемся вспомогательной функцией `make_grid` [🛠️[doc]](https://pytorch.org/vision/stable/generated/torchvision.utils.make_grid.html#make-grid) из `torchvision.utils` [🛠️[doc]](https://pytorch.org/vision/stable/utils.html)\n",
    "\n",
    "На вход метод получает batch изображений ($B \\times C \\times H \\times W$) в формате `torch.Tensor` и визуализирует их в форме таблице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "\n",
    "img_grid = utils.make_grid(\n",
    "    (weight_tensor + 1) / 2, pad_value=1\n",
    ")  # combine weights from all channel into table, note remapping to (0,1) range\n",
    "print(\"Output is CxHxW image\", img_grid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_grid` часто используют, чтобы отображать изображения в TensorBoard.\n",
    "\n",
    "А чтобы отобразить получившуюся таблицу в блокноте средствами matplotlib, нам потребуется поменять порядок хранения данных, поместив каналы на первое место."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.imshow(\n",
    "    np.transpose(img_grid, (1, 2, 0))\n",
    ")  # change channel order for compability with numpy & matplotlib\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель научилась улавливать простые геометрические формы: края под разными углами, точки того или иного цвета. Фильтры AlexNet'а оказались настолько большими, что частично захватили не только простую локальную информацию, но и сложные градиенты или решётки.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация фильтров промежуточных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, выполнить ту же операцию для фильтров на скрытых слоях едва ли представляется разумным: в отличие от трёхканальных фильтров первого слоя, веса которых легко визуализировать, фильтры поздних слоёв имеют гораздо больше каналов, что затрудняет их визуализацию. Пожалуй, единственным вариантом  является поканальное отображение весов, которое довольно сложно трактовать. Убедимся в этом, на примере ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher Layer: Visualize Filter**\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/weight_visualization.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em> We can visualize filters at higher layers, but not that interesting</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\">ConvNetJS CIFAR-10 example</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем веса $2$-го сверточного слоя AlexNet.\n",
    "Слой доступен через `features[3] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_of_conv2_layer = alexnet.features[3].weight.data  # extract weights\n",
    "print(weights_of_conv2_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нем $192$ фильтра, в каждом $64$ ядра. Поэтому ограничимся первым фильтром и выведем все его ядра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_filter_kernels = weights_of_conv2_layer[0]\n",
    "print(first_filter_kernels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы использовать `image_grid`, входной тензор должен иметь формат $(B \\times C \\times H \\times W)$. Поэтому добавим размерность, соответствующую каналам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid = utils.make_grid(\n",
    "    weights_of_conv2_layer[0].unsqueeze(1), pad_value=1  # add fake channel dim\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.imshow(\n",
    "    np.transpose((img_grid + 1) / 2, (1, 2, 0))\n",
    ")  # change channel order for compability with numpy\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерпретация такой визуализации довольно затруднительна, зато мы разобрались, как получать доступ к весам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация карт активаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее очевидный метод визуализации заключается в том, чтобы показать активации сети во время прямого прохода. Для сетей ReLU активации обычно начинают выглядеть относительно сгущенными и плотными, но по мере развития обучения активации обычно становятся более редкими и локализованными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На последних слоях свёрточной нейронной сети размеры рецептивных полей нейронов становятся сравнимы с размером исходного изображения, поэтому при визуализации их карт активаций становится понятно, какие нейроны реагируют на какие части изображений.\n",
    "\n",
    "К примеру, на изображении ниже активация выделенного нейрона достигнута благодаря пикселям, примерно соответствующим расположению лица человека, поэтому можно предположить, что он научился находить лица на изображении. Более подробно об этом можно почитать в статье [Understanding Neural Networks Through Deep Visualization 🎓[arxiv]](https://arxiv.org/abs/1506.06579)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing Activations**\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/visualization_activations.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em> conv5 feature map is $128\\times13\\times13$; visualize as $128$ $13\\times13$ grayscale images</em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf\">Visualizing and Understanding</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от весов, карты активаций не сохраняются в памяти. Для того, чтобы получить к ним доступ, в PyTorch предусмотрен механизм под названием [Hooks 🛠️[doc]](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks).\n",
    "\n",
    "Благодаря ему можно получить доступ к выходам или входам слоя как при прямом, так и при обратном распространении сигнала через сеть.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зарегистрируем свой hook. Он просто выведет в консоль размеры карты активации (выхода слоя).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html\n",
    "\n",
    "\n",
    "def module_hook(module: nn.Module, input, output):  # For nn.Module objects only.\n",
    "    print(\"Hi, i am hook_1 ! \", input[0].shape)  # activation_map of previous layer\n",
    "\n",
    "\n",
    "handle = alexnet.features[10].register_forward_hook(\n",
    "    module_hook\n",
    ")  # attach hook to last conv layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что он работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "out = alexnet(torch.randn(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы удалить hook, используйте метод `remove` дескриптора, который возвращает метод `register_forward_hook`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()\n",
    "out = alexnet(torch.randn(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывода нет, hook отключился!\n",
    "\n",
    "Теперь напишем hook, который выведет нам карту активации.\n",
    "Так как на выходе данного слоя $256$ каналов, выведем каждый отдельно, подав на вход `make_grid` тензор с $256$ элементами.\n",
    "\n",
    "Для этого потребуется:\n",
    "* удалить batch-измерение;\n",
    "* добавить измерение, имитирующее канал для картинок.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module_hook(module: nn.Module, input, output):\n",
    "    # activation_map = output.squeeze(0).unsqueeze(1) # alternative solution\n",
    "    activation_map = input[0].permute(1, 0, 2, 3)  # B <--> C\n",
    "    img_grid = (\n",
    "        utils.make_grid(activation_map, pad_value=1, nrow=16, normalize=True) * 255\n",
    "    )\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    plt.imshow(\n",
    "        img_grid.permute(1, 2, 0)\n",
    "        .numpy()\n",
    "        .astype(\"uint8\")  # change channel order for compability with numpy\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "handle = alexnet.features[12].register_forward_hook(module_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Чтобы карта активаций  была интерпретируема, надо использовать реальное изображение. Загрузим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/fox.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_fox = Image.open(\"fox.jpg\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.imshow(img_fox)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем изображение в тензор и подадим на вход модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "tensor = transform(img_fox)\n",
    "out = alexnet(tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут уже можно увидеть некоторые паттерны. Видно, что многие фильтры реагируют на лисицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Единственная опасная ловушка, которую можно легко заметить с помощью этой визуализации, заключается в том, что некоторые карты активации могут быть равны нулю для многих различных входов. Это может указывать на мертвые фильтры и может быть симптомом слишком высокой скорости обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отключим наш hook, чтобы он не мешал дальнейшим экспериментам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют техники, которые позволяют визуализировать паттерн, на который лучше всего активируется конкретный нейрон. Они основаны на подсчете градиента не по весам, а по входу (изображению) и его постепенной модификации.\n",
    "\n",
    "Примеры:\n",
    "\n",
    "* [[demo] 🎮 Exploring Neural Networks with Activation Atlases](https://distill.pub/2019/activation-atlas/)\n",
    "\n",
    "* [[blog] ✏️ Feature Visualization](https://distill.pub/2017/feature-visualization/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к картам активации предпоследнего слоя. В AlexNet предпоследний слой полносвязный, соответственно, активации — это вектор.\n",
    "\n",
    "На вход сети мы подали изображение, закодированное при помощи $150528$ чисел ($224\\times224\\times3 = 150528$), а на выходе получили вектор из $4096$ чисел.\n",
    "\n",
    "Фактически нейросеть понизила размерность наших данных (в $\\approx36$ раз). При этом данные в векторе достаточно информативны, так как позволяют произвести классификацию на $1000$ классов.\n",
    "\n",
    "Такие векторы признаков называются **embedding** и широко используются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Последний слой**\n",
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/feature_extractor.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы убедиться в полезности полученных представлений, кластеризуем их при помощи [k-nearest neighbors 📚[wiki]](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) алгоритма.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Последний слой: ближайшие соседи**\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/last_layer_knn.jpg\" width=\"900\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\">ImageNet Classification with Deep Convolutional Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы получить embedding изображения, отключим последний слой. Выведем структуру модели, чтобы найти его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И заменим его пустышкой `nn.Identity()` — класс, который возвращает вход без изменений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.classifier[6] = nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize, Compose\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "transform = Compose(\n",
    "    [Resize(224), ToTensor(), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "testset = CIFAR10(root=\"./CIFAR10\", train=False, download=True, transform=transform)\n",
    "train, test, _ = random_split(testset, [512, 128, 10000 - 512 - 128])\n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим функцию, которая сохраняет выходы измененной модели в массиве, а также возвращает метки классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_embeddings(loader):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    for img, label in tqdm(loader):\n",
    "        emb = alexnet(img)\n",
    "        embeddings.append(emb.detach())\n",
    "        labels.append(label)\n",
    "    embeddings = torch.stack(embeddings).reshape(-1, 4096).numpy()\n",
    "    labels = torch.stack(labels).flatten().numpy()\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Превратим картинки в векторы признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x, y = get_embeddings(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть $512$ векторов по $4096$ значения в каждом и $512$ меток классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Обучим\" на них k-NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим векторы признаков (embeddings) для тестовых картинок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=32, shuffle=False, drop_last=True)\n",
    "test_emb, gt_labels = get_embeddings(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем предсказания и считаем accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = neigh.predict(test_emb)\n",
    "\n",
    "accuracy = accuracy_score(gt_labels, y_pred)\n",
    "print(\"k-NN accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, активации на последних слоях сети достаточно информативны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обучить нейросеть  на своих данных, когда их мало?\n",
    "\n",
    "Можно взять обученную модель, заменить у нее несколько последних слоев и обучить только их.\n",
    "\n",
    "Ранее обученные слои будут извлекать признаки (feature extractor), и полученные таким образом представления (embedding) будут классифицироваться вновь добавленными слоями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/transfer_learning_change_classes_scheme.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, не все фильтры модели будут использованы эффективно на новой задаче. К примеру, если мы работаем с изображениями, связанными только с едой, не все фильтры на скрытых слоях предобученной на ImageNet модели окажутся полезны для нашей задачи.\n",
    "\n",
    "Поэтому **после того, как был обучен новый классификатор**, можно дообучить все или некоторые промежуточные слои. При этом используют меньший learning rate, чем при обучении нейросети с нуля: мы знаем, что по крайней мере часть весов нейросети выполняет свою задачу хорошо, и не хотим испортить это быстрыми изменениями.\n",
    "\n",
    "Поэтому такой подход называется  **fine-tuning** — мы дополнительно \"настраиваем\" и промежуточные слои под нашу задачу.\n",
    "\n",
    "Можно делать настройку постепенно: сначала учить только последние добавленные нами слои сети, затем самые близкие к ним, и после этого учить уже все веса нейросети вместе.\n",
    "\n",
    "Иногда fine-tuning считается синонимом Transfer learning, в этом случае часть от предтренированной сети называют **backbone** (\"позвоночник\"), а добавленную часть — **head** (\"голова\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 1. Получение предварительно обученной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательно рассмотрим шаги, необходимые для реализации подхода transfer learning.\n",
    "\n",
    "Первым шагом является выбор предварительно обученной модели, которую мы хотели бы использовать в качестве основы для обучения. Основным предположением является то, что признаки, которые умеет выделять из данных предобученная модель, хорошо подойдут для решения нашей частной задачи. Поэтому эффект от Transfer learning будет тем лучше, чем более схожими будут **домены** в нашей задаче и в задаче, на которой предварительно обучалась модель.\n",
    "\n",
    "Для задач обработки изображений очень часто используются модели, предобученные на ImageNet. Такой подход распространен, однако если ваша задача связана, например, с обработкой снимков клеток под микроскопом, то модель, предобученная на более близком домене (тоже на снимках клеток, пусть и совсем других), может быть лучшим начальным решением."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/transfer_learning_step_1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.alexnet(weights=\"AlexNet_Weights.DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 2. Заморозка предобученных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы предполагаем, что первые слои модели уже хорошо натренированы выделять важные признаки из данных, и мы не хотим их \"сломать\".\n",
    "\n",
    "Если начнем дообучать их вместе с новыми слоями, которые инициализируются случайно, то на первых шагах обучения ошибка будет большой и мы можем сильно изменить \"хорошие\" предобученные веса.\n",
    "\n",
    "Поэтому требуется \"заморозить\" предобученные веса. На практике заморозка означает **отключение подсчета градиентов**. Таким образом, при последующем обучении параметры с отключенным подсчетом градиентов не будут обновляться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/transfer_learning_step_2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 3. Добавление новых обучаемых слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от начальных слоев, которые выделяют достаточно общие признаки из данных, более близкие к выходу слои предобученной модели сильно специфичны конкретно под ту задачу, на которую она обучалась. Для моделей, предобученных на ImageNet, последний слой заточен конкретно под предсказание 1000 классов из этого набора данных. Кроме этого, последние слои могут не подходить под новую задачу архитектурно: в новой задаче может быть меньше классов, 10 вместо 1000. Поэтому требуется **заменить последние один или несколько слоев** предобученной модели на новые, подходящие под нашу задачу. При этом, естественно, веса в этих слоях будут инициализированы случайно. Именно эти слои мы и будем обучать на следующем шаге.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/transfer_learning_step_3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model.classifier[6] = nn.Linear(4096, 10, bias=True)  # For CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что обучаться будет только вновь добавленный слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, \"\\t\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 4. Обучение новых слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все, что нам теперь нужно, — обучить новые слои на наших данных. При этом замороженные слои используются лишь как **экстрактор высокоуровневых признаков**.\n",
    "Обучение такой модели существенно ничем не отличается от обучения любой другой модели: используются обучающая и валидационная выборки, контролируется изменение функции потерь и функционала качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/transfer_learning_step_4.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize, Compose\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "transform = Compose(\n",
    "    [Resize(224), ToTensor(), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "testset = CIFAR10(root=\"./CIFAR10\", train=False, download=True, transform=transform)\n",
    "train, test, _ = random_split(testset, [512, 128, 10000 - 512 - 128])\n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И напишем функцию для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs=1, lr=1e-3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        for imgs, labels in tqdm(train_loader):\n",
    "            optimizer.zero_grad()  # Clean existing gradients\n",
    "            outputs = model(imgs.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()\n",
    "        print(f\"\\nEpoch {epoch} Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, 5)  # train only last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели могут быть слои, поведение которых различно при выводе (inference) и обучении. Например, слои Dropout и Batchnorm, которые будут обсуждаться на следующей лекции.\n",
    "\n",
    "Если такие слои в модели есть, то их следует перевести в inference режим, используя метод `nn.Module.eval()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Шаг 5. Тонкая настройка модели (fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как мы обучили новые слои модели, и они уже как-то решают задачу, мы можем разморозить ранее замороженные веса, чтобы **тонко настроить** их под нашу задачу в надежде, что это позволит еще немного повысить качество.\n",
    "\n",
    "Нужно быть осторожным на этом этапе: использовать learning rate на порядок или два меньший, чем при основном обучении, и одновременно с этим следить за возникновением переобучения. Переобучение при fine-tuning может возникать из-за того, что мы резко увеличиваем количество настраиваемых параметров модели, но при этом наш датасет остается небольшим, и мощная модель может начать заучивать обучающие данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/transfer_learning_step_5.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_model(model, num_epochs=3, lr=1e-5)  # fine tune all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test, batch_size=32, shuffle=False, drop_last=True)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for imgs, labels in test_loader:\n",
    "    outputs = model(imgs.to(device))\n",
    "    y_true.append(labels.numpy())\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    y_pred.append(preds.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "y_true = np.stack(y_true).flatten()\n",
    "y_pred = np.stack(y_pred).flatten()\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy : {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность на 512 не репрезентативна, но она уже выше, чем получилась в домашнем задании."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аугментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Операция свертки инвариантна к сдвигу (не важно, где на изображении находится объект), но не к масштабированию и повороту.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/affine1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У этой проблемы нет красивого решения: если в обучающей выборке не было котов, висящих вверх лапами, то модель не будет их различать.\n",
    "\n",
    "Нужно дополнить датасет такими изображениями, и самый простой способ это сделать — это преобразовать (отразить, повернуть, масштабировать и т.д.) уже имеющиеся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Аугмента́ция** (от лат. augmentatio — увеличение, расширение) — увеличение выборки обучающих данных через модификацию существующих данных.\n",
    "\n",
    "Модели глубокого обучения обычно требуют большого количества данных для обучения. В целом, чем больше данных, тем лучше для обучения модели. В то же время получение огромных объемов данных сопряжено со своими проблемами (например, с нехваткой размеченных данных или с трудозатратами, сопряженными с разметкой).\n",
    "\n",
    "Вместо того, чтобы тратить дни на сбор данных вручную, мы можем использовать методы аугментации для автоматической генерации новых примеров из уже имеющихся.\n",
    "\n",
    "Помимо увеличения размеченных датасетов, многие методы *self-supervised learning* построены на использовании разных аугментаций одного и того же сэмпла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.1/L06/out/augmentations_examples.png\" width=\"700\"></center>\n",
    "<center><em>Примеры аугментаций картинки. </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важный момент**: при обучении модели мы используем разбиение данных на `train-val-test`. Аугментации стоит применять только на `train`. Почему так? Конечная цель обучения нейросети — это применение на реальных данных, которые сеть не видела. Поэтому для адекватной оценки качества модели валидационные и тестовые данные изменять не нужно.\n",
    "\n",
    "В любом случае, `test` должен быть отделен от данных еще до того, как они попали в `DataLoader` или нейросеть.\n",
    "\n",
    "Другое дело, что аугментации на тесте можно использовать как метод ансамблирования в случае классификации. Можно взять sample → создать несколько его копий → по-разному их аугментировать → предсказать класс на каждой из этих аугментированных копий → выбрать наиболее вероятный класс голосованием (такой функционал реализован, например, в YOLO, о которой речь пойдет в следующих лекциях)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инструментарий для аугментаций [встроен в PyTorch 🛠️[doc]](https://pytorch.org/vision/stable/transforms.html). Мы уже пользовались им, когда создавали датасет и передавали в конструктор параметр **transform**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize, Compose\n",
    "\n",
    "transform = Compose(\n",
    "    [Resize(224), ToTensor(), Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "testset = CIFAR10(root=\"./CIFAR10\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно, экземпляр объекта `torchvision.transforms.Resize`, созданный первым в списке (`Resize(224)`), и аугментирует изображение.\n",
    "\n",
    "*Отметим что аугментация происходит \"на лету\": аугментированное изображение не сохраняется на диск и сразу передается в модель.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет `torchvision.transforms` [🛠️[doc]](https://pytorch.org/vision/0.9/transforms.html) позволяет выполнить  довольно много различных аугментаций. Посмотрим на некоторые из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Загрузим и отобразим пример картинки. Картинку отмасштабируем, чтобы она не занимала весь экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# setting random seed for reproducible illustrations\n",
    "torch.manual_seed(42)\n",
    "\n",
    "URL = (\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.1/L06/capybara_image.jpg\"\n",
    ")\n",
    "!wget -q $URL -O test.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n",
    "\n",
    "input_img = Image.open(\"/content/test.jpg\")\n",
    "input_img = transforms.Resize(size=300)(input_img)\n",
    "display(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций картинок. С полным списком можно ознакомиться на сайте [документации torchvision 🛠️[doc]](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформация `transforms.Random Rotation` [🛠️[doc]](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html?highlight=randomrotation#torchvision.transforms.RandomRotation) принимает параметр `degrees` — диапазон углов, из которого выбирается случайный угол для поворота изображения.\n",
    "\n",
    "Создадим переменную `transform`, в которую добавим нашу аугментацию, и применим ее к исходному изображению. Затем запустим следующую ячейку несколько раз подряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_augmented_img(transform, input_img):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    augmented_img = transform(input_img)\n",
    "    ax[0].imshow(input_img)\n",
    "    ax[0].set_title(\"Original img\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    ax[1].imshow(augmented_img)\n",
    "    ax[1].set_title(\"Augmented img\")\n",
    "    ax[1].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "transform = transforms.RandomRotation(degrees=(0, 180))\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Blur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transforms.GaussianBlur` [🛠️[doc]](https://pytorch.org/vision/main/generated/torchvision.transforms.GaussianBlur.html?highlight=gaussianblur#torchvision.transforms.GaussianBlur) размывает изображение с помощью фильтра Гаусса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Erasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transforms.RandomErasing` [🛠️[doc]](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomErasing.html?highlight=transforms+randomerasing#torchvision.transforms.RandomErasing) стирает на изображении произвольный прямоугольник. Она имеет параметр `p` — вероятность, с которой данная трансформация вообще применится к изображению.\n",
    "\n",
    "Данная трансформация работает только с `torch.Tensor`, поэтому предварительно нужно применить трансформацию `ToTensor`, а затем `ToPILImage`, чтобы воспользоваться нашей функцией для отображения.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.RandomErasing(p=1), transforms.ToPILImage()]\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не лишним будет заметить, что некоторые трансформации могут существенно исказить изображение. Например, здесь `RandomErasing` практически полностью стерла основной объект на снимке — капибару. Такая грубая аугментация может только навредить процессу обучения, и на практике нужно быть осторожным.\n",
    "\n",
    "`RandomErasing` также имеет параметр `scale` — диапазон соотношения стираемой области к входному изображению. Попробуем уменьшить этот диапазон относительно значения по умолчанию, чтобы избежать нежелательного эффекта стирания капибары."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=1, scale=(0.02, 0.1)),\n",
    "        transforms.ToPILImage(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ColorJitter\n",
    "`transforms.ColorJitter` [🛠️[doc]](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html?highlight=colorjitter#torchvision.transforms.ColorJitter) случайным образом меняет яркость, контрастность, насыщенность и оттенок изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ColorJitter(brightness=0.5, hue=0.3)\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого будем использовать метод `transforms.Compose` [🛠️[doc]](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html?highlight=compose#torchvision.transforms.Compose), с которым мы хорошо знакомы. Нам нужно будет создать `list` со всеми аугментациями, которые будут применены последовательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0),\n",
    "        transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещение нескольких аугментаций случайным образом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, три аугментации, примененные последовательно, достаточно сильно изменили картинку. Разумнее применять не все сразу. Для этого есть соответствующие классы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font size=\"4\">Random Apply</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы применять аугментации случайным образом, можно воспользоваться классом `transforms.RandomApply` [🛠️[doc]](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomApply.html?highlight=randomapply#torchvision.transforms.RandomApply), конструктор которого принимает на вход  список аугментаций и вероятность `p`, с которой каждая аугментация будет применена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomApply(\n",
    "    transforms=[\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5),\n",
    "        transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "    ],\n",
    "    p=0.9,\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font size=\"4\">Random Choice</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В других случаях может быть полезен класс `transforms.RandomChoice` [🛠️[doc]](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomChoice.html?highlight=randomchoice#torchvision.transforms.RandomChoice), конструктор которого принимает на вход список аугментаций `transforms`, выбирает из него **одну** случайную аугментацию и применяет ее к изображению. Необязательным параметром является список вероятностей `p`, который указывает, с какой вероятностью каждая из аугментаций может быть выбрана из списка (по умолчанию каждая может быть выбрана равновероятно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomChoice(\n",
    "    transforms=[\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0),\n",
    "        transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "    ],\n",
    "    p=[0.2, 0.4, 0.6],\n",
    ")\n",
    "\n",
    "plot_augmented_img(transform, input_img)\n",
    "plot_augmented_img(transform, input_img)\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример создания собственной аугментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда может оказаться, что среди широкого спектра реализованных аугментаций нет такой, которую вы хотели бы применить к своим данным. В таком случае ее можно описать в виде класса и использовать наравне с реализованными в библиотеке.\n",
    "\n",
    "Главное, что необходимо описать при создании класса, — метод `__call__`. Он должен принимать изображение (оно может быть представлено в формате `PIL.Image`, `np.array` или `torch.Tensor`), делать с ним интересующие нас видоизменения и возвращать измененное изображение.\n",
    "\n",
    "Рассмотрим пример добавления на изображение [шума \"соль и перец\" 📚[wiki]](https://ru.wikipedia.org/wiki/Salt_and_pepper). Наш метод аугментации будет и принимать на вход, и возвращать `PIL.Image`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class SaltAndPepperNoise:\n",
    "    \"\"\"\n",
    "    Add a \"salt and pepper\" noise to the PIL image\n",
    "    __call__ method returns PIL Image with noise\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.01):\n",
    "        self.p = p  # noise level\n",
    "\n",
    "    def __call__(self, pil_image):\n",
    "        np_image = np.array(pil_image)\n",
    "\n",
    "        # create random mask for \"salt\" and \"pepper\" pixels\n",
    "        salt_ind = np.random.choice(\n",
    "            a=[True, False], size=np_image.shape[:2], p=[self.p, 1 - self.p]\n",
    "        )\n",
    "        pepper_ind = np.random.choice(\n",
    "            a=[True, False], size=np_image.shape[:2], p=[self.p, 1 - self.p]\n",
    "        )\n",
    "\n",
    "        # add \"salt\" and \"pepper\"\n",
    "        np_image[salt_ind] = 255\n",
    "        np_image[pepper_ind] = 0\n",
    "\n",
    "        return Image.fromarray(np_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SaltAndPepperNoise(p=0.03)\n",
    "\n",
    "plot_augmented_img(transform, input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация внутри `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем папку с картинками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download files\n",
    "!wget -q \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/for_transforms.Compose.zip\" -O data.zip\n",
    "!unzip -qn \"data.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс `AugmentationDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.img_list = glob.glob(root + \"*.jpg\")\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.img_list[i])\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `list` с аугментациями, которые мы хотим применить. Многие аугментации в PyTorch работают с Pillow Image, так что картинки не обязательно преобразовывать в тензоры (`transforms.ToTensor()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5),\n",
    "        # ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "augmentated_dataset = AugmentationDataset(\n",
    "    \"/content/for_transforms.Compose/\", transforms=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим аугментированные изображения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 2))\n",
    "for i, img in enumerate(augmentated_dataset):\n",
    "    plt.subplot(1, len(augmentated_dataset) + 1, 1 + i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.array(img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация в реальных задачах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме методов, реализованных в PyTorch, существуют и специализированные библиотеки для аугментации изображений, в которых реализованы дополнительные возможности (например, наложение теней, бликов или пятен воды на изображение).\n",
    "\n",
    "Например:\n",
    "- [[doc] 🛠️ Albumentations](https://albumentations.ai)\n",
    "- [[doc] 🛠️ imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "- [[doc] 🛠️ AugLy](https://github.com/facebookresearch/AugLy)\n",
    "\n",
    "**Важно: при выборе методов аугментации имеет смысл использовать только те, которые будут в реальной жизни.**\n",
    "\n",
    "Например, нет смысла:\n",
    "- делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным,\n",
    "- отражать человека вверх ногами, если мы не предполагаем его таким распознавать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении моделей в PyTorch нам часто приходится переписывать цикл обучения (train loop). Это дублирование кода, которое нарушает принцип [DRY 📚[wiki]](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\n",
    "\n",
    "Кроме того, нам нужно следить за процессом обучения модели. Например если loss \"взрывается\" или выходит на плато, как правило, есть смысл остановить обучение. Чтобы контролировать этот процесс, приходится добавлять дополнительный код для вывода и/или логирования метрик.\n",
    "\n",
    "При проведении реальных экспериментов логирование результатов станет необходимым. В задании вам предстоит научиться работать с фреймворком [Lightning 🐾[git]](https://github.com/Lightning-AI/lightning), который облегчает написание train loop и логирование результатов.\n",
    "\n",
    "Дополнительные материалы:\n",
    "* [[colab] 🥨 Знакомство с TensorBoard](https://colab.research.google.com/drive/1-RkUgYJPGyg3ZJ_Z7b4jRALVToicgpCZ#scrollTo=DGkBwvykm_Ci)\n",
    "* [[colab] 🥨 Основы работы с Lightning](https://colab.research.google.com/drive/16Q_k7NR4oGMh5CrqErsdzWwpihlK4lbH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практические рекомендации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Используйте [готовые модели 🛠️[doc]](https://pytorch.org/vision/stable/models.html).\n",
    "*   Используйте фильтры $3 \\times 3 $.\n",
    "*   Чтобы уменьшить пространственные размеры, используйте свертку со `stride = 2`.\n",
    "*   Сохраняйте логи и следите за ходом обучения при помощи инструментов [TensorBoard 🛠️[doc]](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) и [Weights&Biases 🛠️[doc]](https://wandb.ai/site).\n",
    "*   Отлаживайте код на на части датасета: `debug_train, debug_val, _ = random_split(dataset, [5000, 1000, 54000])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>Литература</font>\n",
    "\n",
    "<font size=5>Введение в сверточные нейронные сети:</font>\n",
    "\n",
    "* [[blog] ✏️ Всё, что вы хотели знать о перцептронах Розенблатта, но боялись спросить](https://habr.com/ru/companies/sberdevices/articles/529932/)\n",
    "* [[wiki] 📚 Viola–Jones object detection framework](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework)\n",
    "\n",
    "<font size=5>Сверточный слой нейросети:</font>\n",
    "\n",
    "* [[blog] ✏️ Какого цвета Марс?](https://habr.com/ru/articles/160621/s)\n",
    "\n",
    "<font size=5>Применение свёрточных слоёв:</font>\n",
    "\n",
    "* [[doc] 🛠️ LeNet PyTorch documentation](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "\n",
    "<font size=5>Другие виды сверток:</font>\n",
    "\n",
    "* [[blog] ✏️ Convolution vs. Cross-Correlation](https://glassboxmedicine.com/2019/07/26/convolution-vs-cross-correlation/)\n",
    "* [[blog] ✏️ 2D Convolution as a Matrix-Matrix Multiplication](https://www.baeldung.com/cs/convolution-matrix-multiplication)\n",
    "* [[blog] ✏️ Forward and Backward Convolution Passes as Matrix Multiplication](https://danieltakeshi.github.io/2019/03/09/conv-matmul/)\n",
    "* [[blog] ✏️ The NIFTI file format](https://brainder.org/2012/09/23/the-nifti-file-format/)\n",
    "* [[doc] 🛠️ NiBabel](https://nipy.org/nibabel/coordinate_systems.html)\n",
    "* [[doc] 🛠️ PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)\n",
    "* [[wiki] 📚 Adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix)\n",
    "* [[doc] 🛠️ Drawing — basic functionality for visualizing graphs](https://networkx.org/documentation/networkx-1.10/reference/drawing.html)\n",
    "* [[arxiv] 🎓 Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907).\n",
    "* [[doc] 🛠️ GNN Cheatsheet](https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html)\n",
    "\n",
    "<font size=5>Визуализация:</font>\n",
    "\n",
    "* [[arxiv] 🎓 Understanding Neural Networks Through Deep Visualization](https://arxiv.org/abs/1506.06579)\n",
    "* [[demo] 🎮 Exploring Neural Networks with Activation Atlases](https://distill.pub/2019/activation-atlas/)\n",
    "* [[blog] ✏️ Feature Visualization](https://distill.pub/2017/feature-visualization/)\n",
    "\n",
    "<font size=5>Lightning:</font>\n",
    "\n",
    "* [[git] 🐾 Lightning](https://github.com/Lightning-AI/lightning)\n",
    "* [[doc] 🛠️ TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n",
    "* [[doc] 🛠️ Weights&Biases](https://wandb.ai/site)\n",
    "\n",
    "<font size=5>Дополнительно:</font>\n",
    "\n",
    "* [[blog] ✏️ Свёртка в Deep Learning простыми словами](https://www.reg.ru/blog/svyortka-v-deep-learning-prostymi-slovami)\n",
    "* [[arxiv] 🎓 A guide to convolution arithmetic for deep\n",
    "learning](https://arxiv.org/pdf/1603.07285v1.pdf)\n",
    "* [[video] 📺 But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA)\n",
    "* [[video] 📺 Fundamental Algorithm of Convolution in Neural Networks](https://www.youtube.com/watch?v=eMXuk97NeSI&list=PLZDCDMGmelH-pHt-Ij0nImVrOmj8DYKbB)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
