{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">GAN — Генеративно-состязательные нейронные сети</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение  в генеративно-состязательные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом курсе мы, в основном, работали с **размеченными** данными. Мы научили нейронные сети решать задачи классификации, сегментации и т.д. \n",
    "\n",
    "В этой лекции мы разберемся, как научить нейросеть создавать что-то новое.\n",
    "\n",
    "\n",
    "Как подойти к такой задаче с помощью нейронных сетей?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Постановка задачи генерации**\n",
    "\n",
    "**Дано**: неразмеченные данные\n",
    "\n",
    "**Выход**: новые данные, которые будут удовлетворять следующим условиям:\n",
    "* Новые данные должны быть **похожи** на исходные.\n",
    "* Но **не повторять** их в точности (или повторять, при случайном стечении обстоятельств).\n",
    "* Чтобы результат генерации был различным при разных запусках, нам нужен **элемент случайности**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Эволюция в генерации изображений лиц:**\n",
    "\n",
    "[Множество примеров различных генераторов](https://thisxdoesnotexist.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/faces_generation_quality_progress.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся с **элементом случайности**. В нейронных сетях мы привыкли к **воспроизводимости** результата: в режиме валидации мы можем несколько раз подать на вход один и тот же объект и получить один и тот результат.  Возникает два вопроса:\n",
    "- что подавать на **вход** сети для генерации?\n",
    "- как реализовать **случайность**?\n",
    "\n",
    "Ответ на оба вопроса: подавать в качестве **входа** вектор **случайного шума**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/generator_model_pipeline.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему именно **вектор**? Почему не одно **случайное число**? \n",
    "\n",
    "**Ответ**: входной вектор можно рассматривать, как **признаки** генерируемого объекта. Каждый такой признак - **независимая случайная величина**. Если мы будем передавать только одно случайное число, то генерация будет однообразной. Чем больше признаков (степеней свободы) у входного вектора, тем разнообразнее будет результат генерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть случайный шум **большей размерности** даёт нам **больше вариабельности**  для генерации. Это называется **input latent space** - входное латентное пространство.\n",
    "\n",
    "**Note:** *из-за неустоявшейся терминологии случайное распределение на входе генератора называется латентным пространством, так же как и скрытое пространство в автоэнкодерах. Поэтому в этой лекции будем называть его **входным** латентным пространством. Также в статьях встречается вариант: predefined latent space.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждую компоненту латентного пространства можно рассматривать как отдельную шкалу, вдоль которой изменяются определенные свойства генерируемых объектов. Например, можно выбрать четыре латентных вектора и посмотреть, как генерируемые объекты плавно изменяются при переходе от одного вектора к другому.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/input_latent_space_lin_interpol.png\" width=\"600\">\n",
    "\n",
    "*Линейные интерполяции между четырьмя изображениями в латентном пространстве.*\n",
    "\n",
    "[M. Pieters, M. Wiering](https://arxiv.org/abs/1803.09093)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Размерность входного латентного пространства"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В выборе размерности входного латентного пространства важно соблюсти  баланс.\n",
    "- при **низкой размерности** возникнет проблема **низкой вариабельности**. \n",
    "\n",
    "Пример: генератор лиц с входным вектором длины 1. Результатом работы генератора будет всего одна шкала, вдоль которой будут расположены генерируемые изображения. Скорее всего, генератор выучит наиболее простую и \"очевидную\" шкалу - от молодой женщины блондинки к пожилому мужчине брюнету. У такой сети будет низкая вариабельность - она не сможет сгенерировать, например, рыжего ребенка в очках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- при **большой размерности** латентного пространства, пространство может быть слишком **разреженным**. \n",
    "\n",
    "При обучении модели количество точек в этом латентном пространстве будет настолько мало, что в основном пространство будет состоять из пустот. Тогда модель будет крайне некачественно генерировать объекты в точках латентного пространства, далеких от точек обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший способ выбрать длину вектора - это найти публикацию с похожей задачей и взять значение из нее.\n",
    "\n",
    "Если такой информации нет, то придется экспериментировать. Лучше начинать с низкой размерности латентного пространства, чтобы наладить работу всей сети, пусть и с низким разнообразием, а затем проводить эксперименты по поиску оптимальной размерности.\n",
    "\n",
    "Можно использовать собственные знания в предметной области: спросите себя, сколькими вещественными числами можно описать важную информацию об объекте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распределение входных латентных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы знаем из лекции про обучение сети, инициализация весов и нормализация входных данных имеют существенный вклад в работу модели. Поэтому, принято использовать **многомерное нормальное распределение** для input latent space. Оно лучше взаимодействует с весами модели и улучшает сходимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/binomial_distribution.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em><a href=\"https://matplotlib.org/3.1.0/gallery/lines_bars_and_markers/scatter_hist.html\">Двумерное нормальное распределение</a></p> </em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивный подход в решении задачи генерации\n",
    "(как делать на практике НЕ нужно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем создать генератор точек на параболе. Самым тривиальным решением, кажется, подача случайного шума на вход сети и будем ожидать на выходе точки параболы. Проверим, как это будет работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x * x\n",
    "    return np.hstack((x.reshape(-1, 1), y.reshape(-1, 1))) # Create num of correct dots(x,y) on parabola \n",
    "\n",
    "pairs = gen_pair(100)\n",
    "plt.scatter(pairs[:, 0], pairs[:, 1])\n",
    "plt.title(\"Random dots on parabola,\\nwhich will use like a dataset.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём размерность входного латентного пространства $ls = 1$ и объединим шум с точками в датасеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define input parameters \n",
    "n_batches = 10\n",
    "batch_size = 128\n",
    "ls = 1 # latent space\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(size=(n_batches * batch_size, ls), dtype=torch.float)\n",
    "print(f\"NN Input: noise.shape: {noise.shape}\")\n",
    "\n",
    "# Generates dots on parabola\n",
    "xy_pair = gen_pair(num=(n_batches * batch_size))\n",
    "xy_pair = torch.tensor(xy_pair, dtype=torch.float)\n",
    "print(f\"NN Output: xy_pair.shape: {xy_pair.shape}\")\n",
    "\n",
    "dataset = TensorDataset(noise, xy_pair) # model inputs, model outputs\n",
    "trainset, testset = train_test_split(dataset, train_size=0.8) # split dataset for train and test\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим простую модель, которая будет ожидать шум на вход, и генерировать точки на выходе. (Обратите внимание, что функция активации на последнем слое отсутствует, поскольку мы не ограничиваем наш генератор в каком-то диапазоне.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GenModel(nn.Module):\n",
    "    def __init__(self, latent_space):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2)) # x,y\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки лосса.\n",
    "\n",
    "Так как мы не знаем, в каком месте параболы генератор создаст новую точку, то непонятно, с каким элементом из датасета ее сравнивать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №1**\n",
    "\n",
    "Для сгенерированного $x$ аналитически вычислять $y_{target}=x*x$ и считать разницу между $y$ сгенерированным моделью и $y_{target}$ вычисленным аналитически:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pair, label):\n",
    "  # All inputs are batches\n",
    "  x_fake = pair[:, 0] \n",
    "  y_fake = pair[:, 1]\n",
    "  return torch.abs(x_fake * x_fake - y_fake).mean() # average by batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это будет работать. \n",
    "\n",
    "Однако, если мы знаем способ точно предсказать выход по входу значит задача уже решена и нейронная сеть не нужна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №2**\n",
    "\n",
    "Найти в датасете точку  $ target = (x_{target},y_{target})$ наиболее близкую к созданной генератором $ generated = (x,y)$ и использовать расстояние между этими точками качестве лосс. \n",
    "\n",
    "$$ Loss = min(dist(target_{i},generated))$$\n",
    "\n",
    "В пространстве высокой размерности такой поиск будет весьма ресурсозатратным, но в нашем учебном примере работать будет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "  def __init__(self, targets):\n",
    "    super().__init__()\n",
    "    self.targets = targets # Remember all real samples, impossible in real world\n",
    "\n",
    "  def forward(self,input, dummy_target=None):\n",
    "    dist = torch.cdist(input, self.targets) # claculate pairwise distances (euc.)\n",
    "    min_dist, index = torch.min(dist, dim = 1) # take the best\n",
    "    return min_dist.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательный код для вывода loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_test_loss(model, loader):\n",
    "    test_data = next(iter(loader))\n",
    "    test_loss = Loss(test_data[1])\n",
    "    outputs = model(samples.to(device))\n",
    "    return test_loss(outputs.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной код обучения.\n",
    "\n",
    "Целевые точки из датасета запоминаются в loss, затем идет обычный цикл обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 600\n",
    "model = GenModel(latent_space = ls)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "all_train_targets = next(iter(train_loader))[1]\n",
    "criterion = Loss(all_train_targets.to(device))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_epoch = 0\n",
    "    for samples, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples.to(device))\n",
    "        loss = criterion(outputs.to(device), labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "        \n",
    "    loss_test = get_test_loss(model, test_loader)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch={epoch} train_loss={loss_epoch/len(train_loader):.4} test_loss={loss_test:.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим результаты генерации на шуме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(model, pairs, ls=1):\n",
    "    model.eval().to('cpu')\n",
    "    noise = torch.tensor(np.random.normal(size=(1000, ls)), dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "        xy_pair_gen = model(noise)\n",
    "\n",
    "    xy_pair_gen = xy_pair_gen.detach().numpy()\n",
    "    plt.scatter(pairs[:, 0], pairs[:, 1], color='red', label='real')\n",
    "    plt.scatter(xy_pair_gen[:, 0], xy_pair_gen[:, 1], color='blue', label='generated')\n",
    "    plt.axis([-1, 1, 0, 1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    model.to(device)\n",
    "\n",
    "test_image(model, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что модель генерирует точки, лежащие на параболе, при этом все они лежат в довольно узком интервале по оси х. \n",
    "\n",
    "\n",
    "Это неудивительно: в лосс - функции мы прописали, что сгенерированная точка должна лежать на параболе и модель обучилась. А информацию о том, в каких частях кривой должны оказаться точки, мы в loss никак не кодировали.\n",
    "\n",
    "Более того модель может научиться хорошо генерировать одну единственную точку и при этом лосс может стать нулевым.\n",
    "\n",
    "\n",
    "Итак, надо решить две проблемы:\n",
    "\n",
    "\n",
    "1.   Закодировать в Loss условие о том, что точки должны быть различными\n",
    "2.   Придумать способ проверки не требующий перебора всего датасета\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дискриминатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем наказывать нейронную сеть не напрямую стандартной лосс функцией, а второй **сетью**, которая определяет, лежит ли сгенерированная точка на параболе. \n",
    "\n",
    "Создадим сеть-классификатор точек (лежит/не лежит на параболе), которую назовём **дискриминатор** или критик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisModel(nn.Module):\n",
    "    def __init__(self, n_points):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15, 1), # real/fake\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 2 * n_points)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае задача дискриминатора - определять, принадлежит ли объект к распределению обучающей выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итого** мы имеем: \n",
    "- **генератор**, выдающий точки, которые могут принадлежать параболе, а могут не принадлежать ей\n",
    "- **дискриминатор**, который будет их различать\n",
    "\n",
    "Мы будем подавать в **дискриминатор** **правильные точки** (чтобы он знал, как это должно выглядеть), и **точки, которые выдаёт генератор**, считая их подделкой.\n",
    "\n",
    "Таким образом, **генератор** будет учиться **подражать** реальным данным, а дискриминатор будет учиться **отличать** реальные точки, от подделок. \n",
    "\n",
    "Мы пришли к идее **генеративно-состязательных** нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generative adversarial network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2014 Generative Adversarial Networks (Goodfellow et al., 2014)](https://arxiv.org/abs/1406.2661) (**Cited by 33430!!!**)\n",
    "\n",
    "[Видео разбор оригинальной статьи](https://youtu.be/eyxmSmjmNS0)\n",
    "\n",
    "[Видео лекции Иана Гудфеллоу](https://www.youtube.com/watch?v=HGYYEUSm-0Q)\n",
    "\n",
    "**Генеративно-состязательную** сеть описал Иан Гудфеллоу из компании Google (на тот момент) в 2014 году. Сейчас он возглавляет подразделение машинного обучения в Apple. Принцип состязательности в сети **GAN** нередко описывается через метафоры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/generative_adversarial_network_scheme.png\" width=\"700\"></center>\n",
    "    <center><em>Схематичное представление архитектуры GAN </em></center>\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор - фальшивомонетчик!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще со времен **AlexNet** мы знаем, что если мы что-то и умеем делать с нейросетями - так это **классификаторы**. В классическом GAN **дискриминатор** выполняет простейшую из задач классификации - **бинарную классификацию** (либо *real*, либо *fake*). А вот задача **генерации** каким-то прямым образом на тот момент решена не была.\n",
    "\n",
    "Как использовать всю мощь классификатора для создания генератора?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что есть фальшивомонетчик $G$ (generator) и банкир с прибором для проверки подлинности купюр $D$ (discriminator).\n",
    "\n",
    "Фальшивомонетчик черпает вдохновение из генератора случайных чисел в виде случайного шума $z$ и создает подделки $G(z)$.\n",
    "\n",
    "Банкир $D$ получает на вход пачку купюр $x$, проверяет их подлинность и сообщает вектор $D(x)$, состоящий из чисел от нуля до единицы - свою уверенность (вероятность) по каждой купюре в том, что она настоящая. Его цель - выдавать нули для подделок $D(G(z))$ и единицы для настоящих денег $D(x)$. Задачу можно записать как максимизацию произведения $D(x)(1-D(G(z)))$, а произведение, в свою очередь, можно представить как сумму через логарифм.\n",
    "\n",
    "Таким образом, задача банкира - максимизировать $log(D(x))+log(1-D(G(z)))$.\n",
    "\n",
    "Цель фальшивомонетчика прямо противоположна - максимизировать $D(G(z))$, то есть убедить банкира в том, что подделки настоящие.\n",
    "\n",
    "Продолжая аналогию, обучение генератора можно представить так: фальшивомонетчик не просто генерирует подделки наудачу. Он добывает прибор для распознавания подделок, разбирает его, смотрит, как тот работает, и затем создает подделки, которые смогут обмануть этот прибор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математически - это **[игра](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%B8%D0%B3%D1%80) двух игроков**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min\\limits_{\\theta_g}  \\max\\limits_{\\theta_d} [\\mathbb{E}_{x _\\sim p(data)} log(D_{\\theta_d}(x)]+\\mathbb{E}_{z _\\sim p(z)} \n",
    "[log(1-D_{\\theta_d}(G_{\\theta_g}(z))]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дискриминатор** \n",
    "- обучается при **фиксированном генераторе** ${G}_{\\theta_{g}}$,\n",
    "- **максимизирует** функцию выше относительно $\\theta_d$ (**градиентный подъем**),\n",
    "- решает задачу **бинарной классификации**: старается присвоить $1$ точкам данных из обучающего набора $E_{x∼p_{data}}$ и 0 сгенерированным выборкам $E_{z∼p(z)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Генератор**\n",
    "- обучается при **фиксированном дискриминаторе** $D_{θ_d}$,\n",
    "- получает градиенты весов за счет backpropagation через дискриминатор,\n",
    "- **минимизирует** функцию выше относительно $\\theta_d$ (**градиентный спуск**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посредством **чередования** градиентного **подъема** и **спуска** сеть можно обучить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **подъем** на **дискриминаторе**:\n",
    "\n",
    "\n",
    "$$\\max\\limits_{\\theta_d} [\\mathbb{E}_{x _\\tilde{}p(data)} log(D_{\\theta_d}(x)+\\mathbb{E}_{z _\\tilde{}p(z)} log(1-D_{\\theta_d}(G_{\\theta_g}(z)))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **спуск** на **генераторе**:\n",
    "\n",
    "\n",
    "$$\\min\\limits_{\\theta_g} \\mathbb{E}_{z _\\tilde{}p(z)} log(1-D_{\\theta_d}(G_{\\theta_d}(z)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный **спуск** на **генераторе** эквивалентен градиентному **подъему**\n",
    "\n",
    "$$\\max\\limits_{\\theta_g} \\mathbb{E}_{z _\\tilde{}p(z)} log(D_{\\theta_d}(G_{\\theta_d}(z)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе совместного конкурентного обучения, если система достаточно сбалансирована, достигается **минимаксное состояние равновесия**, в котором обе сети эффективно учатся.\n",
    "\n",
    "Сгенерированные удачно обученной нейросетью изображения практически неотличимы от настоящих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хорошенько подумать - то можно прийти к выводу, что **loss-функция** в **GAN** это не какая-то функция заданная людьми, а еще одна **нейросеть**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преимущества GAN**\n",
    "* Теоретические **гарантии сходимости**\n",
    "* Можно обучать обычным **SGD/Adam**\n",
    "* Решает в явном виде задачу **generative modeling**, но неявным образом (**нейросети**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Недостатки GAN**\n",
    "* **Нестабильное обучение**\n",
    "* Очень **долгая сходимость**\n",
    "* **Mode-collapsing** (модель выдает одно и то же изображение или один и тот же класс и т.д., независимо от того, какие входные данные ей подаются)\n",
    "* **Исчезновение градиента**: дискриминатор настолько хорошо научился отличать сгенерированные образцы от реальных, что градиент весов генератора становится равным 0: в какую сторону бы генератор не изменил свои веса, дискриминатор все равно идеально распознает фальшивки\n",
    "* Поиск оптимальных параметров - **pure luck**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Практический пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим наш **генератор** и **дискриминатор**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_space, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_space, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)) # x,y\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1), # real/fake\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим **входные параметры**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10 # latent space \n",
    "num_epochs = 10000\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что у нас, так же как и в первом примере, есть переменная **latent space**. Это тот шум, из которого мы будем генерировать наши точки. Закон сохранения масс в действии - нельзя создать что-то из ничего!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим все необходимое для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "criterion = nn.BCELoss() # Loss\n",
    "gen = Generator(latent_space=latent_dim, hidden_dim=50).to(device)\n",
    "disc = Discriminator(hidden_dim=50).to(device)\n",
    "\n",
    "# 2 optimizers for Discriminator and Generator \n",
    "optimizerD = torch.optim.Adam(disc.parameters(), lr=3e-4)\n",
    "optimizerG = torch.optim.Adam(gen.parameters(), lr=3e-4)\n",
    "\n",
    "# Fix noise to compare\n",
    "fixed_noise = torch.randn(128, latent_dim, device=device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы используем `BCELoss` (**Binary Cross Entropy**). Давайте разберемся почему:\n",
    "\n",
    "- **Дискриминатор** решает задачу **бинарной классификации**. Для этой задачи хорошо подходит **BCE**.\n",
    "- Требование к генератору может быть сформулировано как \"объектам сгенерированным **генератором**, **дискриминатором** должна быть присвоена **высокая вероятность**\". Для **\"идеального\" генератора**, который всегда генерирует фотореалистичные результаты, значения **$D(G(z))$** всегда должны быть **близки к 1**. Для этой задачи хорошо подходит **BCE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которая создает точки на параболе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair(num=100):\n",
    "    x = np.random.uniform(low=-1, high=1, size=(num,))\n",
    "    y = x * x\n",
    "    return torch.tensor(np.hstack((x.reshape(-1, 1), y.reshape(-1, 1))), dtype=torch.float) # Create num of correct dots(x,y) on parabola "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что сейчас будет происходить? $$$$\n",
    "\n",
    "* Обучение дискриминатора\n",
    "    * обнулим градиенты **дискриминатора**\n",
    "    * real точки\n",
    "        * создадим набор **real точек**, которые лежат на параболе\n",
    "        * посчитаем лосс дискриминатора на **real точках** и **real метках** $\\text{loss D}_\\text{real}$\n",
    "        * посчитаем градиенты для **дискриминатора**\n",
    "\n",
    "    <img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gan_training_algorithm_1.png\" width=\"1000\">\n",
    "\n",
    "    * fake точки\n",
    "        * сгенерируем случайный шум $z$\n",
    "        * возьмем наш не обученный **генератор** и создадим с его помощью **fake точки** из $z$\n",
    "        * посчитаем лосс дискриминатора на **fake точках** и **fake метках** $\\text{loss D}_\\text{fake}$\n",
    "        * посчитаем градиенты для **дискриминатора** (они сложатся с уже посчитанными ранее)\n",
    "    * обновление весов \n",
    "        * сделаем шаг обучения **дискриминатора** (обвновим его веса)\n",
    "        * **генератор** не обучается\n",
    "\n",
    "    <img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gan_training_algorithm_2.png\" width=\"1000\">\n",
    "\n",
    "* Обучение генератора\n",
    "    * обнулим градиенты **генератора**\n",
    "    * сгенерируем случайный шум $z$\n",
    "    * создадим с помощью **генератора** набор **fake точек** из $z$\n",
    "    * посчитаем значение функции потерь дискриминатора на **fake точках** и **real метках** $\\text{loss G}$ (подмена меток)\n",
    "    * посчитаем градиенты для **генератора**\n",
    "    * сделаем шаг обучения **генератора** (обвновим его веса)\n",
    "    * **дискриминатор** не обучается\n",
    "\n",
    "    <img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gan_training_algorithm_3.png\" width=\"1000\">\n",
    "\n",
    "Смотрите код внимательно, чтобы понять, о чем речь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "#Main Training Loop\n",
    "print(\"Training...\")\n",
    "print(device)\n",
    "\n",
    "x = []\n",
    "y_D = []\n",
    "y_G = []\n",
    "for epoch in range(num_epochs):\n",
    "      #max log(D(x)) + log(1 - D(G(z)))\n",
    "      #train on real points\n",
    "      disc.zero_grad()\n",
    "      \n",
    "      # Define real points\n",
    "      real_points = gen_pair(num=batch_size).to(device)\n",
    "      label = torch.full((batch_size,), real_label, dtype=torch.float, device=device).view(-1)\n",
    "      \n",
    "      # Train disc on real_points\n",
    "      output = disc(real_points).view(-1)\n",
    "      errD_real = criterion(output, label)\n",
    "      errD_real.backward()\n",
    "\n",
    "      # Define fake points\n",
    "      # This dots generated by generator transform from latent space \n",
    "      noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "      fake_points = gen(noise)\n",
    "      label.fill_(fake_label)\n",
    "\n",
    "      # Train disc on fake_points \n",
    "      output = disc(fake_points.detach()).view(-1)\n",
    "      errD_fake = criterion(output, label)\n",
    "      errD_fake.backward()\n",
    "\n",
    "      # Discriminator loss(real+fake)\n",
    "      errD = errD_real + errD_fake\n",
    "\n",
    "      optimizerD.step()\n",
    "\n",
    "      #max log(D(G(z)))\n",
    "      # Now, train generator\n",
    "      gen.zero_grad()\n",
    "\n",
    "      # Let's tell the discriminator that our generator creates real points\n",
    "      label.fill_(real_label)\n",
    "\n",
    "      output = disc(fake_points).view(-1)\n",
    "\n",
    "      errG = criterion(output, label)\n",
    "\n",
    "      errG.backward()\n",
    "\n",
    "      optimizerG.step()\n",
    "\n",
    "      # Plotting every N epoch \n",
    "      x.append(epoch)\n",
    "      y_D.append(errD.item() / 2)\n",
    "      y_G.append(errG.item())\n",
    "      \n",
    "      if epoch % 250 == 0:\n",
    "        fig, ax = plt.subplots(nrows=2)\n",
    "        ax[0].plot(x, y_D, color='red', lw=1, label='D')\n",
    "        ax[0].plot(x, y_G, color='green', lw=1, label='G')\n",
    "\n",
    "        # Generates dots from fixed_noise \n",
    "        fake_points = gen(fixed_noise)\n",
    "        ax[1].scatter(fake_points.detach().to('cpu')[:, 0], fake_points.detach().to('cpu')[:, 1], color='green', alpha=0.5)\n",
    "        ax[1].scatter(real_points.detach().to('cpu')[:, 0], real_points.detach().to('cpu')[:, 1], color='red', alpha=0.5)\n",
    "        ax[1].set_xlim(-1, 1)\n",
    "        ax[1].set_ylim(0, 1)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Класс!** У нас получилось (если вдруг не сошлось за 10000 эпох, перезапустите заново, к сожалению, **фиксация seed еще не гарантирует стабильность GAN**). Особенно круто смотреть как красиво лосс **дискриминатора** и **генератора** сходятся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN — Генерация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью **GAN** можно, разумеется, генерировать не только точки на параболе. Можно генерировать, например, изображения. Но поднимаются закономерные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как из шума на входе сети получить изображение?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым простым ответом будет: взять шум, пропустить его через **полносвязные слои** и сделать **reshape** до нужного разрешения. В целом, это будет работать.\n",
    "\n",
    "\n",
    "Однако **DCGAN - Deep Convolutional GAN** использует **сверточные** и **сверточно-транспонированные** (*convolutional* и *convolutional-transpose*) слои в дискриминаторе и генераторе соответственно. Впервые метод **DCGAN** был описан в статье [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford et al., 2015)](https://arxiv.org/abs/1511.06434)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/deep_convolutional_gan_scheme.png\" width=\"700\"></center>\n",
    "<center><em>Схема работы DCGAN (Radford et al., 2015).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже видна разница в генерации при помощи исключительно **полносвязных слоёв** и при помощи **обратных свёрток**. Очевидно, результат **DCGAN** лучше, чем **GAN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/gan_dcgan_mnist_examples.png\" width=\"600\"></center>\n",
    "<center><em>Сравнение результатов на MNIST (Radford et al., 2015)</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход генератора подают шум для создания разнообразных объектов. Этот шум представляет собой вектор в многомерном пространстве. Один вектор - один сгенерированный объект. Задача дискриминатора - преобразовать вектор в изображение.\n",
    "\n",
    "Такое преобразование возможно при помощи транспонированных сверточных (convolution-transpose, иногда называют fractionally strided convolution) слоев. Как и обычные сверточные слои, эти слои используют сверточные ядра, но перед вычислением сверток они увеличивают размер исходного изображения, \"раздвигая\" пиксели и заполняя образующиеся промежутки между пикселями нулями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/dcgan_architecture.png\" width=\"700\"></center>\n",
    "<center><em>Зеркальная архитектура DCGAN </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution-Transpose Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте кратко вспомним, что делают CT слои?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как работают **транспонированные сверточные** слои хорошо демонстрирует рисунок ниже. Между точками исходной карты признаков (темно синие квадраты) вставляется сетка из нулей (голубые квадраты) после чего над полученной разреженной матрицей признаков совершается обычная **свертка** с шагом 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из изображения 3х3 в изображение 5х5 с использованием паддинга (Padding, strides, transposed):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/convolution_transpose_layer.gif\" </center>\n",
    "<center><em>Из изображения 3х3 в изображение 5х5 с использованием паддинга (Padding, strides, transposed).</em></center>\n",
    "\n",
    "[Подробнее можно почитать тут](https://github.com/vdumoulin/conv_arithmetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10)) * 255 # one 3-channel image with 10x10 size\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convT = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "y = convT(x)\n",
    "print(y.shape)  # One 3-chanells image with 12x12 size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученное изображение не похоже на входное - потому что были применены свёрточные ядра со случайными коэффициентами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "ax[0].imshow(x[0].permute(1, 2, 0).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow(y[0].permute(1, 2, 0).detach().numpy().astype(np.uint8))\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('After ConvTranspose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Другие способы повышения разрешения  - Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо **обратных свёрток**, существует другие методы повышения разрешения из низкой размерности.\n",
    "\n",
    "Самый простой способ - выполнить повышение разрешения с помощью **интерполяции**. Давайте вспомним, что в PyTorch это осуществляется слоем [Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 3, 10, 10)) # one 3-channal image with 10x10 size\n",
    "print(\"Input shape:\", x.shape)\n",
    "    \n",
    "upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "y = upsample(x)\n",
    "\n",
    "print(\"Output shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "ax[0].imshow((x[0].permute(1, 2, 0) * 256).detach().numpy().astype(np.uint8))\n",
    "ax[1].imshow((y[0].permute(1, 2, 0).detach().numpy() * 256).astype(np.uint8))\n",
    "ax[0].set_title('Input')\n",
    "ax[1].set_title('After Upsample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обученного DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на пример обученного **DCGAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'DCGAN', pretrained=True, useGPU=use_gpu)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "num_images = 16\n",
    "noise, _ = model.buildNoiseData(num_images)\n",
    "with torch.no_grad():\n",
    "    generated_images = model.test(noise)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16 * 3, 2 * 3))\n",
    "ax.imshow(torchvision.utils.make_grid(generated_images).permute(1, 2, 0).cpu().numpy(), \n",
    "          interpolation='nearest', aspect='equal')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практический пример DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем сами написать свой **DCGAN** и обучить его на датасете **FashionMNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2        # Num of epochs\n",
    "batch_size = 64       # batch size\n",
    "lr = 2e-4             # Learning rate\n",
    "b1 = 0.5              # Adam: decay of first order momentum of gradient\n",
    "b2 = 0.999            # Adam: decay of first order momentum of gradient\n",
    "num_cpu = 8           # Num of cpu threads to generate batch \n",
    "latent_dim = 100      # latent space\n",
    "img_size = 32         # images size\n",
    "channels = 1          # Num of channels \n",
    "sample_interval = 450 # interval between image sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно мы **инициализируем веса** случайным образом, но ничто не мешает нам инициализировать их так, как мы хотим. В [оригинальной статье](https://arxiv.org/pdf/1511.06434.pdf) про **DCGAN** предложено инициализировать веса нормальным распределением с центром в нуле и стандартным отклонением 0,02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, как преобразуется **шум** в **генераторе**:\n",
    "* Сначала с помощью **полносвязного слоя** он преобразуется в **первичные фичи**\n",
    "* Потом с помощью функции **view**, **ресэмплится** в картинку низкого разрешения\n",
    "* Потом, проходя через **conv_blocks** поочерёдно применяются **Upsample** и **ОБЫЧНЫЕ** свёртки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize Generator and Discriminator \n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Initialize weight\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для отображения изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_gen_img(model, latent_dim=100):\n",
    "    z = Tensor(np.random.normal(0, 1, (9, latent_dim))) # define latent dim\n",
    "    \n",
    "    # Generate noise from latent dim \n",
    "    sample_images = generator(z) \n",
    "    sample_images = sample_images.cpu().detach()\n",
    "\n",
    "    # Plotting images\n",
    "    grid = make_grid(sample_images, nrow=3, ncols=3, normalize=True).permute(1, 2, 0).numpy()\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим данные и загрузим и их в Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Tensor(imgs.shape[0], 1).fill_(1.0)\n",
    "        fake = Tensor(imgs.shape[0], 1).fill_(0.0)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(Tensor)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        batches_done = epoch * len(data_loader) + i\n",
    "        if batches_done % sample_interval == 0:\n",
    "            print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, num_epochs, i, len(data_loader), d_loss.item(), g_loss.item()))\n",
    "            fig = show_gen_img(generator)\n",
    "            plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы картинки обрели приличный вид, хватает 2 эпох. Чтобы стали выглядеть хорошо - 5 эпох."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cGAN — GAN с условием"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cGAN** расшифровывается как **Conditional Generative Adversarial Net** - это **GAN** с условием. Условие может быть любым, например, генерация конкретной цифры. В этом случае нам нужен уже размеченный датасет, для того чтобы обучить дискриминатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/conditional_gan_scheme.png\" width=\"800\"></center>\n",
    "<center><em>Схема работы cGAN. Label Y добавляется к случайному шуму, тем самым мы говорим генератору генерировать случайное изображение нужного класса. Так же он подаётся в дискриминатор в качестве входа, чтобы дискриминатор знал какое изображение классифицировать как реальное, а какое как вымышленное.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение в данном случае будет аналогичным обучению **GAN**, мы будем обучать сети, чередуя реальные данные и сгенерированные, добавив `label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/cGANS_results_20_and_50_epochs_mnist.png\" width=\"600\"></center>\n",
    "<center><em>Сравнение результатов cGAN и cDCGAN.</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как закодировать метки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку подавать в сеть числа от 0 до 9 (в случае **MNSIT**) нет смысла, то нужно придумать, как подавать их в нейронную сеть. На помощь приходят **Embeddings**. Мы можем представить каждую метку в виде вектора с десятью элементами.\n",
    "\n",
    "[Документация nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = next(iter(data_loader))\n",
    "\n",
    "label_emb = nn.Embedding(10, 10)\n",
    "\n",
    "e = label_emb(labels)\n",
    "\n",
    "print(f\"Label: {labels[0]}\")\n",
    "print(f\"Embedding for this label: {e[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После чего, **эмбединги** меток обычно склеиваются с входами сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Почему нельзя подать просто число?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы будем подавать просто число, например 0.1 для единицы, и 0.5 для пяти, то вход у нас будет непрерывным, что довольно нелогично, тогда при небольшом изменении входа, мы будем генерировать другую цифру. А так же потому что сети будет сложнее выучить небольшие расхождения в этом небольшом интервале. В случае с векторным представлением мы избегаем этих проблем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модификации cGAN\n",
    "\n",
    "Метки классов можно подавать не только способом, описанным выше. Можно вместо подачи их в дискриминатор сделать так, чтобы он их предсказывал - **Semi-Supervized GAN**.\n",
    "\n",
    "Или же не подавать label в дискриминатор, но ждать от него классификации в соответствии с классом, который мы хотим получить от генератора - это **InfoGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна модификация cGAN - это **AC-GAN** (auxiliary classifier) в которой единственное различие заключается в том, что дискриминатор должен помимо распознавания реальных и фейковых изображений ещё и классифицировать их. Он имеет эффект стабилизации процесса обучения и позволяет генерировать большие высококачественные изображения, изучая представление в скрытом пространстве, которое не зависит от метки класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gans_zoo_schemes.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProGAN, StyleGAN, StyleGAN2, Alias-Free GAN\n",
    "\n",
    "[2017 Progressive Growing of GANs for Improved Quality, Stability, and Variation (ProGAN) [Karras et al., 2017]](https://arxiv.org/abs/1710.10196)\n",
    "\n",
    "[2018 A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN) [Karras et al., 2018]](https://arxiv.org/abs/1812.04948)\n",
    "\n",
    "[2019 Analyzing and Improving the Image Quality of StyleGAN (StyleGAN2) [Karras et al., 2019]](https://arxiv.org/abs/1912.04958)\n",
    "\n",
    "[2021 Alias-Free Generative Adversarial Networks (Alias-Free GAN) [Karras et al., 2021]](https://arxiv.org/abs/2106.12423)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На момент обновления блокнота (лето 2022 года), топовой архитектурой **GAN** для генерации изображений считается **Alias Free GAN**. Это можно посмотреть, например, [тут](https://paperswithcode.com/task/image-generation). В большинстве соревнований выигрывает **StyleGAN-XL** - обученный на картинки большого размера **Alias Free GAN**.\n",
    "\n",
    "Для того что бы понять как он работает, давайте проследим за эволюцией работ от коллектива авторов из NVIDIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ProGAN**, или **Progressively Growing GAN** - это генеративная состязательная сеть, призванная улучшить качество генерации изображений с помощью добавления слоёв во время генерации. Сначала сеть учится генерировать **изображения низкого разрешения**, потом добавляется слой, который создаёт **более высокое разрешение** из фич предыдущего слоя и так далее. \n",
    "\n",
    "Другими словами, вместо того, чтобы пытаться обучить сразу все слои генератора и дискриминатора, как это обычно делается, авторы статьи постепенно **наращивали** свой **GAN**, по одному слою за раз, для обработки все более высоких разрешений изображений.\n",
    "\n",
    "\n",
    "Эта идея позволяет **плавно увеличивать размер** изображения и добиваться **лучшей сходимости**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/pro_gan_how_it_works.gif\" width=\"600\"></center>\n",
    "<center><em>Принцип работы ProGAN (Karras et al., 2017)</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы все получилось авторы сначала искусственно уменьшили свои обучающие изображения до очень **маленького разрешения** (всего **4x4** пикселя). Они создали **генератор** с несколькими слоями для синтеза изображений с таким низким разрешением и соответствующий **дискриминатор** с зеркальной архитектурой. Поскольку эти сети были такими маленькими, они обучались относительно быстро, но усваивали только **крупномасштабные структуры**, видимые на сильно размытых изображениях.\n",
    "\n",
    "Когда первые слои завершили обучение, они добавили еще один слой к **G** и **D**, удвоив выходное разрешение до **8x8**. **Обученные веса** в предыдущих слоях сохранялись, но **не фиксировались**. \n",
    "\n",
    "Обучение продолжалось до тех пор, пока **GAN** снова не стал синтезировать убедительные изображения, на этот раз в новом разрешении **8x8**.\n",
    "\n",
    "Таким образом, они продолжали добавлять слои, удваивать разрешение и тренироваться до тех пор, пока не был достигнут **желаемый размер** выходного сигнала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучать мы его, конечно же, не будем, но поиграться с обученной моделью можем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "\n",
    "# load a model trained on a celebrity dataset \"celebA\"\n",
    "# resolution of generated images 512 x 512 pixel\n",
    "model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub',\n",
    "                       'PGAN', model_name='celebAHQ-512',\n",
    "                       pretrained=True, useGPU=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем что-нибудь сгенерировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "noise, _ = model.buildNoiseData(num_images)\n",
    "with torch.no_grad():\n",
    "    generated_images = model.test(noise)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "grid = torchvision.utils.make_grid(generated_images.clamp(min=-1, max=1), scale_each=True, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).to('cpu').numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StyleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/style_gan_scheme.png\" width=\"400\"></center>\n",
    "<center><em>Сравнение архитектуры ProGAN и StyleGAN (<a href=\"https://arxiv.org/pdf/1812.04948.pdf\">Karras et al., 2018</a>)</p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея с **ProGAN** оказалась хорошей. В 2018 году, команде исследователей из **NVIDIA** (все той же что и в **ProGAN**) пришла в голову мысль как процесс обучения **GAN** можно было бы сделать еще лучше. Они представили **StyleGAN** - расширение **ProGAN**. В дополнение к инкрементальному **росту моделей** во время обучения, **Style GAN** значительно изменяет **архитектуру** самого **генератора**.\n",
    "\n",
    "Обычный **генератор** в **классическом GAN** работает так - берем **шум**, пропускаем его через некую обученную сеть-генератор и получаем **изображение** на выходе. Генератор же **StyleGAN** больше **не берет** на вход **вектор из латентного пространства**; вместо этого для создания синтетического изображения используются **два** новых **источника случайности**: отдельная картирующая сеть (**Mapping Network**) и добавление **гауссовского шума** к выходам слоев.\n",
    "\n",
    "На выходе из **Mapping Network** получают вектор, определяющий стили, который интегрируется в каждую точку модели генератора с помощью нового слоя, называемого **слоем адаптивной нормализации** (Adaptive Instance Normalization или **AdaIN**). Использование этого вектора стилей дает контроль над стилем генерируемого изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберемся подробнее с тем, как работают стили. Вектор стилей состоит из пар чисел $(y_{s,i}, y_{b,i})$, которые в блоке **AdaIN** модифицируют слои карты признаков по следующей формуле (нормировка и линейное преобразование): \n",
    "$$AdaIN(x_i, y_i) = \\frac{x_i \\mu(x_i)}{σ(x_i)}+y_{b,i} $$\n",
    "\n",
    "Для каждого блока **AdaIN** для каждого слоя карты признаков предназначена своя пара чисел. Вектор стиля генерируется для всех слоев одновременно, это позволяет обеспечить лучшую согласованность между признаками разной сложности. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/stylegan_style.png\" width=\"750\"></center>\n",
    "\n",
    "<center><em>Пример смешения стилей двух изображений. Изображения поделены на две группы A и B. Группа A - левый столбик, группа B - верхняя строка. При генерации остальных изображений, часть вектора стиля изображения из A заменяли на часть вектора стиля изображения из B. В верхних изображениях, были заменены векторы стиля, отвечающие за преобразование самых ранних слоев, отвечающих за общий силуэт изображения (форма и положение головы, пол и возраст). Ниже даны изображения, где заменили средние слои (на них пол и возраст сохраняется, при этом меняются черты лица). В последней строке заменили финальную часть вектора стиля (это повлияло только на цветовую гамму). (<a href=\"https://arxiv.org/pdf/1812.04948.pdf\">Karras et al., 2018</a>)</p> </em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title run StyleGAN\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
    "    def __init__(self, input_size, output_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True):\n",
    "        super().__init__()\n",
    "        he_std = gain * input_size**(-0.5) # He init\n",
    "        # Equalized learning rate and custom learning rate multiplier.\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "        return F.linear(x, self.weight * self.w_mul, bias)\n",
    "\n",
    "class MyConv2d(nn.Module):\n",
    "    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True,\n",
    "                intermediate=None, upscale=False):\n",
    "        super().__init__()\n",
    "        if upscale:\n",
    "            self.upscale = Upscale2d()\n",
    "        else:\n",
    "            self.upscale = None\n",
    "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5) # He init\n",
    "        self.kernel_size = kernel_size\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.intermediate = intermediate\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "        \n",
    "        have_convolution = False\n",
    "        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n",
    "            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n",
    "            # this really needs to be cleaned up and go into the conv...\n",
    "            w = self.weight * self.w_mul\n",
    "            w = w.permute(1, 0, 2, 3)\n",
    "            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n",
    "            w = F.pad(w, (1,1,1,1))\n",
    "            w = w[:, :, 1:, 1:]+ w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n",
    "            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1)-1)//2)\n",
    "            have_convolution = True\n",
    "        elif self.upscale is not None:\n",
    "            x = self.upscale(x)\n",
    "    \n",
    "        if not have_convolution and self.intermediate is None:\n",
    "            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size//2)\n",
    "        elif not have_convolution:\n",
    "            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size//2)\n",
    "        \n",
    "        if self.intermediate is not None:\n",
    "            x = self.intermediate(x)\n",
    "        if bias is not None:\n",
    "            x = x + bias.view(1, -1, 1, 1)\n",
    "        return x\n",
    "\n",
    "class NoiseLayer(nn.Module):\n",
    "    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(channels))\n",
    "        self.noise = None\n",
    "    \n",
    "    def forward(self, x, noise=None):\n",
    "        if noise is None and self.noise is None:\n",
    "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
    "        elif noise is None:\n",
    "            # here is a little trick: if you get all the noiselayers and set each\n",
    "            # modules .noise attribute, you can have pre-defined noise.\n",
    "            # Very useful for analysis\n",
    "            noise = self.noise\n",
    "        x = x + self.weight.view(1, -1, 1, 1) * noise\n",
    "        return x\n",
    "\n",
    "class StyleMod(nn.Module):\n",
    "    def __init__(self, latent_size, channels, use_wscale):\n",
    "        super(StyleMod, self).__init__()\n",
    "        self.lin = MyLinear(latent_size,\n",
    "                            channels * 2,\n",
    "                            gain=1.0, use_wscale=use_wscale)\n",
    "        \n",
    "    def forward(self, x, latent):\n",
    "        style = self.lin(latent) # style => [batch_size, n_channels*2]\n",
    "        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n",
    "        style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n",
    "        x = x * (style[:, 0] + 1.) + style[:, 1]\n",
    "        return x\n",
    "\n",
    "class PixelNormLayer(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "class BlurLayer(nn.Module):\n",
    "    def __init__(self, kernel=[1, 2, 1], normalize=True, flip=False, stride=1):\n",
    "        super(BlurLayer, self).__init__()\n",
    "        kernel=[1, 2, 1]\n",
    "        kernel = torch.tensor(kernel, dtype=torch.float32)\n",
    "        kernel = kernel[:, None] * kernel[None, :]\n",
    "        kernel = kernel[None, None]\n",
    "        if normalize:\n",
    "            kernel = kernel / kernel.sum()\n",
    "        if flip:\n",
    "            kernel = kernel[:, :, ::-1, ::-1]\n",
    "        self.register_buffer('kernel', kernel)\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # expand kernel channels\n",
    "        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n",
    "        x = F.conv2d(\n",
    "            x,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding=int((self.kernel.size(2)-1)/2),\n",
    "            groups=x.size(1)\n",
    "        )\n",
    "        return x\n",
    "\n",
    "def upscale2d(x, factor=2, gain=1):\n",
    "    assert x.dim() == 4\n",
    "    if gain != 1:\n",
    "        x = x * gain\n",
    "    if factor != 1:\n",
    "        shape = x.shape\n",
    "        x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n",
    "        x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n",
    "    return x\n",
    "\n",
    "class Upscale2d(nn.Module):\n",
    "    def __init__(self, factor=2, gain=1):\n",
    "        super().__init__()\n",
    "        assert isinstance(factor, int) and factor >= 1\n",
    "        self.gain = gain\n",
    "        self.factor = factor\n",
    "    def forward(self, x):\n",
    "        return upscale2d(x, factor=self.factor, gain=self.gain)\n",
    "\n",
    "class G_mapping(nn.Sequential):\n",
    "    def __init__(self, nonlinearity='lrelu', use_wscale=True):\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "        layers = [\n",
    "            ('pixel_norm', PixelNormLayer()),\n",
    "            ('dense0', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense0_act', act),\n",
    "            ('dense1', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense1_act', act),\n",
    "            ('dense2', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense2_act', act),\n",
    "            ('dense3', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense3_act', act),\n",
    "            ('dense4', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense4_act', act),\n",
    "            ('dense5', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense5_act', act),\n",
    "            ('dense6', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense6_act', act),\n",
    "            ('dense7', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense7_act', act)\n",
    "        ]\n",
    "        super().__init__(OrderedDict(layers))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        # Broadcast\n",
    "        x = x.unsqueeze(1).expand(-1, 18, -1)\n",
    "        return x\n",
    "\n",
    "class Truncation(nn.Module):\n",
    "    def __init__(self, avg_latent, max_layer=8, threshold=0.7):\n",
    "        super().__init__()\n",
    "        self.max_layer = max_layer\n",
    "        self.threshold = threshold\n",
    "        self.register_buffer('avg_latent', avg_latent)\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 3\n",
    "        interp = torch.lerp(self.avg_latent, x, self.threshold)\n",
    "        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1)\n",
    "        return torch.where(do_trunc, interp, x)\n",
    "\n",
    "class LayerEpilogue(nn.Module):\n",
    "    \"\"\"Things to do at the end of each layer.\"\"\"\n",
    "    def __init__(self, channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if use_noise:\n",
    "            layers.append(('noise', NoiseLayer(channels)))\n",
    "        layers.append(('activation', activation_layer))\n",
    "        if use_pixel_norm:\n",
    "            layers.append(('pixel_norm', PixelNorm()))\n",
    "        if use_instance_norm:\n",
    "            layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n",
    "        self.top_epi = nn.Sequential(OrderedDict(layers))\n",
    "        if use_styles:\n",
    "            self.style_mod = StyleMod(dlatent_size, channels, use_wscale=use_wscale)\n",
    "        else:\n",
    "            self.style_mod = None\n",
    "    def forward(self, x, dlatents_in_slice=None):\n",
    "        x = self.top_epi(x)\n",
    "        if self.style_mod is not None:\n",
    "            x = self.style_mod(x, dlatents_in_slice)\n",
    "        else:\n",
    "            assert dlatents_in_slice is None\n",
    "        return x\n",
    "\n",
    "\n",
    "class InputBlock(nn.Module):\n",
    "    def __init__(self, nf, dlatent_size, const_input_layer, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        super().__init__()\n",
    "        self.const_input_layer = const_input_layer\n",
    "        self.nf = nf\n",
    "        if self.const_input_layer:\n",
    "            # called 'const' in tf\n",
    "            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n",
    "            self.bias = nn.Parameter(torch.ones(nf))\n",
    "        else:\n",
    "            self.dense = MyLinear(dlatent_size, nf*16, gain=gain/4, use_wscale=use_wscale) # tweak gain to match the official implementation of Progressing GAN\n",
    "        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "        self.conv = MyConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n",
    "        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "        \n",
    "    def forward(self, dlatents_in_range):\n",
    "        batch_size = dlatents_in_range.size(0)\n",
    "        if self.const_input_layer:\n",
    "            x = self.const.expand(batch_size, -1, -1, -1)\n",
    "            x = x + self.bias.view(1, -1, 1, 1)\n",
    "        else:\n",
    "            x = self.dense(dlatents_in_range[:, 0]).view(batch_size, self.nf, 4, 4)\n",
    "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
    "        x = self.conv(x)\n",
    "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class GSynthesisBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        # 2**res x 2**res # res = 3..resolution_log2\n",
    "        super().__init__()\n",
    "        if blur_filter:\n",
    "            blur = BlurLayer(blur_filter)\n",
    "        else:\n",
    "            blur = None\n",
    "        self.conv0_up = MyConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n",
    "                                 intermediate=blur, upscale=True)\n",
    "        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "        self.conv1 = MyConv2d(out_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)\n",
    "        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "            \n",
    "    def forward(self, x, dlatents_in_range):\n",
    "        x = self.conv0_up(x)\n",
    "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
    "        x = self.conv1(x)\n",
    "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
    "        return x\n",
    "\n",
    "class G_synthesis(nn.Module):\n",
    "    def __init__(self,\n",
    "        dlatent_size        = 512,          # Disentangled latent (W) dimensionality.\n",
    "        num_channels        = 3,            # Number of output color channels.\n",
    "        resolution          = 1024,         # Output resolution.\n",
    "        fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
    "        fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
    "        fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
    "        use_styles          = True,         # Enable style inputs?\n",
    "        const_input_layer   = True,         # First layer is a learned constant?\n",
    "        use_noise           = True,         # Enable noise inputs?\n",
    "        randomize_noise     = True,         # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n",
    "        nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu'\n",
    "        use_wscale          = True,         # Enable equalized learning rate?\n",
    "        use_pixel_norm      = False,        # Enable pixelwise feature vector normalization?\n",
    "        use_instance_norm   = True,         # Enable instance normalization?\n",
    "        dtype               = torch.float32,  # Data type to use for activations and outputs.\n",
    "        blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        def nf(stage):\n",
    "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "        self.dlatent_size = dlatent_size\n",
    "        resolution_log2 = int(np.log2(resolution))\n",
    "        assert resolution == 2**resolution_log2 and resolution >= 4\n",
    "\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "        num_layers = resolution_log2 * 2 - 2\n",
    "        num_styles = num_layers if use_styles else 1\n",
    "        torgbs = []\n",
    "        blocks = []\n",
    "        for res in range(2, resolution_log2 + 1):\n",
    "            channels = nf(res-1)\n",
    "            name = '{s}x{s}'.format(s=2**res)\n",
    "            if res == 2:\n",
    "                blocks.append((name,\n",
    "                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n",
    "                                      use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
    "                \n",
    "            else:\n",
    "                blocks.append((name,\n",
    "                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
    "            last_channels = channels\n",
    "        self.torgb = MyConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale)\n",
    "        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n",
    "        \n",
    "    def forward(self, dlatents_in):\n",
    "        # Input: Disentangled latents (W) [minibatch, num_layers, dlatent_size].\n",
    "        # lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0), trainable=False), dtype)\n",
    "        batch_size = dlatents_in.size(0)       \n",
    "        for i, m in enumerate(self.blocks.values()):\n",
    "            if i == 0:\n",
    "                x = m(dlatents_in[:, 2*i:2*i+2])\n",
    "            else:\n",
    "                x = m(x, dlatents_in[:, 2*i:2*i+2])\n",
    "        rgb = self.torgb(x)\n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\n",
    "!wget https://edunet.kea.su/repo/EduNet-web_dependencies/L13/weights_stylegan.pt \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим модель и загрузим в нее обученные веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stylegan = nn.Sequential(OrderedDict([\n",
    "    ('g_mapping', G_mapping()),\n",
    "    ('g_synthesis', G_synthesis())    \n",
    "]))\n",
    "\n",
    "model_stylegan.load_state_dict(torch.load('/content/weights_stylegan.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model_stylegan.eval()\n",
    "model_stylegan.to(device)\n",
    "\n",
    "nb_rows = 2\n",
    "nb_cols = 5\n",
    "nb_samples = nb_rows * nb_cols\n",
    "latents = torch.randn(nb_samples, 512, device=device)\n",
    "with torch.no_grad():\n",
    "    imgs = model_stylegan(latents)\n",
    "    imgs = (imgs.clamp(-1, 1) + 1) / 2.0 # normalization to 0..1 range\n",
    "imgs = imgs.cpu()\n",
    "\n",
    "imgs = torchvision.utils.make_grid(imgs, nrow=nb_cols)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.imshow(imgs.permute(1, 2, 0).detach().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь посмотрим на плавные переходы одной картинки в другую с помощью интерполяции латентного пространства"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно посчитаем и сохраним картинки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf frames/*\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nb_latents = 3\n",
    "nb_interp = 50\n",
    "\n",
    "fixed_latents = [torch.randn(1, 512, device=device) for _ in range(nb_latents)]\n",
    "latents = []\n",
    "for i in range(len(fixed_latents) - 1):\n",
    "    a = fixed_latents[i]\n",
    "    b = fixed_latents[i + 1]\n",
    "    latents.append(a + (b - a) * torch.linspace(0, 1, nb_interp, device=device).unsqueeze(1))\n",
    "# latents.append(fixed_latents[-1])\n",
    "latents = torch.cat(latents, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n, latent in tqdm(enumerate(latents), total=len(latents)):\n",
    "        latent = latent.to(device)\n",
    "        img = model_stylegan(latent.unsqueeze(0))\n",
    "        img = img.clamp_(-1, 1).add_(1).div_(2.0)        \n",
    "        img = img.detach().squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        plt.savefig('/content/frames/frame_%03d.png' % n)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сделаем из них гифку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "imgs = []\n",
    "path = '/content/frames/'\n",
    "for filename in np.sort(os.listdir(path)):\n",
    "    imgs.append(imageio.imread(path + filename))\n",
    "imageio.mimsave('StyleGAN.gif', imgs, fps=10)\n",
    "\n",
    "Image(open('StyleGAN.gif', 'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StyleGAN 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Естественно, команда авторов на этом не остановилась и спустя год предложила **StyleGAN2** - генеративную состязательную сеть, построенную на базе **StyleGAN** с рядом усовершенствований:\n",
    "- адаптивная нормализация (**AdaIN**) была переработана и заменена на технику нормализации, называемую демодуляцией весов (**weight demodulation**). Теперь вектор стиля модифицирует **не значения признаков**, а **веса свертки**. **AdlIN** приводила к характерным **артефактам** в виде пятен на изображении (можно увидеть на правой щеке одного из изображений, в гифке выше)\n",
    "\n",
    "- введена улучшенная **схема обучения** по прогрессивно растущей схеме, которая достигает той же цели - обучение начинается с фокусировки на изображениях **низкого разрешения**, а затем постепенно смещает фокус на все более **высокие разрешения** - **без изменения топологии** сети во время обучения. \n",
    "- предлагаются новые типы регуляризации, такие как ленивая регуляризация (**lazy regularization**, регуляризация [R1](https://paperswithcode.com/method/r1-regularization) для экономии вычислительных ресурсов применяется не всегда, а на каждом 16-ом батче) и регуляризация по длине пути (**path length regularization**, сеть штрафуют, если небольшое изменение вектора латентного пространство слишком сильно или слишком слабо меняет итоговое изображение)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/style_gan_detailed_scheme.png\" width=\"800\">\n",
    "<center><em>Сравнение архитектуры StyleGAN и StyleGAN2 (<a href=\"https://arxiv.org/pdf/1912.04958.pdf\"> Karras et al., 2019</a>)</p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть **А** - это та же архитектура **StyleGAN**.\n",
    "\n",
    "В части **В** показан детальный вид архитектуры **StyleGAN**. \n",
    "\n",
    "В части **C** они заменили **AdaIN** на **Modulation** (или масштабирование признаков) и **Normalization**. Кроме того, в части **C** они перенесли добавление шума и смещения за пределы блока. \n",
    "\n",
    "Наконец, в части **D** вы можете увидеть, что **веса** корректируются с помощью стиля, а **нормализация** заменена операцией \"демодуляции\" (**demodulation**), комбинированные операции называются \"Демодуляция весов\" (**weight demodulation**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch\n",
    "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/stylegan2-ada-pytorch\n",
    "!python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n",
    "    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ffhq.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)['G_ema'].to(device)  # torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn([1, G.z_dim]).to(device)    # latent codes\n",
    "c = None                                # class labels (not used in this example)\n",
    "img = G(z, c)                           # NCHW, float32, dynamic range [-1, +1]\n",
    "\n",
    "img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8).cpu()[0]\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf frames/*\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "nb_latents = 3\n",
    "nb_interp = 50\n",
    "\n",
    "fixed_latents = [torch.randn([1, G.z_dim], device=device) for _ in range(nb_latents)]\n",
    "latents = []\n",
    "for i in range(len(fixed_latents) - 1):\n",
    "    a = fixed_latents[i]\n",
    "    b = fixed_latents[i + 1]\n",
    "    latents.append(a + (b - a) * torch.linspace(0, 1, nb_interp, device=device).unsqueeze(1))\n",
    "\n",
    "latents = torch.cat(latents, dim=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n, latent in tqdm(enumerate(latents), total=len(latents)):\n",
    "        latent = latent.to(device)\n",
    "        img = G(latent.unsqueeze(0), c)\n",
    "        img = img.clamp_(-1, 1).add_(1).div_(2.0)        \n",
    "        img = img.detach().squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        plt.savefig('/content/stylegan2-ada-pytorch/frames/frame_%03d.png' % n)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "path = '/content/stylegan2-ada-pytorch/frames/'\n",
    "for filename in np.sort(os.listdir(path)):\n",
    "    imgs.append(imageio.imread(path + filename))\n",
    "imageio.mimsave('StyleGAN2.gif', imgs, fps=10)\n",
    "\n",
    "Image(open('StyleGAN2.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alias-Free GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Alias-Free GAN](https://nvlabs.github.io/stylegan3/) так же называют StyleGAN3.\n",
    "\n",
    "В 2021 году авторы заметили, что, несмотря на **иерархическую природу** генерации (как мы видели у **StyleGAN** ранние слои отвечают за форму и положение головы, средние за черты лица и т.д.) генерации изображений типичными **GAN**, генерация зависит от **абсолютных координат** пикселей. Это проявляется в том, что детали оказываются приклеенными к координатам изображения, а не к поверхностям изображаемых объектов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/compare_style_and_alias_free_gans.gif\" width=\"800\">\n",
    "<center><em>Сравнение сгенерированных лиц с помощью StyleGAN2 и Anti-Alias GAN (Karras et al., 2021). Обратите внимание на то, как детали бороды (StyleGAN2) как бы \"приклеиваются\" к пространству.</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это связанно с тем, что при генерации картинок используются **нелинейные** (а они должны быть нелинейными, иначе нейросеть не научиться так хорошо [аппроксимировать](https://towardsai.net/p/deep-learning/understanding-the-universal-approximation-theorem)) **функции активации** (а в случае **ReLU** еще и не гладкие). Это приводит к появлению на карте признаков (и на финальном изображении) высокачастотного шума. Идея **Alias-Free GAN** заключается в добавлении **фильтров нижних частот** **ФНЧ**, которые фильтруют такой шум. **ФНЧ** - это по сути свертка с фиксированными весами, которая убирает частые изменения признаков (рябь на картинке).\n",
    "\n",
    "**Alias-Free GAN** соответствуют по метрике [расстояния Фреше](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%A4%D1%80%D0%B5%D1%88%D0%B5) (Fr´echet Inception distance или FID) **StyleGAN2**, но изображения, созданные с ее помощью содержат меньше заметных глазу артефактов. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Отличное видео summary Alias-Free GAN с привязкой к StyleGan2](https://www.youtube.com/watch?v=0zaGYLPj4Kk&ab_channel=TwoMinutePapers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тонкости обучения GANов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Статья - детальный разбор тонкностей и советов](https://beckham.nz/2021/06/28/training-gans.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частые/простые ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Убедитесь, что сгенерированые сэмплы в том же диапазоне, что и реальные данные.** Например, реальные данные `[-1,1]`, при этом генерируются данные `[0,1]`. Это не хорошо, так как это подсказка для дискриминатора. \n",
    "* **Убедитесь, что сгенерированные сэмплы того же размера, что и реальные данные.** Например, размер картинок в MNIST `(28,28)`, а генератор выдает `(32,32)`. В таком случае нужно либо изменить архитектуру генератора, чтобы получать на выходе размер `(28,28)`, либо сделать ресайз реальных данных до `(32,32)`.\n",
    "* **Старайтесь не использовать `BatchNorm`**. Проблема `BN` в том, что во время обучения его внутренняя статистика считается по минибатчу, а во время инференса она вычисляется как *moving average*, что в свою очередь может повлечь непредсказуемые результаты. Если архитектура GAN предполагает нормализацию - то лучше использовать **`InstanceNorm`**.\n",
    "* **Визуализируйте свои лоссы в процессе обучения**. Для этого существует множество прекрасных библиотек (например `tensorboard`). Следить за бегущими по экрану цифрами от двух соревнующихся между собой лоссов - бессмысленно.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем давать преимущество дискриминатору"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ваша версия **GAN** работает не так хорошо как вам хотелось бы, попробуйте дать своему **дискриминатору** преимущество, обучив его на относительно большее число итераций, чем **генератор**. Другими словами, чем лучше **дискриминатор** различает настоящие и фальшивые данные, тем лучше сигнал, который **генератор** может извлечь из него. Обратите внимание, что, эта логика не имела смысла во времена \"до WGAN\", поскольку слишком хорошая работа дискриминатора вредила обучению.\n",
    "\n",
    "Например:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def train_on_batch(x, iter_, n_gen=5):\n",
    "     Generator:\n",
    "    ...\n",
    "    ...\n",
    "    if iter_ % n_gen == 0:\n",
    "        g_loss.backward()\n",
    "        opt_g.step()\n",
    "        \n",
    "     Discriminator:\n",
    "    ...\n",
    "    ...\n",
    "    d_loss.backward() \n",
    "    d_loss.step()\n",
    "```\n",
    "\n",
    "Где `iter_` - текущая итерация шага градиента, а `n_gen` определяет интервал между обновлениями генератора. В данном случае, поскольку он равен 5, мы можем считать, что это означает, что дискриминатор обновляется в 5 раз чаще, чем генератор.\n",
    "\n",
    "Естественно работает не всегда и не везде. Но попробовать стоит\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование оптимизатора ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно обратить внимание, что почти во всех статьях по **GAN** используется **ADAM**. Сложно сказать, почему так получается, но он работает и работает очень хорошо. Если качество вашего **GAN** оставляет желать лучшего - скорее всего оптимизатор тут не причем. Ищите ошибку где-то еще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `epsilon` **ADAM** по умолчанию в PyTorch равен `1e-8`, что может вызвать проблемы после длительного периода обучения, например, лоссы потери периодически взрываются или увеличиваются. Подробнее об этом на [StackOverflow](https://stackoverflow.com/questions/42327543/adam-optimizer-goes-haywire-after-200k-batches-training-loss-grows) и в комментарии на [Reddit](https://www.reddit.com/r/reinforcementlearning/comments/j9rflf/intuitive_explanation_for_adams_epsilon_parameter/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top K Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье [Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples (Sinha et al., 2020)](https://arxiv.org/abs/2002.06224) утверждается, что если просто **обнулить вклад градиента от сэмплов, которые дискриминатор считает поддельными**, генератор обучается значительно лучше, достигая нового **SOTA** (реализуется в одну строчку)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Краткое описание примечательных моделей GAN\n",
    "\n",
    "**GAN** моделей настолько много, что нет смысла рассказывать о всех. Так или иначе, сейчас многие новые нейросети используют принципы **GAN** для обучения. Будь-то распознавание или сегментация.\n",
    "\n",
    "[Самые современные генеративные модели](https://paperswithcode.com/methods/category/generative-models)\n",
    "\n",
    "[exactly how the NVIDIA GauGAN neural network works](https://sudonull.com/post/29972-Pictures-from-rough-sketches-how-exactly-the-NVIDIA-GauGAN-neural-network-works-ITSumma-Blog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN для решения задачи распознавания капчи\n",
    "\n",
    "[Yet Another Text Captcha Solver, (Ye et all, 2018)](https://zwang4.github.io/publication/ccs18/)\n",
    "\n",
    "\n",
    "В чем идея: для обучения **классифицирующей** нейронной сети требуется **много данных**. Ботов, собирающих изображения капч, легко распознать и заблокировать.\n",
    "\n",
    "Решение: Почему бы нам не **сгенерировать** с помощью **GAN**, примеров для обучения классификатора?\n",
    "\n",
    "Так и сделали исследователи из Великобритании и Китая, собрали всего 500 образцов от 11 сервисов капчи, используемых на 32 сайтах из топ-50 в рейтинге Alexa (рейтинг самых посещаемых сайтов от Amazon в настоящее время не поддерживается). На сбор разработчики потратили всего 2 часа. В процессе же обучения было «синтезировано» более 200 000 тысяч капч.\n",
    "\n",
    "Удалось обойти текстовые **CAPTCHA** со 100% точностью на сайтах Megaupload, Blizzard и Authorize.NET. Кроме того, метод превзошел другие подходы при взломе капчи на Amazon, Digg, Slashdot, PayPal, Yahoo, QQ и других сайтах.\n",
    "\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/generate_captchas.png\" width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Pix\n",
    "\n",
    "[Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2016)](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "**Pix2Pix** - сети, переводящая пиксельные рисунки в реалистичные изображения. Она тоже использует принципы **GAN** для работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/pix2pix_scheme.png\" width=\"500\"></center>\n",
    "<center><em>Схема работы Pix2Pix (Isola et al., 2016).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробовать сеть можно на [сайте 1](https://affinelayer.com/pixsrv/) и на [сайте 2](https://affinelayer.com/pix2pix/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё примеры:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/pix2pix_results_examples.png\" width=\"900\"></center>\n",
    "<center><em>Еще примеры работы Pix2Pix (Isola et al., 2016).</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семантическая генерация\n",
    "\n",
    "Помимо **шума**, в модель можно подавать **описание** того, что мы хотим получить. Это описание может быть разным, например **текст** или **вектор заданных свойств**.\n",
    "\n",
    "Например:\n",
    "* Класс объекта\n",
    "* Углы и повороты\n",
    "* Заданные параметры трансформаций\n",
    "* Сегментация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learning to Generate Chairs, Tables and Cars\n",
    "with Convolutional Networks (Dosovitskiy et al., 2017)](https://arxiv.org/abs/1411.5928)\n",
    "\n",
    "Эта статья - эксперимент на тему того, можно ли использовать **GAN** для дизайна 3D объектов, таких как стулья, столы и автомобили. При этом авторы задают параметры объекта - его класс, положение в пространстве (c какой точки мы смотрим на объект), цвет, яркость и т.д. Для сгенерированных объектов также решалась задача сегментации - это дает возможность вставить объект на произвольный фон. \n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/3d_gan_scheme.png\" width=\"700\"></center>\n",
    "<center><em>Схема работы GAN, который учитывает много параметров, например угол обзора и класс (Dosovitskiy et al., 2017).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная статья показала, что GAN может не только придумывать новые картинки, которые отсутствовали в train, но и понимать геометрию объекта и работать с пространственным положением. \n",
    "\n",
    "Из интересных результатов, данная нейронная сеть позволяет “складывать” и “вычитать” стулья. Для этого используется сложение и вычитание признаков на выходе FC-2 (см. картинку выше).\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/semantic_chairs.png\" width=\"350\"></center>\n",
    "<center><em>Интерпретируемые результаты генерации при проведении арифметических операций между признаками (Dosovitskiy et al., 2017).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/3d_gan_results.png\" width=\"300\"></center>\n",
    "<center><em>Результат генерации и интерполяции между машинками (Dosovitskiy et al., 2017).</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to image\n",
    "\n",
    "Существует задача генерации изображения по текстовому описанию. Такую задачу можно решать используя в качестве части источника случайности текстовое описание. \n",
    "\n",
    "Одна из возможных архитектур сетей text-to-image с использованием RNN-сети на входе:\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/tex_to_image_gan_scheme.png\" width=\"800\"></center>\n",
    "<center><em>Схема работы Generative Adversarial Text to Image Synthesis (Reed et al., 2016) </em></center>\n",
    "\n",
    "[Generative Adversarial Text to Image Synthesis (Reed et al., 2016)](https://arxiv.org/pdf/1605.05396.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/control_gan_results.png\" width=\"500\">\n",
    "\n",
    "<em>Результаты работы Adversarial Text to Image Synthesis (Reed et al., 2016).</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ControlGAN\n",
    "\n",
    "[Controllable Generative Adversarial Network (Lee et al., 2017)](https://arxiv.org/abs/1708.00598)\n",
    "\n",
    "[Код ControlGAN](https://github.com/mrlibw/ControlGAN) - Pytorch реализация  для управляемого преобразования текста в изображение. На входе используется эмбеддинг слов, который подаётся во множество генераторов и дискриминаторов.\n",
    "\n",
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/control_gan_scheme.jpg\" width=\"800\"></center>\n",
    "<center><em>Схема работы ControlGAN (https://github.com/mrlibw/ControlGAN).</em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментировать:\n",
    "\n",
    "[Colab, где можно вводить собственный текст](https://colab.research.google.com/drive/1_3sCa9QvUI0OqWqx2rQvsSh3C9xfPssZ?usp=sharing)\n",
    "\n",
    "[Open AI DALL-E интерактив](https://openai.com/blog/dall-e/)\n",
    "\n",
    "[OpenSource версия DALL-E](https://colab.research.google.com/drive/1b8va5g852hq3p7yro7xWY3Cc-bd2CRdv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более современная архитектура с использованием механизма внимания:\n",
    "\n",
    "[Text-to-Image Generation with Attention Based Recurrent Neural Networks (Zia et al., 2020)](https://arxiv.org/abs/2001.06658)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQGAN + CLIP\n",
    "\n",
    "[ Пример тетради для работы с VQGAN + CLIP](https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "<img src = \"https://edunet.kea.su/repo/EduNet-content/L13/out/vqgan_clip_result.png\" width='300'>\n",
    "\n",
    "</center>\n",
    "\n",
    "<center><em>Изображение сгенерировано нейросетью (запрос: self-conciousness, artstation)</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача переноса стиля\n",
    "    \n",
    "[PapersWithCode Image Stylization](https://paperswithcode.com/task/image-stylization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "\n",
    "Перенос стиля (style transfer)  — одно из наиболее креативных приложений сверточных нейронных сетей. Взяв контент с одного изображения и стиль от второго, нейронная сеть объединяет их в одно художественное произведение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/style_transfer_result.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/style_transfer_input_reference_result.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для целей переноса стиля [можно использовать GAN](https://towardsdatascience.com/style-transfer-with-gans-on-hd-images-88e8efcf3716) (но так делают редко):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/L13/out/gan_for_style_transfer.png\" width=\"600\"></center>\n",
    "<center><em>Сеть состоит из одного генератора (G) и сиамского дискриминатора (D): G принимает изображение-контент (А) на входе и выводит некое итоговое изображение G(A). Дискриминатор  принимает эти изображения и изображение-стиль (B) в качестве входных данных и выводит скрытый вектор. Siamese Discriminator преследует 2 цели: дать информацию генератору, как создавать более реалистичные изображения, и поддерживать в этих поддельных изображениях корреляцию (то есть «контент») с исходными.</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Flow\n",
    "\n",
    "[Style Flow](https://arxiv.org/pdf/2008.02401.pdf)\n",
    "\n",
    "\n",
    "Управление процессом генерации с использованием (семантических) атрибутов, сохраняя при этом качество выходных данных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image-to-Image Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы не будем вдаваться в подробности как работают следующие модели, но мы предлагаем вам ссылки на Colab`ы где с ними можно самостоятельно поиграться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GANs N' Roses**\n",
    "\n",
    "[GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (Chong et al., 2021)](https://arxiv.org/abs/2106.06561)\n",
    "\n",
    "[Colab](https://colab.research.google.com/github/mchong6/GANsNRoses/blob/main/inference_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/image_to_image_result.gif\"></center>\n",
    "<center><em>Результаты работы GANs N' Roses (Chong et al., 2021)</em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CycleGAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (Zhu et al., 2017)](https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "[Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L13/cycle_gan_horse_to_zebra.gif\" width=\"800\"/></center>\n",
    "<center><em>Результаты работы CycleGAN (Zhu et al., 2017)</em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"6\"> Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принципы генеративно-состязательных сетей вышли далеко за пределы генерации из шума. Сейчас с помощью GAN создаются сложнейшие state-of-art сети для самых разнообразных задач. В лекции были рассмотрены самые главные модели: GAN - для генерации из шума, DCGAN - для генерации изображений, с помощью развёрток и cGAN - сети с генерацией по условию. \n",
    "\n",
    "Совмещая различные сети и подходы можно сконструировать множество самых разнообразных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"> Использованная литература\n",
    "\n",
    "<font size=\"5\"> GAN\n",
    "\n",
    "[Книга по генеративным сетям](https://habr.com/ru/company/piter/blog/504956/)\n",
    "\n",
    "[Generative Adversarial Networks (Goodfellow et al., 2014)](https://arxiv.org/abs/1406.2661)\n",
    "\n",
    "[Видео разбор оригинальной статьи GAN](https://youtu.be/eyxmSmjmNS0)\n",
    "\n",
    "[Видео лекции Иана Гудфеллоу](https://www.youtube.com/watch?v=HGYYEUSm-0Q)\n",
    "\n",
    "[Generative adversarial networks](https://deepgenerativemodels.github.io/notes/gan/)\n",
    "\n",
    "[Самые современные генеративные модели](https://paperswithcode.com/methods/category/generative-models)\n",
    "\n",
    "[exactly how the NVIDIA GauGAN neural network works](https://sudonull.com/post/29972-Pictures-from-rough-sketches-how-exactly-the-NVIDIA-GauGAN-neural-network-works-ITSumma-Blog)\n",
    "\n",
    "<font size=\"5\"> DCGAN\n",
    "\n",
    "[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford et al., 2015)](https://arxiv.org/abs/1511.06434).\n",
    "\n",
    "[DCGAN TUTORIAL](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)\n",
    "\n",
    "<font size=\"5\"> Wassserstein GAN\n",
    "\n",
    "[Wasserstein GAN (Arjovsky et. al., 2017)](https://arxiv.org/abs/1701.07875)\n",
    "\n",
    "[Блог пост о Wasserstein GAN](https://vincentherrmann.github.io/blog/wasserstein/)\n",
    "\n",
    "[Improved Training of Wasserstein GANs (Gulrajani et al., 2017)](https://arxiv.org/abs/1704.00028)\n",
    "\n",
    "[Spectral Normalization for Generative Adversarial Networks (Miyato et al., 2018)](https://arxiv.org/abs/1802.05957).\n",
    "\n",
    "<font size=\"5\"> ProGAN -> StyleGAN -> StyleGAN2 -> Alias-Free GAN\n",
    "\n",
    "[Progressive Growing of GANs for Improved Quality, Stability, and Variation (ProGAN) [Karras et al., 2017]](https://arxiv.org/abs/1710.10196)\n",
    "\n",
    "[A Style-Based Generator Architecture for Generative Adversarial Networks (StyleGAN) [Karras et al., 2018]](https://arxiv.org/abs/1812.04948)\n",
    "\n",
    "[Analyzing and Improving the Image Quality of StyleGAN (StyleGAN2) [Karras et al., 2019]](https://arxiv.org/abs/1912.04958)\n",
    "\n",
    "[Alias-Free Generative Adversarial Networks (Alias-Free GAN) [Karras et al., 2021]](https://arxiv.org/abs/2106.12423)\n",
    "\n",
    "<font size=\"5\"> Тонкости обучения GAN\n",
    "\n",
    "[Статья - детальный разбор тонкностей и советов](https://beckham.nz/2021/06/28/training-gans.html)\n",
    "\n",
    "[Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples (Sinha et al., 2020)](https://arxiv.org/abs/2002.06224)\n",
    "\n",
    "<font size=\"5\"> GAN Zoo:\n",
    "\n",
    "<font size=\"5\"> BigGAN\n",
    "\n",
    "[Large Scale GAN Training for High Fidelity Natural Image Synthesis (Brock et al., 2018)](https://arxiv.org/abs/1809.11096)\n",
    "\n",
    "<font size=\"5\"> StackGAN\n",
    "\n",
    "[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al., 2016)](https://arxiv.org/abs/1612.03242)\n",
    "\n",
    "[StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks (Zhang et al., 2017)](https://arxiv.org/abs/1710.10916)\n",
    "\n",
    "[Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (Ledig et al., 2016)](https://arxiv.org/abs/1609.04802)\n",
    "\n",
    "[Let’s Read Science! “StackGAN: Text to Photo-Realistic Image Synthesis”](https://medium.com/@rangerscience/lets-read-science-stackgan-text-to-photo-realistic-image-synthesis-4562b2b14059)\n",
    "\n",
    "[Deep Learning Generative Models for Image Synthesis and Image Translation](https://www.rulit.me/data/programs/resources/pdf/Generative-Adversarial-Networks-with-Python_RuLit_Me_610886.pdf)\n",
    "\n",
    "[youtube [StackGAN++] Realistic Image Synthesis with Stacked Generative Adversarial Networks | AISC](https://www.youtube.com/watch?v=PXWIaLE7_NU)\n",
    "\n",
    "[youtube Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://www.youtube.com/watch?v=crI5K4RCZws)\n",
    "\n",
    "<font size=\"5\"> ControlGAN\n",
    "\n",
    "[Controllable Generative Adversarial Network](https://arxiv.org/pdf/1708.00598.pdf)\n",
    "\n",
    "[Controllable Text-to-Image Generation](https://arxiv.org/pdf/1909.07083.pdf)\n",
    "\n",
    "[Image Generation and Recognition (Emotions)](https://arxiv.org/pdf/1910.05774.pdf)\n",
    "\n",
    "[Natural Language & Text-to-Image 2019](https://meta-guide.com/data/data-processing/text-to-image-systems/natural-language-text-to-image-2019)\n",
    "\n",
    "<font size=\"5\"> AC-GAN\n",
    "\n",
    "[How to Develop an Auxiliary Classifier GAN (AC-GAN) From Scratch with Keras](https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/)\n",
    "\n",
    "[Understanding ACGANs with code[PyTorch]](https://towardsdatascience.com/understanding-acgans-with-code-pytorch-2de35e05d3e4)\n",
    "\n",
    "[An Auxiliary Classifier Generative Adversarial Framework for Relation Extraction](https://arxiv.org/pdf/1909.05370.pdf)\n",
    "\n",
    "[A Multi-Class Hinge Loss for Conditional GANs](https://openaccess.thecvf.com/content/WACV2021/papers/Kavalerov_A_Multi-Class_Hinge_Loss_for_Conditional_GANs_WACV_2021_paper.pdf)\n",
    "\n",
    "<font size=\"5\"> Domain Transfer Network\n",
    "\n",
    "[Unsupervised Cross-Domain Image Generation (Taigma et al., 2016)](https://arxiv.org/abs/1611.02200)\n",
    "\n",
    "<font size=\"5\"> Pix2Pix\n",
    "\n",
    "[Image-to-Image Translation with Conditional Adversarial Networks (Isola et al., 2016)](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "<font size=\"5\"> Семантическая генерация\n",
    "\n",
    "[Learning to Generate Chairs, Tables and Cars\n",
    "with Convolutional Networks (Dosovitskiy et al., 2017)](https://arxiv.org/abs/1411.5928)\n",
    "\n",
    "<font size=\"5\"> Text to image\n",
    "\n",
    "[Text-to-Image Generation with Attention Based Recurrent Neural Networks (Zia et al., 2020)](https://arxiv.org/abs/2001.06658)\n",
    "\n",
    "<font size=\"5\">Image-to-Image\n",
    "\n",
    "[GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (Chong et al., 2021)](https://arxiv.org/abs/2106.06561)\n",
    "\n",
    "[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (Zhu et al., 2017)](https://arxiv.org/abs/1703.10593)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Ссылки\n",
    "\n",
    "[GitHub MNIST CelebA cGAN cDCGAN](https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN)\n",
    "\n",
    "[GitHub Text-to-Photo realistic Image Synthesis with Stacked Generative Adversarial Networks](https://github.com/zeusm9/Text-to-Photo-realistic-Image-Synthesis-with-Stacked-Generative-Adversarial-Networks)\n",
    "\n",
    "[GitHub ControlGAN](https://github.com/mrlibw/ControlGAN)\n",
    "\n",
    "[GitHub ControlGAN-Tensorflow](https://github.com/taki0112/ControlGAN-Tensorflow)\n",
    "\n",
    "[GitHub Keras-ACGan](https://github.com/lukedeo/keras-acgan)\n",
    "\n",
    "[Множество примеров различных генераторов](https://thisxdoesnotexist.com)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
