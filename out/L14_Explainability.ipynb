{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">Explainability</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Причины использования Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели машинного обучения представляют собой черный ящик."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/input_blackbox_output.png\" alt=\"alttext\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда это  становится препятствием для внедрения моделей.\n",
    "\n",
    "Есть как минимум три причины, по которым нас может интересовать объяснение предсказаний модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Обнаружение некорректных зависимостей </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель может использовать совсем не те признаки, которые соответствуют реальным объектам.\n",
    "\n",
    "Например, ориентироваться на фон или водяной знак, а не на реальные свойства объекта.\n",
    "\n",
    "Пример из статьи [\"Why Should I Trust You?\"](https://arxiv.org/abs/1602.04938)\n",
    "Авторы обучили классификатор волков и эскимосских собак (хаски). Исследователи на изображениях, отобранных так, чтобы на всех фотографиях волков на фоне был снег, а на фотографиях хаски — нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/2_bad_models_prediction.png\" alt=\"alttext\" width=\"400\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1602.04938.pdf\">Explaining the Predictions of Any Classifier</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Доверие к предсказаниям </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/nuclear.jpg\" alt=\"alttext\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нельзя остановить ядерную электростанцию или назначить пациенту опасное лечение на основании предсказания «черного ящика».\n",
    "Даже маловероятная ошибка в таких случаях будет иметь тяжелые последствия. Поэтому человек, принимающий решение, должен понимать, на основе каких признаков или симптомов сделано предсказание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Публикации в научных журналах </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вероятность публикации статьи значительно повышается, если автор смог объяснить происхождение результатов своего исследования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/scientific_publication.png\" alt=\"alttext\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Explainability & Interpretability </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В англоязычной литературе можно встретить два термина, связанные с темой доверия: Explainability и Interpretability\n",
    "\n",
    "**Explainability** — методики, позволяющие объяснить механизм функционирования модели.\n",
    "\n",
    "Например, для линейной регрессии это анализ коэффициентов при параметрах.\n",
    "\n",
    "\n",
    "**Interpretability** — анализ того, как изменение входов модели влияло на ее выходы.\n",
    "\n",
    "Например, закрашивая часть пикселей изображения, можно выяснить, какие из них повлияют на предсказания (пример с хаски и волками).\n",
    "\n",
    "\n",
    "В данном блокноте будем рассматривать оба типа методов.\n",
    "\n",
    "[Machine Learning Explainability vs Interpretability: Two concepts that could help restore trust in AI](https://www.kdnuggets.com/2018/12/machine-learning-explainability-interpretability-ai.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/data_pipeline.png\" alt=\"alttext\" width=\"850\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка важности признаков для простых моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка важности признака для линейных моделей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае с **линейными моделями** нам было сравнительно легко определить, какие признаки модель считает важными. Если модель присваивает какому-то признаку **большой вес**, то этот признак сильнее влияет на **предсказание**.\n",
    "\n",
    "\n",
    "**ПРИЗНАКИ ДОЛЖНЫ БЫТЬ СРАВНИМЫ**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Пример для табличных данных (Boston Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера скачаем **датасет жилья Бостона** (boston_dataset), в котором проанализируем зависимость цены на жилье от параметров жилья и района, в котором оно находится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "boston_dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/boston_dataset.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "x_data = boston_dataset.iloc[:, :-1]\n",
    "y_data = boston_dataset[\"target\"].values\n",
    "\n",
    "boston_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем **коэффициенты** признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"name\": x_data.columns, \"coef\": model.coef_})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.barplot(data=df, y=\"name\", x=\"coef\", color=sns.xkcd_rgb[\"azure\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_data, model.predict(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на статистику признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что самый “важный” признак `NOX` имеет стандартное отклонение 0.115878, а, например, `LSTAT` имеет стандартное отклонение на порядок больше. Это значит, что веса перед признаком будут не только показывать важность, но и масштабировать признаки. Чтобы избежать этого, нужно сделать нормализацию.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на **коэффициенты** после **нормализации** данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_data = ss.fit_transform(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_importance = pd.DataFrame(\n",
    "    {\"name\": boston_dataset.columns[:-1], \"coef\": model.coef_}\n",
    ")\n",
    "linear_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "sns.barplot(\n",
    "    data=linear_importance,\n",
    "    y=\"name\",\n",
    "    x=\"coef\",\n",
    "    color=sns.xkcd_rgb[\"azure\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_data, model.predict(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим абсолютные значения для сравнения с другими оценками важности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_importance[\"abs(coef)\"] = linear_importance.coef.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, **качество модели** не изменилось, зато новая оценка позволяет сравнивать признаки по **важности**, а не по диапазону значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка важности признака для дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае с **деревьями** всё далеко не так очевидно: дерево не знает такой концепции, как **\"вес признака\"**.\n",
    "\n",
    "Универсального критерия значимости для деревьев у нас нет, и, в зависимости от задачи и от того, как эти признаки устроены, ответы могут быть разными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С одним из способов оценки качества признаков для дерева решений мы познакомились, когда строили дерево решений на третьей лекции. Мы использовали метрику $\\text{Gini}$:\n",
    "\n",
    "$$\\large \\text{Gini} = 1 - \\sum_ip_i^2$$\n",
    "\n",
    "где $p_i$ — вероятность того, что объект, попавший в данный  лист, относится к $i$-ому классу. Чем меньше $\\text{Gini}$ в листьях, тем качественнее узел, от которого “растут” листья, разделяет классы.\n",
    "\n",
    "\n",
    "Для того, чтобы охарактеризовать “хорошесть” узла, мы строили метрику $\\text{impurity}$, в которой суммировали $\\text{Gini}$ листьев данного узла с весами, равными доле объектов, попавших в данный лист. После чего смотрели, на сколько $\\text{impurity}$ уменьшилось ($\\text{decrease}$) на данном узле (стало ближе к “идеальному” нулю).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle \\large \\text{Impurity decrease} = \\text{Gini}_0 - (\\frac{n_1}{n_1+n_2}\\text{Gini}_1 + \\frac{n_2}{n_1+n_2}\\text{Gini}_2),$$\n",
    "\n",
    "где $n_1, n_2$ — число объектов в листьях,\n",
    "\n",
    "$ \\quad\\  \\text{Gini}_0$ — чистота исходного узла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Именно $\\text{impurity decrease}$ используется для расчета атрибута `feature_importances_` в [sklearn](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html). Такая метрика встроена в любое дерево решений. Для случайного леса (и других ансамблей) просто выдается среднее по деревьям.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У данного метода есть **недостаток**: он завышает качество признаков с большим количеством возможных значений.\n",
    "\n",
    "Даже если признак не информативен, но у него много значений, на нем можно выбрать большое количество порогов, по которым можно разбить данные, что приведет к переобучению.\n",
    "\n",
    "Бинарные и маломерные категориальные признаки в любом случае будут получать заниженное качество по сравнению с вещественными, даже если те дают худшее разбиение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример для табличных данных (Boston Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на важность признаков, основанную на **impurity decrease**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "model = RandomForestRegressor(random_state=rng)\n",
    "model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_importance = pd.DataFrame(\n",
    "    {\"name\": boston_dataset.columns[:-1], \"feature importances\": model.feature_importances_}\n",
    ")\n",
    "gini_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Linear model\")\n",
    "sns.barplot(\n",
    "    data=linear_importance,\n",
    "    y=\"name\",\n",
    "    x=\"abs(coef)\",\n",
    "    color=sns.xkcd_rgb[\"azure\"],\n",
    "    orient=\"h\",\n",
    ")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Random forest\")\n",
    "sns.barplot(\n",
    "    data=gini_importance, y=\"name\", x=\"feature importances\", color=sns.xkcd_rgb[\"azure\"], orient=\"h\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что важность признаков для одних и тех же данных зависит от модели. При этом признаки RM — количество комнат в доме, и LSTAT — процент людей с низким социальным статусом (без среднего образования, безработных), важны для обеих моделей. Что достаточно логично."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = boston_dataset.iloc[:, :-1]\n",
    "for item in x_data.columns:\n",
    "    print(f\"{item:<10}: {x_data[item].nunique():<4} unique values \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересно отметить, что оба признака (RM и LSTAT), оказавшихся наиболее важными для Random Forest, имеют большое количество уникальных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы, изучающие отклик модели на изменение входных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы рассмотрим методы, изучающие отклик модели на изменение входных данных. Они также называются **Model-Agnostic Methods**. Эти методы находятся ближе всего к концепции “черного ящика”. Они изучают взаимосвязь входов и выходов модели и пытаются объяснить связь между ними."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/input_blackbox_output.png\" alt=\"alttext\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICE (Individual Conditional Expectation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажем, что это может работать на простой модели с использованием простого и интуитивно понятного метода:\n",
    "1. Обучаем и **фиксируем модель**.\n",
    "2. Выбираем **один объект**.\n",
    "3. Выбираем **один признак** этого объекта, который мы будем **менять в некотором диапазоне**, все остальные признаки фиксируем.\n",
    "4. Меняем этот признак и смотрим, как меняется **предсказание модели**.\n",
    "5. Строим кривую зависимости **целевого значения** от **изменяемого признака** для модели.\n",
    "6. Повторяем для другого объекта.\n",
    "\n",
    "Называется этот метод Individual Conditional Expectation (индивидуальное условное ожидание).\n",
    "\n",
    "Посмотрим, как это работает для Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример для табличных данных (Boston Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим и предобработаем  данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "boston_dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/boston_dataset.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "x_data = boston_dataset.iloc[:, :-1]\n",
    "y_data = boston_dataset[\"target\"].values\n",
    "\n",
    "boston_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "model = RandomForestRegressor(random_state=rng)\n",
    "model.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам интересно посмотреть на два признака, которые были наиболее важны для предсказания: **RM** — количество комнат, **LSTAT** — процент людей с низким социальным статусом, и на один признак, который для  **Random Forest** не важен: **B** — характеризует этнический состав района."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "_, ax = plt.subplots(ncols=3, figsize=(15, 5), sharey=True, constrained_layout=True)\n",
    "\n",
    "features_info = {\n",
    "    \"features\": [\"RM\", \"LSTAT\", \"B\"],\n",
    "    \"kind\": \"both\",\n",
    "    \"centered\": False,\n",
    "}\n",
    "\n",
    "common_params = {\n",
    "    \"subsample\": 50,\n",
    "    \"n_jobs\": 2,\n",
    "    \"grid_resolution\": 20,\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "x_data = pd.DataFrame(x_data, columns=boston_dataset.iloc[:, :-1].columns)\n",
    "\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    model,\n",
    "    x_data,\n",
    "    **features_info,\n",
    "    ax=ax,\n",
    "    **common_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синие линии — это отдельные объекты. По оси x — изменяемые признаки для этих объектов, по оси y — изменение целевого значения. Оранжевым цветом нарисовано среднее по объектам.\n",
    "\n",
    "Видно, что увеличение количества комнат в большинстве случаев положительно влияет на цену жилья, увеличение процента людей с низким социальным статусом — отрицательно, а этнический состав не важен.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME (Local Interpretable Model-agnostic Explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам бы хотелось оценивать все признаки одновременно для любой модели. Для этого можно попробовать **заменить \"черный ящик\"** (black-box) **\"стеклянным\"** (glass-box).\n",
    "\n",
    "Ключевая идея [__LIME__](https://arxiv.org/abs/1602.04938) — **локальная аппроксимация сложно-интерпретируемой** (black-box) модели при помощи **легко-интерпретируемой** (glass-box)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/lime_idea.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разбирать идею по кусочкам:\n",
    "1. **Локальная аппроксимация** — значит, что мы берем **один объект** и **модифицируем** его (изменяем признаки), чтобы получить небольшой датасет, локализованный вокруг исходного объекта.\n",
    "2. **Сложно интерпретируемая модель** — модель, для которой мы проводим оценку (наш \"черный ящик\"). Ее мы используем для того, чтобы получить **предсказания** для датасета, построенного на основе одного объекта.\n",
    "3. Таким образом, мы получаем **датасет**, включающий **признаки** и **предсказания** “черного ящика”. На этом датасете **учим** “стеклянный ящик” — **легко-интерпретируемую модель**. Например, линейную модель или дерево. Определять важность признаков для нее мы умеем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LIME** можно проиллюстрировать следующим изображением:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/lime_example.png\" width=\"600\"></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По **осям** отложены значения **двух непрерывных признаков**.\n",
    "\n",
    "**Жирный фиолетовый крест** — объясняемый объект. **Точки** и **кресты** — модификации исходного объекта, принадлежащие разным классам. **Метки классов** получены с помощью \"черного ящика\". **Граница между цветными зонами** — разделяющая поверхность, которую задает “черный ящик”. В качестве “стеклянного ящика” используется линейная модель, которую обучают на крестах и точках, она задает разделяющую **линию, выделенную серым**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим найти **glass-box** модель $g(x)$, которая будет локально аппроксимировать **black-box** модель $f(x)$ в окрестности объекта интереса $x^*$. $G$ — семейство интерпретируемых моделей (линейные модели, деревья, ..). Искомая аппроксимация будет выглядеть следующим образом:\n",
    "\n",
    "$$\\hat{g}=\\underset{g\\in G}{\\mathrm{argmin}}L(f,g,\\pi_x)+\\Omega (g),$$\n",
    "\n",
    "где $\\pi_x$ — **расстояние**, определяющееся по некоторой метрике от **сгенерированных объектов до объекта интереса**,\n",
    "\n",
    "**функция ошибки** $L()$ измеряет несоответствие между предсказаниями моделей $f()$ и $g()$,\n",
    "\n",
    "а $\\Omega (g)$ — **штраф за сложность модели**  $g()$. На практике зачастую, чтобы не заниматься оптимизацией этого штрафа, просто вводят некоторое ограничение на сложность моделей $g()$. Для деревьев это может быть **глубина дерева**, для линейных моделей — **число ненулевых коэффициентов**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто нам придется интерпретировать модели с **огромным числом признаков**. Чтобы упростить задачу, LIME может создавать **интерпретируемые представления признаков**.\n",
    "\n",
    "Модели $f()$ и $g()$  могут **оперировать разными пространствами признаков**, $f()$ — пространством размерности $p$ ($R^p$), соответствующей количеству признаков в исходных данных, $g()$ — пространством размерности $q$ ($R^q$), при этом $q<<p$. Пространство $R^q$ называется *интерпретируемым представлением* признаков. Пусть некая функция $h(x)$ переводит пространство признаков $R^p$ в $R^q$.\n",
    "\n",
    "Саму функцию, оценивающую расхождение между предсказаниями моделей $f()$ и $g()$, можно представить следующим образом:\n",
    "\n",
    "$$\\large L(f,g,\\pi_x)=\\sum_{z,z'}\\pi_x(z)(f(z)-g(z'))^2,$$\n",
    "\n",
    "где $z$ и $z'$ — наборы искусственно сгенерированных объектов в окрестности $x^*$ в пространствах $R^p$ и $R^q$ соответственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем собрать целиком алгоритм получения объяснения вклада переменных. Представим его в виде псевдокода:\n",
    "\n",
    "1. Дано: $x^*$ — **объект для интерпретации** вкладов признаков в модель, $N$ — размер искусственного датасета в окрестности целевого объекта, $similarity$ — **метрика расстояния**.\n",
    "2. $x' = h(x^*)$ переведем целевой объект в **пространство меньшей размерности**\n",
    "\n",
    "```\n",
    "z' = []\n",
    "for i in rnage(N):\n",
    "    z'[i] = sample_around(x')\n",
    "    y'[i] = f(z[i])\n",
    "    pi_x'[i] = similarity(x', z'[i])\n",
    "\n",
    "return K-Lasso(y', x', pi_x')\n",
    "```\n",
    "\n",
    "Выдачей алгоритма служит линейная модель, отобравшая К признаков на основе $y'$, $x'$, $\\pi_x'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как получить набор объектов вблизи искомого?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*  В **текстах** можно **удалить слово**;\n",
    "*  Для **изображений** можно **делить картинку на области** (суперпиксели) и поочередно закрашивать их одним и тем же цветом (средним).\n",
    "* Для **табличных данных**: для **бинарных** переменных в низкоразмерном пространстве достаточно просто менять значение переменной на противоположное (**0 на 1** и **1 на 0**). Для **вещественных переменных** были предложены разные варианты. Например, к ним можно прибавлять **Гауссов шум** или **дискретизировать** (например, по квантилям)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ограничения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описанный подход позволяет интерпретировать поведение модели **только в некоторой области** вблизи интересующего нас экземпляра.\n",
    "\n",
    "На практике этого может быть достаточно. **Работает быстро**, так как не требует перебора всех комбинаций признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример NLP (объяснения классификации статей по религиозному принципу)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном примере мы будем использовать две модели из классического NLP, с которыми раньше не работали. Скажем несколько слов о том, как они работают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "**TF-IDF** — способ численного представления документа, оценивает  **важность слова в контексте документа**. Состоит из двух множителей $TF$ и $IDF$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая идея, которая стоит за **TF-IDF** — это **если слово часто встречается в документе, оно важное**. За это отвечает $TF$.\n",
    "\n",
    "$TF$ (term frequency) — частота вхождения слова в документ, для которого рассчитывается значение:\n",
    "\n",
    "$$\\large TF(t, d) = \\frac{n_t}{\\sum_{k}n_k}$$\n",
    "\n",
    "$n_t$ — количество повторов слова $t$ в документе $d$,\n",
    "\n",
    "$\\sum_{k}n_k$ — общее количество слов  $t$ в документе $d$ с повторами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая идея **TF-IDF** — **если слово встречается во многих документах, его ценность снижается**.\n",
    "\n",
    "Пример: местоимения встречаются в большинстве текстов, но не несут смысловой нагрузки.\n",
    "\n",
    "За это отвечает $IDF$.\n",
    "\n",
    "$IDF$ (inverse document frequency) — логарифм обратной частоты вхождения слова в документы.\n",
    "$$\\large IDF(t, D) = \\log{\\frac{|D|}{|\\{d_i \\in D| t_i \\in d_i \\}|}}$$\n",
    "\n",
    "где $|D|$ — число документов в коллекции,\n",
    "$|\\{d_i \\in D| t_i \\in d_i \\}|$ — число документов в коллекции со словом $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая формула:\n",
    "$$\\large TF-IDF(t, d, D) = TF(t,d)⋅IDF(t, D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Пример работы `TfidfVectorizer` из [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "Каждому документу сопоставляется вектор, равный длине словаря. Ненулевые значения вектора хранятся в виде разреженных матриц [‘scipy.sparse.csr_matrix’](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Tf-idf dictionary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Tf-idf dictionary len:\", len(vectorizer.get_feature_names_out()))\n",
    "print(\"Tf-idf shape:\", x.shape)\n",
    "print(\"Tf-idf type:\", type(x))\n",
    "print(\"Tf-idf values:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (TF — term frequency, IDF — inverse document frequency):\n",
    "\n",
    "[[wiki] TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)\n",
    "\n",
    "[[code] sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "[[doc] TF-IDF Vectorizer scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/christian_or_atheist.png\" alt=\"alttext\" width=\"900\"/>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1602.04938.pdf\">Explaining the Predictions of Any Classifier</a><</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем датасет [fetch_20newsgroups](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html):\n",
    "\n",
    "Данные [«The 20 Newsgroups»](http://qwone.com/~jason/20Newsgroups/) — это коллекция примерно из **20000 новостных документов**, разделенная (приблизительно) равномерно между **20 различными категориями**. Изначально она собиралась Кеном Ленгом (Ken Lang), возможно, для его работы [«Newsweeder: Learning to filter netnews»](https://www.sciencedirect.com/science/article/abs/pii/B9781558603776500487) («Новостной обозреватель: учимся фильтровать новости из сети»).\n",
    "\n",
    "Коллекция «The 20 newsgroups» стала популярным набором данных для экспериментов с техниками машинного обучения для текстовых приложений, таких как классификация текста или его кластеризация.\n",
    "\n",
    "[[code] Fetching data, training a classifier](https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html)\n",
    "\n",
    "В данном примере мы будем использовать [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html?highlight=multinomial%20naive%20bayes#sklearn.naive_bayes.MultinomialNB) для классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\")\n",
    "# making class names shorter\n",
    "class_names = [\n",
    "    x.split(\".\")[-1] if \"misc\" not in x else \".\".join(x.split(\".\")[-2:])\n",
    "    for x in newsgroups_train.target_names\n",
    "]\n",
    "class_names[3] = \"pc.hardware\"\n",
    "class_names[4] = \"mac.hardware\"\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знаменитый набор данных из 20 групп новостей является эталоном, он использовался для сравнения различных моделей в нескольких статьях.\n",
    "\n",
    "Мы берем **два класса**, которые трудно различить, потому что в них много схожих слов: **христианство и атеизм**.\n",
    "\n",
    "Обучая модель, мы получаем **точность тестового набора 83,5%**, что является удивительно высоким показателем. Если бы точность была нашим единственным мерилом доверия, мы бы точно доверились этому классификатору.\n",
    "\n",
    "Однако давайте посмотрим на объяснение на рисунке для произвольного экземпляра в тестовом наборе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Again, let's use the tfidf vectorizer, commonly used for text.\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False)\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Train the model\n",
    "model_nb = MultinomialNB(alpha=0.01)\n",
    "model_nb.fit(train_vectors, newsgroups_train.target)\n",
    "\n",
    "# Calculate F1_score\n",
    "pred = model_nb.predict(test_vectors)\n",
    "sklearn.metrics.f1_score(newsgroups_test.target, pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_vectors.shape)\n",
    "print(type(train_vectors[0]), train_vectors[0].shape)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из кода, текст подается на вход модели не в сыром виде, а после предобработки объектом vectorizer.\n",
    "\n",
    "[LimeTextExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#lime.lime_text.LimeTextExplainer)  ждет на вход данные и класс модели:\n",
    "\n",
    "\n",
    "```\n",
    "explain_instance(\n",
    "    text_instance,\n",
    "    classifier_fn,\n",
    "    labels=(1, ),\n",
    "    top_labels=None,\n",
    "    num_features=10,\n",
    "    num_samples=5000,\n",
    "    distance_metric='cosine',\n",
    "    model_regressor=None)\n",
    "```\n",
    "\n",
    "**classifier_fn** *— classifier prediction probability function, which takes a list of d strings and outputs a (d, k) numpy array with prediction probabilities, where k is the number of classes. For ScikitClassifiers , this is classifier.predict_proba.*\n",
    "\n",
    "\n",
    "\n",
    " Поэтому в примере используется обертка над классом, преобразовывающим данные, и моделью:\n",
    "\n",
    " [sklearn.pipeline.make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html\n",
    "model_with_preprocessing = make_pipeline(vectorizer, model_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что этот классификатор имеет очень высокий F1_score. [Руководство sklearn для 20 newsgroups](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) указывает, что **Multinomial Naive Bayes переучивается** на этом наборе данных, изучая нерелевантные взаимосвязи, такие как заголовки.\n",
    "\n",
    "Теперь мы используем **LIME** для объяснения индивидуальных прогнозов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае мультикласса мы должны определить, для каких меток хотим получить объяснения, с помощью параметра «labels». **Сгенерируем пояснения** для меток 0 и 17:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "explainer = LimeTextExplainer(class_names=class_names, random_state=rng)\n",
    "idx = 1340\n",
    "exp = explainer.explain_instance(\n",
    "    newsgroups_test.data[idx],\n",
    "    model_with_preprocessing.predict_proba,\n",
    "    num_features=6,\n",
    "    labels=[0, 15],\n",
    ")\n",
    "print(\"Document id: %d\" % idx)\n",
    "print(\n",
    "    \"Predicted class =\",\n",
    "    class_names[model_nb.predict(test_vectors[idx]).reshape(1, -1)[0, 0]],\n",
    ")\n",
    "print(\"True class: %s\" % class_names[newsgroups_test.target[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возвращается специальный объект класса [Explanation](https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=Explanation#lime.explanation.Explanation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попросим **LIME** сгенерировать метки для K=2 классов. Чтобы увидеть, какие ярлыки имеют объяснения, используйте функцию `available_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1340\n",
    "exp = explainer.explain_instance(\n",
    "    newsgroups_test.data[idx],\n",
    "    model_with_preprocessing.predict_proba,\n",
    "    num_features=6,\n",
    "    top_labels=2,\n",
    ")\n",
    "print(exp.available_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp.as_list(label=0))\n",
    "print(exp.as_list(label=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что положительный и отрицательный знаки относятся к конкретной метке, так что слова, отрицательные по отношению к классу 0, могут быть положительными по отношению к классу 15, и наоборот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=newsgroups_test.data[idx], labels=(0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посмотрим на **визуализацию объяснений**.\n",
    "Обратите внимание на то, что для каждого класса слова в правой части строки являются «положительными», а слова в левой части — «отрицательными» для объясняемого класса.\n",
    "\n",
    "Также видно, что в классификаторе используются как **разумные слова** (такие как «геноцид», «Лютер», «семитский» и т. д.), так и **неразумные** («рис», «сова»).\n",
    "\n",
    "Давайте увеличим масштаб и просто посмотрим на **объяснения класса «атеизм»**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=newsgroups_test.data[idx], labels=(15,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глядя на этот пример, можно увидеть, что в заголовке или кавычках может быть и полезный сигнал, который будет помогать обобщению (например,  в строке «Тема»).\n",
    "\n",
    "А есть и **слова, которые нельзя обобщать** (например, адреса электронной почты и названия учреждений)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Пример изображения (ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Local Interpretable Model-Agnostic Explanations (LIME): An Introduction](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/)\n",
    "\n",
    "[[git] Using Lime with Pytorch](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Идея\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберемся, как работают **интерпретируемые представления признаков для картинок**.\n",
    "\n",
    "На рисунке ниже показан пример того, как **LIME** работает для **классификации изображений**.\n",
    "\n",
    "Представьте, что мы хотим объяснить классификатор, который предсказывает, насколько вероятно, что изображение содержит древесную лягушку.\n",
    "\n",
    "Мы берем изображение слева и делим его на **интерпретируемые компоненты** (смежные [суперпиксели](https://www.iro.umontreal.ca/~mignotte/IFT6150/Articles/SLIC_Superpixels.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/lime_interpretable_components.jpg\" alt=\"alttext\" width=\"550\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/\">Local Interpretable Model-Agnostic Explanations (LIME): An Introduction</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы **отключаем** некоторые из **суперпикселей** (закрашиваем серым).\n",
    "\n",
    "Для каждой такой картинки мы получаем **вероятность** того, что **на изображении есть древесная лягушка**, и формируем датасет из частично закрашенных картинок и предсказаний.\n",
    "\n",
    "Затем мы **обучаем линейную модель** на этом наборе данных. Веса, соответствующие суперпикселю, будут объяснять его вклад в предсказание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/lime_explanation_pipeline.jpg\" alt=\"alttext\" width=\"750\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/\">Local Interpretable Model-Agnostic Explanations (LIME): An Introduction</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем предсказание сверточной сети **Google Inception**. Посмотрим, почему ее классификатор предсказывает «древесную лягушку» как наиболее вероятный класс, за которым следуют «бильярдный стол» и «воздушный шар» с более низкими вероятностями.\n",
    "\n",
    "Мы видим, что **классификатор** в первую очередь **фокусируется на морде лягушки** как на объяснении предсказанного класса.\n",
    "\n",
    "Это также проливает свет на то, почему «бильярдный стол» имеет ненулевую вероятность: руки и глаза лягушки напоминают бильярдные шары, особенно на зеленом фоне. Точно так же сердце похоже на красный воздушный шар.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/lime_explaination_google_inception.jpg\" alt=\"alttext\" width=\"750\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/\">Local Interpretable Model-Agnostic Explanations (LIME): An Introduction</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q 'https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/cat_and_dog1.jpg' -O cat_and_dog1.jpg\n",
    "!wget -q 'https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/cat_and_dog2.png' -O cat_and_dog2.png\n",
    "!wget -q 'https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/imagenet_class_index.json' -O imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_image(path):\n",
    "    with open(os.path.abspath(path), \"rb\") as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "img = get_image(\"cat_and_dog1.jpg\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно преобразовать это изображение в тензор PyTorch и нормализовать его для использования в нашей предварительно обученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# resize & normalize\n",
    "\n",
    "\n",
    "def get_input_transform():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "# for get croped img from input tensor\n",
    "\n",
    "\n",
    "def get_reverse_transform():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Normalize(\n",
    "                mean=(0.0, 0.0, 0.0), std=(1 / 0.229, 1 / 0.224, 1 / 0.225)\n",
    "            ),\n",
    "            transforms.Normalize(\n",
    "                mean=(-0.485, -0.456, -0.406),\n",
    "                std=(1.0, 1.0, 1.0),\n",
    "            ),\n",
    "            transforms.Lambda(lambda x: torch.permute(x, (0, 2, 3, 1))),\n",
    "            transforms.Lambda(lambda x: x.detach().numpy()),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def get_input_tensors(img):\n",
    "    transform = get_input_transform()\n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "def get_crop_img(img_tensor):\n",
    "    transform = get_reverse_transform()\n",
    "    return transform(img_tensor)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Загрузим предобученную модель ResNet18, доступную в PyTorch, и классы изображений из ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "\n",
    "idx2label, cls2label, cls2idx = [], {}, {}\n",
    "with open(os.path.abspath(\"/content/imagenet_class_index.json\"), \"r\") as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    lable2idx = {class_idx[str(k)][1]: k for k in range(len(class_idx))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим предсказание. А после этого полученные нами прогнозы (логиты) пропустим через softmax, чтобы получить вероятности и метки классов для 5 лучших прогнозов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "img_t = get_input_tensors(img)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "logits = model(img_t.to(device))\n",
    "\n",
    "probs = F.softmax(logits, dim=1)\n",
    "probs5 = probs.topk(5)\n",
    "plt.imshow(get_crop_img(img_t))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "tuple(\n",
    "    (p, c, idx2label[c])\n",
    "    for p, c in zip(\n",
    "        probs5[0][0].detach().cpu().numpy(), probs5[1][0].detach().cpu().numpy()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(tabby — это тоже кошка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lime генерирует массив изображений из исходного входного изображения\n",
    "\n",
    "Таким образом, нам нужно предоставить конструктору:\n",
    "1. Исходное изображение в виде массива numpy\n",
    "2. Функцию классификации, которая будет принимать массив искаженных изображений в качестве входных данных и генерировать вероятности для каждого класса для каждого изображения в качестве выходных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому потребуется вспомогательная функция для обработки пакета изображений, в соответствии с API LIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def batch_predict(images):  # images are numpy arrays\n",
    "    model.eval()\n",
    "    transform = get_input_transform()\n",
    "    batch = torch.stack(tuple(transform(Image.fromarray(i)) for i in images), dim=0)\n",
    "\n",
    "    model.to(device)\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    logits = model(batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим экзепляр ImageExplainer и сгенерируем объект explanation. У `LimeImageExplainer` есть особенность: он работает только с numpy.array и 3-х канальными изображениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_image\n",
    "\n",
    "img_t = get_input_tensors(img)\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer(random_state=42)\n",
    "explanation = explainer.explain_instance(\n",
    "    np.array(255 * get_crop_img(img_t)).astype(\n",
    "        np.uint8\n",
    "    ),  # Lime assume that input is a numpy array :(\n",
    "    batch_predict,  # classification function\n",
    "    top_labels=5,\n",
    "    hide_color=0,\n",
    "    num_samples=1000,  # number of images that will be sent to classification function\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем top5 предсказаний, сделанных через LIME.\n",
    "\n",
    "P.S. Они не обязаны совпадать с предсказаниями для картинки без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, id in enumerate(explanation.top_labels):\n",
    "    print(i, idx2label[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся маской на изображении и посмотрим области, которые дают лучший прогноз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n",
    "\n",
    "for i, id in enumerate(explanation.top_labels[:2]):\n",
    "    temp, mask = explanation.get_image_and_mask(\n",
    "        id, positive_only=False, num_features=5, hide_rest=False\n",
    "    )\n",
    "    img_boundry = mark_boundaries(temp, mask)\n",
    "    ax[i].imshow(img_boundry)\n",
    "    ax[i].set_title(idx2label[id])\n",
    "    ax[i].axis(\"off\")\n",
    "    # number of clusters to be shown in the image: num_features=5\n",
    "    # show or not negatively impacting clusters: positive_only=False\n",
    "    # first 5 may be only positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зеленым цветом обозначена область наивысшего прогноза, оранжевым — области, которые меньше всего соответствуют нашему прогнозу. При `positive_only=False` будут показаны только границы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И другое изображение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_2 = get_image(\"cat_and_dog2.png\")\n",
    "plt.imshow(img_2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запуск Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = get_input_tensors(img_2)\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(\n",
    "    np.array(255 * get_crop_img(img_t)).astype(np.uint8),\n",
    "    batch_predict,  # classification function\n",
    "    top_labels=5,\n",
    "    hide_color=0,\n",
    "    num_samples=1000,\n",
    ")  # number of images that will be sent to classification function\n",
    "# Display top labels\n",
    "for i, id in enumerate(explanation.top_labels):\n",
    "    print(i, idx2label[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем сегменты, наиболее повлиявшие на каждое предсказание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(50, 10))\n",
    "\n",
    "for i, id in enumerate(explanation.top_labels):\n",
    "    temp, mask = explanation.get_image_and_mask(\n",
    "        id, positive_only=False, num_features=5, hide_rest=False\n",
    "    )\n",
    "    img_boundry = mark_boundaries(temp, mask)\n",
    "    ax[i].imshow(img_boundry)\n",
    "    ax[i].set_title(idx2label[id], fontsize=40)\n",
    "    ax[i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура LIME является гибкой в ​​том смысле, что любую интерпретируемую модель можно использовать для объяснения прогнозов.\n",
    "\n",
    "Кроме того, концепция интерпретируемых компонентов позволяет применять LIME для объяснения данных большой размерности, таких как классификация изображений или текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Принцип работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/shap_scheme.png\" alt=\"alttext\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель [**SHAP**](https://christophm.github.io/interpretable-ml-book/shap.html) — объяснить предсказание объекта $x$ путем **вычисления вклада каждого признака** в предсказание. Для этого вычисляются SHAP-значения, основанные на **значениях Шепли** из **теории игр**.\n",
    "\n",
    "**SHAP** рассматривает объясняемую **модель как игру**, а **признаки**, используемые в обучении, как **коалицию игроков**. **SHAP-значения** говорят нам, как **справедливо распределить \"выигрыш\" между игроками** — это вклад, который каждый игрок вносит в предсказание модели. SHAP основывается на анализе локальной важности признаков, соответственно, \"выигрыш\" — это предсказание, сделанное моделью для одного образца.\n",
    "\n",
    "При этом игрок не обязательно должен быть индивидуальным признаком, он может состоять из **группы признаков**. Например, для моделей, работающих с изображениями, пиксели могут быть сгруппированы в суперпиксели, а \"выигрыш\" распределяется между ними.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, каким образом можно определить вклад признака в предсказание, сделанное моделью? Предположим, у нас есть **модель**, которая **предсказывает доход человека на основании его возраста, пола и профессии**. Для определения вклада каждого признака рассмотрим все возможные комбинации $f$ признаков в модели ($f$ от $0$ до $3$) и представим их в виде графа:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/shap_features_graph.png\" alt=\"alttext\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Элементы теории кооперативных игр](http://old.math.nsc.ru/~mathecon/Marakulin/CooGAMES.pdf)\n",
    "\n",
    "Здесь каждая вершина изображает набор признаков, а каждое **ребро** — **добавление нового признака** в набор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее **SHAP обучает модель на каждом наборе признаков** в графе (сохраняя гиперпараметры модели и набор тренировочных объектов).\n",
    "\n",
    "Предположим, что мы **обучили модель на всех имеющихся наборах** признаков и сделали предсказание зарплаты для объекта $x_0$.\n",
    "\n",
    "**Нулевая модель** делает самое простое \"предсказание\" — **усредняет зарплату**, не учитывая никакие признаки (точность соответствующая).\n",
    "Дальше мы можем использовать по одному признаку, более сложные модели базируются на нескольких признаках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/shap_predictions_graph.png\" alt=\"alttext\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь в вершинах графа находятся предсказания, сделанные соответствующей моделью для объекта $x_0$.\n",
    "\n",
    "Зная предсказания всех возможных моделей для одного объекта, мы можем посчитать вклад каждого признака в предсказание. Вклад признака в предсказание высчитывается на основании его **маржинальных вкладов** *(marginal contribution)*. **Маржинальный вклад признака** — это **разница** между **предсказанием модели**, обученной на наборе, **включающей** данный **признак**, и модели, обученной на том же наборе **без** данного **признака**. В данном случае можно рассчитать  маржинальный вклад для признака как разницу предсказаний моделей, соединенных ребром в графе, а вес ребра и будет являться маржинальным вкладом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, маржинальный вклад для признака Age в предсказание нулевой модели для объекта $x_0$ рассчитывается следующим образом:\n",
    "\n",
    "$$ \\large MC_{Age,\\{Age\\}}(x_0)=Predict_{\\{Age\\}}(x_0)-Predict_{\\emptyset}(x_0)=40k\\,\\$-50k\\,\\$=-10k\\,\\$$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы **оценить вклад признака** в предсказание модели, нужно учесть его **маржинальные вклады** во все модели, где этот **признак присутствует** (в графе выделены соответствующие ребра):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/shap_estimation_important_features.png\" alt=\"alttext\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP-значение является общим вкладом признака в предсказание и вычисляется как **взвешенная сумма маржинальных вкладов**:\n",
    "$ \\large\n",
    "\\begin{multline*}\n",
    "SHAP_{Age}(x_0)=w_1\\cdot MC_{Age,\\{Age\\}}(x_0)+w_2\\cdot MC_{Age,\\{Age, Gender\\}}(x_0)\\\\+w_3\\cdot MC_{Age,\\{Age, Job\\}}(x_0)+w_4\\cdot MC_{Age,\\{Age, Gender, Job\\}}(x_0)\n",
    "\\end{multline*}\n",
    "$\n",
    "\n",
    "Веса определяются согласно правилу: сумма весов маржинальных вкладов для каждого уровня графа (числа признаков в наборе $f$) должна быть равной 1.\n",
    "\n",
    "Таким образом, нетрудно рассчитать веса маржинальных вкладов признака Age:\n",
    "* Первый уровень содержит $3$ ребра, каждое из которых будет иметь вес $\\displaystyle \\frac{1}{3}$\n",
    "* Второй уровень содержит $6$ ребер, каждое из которых будет иметь вес $\\displaystyle \\frac{1}{6}$\n",
    "* Третий уровень содержит $3$ ребра, каждое из которых будет иметь вес $\\displaystyle \\frac{1}{3}$\n",
    "\n",
    "Таким образом: $\\displaystyle w_1=w_4=\\frac{1}{3}, \\; w_2=w_3=\\frac{1}{6}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае вес маржинального вклада в модель, обученную на наборе из $f$ признаков, обратно пропорционален числу маржинальных вкладов во все модели, обученные на наборе из $f$ признаков.\n",
    "\n",
    "Число маржинальных вкладов во все модели, обученные на наборе из $f$ признаков, может быть рассчитано как: $f\\cdot C_{F}^{f}$, где $F$ — количество признаков. Например, рассчитаем вес $w_2$ маржинального вклада $MC_{Age,\\{Age, Gender\\}}(x_0)$, согласно правилу:\n",
    "\n",
    "$$\\displaystyle w_2=\\left[f\\cdot C_{F}^{f}\\right]^{-1}=\\left[2\\cdot C_{3}^{2}\\right]^{-1}=\\frac{1}{6}$$\n",
    "\n",
    "где $\\large C_{F}^{f}$ — биномиальный коэффициент.\n",
    "\n",
    "$$\\large\n",
    "\\begin{pmatrix}\n",
    "F\\\\\n",
    "f\n",
    "\\end{pmatrix}=C_{F}^{f} = \\frac{F!}{f!(F-f)!}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/shap_compute_marginal_weights.png\" alt=\"alttext\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие SHAP-значения можно легко посчитать для каждого признака для каждого объекта и, например, усреднить по объектам какой-то группы.\n",
    "\n",
    "Например, можно взять все объекты, которые модель оценивает высоко, и посмотреть значимые дли них признаки. Либо, если у нас есть какая-то особенная группа объектов, можем контролировать, что для них модель выучивает именно важные признаки, а не какой-то мусор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У описанного выше метода есть недостаток: нам нужно обучить огромное количество моделей. Для всего 10-ти признаков это $10\\cdot2^9 = 5120$ моделей.\n",
    "\n",
    "На помощь нам приходит статья [A Unified Approach to Interpreting Model Predictions](https://arxiv.org/pdf/1705.07874.pdf). В ней введено понятие **аддитивного метода атрибуции признаков** в котором в качестве “стеклянного ящика” используется функция бинарных (есть признак/нет признака) переменных:\n",
    "\n",
    "$$g(z') = \\phi_0 +\\sum_{i=1}^M\\phi_i z_i'$$\n",
    "\n",
    "где $z'\\in\\{0,1\\}^M$ — вектор из $0$ и $1$ длины $M$: $0$ — отсутсвие признака, $1$ — наличие признака, $M$ — количество упрощенных признаков (например, суперпикселей для изображения). $ϕ_i$ — вклад (важность) $i$-того упрощенного признака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для адекватного описания работы модели данный метод должен удовлетворять трем свойствам:\n",
    "1. **Локальная точность:** результат модели объяснения $g(x’)$ должен совпадать с результатом оригинальной модели $f(x)$\n",
    "\n",
    "$$f(x)=g(x')$$\n",
    "\n",
    "> $x' = \\{x_1, x_2, ...,x_M\\}$ — упрощенные признаки объекта (например: суперпиксели для изображения), вектор $z’$ выше характеризует присутствие/отсутствие этих признаков.\n",
    "\n",
    "\n",
    "\n",
    "> $x$ — оригинальный набор признаков объекта (например: значения rgb пикселей изображения).\n",
    "\n",
    "> Упрощенный набор признаков соответствует оригинальному набору:\n",
    "\n",
    "$$x=h_x(x')$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Отсутствие:** если признак отсутствует — он вносит нулевой вклад\n",
    "\n",
    "$$x_i = \\text{None} \\to \\phi_i = 0$$\n",
    "\n",
    "3. **Консистентность:** если модель $f$ изменяется таким образом $f'$, что вклад некоторых упрощенных признаков $\\phi_i$ увеличивается или остается прежним, независимо от вклада других признаков, то выход модели не может уменьшиться\n",
    "\n",
    "$$\\phi_i(f', x) \\geq \\phi_i(f, x) \\to f'_x(z') - f'_x(z'\\verb!\\!i) \\geq f_x(z') - f_x(z'\\verb!\\!i)$$\n",
    "\n",
    "\n",
    "> где $z'\\verb!\\!i$ — это $z'$ при $z_i=0$.\n",
    "\n",
    "\n",
    "В [статье](https://arxiv.org/pdf/1705.07874.pdf) показано, что единственным методом адитивной атрибуции, удовлетворяющим всем свойствам являются значения Шепли, которые можно записать, как\n",
    "$$\\phi_i(f, x) = \\sum_{z'⊆x'} \\frac{|z|!(M-|z'|-1)!}{M!}[f_x(z')-f_x(z'\\verb!\\!i)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения Шепли можно [получить](https://arxiv.org/pdf/1705.07874.pdf) (вывод есть в статье) используя **линейный LIME** 🍋\n",
    "\n",
    "$$\\hat{g}=\\underset{g\\in G}{\\mathrm{argmin}}L(f,g,\\pi_{x'})+\\Omega (g)$$\n",
    "\n",
    "со следующими параметрами\n",
    "\n",
    "$$\\Omega(g) = 0$$\n",
    "\n",
    "$$\\pi_{x'}(z')=\\frac{M-1}{C^{|z'|}_M|z'|(M-|z'|)}$$\n",
    "\n",
    "$$L(f, g, \\pi_{x'}) = \\sum_{z' \\in Z}[f(h^{-1}(z'))-g(z')]^2\\pi_{x'}(z')$$\n",
    "\n",
    "где $|z'|$ — количество ненулевых элементов $z'$, $C^k_n$ — биномиальный коэффициент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой метод расчета называется Kernel SHAP. Он позволяет не обучать огромное количество моделей и реализовать вычисление значений Шепли за конечное время. Именно так считаются значения Шепли в библиотеке SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример для табличных данных (Boston Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим пакет SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install -q shap\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для примера скачаем датасет жилья Бостона (boston_dataset), в котором проанализируем зависимость цены на жилье от параметров жилья и района, в котором оно находится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем данные на train и test и обучим Random Forest модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load dataset\n",
    "boston_dataset = pd.read_csv(\n",
    "    \"https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/boston_dataset.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "x_data = boston_dataset.iloc[:, :-1]\n",
    "y_data = boston_dataset[\"target\"]\n",
    "\n",
    "# Split the data into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build the model with the random forest regression algorithm\n",
    "rng = np.random.RandomState(42)\n",
    "model = RandomForestRegressor(n_jobs=-1, max_depth=4, random_state=rng)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим Shap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# explain the model's predictions using SHAP\n",
    "# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(shap_values))  # numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Force plots**\n",
    "\n",
    "Хороший способ визуализировать вклад каждого признака в конкретный прогноз — использовать график сил.\n",
    "\n",
    "В приведенном ниже примере показан график силы для первого объекта в тестовом наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "# visualize the first prediction’s explanation\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], x_test.iloc[0, :], matplotlib=True)\n",
    "# https://github.com/shap/shap/issues/279 very last answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $f(x)$ — это прогноз модели по анализируемому объекту недвижимости. А base_value — это средний прогноз по всему набору тестовых данных. Или, другими словами, это значение, которое можно было бы спрогнозировать, если бы мы не знали никаких характеристик текущего примера.\n",
    "\n",
    "* Элементы, которые способствуют увеличению цены, показаны красным, а те, которые уменьшают — синим.\n",
    "\n",
    "* Длина вектора — “сила” влияния. Численное значение (оно не совпадает с длиной) — важность признака. Из нашего анализа мы помним, что увеличение RM положительно влияет на предсказание, но для данного объекта RM маленькое, поэтому влияет на цену отрицательно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Waterfall_plot**\n",
    "\n",
    "Другой способ понимания влияния факторов для конкретного примера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation using waterfall\n",
    "\n",
    "explainer_for_wf = shap.Explainer(model, x_train)\n",
    "exlanation = explainer_for_wf(x_test)\n",
    "\n",
    "shap.plots.waterfall(exlanation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот график объясняет движущие силы конкретного прогноза:\n",
    "\n",
    "Влияние каждого отдельного признака (наименее значимые признаки объединяются в одну группу) представлено стрелками, которые перемещают логарифмическое отношение шансов влево и вправо, начиная с базового значения (внизу картинки).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Указанный выше пример приведен только для одного объекта.\n",
    "\n",
    "Если мы возьмем много пояснений Force plots, повернем их на 90 градусов, а затем сложим их по горизонтали, мы сможем увидеть объяснения для всего набора данных (в notebook этот график является интерактивным):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "# visualize the training set predictions\n",
    "shap.force_plot(explainer.expected_value, shap_values, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* По оси x — объекты, по оси y — вклад признаков в предсказание для каждого объекта. Важно заметить, что объекты упорядочены по similarity, которая считается по расстоянию между признаками.\n",
    "* Можно использовать выпадающее меню, чтобы посмотреть, как будет выглядеть график без упорядочивания или при упорядочивании по конкретному признаку.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary plot**\n",
    "\n",
    "Сводный график с `plot_type = 'bar'` даст нам график важности признаков.\n",
    "\n",
    "Признаки с высокой предсказательной способностью показаны вверху, а с низкой предсказательной силой — внизу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что, согласно Shap, наименьшей предсказательной способностью обладают признаки **CHAS, ZN, RAD** и **INDUS**.\n",
    "\n",
    "Здесь мы только что рассмотрели алгоритм TreeExplainer для интерпретации модели.\n",
    "\n",
    "Вы можете изучить остальные алгоритмы: DeepExplainer, KernelExplainer, LinearExplainer и GradientExplainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример NLP (перевод с английского на русский)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример интерпретации модели для предварительно обученной модели машинного перевода\n",
    "[Machine Translation Example](https://github.com/slundberg/shap/blob/master/notebooks/text_examples/translation/Machine%20Translation%20Explanations.ipynb). Для перевода будем использовать предобученную [модель-трансформер](https://arxiv.org/abs/1810.04805)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем одну из моделей [huggingface ](https://github.com/huggingface/transformers)\n",
    "\n",
    "\n",
    "Модель: [Language Technology in Helsinki](https://blogs.helsinki.fi/language-technology/)\n",
    "\n",
    "\n",
    "[Language Technology Research Group at the University of Helsinki](https://huggingface.co/Helsinki-NLP)\n",
    "\n",
    "\n",
    "[Helsinki-NLP/opus-mt-en-ru](https://huggingface.co/Helsinki-NLP/opus-mt-en-ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем модель.\n",
    "\n",
    "Для этого используется класс-фабрика, на вход которому передается имя модели, а возвращает он объект соответствующего класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lang = \"en\"\n",
    "target_lang = \"ru\"\n",
    "model_name = f\"Helsinki-NLP/opus-mt-{lang}-{target_lang}\"\n",
    "\n",
    "# Download the model and the tokenizer\n",
    "# can also try translation with different pre-trained models\n",
    "\n",
    "# It's a Factory pattern\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "clear_output()\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае нам вернулся объект типа [MarianMT](https://huggingface.co/transformers/model_doc/marian.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим [токенайзер](https://huggingface.co/transformers/main_classes/tokenizer.html).\n",
    "\n",
    "Как мы уже обсуждали, токенайзер преобразует слова и знаки препинания из исходного текста в токены, которые можно подать на вход модели. В данном случае возвращаются id. При этом не всегда одно слово преобразуется в один токен, иногда слово разбивается по слогам на несколько токенов.\n",
    "\n",
    "\n",
    "Создается токенайзер так же фабрикой по имени модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(type(tokenizer))\n",
    "\n",
    "input = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "print(input)\n",
    "\n",
    "translated = model.generate(\n",
    "    **tokenizer(\"Hello world!\", return_tensors=\"pt\").to(device), max_new_tokens=512\n",
    ")\n",
    "# ** -  is dictionary unpack operator\n",
    "# https://pavel-karateev.gitbook.io/intermediate-python/sintaksis/args_and_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь переведем целую фразу и проанализируем, как выход модели связан со входом.\n",
    "\n",
    "Для этого создадим объект [shap.Explainer](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html), который в данном случае инициализируется экземпляром модели и экземпляром токенайзера*.\n",
    "\n",
    "\n",
    "Вместо того, чтобы запускать саму модель, мы запускаем `Explainer` (неявно вызывая его метод `__call__`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* В действительности вторым параметром конструктора `shap.Explainer` не обязательно должен быть токенайзер. `shap.Explainer` принимает объект, поддерживающий интерфейс `masker`:\n",
    "\n",
    "```masked_args = masker(*model_args, mask=mask)```\n",
    "\n",
    "Он используется для исключения части аргументов, и токенайзеры поддерживают этот интерфейс (`shap.TokenMasker`). Благодаря такому подходу shap может работать с различными моделями как с \"черным ящиком\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# define the input sentences we want to translate\n",
    "data = [\n",
    "    \"Transformers are a type of neural network architecture that have been gaining popularity. Transformers were developed to solve the problem of sequence transduction, or neural machine translation.\"\n",
    "    \"That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc..\"\n",
    "]\n",
    "\n",
    "# we build an explainer by passing the model we want to explain and\n",
    "# the tokenizer we want to use to break up the input strings\n",
    "explainer = shap.Explainer(model, tokenizer, max_new_tokens=512)\n",
    "\n",
    "# explainers are callable, just like models\n",
    "explanation = explainer(data)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получаем объект класса [shap.Explanation](https://shap.readthedocs.io/en/latest/generated/shap.Explanation.html#shap-explanation), который содержит значения Шепли для каждого токена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data\", explanation.data)\n",
    "print(\"Shap values\", explanation.values)\n",
    "print(\"Shape\", explanation.shape)  # 1, in, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, используя магию shap, можно визуализировать результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.plots.text(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная языковая модель предсказывает эмбеддинги — вектора, которые преобразуются в токены. При этом SHAP для оценки важности использует сжатые представления эмбеддингов. В данном случае наибольший интерес представляет не раскраска outputs (абсолютное значение сжатого представления эмбеддинга на выходе), а подветка inputs, которая появляется, когда мы нажимаем на выходной токен. Она показывает, какие входные токены влияют на выходной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример NLP (абстрактное обобщение текста)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[text] plot](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/text.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере интерпретируется модель генерации объяснений для предварительно обученной модели для составления краткого резюме статьи.\n",
    "\n",
    "Используется датасет Extreme Summarization [XSum](https://huggingface.co/sshleifer/distilbart-xsum-12-6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-xsum-12-6\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-xsum-12-6\").to(\n",
    "    device\n",
    ")\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"xsum\", split=\"train\", trust_remote_code=True)  # load dataset\n",
    "s = dataset[\"document\"][0:1]  # slice inputs from dataset to run model inference on\n",
    "explainer = shap.Explainer(model, tokenizer)  # create an explainer object\n",
    "explanation = explainer(s)  # Compute shap values\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.plots.text(explanation)  # Visualize shap explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентные методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея метода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имея информацию о структуре модели, можно использовать градиент, чтобы понять, как входы связаны с выходом.\n",
    "\n",
    "Градиент указывает направление возрастания функции. Если мы выберем  **logit, соответствующий метке наиболее вероятного класса**, и посчитаем для него **градиент по исходному изображению**, мы можем узнать, какие пиксели нужно “усилить”, чтобы модель была более уверена в ответе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/saliency_via_backprop.png\" width=\"800\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1312.6034.pdf\">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\n",
    "</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут важно заметить, что пример из статьи на картинке выше — это **cherry picked** (специально отобранный пример, для которого данный метод дает хороший результат). Давайте посмотрим на нашей картинке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Пример изображения (ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q 'https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/cat_and_dog1.jpg' -O cat_and_dog1.jpg\n",
    "!wget -q 'https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/imagenet_class_index.json' -O imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_image(path):\n",
    "    with open(os.path.abspath(path), \"rb\") as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "img = get_image(\"cat_and_dog1.jpg\")\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам нужно преобразовать это изображение в тензор PyTorch, а также его нормализовать для использования в нашей предварительно обученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# resize & normalize\n",
    "\n",
    "\n",
    "def get_input_transform():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "# for get croped img from input tensor\n",
    "\n",
    "\n",
    "def get_reverse_transform():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Normalize(\n",
    "                mean=(0.0, 0.0, 0.0), std=(1 / 0.229, 1 / 0.224, 1 / 0.225)\n",
    "            ),\n",
    "            transforms.Normalize(\n",
    "                mean=(-0.485, -0.456, -0.406),\n",
    "                std=(1.0, 1.0, 1.0),\n",
    "            ),\n",
    "            transforms.Lambda(lambda x: torch.permute(x, (0, 2, 3, 1))),\n",
    "            transforms.Lambda(lambda x: x.detach().numpy()),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def get_input_tensors(img):\n",
    "    transform = get_input_transform()\n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "def get_crop_img(img_tensor):\n",
    "    transform = get_reverse_transform()\n",
    "    return transform(img_tensor)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Загрузим предобученную модель ResNet18, доступную в PyTorch, и классы изображений из ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "\n",
    "idx2label, cls2label, cls2idx = [], {}, {}\n",
    "with open(os.path.abspath(\"/content/imagenet_class_index.json\"), \"r\") as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    lable2idx = {class_idx[str(k)][1]: k for k in range(len(class_idx))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим предсказание. А после этого полученные нами прогнозы (logits) пропустим через softmax, чтобы получить вероятности и метки классов для 6 лучших прогнозов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))\n",
    "img_t = get_input_tensors(img)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "logits = model(img_t.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def top_k_class(logits, k=6):\n",
    "    prediction = F.softmax(logits, dim=1)\n",
    "    top_props, top_inds = prediction.topk(k)\n",
    "\n",
    "    for i in range(k):\n",
    "        category_name = idx2label[top_inds[0][i].item()]\n",
    "        score = top_props[0][i].item()\n",
    "        print(f\"{category_name} {top_inds[0][i].item()}: {100 * score:.1f}%\")\n",
    "\n",
    "\n",
    "top_k_class(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Включаем расчет градиента для изображения. Делаем предсказание. Выбираем наиболее вероятный logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t.requires_grad = True  # Tell pytorch to compute grads w.r.t. inputs too\n",
    "\n",
    "logits = model(img_t.to(device))  # [1,1000] batch of one element, 1000 class scores\n",
    "top_score, top_idx = logits[0].topk(1)  # Get id of class with best score\n",
    "id = top_idx[0].item()\n",
    "\n",
    "print(id, idx2label[id])  # Print the label this class\n",
    "\n",
    "score = logits[:, id]  # Model output for particular class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выхода модели, соответствующего нашему классу, рассчитываем градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "score.backward(retain_graph=True)\n",
    "\n",
    "# retain_grad = True is not nessesary\n",
    "# But if we run this code second time, we got a torch error without it\n",
    "# because pytorch want to accumulate gradients explicitly\n",
    "\n",
    "print(img_t.grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У входного тензора (изображения) появился градиент, который указывает на то, как каждый элемент повлиял на выход модели. Отобразим этот градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as P\n",
    "\n",
    "\n",
    "# Helper method to display grad\n",
    "def grad_to_image(raw_grads, percentile=99):\n",
    "    gradients = raw_grads.detach().cpu().numpy()\n",
    "    gradients = np.transpose(gradients, (0, 2, 3, 1))[0]\n",
    "\n",
    "    image_2d = np.sum(np.abs(gradients), axis=2)\n",
    "\n",
    "    vmax = np.percentile(image_2d, percentile)\n",
    "    vmin = np.min(image_2d)\n",
    "\n",
    "    return np.clip((image_2d - vmin) / (vmax - vmin), 0, 1)\n",
    "\n",
    "\n",
    "def plot_saliency_map(img_tensor, saliency_map):\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    img = get_crop_img(img_t)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(saliency_map, cmap=P.cm.gray, vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_map = grad_to_image(img_t.grad)\n",
    "plot_saliency_map(img_t, saliency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Карта важности (saliency map)**, полученная таким образом, получается очень зашумленной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема насыщения (saturation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из недостатков **карты важности** (saliency map), полученной методом **Vanilla Gradient**, является **проблема насыщения** (saturation). Простыми словами эту проблему можно сформулировать так: если какой-то **признак “идеально” характеризует объект**, как принадлежащий к определенному классу, то **градиент** этого признака по логиту этого класса **будет нулевым**. То есть **Vanilla Gradient** будет **занижать важность очень хороших признаков**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/saturation_problem.png\" width=\"800\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1704.02685.pdf\">Learning Important Features Through Propagating Activation Differences</a></em>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более математично проблему можно описать так. Пусть $h$ — аналог активации некоторого нейрона, вычисляющийся как $h=max(0,1-i_1-i_2)$. Если мы возьмем значения признаков $i_1=1$ и $i_2=1$, то на выходе получим значение $h=0$. Далее по очереди будем занулять значения каждого из признаков, внося таким образом пертурбации: $i_1=0$ и $i_2=1$,  $i_1=1$ и $i_2=0$. В обоих случаях выход по-прежнему будет $h=0$. Может сложиться обманчивое впечатление, что ни один из признаков не влияет на результат вычисления. Таким образом, мы столкнулись с проблемой, что подход, основанный на изменении признаков, будет занижать значимость признаков, чей вклад в результат достиг насыщения. Аналогично градиентные методы также будут недооценивать важность признаков при насыщении, поскольку градиент в данном случае будет равным 0.\n",
    "\n",
    "Проблема насыщения не является редкой, в частности, с ней можно столкнуться в биологии при построении [моделей](https://www.nature.com/articles/nmeth.3547), объясняющих вклад единичных мутаций на то или иное свойство организма, что связано с вырожденностью генетического кода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Принцип взятия градиента по входу используется при **состязательных атаках (adversarial attacks)**.\n",
    "\n",
    "Если не просто визуализировать градиент, а с его помощью менять изображение, усиливая определенную метку класса, то можно обмануть сеть и заставить ее неверно классифицировать картинку, незначительно поменяв ее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/adversarial_attack.jpg\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее:\n",
    "\n",
    "[[wiki] Adversarial machine learning](https://en.wikipedia.org/wiki/Adversarial_machine_learning)\n",
    "\n",
    "[Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/adversarial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothGrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея метода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемой Vanilla Gradient Ascent является большая зашумленность карты важности. Было придумано несколько способов борьбы с этим, один из них был предложен в статье [SmoothGrad: removing noise by adding noise](https://arxiv.org/pdf/1706.03825.pdf).\n",
    "\n",
    "Как вы можете догадаться из названия статьи, идея **SmoothGrad** заключается в **добавлении** к исходному изображению $x$ **гауссовского шума**:\n",
    "\n",
    "$$\\large x+\\mathcal{N}(0, \\sigma^2).$$\n",
    "\n",
    "Для набора зашумленных изображений с помощью **Vanilla Gradient** рассчитываются **карты важности (saliency map)**:\n",
    "\n",
    "$$\\large M_c(x+\\mathcal{N}(0, \\sigma^2)).$$\n",
    "\n",
    "**Карты важности**, полученные от зашумленных изображений, **усредняются**:\n",
    "\n",
    "$$\\large SmoothGrad = \\frac{1}{n}\\sum_{1}^{n}M_c(x+\\mathcal{N}(0, \\sigma^2)),$$\n",
    "\n",
    "$SmoothGrad$ — результат SmoothGrad.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/smoth_grad.png\" width=\"800\">\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1706.03825.pdf\">SmoothGrad: removing noise by adding noise\n",
    "</a></em></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье рекомендуют выбирать $n = 50$ и 10–20% шума. Уровень шума определяют как отношение:\n",
    "\n",
    "$$\\large \\frac{\\sigma}{x_{max}-x_{min}}.$$\n",
    "\n",
    "SmoothGrad частично решает проблему насыщения, “раскачивая” хорошие признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Пример изображения (ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации работы SmoothGrad используем [код](https://github.com/PAIR-code/saliency/blob/master/Examples_pytorch.ipynb) c [сайта](https://pair-code.github.io/saliency/), посвященного [статье](https://arxiv.org/pdf/1706.03825.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У кода есть особенность: нужно написать функцию `call_model_function`, вызывающую модель.\n",
    "\n",
    "При этом любое изображение, поданное в метод `GetMask` класса `GradientSaliency`, будет преобразовано в `np.array`. К тому же, размер входного изображения и карты важности на выходе должен совпадать, что осложняет использование `torchvision.transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import saliency.core as saliency\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "\n",
    "\n",
    "def call_model_function(img, call_model_args=None, expected_keys=None):\n",
    "    img_t = torch.tensor(np.transpose(img, (0, 3, 1, 2)))\n",
    "    transform = transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "    )\n",
    "    img_t = transform(img_t)\n",
    "    img_t.requires_grad_(True)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logits = model(img_t.float().to(device))\n",
    "\n",
    "    top_score, top_idx = logits[0].topk(1)  # Get id of class with best score\n",
    "    target_class_idx = top_idx[0].item()\n",
    "\n",
    "    output = logits[:, target_class_idx]\n",
    "    grads = torch.autograd.grad(\n",
    "        output, img_t, grad_outputs=torch.ones_like(output)\n",
    "    )  # output[:, target_class_idx]\n",
    "    grads = torch.movedim(grads[0], 1, 3)\n",
    "    gradients = grads.detach().numpy()\n",
    "    return {saliency.base.INPUT_OUTPUT_GRADIENTS: gradients}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем Vanilla Gradient, рассчитанный для исходного изображения с помощью установленной библиотеки, и результат  SmoothGrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_image(\"cat_and_dog1.jpg\")\n",
    "img_t = get_input_tensors(img)\n",
    "img_arr = get_crop_img(img_t)\n",
    "\n",
    "gradient_saliency = saliency.GradientSaliency()\n",
    "\n",
    "vanilla_mask_3d = gradient_saliency.GetMask(img_arr, call_model_function)\n",
    "smoothgrad_mask_3d = gradient_saliency.GetSmoothedMask(img_arr, call_model_function)\n",
    "\n",
    "# Call the visualization methods to convert the 3D tensors to 2D grayscale.\n",
    "vanilla_mask_grayscale = saliency.VisualizeImageGrayscale(vanilla_mask_3d)\n",
    "smoothgrad_mask_grayscale = saliency.VisualizeImageGrayscale(smoothgrad_mask_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as P\n",
    "\n",
    "\n",
    "def ShowGrayscaleImage(im, title=\"\", ax=None):\n",
    "    if ax is None:\n",
    "        P.figure()\n",
    "    P.axis(\"off\")\n",
    "    P.imshow(im, cmap=P.cm.gray, vmin=0, vmax=1)\n",
    "    P.title(title)\n",
    "\n",
    "\n",
    "# Set up matplot lib figures.\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 5)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_arr)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ShowGrayscaleImage(\n",
    "    vanilla_mask_grayscale, title=\"Vanilla Gradient\", ax=P.subplot(1, 3, 2)\n",
    ")\n",
    "ShowGrayscaleImage(smoothgrad_mask_grayscale, title=\"SmoothGrad\", ax=P.subplot(1, 3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея метода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий метод, который мы посмотрим, называется **Integrated Gradients**. Он напоминает **SmoothGrad** тем, что мы намеренно \"портим\" изображения. Давайте разберемся, как он работает.\n",
    "\n",
    "В методе **Integrated Gradients** мы выбираем **опорное изображение** $x'$. В качестве опорного изображения используется черный фон (все нули по RGB каналам). Оцениваемое изображение $x$ примешивают к опорному изображению $x’$ с пропорцией $\\alpha$:\n",
    "\n",
    "$$\\large x'+\\alpha(x-x')$$\n",
    "\n",
    "Таким образом мы портим изображение и постепенно его восстанавливаем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/IG_fireboat_1.png\" width=\"900\">\n",
    "\n",
    "<em>Source: <a href=\"https://www.tensorflow.org/tutorials/interpretability/integrated_gradients\">Tensorflow tutorials: Integrated gradients\n",
    "</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для смеси изображений считается **Vanilla Gradient**:\n",
    "\n",
    "$$\\large M_c(x'+\\alpha(x-x'))$$\n",
    "\n",
    "Формула, лежащая в основе **Integrated Gradients**, была предложена в [статье](https://arxiv.org/pdf/1703.01365.pdf). Это — интегральное значение градиента при восстановлении изображения.\n",
    "\n",
    "$$\\large IntegratedGrads(x) = (x-x')\\cdot\\int_{\\alpha=0}^1 M_c(x'+\\alpha(x-x'))dα$$\n",
    "\n",
    "Множитель $(x-x)’$ появился, т.к. изначально градиент был по $dx = (x-x’)d\\alpha$.\n",
    "\n",
    "В расчетах интеграл аппроксимируется суммой:\n",
    "\n",
    "\n",
    "$$\\large IntegratedGrads(x) \\approx (x-x')\\cdot\\sum_{k=1}^m M_c(x'+\\frac{k}{m}(x-x'))\\cdot\\frac{1}{m}$$\n",
    "\n",
    "Значение $m$ выбирают в диапазоне от 20 до 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример результата:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/IG_fireboat.png\" width=\"500\">\n",
    "\n",
    "<em>Source: <a href=\"https://www.tensorflow.org/tutorials/interpretability/integrated_gradients\">Tensorflow tutorials: Integrated gradients\n",
    "</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Gradients частично решает проблему насыщения за счет изменения изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Пример изображения (ResNet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(img_t.to(device))\n",
    "output = F.softmax(output, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "predicted_label = idx2label[pred_label_idx.item()]\n",
    "print(\"Predicted:\", predicted_label, \"(\", prediction_score.squeeze().item(), \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "\n",
    "integrated_gradients = IntegratedGradients(model)\n",
    "attributions_ig = integrated_gradients.attribute(\n",
    "    img_t.to(device), target=pred_label_idx, n_steps=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency_map = grad_to_image(attributions_ig)\n",
    "\n",
    "# Set up matplot lib figures.\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img_arr)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ShowGrayscaleImage(\n",
    "    vanilla_mask_grayscale, title=\"Vanilla Gradient\", ax=P.subplot(1, 4, 2)\n",
    ")\n",
    "ShowGrayscaleImage(smoothgrad_mask_grayscale, title=\"SmoothGrad\", ax=P.subplot(1, 4, 3))\n",
    "ShowGrayscaleImage(saliency_map, title=\"Integrated Gradients\", ax=P.subplot(1, 4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пакет [captum](https://captum.ai/docs/attribution_algorithms) можно использовать и для других модальностей данных, например, для [NLP BERT](https://captum.ai/tutorials/Bert_SQUAD_Interpret2). Там реализовано большое количество модификаций алгоритма Integrated Gradients и не только."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Deep Explainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея метода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм [DeepLIFT](https://arxiv.org/abs/1704.02685) — развитие идеи **Vanilla Gradient**, чем-то похожее на Integrated Gradients. Основная идея **DeepLIFT** заключается в том, что он оценивает важность признака с точки зрения **отличий от некоторого «референса»**, где референс выбирается в соответствии с решаемой проблемой. Референс для входных данных представляет собой некий нейтральный объект, у которого отсутствует специфическое свойство, например, таким свойством может быть присутствие/отсутствие того или иного объекта на изображении. Помимо референса для входных данных, определяется референс для каждого нейрона (активация соответствующего нейрона, рассчитанная для референсного объекта), аналогично референсом выхода сети будет вычисленный выход для референсного объекта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepLIFT объясняет разницу между выходом сети для целевого объекта и выходом сети для референсного объекта на основании разницы между этими объектами. Пусть $t$ представляет собой выход некоторого нейрона/сети, а $x_1,x_2,...,x_n$ — нейроны одного из предшествующих слоев или множества слоев, необходимые для расчета $t$. Пусть $t_0$ — референсная активация для $t$. Тогда мы можем определить разницу выхода нейрона/сети для целевого объекта с выходом для референсного объекта как $\\Delta t=t-t_0$. Тогда мы можем разложить ее на вклады $C_{\\Delta x_i \\Delta t}$  разниц между активациями нейронов в предыдущих слоях для целевого объекта и референса ($\\Delta x_i$):\n",
    "\n",
    "$$\\large \\sum_{i=1}^nC_{\\Delta x_i \\Delta t}=\\Delta t$$\n",
    "\n",
    "Как именно DeepLIFT вычисляет эти вклады останется за рамками рассмотрения этой лекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование такого подхода позволяет DeepLIFT решать проблему насыщения, приведенную выше, а также другую проблему, при которой градиент может совершать спонтанные скачки. Для иллюстрации этого примера рассмотрим функцию ReLU со сдвигом на $-10$: $y=max(0,x-10)$. Для этой функции градиент и вход, умноженный на градиент (этот подход также может быть использован для объяснения предсказаний нейронных сетей), имеют разрыв в точке $x=10$. В отличие от этого подход, основанный на разнице с референсом, дает непрерывную величину оценки вклада признака."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/deeplift_relu.png\" width=\"800\">\n",
    "\n",
    "<center><p><em>Source: <a href=\"https://arxiv.org/pdf/1704.02685.pdf\">Learning Important Features Through Propagating Activation Differences</a></p> </em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Пример изображения (ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве референса конкретно в SHAP DeepExlainer, построенном на основе DeepLIFT, используется не один объект, а усреднение набора произвольных изображений из датасета ImageNet (`shap.datasets.imagenet50`).\n",
    "\n",
    "Попробуем воспользоваться этим на практике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q 'https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/cat_and_dog1.jpg' -O cat_and_dog1.jpg\n",
    "!wget -q 'https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/cat_and_dog2.png' -O cat_and_dog2.png\n",
    "!wget -q 'https://edunet.kea.su/repo/EduNet-web_dependencies/datasets/imagenet_class_index.json' -O imagenet_class_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_image(path):\n",
    "    with open(os.path.abspath(path), \"rb\") as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "img = get_image(\"cat_and_dog1.jpg\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# resize & normalize\n",
    "\n",
    "\n",
    "def get_input_transform():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "# for get croped img from input tensor\n",
    "\n",
    "\n",
    "def get_reverse_transform():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Normalize(\n",
    "                mean=(0.0, 0.0, 0.0), std=(1 / 0.229, 1 / 0.224, 1 / 0.225)\n",
    "            ),\n",
    "            transforms.Normalize(\n",
    "                mean=(-0.485, -0.456, -0.406),\n",
    "                std=(1.0, 1.0, 1.0),\n",
    "            ),\n",
    "            transforms.Lambda(lambda x: torch.permute(x, (0, 2, 3, 1))),\n",
    "            transforms.Lambda(lambda x: x.detach().numpy()),\n",
    "        ]\n",
    "    )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def get_input_tensors(img):\n",
    "    transform = get_input_transform()\n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transform(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "def get_crop_img(img_tensor):\n",
    "    transform = get_reverse_transform()\n",
    "    return transform(img_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "\n",
    "\n",
    "idx2label, cls2label, cls2idx = [], {}, {}\n",
    "with open(os.path.abspath(\"/content/imagenet_class_index.json\"), \"r\") as read_file:\n",
    "    class_idx = json.load(read_file)\n",
    "    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "    lable2idx = {class_idx[str(k)][1]: k for k in range(len(class_idx))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "img_t = get_input_tensors(img)\n",
    "model.eval()\n",
    "model.cpu()\n",
    "logits = model(img_t)\n",
    "\n",
    "probs = F.softmax(logits, dim=1)\n",
    "probs5 = probs.topk(5)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.imshow(get_crop_img(img_t))\n",
    "plt.show()\n",
    "tuple(\n",
    "    (p, c, idx2label[c])\n",
    "    for p, c in zip(probs5[0][0].detach().numpy(), probs5[1][0].detach().numpy())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на изображения, выбранные в качестве референса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "imagenet_50, broken_targets = shap.datasets.imagenet50()\n",
    "\n",
    "print(\"Data shape:\", imagenet_50.shape, \", type: \", type(imagenet_50))\n",
    "# Show first image\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.imshow(imagenet_50[0].astype(\"int\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно взглянуть и на остальные картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=10, figsize=(25, 10))\n",
    "for i, imgs in enumerate(imagenet_50):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    ax[col, row].imshow(imgs.astype(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for performance reason use as background only 10 images in PyTorch format\n",
    "transform = transforms.Normalize(\n",
    "    mean=(0.485, 0.456, 0.406),\n",
    "    std=(0.229, 0.224, 0.225),\n",
    ")\n",
    "background = transform(\n",
    "    torch.tensor(imagenet_50[0:10]).permute(0, 3, 1, 2).to(device) / 255\n",
    ")\n",
    "\n",
    "# https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html\n",
    "explainer = shap.DeepExplainer(model.to(device), background)\n",
    "shap_values = explainer.shap_values(img_t)  # List\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию возвращаются shap индексы для каждого класса, для каждого пикселя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes\", len(shap_values))\n",
    "print(\"Values\", shap_values[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отображения результатов используем метод [shap.image_plot](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.image_plot.html?highlight=image_plot).\n",
    "\n",
    "Его API ждет данные в виде списков numpy-массивов, поэтому нам потребуется преобразовать данные.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = get_image(\"cat_and_dog1.jpg\")\n",
    "# Get indexes of top5 classes predicted by model\n",
    "top5_indexes = probs5.indices.squeeze(0).numpy().astype(\"int\")\n",
    "\n",
    "# Get shap values for this classes\n",
    "shap_values_for_top_results = np.array(shap_values)[top5_indexes]\n",
    "\n",
    "shap_list_numpy = []  # list of np.array\n",
    "\n",
    "for v in shap_values_for_top_results:\n",
    "    # because image_plot accept data in form(#samples x width x height x channels)\n",
    "    # move cannel to last dimension [10, 1, 28, 28] -> (10,  28, 28, 1)\n",
    "    shap_list_numpy.append(np.moveaxis(v, (1), (3)))\n",
    "\n",
    "# Prepare test image\n",
    "test_image = get_crop_img(img_t).reshape((1, 224, 224, 3))\n",
    "\n",
    "# Get labels for top5 classes\n",
    "\n",
    "shap_labels = np.array(idx2label)[top5_indexes]\n",
    "shap_labels = [list(shap_labels)]  # One list for sample\n",
    "\n",
    "print(\n",
    "    \"Len of shap_values list\", len(shap_values_for_top_results)\n",
    ")  # number of classes to explain\n",
    "print(\"Shape of one value\", shap_values_for_top_results[0].shape)  # n_samples, H,W,C\n",
    "print(shap_labels)  # n_samples, number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь визуализируем результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_list_numpy, test_image, labels=shap_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике чаще всего интересны объяснения только для нескольких классов с максимальной уверенностью.\n",
    "\n",
    "И в документации описан параметр, который позволяет возвращать объяснения только для этих классов.\n",
    "\n",
    "[[doc] shap.DeepExplainer](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.DeepExplainer.html)\n",
    "\n",
    "`ranked_outputs = 5, output_rank_order ='max'`\n",
    "\n",
    "При этом возвращается кортеж:\n",
    "shap_values и индексы классов, для которых получено объяснение.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_values_for_best_pred, indexes = explainer.shap_values(\n",
    "    img_t, ranked_outputs=5, output_rank_order=\"max\"\n",
    ")  # List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты снова надо преобразовать из PyTorch формата:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_list_numpy = []  # list of np.array\n",
    "\n",
    "for v in shape_values_for_best_pred:\n",
    "    # because image_plot accept data in form(#samples x width x height x channels)\n",
    "    # move cannel to last dimension [10, 1, 28, 28] -> (10,  28, 28, 1)\n",
    "    shap_list_numpy.append(np.moveaxis(v, (1), (3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.image_plot(shap_list_numpy, test_image, labels=shap_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Развитием **Gradient Ascent** для сверточных нейронных сетей **CNN** является метод **Grad-CAM** (Class activation maps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/12/GradCAM-architecture.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Идея метода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После каждого **сверточного слоя** нейронной сети мы получаем **карты признаков**, сохраняющие информацию о **расположении объектов** на исходном изображении. При этом все значения признаков для одного канала получаются одним и тем же преобразованием исходного изображения (получаются применением одинаковых сверток с одинаковыми весами), то есть **один канал** — это **карта одного признака**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Посмотрим на карты признаков для **ResNet18**. Для этого загрузим модель вместе с весами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам интересны сложные признаки, которые выделяются на последних сверточных слоях. ResNet18 был обучен на ImageNet с размерами входного изображения $224\\times224$. Посмотрим на размеры на выходе последнего сверточного слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.to(\"cpu\"), (3, 224, 224), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На последнем сверточном слое получаем $512$ каналов $7\\times7$. Для их сохранения напишем hook, в котором будем сохранять значения активации на выходе модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_forward_hook(history_dict, key):\n",
    "    def forward_hook(self, input_, output):\n",
    "        history_dict[key] = output.detach().clone()\n",
    "\n",
    "    return forward_hook\n",
    "\n",
    "\n",
    "def register_model_hooks(model):\n",
    "    hooks_data_history = defaultdict(list)\n",
    "    forward_hook = get_forward_hook(hooks_data_history, \"feature_map\")\n",
    "    model._modules[\"layer4\"].register_forward_hook(forward_hook)\n",
    "    return hooks_data_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработаем картинку: приведем к размеру $224\\times224$ и нормализуем изображение в соответствии со статистикой ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = get_input_tensors(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропустим картинку через сеть и сохраним значения активаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "history = register_model_hooks(model)\n",
    "output = model(img_t)\n",
    "\n",
    "print(history[\"feature_map\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарисуем первые 6 карт признаков. Чтобы растянуть карты по размеру изображения, используем `extent` и `interpolation='bilinear'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axs = axs.ravel()\n",
    "for i in range(6):\n",
    "    axs[i].imshow(img_arr)\n",
    "    axs[i].imshow(\n",
    "        history[\"feature_map\"][0][i],\n",
    "        alpha=0.6,\n",
    "        extent=(0, 224, 224, 0),\n",
    "        interpolation=\"nearest\",\n",
    "        cmap=\"jet\",\n",
    "    )\n",
    "    axs[i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним предсказание модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "number_of_top_classes = 6\n",
    "\n",
    "prediction = F.softmax(output, dim=1)\n",
    "top_props, top_inds = prediction.topk(number_of_top_classes)\n",
    "\n",
    "\n",
    "for i in range(number_of_top_classes):\n",
    "    category_name = idx2label[top_inds[0][i].item()]\n",
    "    score = top_props[0][i].item()\n",
    "    print(f\"{category_name} {top_inds[0][i].item()}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы смогли понять, к какой части изображения относятся те или иные признаки. Теперь попробуем понять, как они соотносятся с классом. Мы помним, что градиент указывает направление возрастания функции. Если мы выберем логит, соответствующий метке класса, и посчитаем для него градиент, то мы сможем увидеть, какие признаки имеют положительные значения (при их увеличении модель будет больше уверена в оценке)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем хук для сохранения значения градиента. Мы смотрим значения градиента на выходе слоя перед `AdaptiveAvgPool2d`, поэтому сохраним только средние значения (значение градиента для карт признаков одного канала будет одинаковым)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backward_hook(history_dict, key):\n",
    "    def backward_hook(self, grad_input_, grad_output):  # for tensors\n",
    "        history_dict[key] = (\n",
    "            grad_output[0].detach().clone().mean(dim=[2, 3], keepdim=True)\n",
    "        )\n",
    "\n",
    "    return backward_hook\n",
    "\n",
    "\n",
    "def register_model_hooks(model):\n",
    "    hooks_data_history = defaultdict(list)\n",
    "\n",
    "    forward_hook = get_forward_hook(hooks_data_history, \"feature_map\")\n",
    "    model._modules[\"layer4\"].register_forward_hook(forward_hook)\n",
    "\n",
    "    backward_hook = get_backward_hook(hooks_data_history, \"weight\")\n",
    "    model._modules[\"layer4\"].register_full_backward_hook(backward_hook)\n",
    "    return hooks_data_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итоговая формула **Grad-CAM** (Class activation maps):\n",
    "\n",
    "$$\\large CAM = ReLU(\\sum_{i=1}^{Nch}w_iA_i)$$\n",
    "\n",
    "где $A_i$ — каналы карты признаков, $w_i$ — веса, полученные пропусканием градиента по логиту, соответствующему метке класса. $ReLU$ используется потому, что нам интересны только положительно влияющие на метку класса признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, рассчитывающая CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam_map(model, img, class_num):\n",
    "    history = register_model_hooks(model)\n",
    "\n",
    "    output = model.eval()(img)\n",
    "    activation = history[\"feature_map\"]\n",
    "\n",
    "    output[0, class_num].backward()\n",
    "    weight = history[\"weight\"]\n",
    "\n",
    "    cam_map = F.relu((weight[0] * activation[0]).sum(0)).detach().cpu()\n",
    "    return cam_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация важности признаков для top6 классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "fif, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axs = axs.ravel()\n",
    "for i in range(6):\n",
    "    cam_map = get_cam_map(model, img_t, top_inds[0][i])\n",
    "\n",
    "    axs[i].imshow(img_arr)\n",
    "    axs[i].imshow(\n",
    "        cam_map,\n",
    "        alpha=0.6,\n",
    "        extent=(0, 224, 224, 0),\n",
    "        interpolation=\"nearest\",\n",
    "        cmap=\"jet\",\n",
    "    )\n",
    "    axs[i].set_title(idx2label[top_inds[0][i].item()])\n",
    "    axs[i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "fif, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axs = axs.ravel()\n",
    "for i in range(6):\n",
    "    cam_map = get_cam_map(model, img_t, top_inds[0][i])\n",
    "\n",
    "    axs[i].imshow(img_arr)\n",
    "    axs[i].imshow(\n",
    "        cam_map,\n",
    "        alpha=0.6,\n",
    "        extent=(0, 224, 224, 0),\n",
    "        interpolation=\"bilinear\",\n",
    "        cmap=\"jet\",\n",
    "    )\n",
    "    axs[i].set_title(idx2label[top_inds[0][i].item()])\n",
    "    axs[i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Пример изображения (ResNet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сделать все то же самое с помощью библиотеки  Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "target_layers = [model._modules[\"layer4\"]]\n",
    "\n",
    "cam = GradCAM(model=model, target_layers=target_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axs = axs.ravel()\n",
    "for i in range(6):\n",
    "    target = [ClassifierOutputTarget(top_inds[0][i])]\n",
    "    cam_map = cam(input_tensor=img_t, targets=target)\n",
    "\n",
    "    axs[i].imshow(img_arr)\n",
    "    axs[i].imshow(cam_map[0], alpha=0.6, interpolation=\"bilinear\", cmap=\"jet\")\n",
    "    axs[i].set_title(idx2label[top_inds[0][i].item()])\n",
    "    axs[i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке также реализованы другие методы визуализации, например, Grad-CAM++, использующий градиенты второго порядка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAMPlusPlus\n",
    "\n",
    "target_layers = [model._modules[\"layer4\"]]\n",
    "cam_plus = GradCAMPlusPlus(model=model, target_layers=target_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axs = axs.ravel()\n",
    "for i in range(6):\n",
    "    target = [ClassifierOutputTarget(top_inds[0][i])]\n",
    "    cam_map = cam_plus(input_tensor=img_t, targets=target)\n",
    "\n",
    "    axs[i].imshow(img_arr)\n",
    "    axs[i].imshow(cam_map[0], alpha=0.6, interpolation=\"bilinear\", cmap=\"jet\")\n",
    "    axs[i].set_title(idx2label[top_inds[0][i].item()])\n",
    "    axs[i].axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad-CAM++ для ряда изображений работает лучше Grad-CAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/Grad_CAM_plus_plus.png\" alt=\"alttext\" width=\"600\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1710.11063.pdf\">Grad-CAM++: Improved Visual Explanations for\n",
    "Deep Convolutional Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее о других методах читайте в [документации Grad-Cam](https://jacobgil.github.io/pytorch-gradcam-book/introduction.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критика градиентных методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентные методы имеют существенное преимущество перед SHAP и LIME: они вычисляются быстрее, чем методы, не зависящие от моделей. Но у них есть ряд недостатков. Недостаток в том, что в статьях, где предлагались такие методы, **качество работы оценивалось визуально** “на глаз”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошим примером проблемы визуальной оценки является метод **Guided Backpropagation**, описанный в [статье](https://arxiv.org/pdf/1412.6806.pdf). Объяснение метода и код можно найти по [ссылке](https://leslietj.github.io/2020/07/22/Deep-Learning-Guided-BackPropagation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/Guided_Backpropagation.png\" alt=\"alttext\" width=\"600\"/></center>\n",
    "\n",
    "<center><em>Картинка получена применением модели ResNet18 и Guided Backpropagation</em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуитивно нам кажется, что **Guided Backpropagation** дает хорошее объяснение работы модели, потому что она выделяет границы, по которым легко узнать объект. В статье [Sanity Checks for Saliency Maps](https://arxiv.org/pdf/1810.03292.pdf) показано, что данный метод инвариантен к рандомизации весов верхних слоев модели, что ставит под сомнения его способности объяснения работы модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/guided_backprop_demo.png\" alt=\"alttext\" width=\"900\"/></center>\n",
    "\n",
    "<center><em>Показан результат Guided Backpropagation  при рандомизации слоев в модели Inception v3, начиная с последнего.\n",
    "\n",
    "Source: <a href=\"https://arxiv.org/pdf/1810.03292.pdf\">Sanity Checks for Saliency Maps.</a></em></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье [Interpretation of Neural Networks is Fragile](https://arxiv.org/abs/1710.10547) было показано, что небольшое случайное возмущение, добавленное к картинке, не влияющее на результат предсказания, может существенно поменять карты важности, что является спорной критикой, потому что на результат работы нейросети такое возмущение тоже может повлиять:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/criticism_grad_1.png\" alt=\"alttext\" width=\"500\"/></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/1710.10547\">Interpretation of Neural Networks is Fragile</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом [остается проблема](https://arxiv.org/pdf/1912.01451.pdf) оценки качества Saliency Maps. Это не значит, что градиентные методы нельзя использовать. Это значит, что нужно использовать их с осторожностью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы, специфичные для трансформеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В трансформерах есть механизм **self-attention**, который кажется естественным способом определить, какие токены текста/кусочки изображения имеют большую важность для предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/self_attention.png\"  width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это бы отлично работало, будь у нас только один блок **self-attention**, но архитектура трансформера состоит из **нескольких блоков кодера/декодера**, составленных друг за другом. От слоя к слою за счет того же **self-attention** **информация в эмбеддингах перемешивается** сильнее и сильнее. Так, например, для классификации текста мы можем использовать эмбеддинг на выходе **BERT**, соответствующий `[CLS]` токену , в котором на входе **BERT** не было никакой информации о последующем тексте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/transformer_text_translation_example.png\" width=\"800\">\n",
    "\n",
    "<em>Source: [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, в стандартном блоке трансформера есть **residual соединения**. Из-за этого информация о токенах/патчах проходит не только через **attention**, но и через **residual соединения**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/transformer_architecture.png\" width=\"450\">\n",
    "\n",
    "<em>Архитектура трансформера</em>\n",
    "\n",
    "<em>Source: <a href=\"https://arxiv.org/pdf/1706.03762.pdf\"> Attention Is All You Need</a></em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этим причинам объяснение работы трансформера через attention — непростая задача.\n",
    "\n",
    "**Предупреждение:** методы объяснения **attention** только частично объясняют работу трансформера, они  разнообразны и могут давать противоречивые результаты.\n",
    "\n",
    "Давайте для начала немного поизучаем, как выглядят значения self-attention в трансформерах. Подгружаем библиотеку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем базовый русскоязычный разговорный [BERT от Deep Pavlov](https://huggingface.co/DeepPavlov/rubert-base-cased-conversational), [обученный](https://huggingface.co/blanchefort/rubert-base-cased-sentiment) для определения настроения коротких русских текстов. Загружаем токенизатор и модель. Ставим флаг `output_attentions=True`, чтобы модель возвращала значения attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizerFast, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"blanchefort/rubert-base-cased-sentiment\",\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"blanchefort/rubert-base-cased-sentiment\",\n",
    "    output_attentions=True,  # for save attention\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовим предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Мама мыла раму\",\n",
    "    \"Фильм сделан откровенно плохо\",\n",
    "    \"Максимально скучный сериал, где сюжет высосан из пальца\",\n",
    "    \"Я был в восторге\",\n",
    "    \"В общем, кино хорошее и есть много что пообсуждать\",\n",
    "]\n",
    "\n",
    "tokens = [\n",
    "    [\"[cls]\"] + tokenizer.tokenize(sentence) + [\"[sep]\"] for sentence in sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как разбивается на токены предложение. На выходе токенизатора номера токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "print(f\"Tokens: {tokens[item]}\")\n",
    "token_ids = [tokenizer.encode(sentence) for sentence in sentences]\n",
    "print(f\"Token ids: {token_ids[item]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на предсказания модели, чтобы проверить, насколько она адекватна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "ans = {0: \"NEUTRAL\", 1: \"POSITIVE\", 2: \"NEGATIVE\"}\n",
    "\n",
    "for item in range(5):\n",
    "    input_ids = torch.tensor([token_ids[item]])\n",
    "    model_output = model(input_ids)\n",
    "    predicted = torch.argmax(model_output.logits, dim=1).numpy()\n",
    "    print(f\"Text: {sentences[item]}\")\n",
    "    print(f\"Predict lable = {predicted}, {ans[predicted.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной моделе 12 слоев (блоков трансформеров), поэтому модель возвращает кортеж из 12 тензоров. Каждый слой имеет 12 голов self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 1\n",
    "input_ids = torch.tensor([token_ids[item]])\n",
    "model_output = model(input_ids)\n",
    "\n",
    "attentions = model_output.attentions\n",
    "print(f\"Text: {sentences[item]}\")\n",
    "print(f\"Tokens: {tokens[item]}\")\n",
    "print(f\"Number of layers: {len(attentions)}\")\n",
    "print(\n",
    "    f\"Attention size: {attentions[0].shape} \"\n",
    "    \"[batch x attention_heads x seq_size x seq_size]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем в однородный массив для удобства манипуляций.\n",
    "\n",
    "Код этой части лекции основан на [репозитории](https://github.com/samiraabnar/attention_flow).\n",
    "[Cтатья](https://arxiv.org/pdf/2005.00928.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_array(attentions):\n",
    "    attentions_arr = [attention.detach().numpy() for attention in attentions]\n",
    "    return np.asarray(attentions_arr)[:, 0]\n",
    "\n",
    "\n",
    "attentions_arr = to_array(attentions)\n",
    "print(\n",
    "    f\"Shape: {attentions_arr.shape} \" \"[layers x attention_heads x seq_size x seq_size]\"\n",
    ")\n",
    "print(f\"Type: {type(attentions_arr)}, Dtype: {attentions_arr.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на **нулевую голову нулевого слоя**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_ticks = tokens[item]\n",
    "y_ticks = tokens[item]\n",
    "\n",
    "sns.heatmap(\n",
    "    attentions_arr[0][0],\n",
    "    xticklabels=x_ticks,\n",
    "    yticklabels=y_ticks,\n",
    "    cmap=\"YlOrRd\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут по **оси x** — токены, **на** которые смотрит внимание, по **оси y** — токены, **куда** записывается результат прохождения слоя.\n",
    "\n",
    "Так для слова “плохо” первая голова внимания первого слоя больше всего смотрит на слова “фильм” и “ откровенно”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации внимания можно использовать библиотеку [bertviz](https://pypi.org/project/bertviz/) ([статья](https://arxiv.org/pdf/1904.02679.pdf),\n",
    "[код](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import bertviz\n",
    "except ModuleNotFoundError:\n",
    "    !pip install -q bertviz\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут Layer — это выбор слоя, цвета — головы self-attention, слева — куда записывается, справа — на какие токены смотрит. Яркость соединяющих линий — величина attention (чем ярче, тем больше). Картину, аналогичную картине выше, можно получить, *дважды щелкнув на первый синий квадрат*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bertviz\n",
    "from bertviz import head_view\n",
    "\n",
    "head_view(model_output.attentions, tokens[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усредним значения по головам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_sum(attention_arr):\n",
    "    return attentions_arr.sum(axis=1) / attentions_arr.shape[1]\n",
    "\n",
    "\n",
    "attention_head_sum = head_sum(attentions_arr)\n",
    "print(f\"{attention_head_sum.shape} [layers x seq_size x seq_size]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на усредненное по головам внимание **на первом слое**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = tokens[item]\n",
    "y_ticks = tokens[item]\n",
    "\n",
    "sns.heatmap(\n",
    "    attention_head_sum[0],\n",
    "    xticklabels=x_ticks,\n",
    "    yticklabels=y_ticks,\n",
    "    cmap=\"YlOrRd\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И на **последнем слое**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = tokens[item]\n",
    "y_ticks = tokens[item]\n",
    "\n",
    "sns.heatmap(\n",
    "    attention_head_sum[11],\n",
    "    xticklabels=x_ticks,\n",
    "    yticklabels=y_ticks,\n",
    "    cmap=\"YlOrRd\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По значениям справа от графика видно, что внимание на последнем слое изменяется в **узком диапазоне значений**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на **значения внимания для записываемого в эмбеддинг [CLS] токена** (эмбеддинг с него используется для классификации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = tokens[item]\n",
    "y_ticks = [i for i in range(12, 0, -1)]\n",
    "\n",
    "sns.heatmap(\n",
    "    np.flip(attention_head_sum[:, 0, :], axis=0),\n",
    "    xticklabels=x_ticks,\n",
    "    yticklabels=y_ticks,\n",
    "    cmap=\"YlOrRd\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что после 6-го слоя значения внимания выравниваются. Это связано с тем, что механизм **self-attention** смешивает информацию о токенах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Residual connection</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сначала определимся, что делать с **residual соединениями**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://edunet.kea.su/repo/EduNet-content/dev-2.0/L14/out/transformer_architecture.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual соединение** можно записать как\n",
    "\n",
    "$$ \\large V_{l+1} = V_l + W_{att}V_l,$$\n",
    "\n",
    "где $W_{att}$ — матрица внимания, а $V_l$ — эмбеддинги.\n",
    "\n",
    "После нормализации это можно переписать как\n",
    "\n",
    "$$\\large A=0.5W{att}+0.5I,$$\n",
    "\n",
    "где $I$ — единичная матрица.\n",
    "\n",
    "[Подробнее](https://arxiv.org/pdf/2005.00928.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(attention_head_sum):\n",
    "    attention_head_sum += np.eye(attention_head_sum.shape[1])[None, ...]\n",
    "    return attention_head_sum / attention_head_sum.sum(axis=-1)[..., None]\n",
    "\n",
    "\n",
    "attention_res = residual(attention_head_sum)\n",
    "\n",
    "x_ticks = tokens[item][1:-1]\n",
    "y_ticks = [i for i in range(12, 0, -1)]\n",
    "\n",
    "sns.heatmap(\n",
    "    np.flip(attention_res[:, 0, 1:-1], axis=0),\n",
    "    xticklabels=x_ticks,\n",
    "    yticklabels=y_ticks,\n",
    "    cmap=\"YlOrRd\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention rollout\n",
    "“Разворачивание внимания” (**Attention rollout**) — предложенный в [статье](https://arxiv.org/pdf/2005.00928.pdf) способ отслеживания информации, распространяемой от входного к выходному блоку, в котором значение внимания рассматривается как доля пропускаемой информации.  Доли информации перемножаются и суммируются. Итоговая формула — рекурсивное матричное перемножение.\n",
    "\n",
    "\\begin{align}\n",
    "\\widetilde{A}(l_i) = \\left\\{\n",
    "\\begin{array}{cl}\n",
    "A(l_i)\\widetilde{A}(l_{i-1}) & i>0 \\\\\n",
    "A(l_i) & i = 0.\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(attention_res):\n",
    "    rollout_attention = np.zeros(attention_res.shape)\n",
    "    rollout_attention[0] = attention_res[0]\n",
    "    n_layers = attention_res.shape[0]\n",
    "    for i in range(1, n_layers):\n",
    "        rollout_attention[i] = attention_res[i].dot(rollout_attention[i - 1])\n",
    "    return rollout_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_attention = rollout(attention_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = tokens[item][1:-1]\n",
    "y_ticks = [i for i in range(12, 0, -1)]\n",
    "\n",
    "sns.heatmap(\n",
    "    np.flip(rollout_attention[:, 0, 1:-1], axis=0),\n",
    "    xticklabels=x_ticks,\n",
    "    yticklabels=y_ticks,\n",
    "    cmap=\"YlOrRd\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализацию для ViT и картинок можно найти [тут](https://huggingface.co/spaces/probing-vits/attention-rollout/tree/main) (API упало, но код рабочий, проверено).\n",
    "\n",
    "Реализация на PyTorch rollout для BERT и некоторых других методов: [API](https://huggingface.co/spaces/amsterdamNLP/attention-rollout), [код](https://huggingface.co/spaces/amsterdamNLP/attention-rollout/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другим вариантом рассмотрения распространения внимания является **attention flow**. В нем трансформер представляют в виде направленного графа, **узлы** которого представляют собой **эмбеддинги** между слоями, а **ребра** — связи в виде **attention** с ограниченной емкостью (передающей способностью)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet-web_dependencies/dev-2.0/L14/attention_flow.png\" width=\"700\">\n",
    "\n",
    "<em>Source: <a href=\"https://github.com/samiraabnar/attention_flow/blob/master/bert_example.ipynb\">Bert Example\n",
    "</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В такой постановке задача нахождения роли токенов/частей изображения в результирующем эмбеддинге сводится к [задаче о максимальном потоке](https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B0_%D0%BE_%D0%BC%D0%B0%D0%BA%D1%81%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D0%BC_%D0%BF%D0%BE%D1%82%D0%BE%D0%BA%D0%B5) (задача нахождения такого потока по транспортной сети, что сумма потоков в пункт назначения была максимальна). Это известная алгоритмическая задача, которую мы не будем разбирать в рамках этого курса.\n",
    "\n",
    "[Код](https://github.com/samiraabnar/attention_flow), [статья](https://arxiv.org/pdf/2005.00928.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-weighted Attention Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод, объединяющий GradCam и Attention Rollout, позволяет оценить положительный и отрицательный вклад токенов / частей изображения в итоговый результат. Предложен в [статье](https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf), реализацию можно найти тут: [API](https://huggingface.co/spaces/amsterdamNLP/attention-rollout), [код](https://huggingface.co/spaces/amsterdamNLP/attention-rollout/tree/main).\n",
    "\n",
    "Методы, основанные на механизме внимания, только частично объясняют работу трансформера и на сегодняшний день не являются надежным методом оценки важности признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В ходе урока мы убедились в важности интерпретации работы моделей Искусственного Интеллекта.\n",
    "* Рассмотрели основные методы, которые используются для оценки важности признаков: мы начали с классических методов, посмотрели на  Model-Agnostic методы, градиентные методы, методы, специфичные для трансформеров.\n",
    "* Рассмотрели применение библиотек на примерах:\n",
    "    - Табличные данные\n",
    "    - NLP (машинный перевод текста, создание резюме статьи и классификации текстов)\n",
    "    - CV\n",
    "\n",
    "Пренебрежение объяснением того, почему модель дала тот или иной результат, ведет к недоверию не только к самой модели, но и к конкретным прогнозам. А, следовательно, является существенным препятствием для дальнейшего введения Вашей идеи в production.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"6\">Список литературы</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[git] DEN](https://github.com/isaacrob/DEN): метод, основанный на новом режиме обучения сиамской нейронной сети без учителя и функции потерь, который называется Differentiating EmbeddingNetworks (DEN).\n",
    "\n",
    "Сиамская нейронная сеть находит отличительные или похожие черты между конкретными парами образцов в наборе данных и использует эти функции, чтобы встроить набор данных в пространство более низкой размерности, где он может быть визуализирован.\n",
    "\n",
    "В отличие от существующих алгоритмов визуализации, таких как UMAP или t-SNE, DEN является параметрическим, то есть его можно интерпретировать такими методами, как SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Книга:</font>\n",
    "\n",
    "[A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)\n",
    "\n",
    "<font size=\"5\">Статьи:</font>\n",
    "\n",
    "[How to Interpret Machine Learning Models with SHAP](https://www.youtube.com/watch?v=m60swo-th4E)\n",
    "\n",
    "[Бесплатный курс от Kaggle: Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "\n",
    "[EXPLAINABLE AI IN CREDIT RISK MANAGEMENT](https://arxiv.org/pdf/2103.00949v1.pdf)\n",
    "\n",
    "[Predicting Driver Fatigue in Automated Driving with Explainability](https://arxiv.org/pdf/2103.02162v1.pdf)\n",
    "\n",
    "[Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods](https://arxiv.org/pdf/1911.02508v2.pdf)\n",
    "\n",
    "[Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/intro.html)\n",
    "\n",
    "<font size=\"5\">SHAP</font>\n",
    "\n",
    "[Welcome to the SHAP documentation](https://shap.readthedocs.io/en/stable/index.html)\n",
    "\n",
    "[Git](https://github.com/slundberg/shap)\n",
    "\n",
    "[A Unified Approach to Interpreting Model Predictions](https://arxiv.org/pdf/1705.07874v2.pdf)\n",
    "\n",
    "[SHAP (SHapley Additive exPlanations)](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "\n",
    "<font size=\"5\">LIME</font>\n",
    "\n",
    "[“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938.pdf)\n",
    "\n",
    "[What does LIME really see in images?](https://arxiv.org/pdf/2102.06307v1.pdf)\n",
    "\n",
    "[Git](https://github.com/marcotcr/lime)\n",
    "\n",
    "<font size=\"5\">BORUTA</font>\n",
    "\n",
    "[Feature Selection with the Boruta Package](https://www.researchgate.net/publication/280138095_Feature_Selection_with_Boruta_Package)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
