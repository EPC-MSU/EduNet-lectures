{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение на реальных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С какими проблемами чаще всего сталкиваются люди, работающие с машинным обучением, в реальной жизни?\n",
    "- нехватка данных\n",
    "- недостаток размеченных данных\n",
    "- не качественная разметка\n",
    "- низкое качество изображений\n",
    "- не сбалансированность датасета\n",
    "- переобучение\n",
    "- **какие еще?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как решить проблему малого количества данных?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ResNet (популярная архитектура для классификации изображений), заняла 1-е место в классификационном конкурсе ILSVRC 2015 с улучшением примерно на 50% по сравнению с предыдущим уровнем развития техники. ResNet не только имела очень сложную и глубокую архитектуру, но и была обучена на 1,2 млн изображений.\n",
    "\n",
    "Как в отрасли, так и в академических кругах хорошо известно, что при достаточно большом количестве данных, очень разные алгоритмы DL работают практически одинаково. Следует отметить, что большие данные должны содержать значимую информацию, а не просто шум, чтобы модель могла извлекать уроки из них. -->\n",
    "[Блог-пост о том как решать проблему малого количества данных?](https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d)\n",
    "\n",
    "На рисунке показаны основные проблемы, с которыми приходится сталкиваться при работе с небольшими наборами данных, а также возможные подходы и методы их решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_1-1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Работа с несбалансированным датасетом\n",
    "\n",
    "Сначала, давайте рассмотрим методы работы с датасетами в которых классы представлены не равномерно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "x, y = make_classification(n_samples=10000, n_features=10, n_classes=2)\n",
    "sns.displot(y, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь его испортим (изменим соотношение класса `0` и класса `1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = np.random.choice(y[y == 1], size=np.int(0.1 * len(y[y == 1])))\n",
    "y_new = np.hstack([y[y == 0], y_new])\n",
    "x_new = x[y_new]\n",
    "\n",
    "sns.displot(y_new, height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Что можно сделать, что бы как-то улучшить ситуацию используя методы ML:\n",
    "\n",
    "- **Измените функцию потерь**: для задач классификации мы часто используем кросс-энтропийную потерю и редко используем среднюю абсолютную ошибку или среднеквадратичную ошибку для обучения и оптимизации нашей модели.\n",
    "\n",
    "- В случае несбалансированных данных, модель учится распознавать доминирующий класс чаще чем все остальные, соответственно, она выучивает статистическое распределение классов и считает что чем чаще она предсказывает этот класс - тем меньше она ошибается. Что бы как-то решить эту проблему - мы можем **добавить веса к потерям**, соответствующим различным классам, чтобы выровнять это смещение данных. Например, если у нас есть два класса в соотношении 4:1, мы можем применить веса в соотношении 1:4 к вычислению функции потерь, чтобы данные были сбалансированы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод помогает нам легко смягчить проблему несбалансированных данных и улучшить обобщение модели для разных классов. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", np.unique(y_new), y_new)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем указать явные веса классов в соответствии с нашими требованиями, дополнительные сведения смотри в документации Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Create decision tree classifer object\n",
    "# clf = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "\n",
    "# # Train model\n",
    "# model = clf.fit(x, y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один способ решение проблемы несбалансированных данных - собственно их балансирование. Это может быть сделано либо за счет увеличения частоты класса меньшинства (_oversampling_), либо за счет уменьшения частоты класса большинства (_undersampling_) с помощью методов случайной или кластерной выборки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно _oversampling_ предпочтителен, когда общий размер данных небольшой, а _undersampling_ полезен, когда у нас есть большой объем данных. Точно так же случайная или кластерная выборка определяется тем, насколько хорошо распределены данные.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_0.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот тут можно прочитать подробнее: [Imbalanced Data : How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/)\n",
    "\n",
    "`Resampling` может быть легко выполнен с помощью пакета [imbalanced-learn](https://pypi.org/project/imbalanced-learn/), как показано ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(x_new, y_new)\n",
    "sns.displot(y_res, height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация синтетических данных: \n",
    "\n",
    "хотя `oversampling` или `undersampling` помогает сбалансировать данные, дублирование данных увеличивает вероятность переобучения.\n",
    "\n",
    "Другой подход к решению этой проблемы - создание синтетических данных с помощью данных о классе меньшинств.\n",
    "\n",
    "**Synthetic Minority Over-sampling Technique (SMOTE)** или **Modified- SMOTE** - два таких метода, которые генерируют синтетические данные. Проще говоря, SMOTE берет точки данных класса меньшинства и создает новые точки данных, которые лежат между любыми двумя ближайшими точками данных, соединенными прямой линией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого алгоритм вычисляет расстояние между двумя точками данных в пространстве признаков, умножает расстояние на случайное число от 0 до 1 и помещает новую точку данных на этом новом расстоянии от одной из точек данных, используемых для определения расстояния. **Обратите внимание,** что количество ближайших соседей, учитываемых для генерации данных, также является гиперпараметром и может быть изменено в зависимости от требований."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_0-1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**M-SMOTE** - это модифицированная версия SMOTE, которая также принимает во внимание базовое распределение класса меньшинства.\n",
    "\n",
    "Алгоритм классифицирует образцы классов меньшинств на 3 отдельные группы - образцы безопасности / безопасности, образцы границ и образцы скрытого шума."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это делается путем вычисления расстояний между выборками класса меньшинства и выборками обучающих данных.\n",
    "\n",
    "В отличие от SMOTE, алгоритм случайным образом выбирает точку данных из k ближайших соседей для выборки безопасности, выбирает ближайшего соседа из выборок границ и ничего не делает для устранения скрытого шума.\n",
    "\n",
    "Вот тут можно прочитать подробнее: [SMOTE explained for noobs - Synthetic Minority Over-sampling TEchnique line by line](https://rikunert.com/SMOTE_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датасет и посмотрим на то какие в нем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=42)\n",
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y == label)[0]\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте сделаем oversampling класас меньшинства с помощью SMOTE и построим график преобразованного набора данных.\n",
    "\n",
    "Мы будем использовать реализацию `SMOTE` из библиотеки `imbalanced-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "# summarize the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "\trow_ix = where(y == label)[0]\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обнаружение аномалий / изменений\n",
    "\n",
    "В случае сильно несбалансированных наборов данных, таких как мошенничество или машинный сбой, стоит задуматься, могут ли такие примеры рассматриваться как аномалия (выброс) или нет. Если такое событие и впрямь может считаться аномальным, мы можем использовать такие модели, как `OneClassSVM`, методы кластеризации или методы обнаружения гауссовских аномалий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти методы требуют изменения способа мышления: мы будем рассматривать аномалии, как отдельный класс выбросов. Это может помочь нам найти новые способы разделения и классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обнаружение изменений похоже на обнаружение аномалий, за исключением того, что мы ищем изменение или отличие, а не аномалию. Это могут быть изменения в поведении пользователя, наблюдаемые по шаблонам использования или банковским транзакциям."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем пример обнаружения аномалий с помощью `OneClassSVM` из библиотеки Sk-Learn (там же, можно найти еще множеество различных алгоритмов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.datasets import make_blobs\n",
    "from numpy import quantile, where, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Создадим датасет\n",
    "x, _ = make_blobs(n_samples=1000, centers=1, cluster_std=1.1, center_box=(0, 0))\n",
    "plt.scatter(x[:, 0], x[:, 1], alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроим наш детектор аномалий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = OneClassSVM(kernel=\"rbf\", gamma=0.01, nu=0.03)\n",
    "print(svm)\n",
    "svm.fit(x)\n",
    "pred = svm.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем аномальные точки предсказанные детектором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anom_index = where(pred == -1)\n",
    "values = x[anom_index]\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1], alpha=0.25)\n",
    "plt.scatter(values[:, 0], values[:, 1], color=\"r\", marker=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Методы объединения (Ensembling Techniques): \n",
    "\n",
    "Помимо того, что бы делать что-то с данными, мы можем попробовать сделать что-то с самой моделью. \n",
    "Идея объединения нескольких слабых учеников / разных моделей показала отличные результаты при работе с несбалансированными наборами данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bagging` и `Boosting` методы также показали отличные результаты при решении различных задач, и их следует изучить вместе с методами, описанными выше, для получения лучших результатов.\n",
    "\n",
    "Вот тут подробнее: [Imbalanced Data : How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "\n",
    "Мы рассмотрели несколько наиболее часто используемых методов работы с несбалансированными данными для традиционных алгоритмов машинного обучения.\n",
    "\n",
    "Один или многие из вышеперечисленных методов могут быть хорошей отправной точкой в ​​зависимости от конкретной бизнес-задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Блог пост про 8 тактик борьбы с несбалансированными классами в наборе данных машинного обучения](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "\n",
    "1. **Определите, можете ли вы собрать больше данных?**\n",
    "2. **Попробуйте изменить метрику оценки модели.**\n",
    "    Accuracy не является показателем, который следует использовать при работе с несбалансированным набором данных. [Есть метрики, которые были разработаны для работы с несбалансированными классами.](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/#:~:text=Classification%20Accuracy%20is%20Not%20Enough%3A%20More%20Performance%20Measures%20You%20Can%20Use,-By%20Jason%20Brownlee&text=When%20you%20build%20a%20model,This%20is%20the%20classification%20accuracy)\n",
    "\n",
    "3. **Применить `Resampling`** (см. выше)\n",
    "4. **Попробовать создать синтетические образцы данных**\n",
    "5. **Не следует использовать свой любимый алгоритм для каждой задачи.**\n",
    "    Необходимо проверять различные типы алгоритмов для решения конкретной проблемы. Например, `Random Forest` часто хорошо работает с несбалансированными наборами данных.\n",
    "6. **Попробуйте модели с санкциями**\n",
    "    Штрафная классификация накладывает дополнительные штрафы на модель за ошибки классификации в классе меньшинства во время обучения. Эти штрафы могут склонить модель к уделению большего внимания классу меньшинства.\n",
    "7. **Попробуйте другую точку зрения**\n",
    "    Есть области исследований, посвященные несбалансированным наборам данных. Двe, которые вы могли бы рассмотреть, - это [обнаружение аномалий](https://en.wikipedia.org/wiki/Anomaly_detection) и [обнаружение изменений](https://en.wikipedia.org/wiki/Change_detection).\n",
    "8. **[Попробуйте проявить творческий подход](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)**.\n",
    "    По-настоящему залезьте внутрь своей проблемы и подумайте, как разбить ее на более мелкие проблемы, которые легче решить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перейдем к особенностям работы с несбалансированными классами в моделях нейронных сетей**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментация\n",
    "\n",
    "Сам термин пришел из музыки:\n",
    "\n",
    "**Аугмента́ция** (позднелат. augmentatio — увеличение, расширение) — [техника ритмической композиции в старинной музыке](https://ru.wikipedia.org/wiki/Аугментация).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели глубокого обучения обычно требуют большого количества данных для обучения. В целом, чем больше данных, тем лучше для обучения модели. В то же время получение огромных объемов данных сопряжено со своими проблемами (например с нехваткой размеченных данных или с трудозатратами сопряженными с разметкой)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо того, чтобы тратить дни на сбор данных вручную, мы можем использовать методы аугментации для автоматической генерации новых примеров из уже имеющихся."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо увеличения размеченных датасетов, многие методы *self-supervised learning* построены на использовании разных аугмеентаций одного и того же сэмпла.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для аугментации будем использовать мультимодальную (работающую с различными модальностями, как например текст, картинки, звук и тд) библиотеку с забавным названием `AugLy` (*созвучно с ugly - стремный*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ВНИМАНИЕ: ПОСЛЕ ЗАПУСКА ЭТОЙ ЯЧЕЙКИ ТРЕБУЕТСЯ ПЕРЕЗАПУСТИТЬ СРЕДУ ВЫПОЛНЕНИЯ (Среда выполнения -> перезапустить среду выполнения)\n",
    "!pip install -U augly\n",
    "!sudo apt-get install python3-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важный момент**: при обучении модели мы используем разбиение данных на `train-val-test`. Аугментации стоит применять только на `train`. Почему так? Конечная цель обучения нейросети - это применение на реальных данных, которые сеть не видела. Вот и портить их не надо.\n",
    "\n",
    "В любом случае, `test` должен быть отделен от данных еще до того как они попали в `DataLoader` или нейросеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое дело, что аугментации на тесте можно использовать как метод ансамблинга в случае классификации. Можно взять sample -> создать несколько его копий -> по разному их аугментировать -> предсказать класс на каждой из этих аугментированных копий -> а потом выбрать наиболее вероятный класс голосованием (такой функционал реализован например в [YOLOv5](https://github.com/ultralytics/yolov5/blob/d204a61834d0f6b2e73c1f43facf32fbadb6b284/models/yolo.py#L121), о которой реречь пойдет в следующих лекциях)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изображения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеку и отобразим пример картинки. Картинку отмасштабируем, что бы она не занимала весь экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import augly.image as imaugs\n",
    "import augly.utils as utils\n",
    "from IPython.display import display\n",
    "\n",
    "# Get input image, scale it down to avoid taking up the whole screen\n",
    "input_img_path = os.path.join(\n",
    "    utils.TEST_URI, \"image\", \"inputs\", \"dfdc_1.jpg\"\n",
    ")\n",
    "\n",
    "# We can use the AugLy scale augmentation\n",
    "input_img = imaugs.scale(input_img_path, factor=0.2)\n",
    "display(input_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций картинок. С полным списком можно ознакомиться на [сайте библиотеки](https://github.com/facebookresearch/AugLy/blob/main/augly/image/__init__.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие параметры принимает на вход `Random Rotation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? imaugs.RandomRotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим перменную `transform` в которую добавим нашу аугментацию и применим ее к исходному изображению. Затем запустим следующую ячейку несколько раз подряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = imaugs.RandomRotation(min_degrees=-45.0, # Минимальное пороговое значение поворота\n",
    "                                  max_degrees=45     # Максимальное пороговое значение поворота\n",
    "                                  )\n",
    "\n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание как `AugLy` делает `crop` - на выходе мы получаем только ту часть повернутого изображения, которая содержит собственно изображение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? imaugs.RandomBlur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = imaugs.RandomBlur(min_radius=0.1, # Минимальное пороговое значение размытия (чем больше радиус - тем \"размытие изображение\")\n",
    "                              max_radius=10   # Максимальное пороговое значение размытия\n",
    "                              )\n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aspect Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? imaugs.RandomAspectRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = imaugs.RandomAspectRatio(min_ratio=0.1, \n",
    "                                     max_ratio=3\n",
    "                                     ) \n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого будем использовать метод `Compose`. Нам нужно будет создать `list` со всеми аугментациями, которые будут применены последовательно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = imaugs.Compose(\n",
    "    [\n",
    "     imaugs.RandomPixelization(min_ratio=0.1,\n",
    "                               max_ratio=0.9),\n",
    "     imaugs.RandomEmojiOverlay(emoji_size=0.25, \n",
    "                               x_pos=0.43, \n",
    "                               y_pos=0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### А что если мы хотим применять аугментации случайным образом?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого воспользуемся методом `ApplyLambda`, который на вход принимает метод аугментации, и вероятность `p` что эта аугментация будет применена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? imaugs.ApplyLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этот раз мы добавим еще один элемент случайности - будем генерировать цвет фона и цвет текста случайным образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_bg_color=(np.random.randint(0,255), # Красный канал - случайные значения от 0 до 255\n",
    "               np.random.randint(0,255), # Зеленый канал\n",
    "               np.random.randint(0,255)) # Синий канал\n",
    "\n",
    "text_color = (255-meme_bg_color[0], # Найдем контрастный цвет\n",
    "              255-meme_bg_color[1], \n",
    "              255-meme_bg_color[2])\n",
    "\n",
    "transform_to_apply = imaugs.MemeFormat(caption_height=50, \n",
    "                                       meme_bg_color=meme_bg_color,\n",
    "                                       text_color=text_color)\n",
    "\n",
    "transform = imaugs.ApplyLambda(aug_function=transform_to_apply, p=0.5)\n",
    "\n",
    "aug_image = transform(input_img)\n",
    "display(aug_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Аугментация внутри `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем папку с картинками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = os.path.join(utils.TEST_URI, \"image\", \"inputs\")\n",
    "input_img_path = os.listdir(image_folder)\n",
    "input_img_path = [image_folder +'/' + x for x in input_img_path]\n",
    "print(len(input_img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "\n",
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, image_list, transforms=None):\n",
    "        self.image_list = image_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = imaugs.scale(self.image_list[i], factor=0.5)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return torch.tensor(img, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вспомогательную функцию для отображения картинок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `list` с аугментациями, которые мы хотим применить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = [\n",
    "    imaugs.RandomBrightness(),\n",
    "    imaugs.RandomEmojiOverlay(),\n",
    "    imaugs.RandomRotation(),\n",
    "    imaugs.Resize(128,128),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AugLy` кросс-фреймворковая библиотека, которая работает с `PIL` картинками. Что бы загрузить аугментации в `PyTorch`, нам необходимо эти картинки преобразовать в тензоры. Для этого воспользуемся родным торчевым преобразованием `transforms.ToTensor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "tensor_transforms = transforms.Compose(augmentations + [transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обернем все в `DataLoader` и отобразим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(input_img_path, tensor_transforms), batch_size=3\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров аугментаций аудио. С полным списком можно ознакомиться на [сайте библиотеки](https://github.com/facebookresearch/AugLy/blob/main/augly/image/__init__.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеку и посмотрим на пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import augly.audio as audaugs\n",
    "import augly.utils as utils\n",
    "from IPython.display import display, Audio\n",
    "from augly.audio.utils import validate_and_load_audio\n",
    "\n",
    "# Get input audio\n",
    "input_audio ='https://sampleswap.org/samples-ghost/VOCALS%20and%20SPOKEN%20WORD/NASA%20Space%20Shuttle%20Communications/433[kb]NASA-on-at-the-180.wav.mp3'\n",
    "\n",
    "display(Audio(input_audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим наш аудиоофайл в Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_audio_arr, sr = validate_and_load_audio(input_audio) #sr - sampling rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? audaugs.AddBackgroundNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = np.random.randint(1,10) # Случайное Signal-to-Noise ratio\n",
    "\n",
    "transform = audaugs.AddBackgroundNoise(snr_level_db=snr) \n",
    "aug_audio, aug_sr = transform(input_audio_arr, sample_rate=sr)\n",
    "\n",
    "display(Audio(aug_audio, rate=aug_sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним волновые картины и спектрограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "def produce_plots(input_audio_arr, aug_audio, sr):\n",
    "    f,t, Sxx_in = spectrogram(input_audio_arr, fs=sr) #Расчитаем спектрограмму для исходного сигнала (f - частота, t - время)\n",
    "    f,t, Sxx_aug = spectrogram(aug_audio, fs=sr)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20,5)) \n",
    "\n",
    "    ax[0,0].plot(input_audio_arr)\n",
    "    ax[0,0].set_xlim(0,len(input_audio_arr))\n",
    "    ax[0,0].set_xticks([])\n",
    "    ax[0,0].set_title('Исходное аудио')\n",
    "\n",
    "    ax[0,1].plot(aug_audio)\n",
    "    ax[0,1].set_xlim(0,len(input_audio_arr))\n",
    "    ax[0,1].set_xticks([])\n",
    "    ax[0,1].set_title('Аугментированное аудио')\n",
    "\n",
    "    ax[1,0].imshow(np.log(Sxx_in), \n",
    "                   extent = [t.min(), t.max(), f.min(), f.max()],\n",
    "                   aspect='auto',\n",
    "                   cmap='inferno')\n",
    "    ax[1,0].set_ylabel('Frequecny, Hz')\n",
    "    ax[1,0].set_xlabel('Time,s')\n",
    "    \n",
    "    ax[1,1].imshow(np.log(Sxx_aug), \n",
    "                   extent = [t.min(), t.max(), f.min(), f.max()],\n",
    "                   aspect='auto',\n",
    "                   cmap='inferno')\n",
    "    ax[1,1].set_ylabel('Frequecny, Hz')\n",
    "    ax[1,1].set_xlabel('Time,s')\n",
    "\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "produce_plots(input_audio_arr, np.mean(aug_audio, axis=0), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Stretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? audaugs.TimeStretch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stretch_factor = np.random.uniform(0.25, 0.75) # Случайный коэффициент растяжения\n",
    "\n",
    "transform = audaugs.TimeStretch(rate=stretch_factor) \n",
    "aug_audio, aug_sr = transform(input_audio_arr, sample_rate=sr)\n",
    "\n",
    "display(Audio(aug_audio, rate=aug_sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещаем несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае с картинками мы можем совмещать несколько аугментаций вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? audaugs.ChangeVolume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_between_clicks = np.random.uniform(0.5, 1)\n",
    "\n",
    "transform = audaugs.Compose(\n",
    "    [\n",
    "     audaugs.Clicks(seconds_between_clicks=time_between_clicks), # Добавим щелчки\n",
    "     audaugs.Reverb(), # и добавим ревебрацию\n",
    "    ]\n",
    ")\n",
    "\n",
    "aug_audio, aug_sr = transform(input_audio_arr, sample_rate=sr)\n",
    "\n",
    "display(Audio(aug_audio, rate=aug_sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(input_audio_arr, np.mean(aug_audio, axis=0), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что порядок имеет значение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_between_clicks = np.random.uniform(0.5, 1)\n",
    "\n",
    "transform = audaugs.Compose(\n",
    "    [\n",
    "     audaugs.Reverb(), # Сначала добавим ревебрацию\n",
    "     audaugs.Clicks(seconds_between_clicks=time_between_clicks), # А потом добавим щелчки\n",
    "    ]\n",
    ")\n",
    "\n",
    "aug_audio, aug_sr = transform(input_audio_arr, sample_rate=sr)\n",
    "\n",
    "display(Audio(aug_audio, rate=aug_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(input_audio_arr, np.mean(aug_audio, axis=0), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим несколько примеров аугментаций текста. С полным списком можно ознакомиться на [сайте библиотеки](https://github.com/facebookresearch/AugLy/blob/main/augly/text/__init__.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import augly.text as textaugs\n",
    "\n",
    "# Define input text\n",
    "input_text = \"Hello, future of AI for Science! How are you today?\"\n",
    "input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Иммитация опечаток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? textaugs.SimulateTypos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = textaugs.SimulateTypos()\n",
    "\n",
    "aug_text = transform(input_text)\n",
    "aug_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Замена букв на похожие символы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "? textaugs.ReplaceSimilarChars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = textaugs.ReplaceSimilarChars()\n",
    "\n",
    "aug_text = transform(input_text)\n",
    "aug_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И многое многое другое. А еще AugLy умеет работать с видео, но это выходит за рамки сегодняшнего обсуждения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме методов, реализованных в AugLy, существуют и специализированные библиотеки. \n",
    "\n",
    "Для аугментации изображений:\n",
    "- [torchvision](https://pytorch.org/vision/stable/transforms.html)\n",
    "- [lbumentations](https://albumentations.ai)\n",
    "- [imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "- [Kornia](https://kornia.readthedocs.io/en/latest/introduction.html)\n",
    "\n",
    "Для аугментации звука (и волновых функций в целом):\n",
    "- [torchaudio](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html)\n",
    "- [torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations)\n",
    "\n",
    "Для аугментации текста:\n",
    "- [TextAugment](https://github.com/dsfsi/textaugment)\n",
    "\n",
    "**При выборе методов аугментации имеет смысл использовать те, которые будут в реальной жизни. Например, нет смысла делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным, или отражать человека вверх-ногами, если мы не предполагаем его таким распознавать.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $\\color{brown}{\\text{Допольнительная информация}}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Примеры аугументации в [TORCHVISION.TRANSFORMS](https://pytorch.org/vision/stable/transforms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img/L11_13.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "# скачаем необходимые файлы\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=11LBkRRlagM4ijHsXkytgSae4NJtW98sB' -O data.zip\n",
    "with ZipFile('data.zip', 'r') as folder: # Create a ZipFile Object and load sample.zip in it\n",
    "    folder.extractall() # Extract all the contents of zip file in current directory\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/content/for_transforms.Compose\")\n",
    "img_list = os.listdir()\n",
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationDataset(Dataset):\n",
    "    def __init__(self, image_list, transforms=None):\n",
    "        self.image_list = image_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = plt.imread(self.image_list[i])\n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "        img = np.array(img).astype(np.uint8)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return torch.tensor(img, dtype=torch.float)\n",
    "\n",
    "\n",
    "def show_img(img):\n",
    "    plt.figure(figsize=(40, 38))\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько методов аугментации изображений, и мы обсудим некоторые из наиболее распространенных и широко используемых.\n",
    "\n",
    "Вы можете попробовать другие методы в соответствии с вашими требованиями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Image Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.RandomRotation(50, expand=True),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.RandomCrop((120, 120)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.GaussianBlur(7, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((164, 164)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Augmentation_dataloader = DataLoader(\n",
    "    AugmentationDataset(img_list, transform), batch_size=8, shuffle=True\n",
    ")\n",
    "\n",
    "data = iter(Augmentation_dataloader)\n",
    "show_img(torchvision.utils.make_grid(data.next()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует и более сложные способы аугментации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Mixup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Аугументация при помощи синтеза данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_4.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В домашнем задании Вам предстоит провести эксперимент: обучить нейронную сеть на датасете без аугментации, и после нее.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме методов, реализованных в Pytorch, существуют и специализированные библиотеки для аугментации изображений. \n",
    "\n",
    "Например:\n",
    "- [lbumentations](https://albumentations.ai)\n",
    "- [imgaug](https://imgaug.readthedocs.io/en/latest/index.html)\n",
    "- [augly](https://github.com/facebookresearch/AugLy)\n",
    "- [Kornia](https://kornia.readthedocs.io/en/latest/introduction.html)\n",
    "\n",
    "**При выборе методов аугментации имеет смысл использовать те, которые будут в реальной жизни. Например, нет смысла делать перевод изображения в черно-белое, если предполагается, что весь входящий поток будет цветным, или отражать человека вверх-ногами, если мы не предполагаем его таким распознавать.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "----\n",
    "Как быстро обучить нейросеть на своих данных, когда их мало"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для таких типовых задач, как классификация изображений, можно воспользоваться готовой архитектурой (AlexNet, VGG, Inception, ResNet и т.д.) и обучить нейросеть на своих данных. Реализации таких сетей с помощью различных фреймворков уже существуют, так что на данном этапе можно использовать одну из них как черный ящик, не вникая глубоко в принцип её работы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, глубокие нейронные сети требовательны к большим объемам данных для сходимости обучения. И зачастую, в нашей частной задаче недостаточно данных для того, чтобы хорошо натренировать все слои нейросети. `Transfer Learning` решает эту проблему. Зачем обучать сеть заново, если можно использовать уже обученную на миллионе изображений и дообучить на свой датасет?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В PyTorch есть много предобученных сетей: [`torchvision.models`](https://pytorch.org/vision/stable/models.html)\n",
    "\n",
    "Для этого,  нужно отключить какие-то промежуточные слои. Тогда можно использовать то, что называется `Fine turning` - не нужно обучать всю модель, а достаточно только ее новую часть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети, которые используются для классификации, как правило, содержат N выходных нейронов в последнем слое, где N — это количество классов. Такой выходной вектор трактуется как набор вероятностей принадлежности к классу. \n",
    "\n",
    "В реальных задачах распознавания изображений, количество классов может отличаться от того, которое было в исходном датасете. В таком случае нам придётся полностью выкинуть этот последний слой и поставить новый, с нужным количеством выходных нейронов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_5.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачастую в конце классификационных сетей используется полносвязный слой. Так как мы заменили этот слой, использовать предобученные веса для него уже не получится. Придется тренировать его с нуля, инициализировав его веса случайными значениями. Веса для всех остальных слоев мы загружаем из предобученной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные стратегии дообучения модели. Мы воспользуемся следующей: будем тренировать всю сеть из конца в конец (end-to-end), а предобученные веса не будем фиксировать, чтобы дать им немного скорректироваться и подстроиться под наши данные. Такой процесс называется тонкой настройкой (fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Структурные компоненты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи нам понадобятся следующие компоненты:\n",
    "\n",
    "1. Описание модели нейросети\n",
    "2. Пайплайн обучения\n",
    "3. Инференс пайплайн\n",
    "4. Предобученные веса для этой модели\n",
    "5. Данные для обучения и валидации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_6.png\" width=\"650\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код обучения модели состоит из следующих шагов:\n",
    "\n",
    "1. Построение train/validation пайплайнов данных\n",
    "2. Построение train/validation графов (сетей)\n",
    "3. Надстраивание классификационной функция потерь (cross entropy loss) поверх train графа\n",
    "4. Код, необходимый для вычисления точности предсказания на валидационной выборке во время обучения\n",
    "5. Логика загрузки предобученных весов из снэпшота\n",
    "6. Создание различных структур для обучения\n",
    "7. Непосредственно сам цикл обучения (итерационная оптимизация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний слой графа конструируется с нужным нам количеством нейронов и исключается из списка параметров, загружаемых из предобученного снэпшота.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель\n",
    "\n",
    "Давайте рассмотрим пример практической реализации такого подхода ([код переработан из этой статьи](https://learnopencv.com/image-classification-using-transfer-learning-in-pytorch/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Загрузим датасет и удалим из него 90% файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/content/2750/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
    "!unzip EuroSAT.zip\n",
    "\n",
    "from random import sample\n",
    "for folder in os.listdir(path):\n",
    "  files = os.listdir(path+folder)\n",
    "  for file in sample(files,int(len(files)*0.9)):\n",
    "      os.remove(path+folder+'/'+file)\n",
    "      \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим аугментации. Для разнообразия будем использовать родные аугментации из библиотеки PyTorch Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Transforms to the Data\n",
    "image_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # На валидации у нас никаких аугментаций!\n",
    "    \"valid\": transforms.Compose( \n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    # На тесте тоже!\n",
    "    \"test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=128),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=path)\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.1), int(len(dataset)*0.1)])\n",
    "train_set.dataset.transform = image_transforms['train']\n",
    "valid_set.dataset.transform = image_transforms['valid']\n",
    "test_set.dataset.transform = image_transforms['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set), len(valid_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "bs = 64\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "print(idx_to_class)\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size, valid_data_size = len(train_set), len(valid_set)\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_data_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_set, batch_size=bs, shuffle=False)\n",
    "test_data_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас не так уж и много изображений!! Обычная нейронка справиться не должна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим Alexnet без весов и попробуем обучить с 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=False)\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы заменяем последний слой модели Alexnet одним последовательным слоем с 4096 нейронами и который имеет `num_classes` выходов, соответствующих числу классов в нашем подмножестве.\n",
    "\n",
    "То есть мы \"сказали\" нашей модели распознавать не 1000, а только N классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim=1))\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем мы определяем функцию потерь и оптимизатор, который будет использоваться для обучения.\n",
    "\n",
    "Мы используем функцию Negative Loss Likelihood, поскольку она полезна для классификации нескольких классов.\n",
    "\n",
    "И оптимизатор Adam - один из самых популярных оптимизаторов, потому что он может адаптировать скорость обучения для каждого параметра индивидуально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-4)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тренировки и валидации нашей можели напишем отдельную функцию, которую итмеет смысл изучить подробнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "    \"\"\"\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "\n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n",
    "\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clean existing gradients\n",
    "            outputs = model(\n",
    "                inputs\n",
    "            )  # Forward pass - compute outputs on input data using the model\n",
    "            loss = loss_criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagate the gradients\n",
    "            optimizer.step()  # Update the parameters\n",
    "\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # # Compute the accuracy\n",
    "            # ret, predictions = torch.max(outputs.data, 1)\n",
    "            # correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            train_correct += (torch.argmax(outputs, dim=-1)== labels).float().sum()\n",
    "\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        train_accuracy = 100 * train_correct / (len(train_data_loader)*bs)\n",
    "\n",
    "        val_correct = 0\n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.eval()  # Set to evaluation mode\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    inputs\n",
    "                )  # Forward pass - compute outputs on input data using the model\n",
    "                loss = loss_criterion(outputs, labels)  # Compute loss\n",
    "                valid_loss += loss.item() * inputs.size(0)  # Compute the total loss for the batch and add it to valid_loss\n",
    "\n",
    "                val_correct += (torch.argmax(outputs, dim=-1) == labels).float().sum()\n",
    "                # # Calculate validation accuracy\n",
    "                # ret, predictions = torch.max(outputs.data, 1)\n",
    "                # correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # # Convert correct_counts to float and then compute the mean\n",
    "                # acc = torch.mean(correct_counts.float())\n",
    "\n",
    "                # # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                # valid_acc += acc.item() * inputs.size(0)\n",
    "        val_accuracy = 100 * val_correct / (len(valid_data_loader)*bs)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss / (len(train_data_loader)*bs)\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss / (len(valid_data_loader)*bs)\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, train_accuracy, val_accuracy])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(\n",
    "            \"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(\n",
    "                epoch + 1,\n",
    "                avg_train_loss,\n",
    "                train_accuracy.detach().cpu(),\n",
    "                avg_valid_loss,\n",
    "                val_accuracy.detach().cpu(),\n",
    "                epoch_end - epoch_start,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), loss_func, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_fresh.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на графики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну так себе результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробовать использовать *Fine-tunning*. Загрузим предобученную Alexnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del alexnet\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку большинство параметров в нашей предварительно обученной модели уже обучены, мы сбрасываем значение поля requires_grad в значение false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заморозка параметров**: отключение переменных предобученной модели для последующих изменений. \"Замораживая\" переменные предобученной модели мы гарантируем, что обучаться будет только последний классификационный слой, а значения предобученной модели останутся прежними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "trained_model, history = train_and_validate(\n",
    "    alexnet.to(device), loss_func, optimizer, num_epochs\n",
    ")\n",
    "\n",
    "torch.save(history, \"history_finetuning.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history = np.array(history)\n",
    "ax[0].plot(history[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "ax[1].plot(history[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сравним между собой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "history_fresh = np.array(torch.load('history_fresh.pt'))\n",
    "history_finetuning = np.array(torch.load('history_finetuning.pt'))\n",
    "\n",
    "ax[0].plot(history_fresh[:,:2])\n",
    "ax[0].plot(history_finetuning[:,:2])\n",
    "ax[0].legend([\"Train Loss\", \"Val Loss\"])\n",
    "\n",
    "ax[1].plot(history_fresh[:,2:])\n",
    "ax[1].plot(history_finetuning[:,2:])\n",
    "ax[1].legend([\"Train Accuracy\", \"Val Accuracy\"])\n",
    "ax[0].set_xlabel(\"Epoch Number\")\n",
    "ax[1].set_xlabel(\"Epoch Number\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определенно стало лучше =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_image_name, device):\n",
    "    \"\"\"\n",
    "    Function to predict the class of a single test image\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param test_image_name: Test image\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    transform = image_transforms[\"test\"]\n",
    "    test_image = torch.tensor(np.asarray(test_image_name))#[::-1,...].copy())\n",
    "    test_image = transforms.ToPILImage()(test_image)\n",
    "    plt.imshow(test_image)\n",
    "\n",
    "    test_image_tensor = test_image_name.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_image_tensor).to(device)\n",
    "        ps = torch.exp(out).to(device)\n",
    "        topk, topclass = ps.topk(3, dim=1)\n",
    "        for i in range(3):\n",
    "            print(\n",
    "                \"Predcition\",\n",
    "                i + 1,\n",
    "                \":\",\n",
    "                idx_to_class[topclass.cpu().numpy()[0][i]],\n",
    "                \", Score: \",\n",
    "                round(topk.cpu().numpy()[0][i], 2),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[0])\n",
    "predict(\n",
    "    trained_model.to(device),\n",
    "    valid_set[[np.where([x[1] == 0 for x in valid_set])[0]][0][1]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[6])\n",
    "predict(\n",
    "    trained_model,\n",
    "    valid_set[[np.where([x[1] == 6 for x in valid_set])[0]][0][10]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shoud be %s\\n\" % idx_to_class[8])\n",
    "predict(\n",
    "    trained_model,\n",
    "    valid_set[[np.where([x[1] == 8 for x in valid_set])[0]][0][5]][0],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Мы только что увидели, как использовать предварительно обученную модель, обученную для 1000 классов ImageNet.\n",
    "\n",
    "* Она очень эффективно классифицирует изображения, принадлежащие к N различным классам, которые нас интересуют.\n",
    "\n",
    "* Также мы убедидлись в эффективности такого подхода для обучении классификации на небольшом наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot learning\n",
    "\n",
    "[Ищем знакомые лица](https://habr.com/ru/post/317798/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глубокие сверточные нейронные сети стали самыми современными методами для задач классификации изображений. Однако одно из самых больших ограничений - они требуют большого количества размеченных данных.\n",
    "\n",
    "Во многих приложениях иногда невозможно собрать такой объем данных и `One Shot Learning` направлен на решение этой проблемы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предполагается, что при классификации по методу `One Shot Learning` нам требуется только один обучающий пример для каждого класса. Отсюда и название One Shot. Давайте попробуем разобраться на реальном практическом примере."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, мы хотим создать систему распознавания лиц для небольшой организации, в которой всего 10 сотрудников (небольшое количество упрощает задачу).\n",
    "\n",
    "Используя традиционный подход к классификации, мы можем придумать систему, которая выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_7.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблемы:\n",
    "\n",
    "а) Чтобы обучить такую ​​систему, нам сначала потребуется много разных изображений каждого из 10 человек в организации, что может оказаться невозможным. (Представьте, что вы делаете это для организации с тысячами сотрудников).\n",
    "\n",
    "б) Что делать, если новый человек присоединяется к организации или покидает ее? Вам нужно снова взять на себя боль сбора данных и заново обучить всю модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это практически невозможно, особенно для крупных организаций, где набор и увольнение происходит почти каждую неделю.\n",
    "\n",
    "Теперь давайте разберемся, как подойти к этой проблеме, используя классификацию по `One Shot Learning`, которая помогает решить обе указанные выше проблемы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо того, чтобы напрямую классифицировать входное (тестовое) изображение каждого из 10 человек в организации, эта сеть вместо этого принимает дополнительное эталонное изображение человека в качестве входных данных и будет производить оценку сходства, обозначающую шансы того, что два входных изображения принадлежат одному и тому же человеку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что эта сеть не учится классифицировать изображение напрямую по какому-либо из выходных классов. Скорее, он изучает функцию подобия, которая принимает два изображения в качестве входных данных и выражает, насколько они похожи.\n",
    "\n",
    "Как это решает две проблемы, о которых мы говорили выше?\n",
    "- Для обучения этой сети нам не требуется слишком много экземпляров класса, а для построения хорошей модели достаточно лишь нескольких экземпляров.\n",
    "- Но самое большое преимущество в простоте ее обучения в случае появления нового сотрудника."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, чтобы сеть могла обнаружить его лицо, нам требуется только одно изображение его лица, которое будет сохранено в базе данных. Используя его в качестве эталонного изображения, сеть будет вычислять сходство для любого нового экземпляра,  представленного ей.\n",
    "\n",
    "Таким образом, мы говорим, что сеть предсказывает счет за `One Shot` (один выстрел)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код, который мы будем использовать, является реализацией методологии, описанной в этой [исследовательской статье](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) Грегори Коха и др.\n",
    "\n",
    "Архитектура модели соответствует описанию в статье. Но прежде чем углубляться в детали, давайте разберемся с архитектурой на высоком уровне.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_8.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Две показанные выше сверточные нейронные сети не являются разными сетями, а представляют собой две копии одной и той же сети, отсюда и название Siamese Networks.\n",
    "\n",
    "В основном они имеют одинаковые параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два входных изображения (x1 и x2) проходят через ConvNet, чтобы сгенерировать вектор признаков фиксированной длины для каждого (h (x1) и h (x2)).\n",
    "\n",
    "Предполагая, что модель нейронной сети обучена правильно, мы можем сделать следующую гипотезу: если два входных изображения принадлежат одному и тому же персонажу, то их векторы признаков также должны быть похожими, а если два входных изображения принадлежат разным символам, то их векторы признаков также будут разными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, поэлементная абсолютная разница между двумя векторами признаков должна сильно отличаться в обоих вышеупомянутых случаях. И, следовательно, оценка сходства, генерируемая выходным сигмовидным слоем, также должна быть разной в этих двух случаях.\n",
    "\n",
    "Это центральная идея сиамских сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_9.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss\n",
    "\n",
    "Для распознавания лиц, мы будем использовать `Triplet loss`, то есть подавать три изображения вместо двух.\n",
    "\n",
    "Триплет состоит из анкора `anchor`, положительного и отрицательного образцов и в основном применяется для распознавания лиц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_10.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `Triplet loss` расстояние между анкором и положительной выборки минимизировано, а расстояние между анкором и отрицательной выборки максимально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"http://edunet.kea.su/repo/src/L11_Transfer_learning/img_licence/l11_11.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Но можно также использовать и `Contrastive Loss`\n",
    "\n",
    "о ней подробнее в статье [Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример реализации сиамской сети\n",
    "---\n",
    "на примере датасета [Georgia Tech face database](http://www.anefian.com/research/face_reco.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "import shutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as img_transf\n",
    "from torchvision import datasets as ds\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#Фиксируем random_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "#Выставлям device для расчетов\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r data\n",
    "!rm -r GTdb_crop.zip\n",
    "!rm -r GTdb_crop\n",
    "!wget http://www.anefian.com/research/GTdb_crop.zip\n",
    "!unzip GTdb_crop.zip -d /content/GTdb_crop\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"/content/data\")\n",
    "for i in range(1, 51):\n",
    "    pattern = \"s\" + str(i).zfill(2)\n",
    "    path = \"/content/data/\" + str(pattern)\n",
    "    f = sorted(\n",
    "        glob.glob(os.path.join(\"/content/GTdb_crop/cropped_faces/\", pattern + \"*\"))\n",
    "    )\n",
    "    os.mkdir(path)\n",
    "    [shutil.copy(i, path) for i in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"/content/data/\"\n",
    "d = os.listdir(\"/content/data\")\n",
    "for i in range(3):\n",
    "    r = random.choice(d)\n",
    "    shutil.copytree(src + r, \"/content/data/testing/\" + r)\n",
    "    d.remove(r)\n",
    "else:\n",
    "    for i in sorted(d):\n",
    "        shutil.copytree(src + i, \"/content/data/training/\" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiaDataset(Dataset):\n",
    "    def __init__(self, imageDir, image_transforms=None):\n",
    "        self.imageDir = imageDir\n",
    "        self.classes = self.imageDir.classes\n",
    "        self.imgs = self.imageDir.imgs\n",
    "        self.transform = image_transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.target = np.random.randint(\n",
    "            0, 2\n",
    "        )  # Выберем либо положительную пару (0), либо отрицательную (1)\n",
    "\n",
    "        img1, label1 = self.imgs[index]  # Выберем первое изображение\n",
    "\n",
    "        new_imgs = list(\n",
    "            set(self.imgs) - set(self.imgs[index])\n",
    "        )  # Создадим новый индекс, что бы не повторять уже выбранное изображение\n",
    "        length = len(new_imgs)\n",
    "\n",
    "        if self.target == 1:\n",
    "            # Найдем отрицательный пример\n",
    "            label2 = label1\n",
    "            while label2 == label1:\n",
    "                choice = np.random.choice(length)\n",
    "                img2, label2 = new_imgs[choice]\n",
    "        else:\n",
    "            # Найдем положительный пример\n",
    "            label2 = label1 + 1\n",
    "            while label2 != label1:\n",
    "                choice = np.random.choice(length)\n",
    "                img2, label2 = new_imgs[choice]\n",
    "\n",
    "        img1 = Image.open(img1).convert(\"RGB\")\n",
    "        img2 = Image.open(img2).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return (img1, img2, self.target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64 * 11 * 11, 4096), nn.Sigmoid(), nn.Linear(4096, 128)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(-1, 64 * 11 * 11)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        pos = (1 - label) * torch.pow(euclidean_distance, 2)\n",
    "        neg = (label) * torch.pow(\n",
    "            torch.clamp(self.margin - euclidean_distance, min=0.0), 2\n",
    "        )\n",
    "        loss_contrastive = torch.mean(pos + neg)\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательная функция\n",
    "def show(img, text=None):\n",
    "    img = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(75, 120, text, fontweight=\"bold\")\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применим аугментации для train\n",
    "im_trans_train = img_transf.Compose(\n",
    "    [\n",
    "        img_transf.Resize((105, 105)),\n",
    "        img_transf.RandomPerspective(),\n",
    "        img_transf.RandomRotation((-15, 15)),\n",
    "        img_transf.RandomHorizontalFlip(),\n",
    "        img_transf.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        img_transf.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Применим аугментации для test\n",
    "im_trans_test = img_transf.Compose(\n",
    "    [img_transf.Resize((105, 105)), img_transf.ToTensor()]\n",
    ")\n",
    "\n",
    "folder_dataset = ds.ImageFolder(\"/content/data/training\")\n",
    "sia_dataset = SiaDataset(\n",
    "    imageDir=folder_dataset,\n",
    "    image_transforms=im_trans_train,\n",
    ")\n",
    "\n",
    "# Создадим случайную инициализацию seed, что бы каждый раз действительно выбирать новые изображения\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    sia_dataset,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "folder_dataset = ds.ImageFolder(\"/content/data/testing\")\n",
    "\n",
    "siatest_dataset = SiaDataset(\n",
    "    imageDir=folder_dataset,\n",
    "    image_transforms=im_trans_test,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    siatest_dataset,\n",
    "    num_workers=2,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=worker_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "net = Siamese().to(device)\n",
    "loss_fun = ContrastiveLoss(margin=1.5)\n",
    "optims = optim.Adam(net.parameters(), lr=3e-4)\n",
    "scheduler = ReduceLROnPlateau(optims, \"min\", verbose=True, factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir saved_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epoch, net, optims, scheduler, save=True, path=\"saved_weight\"):\n",
    "    counter = []\n",
    "    l = []\n",
    "\n",
    "    iter_num = 0\n",
    "\n",
    "    for epoch in range(0, n_epoch):\n",
    "        \n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            img0, img1, label = data\n",
    "            img0, img1, label = img0.to(device), img1.to(device), label.to(device)\n",
    "            optims.zero_grad()\n",
    "            out = net(img0, img1)\n",
    "            op1, op2 = net(img0, img1)\n",
    "            loss = loss_fun(op1, op2, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optims.step()\n",
    "\n",
    "            if i % 12 == 0:\n",
    "                print(\"Epoch {} with {} loss\".format(epoch, loss.item()))\n",
    "                iter_num += 5\n",
    "                counter.append(iter_num)\n",
    "                l.append(loss.item())\n",
    "                if save:  # будем сохранять все этапы обучения модели\n",
    "                    torch.save(net.state_dict(), path + \"/epoch_%i.pt\" % epoch)\n",
    "                scheduler.step(loss)  # используем подстройку lr при достижении \"плато\"\n",
    "    return counter, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter, l = train(30, net, optims, scheduler, save=True, path=\"saved_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(counter, l)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(l, path=\"saved_weight\"):\n",
    "    distances_pos = []\n",
    "    distances_neg = []\n",
    "\n",
    "    net = Siamese().to(device)\n",
    "\n",
    "    best_epoch = np.argmin(l)\n",
    "    print(\"Лучшая эпоха: %i. Loss = %.3f\" % (best_epoch, l[best_epoch]))\n",
    "\n",
    "    # Загрузим лучушю эпоху из сохранения\n",
    "    net.load_state_dict(torch.load(path + \"/epoch_%i.pt\" % best_epoch))\n",
    "    net.eval()\n",
    "\n",
    "    for i, data in enumerate(test_dataloader, 0):\n",
    "        img0, img1, label = data\n",
    "        concatenated = torch.cat((img0, img1))\n",
    "        output1, output2 = net(Variable(img0).to(device), Variable(img1).to(device))\n",
    "        distance = F.pairwise_distance(output1, output2)\n",
    "        if label == 0:\n",
    "            distances_pos.append(distance.item())\n",
    "            if i % 50:\n",
    "                show(\n",
    "                    torchvision.utils.make_grid(concatenated),\n",
    "                    \"Один и тот же! Missmatch: {:.3f}\".format(distance.item()),\n",
    "                )\n",
    "        else:\n",
    "            distances_neg.append(distance.item())\n",
    "            if i % 50:\n",
    "                show(\n",
    "                    torchvision.utils.make_grid(concatenated),\n",
    "                    \"Другой человек! Missmatch: {:.3f}\".format(distance.item()),\n",
    "                )\n",
    "    return distances_pos, distances_neg\n",
    "\n",
    "\n",
    "distances_pos, distances_neg = plot_images(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так оценить работу сложно, давайте посмотрим на распределение расстояний по категориям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "distnaces = {\"Один и тот же\": distances_pos, \"Другой человек\": distances_neg}\n",
    "\n",
    "ax = sns.displot(distnaces, kde=True, stat=\"density\")\n",
    "ax.set(xlabel=\"Pairwise distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто, когда мы пишем и обучаем сети (будь то с нуля или с помощью transfer learning) мы вынуждены угадывать гиперпараметры (lr, betas и тд). В случае с learning rate нам есть от чего оттолкнуться (маленький lr для transfer learning), [константа Karpathy (3e-4)](https://twitter.com/karpathy/status/801621764144971776?lang=en) для Adam, но все же, такой подход не кажется оптимальным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации гиперпараметров существуют готовые решения, которые используют различные методы black-box оптимизации. Разберем одну из наиболее популярных библиотек - [**Optuna**](https://optuna.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте оптимизируем наш learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "\n",
    "# Определим функицю которую будем оптимизировать\n",
    "def objective(trial):\n",
    "    # Создадим границы для \"предположений\" оптимизатора\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "\n",
    "    # Будем каждый раз пересоздавать нашу сеть и все ее парамеры\n",
    "    net = Siamese().to(device)\n",
    "    loss_fun = ContrastiveLoss(margin=1.5)\n",
    "    optims = optim.Adam(\n",
    "        net.parameters(), lr=lr\n",
    "    )  # Теперь lr предлагает optuna в пределах заданых границ\n",
    "    scheduler = ReduceLROnPlateau(optims, \"min\", verbose=True, factor=0.5, patience=3)\n",
    "\n",
    "    # Для экономии времени возьмем только 3 эпохи\n",
    "    counter, l = train(3, net, optims, scheduler, save=False)\n",
    "    return l[-1]\n",
    "\n",
    "\n",
    "# Создадим \"исследование\",\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"Optimal lr\")\n",
    "\n",
    "#\n",
    "study.optimize(\n",
    "    objective, n_trials=15\n",
    ")  # Чем больше итераций - тем выше шансы словить самые оптимальные гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на лучшие параметры\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно же, можно оптимизировать сразу несколько параметров за раз, а еще в качестве параметров можеть быть сама архитектура сети (например количесвто слоев и каналов). Это можно легко сделать по анологии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на историю оптимизации нашего lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, проверим а станет ли реально лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir 'optimal_weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Siamese().to(device)\n",
    "loss_fun = ContrastiveLoss(margin=1.5)\n",
    "optims = optim.Adam(\n",
    "    net.parameters(), lr=study.best_params[\"lr\"]\n",
    ")  # Возьмем lr, который нам предлагает Optuna\n",
    "scheduler = ReduceLROnPlateau(optims, \"min\", verbose=True, factor=0.5, patience=3)\n",
    "\n",
    "counter, l_optim = train(30, net, optims, scheduler, save=True, path=\"optimal_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(counter, l, label=\"no optimization\")\n",
    "plt.plot(counter, l_optim, label=\"optimal params\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_pos, distances_neg = plot_images(l_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "distnaces_optim = {\"Один и тот же\": distances_pos, \"Другой человек\": distances_neg}\n",
    "\n",
    "ax_0 = sns.displot(distnaces, kde=True, stat=\"density\", alpha=0.5)\n",
    "ax_1 = sns.displot(distnaces_optim, kde=True, stat=\"density\", alpha=0.5)\n",
    "ax_0.set(xlabel=\"Pairwise distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "---\n",
    "\n",
    "На этом уроке мы затронули проблемы, которые возникают при обучени на реальных данных.\n",
    "\n",
    "Одна из основных проблем - малые датасеты. Для того, чтобы обучить нейронку на небольшом датасете можно использовать:\n",
    "- `аугментацию`\n",
    "- `Tranfer learning`\n",
    "Однако необходимо помнить, что ни один из этих методов не защитит от ситуации, когда реальные данные будут сильно отличаться от тренировочных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае, когда у нас не только мало данных, но и еще и очень большое (возможно, неизвестное) число классов, можно воспользоваться `One-shot Learning`. В этом случае нейронка обучается не классифицировать изображения, а, наоборот, находить различия между классом и новыми данными. Для этого используются нейронные сети, относящиеся к классу сиамских нейронных сетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же мы разобрали автоматическую оптимизацию гиперпараметров с помощью Optuna и показали, что это эффективный способ сделать нейросеть лучше малой ценой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Литература\n",
    "---\n",
    "\n",
    "### Augmentation\n",
    "\n",
    "[TORCHVISION.TRANSFORMS](https://pytorch.org/vision/stable/transforms.html)\n",
    "\n",
    "[A survey on Image Data Augmentation for Deep Learning](https://link.springer.com/article/10.1186/s40537-019-0197-0)\n",
    "\n",
    "[Data augmentation for improving deep learning in image classification problem](https://www.researchgate.net/publication/325920702_Data_augmentation_for_improving_deep_learning_in_image_classification_problem)\n",
    "\n",
    "[Albumentations](https://github.com/albumentations-team/albumentations)\n",
    "\n",
    "\n",
    "\n",
    "### Transfer Learning\n",
    "[How To Do Transfer Learning For Computer Vision | PyTorch Tutorial](https://www.youtube.com/watch?v=6nQlxJvcTr0)\n",
    "\n",
    "[TRANSFER LEARNING FOR COMPUTER VISION TUTORIAL](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "\n",
    "[Python Pytorch Tutorials # 2 Transfer Learning : Inference with ImageNet Models](https://www.youtube.com/watch?v=Upw4RaERZic)\n",
    "\n",
    "[PyTorch - The Basics of Transfer Learning with TorchVision and AlexNet](https://www.youtube.com/watch?v=8etkVC93yU4)\n",
    "\n",
    "### One-shot Learning\n",
    "\n",
    "[One Shot Learning with Siamese Networks using Keras](https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d)\n",
    "\n",
    "[One-shot image classification by meta learning](https://medium.com/nerd-for-tech/one-shot-learning-fe1087533585)\n",
    "\n",
    "[Siamese Neural Networks for One-shot Image Recognition](http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf)\n",
    "\n",
    "[One-shot learning (Part 1/2): Definitions and fundamental techniques](https://heartbeat.fritz.ai/one-shot-learning-part-1-2-definitions-and-fundamental-techniques-1df944e5836a)\n",
    "\n",
    "[One-Shot Learning (Part 2/2): Facial Recognition Using a Siamese Network](https://heartbeat.fritz.ai/one-shot-learning-part-2-2-facial-recognition-using-a-siamese-network-5aee53196255)\n",
    "\n",
    "[FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "\n",
    "[One shot learning explained using FaceNet](https://medium.com/intro-to-artificial-intelligence/one-shot-learning-explained-using-facenet-dff5ad52bd38)\n",
    "\n",
    "\n",
    "### Hyperparameter optimization\n",
    "[Tuning Hyperparameters with Optuna](https://towardsdatascience.com/tuning-hyperparameters-with-optuna-af342facc549)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
